{"cells":[{"metadata":{},"cell_type":"markdown","source":"# WHAT IS A TOPIC MODELLING?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"A type of statistical modelling for discovering the abstract \"topics\" that occur in a collection of documents.\nA document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/trump-tweets/trumptweets.csv\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0, 15000):\n    review = re.sub('(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"', ' ', df['content'][i])\n    review = review.lower()\n    review = review.split()\n    lm= WordNetLemmatizer() \n    review = [lm.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n    review = ' '.join(review)\n    clean.append(review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['content'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# All the contents are cleaned..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new=pd.DataFrame(df['content'][0:15000])\ndf_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new['tweets']=clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new['tokens']=df_new['tweets'].apply(word_tokenize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new['tokens'][90]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new['tokens'][90]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect = CountVectorizer().fit(df_new['tokens'][90])\nbag_of_words = vect.transform(df_new['tokens'][90])\nsum_words = bag_of_words.sum(axis=0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum_words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get most Frequent Words..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def most_freq_words(s, n=None):\n    vect = CountVectorizer().fit(s)\n    bag_of_words = vect.transform(s)\n    sum_words = bag_of_words.sum(axis=0) \n    freq = [(word, sum_words[0, idx]) for word, idx in vect.vocabulary_.items()]\n    freq =sorted(freq, key = lambda x: x[1], reverse=True)\n    return freq[:n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_freq_words([ word for tweet in df_new.tokens for word in tweet],20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Least Frequent Words..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def least_freq_words(s, n=None):\n    vect = CountVectorizer().fit(s)\n    bag_of_words = vect.transform(s)\n    sum_words = bag_of_words.sum(axis=0) \n    freq = [(word, sum_words[0, idx]) for word, idx in vect.vocabulary_.items()]\n    freq =sorted(freq, key = lambda x: x[1], reverse=False)\n    return freq[:n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"least_freq_words([ word for tweet in df_new.tokens for word in tweet],20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new['tokens'][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(min_df=0)# Here \"min_df\" in the parameter refers to the minimum document frequency and the vectorizer will simply drop all words that occur less than that value set (either integer or in fraction form)\nsentence_transform = vectorizer.fit_transform(df_new['tweets'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence_transform","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence_transform.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\nThe vectorized array looks like:\\n {}\".format(sentence_transform.toarray()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import LatentDirichletAllocation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda = LatentDirichletAllocation(n_components=8, max_iter=5,\n                                learning_method = 'online',\n                                learning_offset = 50.,\n                                random_state = 0)\n#n_components are the number of topics you want to classify.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda.fit(sentence_transform)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words=[]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Arg Sort-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"in_arr = np.array([ 2, 0,  1, 5, 4, 1, 9]) \nprint (\"Input unsorted array : \", in_arr)  \n  \nout_arr = np.argsort(in_arr) \nprint (\"Output sorted array indices : \", out_arr) \nprint(\"Output sorted array in Ascending Order: \", in_arr[out_arr]) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Argsort gives the indices in ascending order by default. To make it descending order we have to add [::-1] with the array.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"out_arr_new=np.argsort(in_arr)[::-1]\nprint(\"Output Sorted Array in Descending Order\",in_arr[out_arr_new])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # Now if we want top 4 numbers in descending order..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"out_arr_new=np.argsort(in_arr)[::-1][:4]\nprint(\"Output Sorted Array in Descending Order\",in_arr[out_arr_new])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out_arr_new=np.argsort(in_arr)[:-4-1:-1]\nprint(\"Output Sorted Array in Descending Order\",in_arr[out_arr_new])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# This is another way to get top 4 numbers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_top_words(model, feature_names, n_top_words):\n    for index, topic in enumerate(model.components_):\n        message = \"\\nTopic{}:\".format(index)\n        message += \" \".join([feature_names[i] for i in topic.argsort()[::-1][:n_top_words]])\n        print(message)\n        words.append(message)\n        print(\"=\"*170)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_top_words = 40\nprint(\"\\nTopics in LDA model: \")\ntf_feature_names = vectorizer.get_feature_names()\nprint_top_words(lda, tf_feature_names, n_top_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_topic = lda.components_[0]\nsecond_topic = lda.components_[1]\nthird_topic = lda.components_[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_topic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_topic.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"second_topic.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,8):\n    wordcloud = WordCloud(max_font_size=40, max_words=40).generate(words[i])\n\n# Display the generated image:\n    plt.figure(figsize=(10,10))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}