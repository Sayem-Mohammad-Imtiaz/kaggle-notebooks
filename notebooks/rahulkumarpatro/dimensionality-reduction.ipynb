{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train=pd.read_csv(\"/kaggle/input/mnist-in-csv/mnist_train.csv\")\ndf_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test=pd.read_csv(\"/kaggle/input/mnist-in-csv/mnist_test.csv\")\ndf_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=df_train.drop('label',axis=1)\nY_train=df_train['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test=df_test.drop('label',axis=1)\nY_test=df_test['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(X_train.iloc[9000].to_numpy().reshape(28,28))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train[9000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Here we are seeing that each image have 28 features due to which the training time increases and performance sometimes decreases..","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# PCA Compression-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#we will keep track of the time taken to perform each transformation and training.\nimport time","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's compress our data using PCA to a degree that preserves 95% variance of the data and only losses only 5%.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca=PCA(n_components=0.95)\nstart = time.time()\n\nX_red= pca.fit_transform(X_train)\nend = time.time()\n\nend - start","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# So PCA took 6 seconds to compress the data..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_red.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.n_components_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# So, only 154 out of 784 features can preserve 95% of the data, \n# This means that the MNIST is originally very sparse and most of the data is rather present at a much lower dimension. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(X_train.iloc[9000].to_numpy().reshape(28,28))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(X_red[1].reshape(7,22))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Nothing is visible..","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Now lets reverse back the compression, using PCA reverse i. inverse transformation. While it reverses the dataset back to having 784 features but the information lost(5%) due to compression never gets recovered.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_return=pca.inverse_transform(X_red)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_return.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# See our dimension is back with 784 features..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(X_return[9000].reshape(28,28))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# See our image is blurred much looking like it has lost dimensions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1,2)\nax1.imshow(X_train.iloc[9000].to_numpy().reshape(28,28))\nax2.imshow(X_return[9000].reshape(28, 28))\nfig.suptitle('Compression and Decompression')\nax1.axis('off')\nax2.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# So here we see losing 5% of the image still looks acceptable.\n# And we have significantly reduced the number of dimensions as well, from 784 to 154 trading a 5% loss in image quality..","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# We have seen PCA helps in reducing the size of data upto a harmless level.\n# Now lets check does the reduced number of dimensions helps in faster training?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# First trying for Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_clf = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs', random_state = 42)\nt_start = time.time()\nlog_clf.fit(X_train, Y_train)\nt_end= time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_end-t_start","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_clf.score(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now lets check for our reduced set..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlog_clf = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs', random_state = 42)\nt_start = time.time()\nlog_clf.fit(X_red, Y_train)\nt_end= time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_end-t_start","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_red_test = pca.transform(X_test)\nlog_clf.score(X_red_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_red_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # See here we see that it was computed 3 times faster than our previous model and accuracy is somewaht similar..","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Lets try for a Random Forest Model-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100, random_state = 42)\nt_start = time.time()\nrfc.fit(X_train,Y_train)\nt_end = time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_end-t_start","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc.score(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now lets check for our reduced set..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=100, random_state = 42)\nt_start = time.time()\nrfc.fit(X_red,Y_train)\nt_end = time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_end-t_start","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc.score(X_red_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# That's bad! Training took more than twice the time it took on original dataset. There is a drop in performance as well! So PCA didn't really help in this case. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# FINDINGS-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. We saw that PCA didn't help the Random Forest rather slowed down the training and even worsened the performance.\n# 2. While in case of Logistic Regression, PCA helped and boosted the training nearly 3 times faster with similar performance..","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# CONCLUSION-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. There you have it clearly, PCA helps,but not always\n# 2. Infact, Dimensionality Reduction does not always leads to faster training, it rather depends on the dataset, the model and the training algorithm used.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Now trying some Non-Linear Dimensionality Reduction--> ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# T-SNE-->\nSince TSNE scales extremely slowly with large dataset, we will not use the full data, rather a sample of just 10000 instances.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['label'] = Y_train\nX = X_train.sample(n=10000, random_state=42)\n\nY = X['label']\nX = X.drop('label', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will use TSNE to reduce the datset down to 2 Dimensions and then plot it using Matplotlib\nfrom sklearn.manifold import TSNE\ntsne = TSNE(n_components = 2, random_state = 42)\nt_start = time.time()\nX_reduced = tsne.fit_transform(X)\nt_end = time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_end-t_start","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# So it took around four minutes to to compress the data to 2 dimensions with 10000 smaples..","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Now plotting with Matplotlib-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nplt.scatter(X_reduced[:,0], X_reduced[:,1], c = Y, cmap='jet')\nplt.colorbar()\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThat looks quite nice! we can see clear separation of clusturs. while only a couple of these clusters seems to overlap, like 3s & 5s and 9s & 4s.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# We can now try for PCS+TSNE-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\n# Using Pipelines..\npca_tsne = Pipeline([\n    ('pca', PCA(n_components=0.95, random_state=42)),\n    ('tsne', TSNE(n_components=2, random_state=42)),\n])\nt_start = time.time()\nX_new = pca_tsne.fit_transform(X)\nt_end = time.time()\nprint(t_end-t_start)\n\nplt.figure(figsize=(12, 8))\nplt.scatter(X_new[:,0], X_new[:,1], c = Y, cmap='jet')\nplt.colorbar()\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nWOW! Did you notice what happened ? The result is quite exactly similar to that of using TSNE alone. But the time is reduced to about half of orginial time(just TSNE).\n\nWell, we already saw earlier that PCA is very fast compressor than others when it comes to large datasets, but algorithms like TSNE creats far better clusters than PCA, it make sense combining PCA(to quickly get rid of useless dimensions) and TSNE(a slower reduction algorithm reducing less heavy data to 2 Dimensions to make good clusters). This can significantly reduce the time.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Now trying some more methods:-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# LLE: Locally Linear Embedding:-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import LocallyLinearEmbedding\nt_start = time.time()\nX_lle = LocallyLinearEmbedding(n_components=2, random_state=42).fit_transform(X)\nt_end = time.time()\nprint(t_end-t_start)\n\nplt.figure(figsize=(12, 8))\nplt.scatter(X_lle[:,0], X_lle[:,1], c = Y, cmap='jet')\nplt.colorbar()\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" It took a while, and also, the visualization is not at all appealing.\n\nlet's now chain this with PCA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# PCA+LLA:-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_lle = Pipeline([\n    (\"pca\", PCA(n_components=0.95, random_state=42)),\n    (\"lle\", LocallyLinearEmbedding(n_components=2, random_state=42)),\n])\nt_start = time.time()\nX_new1= pca_lle.fit_transform(X)\nt_end = time.time()\nprint(t_end-t_start)\n\nplt.figure(figsize=(12, 8))\nplt.scatter(X_new1[:,0], X_new1[:,1], c = Y, cmap='jet')\nplt.colorbar()\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, while the results were same, the time was quite reduced. That's what we had expected!\n\nLet's try a last one! LDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Linear Discriminant Analysis(LDA):-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nt_start = time.time()\nX_lda = LinearDiscriminantAnalysis(n_components=2).fit_transform(X, Y)\nt_end = time.time()\nprint(t_end-t_start)\n\nplt.figure(figsize=(12, 8))\nplt.scatter(X_lda[:,0], X_lda[:,1], c = Y, cmap='jet')\nplt.colorbar()\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Wow! this was quite faster! Although the clusturs are fine, not good.!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# And I guess, We have a clear Winner here, yes..that's TSNE !! It was faster when chained with PCA and the results as well were quite better than others.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# FINDINGS:-->\n\n# 1. TSNE out-performed other algorithm at making clear clusters.\n# 2. PCA helped other algorithms to perform faster reduction.\n# 3. PCA scales faster than other algorithms but is not that good in creating clusters.\n# 4. Manifold based algorithms scale very poorly with larger dataset, hence are very slow.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# CONCLUSIONS:-->\n\n# 1. Manifold based reduction methods scale very poorly with larger dataset, hence are very slow.\n# 2. Chaining PCA with Manifold based reduction methods can help them scale faster ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}