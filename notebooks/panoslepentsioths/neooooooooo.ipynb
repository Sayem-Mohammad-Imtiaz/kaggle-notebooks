{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 3. Import libraries and modules\n!pip install git+https://github.com/qubvel/classification_models.git\nfrom classification_models.keras import Classifiers\nimport numpy as np\nnp.random.seed(123)  # for reproducibility\nimport pandas as pd\nimport numpy as np\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\n!pip install adabelief-tf==0.1.0\nfrom adabelief_tf import AdaBeliefOptimizer\n!pip install keras-rectified-adam\nfrom keras_radam.training import RAdamOptimizer\n!pip install keras-adabound\nfrom keras_adabound import AdaBound\n!pip install git+https://github.com/tensorflow/addons\nimport tensorflow_addons\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n\nbase_dir = '/kaggle/input/104-flowers-garden-of-eden/jpeg-192x192'\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'val')\n\n\nfrom keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale=1./255)\nval_datagen = ImageDataGenerator(rescale=1./255,validation_split=0.2)\n\ntrain_generator = train_datagen.flow_from_directory(train_dir,\n                                                    batch_size=32,\n                                                    class_mode='categorical',\n                                                    # color_mode='grayscale',\n                                                    target_size=(192,192),\n                                                    shuffle=True,\n                                                    subset='training', seed=123)     \n\nvalidation_generator =  val_datagen.flow_from_directory(val_dir,\n                                                        batch_size=32,\n                                                        class_mode='categorical',\n                                                         # color_mode='grayscale',\n                                                         target_size=(192,192),\n                                                        subset='validation', seed=123) ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model =keras.applications.EfficientNetB2(weights=None,input_shape=(192,192,3),include_top=False)\nx = keras.layers.GlobalAveragePooling2D()(base_model.output)\noutput = keras.layers.Dense(104, activation='softmax')(x)\nmodel = keras.models.Model(inputs=[base_model.input], outputs=[output])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k = []\nok = 0\nyes = 0\nzet = 0\nyep = 0\n\n\nopt =  [keras.optimizers.Adam(learning_rate=0.001),\n                  keras.optimizers.SGD(learning_rate=0.1),\n                  keras.optimizers.Nadam(learning_rate=0.001),\n                  keras.optimizers.RMSprop(learning_rate=0.001),\n                  AdaBeliefOptimizer(learning_rate=0.001),\n                  RAdamOptimizer(learning_rate=0.001),\n                  AdaBound(learning_rate=0.001, final_lr=0.1),\n                  tensorflow_addons.optimizers.yogi.Yogi(learning_rate=0.001)]\n        \nfor t in opt:\n    del model\n    del base_model\n    del x\n    del output\n    if (t == opt[0]):\n        print(\"-------------------------------------optimizer = Adam-------------------------------------\")\n    elif (t == opt[1]):\n            print(\"-------------------------------------optimizer = SGD-------------------------------------\")\n    elif (t == opt[2]):\n            print(\"-------------------------------------optimizer = Nadam-------------------------------------\")\n    elif (t == opt[3]):\n            print(\"-------------------------------------optimizer = RMSprop-------------------------------------\")\n    elif (t == opt[4]):\n            print(\"-------------------------------------optimizer = AdaBelief-------------------------------------\")\n    elif (t == opt[5]):\n            print(\"-------------------------------------optimizer = RAdam-------------------------------------\")\n\n    elif (t == opt[6]):\n            print(\"-------------------------------------optimizer = AdaBound-------------------------------------\")\n\n    elif (t == opt[7]):\n            print(\"-------------------------------------optimizer = Yogi-------------------------------------\")\n\n    base_model =keras.applications.EfficientNetB2(weights=None,input_shape=(192,192,3),include_top=False)\n    x = keras.layers.GlobalAveragePooling2D()(base_model.output)\n    output = keras.layers.Dense(104, activation='softmax')(x)\n    model = keras.models.Model(inputs=[base_model.input], outputs=[output])\n    # 8. Compile model\n    model.compile(loss='categorical_crossentropy',\n              optimizer = t,\n              metrics=['accuracy'])\n\n\n       # 9. Fit model on training data\n    k.append(model.fit(train_generator, \n           epochs=34,validation_data=validation_generator, verbose=1))\n\n\n\nwhile (ok<len(k)):  \n\n\n\n    plt.figure(figsize=(10,10))\n    plt.plot(k[ok+0].history['accuracy'],color='b', label=\"opt = Adam\")\n    plt.plot(k[ok+1].history['accuracy'],color='r', label=\"opt = SGD\")\n    plt.plot(k[ok+2].history['accuracy'],color='g', label=\"opt = Nadam\")\n    plt.plot(k[ok+3].history['accuracy'],color='c', label=\"opt = RMSprop\")\n    plt.plot(k[ok+4].history['accuracy'],color='m', label=\"opt = AdaBelief\")\n    plt.plot(k[ok+5].history['accuracy'],color='k', label=\"opt = RAdam\")\n    plt.plot(k[ok+6].history['accuracy'],color='y', label=\"opt = AdaBound\")\n    plt.plot(k[ok+7].history['accuracy'],color='#A18820', label=\"opt = Yogi\")\n    \n    plt.xlabel('Epochs')\n    plt.ylabel('accuracy')\n    plt.legend()\n    plt.show() \n\n    ok +=8\n\n\n\n\n\n\n\nwhile (yes<len(k)):   \n\n\n\n    plt.figure(figsize=(10,10))\n    plt.plot(k[yes+0].history['loss'],color='b', label=\"opt = Adam\")\n    plt.plot(k[yes+1].history['loss'],color='r', label=\"opt = SGD\")\n    plt.plot(k[yes+2].history['loss'],color='g', label=\"opt = Nadam\")\n    plt.plot(k[yes+3].history['loss'],color='c', label=\"opt = RMSprop\")\n    plt.plot(k[yes+4].history['loss'],color='m', label=\"opt = AdaBelief\")\n    plt.plot(k[yes+5].history['loss'],color='k', label=\"opt = RAdam\")\n    plt.plot(k[yes+6].history['loss'],color='y', label=\"opt = AdaBound\")\n    plt.plot(k[yes+7].history['loss'],color='#A18820', label=\"opt = Yogi\")\n    \n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show() \n\n    yes+=8\n\n\n\n\nwhile (zet<len(k)):   \n\n\n\n    plt.figure(figsize=(10,10))\n    plt.plot(k[zet+0].history['val_accuracy'],color='b', label=\"opt = Adam\")\n    plt.plot(k[zet+1].history['val_accuracy'],color='r', label=\"opt = SGD\")\n    plt.plot(k[zet+2].history['val_accuracy'],color='g', label=\"opt = Nadam\")\n    plt.plot(k[zet+3].history['val_accuracy'],color='c', label=\"opt = RMSprop\")\n    plt.plot(k[zet+4].history['val_accuracy'],color='m', label=\"opt = AdaBelief\")\n    plt.plot(k[zet+5].history['val_accuracy'],color='k', label=\"opt = RAdam\")\n    plt.plot(k[zet+6].history['val_accuracy'],color='y', label=\"opt = AdaBound\")\n    plt.plot(k[zet+7].history['val_accuracy'],color='#A18820', label=\"opt = Yogi\")\n    \n    plt.xlabel('Epochs')\n    plt.ylabel('val_accuracy')\n    plt.legend()\n    plt.show() \n\n    zet+=8\n\n\n\n\n\nwhile (yep<len(k)):   \n\n\n\n    plt.figure(figsize=(10,10))\n    plt.plot(k[yep+0].history['val_loss'],color='b', label=\"opt = Adam\")\n    plt.plot(k[yep+1].history['val_loss'],color='r', label=\"opt = SGD\")\n    plt.plot(k[yep+2].history['val_loss'],color='g', label=\"opt = Nadam\")\n    plt.plot(k[yep+3].history['val_loss'],color='c', label=\"opt = RMSprop\")\n    plt.plot(k[yep+4].history['val_loss'],color='m', label=\"opt = AdaBelief\")\n    plt.plot(k[yep+5].history['val_loss'],color='k', label=\"opt = RAdam\")\n    plt.plot(k[yep+6].history['val_loss'],color='y', label=\"opt = AdaBound\")\n    plt.plot(k[yep+7].history['val_loss'],color='#A18820', label=\"opt = Yogi\")\n    \n    plt.xlabel('Epochs')\n    plt.ylabel('val_loss')\n    plt.legend()\n    plt.show() \n\n    yep+=8","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}