{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import savefig\nimport cv2\nnp.set_printoptions(threshold=np.inf)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nuse_cuda=torch.cuda.is_available()\nuse_cuda","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !ls ../input/flickr-image-dataset/flickr30k_images/flickr30k_images/flickr30k_images/ ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir ../working/flickr_sample\n!mkdir ../working/test/\n!mkdir ../working/flickr_sample/flickr_sample\n!mkdir ../working/test/test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!find ../input/flickr-image-dataset/flickr30k_images/flickr30k_images/flickr30k_images/  -maxdepth 1 -type f | head -128 | xargs cp -t ../working/flickr_sample/flickr_sample\n!find ../input/flickr-image-dataset/flickr30k_images/flickr30k_images/flickr30k_images/  -maxdepth 1 -type f | tail -128 | xargs cp -t ../working/test/test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_size=512\ntransform=transforms.Compose([\n        transforms.Resize((image_size,image_size)),\n#         transforms.RandomRotation(degrees=15),\n#         transforms.ColorJitter(),\n#         transforms.RandomHorizontalFlip(),\n#         transforms.CenterCrop(size=224),  # Image net standards\n        transforms.ToTensor()\n#         transforms.Normalize([0.485, 0.456, 0.406],\n#                              [0.229, 0.224, 0.225])  # Imagenet standards\n    ])\ntrain_path=str('../working/flickr_sample')\ntest_path=str('../working/test')\n\n# load the data using ImageFolder\ndata = datasets.ImageFolder(root=train_path,transform=transform)\ntest_data = datasets.ImageFolder(root=test_path,transform=transform)\n\nnum_workers = 0\nbatch_size = 16\n\ndata_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, num_workers=num_workers,shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers,shuffle=True)\nprint(len(data_loader))\nprint(len(test_loader))\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_loader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataiter = iter(data_loader)\n# images,_ = dataiter.next()\n# images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gray_image = cv2.cvtColor(images[0].numpy().transpose((1,2,0)), cv2.COLOR_RGB2GRAY)\n# gray_image   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef func_imshow(image, ax=None, title=None, normalize=True,gray=False):\n    \"\"\"Imshow for Tensor.\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n    image = image.numpy().transpose((1, 2, 0))\n\n    if normalize:\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image = std * image + mean\n        image = np.clip(image, 0, 1)\n\n    if gray is True:\n        ax.imshow(np.dot(image[...,:3], [0.299, 0.587, 0.114]),cmap='gray')\n    else:\n        ax.imshow(image)\n        \n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n\n    return ax\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Input Image"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimages, labels = next(iter(data_loader))\nplot = func_imshow(images[0], normalize=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Target Image"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplot = func_imshow(images[0], normalize=False,gray=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining AutoEncoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass AutoEncoder(nn.Module):\n    def __init__(self):\n        super(AutoEncoder, self).__init__()\n        ## encoder layers ##\n        self.conv1 = nn.Conv2d(3, 128 , 3, padding=1)  \n        self.conv2 = nn.Conv2d(128, 32, 3, padding=1)\n#         self.conv3 = nn.Conv2d(32, 8, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        ## decoder layers ##\n        self.t_conv1 = nn.Conv2d(32, 8, 3, padding=1) \n        self.t_conv2 = nn.Conv2d(8, 16, 3, padding=1)\n#         self.t_conv3 = nn.Conv2d(32, 128, 3, padding=1)\n        self.conv_out = nn.Conv2d(16, 1, 3, padding=1)\n\n\n    def forward(self, x):\n        ## encode ##\n#         print(x.shape)\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n#         print(x.shape)\n        x = F.relu(self.conv2(x))\n#         x = self.pool(x)\n#         print(x.shape)\n#         x = F.relu(self.conv3(x))\n#         x = self.pool(x)\n#         print(x.shape)\n#         print(\"Start Decoder\")\n        \n        ## decode ##\n#         x = F.interpolate(x, scale_factor=2, mode='nearest')\n        x = F.relu(self.t_conv1(x))\n        \n#         print(x.shape)\n        x = F.interpolate(x, scale_factor=2, mode='nearest')\n        x = F.relu(self.t_conv2(x))\n        \n#         print(x.shape)\n#         x = F.upsample(x, scale_factor=2, mode='nearest')\n#         x = F.relu(self.t_conv3(x))\n        \n#         print(x.shape)\n#         x = F.upsample(x, scale_factor=2, mode='nearest')\n        x = F.sigmoid(self.conv_out(x))\n#         print(x.shape)\n                \n        return x\n\n# initialize the NN\nmodel = AutoEncoder()\nif use_cuda:\n    model.cuda()\nprint(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# specify loss function\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# number of epochs to train the model\nn_epochs = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### retraining Code total 250 times","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(1, n_epochs+1):\n    # monitor training loss\n    data_loss = 0.0\n    \n    ###################\n    # train the model #\n    ###################\n    for data in data_loader:\n        # _ stands in for labels, here\n        images, _ = data\n        if use_cuda:\n            images=images.cuda()\n        \n        ## Converting Image to grayscale\n        gray_image = torch.tensor([cv2.cvtColor(i.cpu().numpy().transpose((1,2,0)), cv2.COLOR_RGB2GRAY) for i in images])\n        \n        if use_cuda:\n            gray_image=gray_image.cuda()\n        optimizer.zero_grad()\n        \n        outputs = model(images)\n\n        loss = criterion(outputs, gray_image.reshape(-1,1,image_size,image_size))\n        loss.backward()\n        optimizer.step()\n        data_loss += loss.item()*images.size(0)\n            \n    data_loss = data_loss/len(data_loader)\n    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n        epoch, \n        data_loss\n        ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(), '../working/model_RGB_GREY_15122019.state')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel.load_state_dict(torch.load('../working/model_RGB_GREY_15122019.state'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# obtain one batch of test images\ndataiter = iter(test_loader)\nimages, labels = dataiter.next()\n\n# get sample outputs\noutput = model(images.cuda())\n# prep images for display\nimages1 = images.numpy()\n\n# output is resized into a batch of iages\noutput = output.view(batch_size, 1, 512, 512)\n# use detach when it's an output that requires_grad\noutput = output.cpu().detach().numpy()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the first ten input images and then reconstructed images\nfig, axes = plt.subplots(nrows=2, ncols=2, sharex=True, sharey=True,figsize=(35,35))\n\n# input images on top row, reconstructions on bottom\nfor imagess, row in zip([images1, output], axes):\n    for img, ax in zip(imagess, row):\n        print(np.squeeze(img).shape)\n        ax.imshow(np.squeeze(img.transpose((1,2,0))),cmap='gray')\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gray_image =([cv2.cvtColor(i.transpose((1,2,0)), cv2.COLOR_RGB2GRAY) for i in images]).reshape(-1,1,image_size,image_size)\n# gray_image.reshape(-1,1,image_size,image_size).shape\n\nplot = func_imshow(images[1], normalize=False,gray=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !ls ../input/model-r-g/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model1 = AutoEncoder()\n# model1.load_state_dict(torch.load(open('../input/model-r-g/model_RGB_GREY.state','rb'), map_location='cpu'))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}