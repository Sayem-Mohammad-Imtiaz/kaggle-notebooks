{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"bced201f-2bd2-c647-331b-5b7df500d85f"},"source":"This is a simple Neural Network implementation I learnt from a post in WILDML by Denny Britz "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cfc431c6-b622-b07a-25f1-6b9fd7e5d510"},"outputs":[],"source":"import numpy as np\nimport sklearn\nimport sklearn.linear_model\nfrom sklearn.datasets import make_moons\nimport matplotlib.pyplot as plt\nimport matplotlib"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"60895cc1-d496-8907-2e8b-dcd52fb5228d"},"outputs":[],"source":"# Generate a dataset and plot it\n#The points have 2 classes and they are not linearly seperable\nnp.random.seed(0)\nX, y = sklearn.datasets.make_moons(200, noise=0.20)\nplt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0cd12124-5229-6be3-4b18-d63b0ae3af69"},"outputs":[],"source":"\ndef plot_decision_boundary(pred_func):\n    # Set min and max values and give it some padding\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    h = 0.01\n    # Generate a grid of points with distance h between them\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    # Predict the function value for the whole gid\n    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n    print(Z.shape, xx.shape)\n    Z = Z.reshape(xx.shape)\n    # Plot the contour and training examples\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n\n# Train the logistic regression classifier\nclf = sklearn.linear_model.LogisticRegressionCV()\nclf.fit(X, y)\n#Plot the decision boundary\nplot_decision_boundary(lambda x: clf.predict(x))\nplt.title(\"Logistic Regression\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"28d9f0ca-fb4c-24ba-cbc7-78cdd979e525"},"outputs":[],"source":"#Need to train a neural network which has 2 input nodes [dimensions of I/P] and 2 output nodes [dimensions of O/P]\nnum_examples = len(X) # training set size\nnn_input_dim = 2 # input layer dimensionality\nnn_output_dim = 2 # output layer dimensionality\nepsilon = 0.01 # learning rate for gradient descent\nreg_lambda = 0.01 # regularization strength"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ed246fbb-d591-6931-3900-5ab996603198"},"outputs":[],"source":"# Loss Function\n# Cross-Entropy Loss is used here\ndef calculate_loss(model):\n    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n    # Forward propagation to calculate our predictions\n    z1 = X.dot(W1) + b1\n    a1 = np.tanh(z1)\n    z2 = a1.dot(W2) + b2\n    exp_scores = np.exp(z2)\n    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    # Calculating the loss\n    corect_logprobs = -np.log(probs[range(num_examples), y])\n    data_loss = np.sum(corect_logprobs)\n    # Add regulatization term to loss (optional)\n    data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n    return 1./num_examples * data_loss"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9f6b3c46-d7c5-5518-5475-d598cb6e8957"},"outputs":[],"source":"# Predict Function \ndef predict(model, x):\n    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n    # Forward propagation\n    z1 = x.dot(W1) + b1\n    a1 = np.tanh(z1)\n    z2 = a1.dot(W2) + b2\n    exp_scores = np.exp(z2)\n    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    return np.argmax(probs, axis=1) #Return class with best probability"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b30e69d-2440-1386-24cd-73bf44d55e8b"},"outputs":[],"source":"# This function learns parameters for the neural network and returns the model.\n# - nn_hdim: Number of nodes in the hidden layer\n# - num_passes: Number of passes through the training data for gradient descent\n# - print_loss: If True, print the loss every 1000 iterations\ndef build_model(nn_hdim, num_passes=20000, print_loss=False):\n    \n    # Initialize the parameters to random values. We need to learn these.\n    np.random.seed(0)\n    W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)\n    b1 = np.zeros((1, nn_hdim))\n    W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)\n    b2 = np.zeros((1, nn_output_dim))\n\n    # This is what we return at the end\n    model = {}\n    \n    # Gradient descent. For each batch...\n    for i in range(0, num_passes):\n\n        # Forward propagation\n        z1 = X.dot(W1) + b1\n        a1 = np.tanh(z1)\n        z2 = a1.dot(W2) + b2\n        exp_scores = np.exp(z2)\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n\n        # Backpropagation\n        delta3 = probs\n        delta3[range(num_examples), y] -= 1\n        dW2 = (a1.T).dot(delta3)\n        db2 = np.sum(delta3, axis=0, keepdims=True)\n        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n        dW1 = np.dot(X.T, delta2)\n        db1 = np.sum(delta2, axis=0)\n\n        # Add regularization terms (b1 and b2 don't have regularization terms)\n        dW2 += reg_lambda * W2\n        dW1 += reg_lambda * W1\n\n        # Gradient descent parameter update\n        W1 += -epsilon * dW1\n        b1 += -epsilon * db1\n        W2 += -epsilon * dW2\n        b2 += -epsilon * db2\n        \n        # Assign new parameters to the model\n        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n        \n        # Optionally print the loss.\n        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n        if print_loss and i % 1000 == 0:\n            print (\"Loss after iteration %i:%f\"  %(i, calculate_loss(model)))\n                 \n    \n    return model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7cb5a507-5eab-627e-669d-d70adcfb1714"},"outputs":[],"source":"# Build a model with a 3-dimensional hidden layer\nmodel = build_model(3, print_loss=True)\n\n# Plot the decision boundary\nplot_decision_boundary(lambda x: predict(model, x))\nplt.title(\"Decision Boundary for hidden layer size 3\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d162c4fe-815d-d5ee-b10a-8b5dfdb86a0a"},"outputs":[],"source":"plt.figure(figsize=(16, 32))\nhidden_layer_dimensions = [1, 2, 3, 4, 5, 20, 50]\nfor i, nn_hdim in enumerate(hidden_layer_dimensions):\n    plt.subplot(5, 2, i+1)\n    plt.title('Hidden Layer size %d' % nn_hdim)\n    model = build_model(nn_hdim)\n    plot_decision_boundary(lambda x: predict(model, x))\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"01e15680-0bee-f3af-89ed-804cbabc7ba0"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}