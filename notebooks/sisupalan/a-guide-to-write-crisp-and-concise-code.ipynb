{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Motivation\n\nBit late to the party but the COVID crisis has given me sufficient time to do the things I love to do :) That being said I have a few other ulterior motives as well releasing this kernel :D. What I have noticed in my career in Data Science, is that beginners or enthusiasts who want to step into this field are often mislead by bootcamp courses offering them to magically transform them into a competent data science in **X** days/months (insert relevant term advertised by specific bootcamp company). Well this kernel aims to showcase two things on the Heart Disease Prediction dataset:\n\n1. A starter kernel showing basic steps in a classification problem. Although, even kaggle datasets do not really portray the true picture of messy datasets in the industry, but I have tried to depict my typical process approaching a classification problem.\n\n2. Coding conventions! Even some experienced coders not adhering to this. Typically, I would have even broken this kernel down to 3 different kernels - Data Prep, EDA, Modelling. But for the sake of posterity, I have made a single kernel. Comments have been inserted at relevant places to explain the logic, docstrings for functions have been added, markdowns to segment the code have all been done to show newcomers the importance of writing structured and clean code. Markdowns are your friend, especially if you use kernels/notebooks!\n\nHopefully, this kernel helps someone! \n\n**PS : You can play around with the code and achieve a higher accuracy than the one in this kernel, but that was not the intent of this kernel**"},{"metadata":{},"cell_type":"markdown","source":"### 1. Import required libraries\nKeep all your imports at the top! This clearly shows the reader which libraries you have used."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Selective library imports\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Selectively import functions\nfrom math import sqrt\nfrom IPython.display import display\nfrom collections import OrderedDict\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, classification_report, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Set configurations\n\nYou would want to specify the settings for your kernel/notebook at the top. Also specify the constants (if any) like paths to files. Here, since its only one file we are working with, this is not necessary."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Disable warnings. This is not a good practice generally, but for the sake of aesthetics we are disabling this :D\nwarnings.filterwarnings(\"ignore\")\n\n# Suppress scientific notation\npd.options.display.float_format = '{:20,.2f}'.format\n\n# Set plot sizes\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n\n# Set plotting style\nplt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Write User-Defined Functions\n\nMy rule is, if I have to repeat a piece of code more than twice, I functionize them. Always include a docstring with your function. Here, all the data processing/modelling related functions have a lot of parameters which enable the reader to experiment with a variety of settings. Need a different train/test split? No problem, change the random_state in the function! Need a different scorer? Not to worry! Use the parameter scorer in the functions..."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_dist(df, var, target, var_type='num'):\n    \n    '''Function helper to facet on target variable'''\n    \n    if var_type == 'num':\n        sns.distplot(df.query('target == 1')[var].tolist() , color=\"red\", label=\"{} for target == 1\".format(var))\n        sns.distplot(df.query('target == 0')[var].tolist() , color=\"skyblue\", label=\"{} for target == 0\".format(var))\n        plt.legend()\n        \n    else:\n        fig, ax = plt.subplots(1,2)\n        sns.countplot(data=df.query('target == 1') , color=\"salmon\", x=var, label=\"{} for target == 1\".format(var), ax=ax[0])\n        sns.countplot(data=df.query('target == 0') , color=\"skyblue\", x=var, label=\"{} for target == 0\".format(var), ax=ax[1])\n        fig.legend()\n        fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_data(df, test_size=0.3, random_state=1, scale=True, scaler=MinMaxScaler(), feature_selection=True, k=10):\n    \n    '''Function helper to generate train and test datasets and apply transformations if any'''\n    \n    \n    # Dummify columns\n    dummy_cols = ['cp', 'restecg', 'slope', 'ca', 'thal']\n    df = pd.get_dummies(df, columns=dummy_cols)\n    \n    \n    # All the columns\n    cols = df.columns.tolist()\n    \n    # X cols\n    cols = [col for col in cols if 'target' not in col] \n    \n    # Subset x and y\n    X = df[cols]\n    y = df['target']\n    \n    # Feature selection\n    if feature_selection == True:\n        \n        k_best = SelectKBest(score_func=chi2, k=k)\n        selector = k_best.fit(X, y)\n        selection_results = pd.DataFrame({'feature' : cols, 'selected' : selector.get_support()})\n        selected_features = list(selection_results.query('selected == True')['feature'])\n        X = X[selected_features]\n\n    # Train test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    \n    \n    # Make a copy to apply on. Else Set-copy warning will be displayed\n    X_train_copy = X_train.copy()\n    X_test_copy = X_test.copy()\n\n    # Scale columns if needed\n    if scale == True:\n        scale_cols = ['age', \n                      'trestbps', \n                      'chol', \n                      'thalach', \n                      'oldpeak']\n        \n        # If any features are dropped from feature selection we need to account for that\n        scale_cols = list(set(selected_features) & set(scale_cols))\n        \n        # Define scaler to use\n        scaler = scaler\n\n        # Apply scaling\n        X_train_copy.loc[:, scale_cols] = scaler.fit_transform(X_train[scale_cols])\n        X_test_copy.loc[:, scale_cols] = scaler.transform(X_test[scale_cols])\n      \n    # Return train and tests\n    return X_train_copy, X_test_copy, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def select_model(X_train, y_train, cv=3, nruns=3, scorer='recall'):\n    \n    '''Function helper to automate selection of best baseline model without hyperparameter tuning'''\n\n    record_scorer = []\n    iter_scorer = []\n    model_name = []\n    model_accuracy = []\n\n    # Specify estimators\n    estimators = [('logistic_regression' , LogisticRegression()), \n                  ('random_forest' , RandomForestClassifier(n_estimators=100)),\n                  ('lightgbm' , LGBMClassifier(n_estimators=100)), \n                  ('xgboost' , XGBClassifier(n_estimators=100))]\n\n\n    scorer = scorer\n    \n    # Iterate through the number of runs. Default is 3.\n    for run in range(nruns):\n        print('Running iteration %s with %s as scoring metric' % ((run + 1), scorer))\n\n        for name, estimator in estimators:\n\n            print('Fitting %s model' % name)\n\n            # Run cross validation\n            cv_results = cross_val_score(estimator, X_train, y_train, cv=cv, scoring=scorer)\n\n            # Append all results in list form which will be made into a dataframe at the end.\n            iter_scorer.append((run + 1))\n            record_scorer.append(scorer)\n            model_name.append(name)\n            model_accuracy.append(cv_results.mean())\n\n        print()\n\n    # Use ordered dictionary to set the dataframe in the exact order of columns declared.\n    results = pd.DataFrame(OrderedDict({'Iteration' : iter_scorer, \n                                        'Scoring Metric' : record_scorer, \n                                        'Model' : model_name, \n                                        'Model Accuracy' : model_accuracy}))\n    \n    # Pivot to view results in a more aesthetic form\n    results_pivot = results.pivot_table(index=['Iteration', 'Scoring Metric'], columns=['Model'])\n    \n    # Display the results\n    print('\\nFinal results : ')\n    display(results_pivot)\n\n    # Get the mean performance\n    performance = results_pivot.apply(np.mean, axis=0)\n    performance = performance.reset_index()\n    performance.columns = ['metric', 'model', 'performance']\n    \n    # Get the mean performance\n    performance = results_pivot.apply(np.mean, axis=0)\n    performance = performance.reset_index()\n    performance.columns = ['metric', 'model', 'performance']\n    best_model = performance.loc[performance['performance'].idxmax()]['model']\n\n    # Return the pivot \n    return results_pivot, best_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tune_model(X_train, X_test, y_train, y_test, best_model, scorer='recall'):\n    \n    # Define parameters for each model\n    grid = {'logistic_regression' : {'model' : LogisticRegression(class_weight='balanced', random_state=42), \n                                    'params' : {'C' : [0.01, 0.1, 1, 10, 100]}},\n\n            'random_forest' : {'model' : RandomForestClassifier(class_weight='balanced', random_state=42), \n                            'params' : {'n_estimators' : [100, 200, 300], \n                                        'max_depth' : [3, 5, 7], \n                                        'max_features' : ['log2', 5, 'sqrt']}},\n\n            'lightgbm' : {'model' : LGBMClassifier(class_weight='balanced', random_state=42), \n                        'params' : {'n_estimators' : [100, 200, 300], \n                                    'max_depth' : [3, 5, 7], \n                                    'boosting_type' : ['gbdt', 'dart', 'goss']}},\n\n            'xgboost' : {'model' : XGBClassifier(nthread=-1), \n                        'params' : {'n_estimators' : [100, 200, 300], \n                                    'max_depth' : [3, 5, 7], \n                                    'scale_pos_weight' : [5, 10, 20]}}                        \n                                \n        }\n\n    # Select the best model\n    model = grid[best_model]['model']\n\n    # Define the grid\n    params = grid[best_model]['params']\n\n    # 3 Fold Cross Validation\n    grid = GridSearchCV(model, cv=3, param_grid=params, scoring=scorer, n_jobs=-1, verbose=2)\n    grid.fit(X_train, y_train)\n    \n\n    return(grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_performance(X_train, X_test, y_train, y_test, grid):\n      \n    # Select the model with the best paramters\n    model = grid.best_estimator_\n\n    # Fit the model on the data\n    model.fit(X_train, y_train)\n    \n\n    # Get the training predictions\n    train_predictions = model.predict(X_train)\n    test_predictions = model.predict(X_test) \n    \n    # Get the train and test probabilities\n    train_probabilities = model.predict_proba(X_train)[:, 1]\n    test_probabilities = model.predict_proba(X_test)[:, 1]\n\n    # Get the accuracy for train and test\n    print('Accuracy score for training is : %s' % accuracy_score(y_train, train_predictions))\n    print('Accuracy score for testing is : %s' % accuracy_score(y_test, test_predictions))\n    \n    # Get the classification report for train and test\n    print('\\nClassification report for training is : \\n%s' % classification_report(y_train, train_predictions))\n    print('Classification report for testing is : \\n%s' % classification_report(y_test, test_predictions))\n    \n    # Get the confusion matrix for train and test\n    print('\\nConfusion matrix for training is : \\n%s' % confusion_matrix(y_train, train_predictions))\n    print('Confusion matrix for testing is : \\n%s' % confusion_matrix(y_test, test_predictions))\n    \n    # Get the ROC AUC for train and test\n    print('\\nROC AUC score for training is : %s' % roc_auc_score(y_train, train_probabilities))\n    print('ROC AUC score for testing is : %s' % roc_auc_score(y_test, test_probabilities))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"### 3. Import the required dataset\n\nIdeally I keep paths at the config section. This is an exception as I am dealing with a single file only here. Also, I always use relative paths as it becomes easier when distributing code. "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/heart.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. About the data : \n\nDepending on the type of data, you may want to see additional things other than shape, describe, dtypes, value_counts and missing values."},{"metadata":{},"cell_type":"markdown","source":"#### 4.1 Shape"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.2 Statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary statistic\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.3 Types of Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data types of columns\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.4 Target Proportion"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of target\ndf['target'].value_counts([0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.5 Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.6 Display head"},{"metadata":{"trusted":true},"cell_type":"code","source":"# first few rows of data\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Exploratory Data Analysis\n\nFor numerical columns, density/histogram plots are used. For categorical columns, bar plots are used. You may want to see other kind of plots such as corelation plots, boxplots etc too."},{"metadata":{},"cell_type":"markdown","source":"#### 5.1 Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist(df, 'age', 'target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.2 Sex"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist(df, 'sex', 'target', 'cat')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.3 CP"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist(df, 'cp', 'target', 'cat')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.4 Trestbps"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist(df, 'trestbps', 'target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.5 Chol"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist(df, 'chol', 'target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.6 Fbs"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist(df, 'fbs', 'target', 'cat')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.7 Restecg"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist(df, 'restecg', 'target', 'cat')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.8 Thalach"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist(df, 'thalach', 'target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.9 Exang"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist(df, 'exang', 'target', 'cat')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.10 Oldpeak"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist(df, 'oldpeak', 'target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.11 Slope"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist(df, 'slope', 'target', 'cat')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.12 Ca"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist(df, 'ca', 'target', 'cat')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.13 Thal"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist(df, 'thal', 'target', 'cat')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Model Building\n\nThis block specifies the training process which is quite simple. The pipeline is:\n**Split train/test -> Find best baseline model -> Tune parameters of best baseline model -> Evaluate model performance.**\n\nAlmost all data pipelines are of similar structure. I haven't tried other techniques like stacking here. You may also want to look into that!"},{"metadata":{},"cell_type":"markdown","source":"#### 6.1 Split into train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the train and test datasets\nX_train, X_test, y_train, y_test = process_data(df, test_size=0.3, random_state=100, scale=True, scaler=MinMaxScaler(), feature_selection=True, k=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# View the train dataset\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# View the test dataset\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6.2 Best baseline model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display results of each model\nresults_pivot, best_model = select_model(X_train, y_train, cv=5, nruns=10, scorer='balanced_accuracy')\nresults_pivot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6.3 Hyperparameter Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print and tune the best model\nprint('Tuning model for {}'.format(best_model))\ngrid = tune_model(X_train, X_test, y_train, y_test, best_model, scorer='balanced_accuracy')\ngrid","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. Model Performance\n\nMy go-to metrics for evaluating classification models are:\n\n**Accuracy(unreliable in cases of skewed class proportions of target), classification report showing precision/recall/f1, confusion matrix and roc_auc_score.**\n\nYou may want to look at other metrics such as lift, kappa etc"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display model performance\nmodel_performance(X_train, X_test, y_train, y_test, grid)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}