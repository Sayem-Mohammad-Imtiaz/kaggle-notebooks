{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Heart Stroke Prediction using XGBoost and Random Forest","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas  as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Reading data from the files","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\ndf.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.heatmap(df.corr(),annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dropping nulls in bmi from the dataset\ndf=df.dropna().reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sanity check\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dropping id column because it is unique for each row\ndf=df.drop(columns=['id'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\ndf.boxplot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Boxplot of Numerical features in the data shows the presence of outliers in avg_glucose_level and bmi","metadata":{}},{"cell_type":"code","source":"for i in df.select_dtypes(include=np.number).columns:\n    sns.boxplot(df[i])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Boxplot of Numerical features in the data shows the presence of outliers in avg_glucose_level and bmi","metadata":{}},{"cell_type":"code","source":"# type conversion of hypertension, heart disease and stroke\ndf['hypertension']=df['hypertension'].astype(object)\ndf['heart_disease']=df['heart_disease'].astype(object)\ndf['stroke']=df['stroke'].astype(object)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting data into numerical and categorical\ndf_int=df.select_dtypes(include=np.number)\ndf_cat=df.select_dtypes(exclude=np.number)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sanity check \ndf_int.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sanity check\ndf_cat.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Countplot of our Categorical Variables\nfor i in df_cat:\n    sns.countplot(df[i])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From this we infer that our predictor is very highly imbalanced. To treat imbalance we could go with Oversampling or Undersampling techniques(like SMOTE), but further in this problem we have used SMOTE(Oversampling)","metadata":{}},{"cell_type":"markdown","source":"# Bivariate Analysis","metadata":{}},{"cell_type":"code","source":"pd.crosstab(df['ever_married'],df['stroke']).plot(kind='bar',stacked=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can easily visualize that people who have ever been maried have a larger number of stroke than compared to the non married","metadata":{}},{"cell_type":"code","source":"pd.crosstab(df['work_type'],df['stroke']).plot(kind='bar',stacked=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can easily visualize that people who have been working in private sector have larger possibility of stroke than any other profession","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.kdeplot(df[df['stroke']==0]['age'],shade=True,label='no_stroke')\nsns.kdeplot(df[df['stroke']==1]['age'],shade=True,label='stroke')\nplt.xlabel('Age')\nplt.title('Stroke Density vs Age')\nplt.legend()\n\nplt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can see that people with age between 60-90 have most likely to have a stroke","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.kdeplot(df[df['stroke']==0]['bmi'],shade=True,label='no_stroke')\nsns.kdeplot(df[df['stroke']==1]['bmi'],shade=True,label='stroke')\nplt.legend()\nplt.title('Stroke Density vs BMI ')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can easily  see that bmi is not affecting the target that much","metadata":{}},{"cell_type":"code","source":"# Separating Dependent and Predictor variables\nX=pd.get_dummies(df,columns=df_cat.columns,drop_first=True).iloc[:,:-2]\ny=pd.to_numeric(df['stroke'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting into train and test sets into stratified sets\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,stratify=y,random_state=8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sanity check\ny_train.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sanity check\ny_test.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying SMOTE for treating imbalance in our data\nfrom imblearn.over_sampling import SMOTE\nsmt = SMOTE(random_state=8)\nX_train_sm, y_train_sm = smt.fit_resample(X_train, y_train)\nprint(y_train_sm.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic regression without SMOTE","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_reg=LogisticRegression(max_iter=1000)\nlog_reg.fit(X_train,y_train)\nprint('Train:',log_reg.score(X_train,y_train))\nprint('Test:',log_reg.score(X_test,y_test))\n\ny_pred_lr=log_reg.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(y_pred_lr).value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report,roc_auc_score,roc_curve\nprint(classification_report(y_test,y_pred_lr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression with SMOTE","metadata":{}},{"cell_type":"code","source":"log_regsm=LogisticRegression(max_iter=1000)\nlog_regsm.fit(X_train_sm,y_train_sm)\nprint('Train:',log_regsm.score(X_train_sm,y_train_sm))\nprint('Test:',log_regsm.score(X_test,y_test))\n\ny_pred_lrsm=log_regsm.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred_lrsm))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression with SMOTE gives better Precision,Recall and F1-SCore as compared to Logistic Regression without SMOTE","metadata":{}},{"cell_type":"markdown","source":"### Decision tree without SMOTE","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtc=DecisionTreeClassifier()\ndtc.fit(X_train,y_train)\nprint('Train:',dtc.score(X_train,y_train))\nprint('Test:',dtc.score(X_test,y_test))\n\ny_pred_dt=dtc.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred_dt))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision Tree with SMOTE","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtcsm=DecisionTreeClassifier()\ndtcsm.fit(X_train_sm,y_train_sm)\nprint('Train:',dtcsm .score(X_train_sm,y_train_sm))\nprint('Test:',dtcsm.score(X_test,y_test))\n\ny_pred_dtsm=dtcsm.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred_dtsm))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree with SMOTE gives better Precision,Recall and F1-SCore as compared to Decision Tree without SMOTE","metadata":{}},{"cell_type":"markdown","source":"## KNN without SMOTE","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier()\nknn.fit(X_train,y_train)\nprint('Train:',knn.score(X_train,y_train))\nprint('Test:',knn.score(X_test,y_test))\n\ny_pred_knn=knn.predict(X_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred_knn))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### KNN with SMOTE","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknnsm=KNeighborsClassifier()\nknnsm.fit(X_train_sm,y_train_sm)\nprint('Train:',knnsm.score(X_train_sm,y_train_sm))\nprint('Test:',knnsm.score(X_test,y_test))\n\ny_pred_knnsm=knnsm.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred_knnsm))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### KNN with SMOTE gives better Recall and F1-SCore as compared to KNN without SMOTE","metadata":{}},{"cell_type":"markdown","source":"## Naive Bayes without SMOTE","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnaive=GaussianNB()\nnaive.fit(X_train,y_train)\nprint('Train:',naive.score(X_train,y_train))\nprint('Test:',naive.score(X_test,y_test))\n\ny_pred_gnb=naive.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred_gnb))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Naive Bayes with SMOTE","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnaivesm=GaussianNB()\nnaivesm.fit(X_train_sm,y_train_sm)\nprint('Train:',naivesm.score(X_train_sm,y_train_sm))\nprint('Test:',naivesm.score(X_test,y_test))\n\ny_pred_naivesm=naivesm.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred_naivesm))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Naive Bayes with SMOTE gives lower F1-SCore as compared to Naive Bayes without SMOTE","metadata":{}},{"cell_type":"markdown","source":"# Applying Cross validation to check Sampling Bias","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscore=cross_val_score(LogisticRegression(max_iter=1000),X_train,y_train,cv=10,scoring='accuracy')\nprint(score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscore=cross_val_score(dtc,X_train,y_train,cv=10,scoring='accuracy')\nprint(score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score,roc_curve\n\ndef roc_curve1(model):\n    pred_proba=model.predict_proba(X_test)\n    fpr,tpr,th = roc_curve(y_test,pred_proba[:,1])\n    plt.plot(fpr,tpr)\n    plt.xlim([0.0,1.0])\n    plt.ylim([0.0,1.0])\n    plt.plot([0,1],[0,1],'r--')\n    plt.title('ROC curve for Classifier')\n    plt.xlabel('FPR')\n    plt.ylabel('TPR')\n    plt.text(x = 0.02, y = 0.9, s = ('AUC Score:',round(roc_auc_score(y_test, pred_proba[:,1]),4)))\n    plt.grid(True)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc_curve1(log_reg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### ROC curve along with scores for comparing 2 algorithms\n## 1st classifier\n\ny_pred_proba= log_reg.predict_proba(X_test)[:,1]\n\ny_pred_probasm= log_regsm.predict_proba(X_test)[:,1]\n\nplt.figure(figsize = (8, 8))\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.plot([0, 1], [0, 1],'r--')\nplt.title('ROC-curves for Logistic Regression with and without SMOTE', fontsize = 15)\nplt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\nplt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\nplt.text(x = 0.02, y = 0.9, s = ('AUC Score for log_reg model:',round(roc_auc_score(y_test, y_pred_proba),4)))\n## 2nd classifier\nfpr1, tpr1, thresholds1 = roc_curve(y_test, y_pred_probasm)\nplt.plot(fpr1, tpr1)\nplt.text(x = 0.02, y = 0.8, s = ('AUC Score for log_reg_sm model:',round(roc_auc_score(y_test, y_pred_probasm),4)))\nplt.grid(True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validation Curve for Logistic Regression Model without SMOTE ","metadata":{}},{"cell_type":"code","source":"from warnings import filterwarnings\nfilterwarnings('ignore')\n\nfrom sklearn.model_selection import validation_curve\n\nC_param_range = [0.001,0.01,0.1,1,10,100,1000]\n\nplt.figure(figsize=(15, 10))\n\n# Logistic Regression validation curve\ntrain_scores, test_scores = validation_curve(estimator=log_reg,X=X_train,y=y_train ,param_name='C',param_range=C_param_range)\n\ntrain_mean = np.mean(train_scores,axis=1)\ntrain_std = np.std(train_scores,axis=1)\ntest_mean = np.mean(test_scores,axis=1)\ntest_std = np.std(test_scores,axis=1)\n\nplt.subplot(2,2,1)\nplt.semilogx(C_param_range\n            ,train_mean\n            ,color='blue'\n            ,marker='o'\n            ,markersize=5\n            ,label='training accuracy')\n    \nplt.semilogx(C_param_range\n            ,test_mean\n            ,color='green'\n            ,marker='x'\n            ,markersize=5\n            ,label='test accuracy') \n    \nplt.xlabel('C_parameter')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.5,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validation Curve for Decision Tree Model without SMOTE ","metadata":{}},{"cell_type":"code","source":"dtc_param_range = np.arange(1,13)\n\nplt.figure(figsize=(15, 10))\n\n# Decision Tree validation curve\ntrain_scores, test_scores = validation_curve(estimator=dtc,X=X_train,y=y_train ,param_name='max_depth',param_range=dtc_param_range)\n\ntrain_mean = np.mean(train_scores,axis=1)\ntrain_std = np.std(train_scores,axis=1)\ntest_mean = np.mean(test_scores,axis=1)\ntest_std = np.std(test_scores,axis=1)\n\nlw=0.5\nplt.subplot(2,2,1)\nplt.semilogx(dtc_param_range\n            ,train_mean\n            ,color='blue'\n            ,marker='o'\n            ,markersize=5\n            ,label='training accuracy')\nplt.fill_between(dtc_param_range, train_mean - train_std,\n                 train_mean + train_std, alpha=0.2,\n                 color=\"darkorange\", lw=lw)    \nplt.semilogx(dtc_param_range\n            ,test_mean\n            ,color='green'\n            ,marker='x'\n            ,markersize=5\n            ,label='test accuracy') \nplt.fill_between(dtc_param_range, test_mean - test_std,\n                 test_mean + test_std, alpha=0.2,\n                 color=\"navy\", lw=lw)\n\nplt.xlabel('max_depth_parameter')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.5,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validation Curve for Decision Tree Model with SMOTE ","metadata":{}},{"cell_type":"code","source":"dtc_param_range = np.arange(1,25)\n\nplt.figure(figsize=(15, 10))\n\n# Decision Tree(SMOTE) validation curve\ntrain_scores, test_scores = validation_curve(estimator=dtcsm,X=X_train_sm,y=y_train_sm ,param_name='max_depth',param_range=dtc_param_range)\n\ntrain_mean = np.mean(train_scores,axis=1)\ntrain_std = np.std(train_scores,axis=1)\ntest_mean = np.mean(test_scores,axis=1)\ntest_std = np.std(test_scores,axis=1)\n\nlw=0.5\nplt.subplot(2,2,1)\nplt.semilogx(dtc_param_range\n            ,train_mean\n            ,color='blue'\n            ,marker='o'\n            ,markersize=5\n            ,label='training accuracy')\nplt.fill_between(dtc_param_range, train_mean - train_std,\n                 train_mean + train_std, alpha=0.2,\n                 color=\"darkorange\", lw=lw)    \nplt.semilogx(dtc_param_range\n            ,test_mean\n            ,color='green'\n            ,marker='x'\n            ,markersize=5\n            ,label='test accuracy') \nplt.fill_between(dtc_param_range, test_mean - test_std,\n                 test_mean + test_std, alpha=0.2,\n                 color=\"navy\", lw=lw)\n\nplt.xlabel('max_depth_parameter')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.5,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tuning Decision Tree without SMOTE by taking Hyperparameter range as per Validation Curve","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparams={'criterion':['entropy', 'gini'],'max_depth': range(2, 10),'min_samples_split' : range(1,5)}\ndt=DecisionTreeClassifier()\ngrid=GridSearchCV(dt,params,cv=5)\ngrid.fit(X_train,y_train)\n\nprint('The best value of hyperparameters \"criterion\", \"max_depth\", and \"min_samples_split\"')\nprint(grid.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fitting Decision Tree algo without SMOTE using the hyperparameters deduced above","metadata":{}},{"cell_type":"code","source":"dt_tuned=DecisionTreeClassifier(criterion='entropy',max_depth=2,min_samples_split=2)\ndt_tuned.fit(X_train,y_train)\n\ny_pred_dttuned= dt_tuned.predict(X_test)\n\ndt_tuned.score(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred_dttuned))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt_tuned.score(X_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tuning Decision Tree with SMOTE by taking Hyperparameter range as per Validation Curve","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nParameter_Trials={'max_depth': [11,12,13,14,15,16]}\n\nGrid_Search = GridSearchCV(dtcsm, Parameter_Trials, cv=5, n_jobs=1)\nGridSearchResults=Grid_Search.fit(X_train_sm,y_train_sm)\n\nprint('The best value of hyperparameters \"max_depth\" :')\nprint(Grid_Search.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtsm_tuned=DecisionTreeClassifier(criterion='entropy',max_depth=16)\ndtsm_tuned.fit(X_train_sm,y_train_sm)\n\ny_pred_dtsmtuned= dtsm_tuned.predict(X_test)\n\ndtsm_tuned.score(X_train_sm,y_train_sm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred_dtsmtuned))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ROC-Curve for Decision Tree\nroc_curve1(dtc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ROC-Curve for tuned Decision Tree\nroc_curve1(dt_tuned)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comparing AUC Score along with ROC curve for Decision Tree, tuned Decision Tree, Decision Tree with SMOTE and tuned Decision Tree with SMOTE ","metadata":{}},{"cell_type":"code","source":"y_pred_probad= dtc.predict_proba(X_test)[:,1]\n\ny_pred_probadsm= dtcsm.predict_proba(X_test)[:,1]\n\ny_pred_probad_tuned= dt_tuned.predict_proba(X_test)[:,1]\n\ny_pred_probadsm_tuned= dtsm_tuned.predict_proba(X_test)[:,1]\n\nplt.figure(figsize = (12, 8))\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_probad)\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.plot([0, 1], [0, 1],'r--')\nplt.title('ROC curve for different Models', fontsize = 15)\nplt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\nplt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\nplt.text(x = 0.02, y = 0.9, s = ('AUC Score for DT model:',round(roc_auc_score(y_test, y_pred_probad),4)))\n## 2nd classifier\nfpr1, tpr1, thresholds1 = roc_curve(y_test, y_pred_probadsm)\nplt.plot(fpr1, tpr1)\nplt.text(x = 0.02, y = 0.8, s = ('AUC Score for DT_SMOTE model:',round(roc_auc_score(y_test, y_pred_probadsm),4)))\nplt.grid(True)\n\n## 3rd classifier\nfpr2, tpr2, thresholds2 = roc_curve(y_test, y_pred_probad_tuned)\nplt.plot(fpr2, tpr2)\nplt.text(x = 0.02, y = 0.7, s = ('AUC Score for tuned_DT model:',round(roc_auc_score(y_test, y_pred_probad_tuned),4)))\nplt.grid(True)\n\n## 4th classifier\nfpr3, tpr3, thresholds3 = roc_curve(y_test, y_pred_probadsm_tuned)\nplt.plot(fpr3, tpr3)\nplt.text(x = 0.02, y = 0.6, s = ('AUC Score for tuned_DT_SMOTE model:',round(roc_auc_score(y_test, y_pred_probadsm_tuned),4)))\nplt.grid(True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Applying Ensemble Techniques ","metadata":{}},{"cell_type":"markdown","source":"# Random Forest without SMOTE","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf=RandomForestClassifier()\n\nrf.fit(X_train,y_train)\n\nrf.score(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_rf=rf.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred_rf))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf.score(X_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validation Curve for Random Forest without SMOTE\nrf_param_range = np.arange(1,1000,100)\n\nplt.figure(figsize=(12,8))\n\ntrain_scores, test_scores = validation_curve(estimator=rf,X=X_train,y=y_train ,param_name='n_estimators',param_range=rf_param_range)\n\ntrain_mean = np.mean(train_scores,axis=1)\ntrain_std = np.std(train_scores,axis=1)\ntest_mean = np.mean(test_scores,axis=1)\ntest_std = np.std(test_scores,axis=1)\n\nlw=0.5\nplt.subplot(2,2,1)\nplt.plot(rf_param_range\n            ,train_mean\n            ,color='blue'\n            ,marker='o'\n            ,markersize=5\n            ,label='training accuracy')\nplt.fill_between(rf_param_range, train_mean - train_std,\n                 train_mean + train_std, alpha=0.2,\n                 color=\"darkorange\", lw=lw)\n\nplt.plot(rf_param_range\n            ,test_mean\n            ,color='green'\n            ,marker='x'\n            ,markersize=5\n            ,label='test accuracy') \nplt.fill_between(rf_param_range, test_mean - test_std,\n                 test_mean + test_std, alpha=0.2,\n                 color=\"navy\", lw=lw)\n\n    \nplt.xlabel('n_estimators_parameter')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.9,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validation Curve for Random Forest without SMOTE\nrf_param_range = np.arange(1,13)\n\nplt.figure(figsize=(15, 10))\n\ntrain_scores, test_scores = validation_curve(estimator=rf,X=X_train,y=y_train ,param_name='max_depth',param_range=rf_param_range)\n\ntrain_mean = np.mean(train_scores,axis=1)\ntrain_std = np.std(train_scores,axis=1)\ntest_mean = np.mean(test_scores,axis=1)\ntest_std = np.std(test_scores,axis=1)\n\nlw=0.5\nplt.subplot(2,2,1)\nplt.plot(rf_param_range\n            ,train_mean\n            ,color='blue'\n            ,marker='o'\n            ,markersize=5\n            ,label='training accuracy')\nplt.fill_between(rf_param_range, train_mean - train_std,\n                 train_mean + train_std, alpha=0.2,\n                 color=\"darkorange\", lw=lw)\n\nplt.plot(rf_param_range\n            ,test_mean\n            ,color='green'\n            ,marker='x'\n            ,markersize=5\n            ,label='test accuracy') \nplt.fill_between(rf_param_range, test_mean - test_std,\n                 test_mean + test_std, alpha=0.2,\n                 color=\"navy\", lw=lw)\n\n    \nplt.xlabel('max_depth_parameter')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.9,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tuning Random Forest without SMOTE by taking Hyperparameter range as per Validation Curve","metadata":{}},{"cell_type":"code","source":"params={'criterion':['entropy', 'gini'],'max_depth': range(2,8),'n_estimators' : range(1,200,50)}\n\ngrid_rf=GridSearchCV(rf,params,cv=5)\ngrid_rf.fit(X_train,y_train)\n\nprint('The best value of hyperparameters \"criterion\", \"max_depth\", and \"min_samples_split\"')\nprint(grid_rf.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_tuned=RandomForestClassifier(n_estimators=1,max_depth=2,criterion='entropy')\n\nrf_tuned.fit(X_train,y_train)\nrf_tuned.score(X_train,y_train)\n\ny_pred_tunedrf=rf_tuned.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_tuned.score(X_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(classification_report(y_test,y_pred_tunedrf))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Applying Random Forest with SMOTE ","metadata":{}},{"cell_type":"code","source":"rf_sm=RandomForestClassifier()\n\nrf_sm.fit(X_train_sm,y_train_sm)\nrf_sm.score(X_train_sm,y_train_sm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_rfsm=rf_sm.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred_rfsm))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validation Curve for Random Forest model with SMOTE\nrfsm_param_range = np.arange(1,40)\n\nplt.figure(figsize=(15, 10))\n\ntrain_scores, test_scores = validation_curve(estimator=rf_sm,X=X_train_sm,y=y_train_sm ,param_name='max_depth',param_range=rfsm_param_range)\n\ntrain_mean = np.mean(train_scores,axis=1)\ntrain_std = np.std(train_scores,axis=1)\ntest_mean = np.mean(test_scores,axis=1)\ntest_std = np.std(test_scores,axis=1)\n\nlw=0.5\nplt.subplot(2,2,1)\nplt.plot(rfsm_param_range\n            ,train_mean\n            ,color='blue'\n            ,marker='o'\n            ,markersize=5\n            ,label='training accuracy')\nplt.fill_between(rfsm_param_range, train_mean - train_std,\n                 train_mean + train_std, alpha=0.2,\n                 color=\"darkorange\", lw=lw)\n\nplt.plot(rfsm_param_range\n            ,test_mean\n            ,color='green'\n            ,marker='x'\n            ,markersize=5\n            ,label='test accuracy') \nplt.fill_between(rfsm_param_range, test_mean - test_std,\n                 test_mean + test_std, alpha=0.2,\n                 color=\"navy\", lw=lw)\n\n    \nplt.xlabel('max_depth_parameters')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.9,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validation Curve for Random Forest model with SMOTE\nrfsm_param_range = np.arange(1,1000,100)\n\nplt.figure(figsize=(15, 10))\n\ntrain_scores, test_scores = validation_curve(estimator=rf_sm,X=X_train_sm,y=y_train_sm ,param_name='n_estimators',param_range=rfsm_param_range)\n\ntrain_mean = np.mean(train_scores,axis=1)\ntrain_std = np.std(train_scores,axis=1)\ntest_mean = np.mean(test_scores,axis=1)\ntest_std = np.std(test_scores,axis=1)\n\nlw=0.5\nplt.subplot(2,2,1)\nplt.plot(rfsm_param_range\n            ,train_mean\n            ,color='blue'\n            ,marker='o'\n            ,markersize=5\n            ,label='training accuracy')\nplt.fill_between(rfsm_param_range, train_mean - train_std,\n                 train_mean + train_std, alpha=0.2,\n                 color=\"darkorange\", lw=lw)\n\nplt.plot(rfsm_param_range\n            ,test_mean\n            ,color='green'\n            ,marker='x'\n            ,markersize=5\n            ,label='test accuracy') \nplt.fill_between(rfsm_param_range, test_mean - test_std,\n                 test_mean + test_std, alpha=0.2,\n                 color=\"navy\", lw=lw)\n\n    \nplt.xlabel('n_estimators_parameters')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.9,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nParameter_Trials={'criterion':['gini','entropy'],\n                  'n_estimators': range(100,500,100),'max_depth':[15,16,17,18,19,20,21,22,23]}\n \nGrid_Search = GridSearchCV(rf_sm, Parameter_Trials, cv=5, n_jobs=1)\nGridSearchResults=Grid_Search.fit(X,y)\nGrid_Search.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_smtuned=RandomForestClassifier(criterion='gini',n_estimators=300,max_depth=17)\n\nrf_smtuned.fit(X_train_sm,y_train_sm)\nrf_smtuned.score(X_train_sm,y_train_sm)\n\ny_pred_rfsmtuned=rf_smtuned.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred_rfsmtuned))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comparing AUC Score along with ROC curve for Random Forest, tuned Random Forest, Random Forest with SMOTE and tuned Random Forest with SMOTE ","metadata":{}},{"cell_type":"code","source":"y_pred_probarf= rf.predict_proba(X_test)[:,1]\n\ny_pred_probarfsm= rf_sm.predict_proba(X_test)[:,1]\n\ny_pred_probarf_tuned= rf_tuned.predict_proba(X_test)[:,1]\n\ny_pred_probadrfsm_tuned= rf_smtuned.predict_proba(X_test)[:,1]\n\nplt.figure(figsize = (12, 8))\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_probarf)\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.plot([0, 1], [0, 1],'r--')\nplt.title('ROC curve for Different Models', fontsize = 15)\nplt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\nplt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\nplt.text(x = 0.02, y = 0.9, s = ('AUC Score for rf model:',round(roc_auc_score(y_test, y_pred_probarf),4)))\n## 2nd classifier\nfpr1, tpr1, thresholds1 = roc_curve(y_test, y_pred_probarfsm)\nplt.plot(fpr1, tpr1)\nplt.text(x = 0.02, y = 0.8, s = ('AUC Score for rf_sm model:',round(roc_auc_score(y_test, y_pred_probarfsm),4)))\nplt.grid(True)\n\n## 3rd classifier\nfpr2, tpr2, thresholds2 = roc_curve(y_test, y_pred_probarf_tuned)\nplt.plot(fpr2, tpr2)\nplt.text(x = 0.02, y = 0.7, s = ('AUC Score for rf_tuned model:',round(roc_auc_score(y_test, y_pred_probarf_tuned),4)))\nplt.grid(True)\n\n## 4th classifier\nfpr3, tpr3, thresholds3 = roc_curve(y_test, y_pred_probadrfsm_tuned)\nplt.plot(fpr3, tpr3)\nplt.text(x = 0.02, y = 0.6, s = ('AUC Score for rfsm_tuned model:',round(roc_auc_score(y_test, y_pred_probadrfsm_tuned),4)))\nplt.grid(True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying few Ensemble Techniques","metadata":{}},{"cell_type":"markdown","source":"# XGBoost Classifier","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nX_train_xgb=X_train.astype(np.number)\nX_test_xgb=X_test.astype(np.number)\ny_train_xgb=y_train.astype(np.number)\ny_test_xgb=y_test.astype(np.number)\n\nxgb=XGBClassifier()\nxgb.fit(X_train_xgb,y_train_xgb)\n\nxgb.score(X_train_xgb,y_train_xgb)\n\ny_pred_xgb=xgb.predict(X_test)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred_xgb))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb.score(X_test_xgb,y_test_xgb)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report,accuracy_score,precision_score,confusion_matrix\n\ny_pred_xgb= xgb.predict(X_test_xgb)\ny_proba_xgb= xgb.predict_proba(X_test_xgb)\nprint('Roc_auc score:' ,roc_auc_score(y_test_xgb,y_proba_xgb[:,1]))\nprint('Classification Report:')\nprint(classification_report(y_test_xgb,y_pred_xgb))\n\nprint('precision_score')\nprint(precision_score(y_test_xgb,y_pred_xgb))\n\nprint('confusion_matrix')\nprint(confusion_matrix(y_test_xgb,y_pred_xgb))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest,f_classif\nselect_features = SelectKBest(f_classif,k='all')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AdaBoost Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\nad = AdaBoostClassifier(n_estimators=100)\nad.fit(X_train,y_train)\nprint(\"What is the Testing Accuracy\")\nprint(ad.score(X_test,y_test))\nprint(\"What is the Training Accuracy\")\nprint(ad.score(X_train,y_train))\n\ny_pred_ad=ad.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred_ad))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extra Trees Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\net=ExtraTreesClassifier()\n\net.fit(X_train,y_train)\net.score(X_train,y_train)\n\ny_pred_et=et.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_et=et.predict(X_test)\nprint(classification_report(y_test,y_pred_ad))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"et.score(X_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bagging Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\n\nbg = BaggingClassifier(n_estimators=2)\nbg.fit(X_train,y_train)\nprint(\"What is the Testing Accuracy\")\nprint(bg.score(X_test,y_test))\nprint(\"What is the Training Accuracy\")\nprint(bg.score(X_train,y_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_bg=bg.predict(X_test)\nprint(classification_report(y_test,y_pred_bg))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gradient Boosting","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier()\ngb.fit(X_train,y_train)\n\nprint(\"What is the Training Accuracy\")\nprint(gb.score(X_train,y_train))\n\nprint(\"What is the Testing Accuracy\")\nprint(gb.score(X_test,y_test))\ngb.feature_importances_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_gb=et.predict(X_test)\nprint(classification_report(y_test,y_pred_gb))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}