{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install textstat\n!pip install chart_studio","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load libraries "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import random\nimport math\nimport time\nimport string\nimport nltk\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport datetime\nimport operator\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.cluster import DBSCAN\nfrom nltk.corpus import stopwords\nfrom collections import  Counter\nimport matplotlib.pyplot as plt\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport pyLDAvis.gensim\nfrom tqdm import tqdm\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport pyLDAvis\nimport gensim\nimport spacy\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom pprint import pprint\nfrom copy import deepcopy\nfrom tqdm.notebook import tqdm\nplt.style.use('seaborn')\n%matplotlib inline \nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette() \nremove_words = set(stopwords.words('english')) \nfrom plotly import tools\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport pyLDAvis.sklearn\nfrom pylab import bone, pcolor, colorbar, plot, show, rcParams, savefig\nimport textstat\nimport matplotlib.colors as mcolors\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\nfrom statistics import *\nimport concurrent.futures\nimport geopandas as gpd\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\npunctuations = string.punctuation\nstopwords = list(STOP_WORDS)\n\nparser = English()\n\ndef plot_readability(a,title,bins=0.1,colors=['#3A4750']):\n    trace1 = ff.create_distplot([a], [\" Abstract \"], bin_size=bins, colors=colors, show_rug=False)\n    trace1['layout'].update(title=title)\n    iplot(trace1, filename='Distplot')\n    table_data= [[\"Statistical Measures\",\"Abstract\"],\n                [\"Mean\",mean(a)],\n                [\"Standard Deviation\",pstdev(a)],\n                [\"Variance\",pvariance(a)],\n                [\"Median\",median(a)],\n                [\"Maximum value\",max(a)],\n                [\"Minimum value\",min(a)]]\n    trace2 = ff.create_table(table_data)\n    iplot(trace2, filename='Table')\n    \npunctuations = string.punctuation\nstopwords = list(STOP_WORDS)\n\nparser = English()\ndef spacy_tokenizer(sentence):\n    #reference : https://www.kaggle.com/thebrownviking20/analyzing-quora-for-the-insinceres\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Use function created by others to clean data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"### functions for clean data\ndef format_name(author):\n    middle_name = \" \".join(author['middle'])\n    \n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    \n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    \n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n    \n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    \n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n\n    return \"; \".join(formatted)\ndef load_files(dirname):\n    filenames = os.listdir(dirname)\n    raw_files = []\n\n    for filename in tqdm(filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file)\n    \n    return raw_files\n\ndef generate_clean_df(all_files):\n    cleaned_files = []\n    \n    for file in tqdm(all_files):\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n            format_authors(file['metadata']['authors']),\n            format_authors(file['metadata']['authors'], \n                           with_affiliation=True),\n            format_body(file['abstract']),\n            format_body(file['body_text']),\n            format_bib(file['bib_entries']),\n            file['metadata']['authors'],\n            file['bib_entries']\n        ]\n\n        cleaned_files.append(features)\n\n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text', \n                 'bibliography','raw_authors','raw_bibliography']\n\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df.head()\n    \n    return clean_df\n\n##################################### clean all the dataset #########################\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nbiorxiv_dir = '/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/'\nfilenames = os.listdir(biorxiv_dir)\nall_files = []\nfor filename in filenames:\n    filename = biorxiv_dir + filename\n    file = json.load(open(filename, 'rb'))\n    all_files.append(file)\nfile = all_files[0]\ntexts = [(di['section'], di['text']) for di in file['body_text']]\ntexts_di = {di['section']: \"\" for di in file['body_text']}\n\nbody = \"\"\nfor section, text in texts_di.items():\n    body += section\n    body += \"\\n\\n\"\n    body += text\n    body += \"\\n\\n\"\nauthors = all_files[0]['metadata']['authors']\n\nbibs = list(file['bib_entries'].values())\nformat_authors(bibs[1]['authors'], with_affiliation=False)\nbib_formatted = format_bib(bibs[:5])\ncleaned_files = []\n\nfor file in tqdm(all_files):\n    features = [\n        file['paper_id'],\n        file['metadata']['title'],\n        format_authors(file['metadata']['authors']),\n        format_authors(file['metadata']['authors'], \n                       with_affiliation=True),\n        format_body(file['abstract']),\n        format_body(file['body_text']),\n        format_bib(file['bib_entries']),\n        file['metadata']['authors'],\n        file['bib_entries']\n    ]\n    \n    cleaned_files.append(features)\n    \n    col_names = [\n    'paper_id', \n    'title', \n    'authors',\n    'affiliations', \n    'abstract', \n    'text', \n    'bibliography',\n    'raw_authors',\n    'raw_bibliography'\n]\n\nclean_df = pd.DataFrame(cleaned_files, columns=col_names)\nclean_df.to_csv('biorxiv_clean.csv', index=False)\n\ncomm_dir = '/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/'\ncomm_files = load_files(comm_dir)\ncomm_df = generate_clean_df(comm_files)\ncomm_df.to_csv('clean_comm_use.csv', index=False)\nnoncomm_dir = '/kaggle/input/CORD-19-research-challenge/noncomm_use_subset/noncomm_use_subset/'\nnoncomm_files = load_files(noncomm_dir)\nnoncomm_df = generate_clean_df(noncomm_files)\nnoncomm_df.to_csv('clean_noncomm_use.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\n\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=50, figure_size=(15.0,15.0), \n                   title = None, title_size=20, image_color=False,color = color):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color=color,\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_wordcloud(clean_df['abstract'].values, title=\"Word Cloud of Authors in biorxiv medrxiv Data\",color = 'white')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_wordcloud(comm_df['abstract'].values, title=\"Word Cloud of Authors in comm use subset Data\",color = 'white')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_wordcloud(noncomm_df['abstract'].values, title=\"Word Cloud of Authors in non comm use subset Data\",color = 'white')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Word freqeuncy"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = clean_df['abstract'].dropna()\ndf3 = comm_df[\"abstract\"].dropna()\ndf2 = noncomm_df[\"abstract\"].dropna()\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    #Reference and credits: https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-qiqc\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in df1:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(25), 'green')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in df2:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(25), 'orange')\n\nfreq_dict = defaultdict(int)\nfor sent in df3:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(25), 'black')\n\n\n# Creating two subplots\nfig = tools.make_subplots(rows=2, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words in biorxiv_data\", \n                                          \"Frequent words in comm_data\",\n                                          \"Frequent words in noncomm_data\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\n\n\n\n\nfig['layout'].update(height=1000, width=800, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots of Abstracts\")\niplot(fig, filename='word-plots')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntext_1 = clean_df[\"abstract\"].dropna().apply(spacy_tokenizer)\ntext_2 = comm_df[\"abstract\"].dropna().apply(spacy_tokenizer)\ntext_3 = noncomm_df['abstract'].dropna().apply(spacy_tokenizer)\n#count vectorization\nvectorizer_1= CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\nvectorizer_2= CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\nvectorizer_3 =  CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n\ntext1_vectorized = vectorizer_1.fit_transform(text_1)\ntext2_vectorized = vectorizer_2.fit_transform(text_2)\ntext3_vectorized = vectorizer_3.fit_transform(text_3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlda1 = LatentDirichletAllocation(n_components=5, max_iter=5, learning_method='online',verbose=True)\nlda2= LatentDirichletAllocation(n_components=5, max_iter=5, learning_method='online',verbose=True)\nlda3 = LatentDirichletAllocation(n_components=5, max_iter=5, learning_method='online',verbose=True)\n\nlda_1 = lda1.fit_transform(text1_vectorized)\nlda_2 = lda2.fit_transform(text2_vectorized)\nlda_3 = lda3.fit_transform(text3_vectorized)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def selected_topics(model, vectorizer, top_n=10):\n    for idx, topic in enumerate(model.components_):\n        print(\"Topic %d:\" % (idx))\n        print([(vectorizer.get_feature_names()[i], topic[i])\n                        for i in topic.argsort()[:-top_n - 1:-1]]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"LDA Model of Bioarvix data Abstracts:\")\nselected_topics(lda1, vectorizer_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"LDA Model of clean_comm data Abstracts:\")\nselected_topics(lda2, vectorizer_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"LDA Model of clean_noncomm data Abstracts:\")\nselected_topics(lda3, vectorizer_3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading additional data for visualization "},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nCOVID19_line_list_data = pd.read_csv(\"../input/novel-corona-virus-2019-dataset/COVID19_line_list_data.csv\")\nCOVID19_open_line_list = pd.read_csv(\"../input/novel-corona-virus-2019-dataset/COVID19_open_line_list.csv\")\ncovid_19_data = pd.read_csv(\"../input/novel-corona-virus-2019-dataset/covid_19_data.csv\")\ntime_series_covid_19_confirmed = pd.read_csv(\"../input/novel-corona-virus-2019-dataset/time_series_covid_19_confirmed.csv\")\ntime_series_covid_19_deaths = pd.read_csv(\"../input/novel-corona-virus-2019-dataset/time_series_covid_19_deaths.csv\")\ntime_series_covid_19_recovered = pd.read_csv(\"../input/novel-corona-virus-2019-dataset/time_series_covid_19_recovered.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nCase = pd.read_csv(\"../input/coronavirusdataset/Case.csv\")\nPatientInfo = pd.read_csv(\"../input/coronavirusdataset/PatientInfo.csv\")\nPatientRoute = pd.read_csv(\"../input/coronavirusdataset/PatientRoute.csv\")\nRegion = pd.read_csv(\"../input/coronavirusdataset/Region.csv\")\nSearchTrend = pd.read_csv(\"../input/coronavirusdataset/SearchTrend.csv\")\nTime = pd.read_csv(\"../input/coronavirusdataset/Time.csv\")\nTimeAge = pd.read_csv(\"../input/coronavirusdataset/TimeAge.csv\")\nTimeGender = pd.read_csv(\"../input/coronavirusdataset/TimeGender.csv\")\nTimeProvince = pd.read_csv(\"../input/coronavirusdataset/TimeProvince.csv\")\nWeather = pd.read_csv(\"../input/coronavirusdataset/Weather.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### take a look at data \nprint(covid_19_data.head())\n\ncovid_19_data.describe()\ncovid_19_data.info()\ncovid_19_data.isnull().sum()\ncovid_19_data.head()\n\nprint(time_series_covid_19_confirmed.head())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grouping confirmed, recovered and death cases per country\ngrouped_country = covid_19_data.groupby([\"Country/Region\"],as_index=False)[\"Confirmed\",\"Recovered\",\"Deaths\"].last().sort_values(by=\"Confirmed\",ascending=False)\n\n# Using just first 10 countries with most cases\nmost_common_countries = grouped_country.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FUNCTION TO SHOW ACTUAL VALUES ON BARPLOT\n\ndef show_valushowes_on_bars(axs):\n    def _show_on_single_plot(ax):\n        for p in ax.patches:\n            _x = p.get_x() + p.get_width() / 2\n            _y = p.get_y() + p.get_height()\n            value = '{:.2f}'.format(p.get_height())\n            ax.text(_x, _y, value, ha=\"center\")\n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function returns interactive lineplot of confirmed cases on specific country\n\n\ndef getGrowthPerCountryInteractive(countryName):\n    country = covid_19_data[covid_19_data[\"Country/Region\"] == countryName]\n\n    fig_1 = px.line(country, x=\"observation_date\", y=\"Confirmed\", title=(countryName + \" confirmed cases.\"))\n    fig_2 = px.line(country, x=\"observation_date\", y=\"Deaths\", title=(countryName +\" death cases\"))\n    fig_3 = px.line(country, x=\"observation_date\", y=\"Recovered\", title=(countryName + \" recovered cases\"))\n\n    fig_1.show()\n    fig_2.show()\n    fig_3.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"china = covid_19_data[covid_19_data[\"Country/Region\"] == \"Mainland China\"]\nfig_1 = px.bar(china,x=\"Province/State\",y=\"Confirmed\",color=\"Recovered\",text=\"Deaths\")\nfig_1.show()\nchina_states = china[china[\"Province/State\"] != \"Hubei\"]\nfig_2 = px.bar(china_states,x=\"Province/State\",y=\"Confirmed\", title=\"Confirmed cases in other states of China.\")\nfig_2.show()\nfig_3 = px.bar(china_states, x=\"Province/State\",y=\"Recovered\",color=\"Deaths\",title=\"Recovered vs Deaths in other states of China.\")\nfig_3.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look at time series data"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = time_series_covid_19_confirmed.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed = time_series_covid_19_confirmed.loc[:, cols[4]:cols[-1]]\ndeaths = time_series_covid_19_deaths.loc[:, cols[4]:cols[-1]]\nrecoveries = time_series_covid_19_recovered.loc[:, cols[4]:cols[-1]]\ndates = confirmed.keys()\nworld_cases = []\ntotal_deaths = [] \nmortality_rate = []\ntotal_recovered = [] \nfor i in dates:\n    confirmed_sum = confirmed[i].sum()\n    death_sum = deaths[i].sum()\n    recovered_sum = recoveries[i].sum()\n    world_cases.append(confirmed_sum)\n    total_deaths.append(death_sum)\n    mortality_rate.append(death_sum/confirmed_sum)\n    total_recovered.append(recovered_sum)\ndays_since_1_22 = np.array([i for i in range(len(dates))]).reshape(-1, 1)\nworld_cases = np.array(world_cases).reshape(-1, 1)\ntotal_deaths = np.array(total_deaths).reshape(-1, 1)\ntotal_recovered = np.array(total_recovered).reshape(-1, 1)\ndays_in_future = 10\nfuture_forcast = np.array([i for i in range(len(dates)+days_in_future)]).reshape(-1, 1)\nadjusted_dates = future_forcast[:-10]\nstart = '1/22/2020'\nstart_date = datetime.datetime.strptime(start, '%m/%d/%Y')\nfuture_forcast_dates = []\nfor i in range(len(future_forcast)):\n    future_forcast_dates.append((start_date + datetime.timedelta(days=i)).strftime('%m/%d/%Y'))\nX_train_confirmed, X_test_confirmed, y_train_confirmed, y_test_confirmed = train_test_split(days_since_1_22, world_cases, test_size=0.15, shuffle=False) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> SVM estimtation"},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_confirmed = SVR(shrinking=True, kernel='poly',gamma=0.01, epsilon=1,degree=4, C=0.1)\nsvm_confirmed.fit(X_train_confirmed, y_train_confirmed)\nsvm_pred = svm_confirmed.predict(future_forcast)\n# check against testing data\nsvm_test_pred = svm_confirmed.predict(X_test_confirmed)\nprint('MAE:', mean_absolute_error(svm_test_pred, y_test_confirmed))\nprint('MSE:',mean_squared_error(svm_test_pred, y_test_confirmed))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Linear regression estimation"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlinear_model = LinearRegression(normalize=True, fit_intercept=False)\nlinear_model.fit(X_train_confirmed, y_train_confirmed)\ntest_linear_pred = linear_model.predict(X_test_confirmed)\nlinear_pred = linear_model.predict(future_forcast)\nprint('MAE:', mean_absolute_error(test_linear_pred, y_test_confirmed))\nprint('MSE:',mean_squared_error(test_linear_pred, y_test_confirmed))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Bayesian Ridge estimation"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntol = [1e-4, 1e-3, 1e-2]\nalpha_1 = [1e-7, 1e-6, 1e-5, 1e-4]\nalpha_2 = [1e-7, 1e-6, 1e-5, 1e-4]\nlambda_1 = [1e-7, 1e-6, 1e-5, 1e-4]\nlambda_2 = [1e-7, 1e-6, 1e-5, 1e-4]\n\nbayesian_grid = {'tol': tol, 'alpha_1': alpha_1, 'alpha_2' : alpha_2, 'lambda_1': lambda_1, 'lambda_2' : lambda_2}\n\nbayesian = BayesianRidge()\nbayesian_search = RandomizedSearchCV(bayesian, bayesian_grid, scoring='neg_mean_squared_error', cv=3, return_train_score=True, n_jobs=-1, n_iter=40, verbose=1)\nbayesian_search.fit(X_train_confirmed, y_train_confirmed)\n\nbayesian_search.best_params_\nbayesian_confirmed = bayesian_search.best_estimator_\ntest_bayesian_pred = bayesian_confirmed.predict(X_test_confirmed)\nbayesian_pred = bayesian_confirmed.predict(future_forcast)\nprint('MAE:', mean_absolute_error(test_bayesian_pred, y_test_confirmed))\nprint('MSE:',mean_squared_error(test_bayesian_pred, y_test_confirmed))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Further estimation"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"\n# Future predictions using SVM \nprint('SVM future predictions:')\nset(zip(future_forcast_dates[-10:], svm_pred[-10:]))\n# Future predictions using Linear Regression \nprint('Ridge regression future predictions:')\nset(zip(future_forcast_dates[-10:], bayesian_pred[-10:]))\n# Future predictions using Linear Regression \nprint('Linear regression future predictions:')\nprint(linear_pred[-10:])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualization "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlatest_confirmed = time_series_covid_19_confirmed[dates[-1]]\nlatest_deaths = time_series_covid_19_deaths[dates[-1]]\nlatest_recoveries = time_series_covid_19_recovered[dates[-1]]\nunique_countries =  list(time_series_covid_19_confirmed['Country/Region'].unique())\ncountry_confirmed_cases = []\nno_cases = []\nfor i in unique_countries:\n    cases = latest_confirmed[time_series_covid_19_confirmed['Country/Region']==i].sum()\n    if cases > 0:\n        country_confirmed_cases.append(cases)\n    else:\n        no_cases.append(i)\n        \nfor i in no_cases:\n    unique_countries.remove(i)\n    \nunique_countries = [k for k, v in sorted(zip(unique_countries, country_confirmed_cases), key=operator.itemgetter(1), reverse=True)]\nfor i in range(len(unique_countries)):\n    country_confirmed_cases[i] = latest_confirmed[time_series_covid_19_confirmed['Country/Region']==unique_countries[i]].sum()\n    \n    # number of cases per country/region\nprint('Confirmed Cases by Countries/Regions:')\nfor i in range(len(unique_countries)):\n    print(f'{unique_countries[i]}: {country_confirmed_cases[i]} cases')\nunique_provinces =  list(time_series_covid_19_confirmed['Province/State'].unique())\n# those are countries, which are not provinces/states.\noutliers = ['United Kingdom', 'Denmark', 'France']\nfor i in outliers:\n    unique_provinces.remove(i)\n    province_confirmed_cases = []\nno_cases = [] \nfor i in unique_provinces:\n    cases = latest_confirmed[time_series_covid_19_confirmed['Province/State']==i].sum()\n    if cases > 0:\n        province_confirmed_cases.append(cases)\n    else:\n        no_cases.append(i)\n \n# remove areas with no confirmed cases\nfor i in no_cases:\n    unique_provinces.remove(i)\n    \nunique_provinces = [k for k, v in sorted(zip(unique_provinces, province_confirmed_cases), key=operator.itemgetter(1), reverse=True)]\nfor i in range(len(unique_provinces)):\n    province_confirmed_cases[i] = latest_confirmed[time_series_covid_19_confirmed['Province/State']==unique_provinces[i]].sum()\n    \n    # number of cases per province/state/city\nprint('Confirmed Cases by Province/States (US, China, Australia, Canada):')\nfor i in range(len(unique_provinces)):\n    print(f'{unique_provinces[i]}: {province_confirmed_cases[i]} cases')\n    \n    nan_indices = [] \n\n# handle nan if there is any, it is usually a float: float('nan')\n\nfor i in range(len(unique_provinces)):\n    if type(unique_provinces[i]) == float:\n        nan_indices.append(i)\n\nunique_provinces = list(unique_provinces)\nprovince_confirmed_cases = list(province_confirmed_cases)\n\nfor i in nan_indices:\n    unique_provinces.pop(i)\n    province_confirmed_cases.pop(i)\n    \nchina_confirmed = latest_confirmed[time_series_covid_19_confirmed['Country/Region']=='China'].sum()\noutside_mainland_china_confirmed = np.sum(country_confirmed_cases) - china_confirmed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nprint('Outside Mainland China {} cases:'.format(outside_mainland_china_confirmed))\nprint('Mainland China: {} cases'.format(china_confirmed))\nprint('Total: {} cases'.format(china_confirmed+outside_mainland_china_confirmed))\n# Only show 10 countries with the most confirmed cases, the rest are grouped into the other category\nvisual_unique_countries = [] \nvisual_confirmed_cases = []\nothers = np.sum(country_confirmed_cases[10:])\nfor i in range(len(country_confirmed_cases[:10])):\n    visual_unique_countries.append(unique_countries[i])\n    visual_confirmed_cases.append(country_confirmed_cases[i])\n\nvisual_unique_countries.append('Others')\nvisual_confirmed_cases.append(others)\n# lets look at it in a logarithmic scale \nlog_country_confirmed_cases = [math.log10(i) for i in visual_confirmed_cases]\n\n# Only show 10 provinces with the most confirmed cases, the rest are grouped into the other category\nvisual_unique_provinces = [] \nvisual_confirmed_cases2 = []\nothers = np.sum(province_confirmed_cases[10:])\nfor i in range(len(province_confirmed_cases[:10])):\n    visual_unique_provinces.append(unique_provinces[i])\n    visual_confirmed_cases2.append(province_confirmed_cases[i])\n\nvisual_unique_provinces.append('Others')\nvisual_confirmed_cases2.append(others)\n\n\nplt.figure(figsize=(32, 18))\nplt.barh(visual_unique_provinces, visual_confirmed_cases2)\nplt.title('# of Coronavirus Confirmed Cases in Provinces/States', size=20)\nplt.show()\n\nc = random.choices(list(mcolors.CSS4_COLORS.values()),k = len(unique_countries))\nplt.figure(figsize=(20,20))\nplt.title('Covid-19 Confirmed Cases per Country')\nplt.pie(visual_confirmed_cases, colors=c)\nplt.legend(visual_unique_countries, loc='best')\nplt.show()\n\n\nc = random.choices(list(mcolors.CSS4_COLORS.values()),k = len(unique_countries))\nplt.figure(figsize=(20,20))\nplt.title('Covid-19 Confirmed Cases in Countries Outside of Mainland China')\nplt.pie(visual_confirmed_cases[1:], colors=c)\nplt.legend(visual_unique_countries[1:], loc='best')\nplt.show()\n\nus_regions = list(time_series_covid_19_confirmed[time_series_covid_19_confirmed['Country/Region']=='US']['Province/State'].unique())\nus_confirmed_cases = []\nno_cases = [] \nfor i in us_regions:\n    cases = latest_confirmed[time_series_covid_19_confirmed['Province/State']==i].sum()\n    if cases > 0:\n        us_confirmed_cases.append(cases)\n    else:\n        no_cases.append(i)\n \n## remove areas with no confirmed cases\nfor i in no_cases:\n    us_regions.remove(i)\n\nc = random.choices(list(mcolors.CSS4_COLORS.values()),k = len(unique_countries))\nplt.figure(figsize=(20,20))\nplt.title('Covid-19 Confirmed Cases in the United States')\nplt.pie(us_confirmed_cases, colors=c)\nplt.legend(us_regions, loc='best')\nplt.show()\n\nchina_regions = list(time_series_covid_19_confirmed[time_series_covid_19_confirmed['Country/Region']=='China']['Province/State'].unique())\nchina_confirmed_cases = []\nno_cases = [] \nfor i in china_regions:\n    cases = latest_confirmed[time_series_covid_19_confirmed['Province/State']==i].sum()\n    if cases > 0:\n        china_confirmed_cases.append(cases)\n    else:\n        no_cases.append(i)\n \n# remove areas with no confirmed cases\nfor i in no_cases:\n    china_confirmed_cases.remove(i)\nc = random.choices(list(mcolors.CSS4_COLORS.values()),k = len(unique_countries))\nplt.figure(figsize=(20,20))\nplt.title('Covid-19 Confirmed Cases in the Mainland China')\nplt.pie(china_confirmed_cases, colors=c)\nplt.legend(china_regions, loc='best')\nplt.show()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_series_covid_19_confirmed.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Point visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x=\"Long\", y=\"Lat\", data=time_series_covid_19_confirmed);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import mplleaflet as mpll\n# comment out the last line of this cell for rendering Leaflet map.\nrids = np.arange(time_series_covid_19_confirmed.shape[0])\nnp.random.shuffle(rids)\nf, ax = plt.subplots(1, figsize=(6, 6))\ntime_series_covid_19_confirmed.iloc[rids[:100], :].plot(kind='scatter', x='Long', y='Lat', \\\n                      s=30, linewidth=0, ax=ax);\nmpll.display(fig=f,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from shapely.geometry import Point\nxys_wb = gpd.GeoSeries(time_series_covid_19_confirmed[['Long', 'Lat']].apply(Point, axis=1), \\\n                      crs=\"+init=epsg:4326\")\nxys_wb = xys_wb.to_crs(epsg=3857)\nx_wb = xys_wb.apply(lambda i: i.x)\ny_wb = xys_wb.apply(lambda i: i.y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom bokeh.plotting import figure, output_notebook, show\nfrom bokeh.tile_providers import STAMEN_TONER\noutput_notebook()\nfrom bokeh.plotting import figure, output_notebook, show, ColumnDataSource\noutput_notebook()\nminx, miny, maxx, maxy = xys_wb.total_bounds\ny_range = miny, maxy\nx_range = minx, maxx\n\ndef base_plot(tools='pan,wheel_zoom,reset',plot_width=600, plot_height=400, **plot_args):\n    p = figure(tools=tools, plot_width=plot_width, plot_height=plot_height,\n        x_range=x_range, y_range=y_range, outline_line_color=None,\n        min_border=0, min_border_left=0, min_border_right=0,\n        min_border_top=0, min_border_bottom=0, **plot_args)\n\n    p.axis.visible = False\n    p.xgrid.grid_line_color = None\n    p.ygrid.grid_line_color = None\n    return p\n\noptions = dict(line_color=None, fill_color='#800080', size=4)\n\np = base_plot()\np.add_tile(STAMEN_TONER)\np.circle(x=x_wb, y=y_wb, **options)\n#<div class=\"bk-banner\">\n#<a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n#<span id=\"efa98bda-2ccf-4dbf-ae97-94033d60c79b\">Loading BokehJS ...</span>\n#</div>\n#<bokeh.models.renderers.GlyphRenderer at 0x1052bb5f8>","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datashader as ds\n#from datashader.callbacks import InteractiveImage\nfrom datashader.colors import viridis\nfrom datashader import transfer_functions as tf\nfrom bokeh.tile_providers import STAMEN_TONER\n\np = base_plot()\np.add_tile(STAMEN_TONER)\n\npts = pd.DataFrame({'x': x_wb, 'y': y_wb})\npts['count'] = 1\ndef create_image90(x_range, y_range, w, h):\n    cvs = ds.Canvas(plot_width=w, plot_height=h, x_range=x_range, y_range=y_range)\n    agg = cvs.points(pts, 'x', 'y',  ds.count('count'))\n    img = tf.interpolate(agg.where(agg > np.percentile(agg,90)), \\\n                         cmap=viridis, how='eq_hist')\n    return tf.dynspread(img, threshold=0.1, max_px=4)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(time_series_covid_19_confirmed['Long'], time_series_covid_19_confirmed['Lat'], shade=True, cmap='viridis');\nf, ax = plt.subplots(1, figsize=(9, 9))\nsns.kdeplot(time_series_covid_19_confirmed['Long'], time_series_covid_19_confirmed['Lat'], \\\n            shade=True, cmap='Purples', \\\n            ax=ax);\n\nax.set_axis_off()\nplt.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}