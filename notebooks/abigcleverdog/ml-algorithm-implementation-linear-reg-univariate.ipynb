{"cells":[{"metadata":{},"cell_type":"markdown","source":"Tow approaches for linear regresssion: 1. Gradient Descent (Batch, Stochastic, Mini-Batch) used in many ML algorithms; 2. Normal Equation"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a testing data pair\nx = 4 * np.random.rand(20, 1)\ny = x + 2 * np.random.rand(20, 1)\nsns.scatterplot(list(x), list(y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gradient Descent approach\n- hyphothesis $h_\\theta$: $h_\\theta = \\theta_0 + \\theta_1 \\cdot x$ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;      *linear reg assumes the data fits to a straight line $y = a\\cdot x + b$*\n- cost function $J(\\theta_0, \\theta_1)$: $J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x_i) - y_i)^2$  &nbsp; &nbsp;     *it is similar to the average of SSE (Sum of Squared Error) $SSE = \\sum_{i=1}^{m}(y_i - \\bar{y})^2$* **the difference is $h_\\theta(x_i)$ replaced $\\bar{y}$**, thus minimize cost function is the same as minimize the error, leading to the best fit\n- gradient descent: $\\theta_j = \\theta_j - \\alpha\\cdot\\frac{\\partial}{\\partial \\theta_j}J(\\theta_0, \\theta_1)$\n    - $\\theta_j$ is the $j^{th}$ parameter\n    - $\\alpha$ is the learning rate\n    - $\\frac{\\partial}{\\partial \\theta_j}J(\\theta_0, \\theta_1)$ is the partial derivative of the cost function (error) over the $j^{th}$ parameter, this term will become zero when all parameters are optimized.\n    - each iteration we adjust $\\theta_j$ according to the derivative by the scale of the learning rate $\\alpha$\n        - when $\\alpha$ is too small, it can takes long time for the cost function value to decrease to minimal\n        - when $\\alpha$ is too big, the cost function value can pass the local minimal and swag around and fail to converge\n- gradient descent after taking derivative: $\\theta_j = \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^{m} [(h_\\theta (x_i) - y) x_i]$\n- gradient descent break down to each parameter: \\begin{align}\n \\theta_0 & = \\theta_0 - \\alpha \\frac{1}{k} \\sum_{i=1}^{k} (h_\\theta(x^{i}) - y^{i}) \\\\  \n\\theta_1 & = \\theta_1 - \\alpha \\frac{1}{k} \\sum_{i=1}^{k} ((h_\\theta(x^{i}) - y^{i}) \\cdot x^{i}) \\\\\n\\end{align}\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%timeit\n# iterative implementataion\n\nh = lambda x: theta_0 + theta_1 * x # set hypothesis\ntheta_0 = theta_1 = 0 # initiate the parameters\nlr = 0.05 # learning rate\nepochs = 20\ncosts = []\nparas = []\n\ndef cal_sum(h, x, y):\n    sum0 = sum1 = 0\n    for i in range(len(x)):\n        sum0 += (h(x[i]) - y[i])\n        sum1 += (h(x[i]) - y[i]) * x[i]\n    return sum0, sum1\n\ndef cal_cost(h, x, y):\n    j = 0\n    for i in range(len(x)):\n        j += (h(x[i]) - y[i]) ** 2\n    return j / (2 * len(x))\n\nfor i in range(epochs):\n    sum0, sum1 = cal_sum(h, x[:,0], y[:,0])\n    theta_0 -= lr / len(x) * sum0\n    theta_1 -= lr / len(x) * sum1\n    paras.append((theta_0, theta_1))\n    costs.append(cal_cost(h, x[:,0], y[:,0]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.scatter(x = range(len(costs)), y = costs)\nplt.plot(costs, 'go-');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(x, y, 'bo')\ndef plot_line(t0, t1, x):\n    y = lambda x: t0 + t1*x\n    x_values = [i for i in range(int(min(x))-1, int(max(x))+2)]\n    y_values = [y(x) for x in x_values]\n    color = list(np.random.random(size=3))\n    plt.plot(x_values, y_values, c = color)\nfor t0, t1 in paras:\n    plot_line(t0, t1, x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.concatenate((np.ones((len(x),1)),x), axis = 1) # adding constant term to each train data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%timeit\n\n# implementation with linear algebra\n# for quick review of linear algebra relavent to this part: https://www.holehouse.org/mlclass/03_Linear_algebra_review.html; https://www.youtube.com/watch?v=Dft1cqjwlXE&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=13&t=0s \ndef gradientDescent(X, y, theta, alpha, num_iters):\n    \"\"\"\n       Performs gradient descent to learn theta\n    \"\"\"\n    m = y.size  # number of training examples\n    for i in range(num_iters):\n        y_hat = np.dot(X, theta)\n        theta = theta - alpha * (1.0/m) * np.dot(X.T, y_hat-y)\n    return theta\n\ntheta = np.zeros((2,1))\nepochs = 20 # run 20 single step gd to get cost functions and parameters to compare with the non-linear-algebra approach\ngd_costs = []\ngd_paras = []\n\nfor i in range(epochs):\n    theta = gradientDescent(X, y, theta, 0.05, 1)\n    gd_paras.append(theta)\n    gd_costs.append(cal_cost(lambda x: theta[0] + theta[1]*x, x[:, 0], y[:, 0]))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Linear algebra has no speed advantage in this scenario (small # features and small # train)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(gd_costs, 'go-')\nplt.plot(costs, 'r+');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(x, y, 'bo')\n\nfor t0, t1 in gd_paras:\n    plot_line(t0, t1, x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(18,9))\nplt.subplots_adjust(hspace=.5)\n\nplt.subplot2grid((1,2), (0,0))\nplt.plot(x, y, 'bo')\n\nfor t0, t1 in paras:\n    plot_line(t0, t1, x)\nplt.title('Iterative approach')\n\nplt.subplot2grid((1,2), (0,1))\nplt.plot(x, y, 'bo')\n\nfor t0, t1 in gd_paras:\n    plot_line(t0, t1, x)\nplt.title('Linear Algebra approach')\n\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Normal Equation approach\n- hyphothesis $h_\\theta$: $h_\\theta = \\theta_0 + \\theta_1 \\cdot x$\n- parameters $\\theta = (X^T X)^{-1} X^T y$"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_T = np.transpose(X)\ninverse = np.linalg.inv(np.dot(X_T,X))\ntheta = np.dot(np.dot(inverse, X_T), y)\nprint(theta)\nprint(cal_cost(lambda x: theta[0] + theta[1]*x, x[:, 0], y[:, 0]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/suicide-rates-overview-1985-to-2016/master.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.shape)\nprint(df.country.unique(), df.country.nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.rename(columns={'suicides/100k pop': 'suicides_per_100k_pop',\n                  ' gdp_for_year ($) ': 'gdp_for_year',\n                  'gdp_per_capita ($)': 'gdp_per_capita'}, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_rate = df.groupby('country').suicides_per_100k_pop.mean().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_gdp_cap = df.groupby('country').gdp_per_capita.mean().reset_index()\nnew_df = pd.merge(country_rate, country_gdp_cap, on='country')\nnew_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(data = new_df, x = 'gdp_per_capita', y = 'suicides_per_100k_pop');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(data = new_df, x = 'gdp_per_capita', y = 'suicides_per_100k_pop');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gradient Descent approach for linear regression\n***************"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = new_df.gdp_per_capita/10000 # x turned to be to big that will send y values out of range for python to handle\ny = new_df.suicides_per_100k_pop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# iterative implementation\n\nh = lambda x: theta_0 + theta_1 * x # set hypothesis\ntheta_0 = theta_1 = 0 # initiate the parameters\nlr = 0.05 # learning rate\nepochs = 200\ncosts = []\nparas = []\n\ndef cal_sum(h, x, y):\n    sum0 = sum1 = 0\n    for i in range(len(x)):\n        sum0 += (h(x[i]) - y[i])\n        sum1 += (h(x[i]) - y[i]) * x[i]\n    return sum0, sum1\n\ndef cal_cost(h, x, y):\n    j = 0\n    for i in range(len(x)):\n        j += (h(x[i]) - y[i]) ** 2\n    return j / (2 * len(x))\n\nfor i in range(epochs):\n    sum0, sum1 = cal_sum(h, x, y)\n    theta_0 -= lr / len(x) * sum0\n    theta_1 -= lr / len(x) * sum1\n    paras.append((theta_0, theta_1))\n    costs.append(cal_cost(h, x, y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(costs, 'go-');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(x, y, 'bo')\n\nfor t0, t1 in paras:\n    plot_line(t0, t1, x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.concatenate((np.ones((len(x),1)),pd.DataFrame(x)), axis = 1) # adding constant term to each train data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# implementation with linear algebra\n# for quick review of linear algebra relavent to this part: https://www.holehouse.org/mlclass/03_Linear_algebra_review.html; https://www.youtube.com/watch?v=Dft1cqjwlXE&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=13&t=0s \ndef gradientDescent(X, y, theta, alpha, num_iters):\n    \"\"\"\n       Performs gradient descent to learn theta\n    \"\"\"\n    m = y.size  # number of training examples\n    for i in range(num_iters):\n        y_hat = np.dot(X, theta)\n        theta = theta - alpha * (1.0/m) * np.dot(X.T, y_hat-pd.DataFrame(y))\n    return theta\n\ntheta = np.zeros((2,1))\nepochs = 200 # run 20 single step gd to get cost functions and parameters to compare with the non-linear-algebra approach\ngd_costs = []\ngd_paras = []\n\nfor i in range(epochs):\n    theta = gradientDescent(X, y, theta, 0.05, 1)\n    gd_paras.append(theta)\n    gd_costs.append(cal_cost(lambda x: theta[0] + theta[1]*x, x, y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(gd_costs, 'go-')\nplt.plot(costs, 'r+');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(x, y, 'bo')\n\nfor t0, t1 in gd_paras:\n    plot_line(t0, t1, x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,6))\nplt.subplots_adjust(hspace=.5)\n\nplt.subplot2grid((1,2), (0,0))\nplt.plot(x, y, 'bo')\n\nfor t0, t1 in paras:\n    plot_line(t0, t1, x)\nplt.title('Iterative approach')\n\nplt.subplot2grid((1,2), (0,1))\nplt.plot(x, y, 'bo')\n\nfor t0, t1 in gd_paras:\n    plot_line(t0, t1, x)\nplt.title('Linear Algebra approach')\n\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normal Equation approach\nsource: https://medium.com/@dikshitkathuria1803/normal-equation-using-python-5993454fbb41\n****************\n$\\theta = (X^T X)^{-1} (X^T y)$"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_transpose = np.transpose(X)   #calculating transpose\nx_transpose_dot_x = x_transpose.dot(X)  # calculating dot product\ntemp_1 = np.linalg.inv(x_transpose_dot_x) #calculating inverse\n\ntemp_2 = x_transpose.dot(y)  \n\npara = temp_1.dot(temp_2)\npara","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(x, y, 'bo')\nplot_line(para[0], para[1], x)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}