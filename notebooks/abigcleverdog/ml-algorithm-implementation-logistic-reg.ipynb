{"cells":[{"metadata":{},"cell_type":"markdown","source":"This series of algorithm implementation practice may be too easy for a lot of people. I took linear algebra over 10 years ago and I found it helpful as the practices cleared a lot of blurry spots for me. Link to implementation of linear regression is [here](https://www.kaggle.com/abigcleverdog/ml-algorithm-implementation-linear-reg-multivar)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Compare to Linear Regression:\n\n#### Hypothesis\n\nLinear Regression:\n\n$h_\\theta = \\theta_0 + \\theta_1 \\cdot x$\n\n$h(x) = \\theta^T \\cdot x$\n\nLogistic (sigmoid) Regression:\n\n$h^{Linear}_\\theta = \\theta_0 + \\theta_1 \\cdot x$\n\n$h^{Logistic}_\\theta = \\frac{1}{1 + e^{-h^{Linear}_\\theta}}$\n\n$h(x) = \\sigma (\\theta^T \\cdot x)$\n\n$h(x) = \\frac{1}{1 + e^{-\\theta^T \\cdot x}}$\n\n#### Cost function\n\nLinear Regression:\n\n$J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x_i) - y_i)^2$\n\nLogistic Regression:\n\n$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y_i log(h(x_i)) + (1 - y_i) log(1 - h(x_i))]$\n\n#### Gradient\n\nLinear Regression:\n\n$\\frac{\\partial}{\\partial \\theta_j}J(\\theta_0, \\theta_1) = \\sum_{i=1}^{m} [(h_\\theta (x_i) - y) x_i]$\n\nLogistic Regression:\n\n$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h(x^i) - y^i) x^i_j$"},{"metadata":{},"cell_type":"markdown","source":"### univariate"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# create sample data\nx1 = np.random.rand(10, 1)\nx2 = 1 + np.random.rand(10, 1)\nx = np.append(x1,x2)\ny1 = [0] * 10\ny2 = [1] * 10\ny = y1 + y2\n\nplt.plot(x, y, 'bo')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"theta_0 = theta_1 = 0\ng = lambda x: theta_0 + theta_1 * x\nh = lambda x: 1 / (1 + np.exp(-g(x)))\n\nlr = 0.05 # learning rate\nepochs = 2000\ncosts = []\nparas = []\n\ndef cal_cost(h, x, y):\n    j = 0\n    for i in range(len(x)):\n        j += y[i] * np.log(h(x[i])) + (1 - y[i]) * np.log(1 - h(x[i]))\n    return -j / len(x)\n\ndef cal_sum(h, x, y):\n    sum_0 = sum_1 = 0\n    for i in range(len(x)):\n        sum_0 += (h(x[i]) - y[i])\n        sum_1 += (h(x[i]) - y[i]) * x[i]\n    return sum_0 / len(x), sum_1 / len(x)\n\nfor i in range(epochs):\n    sum_0, sum_1 = cal_sum(h, x, y)\n    theta_0 -= lr * sum_0\n    theta_1 -= lr * sum_1\n    cost = cal_cost(h, x, y)\n    costs.append(cost)\n    paras.append([theta_0, theta_1])\n    \nprint(costs[-10:])\nprint(paras[-10:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(costs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boundaries = [-i[0]/i[1] for i in paras]\nplt.plot(boundaries)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### bivariate"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create sample data\nx_base = np.arange(0,2.0, 0.1)\nx1 = x_base * x_base * 4\nx2 = np.exp(x_base) * 2\ny1 = [0] * 10\ny2 = [1] * 10\ny = y1 + y2\nplt.scatter(x1, x2);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(list(zip(x1, x2, y)), \n               columns =['x1', 'x2', 'y']) \nsns.scatterplot(data = df, x = 'x1', y = 'x2', hue = 'y')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:,0:2]\ny = df.iloc[:,2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# iterative implementation\ntheta = [0, 0, 0]\ng = lambda x: theta[0] + theta[1] * x[0] + theta[2] * x[1]\nh = lambda x: 1 / (1 + np.exp(-g(x)))\n\nlr = 0.5 # learning rate\nepochs = 500\ncosts = []\nparas = []\n\ndef cal_cost(h, x, y):\n    j = 0\n    for i in range(len(x)):\n        j += y[i] * np.log(h(x.iloc[i])) + (1 - y[i]) * np.log(1 - h(x.iloc[i]))\n    return -j / len(x)\n\ndef cal_sum(h, x, y):\n    sum_0 = sum_1 = sum_2 = 0\n    for i in range(len(x)):\n        sum_0 += (h(x.iloc[i]) - y[i])\n        sum_1 += (h(x.iloc[i]) - y[i]) * x.iloc[i][0]\n        sum_2 += (h(x.iloc[i]) - y[i]) * x.iloc[i][1]\n    return sum_0 / len(x), sum_1 / len(x), sum_2 / len(x)\n\ndef log_reg(h, x, y, theta, lr, epochs):\n    for i in range(epochs):\n        sum_0, sum_1, sum_2 = cal_sum(h, x, y)\n        theta[0] -= lr * sum_0\n        theta[1] -= lr * sum_1\n        theta[2] -= lr * sum_2\n        cost = cal_cost(h, x, y)\n        costs.append(cost)\n        paras.append(theta)\n\n    print(costs[-5:])\n    print(paras[-5:])\n    \nlog_reg(h, X, y, theta, lr, epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_line(theta, x):\n    y = lambda x: -(theta[0] + theta[1] * x)/theta[2]\n    x_values = [i for i in range(int(min(x))-1, int(max(x))+2)]\n    y_values = [y(x) for x in x_values]\n    color = list(np.random.random(size=3))\n    plt.plot(x_values, y_values, c = color)\n    \n    \nsns.scatterplot(data = df, x = 'x1', y = 'x2', hue = 'y')\nfor i,t in enumerate(paras):\n    if i%100 == 0: \n        plot_line(t, list(df.iloc[:, 0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The boundary changes very slowly as the samples were easily seperated in initial circles. further optimization does not leading to much change in cost function so there is not much driving force to adjust the boundary. In this specific senario, supported vector machine should work better."},{"metadata":{"trusted":true},"cell_type":"code","source":"# implementation with linear algebra\nX = np.concatenate((np.ones((X.shape[0], 1)) , X), axis = 1)\ntheta = np.zeros(X.shape[1])\n\nlr = 0.5 # learning rate\nepochs = 500\ncosts = []\nparas = []\n\ndef cal_cost(h, x, y):\n    return (-y * np.log(h) - (1-y) * np.log(1-h)).mean()    \n\ndef log_reg(h, x, y, theta, lr, epochs):\n    for i in range(epochs):\n        z = np.dot(X, theta)\n        h = 1/(1 + np.exp(-z))\n        gradient = np.dot(X.T, (h - y)) / y.size\n        theta -= lr * gradient\n        cost = cal_cost(h, x, y)\n        costs.append(cost)\n        paras.append(theta)\n\n    print(costs[-5:])\n    print(paras[-5:])\n    \nlog_reg(h, X, y, theta, lr, epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(data = df, x = 'x1', y = 'x2', hue = 'y')\nfor i,t in enumerate(paras):\n    if i%100 == 0: \n        plot_line(t, list(df.iloc[:, 0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score \n\nX = df.iloc[:,0:2]\ny = df.iloc[:,2]\nmodel = LogisticRegression()\nmodel.fit(X, y)\npredicted_classes = model.predict(X)\naccuracy = accuracy_score(y, predicted_classes)\nparameters = model.coef_\nintercept = model.intercept_\n\nprint(accuracy)\nprint(intercept, parameters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(data = df, x = 'x1', y = 'x2', hue = 'y')\nplot_line([intercept, parameters[0][0], parameters[0][1]], list(df.iloc[:, 0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/suicide-rates-overview-1985-to-2016/master.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.rename(columns={'gdp_per_capita ($)': 'gdp_per_capita'}, inplace = True)\ndf = df[['gdp_per_capita', 'suicides_no', 'sex']]\ndef t_c(df):\n    if df['sex'] == 'female':\n        return 0    \n    else:\n        return 1\ndf['sex'] = df.apply(t_c, axis=1)\ndf['gdp_per_capita'] /= df['gdp_per_capita'].max()*0.01\ndf['suicides_no'] /= df['suicides_no'].max()*0.01\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(data = df, y = 'suicides_no', x = 'gdp_per_capita', hue = 'sex')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:,0:2]\ny = df.iloc[:,2]\n\nX = np.concatenate((np.ones((X.shape[0], 1)) , X), axis = 1)\ntheta = np.zeros(X.shape[1])\n\nlr = 0.15 # learning rate\nepochs = 500\ncosts = []\nparas = []\n\ndef cal_cost(h, x, y):\n    return (-y * np.log(h) - (1-y) * np.log(1-h)).mean()    \n\ndef log_reg(h, x, y, theta, lr, epochs):\n    for i in range(epochs):\n        z = np.dot(X, theta)\n        h = 1/(1 + np.exp(-z))\n        gradient = np.dot(X.T, (h - y)) / y.size\n        theta -= lr * gradient\n        cost = cal_cost(h, x, y)\n        costs.append(cost)\n        paras.append(theta)\n\n    print(costs[-5:])\n    print(paras[-5:])\n    \nlog_reg(h, X, y, theta, lr, epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(data = df, y = 'suicides_no', x = 'gdp_per_capita', hue = 'sex')\nfor i,t in enumerate(paras):\n    if i%100 == 0: \n        plot_line(t, list(df.iloc[:, 0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score \n\nX = df.iloc[:,0:2]\nmodel = LogisticRegression()\nmodel.fit(X, y)\npredicted_classes = model.predict(X)\naccuracy = accuracy_score(y, predicted_classes)\nparameters = model.coef_\nintercept = model.intercept_\n\nprint(accuracy)\nprint(intercept, parameters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(data = df, y = 'suicides_no', x = 'gdp_per_capita', hue = 'sex')\nplot_line([intercept, parameters[0][0], parameters[0][1]], list(df.iloc[:, 0]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}