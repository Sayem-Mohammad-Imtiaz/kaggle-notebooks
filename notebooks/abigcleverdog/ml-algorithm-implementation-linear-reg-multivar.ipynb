{"cells":[{"metadata":{},"cell_type":"markdown","source":"For univariate: https://www.kaggle.com/abigcleverdog/ml-algorithm-implementation-linear-reg-univariate"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a testing data pair\nx = pd.DataFrame([[1,4],[2,5],[3,6],[3,6]])\ny = pd.DataFrame([8,9,12,12])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gradient Descent approach\nnumber of features, n = 2 for the testing data; number of training examples, m = 4\n- hyphothesis $h_\\theta$: $h_\\theta = \\theta_0 \\cdot x[0] + \\theta_1 \\cdot x[1] + \\theta_2 \\cdot x[2] $ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;      *x[0] = 1 is a constant term that added to each train data*\n- cost function $J(\\theta_0, \\theta_1, \\theta_2)$: $J(\\theta_0, \\theta_1, \\theta_2) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^i) - y^i)^2$  &nbsp; &nbsp;     *it is similar to the average of SSE (Sum of Squared Error) $SSE = \\sum_{i=1}^{m}(y_i - \\bar{y})^2$* **the difference is $h_\\theta(x_i)$ replaced $\\bar{y}$**, thus minimize cost function is the same as minimize the error, leading to the best fit\n- gradient descent: $\\theta_j = \\theta_j - \\alpha\\cdot\\frac{\\partial}{\\partial \\theta_j}J(\\theta_0, \\theta_1, \\theta_2)$\n    - $\\theta_j$ is the $j^{th}$ parameter/feature\n    - $\\alpha$ is the learning rate\n    - $\\frac{\\partial}{\\partial \\theta_j}J(\\theta_0, \\theta_1, \\theta_2)$ is the partial derivative of the cost function (error) over the $j^{th}$ parameter, this term will become zero when all parameters are optimized.\n    - each iteration we adjust $\\theta_j$ according to the derivative by the scale of the learning rate $\\alpha$\n        - when $\\alpha$ is too small, it can takes long time for the cost function value to decrease to minimal\n        - when $\\alpha$ is too big, the cost function value can pass the local minimal and swag around and fail to converge\n- gradient descent after taking derivative: $\\theta_j = \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^{m} [(h_\\theta (x^i) - y^i) x_j^i]$\n- gradient descent break down to each parameter: \\begin{align}\n \\theta_0 & = \\theta_0 - \\alpha \\frac{1}{k} \\sum_{i=1}^{k} (h_\\theta(x^{i}) - y^{i}) \\\\  \n\\theta_1 & = \\theta_1 - \\alpha \\frac{1}{k} \\sum_{i=1}^{k} ((h_\\theta(x^{i}) - y^{i}) \\cdot x_1^{i}) \\\\\n\\theta_2 & = \\theta_2 - \\alpha \\frac{1}{k} \\sum_{i=1}^{k} ((h_\\theta(x^{i}) - y^{i}) \\cdot x_2^{i}) \\\\\n\\end{align}\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# iterative implementation. This is more intuitive as we are simply mapping the gradient descent step by step\n\ndef cal_sum(h, x, y):\n    sum0 = sum1 = sum2 = 0\n    for i in range(len(y)):\n        sum0 += (h(x.iloc[i,:]) - y[i])\n        sum1 += (h(x.iloc[i,:]) - y[i]) * x.iloc[i,0]\n        sum2 += (h(x.iloc[i,:]) - y[i]) * x.iloc[i,1]\n    return sum0, sum1, sum2\n\ndef cal_cost(h, x, y):\n    j = 0\n    for i in range(len(x)):\n        j += (h(x.iloc[i,:]) - y[i]) ** 2\n    return j / (2 * len(y))\n\ndef lrdg_iterative(theta, x, y, lr, epochs):\n    theta_0, theta_1, theta_2 = theta\n    h = lambda x: theta_0 + theta_1 * x[0] + theta_2 * x[1] # hypothesis\n    paras, costs = [], []\n    for i in range(epochs):\n        sum0, sum1, sum2 = cal_sum(h, x, y[0])\n        theta_0 -= lr / len(y) * sum0\n        theta_1 -= lr / len(y) * sum1\n        theta_2 -= lr / len(y) * sum2\n        paras.append((theta_0, theta_1, theta_2))\n        costs.append(cal_cost(h, x, y[0]))\n    return paras, costs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"theta = [0,0,0]\nlr = 0.01 # learning rate\nepochs = 20\nit_paras, it_costs = lrdg_iterative(theta, x, y, lr, epochs)\nplt.plot(it_costs, 'go-', label='cost')\nplt.legend()\nplt.ylabel('Costs')\nplt.xlabel('Iterations')\nplt.title('Cost Function');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"theta = [0,0,0]\nlr = 0.15 # learning rate too big, and cost function will not converge\nepochs = 20\nbi_paras, bi_costs = lrdg_iterative(theta, x, y, lr, epochs)\nplt.plot(bi_costs, 'go-', label='cost')\nplt.legend()\nplt.ylabel('Costs')\nplt.xlabel('Iterations')\nplt.title('Cost Function');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%timeit _, _ = lrdg_iterative(theta, x, y, lr, epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# implementation with linear algebra\n# for quick review of linear algebra relavent to this part: https://www.holehouse.org/mlclass/03_Linear_algebra_review.html; https://www.youtube.com/watch?v=Dft1cqjwlXE&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=13&t=0s \ndef gradientDescent(X, y, theta, alpha, num_iters):\n    \"\"\"\n       Performs gradient descent to learn theta\n    \"\"\"\n    m = y[0].size  # number of training examples\n    paras, costs = [], []\n    for i in range(num_iters):\n        y_hat = np.dot(X, theta)\n        theta = theta - alpha * (1.0/m) * np.dot(X.T, y_hat-y)\n        paras.append(theta)\n        costs.append(cal_cost(lambda x: theta[0] + theta[1]*x[0] + theta[2]*x[1], x, y[0]))\n    return theta, paras, costs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.concat([pd.DataFrame(np.ones((len(y),1))),x], axis = 1) # adding constant term to each train data\ntheta = np.zeros((3,1))\nepochs = 20 # run 20 single step gd to get cost functions and parameters to compare with the non-linear-algebra approach\nlr = 0.01\np, la_paras, la_costs = gradientDescent(X, y, theta, lr, epochs)\nplt.plot(la_costs, 'go-', label='Lin-Alg')\nplt.plot(it_costs, 'r+', label='Iterative')\nplt.legend()\nplt.ylabel('Costs')\nplt.xlabel('Iterations')\nplt.title('Cost Function');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%timeit _, _, _ = gradientDescent(X, y, theta, lr, epochs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Implementation with linear algebra yields the same results while taking much less time as `numpy` optimized the calculation under the hood."},{"metadata":{},"cell_type":"markdown","source":"#### Normal Equation approach\n- hyphothesis $h_\\theta$: $h_\\theta = \\theta_0 \\cdot x[0] + \\theta_1 \\cdot x[1] + \\theta_1 \\cdot x[2]$\n- parameters $\\theta = (X^T X)^{-1} X^T y$"},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalEquation(X, y):\n    m = len(y)\n    theta = []\n    \n    # Calculating theta\n    theta = np.linalg.pinv(X.T.dot(X))  ### Please note using np.linalg.inv will sometime yield wrong outcomes\n    theta = theta.dot(X.T)\n    theta = theta.dot(y)\n\n    return theta","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ne_para = normalEquation(X,y)\nne_cost = cal_cost(lambda x: ne_para[0] + ne_para[1]*x[0] + ne_para[2]*x[1], x, y[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%timeit _ = normalEquation(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_df = pd.DataFrame([np.append(ne_cost, ne_para), \n                           np.append(it_costs[-1], it_paras[-1]), \n                           np.append(la_costs[-1], la_paras[-1])],\n                         index = ['Normal Equation', 'Iterative 20 It', 'Linear Algebra 20 It'],\n                         columns = ['Cost', 'theta_0', 'theta_1', 'theta_2'])\ncompare_df['timeit (ms)'] = pd.Series([1.22, 95.4, 67.7], index = compare_df.index)\ncompare_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apparently, the Normal Equation approach is outperforming the Gradient Descent approach in this small data set. As suggested by Andrew Ng, we should start considering Gradient Descent over the Normal Equation when training example > 10000 and calculation of inverse of the large matrix can be expensive.\n\n\n\nNow let's see the functions working on some real data:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/suicide-rates-overview-1985-to-2016/master.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.rename(columns={'suicides/100k pop': 'suicides_per_100k_pop',\n                  ' gdp_for_year ($) ': 'gdp_for_year',\n                  'gdp_per_capita ($)': 'gdp_per_capita'}, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_rate = df.groupby('country').suicides_per_100k_pop.mean().reset_index()\ncountry_gdp_cap = df.groupby('country').gdp_per_capita.mean().reset_index()\ncountry_pop = df.groupby('country').population.mean().reset_index()\nnew_df = pd.merge(country_rate, country_gdp_cap, on='country')\nnew_df = pd.merge(new_df, country_pop, on='country')\nnew_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df['gdp_per_capita'] /= 100000   # too large x values send the y values out of normal range and cause calculation errors\nnew_df['population'] /= 100000000\nnew_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = new_df[['gdp_per_capita', 'population']]\ny = pd.DataFrame(list(new_df.suicides_per_100k_pop))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"theta = [0,0,0]\nlr = 0.08 # learning rate\nepochs = 20\nit_paras, it_costs = lrdg_iterative(theta, x, y, lr, epochs)\nplt.plot(it_costs, 'go-', label='Iterative')\nplt.legend()\nplt.ylabel('Costs')\nplt.xlabel('Iterations')\nplt.title('Cost Function');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.concat([pd.DataFrame(np.ones((len(y[0]),1))),x], axis = 1) # adding constant term to each train data\ntheta = np.zeros((3,1))\nepochs = 20 # run 20 single step gd to get cost functions and parameters to compare with the non-linear-algebra approach\nlr = 0.08\np, la_paras, la_costs = gradientDescent(X, y, theta, lr, epochs)\nplt.plot(la_costs, 'go-', label='Lin-Alg')\nplt.plot(it_costs, 'r+', label='Iterative')\nplt.legend()\nplt.ylabel('Costs')\nplt.xlabel('Iterations')\nplt.title('Cost Function');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"costs_vs_lr = []\nfor i in range(10):\n    lr = 0.01 + i * 0.02\n    theta = np.zeros((3,1))\n    epochs = 20 # run 20 single step gd to get cost functions and parameters to compare with the non-linear-algebra approach\n    p, _paras, _costs = gradientDescent(X, y, theta, lr, epochs)\n    costs_vs_lr.append(_costs)\n\nfor i, costs in enumerate(costs_vs_lr):    \n    color = list(np.random.random(size=3))\n    plt.plot(range(20), costs, c = color, label='lr={:.2f}'.format(0.01 + i * 0.02))\nplt.ylabel('cost')\nplt.legend()\nplt.ylabel('Costs')\nplt.xlabel('Iterations')\nplt.title('Cost Function');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ne_para = normalEquation(X,y)\nne_cost = cal_cost(lambda x: ne_para[0] + ne_para[1]*x[0] + ne_para[2]*x[1], x, y[0])\nprint(ne_cost, it_costs[-1], la_costs[-1])\nprint(ne_para, it_paras[-1], la_paras[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlinReg = LinearRegression()\nreg = linReg.fit(x, y)\nsk_cost = cal_cost(lambda x: reg.intercept_ + reg.coef_[0][0] *x[0] + reg.coef_[0][1]*x[1], x, y[0])\nprint(reg.coef_, ne_para)\nprint(sk_cost, ne_cost)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%timeit _, _ = lrdg_iterative(theta, x, y, lr, epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%timeit _, _, _ = gradientDescent(X, y, theta, lr, epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%timeit _ = normalEquation(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%timeit _ = linReg.fit(x, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_df = pd.DataFrame([np.append(it_costs[-1], it_paras[-1]), \n                           np.append(la_costs[-1], la_paras[-1]), \n                           np.append(ne_cost, ne_para), \n                           np.append([sk_cost, reg.intercept_], reg.coef_[0])],\n                         index = ['Iterative 20 It', 'Linear Algebra 20 It', 'Normal Equation', 'SKLearn'],\n                         columns = ['Cost', 'theta_0', 'theta_1', 'theta_2'])\ncompare_df['timeit (ms)'] = pd.Series([2460, 647, 1.06, 2.27], index = compare_df.index)\ncompare_df","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}