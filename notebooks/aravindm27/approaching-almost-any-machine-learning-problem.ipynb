{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import datasets\nfrom sklearn import manifold\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = datasets.fetch_openml(\n'mnist_784',\nversion=1,\nreturn_X_y=True\n)\npixel_values, targets = data\ntargets = targets.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"single_image = pixel_values[1, :].reshape(28, 28)\nplt.imshow(single_image, cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = manifold.TSNE(n_components=2, random_state=42)\ntransformed_data = tsne.fit_transform(pixel_values[:3000, :])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne_df = pd.DataFrame(\nnp.column_stack((transformed_data, targets[:3000])),\ncolumns=[\"x\", \"y\", \"targets\"]\n)\ntsne_df.targets = tsne_df.targets.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = sns.FacetGrid(tsne_df, hue=\"targets\", size=8)\ngrid.map(plt.scatter, \"x\", \"y\").add_legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Cross Validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a mapping dictionary that maps the quality values from 0 to 5\nquality_mapping = {\n3: 0,\n4: 1,\n5: 2,\n6: 3,\n7: 4,\n8: 5\n}\n# you can use the map function of pandas with\n# any dictionary to convert the values in a given\n# column to values in the dictionary\ndf.loc[:, \"quality\"] = df.quality.map(quality_mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use sample with frac=1 to shuffle the dataframe\n# we reset the indices since they change after\n# shuffling the dataframe\ndf = df.sample(frac=1).reset_index(drop=True)\n# top 1000 rows are selected\n# for training\ndf_train = df.head(1000)\n# bottom 599 values are selected\n# for testing/validation\ndf_test = df.tail(599)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import from scikit-learn\nfrom sklearn import tree\nfrom sklearn import metrics\n# initialize decision tree classifier class\n# with a max_depth of 3\nclf = tree.DecisionTreeClassifier(max_depth=7)\n# choose the columns you want to train on\n# these are the features for the model\ncols = ['fixed acidity',\n'volatile acidity',\n'citric acid','residual sugar',\n'chlorides',\n'free sulfur dioxide',\n'total sulfur dioxide',\n'density',\n'pH',\n'sulphates',\n'alcohol']\n# train the model on the provided features\n# and mapped quality from before\nclf.fit(df_train[cols], df_train.quality)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting for different depth of the decision tree ****"},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate predictions on the training set\ntrain_predictions = clf.predict(df_train[cols])\n# generate predictions on the test set\ntest_predictions = clf.predict(df_test[cols])\n# calculate the accuracy of predictions on\n# training data set\ntrain_accuracy = metrics.accuracy_score(\ndf_train.quality, train_predictions\n)\n# calculate the accuracy of predictions on\n# test data set\ntest_accuracy = metrics.accuracy_score(\ndf_test.quality, test_predictions\n)\ntest_accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE: this code is written in a jupyter notebook\n# import scikit-learn tree and metrics\nfrom sklearn import tree\nfrom sklearn import metrics\n# import matplotlib and seaborn\n# for plotting\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# this is our global size of label text\n# on the plots\nmatplotlib.rc('xtick', labelsize=20)\nmatplotlib.rc('ytick', labelsize=20)\n# This line ensures that the plot is displayed\n# inside the notebook\n%matplotlib inline\n# initialize lists to store accuracies\n# for training and test data\n# we start with 50% accuracy\ntrain_accuracies = [0.5]\ntest_accuracies = [0.5]\n# iterate over a few depth values\nfor depth in range(1, 25):\n    # init the model\n    clf = tree.DecisionTreeClassifier(max_depth=depth)\n    # columns/features for training\n    # note that, this can be done outside\n    # the loop\n    cols = [\n    'fixed acidity',\n    'volatile acidity',\n    'citric acid',\n    'residual sugar',\n    'chlorides',\n    'free sulfur dioxide',\n    'total sulfur dioxide',\n    'density',\n    'pH',\n    'sulphates',\n    'alcohol'\n    ]\n    # fit the model on given features\n    clf.fit(df_train[cols], df_train.quality)\n    # create training & test predictions\n    train_predictions = clf.predict(df_train[cols])\n    test_predictions = clf.predict(df_test[cols])\n    # calculate training & test accuracies\n    train_accuracy = metrics.accuracy_score(\n    df_train.quality, train_predictions\n    )\n    test_accuracy = metrics.accuracy_score(\n    df_test.quality, test_predictions\n    )\n    # append accuracies\n    train_accuracies.append(train_accuracy)\n    test_accuracies.append(test_accuracy)\n# create two plots using matplotlib\n# and seaborn\nplt.figure(figsize=(10, 5))\nsns.set_style(\"whitegrid\")\nplt.plot(train_accuracies, label=\"train accuracy\")\nplt.plot(test_accuracies, label=\"test accuracy\")\nplt.legend(loc=\"upper left\", prop={'size': 15})\nplt.xticks(range(0, 26, 5))\nplt.xlabel(\"max_depth\", size=20)\nplt.ylabel(\"accuracy\", size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Saving Train and Test Data for further processing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.to_csv('train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import pandas and model_selection module of scikit-learn\nimport pandas as pd\nfrom sklearn import model_selection\nif __name__ == \"__main__\":\n    # Training data is in a csv file called train.csv\n    df = pd.read_csv(\"train.csv\")\n    # we create a new column called kfold and fill it with -1\n    df[\"kfold\"] = -1\n    # the next step is to randomize the rows of the data\n    df = df.sample(frac=1).reset_index(drop=True)\n    # fetch targets\n    y = df.quality.values\n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=5)\n    # fill the new kfold column\n    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n        df.loc[v_, 'kfold'] = f\n        # save the new csv with kfold column\n        df.to_csv(\"train_folds.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b = sns.countplot(x='quality', data=df)\nb.set_xlabel(\"quality\", fontsize=20)\nb.set_ylabel(\"count\", fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Stratified K-Fold**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn import model_selection\n\ndef create_folds(data):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n    \n    # calculate the number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = np.floor(1 + np.log2(len(data)))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(data[\"target\"], bins=int(num_bins), labels=False)\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=5)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n        \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n    # return dataframe with folds\n    return data\n\nif __name__ == \"__main__\":\n    # we create a sample dataset with 15000 samples\n    # and 100 features and 1 target\n    X, y = datasets.make_regression(n_samples=15000, n_features=100, n_targets=1)\n    \n    # create a dataframe out of our numpy arrays\n    df = pd.DataFrame(X,columns=[f\"f_{i}\" for i in range(X.shape[1])])\n    df.loc[:, \"target\"] = y\n    \n    # create folds\n    df = create_folds(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Metrics**"},{"metadata":{},"cell_type":"markdown","source":"**Accuracy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(y_true, y_pred):\n    \"\"\"\n    Function to calculate accuracy\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: accuracy score\n    \"\"\"\n    # initialize a simple counter for correct predictions\n    correct_counter = 0\n    \n    # loop over all elements of y_true\n    # and y_pred \"together\"\n    for yt, yp in zip(y_true, y_pred):\n        if yt == yp:\n            \n            # if prediction is equal to truth, increase the counter\n            correct_counter += 1\n            \n    # return accuracy\n    # which is correct predictions over the number of samples\n    return correct_counter / len(y_true)\n\nl1 = [0,1,1,1,0,0,0,1]\nl2 = [0,1,0,1,0,1,0,0]\nprint(accuracy(l1,l2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**OR**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nl1 = [0,1,1,1,0,0,0,1]\nl2 = [0,1,0,1,0,1,0,0]\nmetrics.accuracy_score(l1, l2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def true_positive(y_true, y_pred):\n    \"\"\"\n    Function to calculate True Positives\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: number of true positives\n    \"\"\"\n    # initialize\n    tp = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == yp:\n            tp += 1\n    return tp\n\n\ndef true_negative(y_true, y_pred):\n    \"\"\"\n    Function to calculate True Negatives\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: number of true negatives\n    \"\"\"\n    # initialize\n    tn = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == 0 and yp == 0:\n            tn += 1\n    return tn\n\n\ndef false_positive(y_true, y_pred):\n    \"\"\"\n    Function to calculate False Positives\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: number of false positives\n    \"\"\"\n    # initialize\n    fp = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == 0 and yp == 1:\n            fp += 1\n    return fp\n\n\ndef false_negative(y_true, y_pred):\n    \"\"\"\n    Function to calculate False Negatives\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: number of false negatives\n    \"\"\"\n    # initialize\n    fn = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == 1 and yp == 0:\n            fn += 1\n    return fn\n\nl1 = [0,1,1,1,0,0,0,1]\nl2 = [0,1,0,1,0,1,0,0]\nprint(\"TP: {}\".format(true_positive(l1, l2)))\n\nprint(\"FP: {}\".format(false_positive(l1, l2)))\n\nprint(\"FN: {}\".format(false_negative(l1, l2)))\n\nprint(\"TN: {}\".format(true_negative(l1, l2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Accuracy Score with Confusion Matrix**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy_v2(y_true, y_pred):\n    \"\"\"\n    Function to calculate accuracy using tp/tn/fp/fn\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: accuracy score\n    \"\"\"\n    tp = true_positive(y_true, y_pred)\n    fp = false_positive(y_true, y_pred)\n    fn = false_negative(y_true, y_pred)\n    tn = true_negative(y_true, y_pred)\n    accuracy_score = (tp + tn) / (tp + tn + fp + fn)\n    return accuracy_score\n\nprint(\"Accuracy: {}\".format(accuracy_v2(l1, l2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Precision**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def precision(y_true, y_pred):\n    \"\"\"\n    Function to calculate precision\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: precision score\n    \"\"\"\n    tp = true_positive(y_true, y_pred)\n    fp = false_positive(y_true, y_pred)\n    precision = tp / (tp + fp)\n    return precision\n\nprint(\"Precision: {}\".format(precision(l1, l2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Recall or Sensitivity**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def recall(y_true, y_pred):\n    \"\"\"\n    Function to calculate recall\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: recall score\n    \"\"\"\n    tp = true_positive(y_true, y_pred)\n    fn = false_negative(y_true, y_pred)\n    recall = tp / (tp + fn)\n    return recall\n\nprint(\"Recall: {}\".format(recall(l1, l2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Example for Precision Recall Curve**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\ny_pred = [0.02638412, 0.11114267, 0.31620708, 0.0490937, 0.0191491, 0.17554844, \\\n          0.15952202, 0.03819563, 0.11639273, 0.079377, 0.08584789, 0.39095342, \\\n          0.27259048, 0.03447096, 0.04644807, 0.03543574, 0.18521942, 0.05934905, 0.61977213, 0.33056815]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nprecisions = []\nrecalls = []\n\n# how we assumed these thresholds is a long story\nthresholds = [0.0490937 , 0.05934905, 0.079377, 0.08584789, 0.11114267, 0.11639273, 0.15952202, 0.17554844, 0.18521942, 0.27259048, 0.31620708, 0.33056815, 0.39095342, 0.61977213]\n\n# for every threshold, calculate predictions in binary\n# and append calculated precisions and recalls\n# to their respective lists\nfor i in thresholds:\n    temp_prediction = [1 if x >= i else 0 for x in y_pred]\n    p = precision(y_true, temp_prediction)\n    r = recall(y_true, temp_prediction)\n    precisions.append(p)\n    recalls.append(r)\n  \nplt.figure(figsize=(7, 7))\nplt.plot(recalls, precisions)\nplt.xlabel('Recall', fontsize=15)\nplt.ylabel('Precision', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**F1 Score**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def f1(y_true, y_pred):\n    \"\"\"\n    Function to calculate f1 score\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: f1 score\n    \"\"\"\n    p = precision(y_true, y_pred)\n    r = recall(y_true, y_pred)\n    score = 2 * p * r / (p + r)\n    return score\n\ny_true = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\ny_pred = [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\nprint(\"F1 Score: {}\".format(f1(y_true, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**OR**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nmetrics.f1_score(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Specificity or 1- FPR**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fpr(y_true, y_pred):\n    \"\"\"\n    Function to calculate fpr\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: fpr\n    \"\"\"\n    fp = false_positive(y_true, y_pred)\n    tn = true_negative(y_true, y_pred)\n    return fp / (tn + fp)\n\nprint(\"Specificity: {}\".format(1-f1(y_true, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TPR, FPR for different threshold**"},{"metadata":{"trusted":true},"cell_type":"code","source":"tpr_list = []\nfpr_list = []\n\n# actual targets\ny_true = [0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1]\n\n# predicted probabilities of a sample being 1\ny_pred = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05, 0.9, 0.5, 0.3, 0.66, 0.3, 0.2, 0.85, 0.15, 0.99]\n\n# handmade thresholds\nthresholds = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.99, 1.0]\n\n# loop over all thresholds\nfor thresh in thresholds:\n    # calculate predictions for a given threshold\n    temp_pred = [1 if x >= thresh else 0 for x in y_pred]\n    \n    # calculate tpr\n    temp_tpr = recall(y_true, temp_pred)\n    \n    # calculate fpr\n    temp_fpr = fpr(y_true, temp_pred)\n    \n    # append tpr and fpr to lists\n    tpr_list.append(temp_tpr)\n    fpr_list.append(temp_fpr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Receiver Operating Characteristics (ROC) Curve**\n\n**Plot of TPR FPR**\n\nUsed for Area under ROC curve to specific how good the model performs and how one class is predicted when compared to other"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7, 7))\nplt.fill_between(fpr_list, tpr_list, alpha=0.4)\nplt.plot(fpr_list, tpr_list, lw=3)\nplt.xlim(0, 1.0)\nplt.ylim(0, 1.0)\nplt.xlabel('FPR', fontsize=15)\nplt.ylabel('TPR', fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Log Loss**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ndef log_loss(y_true, y_proba):\n    \"\"\"\n    Function to calculate fpr\n    :param y_true: list of true values\n    :param y_proba: list of probabilities for 1\n    :return: overall log loss\n    \"\"\"\n    # define an epsilon value\n    # this can also be an input\n    # this value is used to clip probabilities\n    epsilon = 1e-15\n    \n    # initialize empty list to store\n    # individual losses\n    loss = []\n    \n    # loop over all true and predicted probability values\n    for yt, yp in zip(y_true, y_proba):\n        # adjust probability\n        # 0 gets converted to 1e-15\n        # 1 gets converted to 1-1e-15\n        yp = np.clip(yp, epsilon, 1 - epsilon)\n        \n        # calculate loss for one sample\n        temp_loss = - 1.0 * (\n        yt * np.log(yp) + (1 - yt) * np.log(1 - yp))\n        \n        # add to loss list\n        loss.append(temp_loss)\n        \n    # return mean loss over all samples\n    return np.mean(loss)\n\ny_true = [0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1]\ny_proba = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05, 0.9, 0.5, 0.3, 0.66, 0.3, 0.2, 0.85, 0.15, 0.99]\nprint(\"Log Loss: {}\".format(log_loss(y_true, y_proba)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**OR**"},{"metadata":{"trusted":true},"cell_type":"code","source":" from sklearn import metrics\nmetrics.log_loss(y_true, y_proba)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Weighted Precision"},{"metadata":{},"cell_type":"markdown","source":"**Macro Averaged Precision** : Calculate precision for all classes individually and then average them"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n\ndef macro_precision(y_true, y_pred):\n    \"\"\"\n    Function to calculate macro averaged precision\n    :param y_true: list of true values\n    :param y_proba: list of predicted values\n    :return: macro precision score\n    \"\"\"\n    # find the number of classes by taking\n    # length of unique values in true list\n    num_classes = len(np.unique(y_true))\n    \n    # initialize precision to 0\n    precision = 0\n    \n    # loop over all classes\n    for class_ in range(num_classes):\n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        # calculate true positive for current class\n        tp = true_positive(temp_true, temp_pred)\n        \n        # calculate false positive for current class\n        fp = false_positive(temp_true, temp_pred)\n        \n        # calculate precision for current class\n        temp_precision = tp / (tp + fp)\n        \n        # keep adding precision for all classes\n        precision += temp_precision\n        \n    # calculate and return average precision over all classes\n    precision /= num_classes\n    return precision\n\ny_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\ny_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n\nprint(\"Macro Precision: {}\".format(macro_precision(y_true, y_pred)))\n\nmetrics.precision_score(y_true, y_pred, average=\"macro\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Micro Averaged Precision** : Calculate class wise true positive and false positive and then use that to calculate overall precision"},{"metadata":{"trusted":true},"cell_type":"code","source":"def micro_precision(y_true, y_pred):\n    \"\"\"\n    Function to calculate micro averaged precision\n    :param y_true: list of true values\n    :param y_proba: list of predicted values\n    :return: micro precision score\n    \"\"\"\n    # find the number of classes by taking\n    # length of unique values in true list\n    num_classes = len(np.unique(y_true))\n    \n    # initialize tp and fp to 0\n    tp = 0\n    fp = 0\n    \n    # loop over all classes\n    for class_ in range(num_classes):\n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        # calculate true positive for current class\n        # and update overall tp\n        tp += true_positive(temp_true, temp_pred)\n        \n        # calculate false positive for current class\n        # and update overall tp\n        fp += false_positive(temp_true, temp_pred)\n        \n    # calculate and return overall precision\n    precision = tp / (tp + fp)\n    return precision\n\ny_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\ny_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n\nprint(\"Micro Weighted Precision: {}\".format(micro_precision(y_true, y_pred)))\n\nmetrics.precision_score(y_true, y_pred, average=\"micro\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Weighted Average Precision:** Similar to Macro Average but with weights to each class given as per the number of elements in each class"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\n\ndef weighted_precision(y_true, y_pred):\n    \"\"\"\n    Function to calculate weighted averaged precision\n    :param y_true: list of true values\n    :param y_proba: list of predicted values\n    :return: weighted precision score\n    \"\"\"\n    # find the number of classes by taking\n    # length of unique values in true list\n    num_classes = len(np.unique(y_true))\n    \n    # create class:sample count dictionary\n    # it looks something like this:\n    # {0: 20, 1:15, 2:21}\n    class_counts = Counter(y_true)\n    \n    # initialize precision to 0\n    precision = 0\n    \n    # loop over all classes\n    for class_ in range(num_classes):\n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        # calculate tp and fp for class\n        tp = true_positive(temp_true, temp_pred)\n        fp = false_positive(temp_true, temp_pred)\n        \n        # calculate precision of class\n        temp_precision = tp / (tp + fp)\n        \n        # multiply precision with count of samples in class\n        weighted_precision = class_counts[class_] * temp_precision\n        \n        # add to overall precision\n        precision += weighted_precision\n    # calculate overall precision by dividing by\n    # total number of samples\n    overall_precision = precision / len(y_true)\n    return overall_precision\n\nprint(\"Weighted Average Precisionj: {}\".format(weighted_precision(y_true, y_pred)))\n\nmetrics.precision_score(y_true, y_pred, average=\"weighted\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Weighted Recall"},{"metadata":{},"cell_type":"markdown","source":"**Macro Averaged Recall** : Calculate recall for all classes individually and then average them"},{"metadata":{"trusted":true},"cell_type":"code","source":"def macro_recall(y_true, y_pred):\n    \"\"\"\n    Function to calculate macro averaged recall\n    :param y_true: list of true values\n    :param y_proba: list of predicted values\n    :return: macro precision score\n    \"\"\"\n    # find the number of classes by taking\n    # length of unique values in true list\n    num_classes = len(np.unique(y_true))\n    \n    # initialize precision to 0\n    recall = 0\n    \n    # loop over all classes\n    for class_ in range(num_classes):\n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        # calculate true positive for current class\n        tp = true_positive(temp_true, temp_pred)\n        \n        # calculate false positive for current class\n        fn = false_negative(temp_true, temp_pred)\n        \n        # calculate recall for current class\n        temp_recall = tp / (tp + fn)\n        \n        # keep adding precision for all classes\n        recall += temp_recall\n        \n    # calculate and return average recall over all classes\n    recall /= num_classes\n    return recall\n\ny_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\ny_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n\nprint(\"Macro Recall: {}\".format(macro_recall(y_true, y_pred)))\n\nmetrics.recall_score(y_true, y_pred, average=\"macro\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Micro Weighted Recall**:  Calculate class wise true positive and false positive and then use that to calculate overall recall"},{"metadata":{"trusted":true},"cell_type":"code","source":"def micro_recall(y_true, y_pred):\n    \"\"\"\n    Function to calculate micro averaged recall\n    :param y_true: list of true values\n    :param y_proba: list of predicted values\n    :return: micro recall score\n    \"\"\"\n    # find the number of classes by taking\n    # length of unique values in true list\n    num_classes = len(np.unique(y_true))\n    \n    # initialize tp and fp to 0\n    tp = 0\n    fn = 0\n    \n    # loop over all classes\n    for class_ in range(num_classes):\n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        # calculate true positive for current class\n        # and update overall tp\n        tp += true_positive(temp_true, temp_pred)\n        \n        # calculate false positive for current class\n        # and update overall tp\n        fn += false_negative(temp_true, temp_pred)\n        \n    # calculate and return overall recall\n    recall = tp / (tp + fn)\n    return recall\n\ny_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\ny_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n\nprint(\"Micro Weighted Recall: {}\".format(micro_recall(y_true, y_pred)))\n\nmetrics.recall_score(y_true, y_pred, average=\"micro\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Weighted Average Recall**: Similar to Macro Average but with weights to each class given as per the number of elements in each class"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\n\ndef weighted_recall(y_true, y_pred):\n    \"\"\"\n    Function to calculate weighted averaged precision\n    :param y_true: list of true values\n    :param y_proba: list of predicted values\n    :return: weighted recall score\n    \"\"\"\n    # find the number of classes by taking\n    # length of unique values in true list\n    num_classes = len(np.unique(y_true))\n    \n    # create class:sample count dictionary\n    # it looks something like this:\n    # {0: 20, 1:15, 2:21}\n    class_counts = Counter(y_true)\n    \n    # initialize recall to 0\n    recall = 0\n    \n    # loop over all classes\n    for class_ in range(num_classes):\n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        # calculate tp and fp for class\n        tp = true_positive(temp_true, temp_pred)\n        fn = false_negative(temp_true, temp_pred)\n        \n        # calculate precision of class\n        temp_recall = tp / (tp + fn)\n        \n        # multiply precision with count of samples in class\n        weighted_recall = class_counts[class_] * temp_recall\n        \n        # add to overall precision\n        recall += weighted_recall\n    # calculate overall precision by dividing by\n    # total number of samples\n    overall_recall = recall / len(y_true)\n    return overall_recall\n\ny_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\ny_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n\nprint(\"Weighted Average Recall: {}\".format(weighted_recall(y_true, y_pred)))\n\nmetrics.recall_score(y_true, y_pred, average=\"weighted\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Weighted Average F1 Score**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def weighted_f1(y_true, y_pred):\n    \"\"\"\n    Function to calculate weighted f1 score\n    :param y_true: list of true values\n    :param y_proba: list of predicted values\n    :return: weighted f1 score\n    \"\"\"\n    # find the number of classes by taking\n    # length of unique values in true list\n    num_classes = len(np.unique(y_true))\n    \n    # create class:sample count dictionary\n    # it looks something like this:\n    # {0: 20, 1:15, 2:21}\n    class_counts = Counter(y_true)\n    \n    # initialize f1 to 0\n    f1 = 0\n    \n    # loop over all classes\n    for class_ in range(num_classes):\n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        # calculate precision and recall for class\n        p = precision(temp_true, temp_pred)\n        r = recall(temp_true, temp_pred)\n        \n        # calculate f1 of class\n        if p + r != 0:\n            temp_f1 = 2 * p * r / (p + r)\n        else:\n            temp_f1 = 0\n            \n        # multiply f1 with count of samples in class\n        weighted_f1 = class_counts[class_] * temp_f1\n        \n        # add to f1 precision\n        f1 += weighted_f1\n        \n    # calculate overall F1 by dividing by\n    # total number of samples\n    overall_f1 = f1 / len(y_true)\n    return overall_f1\n\n\ny_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\ny_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n\nprint(\"Weighted Average F1 Score: {}\".format(weighted_f1(y_true, y_pred)))\n\nmetrics.f1_score(y_true, y_pred, average=\"weighted\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\n\n# some targets\ny_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\n\n#some predictions\ny_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n\n# get confusion matrix from sklearn\ncm = metrics.confusion_matrix(y_true, y_pred)\n\n# plot using matplotlib and seaborn\nplt.figure(figsize=(10, 10))\ncmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\nsns.set(font_scale=2.5)\nsns.heatmap(cm, annot=True, cmap=cmap, cbar=False)\nplt.ylabel('Actual Labels', fontsize=20)\nplt.xlabel('Predicted Labels', fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Precision at K**\n\n**Average Precision at K**\n\n**Mean Average Precision at K**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pk(y_true, y_pred, k):\n    \"\"\"\n    This function calculates precision at k\n    for a single sample\n    :param y_true: list of values, actual classes\n    :param y_pred: list of values, predicted classes\n    :return: precision at a given value k\n    \"\"\"\n    # if k is 0, return 0. we should never have this\n    # as k is always >= 1\n    if k == 0:\n        return 0\n    \n    # we are interested only in top-k predictions\n    y_pred = y_pred[:k]\n    \n    # convert predictions to set\n    pred_set = set(y_pred)\n    \n    # convert actual values to set\n    true_set = set(y_true)\n    \n    # find common values\n    common_values = pred_set.intersection(true_set)\n    \n    # return length of common values over k\n    return len(common_values) / len(y_pred[:k])\n\n\ndef apk(y_true, y_pred, k):\n    \"\"\"\n    This function calculates average precision at k\n    for a single sample\n    :param y_true: list of values, actual classes\n    :param y_pred: list of values, predicted classes\n    :return: average precision at a given value k\n    \"\"\"\n    # initialize p@k list of values\n    pk_values = []\n    \n    # loop over all k. from 1 to k + 1\n    for i in range(1, k + 1):\n        # calculate p@i and append to list\n        pk_values.append(pk(y_true, y_pred, i))\n        \n    # if we have no values in the list, return 0\n    if len(pk_values) == 0:\n        return 0\n    \n    # else, we return the sum of list over length of list\n    return sum(pk_values) / len(pk_values)\n\n\ndef mapk(y_true, y_pred, k):\n    \"\"\"\n    This function calculates mean avg precision at k\n    for a single sample\n    :param y_true: list of values, actual classes\n    :param y_pred: list of values, predicted classes\n    :return: mean avg precision at a given value k\n    \"\"\"\n    # initialize empty list for apk values\n    apk_values = []\n    \n    # loop over all samples\n    for i in range(len(y_true)):\n        # store apk values for every sample\n        apk_values.append(apk(y_true[i], y_pred[i], k=k))\n        \n    # return mean of apk values list\n    return sum(apk_values) / len(apk_values)\n\ny_true = [[1, 2, 3],[0, 2],[1],[2, 3],[1, 0],[]]\ny_pred = [[0, 1, 2],[1],[0, 2, 3],[2, 3, 4, 0],[0, 1, 2],[0]]\n\n\nfor i in range(len(y_true)):\n    for j in range(1, 4):\n        print(f\"\"\"\n        y_true={y_true[i]},\n        y_pred={y_pred[i]},\n        P@{j}={pk(y_true[i], y_pred[i], k=j)},\n        AP@{j}={apk(y_true[i], y_pred[i], k=j)}\n        \"\"\")\n        \nfor i in range(1,4):\n    print(f\"\"\"MAP@{i}={mapk(y_true, y_pred, k=i)}\"\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Mean Absolute Error** = Abs ( True Value – Predicted Value )\n\n**Squared Error**= = ( True Value – Predicted Value )^2\n\n**RMSE** = SQRT ( MSE )\n\n**Percentage Error** = ( ( True Value – Predicted Value ) / True Value ) * 100\n\n**Mean Absolute Percentage Error or MAPE** =  Abs( ( True Value – Predicted Value ) / True Value ) * 100"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_absolute_error(y_true, y_pred):\n    \"\"\"\n    This function calculates mae\n    :param y_true: list of real numbers, true values\n    :param y_pred: list of real numbers, predicted values\n    :return: mean absolute error\n    \"\"\"\n    # initialize error at 0\n    error = 0\n    \n    # loop over all samples in the true and predicted list\n    for yt, yp in zip(y_true, y_pred):\n        # calculate absolute error\n        # and add to error\n        error += np.abs(yt - yp)\n        \n    # return mean error\n    return error / len(y_true)\n\n\ndef mean_squared_error(y_true, y_pred):\n    \"\"\"\n    This function calculates mse\n    :param y_true: list of real numbers, true values\n    :param y_pred: list of real numbers, predicted values\n    :return: mean squared error\n    \"\"\"\n    # initialize error at 0\n    error = 0\n    \n    # loop over all samples in the true and predicted list\n    for yt, yp in zip(y_true, y_pred):\n        # calculate squared error\n        # and add to error\n        error += (yt - yp) ** 2\n        \n    # return mean error\n    return error / len(y_true)\n\n\ndef mean_squared_log_error(y_true, y_pred):\n    \"\"\"\n    This function calculates msle\n    :param y_true: list of real numbers, true values\n    :param y_pred: list of real numbers, predicted values\n    :return: mean squared logarithmic error\n    \"\"\"\n    # initialize error at 0\n    error = 0\n    \n    # loop over all samples in true and predicted list\n    for yt, yp in zip(y_true, y_pred):\n        # calculate squared log error\n        # and add to error\n        error += (np.log(1 + yt) - np.log(1 + yp)) ** 2\n        \n    # return mean error\n    return error / len(y_true)\n\n\ndef mean_percentage_error(y_true, y_pred):\n    \"\"\"\n    This function calculates mpe\n    :param y_true: list of real numbers, true values\n    :param y_pred: list of real numbers, predicted values\n    :return: mean percentage error\n    \"\"\"\n    # initialize error at 0\n    error = 0\n    \n    # loop over all samples in true and predicted list\n    for yt, yp in zip(y_true, y_pred):\n        # calculate percentage error\n        # and add to error\n        # Percentage error undefined if true value is zero:\n        if yt != 0:\n            error += (yt - yp) / yt\n        \n    # return mean percentage error\n    return error / len(y_true)\n\n\ndef mean_abs_percentage_error(y_true, y_pred):\n    \"\"\"\n    This function calculates MAPE\n    :param y_true: list of real numbers, true values\n    :param y_pred: list of real numbers, predicted values\n    :return: mean absolute percentage error\n    \"\"\"\n    # initialize error at 0\n    error = 0\n    \n    # loop over all samples in true and predicted list\n    for yt, yp in zip(y_true, y_pred):\n        # calculate percentage error\n        # and add to error\n        \n        # Percentage error undefined if true value is zero:\n        if yt != 0: \n            error += np.abs(yt - yp) / yt\n        \n    # return mean percentage error\n    return error / len(y_true)\n\n\n# some targets\ny_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\n\n#some predictions\ny_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n\nprint(f\"\"\"\nMean Absolute Error: {mean_absolute_error(y_true, y_pred)}\nMean Squared Error: {mean_squared_error(y_true, y_pred)}\nMean Percentage Error: {mean_percentage_error(y_true, y_pred)}\nMean Absolute Percentage Error: {mean_abs_percentage_error(y_true, y_pred)}\n\"\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Coefficient of Determination R^2**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def r2(y_true, y_pred):\n    \"\"\"\n    This function calculates r-squared score\n    :param y_true: list of real numbers, true values\n    :param y_pred: list of real numbers, predicted values\n    :return: r2 score\n    \"\"\"\n    # calculate the mean value of true values\n    mean_true_value = np.mean(y_true)\n    \n    # initialize numerator with 0\n    numerator = 0\n    \n    # initialize denominator with 0\n    denominator = 0\n    \n    # loop over all true and predicted values\n    for yt, yp in zip(y_true, y_pred):\n        # update numerator\n        numerator += (yt - yp) ** 2\n        \n        # update denominator\n        denominator += (yt - mean_true_value) ** 2\n        \n    # calculate the ratio\n    ratio = numerator / denominator\n    \n    # return 1 - ratio\n    return 1 - ratio\n\n\n# some targets\ny_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\n\n#some predictions\ny_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n\nprint(f\"\"\"R2: {r2(y_true, y_pred)}\"\"\")\n\nmetrics.r2_score(y_true,y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Cohen's Kappa**"},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.cohen_kappa_score(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Matthew’s Correlation Coefficient (MCC)** = MCC ranges from -1 to 1. 1 is perfect prediction, -1 is imperfect prediction, and 0 is random prediction. The formula for MCC is quite simple."},{"metadata":{"trusted":true},"cell_type":"code","source":"def mcc(y_true, y_pred):\n    \"\"\"\n    This function calculates Matthew's Correlation Coefficient\n    for binary classification.\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: mcc score\n    \"\"\"\n    CM = metrics.confusion_matrix(y_true, y_pred)\n    \n    print(cm)\n    tp = CM[0][0]\n    fn = CM[0][1] + CM[0][2]\n    tn = CM[1][1] + CM[1][2] + CM[2][1] + CM[2][2]\n    fp = CM[1][0] + CM[2][0]\n    print(tp,tn,fp,fn)\n    \n    numerator = (tp * tn) - (fp * fn)\n    \n    denominator = ((tp + fp) * (fn + tn) * (fp + tn) * (tp + fn))\n    denominator = denominator ** 0.5\n    \n    return numerator/denominator\n\n# some targets\ny_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\n\n#some predictions\ny_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n\nprint(f\"\"\"MCC: {mcc(y_true, y_pred)}\"\"\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}