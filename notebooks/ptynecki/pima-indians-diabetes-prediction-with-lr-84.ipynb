{"cells":[{"metadata":{"_uuid":"e726fdf52ebf16ccb3c518b4f7ab1cf14d2a526f"},"cell_type":"markdown","source":"# Predict diabetes diagnosis for Pima Female Indians with Logistic Regression\n\n### Author\nPiotr Tynecki  \nLast edition: March 29, 2018\n\n### About the Pima Indian Diabetes Dataset\nThe Pima Indian Diabetes Dataset consists of information on 768 of women population: 268 tested positive and 500 tested negative instances coming from a population near Phoenix, Arizona, USA. Tested positive and tested negative indicates whether the patient is diabetic or not, respectively. Each instance is comprised of 8 attributes, which are all numeric. These data contain personal health data as well as results from medical examinations.\n\nThe detailed attributes in the dataset are listed below:\n\n* Number of times pregnant (*Pregnancies*)\n* Plasma glucose concentration at 2h in an oral glucose tolerance test (*Glucose*)\n* Diastolic blood pressure (*BloodPressure*)\n* Triceps skin fold thickness (*SkinThickness*)\n* 2-h serum insulin (*Insulin*)\n* Body Mass Index (*BMI*)\n* Diabetes pedigree function (*DiabetesPedigreeFunction*)\n* Age (*Age*)\n* Class variable (*Outcome*)"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e932e9dcd37e8a028a853a1c4c0c81a7572ca7d7"},"cell_type":"code","source":"import operator\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score, classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import RFECV","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"10a841dc09fd02bdba1c3eb4ba50559a4baea54e"},"cell_type":"markdown","source":"### Step 1: Exploratory Data Analysis (EDA)\nEDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. It let us to summarize data main characteristics."},{"metadata":{"trusted":false,"_uuid":"489102148db619bb5a3001e4fa1817f08b2d130d"},"cell_type":"code","source":"diabetes = pd.read_csv('../input/diabetes.csv')\ndiabetes.head()","execution_count":2,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1a9ebbce07534bca456b6a5adb3834ff41e68ed4"},"cell_type":"code","source":"diabetes.info()","execution_count":3,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8c630be43dec0bb1cd6eff502a6fb452ec72b17a"},"cell_type":"code","source":"diabetes.shape","execution_count":4,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c1e2eecfc3c6d4d4aa61a0343b4ae914362dbbf2"},"cell_type":"code","source":"diabetes.describe()","execution_count":5,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5f4f3c356304f0c4cd458a701c2373c8d283bdab"},"cell_type":"code","source":"diabetes.groupby('Outcome').size()","execution_count":6,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"423bfc21b12c4480d172da81ef3fa3ed2970bd14"},"cell_type":"code","source":"# Detailed distribution of the features in the dataset\nsns.pairplot(data=diabetes, hue='Outcome')\nplt.show()","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"4060d6af164d3eb4b30900288cd4531b58b0eb77"},"cell_type":"markdown","source":"#### Data quality checks"},{"metadata":{"trusted":false,"_uuid":"9a97007f3626451702bc9666e71c657a9f6d36d6"},"cell_type":"code","source":"diabetes.isnull().sum()","execution_count":8,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"faaa8747c5c169ffd0874634b828aad8e90379e2"},"cell_type":"code","source":"# Display how many 0 value each feature have\nfor field in diabetes.columns[:8]:\n    print('Number of 0-entries for \"{field_name}\" feature: {amount}'.format(\n        field_name=field,\n        amount=np.count_nonzero(diabetes[field] == 0)\n    ))","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"5fdb7116ec41a2b15fdffe19e251ee64277b926b"},"cell_type":"markdown","source":"We could replace 0 values to mean values for each features (excluding Pregnancies field) but it has bad effect on metrics (accuracy, F1-score) at least. So, that's why the code below is commented."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ea8456ff95c9da08d7144f8eb0d6b245f959c556"},"cell_type":"code","source":"# features_with_zeros = diabetes.columns[1:-1]\n    \n# diabetes[features_with_zeros] = diabetes[features_with_zeros].replace(0, np.nan)\n# diabetes[features_with_zeros] = diabetes[features_with_zeros].fillna(diabetes.mean())","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"6c8f079a13c2ca09bcc5ed0c7a446a2238b31860"},"cell_type":"markdown","source":"### Step 2: Feature Engineering"},{"metadata":{"trusted":false,"_uuid":"058b81d0ed014810268374a70154a50f46da3d17"},"cell_type":"code","source":"feature_names = diabetes.columns[:8]\nfeature_names","execution_count":11,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2d2fd86271ce2ac069a4f2d49659a852bcc1c407"},"cell_type":"code","source":"X = diabetes[feature_names]\ny = diabetes.Outcome","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"2b3adecc776b8a50c6668f264f2147d82dbdfc08"},"cell_type":"markdown","source":"#### Correlation Matrix\nA matrix of correlations provides useful insight into relationships between pairs of variables."},{"metadata":{"trusted":false,"_uuid":"ef840bc75039b0fef5a8f5a1f026f9b3d8a01e04"},"cell_type":"code","source":"sns.heatmap(\n    data=X.corr(),\n    annot=True,\n    fmt='.2f',\n    cmap='RdYlGn'\n)\n\nfig = plt.gcf()\nfig.set_size_inches(10, 8)\n\nplt.show()","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"619d66ee3da9af75b33da3f7a240c690f4dccddc"},"cell_type":"markdown","source":"#### Recursive Feature Elimination with Cross Validation\nThe goal of [Recursive Feature Elimination (RFE)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html) is to select features by feature ranking with recursive feature elimination.\n\nFor more confidence of features selection I used K-Fold Cross Validation with [Stratified k-fold](http://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold)."},{"metadata":{"trusted":false,"_uuid":"2483e0a5a7522ed5b593ea08d50278d78aa92ae2"},"cell_type":"code","source":"# I temporarily removed a few Glucose, BloodPressure and BMI rows with 0 values for better RFE result\ndiabetes_mod = diabetes[(diabetes.BloodPressure != 0) & (diabetes.BMI != 0) & (diabetes.Glucose != 0)]\ndiabetes_mod.shape","execution_count":14,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e42b756c00a7cfe0bd2e6d319fdd8c8c31fc2850"},"cell_type":"code","source":"X_mod = diabetes_mod[feature_names]\ny_mod = diabetes_mod.Outcome\n\nstrat_k_fold = StratifiedKFold(\n    n_splits=10,\n    random_state=42\n)\n\nlogreg_model = LogisticRegression()\n\nrfecv = RFECV(\n    estimator=logreg_model,\n    step=1,\n    cv=strat_k_fold,\n    scoring='accuracy'\n)\nrfecv.fit(X_mod, y_mod)\n\nplt.figure()\nplt.title('RFE with Logistic Regression')\nplt.xlabel('Number of selected features')\nplt.ylabel('10-fold Crossvalidation')\n\n# grid_scores_ returns a list of accuracy scores\n# for each of the features selected\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n\nplt.show()\n\nprint('rfecv.grid_scores_: {grid_scores}'.format(grid_scores=rfecv.grid_scores_))\n\n# support_ is another attribute to find out the features\n# which contribute the most to predicting\nnew_features = list(filter(\n    lambda x: x[1],\n    zip(feature_names, rfecv.support_)\n))\n\nprint('rfecv.support_: {support}'.format(support=rfecv.support_))\n\n# Features are the most suitable for predicting the response class\nnew_features = list(map(operator.itemgetter(0), new_features))\nprint('\\nThe most suitable features for prediction: {new_features}'.format(new_features=new_features))","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"4cb36e8cbf7ad41d4752ce6a4f24fa08c2ee100f"},"cell_type":"markdown","source":"### Step 3: Data standardization\n[Standardize features](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) by removing the mean and scaling to unit variance."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"86ce2f51daef0663937d689833cd8b311fd8f349"},"cell_type":"code","source":"# Features chosen based on RFECV result\nbest_features = [\n    'Pregnancies', 'Glucose', 'BMI', 'DiabetesPedigreeFunction'\n]\n\nX = StandardScaler().fit_transform(X[best_features])","execution_count":16,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7c4d0f2c6310bf8e5092edb04ded1d2fe246c60a"},"cell_type":"code","source":"# Split your data into training and testing (80% / 20%)\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    random_state=42,\n    test_size=0.20\n)","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"7b700e2c9c05037ea2c15a5855589a6f7ae3a91e"},"cell_type":"markdown","source":"#### Principal component analysis (PCA)\nThe main goal of a PCA analysis is to identify patterns in data. PCA aims to detect the correlation between variables. If a strong correlation between variables exists, the attempt to reduce the dimensionality only makes sense."},{"metadata":{"trusted":false,"_uuid":"f74d596d1991301112224a54da196b9e3f162c6d"},"cell_type":"code","source":"pca = PCA(n_components=2)\n\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n\nprint(pca.explained_variance_ratio_)\nprint('PCA sum: {:.2f}%'.format(sum(pca.explained_variance_ratio_) * 100))","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"ca26aec41711b46ecb8d42b421c5fe0b05b05e27"},"cell_type":"markdown","source":"### Step 4: Evaluating the performance of Logistic Regression model\nFor this case study I decided to use [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier as a beginning of my Data Science trip.\n\nModel Parameter Tuning with [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) returns the set of parameters which have an imperceptible impact on model evaluation.  \n  \nYou can check it by yourself:"},{"metadata":{"trusted":false,"_uuid":"bc2e6d4d04d5c804fe5fb0e508f80a2fd32075ab"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nc_values = list(np.arange(1, 100))\n\nparam_grid = [\n    {\n        'C': c_values,\n        'penalty': ['l1'],\n        'solver': ['liblinear'],\n        'multi_class': ['ovr'],\n        'random_state': [42]\n    },\n    {\n        'C': c_values,\n        'penalty': ['l2'],\n        'solver': ['liblinear', 'newton-cg', 'lbfgs'],\n        'multi_class': ['ovr'],\n        'random_state': [42]\n    }\n]\n\ngrid = GridSearchCV(\n    LogisticRegression(),\n    param_grid,\n    cv=strat_k_fold,\n    scoring='f1'\n)\ngrid.fit(X, y)\n\n# Best LogisticRegression parameters\nprint(grid.best_params_)\n# Best score for LogisticRegression with best parameters\nprint('Best score: {:.2f}%'.format(grid.best_score_ * 100))","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"2d0c61e7bcd216fec451bd1c0a67a7c4783839df"},"cell_type":"markdown","source":"#### Model learning"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3fe1ec122e452c9bd01e1199d96f41d31ca99054"},"cell_type":"code","source":"log_reg = LogisticRegression(\n    # Parameters chosen based on GridSearchCV result\n    C=1,\n    multi_class='ovr',\n    penalty='l2',\n    solver='newton-cg',\n    random_state=42\n)\nlog_reg.fit(X_train, y_train)\n\nlog_reg_predict = log_reg.predict(X_test)\nlog_reg_predict_proba = log_reg.predict_proba(X_test)[:, 1]","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"6fbb907fc23b64f322dfa4a586155779418b203e"},"cell_type":"markdown","source":"#### Model evaluation"},{"metadata":{"trusted":false,"_uuid":"b4e8945779291d9731c410fc4441752a5c604108"},"cell_type":"code","source":"print('Accuracy: {:.2f}%'.format(accuracy_score(y_test, log_reg_predict) * 100))\nprint('AUC: {:.2f}%'.format(roc_auc_score(y_test, log_reg_predict_proba) * 100))\nprint('Classification report:\\n\\n', classification_report(y_test, log_reg_predict))\nprint('Training set score: {:.2f}%'.format(log_reg.score(X_train, y_train) * 100))\nprint('Testing set score: {:.2f}%'.format(log_reg.score(X_test, y_test) * 100))","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"6e4d5fc8685a8f40b9c1f5d395a208b98e9c1d5e"},"cell_type":"markdown","source":"#### Confusion Matrix\n\nAlso known as an Error Matrix, is a specific table layout that allows visualization of the performance of an algorithm. The table have two rows and two columns that reports the number of False Positives (FP), False Negatives (FN), True Positives (TP) and True Negatives (TN). This allows more detailed analysis than accuracy."},{"metadata":{"trusted":false,"_uuid":"8fc5b514f0f90a031c3f681161cf8c2a50ba2b85"},"cell_type":"code","source":"outcome_labels = sorted(diabetes.Outcome.unique())\n\nsns.heatmap(\n    confusion_matrix(y_test, log_reg_predict),\n    annot=True,\n    xticklabels=outcome_labels,\n    yticklabels=outcome_labels\n)","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"dc50d1ebb7c0431b056aec432cf43f8008cdf10b"},"cell_type":"markdown","source":"#### Receiver Operating Characteristic (ROC)\n\n[ROC curve](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied."},{"metadata":{"trusted":false,"_uuid":"13061c8994297dc30d4c9af60100da7cd62d35d3"},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, log_reg_predict_proba)\n\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for diabetes classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"81662e36fb01953bbe93fb829183a1e5702e0145"},"cell_type":"markdown","source":"#### F1-score after 10-fold cross-validation"},{"metadata":{"trusted":false,"_uuid":"4f631f5e573e3694d8f704109be700a07227de28"},"cell_type":"code","source":"strat_k_fold = StratifiedKFold(\n    n_splits=10,\n    random_state=42\n)\n\nX_pca = pca.transform(X)\n\nfe_score = cross_val_score(\n    log_reg,\n    X_pca,\n    y,\n    cv=strat_k_fold,\n    scoring='f1'\n)\n\nprint(\"F1 after 10-fold cross-validation: {:.2f}% (+/- {:.2f}%)\".format(\n    fe_score.mean() * 100,\n    fe_score.std() * 2\n))","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"dd4e6efe91a4b29468c6d5df763647d76079dce4"},"cell_type":"markdown","source":"### Final step: Conclusions\n\nAfter the application of data standardization, Recursive Feature Elimination (RFE) and Principal Component Analysis (PCA) I achieved the following results:\n\n* Accuracy: ~84%\n* F1-score: 83%\n* Precision: 84%\n* Recall: 84%\n\nFrom my observations it could be one of the highest score for Logistic Regression in Kaggle but the results from F1-score after 10-fold cross-validation really bugging me.\n\nI would love to knows your comments and other tuning proposals for that study case."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}