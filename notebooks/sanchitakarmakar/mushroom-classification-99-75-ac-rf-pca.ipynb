{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the dataset\n\ndataset = pd.read_csv('../input/mushroom-classification/mushrooms.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing the first five rows of dataset\n\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see all the columns have categorical value.\n# We have 22 features (independent variables) and a dependent variable (class).\n\n# We will continue with data preprocessing but lets get some insights of the dataset first.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualising the number of mushrooms that fall in each class - p = poisonous, e=edible\nplt.style.use('dark_background')\nplt.rcParams['figure.figsize']=8,8 \ns = sns.countplot(x = \"class\", data = dataset)\nfor p in s.patches:\n    s.annotate(format(p.get_height(), '.1f'), \n               (p.get_x() + p.get_width() / 2., p.get_height()), \n                ha = 'center', va = 'center', \n                xytext = (0, 9), \n                textcoords = 'offset points')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In the given dataset we have 3916 poisonous mushrooms and 4208 edible mushrooms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = dataset.columns\nprint(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(22,1, figsize=(15,150), sharey = True) \nk = 1\nfor i in range(0,22):\n    s = sns.countplot(x = features[k], data = dataset, ax=axes[i], palette = 'GnBu')\n    axes[i].set_xlabel(features[k], fontsize=20)\n    axes[i].set_ylabel(\"Count\", fontsize=20)\n    axes[i].tick_params(labelsize=15)\n    k = k+1\n    for p in s.patches:\n        s.annotate(format(p.get_height(), '.1f'), \n        (p.get_x() + p.get_width() / 2., p.get_height()), \n        ha = 'center', va = 'center', \n        xytext = (0, 9), \n        fontsize = 15,\n        textcoords = 'offset points')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From above graph we can see how many mushrooms belong to each category in each feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(22,1, figsize=(15,150), sharey = True) \nk = 1\nfor i in range(0,22):\n    s = sns.countplot(x = features[k], data = dataset, hue = 'class', ax=axes[i], palette = 'CMRmap')\n    axes[i].set_xlabel(features[k], fontsize=20)\n    axes[i].set_ylabel(\"Count\", fontsize=20)\n    axes[i].tick_params(labelsize=15)\n    axes[i].legend(loc=2, prop={'size': 20})\n    k = k+1\n    for p in s.patches:\n        s.annotate(format(p.get_height(), '.1f'), \n        (p.get_x() + p.get_width() / 2., p.get_height()), \n        ha = 'center', va = 'center', \n        xytext = (0, 9), \n        fontsize = 15,\n        textcoords = 'offset points')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From above graph we can see how many mushrooms belong to each category in each feature and among those how many are edible and how many are \n# poisonous mushrooms.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = dataset[dataset['class'] == 'p']\ndf2 = dataset[dataset['class'] == 'e']\nprint(df1)\nprint(df2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating independent and dependent variables\nx = dataset.iloc[:,1:].values\ny = dataset.iloc[:,0].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(x[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding missing values\n\ndataset.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categories in each feature x\ncolumn_list = dataset.columns.values.tolist()\n#print(column_list)\nfor column_name in column_list:\n    print(column_name)\n    print(dataset[column_name].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label encoding y - dependent variable\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One hot encoding independent variable x\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nonehotencoder = OneHotEncoder()\nx = onehotencoder.fit_transform(x).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the dataset into training set and test set\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 3)\nx_train = pca.fit_transform(x_train)\nx_test = pca.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the Logistic Regression Model on the Training set\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the test set\ny_pred = classifier.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix and calculating accuracy score\nacscore = []\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nacscore.append(ac)\nprint(cm)\nprint(ac)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the Naive Bayes Classification model on the Training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the test set\ny_pred = classifier.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix and calculating the accuarcy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nacscore.append(ac)\nprint(cm)\nprint(ac)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the RBF Kernel SVC on the Training set\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', random_state=0)\nclassifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting test set\ny_pred = classifier.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix and calculating the accuracy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nacscore.append(ac)\nprint(cm)\nprint(ac)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the optimum number of neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor neighbors in range(3,10,1):\n    classifier = KNeighborsClassifier(n_neighbors=neighbors, metric='minkowski')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\nplt.plot(list(range(3,10,1)), list1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the K Nearest Neighbor Classification on the Training set\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\nclassifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the test set\ny_pred = classifier.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix and calculating the accuracy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nacscore.append(ac)\nprint(cm)\nprint(ac)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the Decision Tree Classification on the Training set\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state=0)\nclassifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the test set \ny_pred = classifier.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix and calculating the accuracy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nacscore.append(ac)\nprint(cm)\nprint(ac)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the XGBoost Classification on the Training set\nfrom xgboost import XGBClassifier\nclassifier = XGBClassifier()\nclassifier.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the test set\ny_pred = classifier.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix and calculating the accuracy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nacscore.append(ac)\nprint(cm)\nprint(ac)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding the optimum number of n_estimators\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor estimators in range(10,150):\n    classifier = RandomForestClassifier(n_estimators = estimators, random_state=0, criterion='entropy')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(10,150)), list1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the Random Forest Classification on the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(criterion = 'entropy', random_state = 0, n_estimators = 100)\nclassifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the test set\ny_pred = classifier.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix and accuracy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nacscore.append(ac)\nprint(cm)\nprint(ac)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing accuracy score of all the classification models we have applied \nprint(acscore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = ['LogisticRegression','NaiveBayes','KernelSVM','KNearestNeighbors','DecisionTree','XGBoost','RandomForest']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualising the accuracy score of each classification model\nplt.rcParams['figure.figsize']=15,8 \nplt.style.use('dark_background')\nax = sns.barplot(x=models, y=acscore, palette = \"rocket\", saturation =1.5)\nplt.xlabel(\"Classifier Models\", fontsize = 20 )\nplt.ylabel(\"% of Accuracy\", fontsize = 20)\nplt.title(\"Accuracy of different Classifier Models\", fontsize = 20)\nplt.xticks(fontsize = 13, horizontalalignment = 'center', rotation = 0)\nplt.yticks(fontsize = 13)\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height:.2%}', (x + width/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So among all classification model Random Forest Classification has highest accuracy score = 99.75%.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}