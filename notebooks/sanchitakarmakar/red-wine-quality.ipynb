{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the dataset\ndataset = pd.read_csv(\"../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\", delimiter=\",\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing the top 5 rows in the dataset\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To find out if there are any missing values \ndataset.isna().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building HeatMap to identify which features are relevant for determining quality of red wine\nplt.figure(figsize=(20,10))\ng = sns.heatmap(data      = dataset.corr(),  \n            square    = True, \n            cbar_kws  = {'shrink': .3}, \n            annot     = True, \n            annot_kws = {'fontsize': 12},\n           )\ng.set(ylim=(0,12))\ng.set(xlim=(0,12))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above HeatMap we can see that 'alcohol', 'sulphates', 'citric acid' are important features in determining 'quality' of red wine","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the distribution of dependent variable (quality)\nsns.distplot(dataset['quality'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the distribution of independent variable (alcohol)\nsns.distplot(dataset['alcohol'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the distribution of independent variable (sulphates)\nsns.distplot(dataset['sulphates'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the distribution of independent variable (citric acid)\nsns.distplot(dataset['citric acid'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some more visualisation\n# Alcohol vs Quality\nsns.barplot(\"quality\", y=\"alcohol\", data=dataset, saturation=.5, palette = 'inferno')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Citric Acid vs Quality\nsns.barplot(\"quality\", y=\"citric acid\", data=dataset,saturation=.5, palette = 'inferno')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Suplhates vs Quality\nsns.barplot(\"quality\", y=\"sulphates\", data=dataset, saturation=.5, palette = 'inferno')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Features = ['citric acid','sulphates','alcohol']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating features(x) and dependent variable(y)\nx = dataset.iloc[:,[2,9,10]].values\ny = dataset.iloc[:,-1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the dataset into training set and test set\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train[:,[2]] = sc.fit_transform(x_train[:,[2]])\nx_test[:,[2]] = sc.transform(x_test[:,[2]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running for loop to determine the number of neighbors for best accuracy\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor neighbors in range(3,20,1):\n    classifier = KNeighborsClassifier(n_neighbors=neighbors, metric='minkowski')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\nplt.plot(list(range(3,20,1)), list1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"From above graph we can see that optimal number of neighbors is 10","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the K Nearest Neighbor Classifier on the Training set with n_neighbors = 10\nclassifier = KNeighborsClassifier(n_neighbors=10, metric='minkowski')\nclassifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the Test Set results\ny_pred = classifier.predict(x_test)\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing the predicted test set results and actual test results\nnp.set_printoptions()\nprint(np.concatenate( (y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix and accuracy\nmylist = []\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nmylist.append(ac)\nprint(cm)\nprint(ac)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distplot between Quality of Red Wine of test set results and predicted results\nplt.rcParams['figure.figsize']=8,4 \nsns.set_style(\"darkgrid\")\nsns.distplot(y_test, color = \"blue\", kde = False, label = \"Test set results\", hist_kws = {\"align\": \"right\"})\nsns.distplot(y_pred, color = \"magenta\", kde = False, label = \"Actual results\", hist_kws = {\"align\": \"left\"})\nplt.xlabel(\"Quality\")\nplt.ylabel(\"Number of People\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-----","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the Naive Bayes Classifier on the Training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the Test Set result\ny_pred = classifier.predict(x_test)\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.set_printoptions()\nprint(np.concatenate( (y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix and accuracy\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(ac)\nmylist.append(ac)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distplot between Quality of Red Wine of test set results and predicted results\nplt.rcParams['figure.figsize']=8,4 \nsns.set_style(\"darkgrid\")\nsns.distplot(y_test, color = \"blue\", kde = False, label = \"Test set results\",  hist_kws = {\"align\": \"left\"})\nsns.distplot(y_pred, color = \"magenta\", kde = False, label = \"Actual results\",  hist_kws = {\"align\": \"right\"})\nplt.xlabel(\"Quality\")\nplt.ylabel(\"Number of People\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the Support Vector Classifier on the Training set\nfrom sklearn.svm import SVC\nclassifier = SVC(random_state=0, kernel = 'rbf')\nclassifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(x_test)\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.set_printoptions()\nprint(np.concatenate( (y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix and accuracy\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(ac)\nmylist.append(ac)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distplot between Quality of Red Wine of test set results and predicted results\nplt.rcParams['figure.figsize']=8,4 \nsns.set_style(\"darkgrid\")\nsns.distplot(y_test, color = \"blue\", kde = False, label = \"Test set results\",  hist_kws = {\"align\": \"left\"})\nsns.distplot(y_pred, color = \"magenta\", kde = False, label = \"Actual results\", hist_kws = {\"align\": \"right\"})\nplt.xlabel(\"Quality\")\nplt.ylabel(\"Number of People\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-----","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determining the max_leaf_nodes using for loop for leaves 2-25\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor leaves in range(2,25):\n    classifier = DecisionTreeClassifier(max_leaf_nodes = leaves, random_state=0, criterion='entropy')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(2,25)), list1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can see the optimum number of max_leaf_nodes = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the Decision Tree Classifier on the Training set\nclassifier = DecisionTreeClassifier(max_leaf_nodes = 5, random_state=0, criterion='entropy')\nclassifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(x_test)\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.set_printoptions()\nprint(np.concatenate( (y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix and accuracy\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(ac)\nmylist.append(ac)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distplot between Quality of Red Wine of test set results and predicted results\nplt.rcParams['figure.figsize']=8,4 \nsns.set_style(\"darkgrid\")\nsns.distplot(y_test, color = \"blue\", kde = False, label = \"Test set results\", hist_kws = {\"align\": \"left\"})\nsns.distplot(y_pred, color = \"magenta\", kde = False, label = \"Actual results\", hist_kws = {\"align\": \"right\"})\nplt.xlabel(\"Quality\")\nplt.ylabel(\"Number of People\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determining the optimum number of n_estimators\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor estimators in range(100,200):\n    classifier = RandomForestClassifier(n_estimators = estimators, random_state=0, criterion='entropy')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(100,200)), list1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the Random Forest Classifier on the Training set\nclassifier = RandomForestClassifier(n_estimators=180, random_state=0, criterion='entropy')\nclassifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(x_test)\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.set_printoptions()\nprint(np.concatenate( (y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix and accuracy\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(ac)\nmylist.append(ac)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distplot between Quality of Red Wine of test set results and predicted results\nplt.rcParams['figure.figsize']=8,4 \nsns.set_style(\"darkgrid\")\nsns.distplot(y_test, color = \"blue\", kde = False, label = \"Test set results\", hist_kws = {\"align\": \"left\"} )\nsns.distplot(y_pred, color = \"magenta\", kde = False, label = \"Actual results\", hist_kws = {\"align\": \"right\"})\nplt.xlabel(\"Quality\")\nplt.ylabel(\"Number of People\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the XGBoost Classifier on the Training set\nfrom xgboost import XGBClassifier\nclassifier = XGBClassifier()\nclassifier.fit(x_train,y_train)\ny_pred = classifier.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.set_printoptions()\nprint(np.concatenate( (y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix and accuracy\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(ac)\nmylist.append(ac)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy score of different models\nmylist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mylist2 = [\"KNearestNeighbours\",\"NaiveBayes\",\"SupportVector\",\"DecisionTree\",\"RandomForest\",\"XGBoost\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the accuracy score for different models\nplt.rcParams['figure.figsize']=10,6 \nsns.set_style(\"darkgrid\")\nax = sns.barplot(x=mylist2, y=mylist, palette = \"rocket\", saturation =1.5)\nplt.xlabel(\"Classifier Models\", fontsize = 20 )\nplt.ylabel(\"Accuracy\", fontsize = 20)\nplt.title(\"Accuracy of different Classifier Models\", fontsize = 20)\nplt.xticks(fontsize = 11, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 13)\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height:.2%}', (x + width/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}