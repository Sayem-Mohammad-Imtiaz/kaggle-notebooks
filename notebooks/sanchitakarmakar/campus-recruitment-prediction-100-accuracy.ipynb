{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Importing the Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing the Dataset","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv('../input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting the non-numerical data into catetgorical data\n\ndataset['gender'] = dataset['gender'].astype('object')\ndataset['ssc_b'] = dataset['ssc_b'].astype('object')\ndataset['hsc_b'] = dataset['hsc_b'].astype('object')\ndataset['hsc_s'] = dataset['hsc_s'].astype('object')\ndataset['degree_t'] = dataset['degree_t'].astype('object')\ndataset['workex'] = dataset['workex'].astype('object')\ndataset['specialisation'] = dataset['specialisation'].astype('object')\n\n#Getting all the categorical columns except the target\ncategorical_columns = dataset.select_dtypes(exclude = 'number').drop('status', axis = 1).columns\n\nprint(categorical_columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First considering only numerical values for feature selection\nX = dataset.iloc[:,[2,4,7,10,12,14]].values\nY = dataset.iloc[:,13].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Checking for null values","metadata":{}},{"cell_type":"code","source":"dataset.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# So salary column contains null values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Taking care of missing data in Salary column","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(X[:,[5]])\nX[:,[5]] = imputer.transform(X[:,[5]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Selection of Numerical Values","metadata":{}},{"cell_type":"code","source":"plt.rcParams['figure.figsize']=15,6 \nsns.set_style(\"darkgrid\")\n\nx = dataset.iloc[:,[2,4,7,10,12,14]]\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,Y)\nprint(model.feature_importances_) \nfeat_importances = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importances.nlargest(12).plot(kind='barh')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# So we can conclude that 'Salary' and 'ssc_p' are two relavent features for predicting the status of placement for a student","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Selection of Categorical Data","metadata":{}},{"cell_type":"code","source":"# Import the function\n#from scipy.stats import chi2_contingency\n#Testing the relationship\n#chi_res = chi2_contingency(pd.crosstab(dataset['status'], dataset['gender']))\n#print('Chi2 Statistic: {}, p-value: {}'.format(chi_res[0], chi_res[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import chi2_contingency\nchi2_check = []\nfor i in categorical_columns:\n    if chi2_contingency(pd.crosstab(dataset['status'], dataset[i]))[1] < 0.05:\n        chi2_check.append('Reject Null Hypothesis')\n    else:\n        chi2_check.append('Fail to Reject Null Hypothesis')\nres = pd.DataFrame(data = [categorical_columns, chi2_check] \n             ).T \nres.columns = ['Column', 'Hypothesis']\nprint(res)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If we choose our p-value level to 0.05, if the p-value test result is more than 0.05 then we fail to reject the Null Hypothesis. \n# This means, there is no relationship between the Feature and Dependent Variable based on the Chi-Square test of independence.\n# And if the p-value test result is less than 0.05 then we reject the Null Hypothesis. \n# This means, there is a relationship between the Feature and Dependent Variable based on the Chi-Square test of independence.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# So we conclude that 'workex' and 'specialisation' are two important features for predicting status.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# So after feature selection of categorical and numerical features, X comes as,\nX = dataset.iloc[:,[2,9,11,14]].values\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(X[:,[3]])\nX[:,[3]] = imputer.transform(X[:,[3]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoding Categorical Values","metadata":{}},{"cell_type":"markdown","source":"#### Finding the categories","metadata":{}},{"cell_type":"code","source":"print(dataset['workex'].unique())\nprint(dataset['specialisation'].unique())\nprint(dataset['status'].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Label encoding","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle1 = LabelEncoder()\nX[:,1] = le1.fit_transform(X[:, 1])\nle2 = LabelEncoder()\nX[:,2] = le2.fit_transform(X[:, 2])\nle3 = LabelEncoder()\nY = le3.fit_transform(Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting the dataset into training set and test set","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Scaling","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train[:,[0,3]] = sc.fit_transform(x_train[:,[0,3]])\nx_test[:,[0,3]] = sc.transform(x_test[:,[0,3]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Applying classification models on the Training set","metadata":{}},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state=0)\nclassifier.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mylist = []\nmylist2 = []\ny_pred = classifier.predict(x_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nac = accuracy_score(y_test, y_pred)\nprint(ac)\nmylist.append(ac)\nmylist2.append('Logistic Regression')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### K Nearest Neighbor","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\nclassifier.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = classifier.predict(x_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nac = accuracy_score(y_test, y_pred)\nprint(ac)\nmylist.append(ac)\nmylist2.append('KNN')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Naive Bayes Classification","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = classifier.predict(x_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nac = accuracy_score(y_test, y_pred)\nprint(ac)\nmylist.append(ac)\nmylist2.append('Naive Bayes')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree Classification","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\nclassifier.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = classifier.predict(x_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nac = accuracy_score(y_test, y_pred)\nprint(ac)\nmylist.append(ac)\nmylist2.append('Decision Tree')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Support Vector Classification","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nclassifier = SVC(kernel='rbf', random_state=0)\nclassifier.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = classifier.predict(x_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nac = accuracy_score(y_test, y_pred)\nprint(ac)\nmylist.append(ac)\nmylist2.append('Support Vector')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest Classification","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)\nclassifier.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = classifier.predict(x_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nac = accuracy_score(y_test, y_pred)\nprint(ac)\nmylist.append(ac)\nmylist2.append('Random Forest')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the accuracy score for different models\nplt.rcParams['figure.figsize']=10,6 \nsns.set_style(\"darkgrid\")\nax = sns.barplot(x=mylist2, y=mylist, palette = \"rocket\", saturation =1.5)\nplt.xlabel(\"Classifier Models\", fontsize = 20 )\nplt.ylabel(\"Accuracy\", fontsize = 20)\nplt.title(\"Accuracy of different Classifier Models\", fontsize = 20)\nplt.xticks(fontsize = 11, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 13)\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height:.2%}', (x + width/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}