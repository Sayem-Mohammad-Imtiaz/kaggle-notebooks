{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from typing import Union, List\n\nimport os\nimport torch\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\n\nfrom torchvision import models\nfrom torch.cuda import device_count\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom pytorch_lightning.core.lightning import LightningModule\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.metrics.functional import f1, accuracy\n\nfrom PIL import Image\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nsns.set()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = \"../input/coronahack-chest-xraydataset\"\nimage_dir = os.path.join(data_dir, \"Coronahack-Chest-XRay-Dataset\", \"Coronahack-Chest-XRay-Dataset\")\ntrain_dir = os.path.join(data_dir, \"Coronahack-Chest-XRay-Dataset\", \"Coronahack-Chest-XRay-Dataset\", \"train\")\ntest_dir = os.path.join(data_dir, \"Coronahack-Chest-XRay-Dataset\", \"Coronahack-Chest-XRay-Dataset\", \"test\")\nmodel_path = \"model\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_RESIZE = 224","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_df = pd.read_csv(os.path.join(data_dir, 'Chest_xray_Corona_Metadata.csv'), index_col=[0])\nmeta_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_summary_df = pd.read_csv(os.path.join(data_dir, 'Chest_xray_Corona_dataset_Summary.csv'), index_col=[0])\nmeta_summary_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the stress smoking\nmeta_df.drop(meta_df[meta_df['Label_1_Virus_category'] == 'Stress-Smoking'].index, inplace=True)\nmeta_df.loc[meta_df[meta_df['Label_2_Virus_category'] == 'SARS'].index, 'Label_2_Virus_category'] = np.NaN\nmeta_df.loc[meta_df[meta_df['Label_2_Virus_category'] == 'Streptococcus'].index, 'Label_2_Virus_category'] = np.NaN","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now check the semi result via plot\nmissing_values = meta_df.isnull().sum()\nmissing_values.loc[['Label_2_Virus_category', 'Label_1_Virus_category']].plot.barh()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace the null values with prespecified labels\ncolumn_nan_values = {'Label_1_Virus_category': 'Normal'}\nmeta_df.fillna(value=column_nan_values, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_df.loc[meta_df['Label_1_Virus_category'] == 'bacteria', 'Label_2_Virus_category'] = 'Bacteria-unknown'\nmeta_df.loc[meta_df['Label_1_Virus_category'] == 'Normal', 'Label_2_Virus_category'] = 'Normal-2'\nmeta_df.loc[(meta_df['Label_1_Virus_category'] == 'Virus') & (meta_df['Label_2_Virus_category'] != 'COVID-19'), 'Label_2_Virus_category'] = 'Virus-unknown'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_label_2_labels = {'Virus': 'Pnemonia-Virus', 'bacteria': 'Pnemonia-Bacteria'}\nmeta_df['Label_1_Virus_category'].replace(new_label_2_labels, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop this column because we don't use this column anymore!\nmeta_df.drop(columns=['Label'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"level_1_labels_to_ids = {\n    'Normal' : 0,\n    'Pnemonia-Virus': 1,\n    'Pnemonia-Bacteria': 2\n}\n\nlevel_2_labels_to_ids = {\n    'Normal-2' : 0,\n    'Virus-unknown' : 1,\n    'COVID-19' : 2,\n    'Bacteria-unknown': 3\n}\n\nlevel_1_id2label = {v: k for k, v in level_1_labels_to_ids.items()}\nlevel_2_id2label = {v: k for k, v in level_2_labels_to_ids.items()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_df['level_1_target'] = meta_df['Label_1_Virus_category'].map(level_1_labels_to_ids)\nmeta_df['level_2_target'] = meta_df['Label_2_Virus_category'].map(level_2_labels_to_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Balance all sets\ntest_idx = meta_df[meta_df['Label_2_Virus_category'] == 'COVID-19'].sample(frac=0.15, random_state=1).index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separate the dataframe\ncovid_samples = meta_df.loc[test_idx]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.concat([meta_df[meta_df['Dataset_type'] == 'TEST'], covid_samples])\ntrain_df = meta_df[meta_df['Dataset_type'] == 'TRAIN']\ntrain_df = train_df[~train_df['X_ray_image_name'].isin(covid_samples['X_ray_image_name'])]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verify if all datasets are accesible!\n\nassert all([os.path.isfile(os.path.join(image_dir,dset.lower(),filename)) for filename, dset in train_df[['X_ray_image_name', 'Dataset_type']].values])\nassert all([os.path.isfile(os.path.join(image_dir,dset.lower(),filename)) for filename, dset in test_df[['X_ray_image_name', 'Dataset_type']].values])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CovidDataset(Dataset):\n\n    def __init__(self,\n                 df,\n                 root_dir,\n                 transform):\n        self.df = df\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_metadata = self.df.iloc[idx]\n        img_path = os.path.join(self.root_dir,img_metadata['Dataset_type'].lower(), img_metadata['X_ray_image_name'])\n\n        image = Image.open(img_path).convert(\"RGB\")\n        image = self.transform(image)\n\n        target_1 = torch.as_tensor(img_metadata['level_1_target'])\n        target_2 = torch.as_tensor(img_metadata['level_2_target'])\n\n        sample = {\n            'image': image,\n            'target_1': target_1,\n            'target_2': target_2\n        }\n        return sample","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build the model!\nclass Identity(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return x\n\n\nclass TwoLevelClassifier(nn.Module):\n\n    def __init__(self,\n                 num_level_1_classes,\n                 num_level_2_classes,\n                 img_size):\n        super().__init__()\n\n        h, w = img_size\n        self.h1 = h - h // 2\n        self.h2 = h + h // 2\n        self.w1 = w - w // 2\n        self.w2 = w + w // 2\n\n        self.resnet = models.resnet18(pretrained=True, progress=True)\n        resnet_features = self.resnet.fc.in_features\n        \n        self.resnet.fc = Identity()\n        self.side_stack = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=3),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32,64, kernel_size=3, stride=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(64,64, kernel_size=3, stride=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(64, num_level_1_classes)\n        )\n        \n        self.level1_classifier = nn.Linear(resnet_features, num_level_1_classes)\n        self.level2_classifier = nn.Linear(num_level_1_classes, num_level_2_classes)\n\n    def forward(self, x):\n        features = self.resnet(x)\n        logits1 = self.level1_classifier(features)\n        cropped_x = x[:, :, self.h1:self.h2, self.w1:self.w2]\n        level_2_feed = self.side_stack(cropped_x) + logits1\n        logits2 = self.level2_classifier(level_2_feed)\n\n        return logits1, logits2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ModelWrapper(LightningModule):\n    def __init__(self, hparams, df_train=None, df_test=None):\n        super().__init__()\n\n        self.df_train = df_train\n        self.df_test = df_test\n        self.hparams = hparams\n        self.batch_size = self.hparams['batch_size']\n        self.lr = self.hparams['lr']\n        self.num_workers = self.hparams['num_workers']\n\n        if df_train is not None:  #\n            train_transforms = transforms.Compose([\n                # transforms.ToPILImage(mode='RGB'),\n                transforms.Resize([IMG_RESIZE, IMG_RESIZE]),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n            ])\n\n            test_transforms = transforms.Compose([\n                # transforms.ToPILImage('RGB'),\n                transforms.Resize([IMG_RESIZE, IMG_RESIZE]),\n                transforms.ToTensor(),\n                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n            ])\n\n            self.train_dataset = CovidDataset(df=df_train, root_dir=self.hparams['image_dir'], transform=train_transforms)\n            self.test_dataset = CovidDataset(df=df_test, root_dir=self.hparams['image_dir'], transform=test_transforms)\n\n        self.model = TwoLevelClassifier(\n            num_level_1_classes=self.hparams['num_level1_classes'],\n            num_level_2_classes=self.hparams['num_level2_classes'],\n            img_size=self.hparams['img_size']\n        )\n\n\n        self.loss_level1 = nn.CrossEntropyLoss()\n        self.loss_level2 = nn.CrossEntropyLoss(weight=torch.as_tensor(self.hparams['label2_weights']))\n\n        self.loss_weights = self.hparams['loss_weights']\n\n    def forward(self, batch):\n        return self.model(batch)\n\n    def train_dataloader(self) -> DataLoader:\n        return DataLoader(dataset=self.train_dataset, batch_size=self.batch_size,\n                          num_workers=self.num_workers, shuffle=True)\n\n    def val_dataloader(self) -> Union[DataLoader, List[DataLoader]]:\n        return DataLoader(dataset=self.test_dataset, batch_size=self.batch_size,\n                          num_workers=self.num_workers, shuffle=False)\n\n    def training_step(self, batch, batch_idx):\n\n        x, y_level1, y_level2 = batch['image'], \\\n                                batch['target_1'], \\\n                                batch['target_2']\n\n        logits1, logits2 = self(x)\n\n        loss1 = self.loss_level1(logits1, y_level1)\n        loss2 = self.loss_level2(logits2, y_level2)\n\n        loss = loss1 * self.loss_weights[0] + loss2 * self.loss_weights[1]\n\n        self.log('train/loss', loss, on_step=True, logger=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n\n        x, y_level1, y_level2 = batch['image'], \\\n                                batch['target_1'], \\\n                                batch['target_2']\n        logits1, logits2 = self(x)\n\n        loss1 = self.loss_level1(logits1, y_level1)\n        loss2 = self.loss_level2(logits2, y_level2)\n        loss = loss1 + loss2\n\n        level1_preds = torch.argmax(logits1, dim=1)\n        level2_preds = torch.argmax(logits2, dim=1)\n\n        level1_acc = accuracy(level1_preds, y_level1)\n        level2_acc = accuracy(level2_preds, y_level2)\n        level1_f1 = f1(level1_preds, y_level1, self.hparams['num_level1_classes'])\n        level2_f1 = f1(level2_preds, y_level2, self.hparams['num_level2_classes'])\n\n        logs = loss, level1_acc, level2_acc, level1_f1, level2_f1\n\n        return logs\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x[0] for x in outputs]).mean()\n        avg_level1_acc = torch.stack([x[1] for x in outputs]).mean()\n        avg_level2_acc = torch.stack([x[2] for x in outputs]).mean()\n        avg_level1_f1 = torch.stack([x[3] for x in outputs]).mean()\n        avg_level2_f1 = torch.stack([x[4] for x in outputs]).mean()\n\n        self.log('val/loss', avg_loss, prog_bar=True, logger=True, on_epoch=True)\n        self.log('val/level1_acc', avg_level1_acc, logger=True, on_epoch=True)\n        self.log('val/level2_acc', avg_level2_acc, logger=True, on_epoch=True)\n        self.log('val/level1_f1', avg_level1_f1, prog_bar=True, logger=True, on_epoch=True)\n        self.log('val/level2_f1', avg_level2_f1, prog_bar=True, logger=True, on_epoch=True)\n\n    def configure_optimizers(self):\n\n        if self.hparams['optimizer'] == 'adam':\n            optimizer = optim.Adam(self.model.parameters(), self.lr)\n        else:  # SGDWithMomentum\n            optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.9)\n\n        return {\n            'optimizer': optimizer,\n            'lr_scheduler': optim.lr_scheduler.StepLR(optimizer, \n                                                      step_size=self.hparams['sch_step_size'], \n                                                      gamma=self.hparams['sch_gamma'])\n        }\n\n    def on_train_end(self):\n        ckpt_path = os.path.join(self.trainer.log_dir, \"checkpoints\", \"min_val_loss.ckpt\")\n        print(f\"Loading best checkpoint from {ckpt_path}\")\n        best_model_ = ModelWrapper.load_from_checkpoint(ckpt_path)\n\n        save_dir = self.hparams['model_save_dir']\n        \n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        print(f\"Saving only pytorch model without the wrapper properties to {os.path.join(save_dir, 'best_model.pt')}\")\n        torch.save(best_model_.model, os.path.join(save_dir, \"best_model.pt\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_EPOCHS = 11\nBATCH_SIZE = 64\nlr = 4e-4\n\nparams = {\n    'batch_size': BATCH_SIZE,\n    'lr': lr,\n    'sch_step_size': 3,  \n    'sch_gamma': 0.5,  \n    'optimizer': 'adam',  \n    'num_workers': 4, \n    'num_level1_classes': 3, \n    'num_level2_classes': 4,  \n    'label2_weights': [0.1, 0.1, 0.8, 0.1],  \n    'loss_weights': [0.5, 1],  \n    'img_size': (IMG_RESIZE, IMG_RESIZE),  \n    'image_dir': image_dir,\n    'model_save_dir': 'model',\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Now train our model!\n\nwrapper = ModelWrapper(hparams=params, df_train=train_df, df_test=test_df)\n\ngpu_num = device_count()\n\ncheckpoint_callback = ModelCheckpoint(\n    save_top_k=1,\n    verbose=True,\n    monitor='val/loss',\n    mode='min',\n    filename='min_val_loss'\n)\n\ntrainer = Trainer(\n    default_root_dir=os.getcwd(),\n    gpus=gpu_num,\n    max_epochs=MAX_EPOCHS,\n    callbacks=[checkpoint_callback]\n)\n\ntrainer.fit(wrapper)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = torch.load(os.path.join(model_path, \"best_model.pt\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_transforms = transforms.Compose([\n    transforms.Resize([IMG_RESIZE, IMG_RESIZE]),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n])\ntest_dataset = CovidDataset(df=test_df, root_dir=image_dir, transform=test_transforms)\ntest_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, dataloader, device_):\n    print(\"Evaluating...\")\n    model.to(device_).eval()\n    with torch.no_grad():\n\n        level1_preds = []\n        level1_targets = []\n\n        level2_preds = []\n        level2_targets = []\n\n        for batch in dataloader:\n            x, y_level1, y_level2 = batch['image'].to(device_), \\\n                                    batch['target_1'].to(device_), \\\n                                    batch['target_2'].to(device_)\n            logits1, logits2 = model(x)\n\n            batch_level1_preds = torch.argmax(logits1, dim=1)\n            batch_level2_preds = torch.argmax(logits2, dim=1)\n\n            level1_preds.extend(batch_level1_preds.tolist())\n            level2_preds.extend(batch_level2_preds.tolist())\n\n            level1_targets.extend(y_level1.tolist())\n            level2_targets.extend(y_level2.tolist())\n\n    return level1_preds, level1_targets, level2_preds, level2_targets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\npreds_1, targets_1, preds_2, targets_2 = evaluate(best_model, test_dataloader, device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Precision, Recall, and F1-Score\nprint(\"\\t\\t***\\tLEVEL 1 CLASSIFICATION METRICS\\t***\")\nprint(classification_report(targets_1, preds_1, target_names=list(level_1_id2label.values()), zero_division=0))\nprint(\"\\t\\t***\\tLEVEL 2 CLASSIFICATION METRICS\\t***\")\nprint(classification_report(targets_2, preds_2, target_names=list(level_2_id2label.values()), zero_division=0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare the const for confusion matrix!\nlevel_1_conf_mat = confusion_matrix(targets_1, preds_1)\nlevel_1_conf_mat = level_1_conf_mat.astype(np.float) / level_1_conf_mat.sum(axis=1)[:, np.newaxis]\n\nlevel_2_conf_mat = confusion_matrix(targets_2, preds_2)\nlevel_2_conf_mat = level_2_conf_mat.astype(np.float) / level_2_conf_mat.sum(axis=1)[:, np.newaxis]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n\naxs[0].title.set_text(\"LEVEL 1 CONFUSION MATRIX\")\naxs[1].title.set_text(\"LEVEL 2 CONFUSION MATRIX\")\n\nsns.heatmap(\n    level_1_conf_mat,\n    cmap='coolwarm',\n    yticklabels=list(level_1_id2label.values()),\n    xticklabels=list(level_1_id2label.values()),\n    annot=True,\n    ax=axs[0]\n)\n\nsns.heatmap(\n    level_2_conf_mat,\n    cmap='coolwarm',\n    yticklabels=list(level_2_id2label.values()),\n    xticklabels=list(level_2_id2label.values()),\n    annot=True,\n    ax=axs[1]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}