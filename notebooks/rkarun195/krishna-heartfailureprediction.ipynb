{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# importing Required Packages \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nimport statsmodels.api as sm\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the Data\ndata=pd.read_csv(\"../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding the Null Values\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Verifying The Correlation between Independent Variables\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.heatmap(data.corr(),annot=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Variables are not correlated "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building a Logistic Regression Model\nlogreg=LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Performing Recursive Feature Elimination to Pick out Best Features available\nrfe=RFE(logreg,5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y=data.pop(\"DEATH_EVENT\")\nX=data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model=rfe.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rfe.support_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Fitting a Logistic Regression model to the train data\nlogreg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train_pred=logreg.predict(X_train)\ny_test_pred=logreg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Predicting it with the test data \nlogreg.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n# Performing Logsistic Regresion using Stats models for better Statistical analysis\nlogml=sm.GLM(y_train,(sm.add_constant(X_train)),family=sm.families.Binomial())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"#predicting Accuracy by removing columns \nfor i in X_train.columns:\n    X_train_df=X_train.drop(i,axis=1)\n    X_test_df=X_test.drop(i,axis=1)\n    logreg.fit(X_train_df,y_train)\n    y_train_pred=logreg.predict(X_train_df)\n    y_test_pred=logreg.predict(X_test_df)\n    print(metrics.accuracy_score(y_test_pred, y_test), i)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Predicting Using KNN \nfor i in range(1,10,2):\n    neigh = KNeighborsClassifier(n_neighbors=i)\n    neigh.fit(X_train, y_train)\n    Y_train_pred=neigh.predict(X_train)\n    Y_test_pred=neigh.predict(X_test)\n    print(neigh.score(X_test,y_test),\" \",i)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Performing Random Forest Modeling\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=1)\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Y_test_pred=clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"clf.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the above methods, the accuracy is as follows :\n\nLogistic Regression -> 86%\nKNN                 -> 68 %\nRandom Forest       -> 93%\n\nRandom-Forest Fits the Model and becomes the best precdictor for the Heart Failure Data "},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}