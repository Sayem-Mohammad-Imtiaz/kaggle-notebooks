{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"> I show you how you can use different hyperparameter optimization techniques and libraries to tune hyperparameters of almost any kind of model or just to optimize any function! \n\nContents:-\n1. GridSearchCV - score(0.8805)\n2. RandomizedSearchCV - score(0.8825) \n3.  Grid/Random Search with Pipelines. - score(0.45990\n4. Bayesian optimization using Gaussian Processes. - score(0.9050)\n5. Hyperopt - score(0.9075)\n6. Optuna - score(0.9085) ---Best","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn import ensemble \nfrom sklearn import metrics\nfrom sklearn import model_selection\n","metadata":{"execution":{"iopub.status.busy":"2021-09-01T09:33:58.943788Z","iopub.execute_input":"2021-09-01T09:33:58.94412Z","iopub.status.idle":"2021-09-01T09:33:58.951912Z","shell.execute_reply.started":"2021-09-01T09:33:58.944087Z","shell.execute_reply":"2021-09-01T09:33:58.950713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/mobile-price-classification/train.csv\")\n# features are all columns without price_range\n# note that there is no id column in this dataset\n# here we have training features\nX = df.drop(\"price_range\", axis=1).values\n# and the targets\ny = df.price_range.values\nX.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-01T08:58:52.016013Z","iopub.execute_input":"2021-09-01T08:58:52.016415Z","iopub.status.idle":"2021-09-01T08:58:52.067254Z","shell.execute_reply.started":"2021-09-01T08:58:52.016377Z","shell.execute_reply":"2021-09-01T08:58:52.066052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nRandomForestClassifier( \n    n_estimators=100, \n    criterion='gini', \n    max_depth=None, \n    min_samples_split=2,\n    min_samples_leaf=1, \n    min_weight_fraction_leaf=0.0,\n    max_features='auto',\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0, \n    min_impurity_split=None, \n    bootstrap=True,\n    oob_score=False,\n    n_jobs=None,\n    random_state=None,\n    verbose=0,\n    warm_start=False, \n    class_weight=None,\n    ccp_alpha=0.0, \n    max_samples=None,\n)\n\nThere are nineteen parameters,\nand all the combinations of all \nthese parameters for all the values\nthey can assume are going to be infinite. \nNormally, we don’t have the resource and\ntime to do this. Thus, we specify a\ngrid of parameters. A search over this\ngrid to find the best combination of parameters \nis known as grid search. \nWe can say that n_estimators can be\n100, 200, 250, 300, 400, 500; \nmax_depth can be 1, 2, 5, 7, 11, 15 and \ncriterion can be gini or entropy. \nThese may not look like a lot of parameters, \nbut it would take a lot of time for computation\nif the dataset is too large.\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-09-01T06:25:11.893146Z","iopub.execute_input":"2021-09-01T06:25:11.893542Z","iopub.status.idle":"2021-09-01T06:25:11.899708Z","shell.execute_reply.started":"2021-09-01T06:25:11.893506Z","shell.execute_reply":"2021-09-01T06:25:11.898657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Grid_search. </br>\nGrid-search is used to find the optimal hyperparameters of a model which results in the most ‘accurate’ predictions.","metadata":{}},{"cell_type":"code","source":"# define the model here\n# i am using random forest with n_jobs=-1\n# n_jobs=-1 => use all cores\nclassifier = ensemble.RandomForestClassifier(n_jobs=-1)\n# define a grid of parameters\n# this can be a dictionary or a list of \n# dictionaries\nparam_grid = {\n    \"n_estimators\" : [100, 200, 300, 400],\n    \"max_depth\" : [1, 4, 6, 9],\n    \"criterion\" : [\"gini\", \"entropy\"],\n}\n\n# initialize grid search\n# estimator is the model that we have defined\n# param_grid is the grid of parameters\n# we use accuracy as our metric. you can define your own\n# higher value of verbose implies a lot of details are printed\n# cv=5 means that we are using 5 fold cv (not stratified)\n\nmodel = model_selection.GridSearchCV(\n    estimator=classifier,\n    param_grid=param_grid,\n    scoring=\"accuracy\",\n    verbose=10,\n    n_jobs=1,\n    cv=5\n)\n\n# fit the model and extract best score\nmodel.fit(X, y)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-01T04:50:54.352978Z","iopub.execute_input":"2021-09-01T04:50:54.353362Z","iopub.status.idle":"2021-09-01T04:52:33.676286Z","shell.execute_reply.started":"2021-09-01T04:50:54.353328Z","shell.execute_reply":"2021-09-01T04:52:33.675315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.best_score_)\nprint(model.best_estimator_.get_params())","metadata":{"execution":{"iopub.status.busy":"2021-09-01T04:53:19.305875Z","iopub.execute_input":"2021-09-01T04:53:19.306257Z","iopub.status.idle":"2021-09-01T04:53:19.312284Z","shell.execute_reply.started":"2021-09-01T04:53:19.306221Z","shell.execute_reply":"2021-09-01T04:53:19.311326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we see that our best five fold accuracy score was 0.8805 and we have the best parameters from our grid search. ","metadata":{}},{"cell_type":"markdown","source":"WOW, it is amazing to see model itself choose its best values. </br>These are the best hyperparameter values for this model. </br>\nYou can try with different parameters values and see if the accuracy score goes high or not, play with parameters.","metadata":{}},{"cell_type":"markdown","source":"- It must also be noted that if you have k- fold cross-validation, you need even more loops which implies even more time to find the perfect parameters. Grid search is therefore not very popular.","metadata":{}},{"cell_type":"markdown","source":"# 2. Randomized Search CV </br>\nRandomizedSearchCV is very useful when we have many parameters to try and the training time is very long. For this example, I use a random-forest classifier, so I suppose you already know how this kind of algorithm works.","metadata":{}},{"cell_type":"markdown","source":"In random search, we randomly select a combination of parameters and calculate the cross-validation score. The time consumed here is less than grid search because we do not evaluate over all different combinations of parameters. We choose how many times we want to evaluate our models, and that’s what decides how much time the search takes.","metadata":{}},{"cell_type":"code","source":"# define the model here\n# i am using random forest with n_jobs=-1\n# n_jobs=-1 => use all cores\nclassifier = ensemble.RandomForestClassifier(n_jobs=-1)\n\n# define a grid of parameters\n# this can be a dictionary or a list of # dictionaries\nparam_grid = {\n    \"n_estimators\" : np.arange(100, 1500, 100),\n    \"max_depth\" : np.arange(1, 20),\n    \"criterion\" : [\"gini\", \"entropy\"],\n}\n\n# initialize random search\n# estimator is the model that we have defined\n# param_distributions is the grid/distribution of parameters\n# we use accuracy as our metric. you can define your own\n# higher value of verbose implies a lot of details are printed\n# cv=5 means that we are using 5 fold cv (not stratified)\n# n_iter is the number of iterations we want\n# if param_distributions has all the values as list,\n# random search will be done by sampling without replacement\n# if any of the parameters come from a distribution,\n# random search uses sampling with replacement\nmodel = model_selection.RandomizedSearchCV(\n    estimator=classifier,\n    param_distributions=param_grid,\n    scoring=\"accuracy\",\n    n_iter=20,\n    verbose=10,\n    n_jobs=1,\n    cv=5\n)\nmodel.fit(X, y)\nprint(f\"Best score: {model.best_score_}\")\n\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(f\"\\t{param_name}: {best_parameters[param_name]}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-01T06:40:27.168652Z","iopub.execute_input":"2021-09-01T06:40:27.169Z","iopub.status.idle":"2021-09-01T06:43:16.519688Z","shell.execute_reply.started":"2021-09-01T06:40:27.168971Z","shell.execute_reply":"2021-09-01T06:43:16.518513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have changed the grid of parameters for random search, and it seems like we even improved the results a little bit.","metadata":{}},{"cell_type":"markdown","source":"# 3. You can also do same thing with some kind of custom.","metadata":{}},{"cell_type":"markdown","source":"- You can also use some kind of pipelines.","metadata":{}},{"cell_type":"markdown","source":"Sometimes, you might want to use a pipeline. For example, let’s say that we are dealing with a multiclass classification problem. In this problem, the training data consists of two text columns, and you are required to build a model to predict the class. ","metadata":{}},{"cell_type":"code","source":"from sklearn import decomposition\nfrom sklearn import preprocessing\nfrom sklearn import pipeline","metadata":{"execution":{"iopub.status.busy":"2021-09-01T05:14:53.145494Z","iopub.execute_input":"2021-09-01T05:14:53.145846Z","iopub.status.idle":"2021-09-01T05:14:53.152481Z","shell.execute_reply.started":"2021-09-01T05:14:53.145814Z","shell.execute_reply":"2021-09-01T05:14:53.151423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = ensemble.RandomForestClassifier(n_jobs=-1)\n\n\n# Here I am creating pipeline\nscl = preprocessing.StandardScaler()\npca = decomposition.PCA()\nrf = ensemble.RandomForestClassifier(n_jobs=-1)\n\nclassifier = pipeline.Pipeline(\n    [\n        (\"scaling\", scl),\n        (\"pca\", pca),\n        (\"rf\", rf),\n    ]\n\n)\n\nparam_grid = {\n    # Here we are using our pipeline key \n    # like #pca__, #scaling__, #rf__\n    \"pca__n_components\" : np.arange(5, 10),\n    \"rf__n_estimators\" : np.arange(100, 1500, 100),\n    \"rf__max_depth\" : np.arange(1, 20),\n    \"rf__criterion\" : [\"gini\", \"entropy\"],\n}\n\nmodel = model_selection.RandomizedSearchCV(\n    estimator=classifier,\n    param_distributions=param_grid,\n    scoring=\"accuracy\",\n    n_iter=10,\n    verbose=10,\n    n_jobs=1,\n    cv=5\n)\nmodel.fit(X, y)\nprint(model.best_score_)\nprint(model.best_estimator_.get_params())","metadata":{"execution":{"iopub.status.busy":"2021-09-01T05:35:51.530396Z","iopub.execute_input":"2021-09-01T05:35:51.531048Z","iopub.status.idle":"2021-09-01T05:38:12.922522Z","shell.execute_reply.started":"2021-09-01T05:35:51.531009Z","shell.execute_reply":"2021-09-01T05:38:12.920814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The score is not good after using randomizedsearchcv with pipeline.","metadata":{}},{"cell_type":"markdown","source":"# 4. Bayesian optimization using Gaussian Processes.\n\n</br>\nYou can read here : https://scikit-optimize.github.io/stable/modules/generated/skopt.gp_minimize.html","metadata":{}},{"cell_type":"code","source":"from functools import partial\nfrom skopt import space\nfrom skopt import gp_minimize\n# Function to minimize. \n# Should take a single list of parameters\n# and return the objective value.\ndef optimize(params, param_names, x, y):\n    params = dict(zip(param_names, params)) # This you cannot use when you tuning multiple params things  \n    model = ensemble.RandomForestClassifier(**params)\n    kf = model_selection.StratifiedKFold(n_splits=5)\n    accuracies = []\n    for idx in kf.split(X=x, y=y):\n        train_idx, valid_idx = idx[0], idx[1]\n        xtrain = x[train_idx]\n        ytrain = y[train_idx]\n        \n        xvalid = x[valid_idx]\n        yvalid = y[valid_idx]\n        \n        model.fit(xtrain, ytrain)\n        preds = model.predict(xvalid)\n        fold_acc = metrics.accuracy_score(yvalid, preds)\n        accuracies.append(fold_acc)\n        \n    # We need to return minimize \n    return -1.0 * np.mean(accuracies)\n\nparam_space = [\n    # Order is matter\n    space.Integer(3, 15, name=\"max_depth\"),\n    space.Integer(100, 600, name=\"n_estimators\"),\n    space.Categorical([\"gini\", \"entropy\"], name=\"criterion\"),\n    space.Real(0.01, 1, prior=\"uniform\", name=\"max_features\")\n]\nparam_names = [\n    \"max_depth\",\n    \"n_estimators\",\n    \"criterion\",\n    \"max_features\"\n]\noptimization_function = partial(\n    optimize, \n    param_names=param_names,\n    x=X,\n    y=y\n)\n\nresult = gp_minimize(\n    optimization_function,\n    dimensions=param_space,\n    n_calls=15,\n    n_random_starts=10,\n    verbose=10,\n)\nprint(dict(zip(param_names, result.x)))\n","metadata":{"execution":{"iopub.status.busy":"2021-09-01T06:13:26.69892Z","iopub.execute_input":"2021-09-01T06:13:26.699274Z","iopub.status.idle":"2021-09-01T06:16:14.476737Z","shell.execute_reply.started":"2021-09-01T06:13:26.699242Z","shell.execute_reply":"2021-09-01T06:16:14.475699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"WOW...with using gp_minimize we got improved our score.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# 5. Hyperopt: Distributed Asynchronous Hyper-parameter Optimization\n","metadata":{}},{"cell_type":"code","source":"from hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope\nfrom functools import partial\nfrom skopt import space\nfrom skopt import gp_minimize\n# Function to minimize. \n# Should take a single list of parameters\n# and return the objective value.\ndef optimize(params, x, y):\n    model = ensemble.RandomForestClassifier(**params)\n    kf = model_selection.StratifiedKFold(n_splits=5)\n    accuracies = []\n    for idx in kf.split(X=x, y=y):\n        train_idx, valid_idx = idx[0], idx[1]\n        xtrain = x[train_idx]\n        ytrain = y[train_idx]\n        \n        xvalid = x[valid_idx]\n        yvalid = y[valid_idx]\n        \n        model.fit(xtrain, ytrain)\n        preds = model.predict(xvalid)\n        fold_acc = metrics.accuracy_score(yvalid, preds)\n        accuracies.append(fold_acc)\n        \n    # We need to return minimize \n    return -1.0 * np.mean(accuracies)\n\nparam_space = {\n    # Order is matter\n    \"max_depth\" : scope.int(hp.quniform(\"max_depth\", 3, 15, 1)),\n    \"n_estimators\" : scope.int(hp.quniform(\"n_estimators\",100, 600, 1)),\n    \"criterion\" : hp.choice(\"criterion\", [\"gini\", \"entropy\"]),\n    \"max_features\" : hp.uniform(\"max_features\", 0.01, 1)\n}\n\noptimization_function = partial(\n    optimize, \n    x=X,\n    y=y\n)\n\ntrials = Trials()\n\nresult = fmin(\n    fn=optimization_function,\n    space=param_space,\n    algo=tpe.suggest,\n    max_evals=15,\n    trials=trials,\n)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T09:06:50.941034Z","iopub.execute_input":"2021-09-01T09:06:50.941398Z","iopub.status.idle":"2021-09-01T09:09:38.402267Z","shell.execute_reply.started":"2021-09-01T09:06:50.941361Z","shell.execute_reply":"2021-09-01T09:09:38.401284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- WOW...with using hyperopt we improved our score futhure more.","metadata":{}},{"cell_type":"markdown","source":"# 6. Optuna\n</br>\nTake a look : https://optuna.org/","metadata":{}},{"cell_type":"code","source":"import optuna\nfrom functools import partial\nfrom skopt import space\nfrom skopt import gp_minimize\nfrom hyperopt import Trials\n\n# Function to minimize. \n# Should take a single list of parameters\n# and return the objective value.\ndef optimize(trails, x, y):\n    criterion = trails.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n    n_estimators = trails.suggest_int(\"n_estimators\", 100, 1500)\n    max_depth = trails.suggest_int(\"max_depth\", 3, 15)\n    max_features = trails.suggest_uniform(\"max_features\", 0.01, 1.0)\n    \n    model = ensemble.RandomForestClassifier(\n        n_estimators=n_estimators,\n        max_depth=max_depth,\n        max_features=max_features,\n        criterion=criterion\n    )\n    kf = model_selection.StratifiedKFold(n_splits=5)\n    accuracies = []\n    for idx in kf.split(X=x, y=y):\n        train_idx, valid_idx = idx[0], idx[1]\n        xtrain = x[train_idx]\n        ytrain = y[train_idx]\n        \n        xvalid = x[valid_idx]\n        yvalid = y[valid_idx]\n        \n        model.fit(xtrain, ytrain)\n        preds = model.predict(xvalid)\n        fold_acc = metrics.accuracy_score(yvalid, preds)\n        accuracies.append(fold_acc)\n        \n    # We need to return minimize \n    return -1.0 * np.mean(accuracies)\n\ntrails = Trials()\noptimization_function = partial(optimize, x=X, y=y)\n\nstudy = optuna.create_study(direction=\"minimize\")\n\nstudy.optimize(optimization_function, n_trials=15)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T09:25:36.018021Z","iopub.execute_input":"2021-09-01T09:25:36.018401Z","iopub.status.idle":"2021-09-01T09:33:58.94209Z","shell.execute_reply.started":"2021-09-01T09:25:36.018369Z","shell.execute_reply":"2021-09-01T09:33:58.940562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow... again we improved our accuracy.\nSo, for this situation the Optuna scored high.\n0.9050","metadata":{}},{"cell_type":"markdown","source":"#################################################</br>\nI like to try to tune my hyper parameters manually  first and then choose a range of values and then \nthrow in some kind of Optimization Algorithm.</br>\n##################################################","metadata":{}},{"cell_type":"raw","source":"","metadata":{}}]}