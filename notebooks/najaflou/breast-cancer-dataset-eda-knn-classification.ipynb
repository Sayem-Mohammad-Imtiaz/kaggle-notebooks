{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h3> INTRODUCTION </h3>\nIn this Kernel, I will have a look at Exploratory Data Analysis of Breast Cancer dataset.\nI'll use a popular machine learning algorithm for classification,called K-Nearest Neighbors (KNN). \nI'm going to use this classification algorithm to build a model based on the data from patients and corresponding tumor data.\nAfter training data, Im going to predict the class of unknown data,using this trained model, to find if the tumor is M = malignant or B = benign.\n\nThis Kernel is mainly build for those who are new to **Data Analysis** and **Machine Learning**. So im going to explain the steps I'll do in details to make it more comprehendable for newbies. ","metadata":{}},{"cell_type":"markdown","source":"First lets start with importing requered Libraries.","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom sklearn.neighbors import KNeighborsClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/data.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Data Wrangling</h2>","metadata":{}},{"cell_type":"markdown","source":"<h3>Identify and handle missing values</h3>\n\nIn our dataset, missing data might come with the question mark \"?\". We will replace \"?\" with NaN (Not a Number),NaN is a python default missing value marker and we prefer to change \"?\" to NaN since it works faster and convenient. Here we use the function:\n.replace(A, B, inplace = True) \nto replace A by B","metadata":{}},{"cell_type":"code","source":"# replace \"?\" to NaN\ndf.replace(\"?\", np.nan, inplace = True)\ndf.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking to see if there are any Null or NaN values in our dataframe. There are two methods to detect these missing values: \n<ol>\n    <li><b>.isnull()</b></li>\n    <li><b>.notnull()</b></li>\n</ol>\nOutput of these two methods are a boolean value dataframe indicating whether there's a missing data or not.","metadata":{}},{"cell_type":"code","source":"missing_data = df.isnull()\nmissing_data.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in missing_data.columns.values.tolist():\n    print(column)\n    print (missing_data[column].value_counts())\n    print(\"\")    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Dealing with missing data </h3>\nin the dataframe we have,we can see that there are 2 columns with no data or less valuable data.\n<br />These columns are **Unnamed: 32** and **id**. Unnamed data is empty,but id column is not empty but the value it has is not useful.<br/>We are going to drop these two columns.","metadata":{}},{"cell_type":"code","source":"list = ['Unnamed: 32','id']\ndf.drop(list, axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Data Normalization</h3>\nData Normalization is the process of changing or transforming values of variables into a similar range.<br/>\nThis normalization can include scaling variables somehow that the variable average is 0, or scaling the variable to have variance of 1 or scaling the variable to lie between 0 to 1.<br/>\n<br/>\nThough in this dataset different features have different ranges and we could Normalize this data,but here we wont change the range and wont normalize since its out of scope for this dataset.\n","metadata":{}},{"cell_type":"markdown","source":"<h3>Exploratory Data Analysis</h3>\nExploratory Data Analysis or EDA  refers to a group of investigative processes performed on our data to discover patterns or anomalies, to test our initial hypothesis and assumptions.<br/>\nThis work is done with the help of summary statistics and graphical representations of the data. EDAs are a good tool to understand the data and get an insight about our data.<br/>\n<br/>\nIn order to obtain descriptive statistics of our data we can use **describe** function which will compute basic statistics for all continous variables.<br/>\nSome information that describe function will provide are: <br/>\n* Count of variables.\n* mean of variables\n* their standard deviation\n* min of each variable\n* max of each variable.<br/>\n<br/>\nLets take a look at our Features and their descriptive statistics.","metadata":{"trusted":true}},{"cell_type":"code","source":"df.describe().transpose()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Default setting of \"describe\" will skip object data type,in order to apply describe function on \"object\" data type we have to add 'include=['object']' :\n<br/>\nAs you can see in the result of describe function here it gives us number (count) for diagnosis, number of unique variables which here are two (B and M) and the top variable which here is B,it has the majority in diagnosis.","metadata":{}},{"cell_type":"code","source":"df.describe(include=['object'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can convert the series given by 'describe' to Dataframe as follows:\n<br/> Here are the numbers for Benign and Malignant :","metadata":{}},{"cell_type":"code","source":"B, M = df['diagnosis'].value_counts()\nprint('Number of Malignant : ', M)\nprint('Number of Benign: ', B)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['diagnosis'].value_counts().to_frame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will do some visualization on our data,in order to do this visualization we will use seaborn library.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nsns.set(style=\"darkgrid\")\nax = sns.countplot(df.diagnosis,label=\"Count\") ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Data to plot\nlabels = 'Benign', 'Malignant'\nsizes = df['diagnosis'].value_counts()\ncolors = ['lightskyblue', 'orange']\nexplode= [0.4,0]\n# Plot\nplt.pie(sizes, explode=explode, labels=labels,radius= 1400 ,colors=colors,\nautopct='%1.1f%%', shadow=True, startangle=90)\n\nplt.axis('equal')\nfig = plt.gcf()\nfig.set_size_inches(7,7)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"width = 12\nheight = 10\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ill perform KNN from here!!!!\n","metadata":{}},{"cell_type":"code","source":"import itertools\nfrom matplotlib.ticker import NullFormatter\nimport matplotlib.ticker as ticker\nfrom sklearn import preprocessing\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Dataset we downloaded previously has categorized patients tumors into two groups: Benign and Malignant. we can use characteristic data of the tumor to predict each tumor's type. It is a classification problem. That is, given the dataset,  with predefined labels, we need to build a model to be used to predict class of a new or unknown case. \n\nThe example focuses on using characteristic data, such as Texture, Radius,Are and so on to predict tumors' patterns. \n\nThe target field, called __diagnosis__, has two possible values that correspond to the two tumor groups, as follows:\n  1- Benign\n  2- Malignant\n\nOur objective is to build a classifier, to predict the class of unknown cases. We will use a specific type of classification called K nearest neighbour.\n","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X= df[['radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']]\nX[0:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=df[\"diagnosis\"].values\ny[0:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normalize Data \nData Standardization give data zero mean and unit variance, it is good practice, especially for algorithms such as KNN which is based on distance of cases:","metadata":{}},{"cell_type":"code","source":"X = preprocessing.StandardScaler().fit(X).transform(X.astype(float))\nX[0:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Test Split  \nOut of Sample Accuracy is the percentage of correct predictions that the model makes on data that that the model has NOT been trained on. Doing a train and test on the same dataset will most likely have low out-of-sample accuracy, due to the likelihood of being over-fit.\n\nIt is important that our models have a high, out-of-sample accuracy, because the purpose of any model, of course, is to make correct predictions on unknown data. So how can we improve out-of-sample accuracy? One way is to use an evaluation approach called Train/Test Split.\nTrain/Test Split involves splitting the dataset into training and testing sets respectively, which are mutually exclusive. After which, you train with the training set and test with the testing set. \n\nThis will provide a more accurate evaluation on out-of-sample accuracy because the testing dataset is not part of the dataset that have been used to train the data. It is more realistic for real world problems.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification ","metadata":{}},{"cell_type":"markdown","source":"## K nearest neighbor (K-NN)","metadata":{}},{"cell_type":"markdown","source":"### Training\n\nLets start the algorithm with k=4 for now:","metadata":{}},{"cell_type":"code","source":"k = 4\n#Train Model and Predict  \nneigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nneigh","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predicting\nwe can use the model to predict the test set:","metadata":{}},{"cell_type":"code","source":"yhat = neigh.predict(X_test)\nyhat[0:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy evaluation\nIn multilabel classification, __accuracy classification score__ function computes subset accuracy. This function is equal to the jaccard_similarity_score function. Essentially, it calculates how match the actual labels and predicted labels are in the test set.","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics\nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(X_train)))\nprint(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### What about other K?\nK in KNN, is the number of nearest neighbors to examine. It is supposed to be specified by User. So, how we choose right K?\nThe general solution is to reserve a part of your data for testing the accuracy of the model. Then chose k =1, use the training part for modeling, and calculate the accuracy of prediction using all samples in your test set. Repeat this process, increasing the k, and see which k is the best for your model.\n\nWe can calucalte the accuracy of KNN for different Ks.","metadata":{}},{"cell_type":"code","source":"Ks = 30\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\nConfustionMx = [];\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat=neigh.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n\n    \n    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n\nmean_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plot  model accuracy  for Different number of Neighbors ","metadata":{}},{"cell_type":"code","source":"plt.plot(range(1,Ks),mean_acc,'g')\nplt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.legend(('Accuracy ', '+/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Nabors (K)')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print( \"The best accuracy was with\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}