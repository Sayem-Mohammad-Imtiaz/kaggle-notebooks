{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:12.246916Z","iopub.execute_input":"2021-07-16T14:05:12.247507Z","iopub.status.idle":"2021-07-16T14:05:12.25149Z","shell.execute_reply.started":"2021-07-16T14:05:12.247464Z","shell.execute_reply":"2021-07-16T14:05:12.250792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"markdown","source":"The main objective of the data set is to find who is diagnosed with cancer","metadata":{}},{"cell_type":"code","source":"df  = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:12.255804Z","iopub.execute_input":"2021-07-16T14:05:12.256137Z","iopub.status.idle":"2021-07-16T14:05:12.313635Z","shell.execute_reply.started":"2021-07-16T14:05:12.256105Z","shell.execute_reply":"2021-07-16T14:05:12.312622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finding the Null Values","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:12.315326Z","iopub.execute_input":"2021-07-16T14:05:12.315908Z","iopub.status.idle":"2021-07-16T14:05:12.323337Z","shell.execute_reply.started":"2021-07-16T14:05:12.315863Z","shell.execute_reply":"2021-07-16T14:05:12.322235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:12.32521Z","iopub.execute_input":"2021-07-16T14:05:12.325563Z","iopub.status.idle":"2021-07-16T14:05:12.335918Z","shell.execute_reply.started":"2021-07-16T14:05:12.325486Z","shell.execute_reply":"2021-07-16T14:05:12.335153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:12.337377Z","iopub.execute_input":"2021-07-16T14:05:12.337907Z","iopub.status.idle":"2021-07-16T14:05:12.352535Z","shell.execute_reply.started":"2021-07-16T14:05:12.337861Z","shell.execute_reply":"2021-07-16T14:05:12.351523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"unnamed 32 is a null value column so we can drop it","metadata":{}},{"cell_type":"code","source":"df['id'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:12.353722Z","iopub.execute_input":"2021-07-16T14:05:12.354054Z","iopub.status.idle":"2021-07-16T14:05:12.371716Z","shell.execute_reply.started":"2021-07-16T14:05:12.354023Z","shell.execute_reply":"2021-07-16T14:05:12.370827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we can see id column has 569 values and it only refers the id in of the row. that doen't impact the dataset","metadata":{}},{"cell_type":"markdown","source":"we are droping unwanted columns from the table","metadata":{}},{"cell_type":"code","source":"y = df.diagnosis\ndf.drop(['id','diagnosis','Unnamed: 32'],inplace=True,axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:12.373257Z","iopub.execute_input":"2021-07-16T14:05:12.373568Z","iopub.status.idle":"2021-07-16T14:05:12.379502Z","shell.execute_reply.started":"2021-07-16T14:05:12.373541Z","shell.execute_reply":"2021-07-16T14:05:12.378513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lets see the columns having categorical values","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:12.38077Z","iopub.execute_input":"2021-07-16T14:05:12.381332Z","iopub.status.idle":"2021-07-16T14:05:12.406498Z","shell.execute_reply.started":"2021-07-16T14:05:12.38129Z","shell.execute_reply":"2021-07-16T14:05:12.405746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we can see all are numerical value","metadata":{}},{"cell_type":"code","source":"y.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:12.408596Z","iopub.execute_input":"2021-07-16T14:05:12.409002Z","iopub.status.idle":"2021-07-16T14:05:12.416382Z","shell.execute_reply.started":"2021-07-16T14:05:12.408959Z","shell.execute_reply":"2021-07-16T14:05:12.415383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x=y,data =df)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:12.418439Z","iopub.execute_input":"2021-07-16T14:05:12.418806Z","iopub.status.idle":"2021-07-16T14:05:12.602436Z","shell.execute_reply.started":"2021-07-16T14:05:12.418777Z","shell.execute_reply":"2021-07-16T14:05:12.601447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"where (M = malignant, B = benign) Benign means not dangerous, Malignant means dangerous","metadata":{}},{"cell_type":"markdown","source":"we don't know much about this data, in order to understand the data, we have to do Feature Analysis, using mean,variance,standard deviation etc. \n\nUsually we check is there any correlation between the features,by analysing mean meadian of the values. \n\nfor this we can use voilin plot which helps to understand the data easily. but since there may be outliers we can use scaling meh=thods like standerdization, Min Max scaling to analyse the data ","metadata":{}},{"cell_type":"code","source":"## lets analyse for 10 features\ndiagnosis =y\ndata_stan = (df - df.mean())/df.std() #standardisation\ndata = pd.concat([y,data_stan.iloc[:,:10]],axis=1) #taking first 10 values\ndata =pd.melt(data,id_vars='diagnosis',var_name='features',value_name='value') # melting the values in a single column to plot easily\nplt.figure(figsize=(10,10))\nsns.violinplot(x ='features',y='value',hue='diagnosis',data=data, split=True,inner='quart')\nplt.xticks(rotation =90)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:12.60363Z","iopub.execute_input":"2021-07-16T14:05:12.603914Z","iopub.status.idle":"2021-07-16T14:05:13.383105Z","shell.execute_reply.started":"2021-07-16T14:05:12.603886Z","shell.execute_reply":"2021-07-16T14:05:13.38218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we can see in  fractal_dimension_mean meadian of Malignant and Bening are equal which says its not good for classification\n\nBut if you see other values there is huge diffrence in the median values of the categories which are good for classification","metadata":{}},{"cell_type":"code","source":"## to understand what pd.melt does\npd.melt(data,id_vars='diagnosis',var_name='features',value_name='value')","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:13.38436Z","iopub.execute_input":"2021-07-16T14:05:13.384671Z","iopub.status.idle":"2021-07-16T14:05:13.40641Z","shell.execute_reply.started":"2021-07-16T14:05:13.38464Z","shell.execute_reply":"2021-07-16T14:05:13.405331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# next 10 features\n\ndata = pd.concat([y,data_stan.iloc[:,10:20]],axis=1)\ndata =pd.melt(data,id_vars='diagnosis',var_name='features',value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x ='features',y='value',hue='diagnosis',data=data,split=True,inner='quart')\nplt.xticks(rotation =90)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:13.407979Z","iopub.execute_input":"2021-07-16T14:05:13.408369Z","iopub.status.idle":"2021-07-16T14:05:13.95852Z","shell.execute_reply.started":"2021-07-16T14:05:13.408336Z","shell.execute_reply":"2021-07-16T14:05:13.957477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see texture_se,smoothness_se, symmetry_se are not suitable for classification","metadata":{}},{"cell_type":"code","source":"# next 10 features\n\ndata = pd.concat([y,data_stan.iloc[:,20:31]],axis=1)\ndata =pd.melt(data,id_vars='diagnosis',var_name='features',value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x ='features',y='value',hue='diagnosis',data=data,split=True,inner='quart')\nplt.xticks(rotation =90)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:13.959885Z","iopub.execute_input":"2021-07-16T14:05:13.960383Z","iopub.status.idle":"2021-07-16T14:05:14.607219Z","shell.execute_reply.started":"2021-07-16T14:05:13.960228Z","shell.execute_reply":"2021-07-16T14:05:14.606445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also use box plot to analyse, but it is mostly used to analyse outliers in the features","metadata":{}},{"cell_type":"code","source":"data = pd.concat([y,data_stan.iloc[:,10:20]],axis=1)\ndata =pd.melt(data,id_vars='diagnosis',var_name='features',value_name='value')\nplt.figure(figsize=(10,10))\nsns.boxplot(x = 'features',y ='value',hue='diagnosis',data =data)\nplt.xticks(rotation = 90)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:14.608246Z","iopub.execute_input":"2021-07-16T14:05:14.608528Z","iopub.status.idle":"2021-07-16T14:05:15.251947Z","shell.execute_reply.started":"2021-07-16T14:05:14.6085Z","shell.execute_reply":"2021-07-16T14:05:15.250925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from above plot we can say radius_se,perimeter_se have similar box plot they might be coorelated, lets doa histogram to analyse these two data","metadata":{}},{"cell_type":"code","source":"## lets use Joint plot\n\nsns.jointplot(df.loc[:,'radius_se'],df.loc[:,'perimeter_se'],kind='reg')","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:15.253503Z","iopub.execute_input":"2021-07-16T14:05:15.253915Z","iopub.status.idle":"2021-07-16T14:05:16.113051Z","shell.execute_reply.started":"2021-07-16T14:05:15.253873Z","shell.execute_reply":"2021-07-16T14:05:16.112041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###  you can use pair plot for ploting major values\ng = sns.PairGrid(data_stan.iloc[:,10:15],diag_sharey=False)\n\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.map_upper(plt.scatter)\ng.map_diag(sns.kdeplot, lw=3)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:16.114279Z","iopub.execute_input":"2021-07-16T14:05:16.114562Z","iopub.status.idle":"2021-07-16T14:05:25.810556Z","shell.execute_reply.started":"2021-07-16T14:05:16.114536Z","shell.execute_reply":"2021-07-16T14:05:25.809511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we have seen many plots let use swam plot to get into next level","metadata":{}},{"cell_type":"code","source":"import time\nsns.set(style=\"whitegrid\", palette=\"muted\")\ndiag = y\ndata = df\ndata_std = (data-data.mean())/data.std()\ndata = pd.concat([diag,data_std.iloc[:,:10]],axis =1)\ndata = pd.melt(data,id_vars = 'diagnosis',value_name='value',var_name='features')\nplt.figure(figsize=(10,10))\ntic = time.time()\nsns.swarmplot(data=data,x='features',y='value',hue='diagnosis')\nplt.xticks(rotation=90)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:25.81185Z","iopub.execute_input":"2021-07-16T14:05:25.812189Z","iopub.status.idle":"2021-07-16T14:05:30.800349Z","shell.execute_reply.started":"2021-07-16T14:05:25.812131Z","shell.execute_reply":"2021-07-16T14:05:30.7995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from the above plot we can easily calssify area_mean,concave points_mean as M and D, but we can't classify fractal_dimension_mean, since D and M are scatterd all over the plot","metadata":{}},{"cell_type":"code","source":"# next 10 values\ndata = pd.concat([diag,data_std.iloc[:,10:20]],axis =1)\ndata = pd.melt(data,id_vars = 'diagnosis',value_name='value',var_name='features')\nplt.figure(figsize=(10,10))\ntic = time.time()\nsns.swarmplot(data=data,x='features',y='value',hue='diagnosis')\nplt.xticks(rotation=90)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:30.801336Z","iopub.execute_input":"2021-07-16T14:05:30.801716Z","iopub.status.idle":"2021-07-16T14:05:40.192091Z","shell.execute_reply.started":"2021-07-16T14:05:30.801688Z","shell.execute_reply":"2021-07-16T14:05:40.191157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"similary we can do for all the features to understand the features in the data","metadata":{}},{"cell_type":"code","source":"# next 10 values\ndata = pd.concat([diag,data_std.iloc[:,20:31]],axis =1)\ndata = pd.melt(data,id_vars = 'diagnosis',value_name='value',var_name='features')\nplt.figure(figsize=(10,10))\ntic = time.time()\nsns.swarmplot(data=data,x='features',y='value',hue='diagnosis')\nplt.xticks(rotation=90)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:40.193453Z","iopub.execute_input":"2021-07-16T14:05:40.19401Z","iopub.status.idle":"2021-07-16T14:05:45.632345Z","shell.execute_reply.started":"2021-07-16T14:05:40.193966Z","shell.execute_reply":"2021-07-16T14:05:45.631309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"in this radius_worst,perimeter_worst,area_worst,concavity_worst,concave points_worst can be easily classified, other features can't be classified \n","metadata":{}},{"cell_type":"markdown","source":"Let understand the coorelation by classic method using heatmap, we could have done this directly, but it is important to understand the data set before getting into feature selection  ","metadata":{}},{"cell_type":"markdown","source":"#### Feature Selection","metadata":{}},{"cell_type":"code","source":"f,ax = plt.subplots(figsize =(18,18))\nsns.heatmap(df.corr(),annot= True)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:45.633881Z","iopub.execute_input":"2021-07-16T14:05:45.634283Z","iopub.status.idle":"2021-07-16T14:05:50.968587Z","shell.execute_reply.started":"2021-07-16T14:05:45.634245Z","shell.execute_reply":"2021-07-16T14:05:50.967246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:50.970232Z","iopub.execute_input":"2021-07-16T14:05:50.97061Z","iopub.status.idle":"2021-07-16T14:05:50.978966Z","shell.execute_reply.started":"2021-07-16T14:05:50.970574Z","shell.execute_reply":"2021-07-16T14:05:50.977865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"values near by 1 indicates higly corelated so we can drop those values.\n\nfor eg:\n    > radius_mean,perimeter_mean,area_mean are highly corelated so taking only one feature radius_mean\n    \n    > radius_worst,perimeter_worst, area_worst are corelated so taking radius_worst alone\n    \n    \n    > radius_se,perimeter_se,area_se are corelated so taking radius_se alone\n    \n    \n    > texture_mean and texture_worst are corelated so taking texture_mean alone\n    \n    \n    > Compactness_mean, concavity_mean and concave points_mean are corelated so taking  Compactness_mean\n    \n    \n    > Compactness_worst, concavity_worst and concave points_worst are corelated so taking  Compactness_worst\n    \n    > concavity_se, concave points_se are co realated \n   \n    > fractal_dimension_se,compactness_se ","metadata":{}},{"cell_type":"code","source":"col_drop = ['perimeter_mean','area_mean','perimeter_worst', 'area_worst','perimeter_se','area_se','texture_worst','concavity_mean','concave points_mean','concavity_se','concavity_worst','fractal_dimension_se']","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:50.981003Z","iopub.execute_input":"2021-07-16T14:05:50.981596Z","iopub.status.idle":"2021-07-16T14:05:50.993868Z","shell.execute_reply.started":"2021-07-16T14:05:50.981551Z","shell.execute_reply":"2021-07-16T14:05:50.992952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(col_drop,axis=1,inplace= True)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:50.997532Z","iopub.execute_input":"2021-07-16T14:05:50.997841Z","iopub.status.idle":"2021-07-16T14:05:51.007386Z","shell.execute_reply.started":"2021-07-16T14:05:50.997812Z","shell.execute_reply":"2021-07-16T14:05:51.006426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:51.00947Z","iopub.execute_input":"2021-07-16T14:05:51.009822Z","iopub.status.idle":"2021-07-16T14:05:51.021686Z","shell.execute_reply.started":"2021-07-16T14:05:51.00979Z","shell.execute_reply":"2021-07-16T14:05:51.020695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f,ax =plt.subplots(figsize=(14,14))\nsns.heatmap(df.corr(),annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:51.022916Z","iopub.execute_input":"2021-07-16T14:05:51.02324Z","iopub.status.idle":"2021-07-16T14:05:53.216664Z","shell.execute_reply.started":"2021-07-16T14:05:51.023209Z","shell.execute_reply":"2021-07-16T14:05:53.215498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we can see radious worst is corelated with radious mean so removing one feature\n\n","metadata":{}},{"cell_type":"code","source":"df.drop('radius_worst',axis=1,inplace= True)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:53.218215Z","iopub.execute_input":"2021-07-16T14:05:53.218611Z","iopub.status.idle":"2021-07-16T14:05:53.225925Z","shell.execute_reply.started":"2021-07-16T14:05:53.218569Z","shell.execute_reply":"2021-07-16T14:05:53.225014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we have chossen our features but we dont know it is correct so lets do random forest","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:53.227176Z","iopub.execute_input":"2021-07-16T14:05:53.227565Z","iopub.status.idle":"2021-07-16T14:05:53.246343Z","shell.execute_reply.started":"2021-07-16T14:05:53.227523Z","shell.execute_reply":"2021-07-16T14:05:53.244934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\nx_tr,x_te,y_tr,y_te = train_test_split(df,y,test_size=0.3,random_state=42)\n\nclf = RandomForestClassifier()\nclf = clf.fit(x_tr,y_tr)\nac = accuracy_score(y_te,clf.predict(x_te))\ncm= confusion_matrix(y_te,clf.predict(x_te))\nprint(ac)\nsns.heatmap(cm,annot=True,fmt='d')","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:53.247853Z","iopub.execute_input":"2021-07-16T14:05:53.248292Z","iopub.status.idle":"2021-07-16T14:05:54.177428Z","shell.execute_reply.started":"2021-07-16T14:05:53.248248Z","shell.execute_reply":"2021-07-16T14:05:54.176477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we have got accuracy arround .95 which is good but we can optimize more, by choosing correct feature","metadata":{}},{"cell_type":"markdown","source":"### Univariate feature Selection.\n\nUnivariate Analysis means Selecting features by comparing single feature to the target and finding the result","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nsele_five = SelectKBest(chi2,k=5).fit(x_tr,y_tr)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:54.178909Z","iopub.execute_input":"2021-07-16T14:05:54.179325Z","iopub.status.idle":"2021-07-16T14:05:54.201169Z","shell.execute_reply.started":"2021-07-16T14:05:54.179283Z","shell.execute_reply":"2021-07-16T14:05:54.199986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sele_five.scores_)\nfor i in range(len(x_tr.columns)):\n    print(x_tr.columns[i], ':' ,sele_five.scores_[i])","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:54.202941Z","iopub.execute_input":"2021-07-16T14:05:54.203299Z","iopub.status.idle":"2021-07-16T14:05:54.216298Z","shell.execute_reply.started":"2021-07-16T14:05:54.203266Z","shell.execute_reply":"2021-07-16T14:05:54.215312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We select first 5 columns having max values ","metadata":{}},{"cell_type":"markdown","source":"radius_mean,texture_mean,radius_se, compactness_worst, concave points_worst","metadata":{}},{"cell_type":"markdown","source":"lets find score for this five values","metadata":{}},{"cell_type":"code","source":"X_tr = sele_five.transform(x_tr)\nX_te = sele_five.transform(x_te)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:54.21754Z","iopub.execute_input":"2021-07-16T14:05:54.217837Z","iopub.status.idle":"2021-07-16T14:05:54.226801Z","shell.execute_reply.started":"2021-07-16T14:05:54.217807Z","shell.execute_reply":"2021-07-16T14:05:54.225819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RDclass = RandomForestClassifier()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:54.228074Z","iopub.execute_input":"2021-07-16T14:05:54.228379Z","iopub.status.idle":"2021-07-16T14:05:54.238337Z","shell.execute_reply.started":"2021-07-16T14:05:54.22835Z","shell.execute_reply":"2021-07-16T14:05:54.237442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RDclass.fit(X_tr,y_tr)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:54.239212Z","iopub.execute_input":"2021-07-16T14:05:54.239463Z","iopub.status.idle":"2021-07-16T14:05:54.463291Z","shell.execute_reply.started":"2021-07-16T14:05:54.239439Z","shell.execute_reply":"2021-07-16T14:05:54.462325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(accuracy_score(y_te,RDclass.predict(X_te)))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:54.464709Z","iopub.execute_input":"2021-07-16T14:05:54.464991Z","iopub.status.idle":"2021-07-16T14:05:54.484905Z","shell.execute_reply.started":"2021-07-16T14:05:54.464963Z","shell.execute_reply":"2021-07-16T14:05:54.483895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we can see 5 features can give accuracy of 96%","metadata":{}},{"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_te,RDclass.predict(X_te)),annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:54.486003Z","iopub.execute_input":"2021-07-16T14:05:54.486379Z","iopub.status.idle":"2021-07-16T14:05:54.707897Z","shell.execute_reply.started":"2021-07-16T14:05:54.486302Z","shell.execute_reply":"2021-07-16T14:05:54.707105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above we can see that we have increased the accuracy from 95% to 96%.because of few wrong prediction. \n\nlet see another selection method.\n","metadata":{}},{"cell_type":"markdown","source":"### Recursive Feature Elemination Method\n\nThis methods uses the machine learning algoritham giiven by us, and assing weigghts to each feature, where smallest weights are cut down from the dataset by recursively going through all the features.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\n\nRF_class_2 = RandomForestClassifier()\nSelect_5 = RFE(estimator=RF_class_2,n_features_to_select=5,step=1)\nSelect_5 = Select_5.fit(x_tr,y_tr)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:54.708954Z","iopub.execute_input":"2021-07-16T14:05:54.70939Z","iopub.status.idle":"2021-07-16T14:05:57.629615Z","shell.execute_reply.started":"2021-07-16T14:05:54.709348Z","shell.execute_reply":"2021-07-16T14:05:57.628557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_tr.columns[Select_5.support_])","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:57.630699Z","iopub.execute_input":"2021-07-16T14:05:57.630982Z","iopub.status.idle":"2021-07-16T14:05:57.636108Z","shell.execute_reply.started":"2021-07-16T14:05:57.630956Z","shell.execute_reply":"2021-07-16T14:05:57.635048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see it has choosen the same features","metadata":{}},{"cell_type":"markdown","source":"From this we can be sure about our feature selection, that we have made correct choice.\n\nLets do Recursive feature elimination with cross validation \n\n### Recursive feature elimination with cross validation and random forest classification","metadata":{}},{"cell_type":"markdown","source":"In previous method we found how many features we needed most by our own choice, \n\nBut in this method we can find how many features can give best accuracy and the choice is made by cross validation","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFECV\n\nRF_Class_3 = RandomForestClassifier()\n\nSelect_RFECV = RFECV(estimator=RF_Class_3, step = 1, cv = 5, scoring='accuracy')\n\nSelect_RFECV.fit(x_tr,y_tr)\n\nprint(Select_RFECV.n_features_)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:05:57.637663Z","iopub.execute_input":"2021-07-16T14:05:57.638055Z","iopub.status.idle":"2021-07-16T14:06:17.81536Z","shell.execute_reply.started":"2021-07-16T14:05:57.638014Z","shell.execute_reply":"2021-07-16T14:06:17.814391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_tr.columns[Select_RFECV.support_])","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:06:17.816484Z","iopub.execute_input":"2021-07-16T14:06:17.816756Z","iopub.status.idle":"2021-07-16T14:06:17.82187Z","shell.execute_reply.started":"2021-07-16T14:06:17.81673Z","shell.execute_reply":"2021-07-16T14:06:17.820712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" We can see it selected  7 features out of all ","metadata":{}},{"cell_type":"markdown","source":"Lets Plot the cross validation scores ","metadata":{}},{"cell_type":"code","source":"plt.figure()\nplt.xlabel('Features')\nplt.ylabel('Accuracy')\nplt.plot(range(1,len(Select_RFECV.grid_scores_)+1),Select_RFECV.grid_scores_)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T14:06:17.823357Z","iopub.execute_input":"2021-07-16T14:06:17.823771Z","iopub.status.idle":"2021-07-16T14:06:18.065251Z","shell.execute_reply.started":"2021-07-16T14:06:17.823729Z","shell.execute_reply":"2021-07-16T14:06:18.064181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we can see at  7 the highest accuracy can be achived ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}