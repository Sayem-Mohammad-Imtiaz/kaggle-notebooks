{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Heart Failure Project"},{"metadata":{},"cell_type":"markdown","source":"Description: In this project we will use 12 factors ( among which age, blood pressure, smoking )  to predict heart failure in various patients. For this purspose, we will explore the Kaggle dataset \"Heart Failure Prediction\""},{"metadata":{},"cell_type":"markdown","source":"## 1. Loading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import necessary modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#load data\ndf=pd.read_csv(\"../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the beginning\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for missing values\ndf.isnull().sum()    \n\n# we notice that there are no missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# investigate data types\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice that the columns regarding __anemia__, __diabetes__, __high blood pressure__, __sex__, __smoking__ and __death__ should have categorical values instead of int64. We now change this.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#change data type for the indicated columns\n\ncat_cols=[\"anaemia\",\"diabetes\",\"high_blood_pressure\",\"sex\",\"smoking\",\"DEATH_EVENT\"]\n\ndf[cat_cols]=df[cat_cols].astype(\"category\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rename DEATH_EVENT column\ndf.rename({\"DEATH_EVENT\":\"death\"},axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check statistical measures of the data\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Correlation matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import matplotlib and seaborn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#create correlation matrix\ncor_mat=df.corr()\n\n# set figure size\nplt.figure(figsize=(9,7))\n\n#create the heatmap\nax=sns.heatmap(cor_mat,cmap=\"Blues\",linewidths=2, linecolor='black',annot=True)\nax.set_ylim([0,7])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The heatmap shows no strong correlation between any two variables. Furthermore, we can see that only the numerical variables have been included."},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Histograms"},{"metadata":{},"cell_type":"markdown","source":"We will use histograms to grasp an idea about the distribution of the numerical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"#set the style of the plots\nsns.set_style(\"darkgrid\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot age\nplt.figure(figsize=(6,6))\nsns.distplot(df.age,bins=10)\nplt.xlabel(\"Age of the patients\")\nplt.ylabel(\"Number of patients\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot creatinine_phosphokinase\nplt.figure(figsize=(6,6))\nsns.distplot(df.creatinine_phosphokinase,bins=10)\nplt.xlabel(\"Level of creatinine phosphokinase\")\nplt.ylabel(\"Number of patients\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot ejection_fraction\nplt.figure(figsize=(6,6))\nsns.distplot(df.ejection_fraction,bins=10)\nplt.xlabel(\"Ejection fraction\")\nplt.ylabel(\"Number of patients\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot platelets\nplt.figure(figsize=(6,6))\nsns.distplot(df.platelets,bins=10)\nplt.xlabel(\"Concentration of platelets\")\nplt.ylabel(\"Number of patients\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot serum_creatinine\nplt.figure(figsize=(6,6))\nsns.distplot(df.serum_creatinine,bins=10)\nplt.xlabel(\"Level of creatinine\")\nplt.ylabel(\"Number of patients\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot serum_sodium\nplt.figure(figsize=(6,6))\nsns.distplot(df.serum_sodium,bins=10)\nplt.xlabel(\"Level of sodium\")\nplt.ylabel(\"Number of patients\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3 Countplots"},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot anaemia for each sex\nplt.figure(figsize=(6,6))\nsns.set_style(\"darkgrid\")\nsns.catplot(x=\"anaemia\",hue=\"death\",data=df,kind=\"count\",col=\"sex\",palette=\"colorblind\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot diabetes for each sex\nplt.figure(figsize=(6,6))\nsns.set_style(\"darkgrid\")\nsns.catplot(x=\"diabetes\",hue=\"death\",data=df,kind=\"count\",col=\"sex\",palette=\"colorblind\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot smoking for each sex\nplt.figure(figsize=(6,6))\nsns.set_style(\"darkgrid\")\nsns.catplot(x=\"smoking\",hue=\"death\",data=df,kind=\"count\",col=\"sex\",palette=\"colorblind\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot high_blood_pressure for each sex\nplt.figure(figsize=(6,6))\nsns.set_style(\"darkgrid\")\nsns.catplot(x=\"high_blood_pressure\",hue=\"death\",data=df,kind=\"count\",col=\"sex\",palette=\"colorblind\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.4 Relational Plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"#the graphs in this section will be produced using plotly\nimport plotly.express as px","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot platelets vs creatinine_phosphokinase\nfig=px.scatter(df,x=\"platelets\",y=\"creatinine_phosphokinase\",color=\"death\",template=\"plotly_dark\",width=1000,height=500)\nfig.update_traces(marker=dict(size=12, line=dict(width=1,color='LightBlue')),selector=dict(mode='markers'))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#plot serum_creatinine vs serum_sodium\nfig=px.scatter(df,x=\"serum_creatinine\",y=\"serum_sodium\",color=\"death\",template=\"plotly_dark\",width=1000,height=500)\nfig.update_traces(marker=dict(size=12, line=dict(width=1,color='LightBlue')),selector=dict(mode='markers'))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#plot serum_creatinine vs creatinine_phosphokinase \nfig=px.scatter(df,x=\"serum_creatinine\",y=\"creatinine_phosphokinase\",color=\"death\",template=\"plotly_dark\",width=1000,height=500)\nfig.update_traces(marker=dict(size=12, line=dict(width=1,color='LightBlue')),selector=dict(mode='markers'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.5 Boxplots "},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# boxplot of the age variable \npx.box(df, x=\"smoking\", y=\"age\",width=1000,height=500,facet_col=\"high_blood_pressure\",color_discrete_sequence=['darkorchid']\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# boxplot of the ejection_fraction variable \npx.box(df, x=\"smoking\", y=\"ejection_fraction\",width=1000,height=500,color=\"high_blood_pressure\",\n       color_discrete_sequence=['crimson','yellow']\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#boxplot of ejection_fraction variable|\npx.box(df, x=\"smoking\", y=\"creatinine_phosphokinase\",width=1000,height=500,color=\"high_blood_pressure\",\n       color_discrete_sequence=['darkgreen','blue']\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# boxplot of the platelets variable \npx.box(df, x=\"smoking\", y=\"platelets\", color=\"sex\",width=1000,height=500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Model creation"},{"metadata":{},"cell_type":"markdown","source":"In this part we will first scale our data and split it into train and test sets. Next, we will use several classification algorithms and record thei performances:\n    1. K-Neighbours\n    2. Logistic Regression\n    3. Random Forest\n    4. AdaBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"#scaling our data\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler=StandardScaler()\n\ncolumns=[\"age\",\"creatinine_phosphokinase\",\"ejection_fraction\",\"platelets\",\"serum_creatinine\",\"serum_sodium\",\"time\"]\n\nfor column in columns:\n    df[column]=scaler.fit_transform(df[column].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split into train and test\n\nfrom sklearn.model_selection import train_test_split\n\nX=df.drop(\"death\",axis=1)\ny=df.death\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=10,test_size=0.3,stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import modules needed for hyperparameter tunning\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV,cross_val_score\n\n#import modules needed for performance check\nfrom sklearn.metrics import confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.1 K-Neighbors"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import the model\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#instantiate the classifier\nknn=KNeighborsClassifier()\n\n#define parameter range\nparam_grid_knn={\"n_neighbors\":range(1,15)}\n\n#run gridsearch \ncv_knn=GridSearchCV(knn,param_grid_knn,cv=10)\ncv_knn.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the best estimator\nbest_knn=cv_knn.best_estimator_\n\n#get the predicted classes\ny_pred_knn=best_knn.predict(X_test)\n\n#get the score for the test set\nprint(\"The score for the tuned KNN model is {}\".format(best_knn.score(X_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the metrics for the two classes\nprint(pd.DataFrame(classification_report(y_test,y_pred_knn,output_dict=True)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the heatmap corresponding to the confusion matrix\nax=sns.heatmap(confusion_matrix(y_test,y_pred_knn),annot=True,cmap=\"GnBu\")\nax.set_ylim([0,2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, KNeighbours model does not have high performance. By increasing the number of considered neighbours, the algorithm\ntends to classify all the labels as 0 due to class imbalance. This leads to high recall for the majority class (ie. __0__ ) and \nlow recall and precision for the minority class (ie. __1__)"},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import the model\nfrom sklearn.linear_model import LogisticRegression\n\n#instantiate the classifier\nlogreg=LogisticRegression(random_state=10,solver='liblinear')\n\n#define parameter range\nparam_grid_logreg={\"C\":np.logspace(-4, 4, 20),'penalty' : ['l1', 'l2']}\n\n#run gridsearch \ncv_logreg=GridSearchCV(logreg,param_grid_logreg,cv=10)\ncv_logreg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the best estimator\nbest_logreg=cv_logreg.best_estimator_\n\n#get the predicted classes\ny_pred_logreg=best_logreg.predict(X_test)\n\n#get the score for the test set\nprint(\"The score for the tuned Logistic Regression is {}\".format(best_logreg.score(X_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the metrics for the two classes\nprint(pd.DataFrame(classification_report(y_test,y_pred_logreg,output_dict=True)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the heatmap corresponding to the confusion matrix\nax=sns.heatmap(confusion_matrix(y_test,y_pred_logreg),annot=True,cmap=\"GnBu\")\nax.set_ylim([0,2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the Logistic Regression does a significantly better job. It has not only higher score for the test sets, but also \nhigher precision and recall for each class"},{"metadata":{},"cell_type":"markdown","source":"### 4.3 Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import the model\nfrom sklearn.ensemble import RandomForestClassifier\n\n#instantiate the classifier\nrf=RandomForestClassifier(random_state=10)\n\n#define parameter range\nparam_grid_rf={\"n_estimators\":range(100,401,50),\"criterion\":[\"gini\",\"entropy\"],\"max_depth\":range(2,10),\n               \"min_samples_leaf\":np.arange(0.1,0.51,0.1),\"max_features\":[\"auto\",\"sqrt\",\"log2\"]\n              }\n\n#run randomized search \ncv_rf=RandomizedSearchCV(rf,param_grid_rf,cv=10)\ncv_rf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the best estimator\nbest_rf=cv_rf.best_estimator_\n\n#get the predicted classes\ny_pred_rf=best_rf.predict(X_test)\n\n#get the score for the test set\nprint(\"The score for the tuned Random Forest is {}\".format(best_rf.score(X_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the metrics for the two classes\nprint(pd.DataFrame(classification_report(y_test,y_pred_rf,output_dict=True)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the heatmap corresponding to the confusion matrix\nax=sns.heatmap(confusion_matrix(y_test,y_pred_rf),annot=True,cmap=\"GnBu\")\nax.set_ylim([0,2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compared to the Logistic Regression model, the Random Forest has higher score for the test set. However, the recall for class __1__ is particularly low, which is confirmed by the confusion matrix."},{"metadata":{},"cell_type":"markdown","source":"### 4.4 AdaBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import the model\nfrom sklearn.ensemble import AdaBoostClassifier\n\n#instantiate the classifier\nada=AdaBoostClassifier(random_state=10)\n\n#range for number of estimators\nparam_grid_ada={\"n_estimators\":range(50,401,50)}\n\n#run the gridsearch\ncv_ada=GridSearchCV(ada,param_grid_ada,cv=3)\ncv_ada.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the best estimator\nbest_ada=cv_ada.best_estimator_\nbest_ada","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the predicted classes\ny_pred_ada=best_ada.predict(X_test)\n\n#get the score for the test set\nprint(\"The score for the tuned AdaBoost model is {}\".format(best_ada.score(X_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the best score we got so far for the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the metrics for the two classes\nprint(pd.DataFrame(classification_report(y_test,y_pred_ada,output_dict=True)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the heatmap corresponding to the confusion matrix\nax=sns.heatmap(confusion_matrix(y_test,y_pred_ada),annot=True,cmap=\"GnBu\")\nax.set_ylim([0,2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The AdaBoost model has fairly good results, having misclassified only a relatively small number of patients."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Final Comments"},{"metadata":{},"cell_type":"markdown","source":"It appears that AdaBooster has the highest performance compared to the other 3 models we fitted.\nHowever, apart from the KNeighbors algorithm, they all generated fairly good results, with accuracies for the test set of over 80%. \n\nThis type of methods could provide effective support to doctors for assessing the severity of the patient's condition. However, more sophisticated models should be buit for them to be reliable.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}