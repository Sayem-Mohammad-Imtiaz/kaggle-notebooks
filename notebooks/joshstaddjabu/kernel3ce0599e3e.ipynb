{"cells":[{"metadata":{"id":"XmuPocdrb_be"},"cell_type":"markdown","source":"### Agenda\n\n* Data Introduction\n    - About the Game\n    - About the Data\n    - Feature Engineering/Selection\n    - The Multicollinearity problem\n\n*  t-SNE\n    - observing bins vs avg win probability of features used for modeling\n    \n* Modeling GBC & MLP\n    - GBC model specifications\n    - MLP model specifications\n    - Hyperpermeter Tuning (MLP)\n\n* Model Evaluation\n    - ROC-AUC\n    - Classification Matrix\n    - Confusion Matrix  \n\n* Practical Uses (It's just a game LOL)\n\n* Final notes future work needed to make practical uses feasible & future work ideas","execution_count":null},{"metadata":{"id":"r397RL62b_bf"},"cell_type":"markdown","source":"### Data Introduction\n\n## About the game\nLeague of Legends (LoL) is a multiplayer online battle arena video game developed and published by Riot Games. In League of Legends, players assume the role of a \"champion\" with unique abilities and battle against a team of other players- or computer-controlled champions. The goal is usually to destroy the opposing team's \"Nexus\", a structure that lies at the heart of a base protected by defensive structures, although other distinct game modes exist as well with varying objectives, rules, and maps. Each League of Legends match is discrete, with all champions starting off relatively weak but increasing in strength by accumulating items and experience over the course of the game.\n\nhttps://www.youtube.com/watch?v=mwERJ6qJPuc\n\nAbout the Data\n\nSo what we have is a ten-minute snapshot of scalars reflecting an aspect of team performance and whether that team won or not. \n\nTarget\n\nOur \"bluewins\" variable is the target and it is a binary feature. The overall goal of this analysis is to extract informative indicators that may lead the blue team to win or lose a match.","execution_count":null},{"metadata":{"id":"dDGlnwmVb_bg","trusted":false},"cell_type":"code","source":"# Importing Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\nfrom sklearn.cluster import KMeans\nfrom sklearn import preprocessing\nfrom sklearn import decomposition\nfrom sklearn.preprocessing import normalize, StandardScaler\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.feature_selection import SelectKBest, f_classif, chi2\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.manifold import TSNE\nimport scipy\nimport time\nimport tensorflow as tf\nfrom tensorflow import keras\nimport datetime, os, io\nfrom google.colab import files\n%matplotlib inline\n%load_ext tensorboard","execution_count":null,"outputs":[]},{"metadata":{"id":"cgBdR5Nogh6x"},"cell_type":"markdown","source":"### The Data \n\nDistribution Chart\nWe have mixed data types here discreet and continuous. There are noticeable skew and kurtosis aspects of these distributions that vary from what I'd expect to see with a normal distribution, but overall it looks pretty good to me. Not too much data cleaning was needed which was nice, but this dataset presented its own problems which I will glance over later. \n\n### Feature Engineering\n\nFor feature engineering, I created 15 different metrics and I also experimented with team acquisition ratios of different buffs based on what is obtainable within the first ten minutes of a game.","execution_count":null},{"metadata":{"id":"JPhFUdwtb_bj","outputId":"710cbf0e-2d22-4d9c-fd5e-c9e2f18a20ec","trusted":false},"cell_type":"code","source":"# Uploading data\nuploaded = files.upload()\ndf_og = pd.read_csv(io.BytesIO(uploaded['high_diamond_ranked_10min.csv']))\ndf_og.columns = map(lambda x:x.lower(), df_og.columns)\ndf_og['gameid'] = df_og['gameid'].astype(str)\nnumeric_columns = df_og.select_dtypes(['int64', 'float64']).columns\nFILL_LIST = []\nfor cols in df_og[:]:\n    if cols in numeric_columns:\n        FILL_LIST.append(cols)\nplt.figure(figsize=(35, 95))\nplt.subplots_adjust(hspace=1, wspace=1)\nfor i, col in enumerate(FILL_LIST):\n    try:\n        plt.subplot(len(FILL_LIST), 7, i+1)\n        sns.distplot(df_og[col], kde=False)\n        plt.title(col)\n    except TypeError:\n        pass\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"id":"vh6UjyMUhaRA","trusted":false},"cell_type":"code","source":"# Just incase\ndf1 = df_og.copy()\n\n# Feature Engineering comparative team performance variables\ndf1['bcspermin_diff'] = df1['bluecspermin'] - df1['redcspermin']\ndf1['btotexp_diff'] = df1['bluetotalexperience'] - df1['redtotalexperience']\ndf1['bavglvl_diff'] = df1['blueavglevel'] - df1['redavglevel']\ndf1['bwardsplaced_diff'] = df1['bluewardsplaced'] - df1['redwardsplaced']\ndf1['bwardsdestroyed_diff'] = df1['bluewardsdestroyed'] - df1['redwardsdestroyed']\ndf1['btowerdeaths_diff'] = df1['bluetowersdestroyed'] - df1['redtowersdestroyed']\ndf1['bkills_diff'] = df1['bluekills'] - df1['redkills']\ndf1['bdeaths_diff'] = df1['bluedeaths'] - df1['reddeaths']\ndf1['bgold_per_min_diff'] = df1['bluegoldpermin'] - df1['redgoldpermin']\ndf1['belite_diff'] = df1['blueelitemonsters'] - df1['redelitemonsters']\ndf1['bdrag_diff'] = df1['bluedragons'] - df1['reddragons']\ndf1['bheralds_diff'] = df1['blueheralds'] - df1['redheralds']\ndf1['blaneminions_diff'] = df1['bluetotalminionskilled'] - df1['redtotalminionskilled']\ndf1['bjgmionions_diff'] = df1['bluetotaljungleminionskilled'] - df1['redtotaljungleminionskilled']\ndf1['bteamtotminionsdiff'] = (df1['blueelitemonsters'] + df1['bluedragons'] + df1['blueheralds'] + df1['bluetotalminionskilled'] + df1['bluetotaljungleminionskilled']) - (df1['redelitemonsters'] + df1['reddragons'] + df1['redheralds'] + df1['redtotalminionskilled'] + df1['redtotaljungleminionskilled'])\n\ndf_ana = df1.loc[:, ['gameid', 'bluewins', 'bluegolddiff', 'blueexperiencediff', 'bkills_diff',\n                     'bavglvl_diff', 'bluegoldpermin', 'bluetotalexperience',\n                     'blueavglevel', 'bteamtotminionsdiff', 'bluekills',\n                     'bcspermin_diff', 'blaneminions_diff', 'bluetotalgold']]","execution_count":null,"outputs":[]},{"metadata":{"id":"bBIcGQKzg2q3"},"cell_type":"markdown","source":"### Features Selected \n\nI made use of skLearn's SelectKBest and Anova test functions to help me pinpoint the best variables to proceed with.\n\n'bluewins' - binary target\n\n'bluegolddiff' - the difference in accumulated gold\n\n'blueexperiencediff' - the difference in accumulated experience\n\n'bluegoldpermin' - gold accumulated per minute\n\n'bluetotalexperience' - total experienced gained\n\n'bteamtotminionsdiff' - blue team total minions including (elite, rift, and jungle minions) killed compared to red team killed\n\n'blaneminions_diff' - blue team total lane minion difference\n\n'bluetotalgold' - blue total gold acquired\n","execution_count":null},{"metadata":{"id":"rFOhjwGZb_bm","outputId":"8be13bed-e245-4a14-c2a5-ef1ef958f104","trusted":false},"cell_type":"code","source":"# Uploading data\nuploaded = files.upload()\ndf = pd.read_csv(io.BytesIO(uploaded['data_chi3_anova9.csv']))\ndf = df.drop(['Unnamed: 0'], axis = 1)\ndf.columns = map(lambda x:x.lower(), df.columns)\ndf['gameid'] = df['gameid'].astype(str)\nnumeric_columns = df.select_dtypes(['int64', 'float64']).columns\nFILL_LIST = []\nfor cols in df[:]:\n    if cols in numeric_columns:\n        FILL_LIST.append(cols)\nplt.figure(figsize=(35, 95))\nplt.subplots_adjust(hspace=1, wspace=1)\nfor i, col in enumerate(FILL_LIST):\n    try:\n        plt.subplot(len(FILL_LIST), 7, i+1)\n        sns.distplot(df[col], kde=False)\n        plt.title(col)\n    except TypeError:\n        pass\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"id":"ZgCq4fXDhxkc"},"cell_type":"markdown","source":"### Novel Data Problem\n\nEvery dataset has it's quirks and this one is no different. Interestingly enough there is a persistence of multicollinearity with the features that correlate with the target considering correlation at a minimum of 0.4. It is for this reason that Principle Component Analysis piped into K-Means could not produce definitive separability between games where the blue team won or lost the match.\n","execution_count":null},{"metadata":{"id":"NyXnUr88b_bv","outputId":"3dea1d73-cd1d-4f1f-8233-a0a2fd1ce2e8","trusted":false},"cell_type":"code","source":"mask = np.zeros_like(df_ana.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(dpi=155)\nsns.heatmap(abs(df_ana.corr()), mask=mask, vmin=0.4, cmap='gist_heat_r')\ndf_ana.corr()['bluewins'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"-jBr_ZY2jH9z"},"cell_type":"markdown","source":"The below cells goes from X & y to implementing and plotting t-SNE.","execution_count":null},{"metadata":{"id":"tX5zap0gb_b5","outputId":"f49b9cbc-b53c-4a85-9cd1-d73b82fdda46","trusted":false},"cell_type":"code","source":"X = df.iloc[:, 2:]\ny = df['bluewins'].values.ravel()\nscaler = preprocessing.StandardScaler()\nX_stand = scaler.fit_transform(X)\nX_stand_df = pd.DataFrame(X_stand, columns=X.columns)\nfeat_cols = [ X_stand_df.columns[i] for i in range(X_stand_df.shape[1]) ]\ndata2 = pd.DataFrame(X_stand_df,columns=feat_cols)\ndata2['y'] = y\ndata2['label'] = data2['y'].apply(lambda i: str(i))\n\n# For reproducability of the results\nnp.random.seed(57)\n\n# random observation selection\nrndperm = np.random.permutation(data2.shape[0])\n\n# Number of observations to use during cluster selection\nN = 9000\n\n# dataframe obj holding randomly selected data\ndata_subset = data2.loc[rndperm[:N],:].copy()\n\n# data values obj from dataframe\ndf_subset = data_subset[feat_cols].values\n\ntime_start = time.time()\n\ntsne = TSNE(n_components=2,\n            verbose=1,\n            n_iter=1000,\n            perplexity=30,\n            learning_rate=300,\n            early_exaggeration=12)\n\ntsne_results = tsne.fit_transform(data_subset)\n\nprint('t-SNE done! Time elasped: {} seconds'.format(time.time() - time_start))\n\ndata_subset['tsne-2d-one'] = tsne_results[:,0]\ndata_subset['tsne-2d-two'] = tsne_results[:,1]","execution_count":null,"outputs":[]},{"metadata":{"id":"quG3H0Emi4Za"},"cell_type":"markdown","source":"### t-SNE\n\nThis is when I made use of the t stochastic neighbor embedded model created by Laurens van der Maaten and Geoffrey Hinton, to explore/visualize dimensional separability between games where the blue team won or lost. This is the final product of that exploration. t-SNE makes use of the student t-test to compare variables in a high and low dimensional space using distance to assume similarity. \n\nIt's easiest to think of every point on this plot as a game with the most similar results of that ten-minute snapshot clustered together and the point's color represents that game result. Looking at this two-dimensional plot and considering the four quadrants of it we can observe that a loss likely to occur if team performance in the first ten minutes of the game is more similar to the games in quadrants I & III vs II & IV, where it appears you are more likely to win as the blue team. There appears to be very little intermingling here of the red and blue points, which is good to see and fun to work through. Seeing is believing and now that we can observe some sort of boundary, we can take a peek at how each of the selected variables impacts where are likely to at the end of a game. ","execution_count":null},{"metadata":{"id":"SfHHM_gib_b9","outputId":"f9a307ea-e7b3-46a5-b085-ee82193272d8","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(16,10))\nsns.scatterplot(\n    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n    hue=\"y\",\n    palette=sns.color_palette(\"hls\", 2),\n    data=data_subset,\n    legend=\"full\",\n    alpha=0.3)","execution_count":null,"outputs":[]},{"metadata":{"id":"ORaXaZN7b_cC","trusted":false},"cell_type":"code","source":"# Binning features in new Dataframes for Visualizing\ndf_bins = df.copy()\ndf_bins['bluegolddiff_bins'] = pd.qcut(df_bins['bluegolddiff'], q=10)\npw_gd = df_bins.groupby('bluegolddiff_bins')['bluewins'].mean()\npw_gd = pw_gd.reset_index()\npw_gd.columns = ['bluegolddiff_bins', 'wp_bluegolddiff']\npw_gd['Delta'] = pw_gd['wp_bluegolddiff'].shift(-1) - pw_gd['wp_bluegolddiff']\n\ndf_bins['blueexperiencediff_bins'] = pd.qcut(df_bins['blueexperiencediff'], q=10)\npw_be = df_bins.groupby('blueexperiencediff_bins')['bluewins'].mean()\npw_be = pw_be.reset_index()\npw_be.columns = ['blueexperiencediff_bins', 'wp_blueexperiencediff']\npw_be['Delta'] = pw_be['wp_blueexperiencediff'].shift(-1) - pw_be['wp_blueexperiencediff']\n\ndf_bins['bluegoldpermin_bins'] = pd.qcut(df_bins['bluegoldpermin'], q=10)\npw_gm = df_bins.groupby('bluegoldpermin_bins')['bluewins'].mean()\npw_gm = pw_gm.reset_index()\npw_gm.columns = ['bluegoldpermin_bins', 'wp_bluegoldpermin']\npw_gm['Delta'] = pw_gm['wp_bluegoldpermin'].shift(-1) - pw_gm['wp_bluegoldpermin']\n\ndf_bins['bluetotalexperience_bins'] = pd.qcut(df_bins['bluetotalexperience'], q=10)\npw_te = df_bins.groupby('bluetotalexperience_bins')['bluewins'].mean()\npw_te = pw_te.reset_index()\npw_te.columns = ['bluetotalexperience_bins', 'wp_bluetotalexperience']\npw_te['Delta'] = pw_te['wp_bluetotalexperience'].shift(-1) - pw_te['wp_bluetotalexperience']\n\ndf_bins['bteamtotminionsdiff_bins'] = pd.qcut(df_bins['bteamtotminionsdiff'], q=10)\npw_tm = df_bins.groupby('bteamtotminionsdiff_bins')['bluewins'].mean()\npw_tm = pw_tm.reset_index()\npw_tm.columns = ['bteamtotminionsdiff_bins', 'wp_bteamtotminionsdiff']\npw_tm['Delta'] = pw_tm['wp_bteamtotminionsdiff'].shift(-1) - pw_tm['wp_bteamtotminionsdiff']\n\ndf_bins['blaneminions_diff_bins'] = pd.qcut(df_bins['blaneminions_diff'], q=10)\npw_lm = df_bins.groupby('blaneminions_diff_bins')['bluewins'].mean()\npw_lm = pw_lm.reset_index()\npw_lm.columns = ['blaneminions_diff_bins', 'wp_blaneminions_diff']\npw_lm['Delta'] = pw_lm['wp_blaneminions_diff'].shift(-1) - pw_lm['wp_blaneminions_diff']\n\ndf_bins['bluetotalgold_bins'] = pd.qcut(df_bins['bluetotalgold'], q=10)\npw_tg = df_bins.groupby('bluetotalgold_bins')['bluewins'].mean()\npw_tg = pw_tg.reset_index()\npw_tg.columns = ['bluetotalgold_bins', 'wp_bluetotalgold']\npw_tg['Delta'] = pw_tg['wp_bluetotalgold'].shift(-1) - pw_tg['wp_bluetotalgold']","execution_count":null,"outputs":[]},{"metadata":{"id":"bB3UuqPxSuNH"},"cell_type":"markdown","source":"### Binning\n\nWhat we are going to see in the next is a series of histograms accompanied by a DataFrame that expresses the average win percentage relating to One of ten bins that feature has been condensed to. As we traverse through each bin, we expect the next bin relates to better performance and reasonably contributes to a higher average win percentage. I also have a delta column that makes it easy to see the average percent change per bin. Keep in mind if we are looking at a different metric than the blue starts the game closer to the middle bins. If not, then the blue team would start the game at bin zero. What I want to guide your eye towards and what I was on the lookout for here is; Where could my team’s effort be spent most effectively up to that ten-minute mark in the game and how can we detect clear wins or losses? (The option becomes available to surrender at 14 mins) So we are looking for the most informative bin changes.  With that in mind let’s proceed.\n","execution_count":null},{"metadata":{"id":"k4zJxeCLjy9-"},"cell_type":"markdown","source":"#### Blue Gold Diff\n\nThe biggest deltas are realized when leaving bins 0 and 7. So ideally the would be targeting the eighth bin starting at the 4th bin, but getting to the 0th bin pretty much means you’re going to get wrecked. Keep in mind that the bins on the end tend to have longer tails, but the fact remains if that’s where you are you’re not doing well.\n","execution_count":null},{"metadata":{"id":"VkbFwme6b_cE","outputId":"6b6343ed-22c5-4c71-e3fe-4b92fd553ccc","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.barplot(x='bluegolddiff_bins', y='wp_bluegolddiff', data=pw_gd)\npw_gd","execution_count":null,"outputs":[]},{"metadata":{"id":"5tDo6EnNj4D5"},"cell_type":"markdown","source":"#### Blue experience Diff\n\nThe goal is pretty similar here starting in the fourth bin and trying to get to the 9th bin while avoiding the 0th bin. \n","execution_count":null},{"metadata":{"id":"C41Px-C7b_cJ","outputId":"a0de46c0-ca02-4c8d-da9b-7906ab701ef8","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.barplot(x='blueexperiencediff_bins', y='wp_blueexperiencediff', data=pw_be)\npw_be","execution_count":null,"outputs":[]},{"metadata":{"id":"rNlILY5hkDbK"},"cell_type":"markdown","source":"#### Blue gold per min\n\nRegarding this bin, its best get out of that first bin, i.e accumulate more than 1601 gold per min or else the game becomes a lot harder.\n","execution_count":null},{"metadata":{"id":"0ByQrWY4b_cN","outputId":"5a68c35e-2c95-4cdf-8c59-ee89486ca4f9","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.barplot(x='bluegoldpermin_bins', y='wp_bluegoldpermin', data=pw_gm)\npw_gm","execution_count":null,"outputs":[]},{"metadata":{"id":"XQXXDKm3kMXp"},"cell_type":"markdown","source":"#### Blue total exp\n\nThis one is similar to the last. The 0th, 8th, 9th bins are key determinants to where your team might end up. \n","execution_count":null},{"metadata":{"id":"HRU-oOcMb_cR","outputId":"5436e458-cd85-4a10-eb27-be8cecf714bd","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.barplot(x='bluetotalexperience_bins', y='wp_bluetotalexperience', data=pw_te)\npw_te","execution_count":null,"outputs":[]},{"metadata":{"id":"wh3-Nai1kSJF"},"cell_type":"markdown","source":"#### Blue team total minions\n\nThis one is interesting in that even the 0th bin still leaves the team with a 20% chance of winning the match on average. So it might be a good idea to pick team battles for lanes in a way that minimizes minions lost and places the team effort into something more fruitful\n","execution_count":null},{"metadata":{"id":"6pJZew_wb_cV","outputId":"423455c3-dca5-4325-934a-087f58d32383","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.barplot(x='bteamtotminionsdiff_bins', y='wp_bteamtotminionsdiff', data=pw_tm)\npw_tm","execution_count":null,"outputs":[]},{"metadata":{"id":"RtaVX7M9kYLl"},"cell_type":"markdown","source":"#### Blue lane minion difference\n\nSomewhat of the same deal here\n","execution_count":null},{"metadata":{"id":"7CHY3WnYb_ca","outputId":"16e1fa4f-9de0-4221-d81e-cd5dfca26287","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.barplot(x='blaneminions_diff_bins', y='wp_blaneminions_diff', data=pw_lm)\npw_lm","execution_count":null,"outputs":[]},{"metadata":{"id":"mvcnHgedkcpG"},"cell_type":"markdown","source":"#### Blue total gold\n\nThe difference between bins 0 and 1 are the biggest we observed so far. Just being in bin 1 give you an average 27-28 percent chance. 12 point bonus on reaching that last bin too. A feature that I would definitely watch all game, something that is always on display during Pro-level games.\n","execution_count":null},{"metadata":{"id":"xr5L9eR9b_cd","outputId":"b79c649e-a30f-4663-c03e-77b83f90a94f","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.barplot(x='bluetotalgold_bins', y='wp_bluetotalgold', data=pw_tg)\npw_tg","execution_count":null,"outputs":[]},{"metadata":{"id":"T2Z98O-weLgN"},"cell_type":"markdown","source":"### Modeling \n\nGBC\nSo I choose to use the GBC arbitrarily as it’s preferred one of mine and I thought based on the information and complexity. I Could have tried other models, but this model is more of a benchmark model while the MLP is really the star of the show. Here’s what hyper-parameters are set to and I used GridSearchCV in a different notebook. \n","execution_count":null},{"metadata":{"id":"8cYCd5OTb_cj","trusted":false},"cell_type":"code","source":"X = df.iloc[:, 2:]\nscaler = StandardScaler()\nX_stand = scaler.fit_transform(X)\nX_df = pd.DataFrame(X_stand, columns=X.columns)\ny = df.iloc[:, 1:2].values.ravel()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZkLYqmGjsZvf","trusted":false},"cell_type":"code","source":"# show model hyperameters\ngbc_tuned = GradientBoostingClassifier(learning_rate=0.01,\n                                       n_estimators=300,\n                                       subsample=0.1,\n                                       min_samples_leaf=2,\n                                       min_samples_split=2, \n                                       max_depth=4,\n                                       max_features=6,\n                                       min_impurity_decrease=0.1)","execution_count":null,"outputs":[]},{"metadata":{"id":"eg7a5bayEfF8"},"cell_type":"markdown","source":"#### GBC Variable Importance Chart\n\nI personally love to see the relative variable feature importance in supervised methods because I lose visibility as a cost of doing business in the universe of unsupervised learning.","execution_count":null},{"metadata":{"id":"tX8pSBqA0ZHA","outputId":"60aaeed2-cb14-457e-d68d-0088134cadf4","trusted":false},"cell_type":"code","source":"gbc_tuned.fit(X_train, y_train)\n# we are making predictions here\ny_preds_train = gbc_tuned.predict(X_train)\ny_preds_test = gbc_tuned.predict(X_test)\nfeature_importance = gbc_tuned.feature_importances_\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, X.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')","execution_count":null,"outputs":[]},{"metadata":{"id":"H47A1VXdE8c1"},"cell_type":"markdown","source":"#### How Does the MLP work?\n\nThe moment at least I have been waiting for is here, the star of the show The MLP! I know the epoch accuracy and loss visualized in this manner looks unsavory, but if you focus your eye to the y-axis of this chart you’ll notice the model bounces in between .03 points quite sporadically. \n\nWhat I built was a DeepNet with 2 hidden layers. There are 7 variables that are feed into a node on their own which allows us to start feeding forward information. These 7 nodes feed the next layer of 70 nodes this input. The DeepNet needs to start with some weights and then iteratively update them to reduce loss. The kernel initializer provides a function to use for initializing the weights applied to the information received from the previous node, before applying the activation function. After that is done the same process is repeated in the second layer with different hyperparameters trained in a different notebook.|","execution_count":null},{"metadata":{"id":"Gbj_2f2N0ktu","outputId":"2fdf2cad-c7ab-4fa6-ca42-e165226e2803","trusted":false},"cell_type":"code","source":"%%time\ndef build_mlp():\n  model = keras.Sequential([keras.layers.Dense(70, input_shape=(None, 7), kernel_initializer='glorot_normal',\n                                               activation='sigmoid',),\n                            keras.layers.Dense(70, activation='selu', kernel_initializer='normal'),\n                            keras.layers.Dense(1, activation='sigmoid', kernel_initializer='he_normal')])\n  optomizer = tf.keras.optimizers.RMSprop(learning_rate=0.001, momentum=0.1)\n  model.compile(optimizer=optomizer, loss='binary_crossentropy', metrics=['accuracy'])\n  return model\n\nlogdir9 = os.path.join(\"logs23\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ntensorboard_callback = tf.keras.callbacks.TensorBoard(logdir9, histogram_freq=1)\n  \nmlp_model = build_mlp()\nmlp_model.fit(X_train, y_train, batch_size=15, epochs=100, verbose=0, callbacks=[tensorboard_callback])\n\ny_mlp = mlp_model.predict(X_test).ravel()\n%tensorboard --logdir logs23","execution_count":null,"outputs":[]},{"metadata":{"id":"iY4qcg8lFFkX"},"cell_type":"markdown","source":"#### Hyperparameter Tuning\n\nHere’s cell construction that leads to the model provided in this report. Most of the parameters specified were chosen by a cell like this. Doing multiple tests across each parameter to find the best one for each layer manually. Some parameters were chosen with other parameters during Grid Searching in a way that made sense to me. The order in which parameters were tested was as follows: optimization algorithm, learning rate, and momentum, kernel initializer, Activation Function, Dropout rate and AlphaDropout for the second level because of the synergy created with the ‘selu’ activation function used, batch size and epochs, finally neurons per layer, excluding the last layer.\n\nAfter doing that for a while, I got an idea to compose a cell that could help me uncover possible blind spots to the previous way of training by essentially testing more scenarios at once.","execution_count":null},{"metadata":{"id":"_pVNrAPFFmgE","trusted":false},"cell_type":"code","source":"## Grid to determine each layer's activation function\ndef create_model(activation='softmax'):\n  model = keras.Sequential([keras.layers.Dense(70, input_shape=(None, 7), activation='sigmoid'),\n                            keras.layers.Dense(70, activation='selu'),\n                            keras.layers.Dense(1, activation=activation)])\n  opt = tf.keras.optimizers.Adagrad(learning_rate=0.005, initial_accumulator_value=0.1, epsilon=1e-07)\n  model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n  return model\n\n# fix random seed for reproductibility\nseed = 7\nnp.random.seed(seed)\n# create model\nmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n\n# defining grid search parameters\nactivation = [\"tanh\", \"sigmoid\", 'relu', 'selu', 'softsign', 'softmax']\nparam_grid = dict(activation=activation)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X_train, y_train)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n  print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"id":"lfNsEl42GOiW"},"cell_type":"markdown","source":"#### Hyperparameter Tuning continued…\n\nHere’s overall where I landed. This didn’t work as well as I thought. I had a hard time quantifying the computational limits of my environment and keeping the model parameter options within that limit. To be honest, there’s probably a better way to compose this algorithm. Perhaps in a fashion that would provide more consistent results (referring to printing a result before the kernel died) and/or quantifying my problem in a solvable manner given the time constraints.\n\nThe last layer receives the output from the second hidden layer and returns an output (for each game the model is used on this output is a number between 0 and 1) that I used for predictions. Should also note that there was no class imbalance, to begin with. \n","execution_count":null},{"metadata":{"id":"V4FI5Q7jF8IT","trusted":false},"cell_type":"code","source":"# STAR GRID CELL\ndef create_model(learning_rate=0.001, momentum=0.0, optimizer='SGD',\n                 loss='binary_crossentropy', init_mode='uniform', neurons=1,\n                 activation='sigmoid', dropout_rate=0.0, weight_constraint=0):\n  model = keras.Sequential([keras.layers.Dense(neurons, input_shape=(None, 7), kernel_initializer='glorot_normal', activation=activation, kernel_constraint=maxnorm(weight_constraint),),\n                            keras.layers.Dropout(dropout_rate),\n                            keras.layers.Dense(neurons, activation=activation, kernel_initializer='normal'),\n                            keras.layers.Dropout(dropout_rate),\n                            keras.layers.Dense(1, activation='sigmoid', kernel_initializer='he_normal')])\n  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n  return model\n\nmodel = KerasClassifier(build_fn=create_model, verbose=0)\n\n# define the grid search parameters\nweight_constraint = [0, 2, 4, 6]\ndropout_rate = [0.0, 0.2, 0.4, 0.6, 0.8]\nactivation = [\"tanh\", \"sigmoid\", 'relu', 'softmax'] # could add 'selu'\ninit_mode = ['uniform', 'normal', 'glorot_normal', 'he_normal', 'he_uniform'] # ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\noptimizer = ['RMSprop']\nloss = ['binary_crossentropy']\nneurons = [70, 85, 100]\nlearning_rate = [0.001, 0.01, 0.05, 0.1]\nmomentum = [.0, .2, .4]\nbatch_size = [14, 34, 72]\nepochs = [200]\n\n# Create Parameter Grid Object & Train Model\nparam_grid = dict(learning_rate=learning_rate, momentum=momentum,\n                  batch_size=batch_size, epochs=epochs,\n                  loss=loss, optimizer=optimizer,\n                  activation=activation,\n                  weight_constraint=weight_constraint,\n                  dropout_rate=dropout_rate)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X_train, y_train)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"id":"nY2kR4fhFSyU","outputId":"4433877c-ab21-4726-ca05-1ce65b5c99ca","trusted":false},"cell_type":"code","source":"df['bluewins'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"cdaEsWlNGf38"},"cell_type":"markdown","source":"### Model Evaluation ROC-AUC\n\nHere I will be comparing these model scores discussing the pros and cons to each.\n\nReceiver Operating Characteristic (ROC) curves are a measure of a classifier’s predictive quality that compares and visualizes the tradeoff between the models’ sensitivity and specificity. The ROC curve displays the true positive rate on the Y-axis and the false positive rate on the X-axis on both a global average and a per-class basis. The ideal point is therefore the top-left corner of the plot: false positives are zero and true positives are one.\n\nThis leads to another metric, area under the curve (AUC), a computation of the relationship between false positives and true positives. The higher the AUC, the better the model generally is. The highest score possible is 1, which would reflect a perfect score.\n\nSo the MLP is doing better than the GBC on both metrics, but not by substantially more. The cost of exploring and developing the MLP was high in technical complexity for me personally, whereas the GBC is easier to reason about. ","execution_count":null},{"metadata":{"id":"62xv03OQ06aj","outputId":"4fcb5253-2cf3-4c08-f87b-c770e55a8eaa","trusted":false},"cell_type":"code","source":"fpr_gbc, tpr_gbc, thresholds_gbc = roc_curve(y_test, y_preds_test)\nauc_gbc = auc(fpr_gbc, tpr_gbc)\n\nfpr_mlp, tpr_mlp, thresholds_mlp = roc_curve(y_test, y_mlp)\nauc_mlp = auc(fpr_mlp, tpr_mlp)\n\nplt.plot(fpr_gbc, tpr_gbc, label='GBC (area = {:.3f})'.format(auc_gbc))\nplt.plot(fpr_mlp, tpr_mlp, label='MLP (area = {:.3f})'.format(auc_mlp))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"VJ8g7ln-1ARW"},"cell_type":"markdown","source":"As we look at the following metrics make a mental note that I experienced higher variance with the MLP’s scores when returning to my work to test the model again. Sometimes there’s higher specificity and others there’s higher sensitivity. Just things to keep in mind.","execution_count":null},{"metadata":{"id":"_zYz2PDH09UU","outputId":"5278401d-2c9b-42cb-d7c5-04fce75c8f83","trusted":false},"cell_type":"code","source":"scores = cross_val_score(gbc_tuned, X_train, y_train, cv=3)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\nprint('GBC AUC : ', auc_gbc)\nprint()\nprint('Confusion Matrix GBC')\nprint(confusion_matrix(y_test, y_preds_test))\nprint()\nprint('Classification Matrix GBC')\nprint(classification_report(y_test, y_preds_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"QjeI_b_h1LO0","outputId":"8541de2d-179f-4565-cd83-870608b69c0b","trusted":false},"cell_type":"code","source":"# Add cross_validation_score for mlp\nprint('MLP AUC : ', auc_mlp)\nprint()\nprint('Confusion Matrix MLP')\nprint(confusion_matrix(y_test, y_mlp.round(decimals=0, out=None)))\nprint()\nprint('Classification Matrix MLP')\nprint(classification_report(y_test, y_mlp.round(decimals=0, out=None)))","execution_count":null,"outputs":[]},{"metadata":{"id":"FINFOIoL1VRc"},"cell_type":"markdown","source":"### Limitations and Future Work\n\nThink back to hyperparameter tuning of the MLP - the output of the final node is actually a value between or including 0 and 1. Here’s the distribution of that output. As we can see, there is some middle ground here that could be exposed to develop some cool near win/loss analysis. But also as an additional usability case, you can use this to determine where your game is by running the model after you pull the data into a notebook and architected a structure to get here.","execution_count":null},{"metadata":{"id":"BZwNL6o51GwE","outputId":"e5d8ac71-49e9-4413-8bbb-1e6440220064","trusted":false},"cell_type":"code","source":"plt.title('Blue Wins Distribution')\nsns.distplot(y)","execution_count":null,"outputs":[]},{"metadata":{"id":"2jRlbeVFo0hF","outputId":"766de646-b05d-4c49-e35c-6523fbdf1690","trusted":false},"cell_type":"code","source":"plt.title(\"MLP Prediction Distribution\")\nsns.distplot(y_mlp)","execution_count":null,"outputs":[]},{"metadata":{"id":"rb-JztIVH_r0"},"cell_type":"markdown","source":"#### Limitations and Future Work continued\n\nPlayer Positivity Score\n  * Communication is key in this game\n  * Swearing is reported by RIOT to negatively affect win rate\n  * And I can personally confirm that your teammates may tilt you if you go in\n  without tough skin and reminding players it's just a game may not help LOL\n\nMore Data AKA More Games\nTen-minute snapshot\n  * Could use whole games\n  * Could also use the time dimension\n\nCould Answer more pinpointed questions - (Surrender becomes available at 14 minutes) How many teams ff when they have more than a 40% chance of winning?\n\n\nAlso, there are various minion types from 2 different lane minions to different jungle and rift minions that provide different team buffs, and the return on those haven't been investigated here.\n\nPlayer skill information relating to the role\n  * If you “main” mid then forced to the jungle you may perform under the\n  the expected curve of player performance which creates openings for the enemy\n  team to take advantage of\n\nPlayer skill relating to champion\n  * Most players look at their “main” champs win rate \n  * What about the champ they are playing against\n  * Could be a counter pick (picked during champion select that has a natural \n  the advantage over another champion)\n\nLate or Early Game Team orientation\n  * Score composed based on the champions on a team and when their power spikes \n  dropoff is likely to occur\n  * “Early Game Teams” have more power in the early game but tend to fall off \n  later\n  * There is also the concept of “Late Game Teams”\n\nSeason Patch\n  * Champion and item buffs and nerfs certainly may have an effect","execution_count":null},{"metadata":{"id":"KbTLpr2yJqGF"},"cell_type":"markdown","source":"### Example Use Cases\n\n- You could predict on your own game\n\n- A team with a coach could use this model for premium game information\n\n- Pregame information to determine where your energy as a player is best spent. Also sharing the information with your team\n\n- To save time, if you're just getting wrecked in a department that leaves your team in a definite loss zone you could use this to know if you should FF. Games can last around 1 to 2 hours but can be resolved in a lot less. Saving everyone a little time and frustration\n","execution_count":null},{"metadata":{"id":"713Z_n4Pky0l"},"cell_type":"markdown","source":"Thanks for checking out the notebook! Feel free to add me on Linkedin and just chat if you're in the mood LOL. \n\nhttps://www.linkedin.com/in/lateef-medley/\n\nBest, \n\nLateef","execution_count":null},{"metadata":{"id":"XtW5ypnGKVA9","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}