{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Visually Exploring and Predicting Fake News"},{"metadata":{},"cell_type":"markdown","source":"In this notebook we perform some basic exploration and visualisation of the Real and Fake Article datasets. \n\nThis includes a few visual summaries of the basic text features, such as word counts and most common words (before and after pre-processing and cleaning operations). In this work, rather than performing extensive cleaning and pre-processing of our text, we conduct only light reformatting operations, such as lowercasing, removing various symbols and punctuation. Stop words are not removed, bcause in this case, they adversely impact the results. It should be noted that this is a common occurrence in many nlp applications, especially for more complex deep learning models, and thus stop words should not be removed by default. \n\nAfter pre-processing and cleaning we move forward and form pre-trained word embeddings for our text dataset using a pre-trained language model. With these embeddings, we explore a variety of different dimensionality reduction and visualisation techniques, including Uniform Manifold Approximation and Projection (UMAP), Latent Semantic Analysis (LSA), and T-distributed Stochastic Neighbor Embedding (t-SNE). Our results show that these achieve similar results in terms of producing a 2-D visualisation for our given pre-trained embeddings. A key difference between the methods was the significant training time required for t-SNE when compared to UMAP or LSA. This is a common finding in NLP, and a good way to compensate is by applying a deterministic dimensionality reduction technique, such as Principle Component Analysis (PCA), prior to using t-SNE. From all of the visualisations, each technique produced satisfactory results in terms of seperation between real and fake articles.\n\nFollowing these visualisations, some basic classical machine learning models are trained on the dataset and compared in terms of accuracy and F1 performance using K-Folds Cross Validation. Using out of the box classifiers, the performance averaged between 97-98%, which is not a bad result for such minimal effort. Some further evaluation methods are used on one of these models, including confusion matrices and a plot of the Reciever Operating Characteristic (ROC) Curve. The results shown in this notebook could be improved upon easily, using individually tuned models and some random / grid searches. For the best results, a combined ensembled model could be formed, or a deep learning LSTM model, CNN/LSTM composite model or transformer architecture. The methods in this notebook were just some quick exploration and analysis methods of the given dataset.\n\n**Table of Contents:**\n\n1. [Imports Dependencies](#imports)\n2. [Creation of Training / Test Splits](#import-data)\n3. [EDA](#EDA)\n4. [Data Cleaning and Processing](#cleaning-processing)\n5. [Pre-Trained Language Model Word Embeddings](#word-embeddings)\n6. [Word Embedding Visualisation Techniques](#word-embedding-visualisation)\n7. [Formation of Training and Validation Splits](#training-splits)\n8. [Cross-validation on a range of models](#cross-validation)\n9. [Model refinements and evaluations](#model-evaluation)\n10. [Visualising results of our chosen model](#chosen-model-results)"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"imports\"></a>\n## 1. Import dependencies and external libs"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport os\nimport pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline\nimport seaborn as sns\nimport spacy\nimport re\nimport umap\n\nfrom collections import Counter, defaultdict\nfrom matplotlib.colors import ListedColormap\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\n\nfrom sklearn.decomposition import PCA, LatentDirichletAllocation, TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, \\\n                            classification_report, f1_score, roc_curve, auc\nfrom sklearn.model_selection import cross_validate, cross_val_score, train_test_split, StratifiedKFold\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\n\n# import classical ml models\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression, Perceptron, RidgeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"import-data\"></a>\n## 2. Import data and inspect basic features"},{"metadata":{"trusted":true},"cell_type":"code","source":"file_path = '/kaggle/input/fake-and-real-news-dataset/'\nfake_df = pd.read_csv(f\"{file_path}Fake.csv\")\ntrue_df = pd.read_csv(f\"{file_path}True.csv\")\n\nfake_df['fake'] = 1\ntrue_df['fake'] = 0\n\ndata_df = pd.concat([fake_df, true_df], ignore_index=True)\ndata_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nax = sns.countplot(x=\"subject\", hue=\"fake\", data=data_df)\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see in this case that the real and fake datasets have unique 'subject' categories between them. Therefore, it is extremely important that we **do not use the 'subject' as a feature for making predictions**; this will introduce **data leakage** that will make it extremely easy for our models to achieve 100% accuracy. \n\nWith this data leakage in our models, we will have extremely high performance on our training and validation sets, but very poor generalisation performance on unseen real world data. This problem is common occurrence throughout data science, and something that we need to carefully look out for throughout projects."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,4))\nax = sns.countplot(x=\"fake\", data=data_df)\nax.set_xticklabels(['Real', 'Fake'])\nplt.xlabel('Classification')\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a fairly balanced mix of real and fake articles, which is good news."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"EDA\"></a>\n## 3. Exploratory Data Analysis of text features"},{"metadata":{},"cell_type":"markdown","source":"A useful and quick technique to quickly compare various text datasets is to assess the word counts within each."},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Distribution of word counts in title and text for true and fake articles"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df['title_length'] = data_df['title'].apply(lambda x : len(x.strip().split()))\ndata_df['text_length'] = data_df['text'].apply(lambda x : len(x.strip().split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.distplot(data_df[data_df['fake'] == 1]['title_length'], \n             kde=False, label='Fake', bins=20)\nsns.distplot(data_df[data_df['fake'] == 0]['title_length'], \n             kde=False, label='True', bins=20)\nplt.xlabel('Title Length', weight='bold')\nplt.title('Length of title comparison', weight='bold')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10, 6))\nplt.title(\"Word counts of article titles\", fontsize=16, weight='bold')\nax = sns.boxplot(x=\"fake\", y=\"title_length\", data=data_df)\nax.set_xticklabels(['Real', 'Fake'])\nax.set_xlabel(\"Article Classification\", fontsize=14, weight='bold') \nax.set_ylabel(\"Length of Entry (Words)\", fontsize=14, weight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.distplot(data_df[data_df['fake'] == 1]['text_length'], \n             kde=False, label='Fake', bins=20)\nsns.distplot(data_df[data_df['fake'] == 0]['text_length'], \n             kde=False, label='True', bins=20)\nplt.xlabel('Text Length', weight='bold')\nplt.title('Length of title comparison', weight='bold')\nplt.xlim(0.0, 4500)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10, 6))\nplt.title(\"Word counts of article text\", fontsize=16, weight='bold')\nax = sns.boxplot(x=\"fake\", y=\"text_length\", data=data_df)\nax.set_xticklabels(['Real', 'Fake'])\nax.set_xlabel(\"Article Classification\", fontsize=14, weight='bold') \nax.set_ylabel(\"Length of Entry (Words)\", fontsize=14, weight='bold')\nplt.ylim(0.0, 3000.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems in general, fake news articles tend to be longer than true articles. This is especially true for the article titles, and in fact the stark difference noted above suggests that we could classify articles to a reasonable accuracy using only title length.  \n\nHowever, in terms of word counts, it is harder to discriminate between real and fake articles using the main text field."},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Top words in true and fake articles"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(text_data):\n    \"\"\" Create a corpus from the given text array of sentences \"\"\"\n    corpus = []\n    for sentence in text_data:\n        for word in sentence.split():\n            corpus.append(word)\n    return corpus\n            \ndef top_words(text_corpus, top_n=25, return_dict=False):\n    \"\"\" Return the top n words from a given corpus \"\"\"\n    def_dict = defaultdict(int)\n    for word in text_corpus:\n        def_dict[word] += 1\n    most_common = sorted(def_dict.items(), key=lambda x : x[1], reverse=True)[:top_n]\n    if return_dict:\n        return most_common, def_dict\n    else:    \n        return most_common","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Fake article top words"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_n = 50\ntext_field = \"title\"\n\nfake_corpus = create_corpus(fake_df[text_field].values)\nfake_top_n_words, fake_symptom_dict = top_words(fake_corpus, top_n=top_n, return_dict=True)\nfake_words, fake_word_counts = zip(*fake_top_n_words)\n\ndef plot_words(word_list, word_counts, n, text_description, figsize=(15,5)):\n    plt.figure(figsize=figsize)\n    plt.xticks(rotation=90)\n    plt.bar(word_list, word_counts)\n    plt.title(f\"Top {n} words in {text_description}\", weight='bold')\n    plt.ylabel(\"Word Count\", weight='bold')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_words(fake_words, fake_word_counts, 50, \"Fake Article Titles\")\nprint(f\"Total unique words in {text_field}: {len(fake_symptom_dict)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Real article top words"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_n = 50\ntext_field = \"title\"\n\ntrue_corpus = create_corpus(true_df[text_field].values)\ntrue_top_n_words, true_symptom_dict = top_words(true_corpus, top_n=top_n, return_dict=True)\ntrue_words, true_word_counts = zip(*true_top_n_words)\n\nplot_words(true_words, true_word_counts, 50, \"True Article Titles\")\nprint(f\"Total unique words in {text_field}: {len(true_symptom_dict)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly we have many duplicate words due to capitalised text. In addition, we have various fields such as [video], punctuation and other symbols within our text data. It appears key names, e.g. Trump, Hillary are extremely popular throughout both the real and fake article datasets."},{"metadata":{},"cell_type":"markdown","source":"### 3.3 Numbers of Proper Nouns, Nouns, and other parts of speech in fake and real articles"},{"metadata":{},"cell_type":"markdown","source":"Sometimes features such as the number of nouns and proper nouns within our text can be good indicators of spam or fake articles. Lets work out some of these features for our data and analyse how they are distributed between real and fake articles:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_nouns_and_prop_nouns(text, model=nlp):\n    \"\"\" Return number of nouns and proper nouns in text \"\"\"\n    # generate POS tags, count and return nouns and proper nouns\n    doc = model(text)\n    pos = [token.pos_ for token in doc]\n    return pos.count('NOUN'), pos.count('PROPN')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This could take some time, since we need to analyse each title using a pre-trained spacy language model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create new features in our dataframe from these counts\ndata_df['propn_count'] = 0\ndata_df['noun_count'] = 0\ndata_df.loc[:, ['noun_count', 'propn_count']] = data_df['title'].apply(count_nouns_and_prop_nouns).values.tolist()\n\n# calculate mean number of proper nouns in real / fake\nreal_propn = data_df[data_df['fake'] == 0]['propn_count'].mean()\nfake_propn = data_df[data_df['fake'] == 1]['propn_count'].mean()\n\n# calculate mean number of nouns in real / fake\nreal_noun = data_df[data_df['fake'] == 0]['noun_count'].mean()\nfake_noun = data_df[data_df['fake'] == 1]['noun_count'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Proper Noun Mean Counts: \\n  - Real: {real_propn:.3f} \\n  - Fake: {fake_propn:.3f}\\n\")\nprint(f\"Noun Mean Counts: \\n  - Real: {real_noun:.3f} \\n  - Fake: {fake_noun:.3f}\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets analyse the distributions of nouns and proper nouns in our title for both real and fake articles:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.distplot(data_df[data_df['fake'] == 1]['propn_count'], \n             kde=False, label='Fake', bins=15)\nsns.distplot(data_df[data_df['fake'] == 0]['propn_count'], \n             kde=False, label='True', bins=15)\nplt.xlabel('Count of Proper Nouns in Title', weight='bold')\nplt.title('Proper Noun Comparison', weight='bold')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10, 6))\nplt.title(\"Word counts of article titles\", fontsize=16, weight='bold')\nax = sns.boxplot(x=\"fake\", y=\"propn_count\", data=data_df)\nax.set_xticklabels(['Real', 'Fake'])\nax.set_xlabel(\"Article Classification\", fontsize=14, weight='bold') \nax.set_ylabel(\"Number of Proper Nouns\", fontsize=14, weight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.distplot(data_df[data_df['fake'] == 1]['noun_count'], \n             kde=False, label='Fake', bins=15)\nsns.distplot(data_df[data_df['fake'] == 0]['noun_count'], \n             kde=False, label='True', bins=15)\nplt.xlabel('Count of Nouns in Title', weight='bold')\nplt.title('Noun Comparison', weight='bold')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10, 6))\nplt.title(\"Word counts of article titles\", fontsize=16, weight='bold')\nax = sns.boxplot(x=\"fake\", y=\"noun_count\", data=data_df)\nax.set_xticklabels(['Real', 'Fake'])\nax.set_xlabel(\"Article Classification\", fontsize=14, weight='bold') \nax.set_ylabel(\"Number of Nouns\", fontsize=14, weight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This actually looks quite promising! On average, it appears Real articles have less proper nouns relative to fake articles. Conversely, real articles appear to have more normal nouns, relative to fake articles that appear to have less. We haven't assessed whether any of these are statistically significant or not. Nevertheless, it is insightful to see and could help our models produced later in discriminating between real and fake articles."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"cleaning-processing\"></a>\n## 4. Cleaning and pre-processing"},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Formation of cleaning functions and applying these to the title and text fields"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_and_tokenise(text, stop_words=False, stem=False, lemmatize=False):\n    \"\"\" Text cleaning function - lowercase and remove stopwords \"\"\"\n    cleaned_text = re.sub('<[^>]*>', '', text.lower())\n    \n    # remove custom unwanted characters from our text\n    cleaned_text = remove_badchars(cleaned_text)\n    \n    # apply stop-word, stemming/lemmatising as required\n    if stop_words:\n        tokens = [word for word in tokenise(cleaned_text, stem=stem, \n                                            lemmatize=lemmatize) if word not in sw]\n    else:\n        tokens = [word for word in tokenise(cleaned_text, stem=stem, lemmatize=lemmatize)]\n    \n    cleaned_text = \" \".join(tokens)\n    \n    return cleaned_text\n\n\ndef remove_badchars(text):\n    \"\"\" Remove certain unwanted symbols and chars \"\"\"\n    delete_chars = \"[]()@''+&'\"\n    space_chars = \"_.-\"\n    table = dict((ord(c), \" \") for c in space_chars)\n    table.update(dict((ord(c), None) for c in delete_chars))\n    return text.translate(table)\n\n\ndef tokenise(text, stem=False, lemmatize=False):\n    \"\"\" Form tokenised stemmed text using a list comp and return \"\"\"\n    if lemmatize:\n        tokenised = [lemmatizer.lemmatize(word) for word in text.split()]\n    elif stem:\n        tokenised = [stemmer.stem(word) for word in text.split()]\n    else:\n        tokenised = [word for word in text.split()]\n    return tokenised","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can perform a range of cleaning and pre-processing operations. In this, we'll perform some basic cleaning operations, including removal of various symbols and punctuation, and lowercasing our text. The above functions have been designed to support stop-words and stemming / lemmatisation operations, however these are more harmful than good for this dataset, so they are not performed."},{"metadata":{"trusted":true},"cell_type":"code","source":"# stop words - append additionals if needed\nsw = stopwords.words('english')\n\n# clean text data - apply stopwords, but not lemmatisation / stemming this time\ndata_df['cleaned_title'] = data_df['title'].apply(clean_and_tokenise, stop_words=False, \n                                                      stem=False, lemmatize=False)\n\n# clean text data - apply stopwords, but not lemmatisation / stemming this time\ndata_df['cleaned_text'] = data_df['text'].apply(clean_and_tokenise, stop_words=False, \n                                                      stem=False, lemmatize=False)\n\ndata_df['combined_text'] = data_df['cleaned_text'] + \" \" + data_df['cleaned_title']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_df = data_df[data_df['fake'] == 0].copy()\nfake_df = data_df[data_df['fake'] == 1].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Lets re-visualise the top n words in each after cleaning"},{"metadata":{},"cell_type":"markdown","source":"Lets see how different our results are now that we've cleaned our data. We'll also combine both title and text fields, and visualise the word distributions."},{"metadata":{"trusted":true},"cell_type":"code","source":"top_n = 50\ntext_field = \"combined_text\"\n\nfake_corpus = create_corpus(fake_df[text_field].values)\nfake_top_n_words, fake_symptom_dict = top_words(fake_corpus, top_n=top_n, return_dict=True)\nfake_words, fake_word_counts = zip(*fake_top_n_words)\nplot_words(fake_words, fake_word_counts, 50, \"Fake Article Combined Title and Text (Cleaned)\", figsize=(15,4))\n\ntrue_corpus = create_corpus(true_df[text_field].values)\ntrue_top_n_words, true_symptom_dict = top_words(true_corpus, top_n=top_n, return_dict=True)\ntrue_words, true_word_counts = zip(*true_top_n_words)\nplot_words(true_words, true_word_counts, 50, \"True Article Combined Title and Text (Cleaned)\", figsize=(15,4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can try a range of things to improve on performance in general when cleaning our data. Notably, we can remove common stop-words, apply stemming (or lemmatisation), or apply further feature engineering."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"word-embeddings\"></a>\n## 5. Obtaining word embeddings using a pre-trained language model"},{"metadata":{},"cell_type":"markdown","source":"There are a wide variety of ways to obtain embeddings for our textual data. One such way, which we'll use in this notebook, is to obtain pre-trained word embeddings from a highly trained language model. For this we'll make use of SpaCy language model, which is super convenient, as the following code shows: "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_df['combined_text']\ny = data_df['fake']\nX.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# you need to ensure you've downloaded and installed 'en_core_web_lg' for this\nnlp = spacy.load('en_core_web_lg', disable=[\"parser\", \"tagger\", \"ner\", \"textcat\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorised_features = np.array([nlp(x).vector for x in X])\nvectorised_features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have a 300 dimensional vector representing each article text data (combined title + text) from our dataset. This is vastly smaller than we would obtain from a bag of words TF-IDF model, which makes it much easier to reduce to 2-dimensions and visualise graphically."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"word-embedding-visualisation\"></a>\n## 6. Visualisation of pre-trained embeddings - a comparison of UMAP, LSA and t-SNE"},{"metadata":{},"cell_type":"markdown","source":"We'll visualise our textual pre-trained embeddings using three different techniques: UMAP, LSA and t-SNE. All things considered, they each produce relatively similar results in terms of seperation performance of real and fake articles."},{"metadata":{},"cell_type":"markdown","source":"### 6.1 Uniform Manifold Approximation and Projection (UMAP)"},{"metadata":{"trusted":true},"cell_type":"code","source":"umap_embedder = umap.UMAP()\n%time umap_features = umap_embedder.fit_transform(vectorised_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sns settings\nsns.set(rc={'figure.figsize':(12,8)})\npalette = sns.hls_palette(2, l=.4, s=.9)\n\n# plot UMAP projection with annotations from k-means clustering\nsns.scatterplot(umap_features[:,0], umap_features[:,1], \n                hue=y, legend='full', \n                palette=palette, s=50, alpha=0.1)\n\nplt.title(f\"UMAP Projection\", \n          weight='bold', size=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.2 Latent Semantic Analysis (LSA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_LSA(word_vectors, word_labels, figsize=(12, 8), alpha=0.3):\n    \"\"\" Perform latent semantic analysis and plot results \"\"\"\n    lsa = TruncatedSVD(n_components=2)\n    lsa.fit(word_vectors)\n    lsa_scores = lsa.transform(word_vectors)\n    color_mapper = {label:idx for idx,label in enumerate(set(word_labels))}\n    color_column = [color_mapper[label] for label in word_labels]\n    colors = ['orange','blue']\n    \n    fig = plt.figure(figsize=figsize)\n    plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=alpha, \n                c=word_labels, cmap=ListedColormap(colors))\n    \n    orange_patch = mpatches.Patch(color='orange', label='Not')\n    blue_patch = mpatches.Patch(color='blue', label='Real')\n    plt.legend(handles=[orange_patch, blue_patch], prop={'size': 16})      \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_LSA(vectorised_features, y, alpha=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.3 t-Distributed Stochastic Neighbor Embedding (t-SNE)"},{"metadata":{},"cell_type":"markdown","source":"t-SNE is much more computationally complex than the previous two methods. Despite this, it often produces very good results for visualising text and high-dimensional data."},{"metadata":{},"cell_type":"markdown","source":"#### Apply PCA first to reduce our feature dimensions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=0.95, random_state=42)\n%time X_reduced = pca.fit_transform(vectorised_features)\nX_reduced.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE(verbose=1, perplexity=50, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time X_embedded = tsne.fit_transform(X_reduced)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12,8)})\npalette = sns.hls_palette(2, l=.4, s=.8)\n\n# plot t-SNE with annotations from k-means clustering\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y, \n                legend='full', palette=palette, alpha=0.2)\nplt.title('t-SNE with true/fake labels', weight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"t-SNE is a great tool for visualising our vectorised text, however it comes with the burden of significant computational complexity for large datasets with many features. For this reason we applied Principle component analysis (PCA) prior to performing t-SNE, so that the overhead is not so high. \n\nIf we were applying this to tf-idf features, the dimensionality of our data would be significantly higher than the 300 we used in this case (with the pre-trained word embeddings), and so this prior reduction of dimensionality becomes increasingly more important when using t-SNE with traditional techniques like bag of words."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"training-splits\"></a>\n## 7. Form training and validation splits"},{"metadata":{},"cell_type":"markdown","source":"We'll form a 80% training and 20% validation data split for our data prior to evaluating the general performance across a range of classical machine learning models."},{"metadata":{"trusted":true},"cell_type":"code","source":"# obtain training and validation splits - 80% training data, 20% val data\nX_train, X_val, y_train, y_val = train_test_split(vectorised_features, y, shuffle=True,\n                                                  test_size = 0.2, random_state=0, stratify=y)\n\nprint(\"Shapes of our data: \\nX_train: {0}\\ny_train: {1}\\nX_val: {2}\\ny_val: {3} \".format(X_train.shape,\n                                                                                         y_train.shape,\n                                                                                         X_val.shape,\n                                                                                         y_val.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"cross-validation\"></a>\n## 8. Evaluate K-folds cross validation performance on training set with a range of models"},{"metadata":{"trusted":true},"cell_type":"code","source":"def multi_model_cross_validation(clf_tuple_list, X, y, K_folds=10, score_type='accuracy', random_seed=0):\n    \"\"\" Find cross validation scores, and print and return results \"\"\"\n    \n    model_names, model_scores = [], []\n    \n    for name, model in clf_list:\n        k_fold = StratifiedKFold(n_splits=K_folds, shuffle=True, random_state=random_seed)\n        cross_val_results = cross_val_score(model, X, y, cv=k_fold, scoring=score_type, n_jobs=-1)\n        model_names.append(name)\n        model_scores.append(cross_val_results)\n        print(\"{0:<40} {1:.5f} +/- {2:.5f}\".format(name, cross_val_results.mean(), cross_val_results.std()))\n        \n    return model_names, model_scores\n\n\ndef boxplot_comparison(model_names, model_scores, figsize=(12, 6), score_type=\"Accuracy\",\n                       title=\"Sentiment Analysis Classification Comparison\"):\n    \"\"\" Boxplot comparison of a range of models using Seaborn and matplotlib \"\"\"\n    \n    fig = plt.figure(figsize=figsize)\n    fig.suptitle(title, fontsize=18)\n    ax = fig.add_subplot(111)\n    sns.boxplot(x=model_names, y=model_scores)\n    ax.set_xticklabels(model_names)\n    ax.set_xlabel(\"Model\", fontsize=16) \n    ax.set_ylabel(\"Model Score ({})\".format(score_type), fontsize=16)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=60)\n    plt.show()\n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list of classifiers to compare - use some additional models this time\nclf_list = [(\"Perceptron\", Perceptron(eta0=0.1)),\n            (\"Logistic Regression\", LogisticRegression(C=10.0, max_iter=200)),\n            (\"Support Vector Machine\", SVC(kernel='linear', C=1.0)),\n            (\"Decision Tree\", DecisionTreeClassifier()),\n            (\"Random Forest\", RandomForestClassifier(n_estimators=300)),\n            (\"Ridge Classifier\", RidgeClassifier()),\n            (\"Gradient Boosting\", GradientBoostingClassifier())]\n\n\n# calculate cross-validation scores and print / plot for each model accordingly\nmodel_names, model_scores = multi_model_cross_validation(clf_list, X_train[:10000], y_train[:10000])\nboxplot_comparison(model_names, model_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"model-evaluation\"></a>\n## 9. Evaluate performance on validation set using the same range of models\n\nLet's take this forward, and assess the performance on our stand-alone validation set when we train our models on the training splits."},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_set_performances(clf_tuple_list, X_train, y_train, X_test, \n                          y_test, score_type='accuracy', print_results=True):\n    \"\"\" Find test set accuracy and F1 Score performance for all classifiers \n        and return \"\"\"\n    \n    model_names, model_accuracies, model_f1 = [], [], []\n    \n    if print_results:\n        print(\"{0:<30} {1:<10} {2:<10} \\n{3}\".format(\"Model\", \"Accuracy\", \n                                                     \"F1-Score\", \"-\"*50))\n    \n    # fit each model to training data and form predictions\n    for name, model in clf_list:\n        \n        # fit on training, predict on test\n        model.fit(X_train, y_train)\n        y_preds = model.predict(X_test)\n        \n        # find accuracy and f1 (macro) scores\n        accuracy = accuracy_score(y_test, y_preds)\n        test_f1 = f1_score(y_test, y_preds, average='macro')\n        \n        # append model results\n        model_names.append(name)\n        model_accuracies.append(accuracy)\n        model_f1.append(test_f1)\n        \n        if print_results:\n            print(\"{0:<30} {1:<10.5f} {2:<10.5f}\".format(name, accuracy, test_f1))\n            \n    return model_names, model_accuracies, model_f1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# obtain accuracy and f1 metrics and print for each model\nmodel_names, test_acc, test_f1 = test_set_performances(clf_list, X_train, y_train, X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# barplot of model accuracies\nsns.set(style=\"darkgrid\")\nsns.barplot(model_names, test_acc, alpha=0.9)\nplt.title('Test set accuracy', weight='bold')\nplt.ylabel('Accuracy', fontsize=12, weight='bold')\nplt.xticks(rotation=90)\nplt.ylim(0.0, 1.0)\nplt.show()\n\n# barplot of model f1 scores\nsns.set(style=\"darkgrid\")\nsns.barplot(model_names, test_f1, alpha=0.9)\nplt.title('Test set F1 Score', weight='bold')\nplt.ylabel('F1 Score (Macro)', fontsize=12, weight='bold')\nplt.xticks(rotation=90)\nplt.ylim(0.0, 1.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"chosen-model-results\"></a>\n## 10. Visualising the results of a chosen model from above"},{"metadata":{},"cell_type":"markdown","source":"We'll take our gradient boosting classifier from above and analyse it further in terms of F1 Score, precision and recall. An effective means of doing this is through using a confusion matrix."},{"metadata":{},"cell_type":"markdown","source":"### 10.1 Plot a confusion matrix for our trained model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(true_y, pred_y, title='Confusion Matrix', figsize=(8,6)):\n    \"\"\" Custom function for plotting a confusion matrix for predicted results \"\"\"\n    conf_matrix = confusion_matrix(true_y, pred_y)\n    conf_df = pd.DataFrame(conf_matrix, columns=np.unique(true_y), index = np.unique(true_y))\n    conf_df.index.name = 'Actual'\n    conf_df.columns.name = 'Predicted'\n    plt.figure(figsize = figsize)\n    plt.title(title)\n    sns.set(font_scale=1.4)\n    sns.heatmap(conf_df, cmap=\"Blues\", annot=True, \n                annot_kws={\"size\": 16}, fmt='g')\n    plt.show()\n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a log reg classifier and predict using test set\ngb_clf = GradientBoostingClassifier()\ngb_clf.fit(X_train, y_train)\npredictions = gb_clf.predict(X_val)\n\n# print performance statistics\nprint(\"Samples incorrectly classified: {0} out of {1}.\".format((y_val != predictions).sum(),\n                                                                len(y_val)))\n\nprint(\"Logistic Regression classifier accuracy: {0:.2f}%\".format(accuracy_score(predictions, y_val)*100.0))\n\n# plot a confusion matrix of our results\nplot_confusion_matrix(y_val, predictions, \n                      title=\"SVC Confusion Matrix\", figsize=(5,5))\n\n# print recall, precision and f1 score results\nprint(classification_report(y_val, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As shown, straight out of the box our classifier obtains a very good accuracy on predicting whether an article is fake or real."},{"metadata":{},"cell_type":"markdown","source":"### 10.2 Plot the Receiver Operating Characteristic (ROC) Curve for our Model"},{"metadata":{},"cell_type":"markdown","source":"A method to further assess these characteristics of our model is the ROC curve. This allows us to determine the optimal threshold to set our model to in order to maximise or true positive rate or false positive rate, depending on our specific use-case."},{"metadata":{"trusted":true},"cell_type":"code","source":"# obtain prediction probabilities for trg and val\ny_val_probs = gb_clf.predict_proba(X_val)\ny_trg_probs = gb_clf.predict_proba(X_train)\n\n# obtain true positive and false positive rates for roc_auc\nfpr, tpr, thresholds = roc_curve(y_train, y_trg_probs[:, 1], pos_label=1)\nroc_auc = auc(fpr, tpr)\n\n# obtain true positive and false positive rates for roc_auc\nval_fpr, val_tpr, val_thresholds = roc_curve(y_val, y_val_probs[:, 1], pos_label=1)\nval_roc_auc = auc(val_fpr, val_tpr)\n\nplt.figure(figsize=(9,9))\nplt.plot(fpr, tpr, label=f\"Train ROC AUC = {roc_auc}\", color='blue')\nplt.plot(val_fpr, val_tpr, label=f\"Val ROC AUC = {val_roc_auc}\", color='red')\nplt.plot([0,1], [0, 1], label=\"Random Guessing\", \n             linestyle=\":\", color='grey', alpha=0.6)\nplt.plot([0, 0, 1], [0, 1, 1], label=\"Perfect Performance\", \n             linestyle=\"--\", color='black', alpha=0.6)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver Operating Characteristic\", weight='bold')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Overall, not a bad performing model, especially considering our gradient boosting model only has the default, untuned parameters. In order to improve this, we could perform grid-search across a range of hyper-parameters in order to maximise the performance for our predictions."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}