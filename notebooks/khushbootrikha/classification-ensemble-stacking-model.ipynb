{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Import Libraries**","metadata":{}},{"cell_type":"code","source":"#loading dataset\nimport pandas as pd\nimport numpy as np\n#visualisation\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n#EDA\nfrom collections import Counter\n# data preprocessing\nfrom sklearn.preprocessing import StandardScaler\n# data splitting\nfrom sklearn.model_selection import train_test_split\n# data modeling\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n#ensembling\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv (r'../input/heart-disease-uci/heart.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking the datatypes and shape of the data:\n","metadata":{}},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking for duplicate rows:\n","metadata":{}},{"cell_type":"code","source":"duplicate_rows_df = df[df.duplicated()]\nprint(duplicate_rows_df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for null values:\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Renaming columns:\n","metadata":{}},{"cell_type":"code","source":"df.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',\n       'exercise_induced_angina', 'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Outlier detection:\n","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\nsns.boxplot(x=df[\"age\"])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\nsns.boxplot(x=df[\"resting_blood_pressure\"])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\nsns.boxplot(x=df[\"cholesterol\"])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\nsns.boxplot(x=df[\"max_heart_rate_achieved\"])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\nsns.boxplot(x=df[\"st_depression\"])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.target.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Statistical details from the data:\n","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Univariate Analysis:\n","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=\"target\", data=df, palette=\"bwr\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure( figsize= (25,25) )\ndf[['age'] ].hist(bins=10)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure( figsize= (25,25) )\ndf[['resting_blood_pressure'] ].hist(bins=10)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure( figsize= (25,25) )\ndf[['cholesterol'] ].hist(bins=10)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure( figsize= (25,25) )\ndf[['max_heart_rate_achieved'] ].hist(bins=10)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure( figsize= (25,25) )\ndf[['st_depression'] ].hist(bins=10)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\nax = sns.countplot(x=\"chest_pain_type\", data=df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10,6))\nx = df['max_heart_rate_achieved']\nax = sns.distplot(x, bins=10)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\nax = sns.countplot(x=\"rest_ecg\", data=df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bivariate Analysis:\n","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\nax = sns.countplot(x=\"chest_pain_type\", hue=\"target\", data=df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\nsns.stripplot(x=\"target\", y=\"max_heart_rate_achieved\", data=df, jitter = 0.01)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\nsns.stripplot(x=\"target\", y=\"age\", data=df, jitter = 0.01)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\nax = sns.regplot(x=\"age\", y=\"resting_blood_pressure\", data=df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\nax = sns.regplot(x=\"age\", y=\"cholesterol\", data=df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\nax = sns.regplot(x=\"cholesterol\", y=\"max_heart_rate_achieved\", data=df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(df.age,df.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Heart Disease Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.savefig('heartDiseaseAndAges.png')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\nax = sns.countplot(x=\"sex\", hue=\"target\", data=df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\nax = sns.countplot(x=\"fasting_blood_sugar\", hue=\"target\", data=df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(df.st_depression,df.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Heart Disease Frequency for ST depression')\nplt.xlabel('ST_Depression')\nplt.ylabel('Frequency')\nplt.savefig('heartDiseaseAndSTdepression.png')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(df.resting_blood_pressure,df.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Heart Disease Frequency for Resting Blood Pressure')\nplt.xlabel('Resting Blood Pressure')\nplt.ylabel('Frequency')\nplt.savefig('heartDiseaseAndRestingbps.png')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multivariate Analysis:\n","metadata":{}},{"cell_type":"code","source":"num_var = ['age', 'resting_blood_pressure', 'cholesterol', 'max_heart_rate_achieved', 'st_depression', 'target' ]\nsns.pairplot(df[num_var], kind='scatter', diag_kind='hist')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation Matrix:\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,12))\nplt.title('Correlation Heatmap of Heart Disease Dataset')\na = sns.heatmap(df.corr(), square=True, annot=True, fmt='.2f', linecolor='white')\na.set_xticklabels(a.get_xticklabels(), rotation=90)\na.set_yticklabels(a.get_yticklabels(), rotation=30)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix = df.corr()\ncorr_matrix['target'].sort_values( ascending = False )","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data split","metadata":{}},{"cell_type":"code","source":"y = data[\"target\"]\nX = data.drop('target',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y_test.unique())\nCounter(y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Standardization","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest-\nIt is an ensemble learning method that operates by constructing a multitude of decision trees at training time and outputs the class that is the mode of the classes or classification or mean prediction(regression) of the individual trees.\n","metadata":{}},{"cell_type":"code","source":"m3 = 'Random Forest Classfier'\nrf = RandomForestClassifier(n_estimators=20, random_state=12,max_depth=5)\nrf.fit(X_train,y_train)\nrf_predicted = rf.predict(X_test)\nrf_conf_matrix = confusion_matrix(y_test, rf_predicted)\nrf_acc_score = accuracy_score(y_test, rf_predicted)\nprint(\"confussion matrix\")\nprint(rf_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Random Forest:\",rf_acc_score*100,'\\n')\nprint(classification_report(y_test,rf_predicted))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature importance","metadata":{}},{"cell_type":"code","source":"imp_feature = pd.DataFrame({'Feature': ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n       'exang', 'oldpeak', 'slope', 'ca', 'thal'], 'Importance': rf.feature_importances_})\nplt.figure(figsize=(10,4))\nplt.title(\"barplot Represent feature importance \")\nplt.xlabel(\"importance \")\nplt.ylabel(\"features\")\nplt.barh(imp_feature['Feature'],imp_feature['Importance'], color = 'blue')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree-\nDecision Trees predict the value of a target variable by learning simple decision rules inferred from the input data features. The decision tree algorithm builds the classification model in the form of a tree structure. It utilizes the if-then rules which are equally exhaustive and mutually exclusive in classification.\n","metadata":{}},{"cell_type":"code","source":"m6 = 'DecisionTreeClassifier'\ndt = DecisionTreeClassifier(criterion = 'entropy',random_state=0,max_depth = 6)\ndt.fit(X_train, y_train)\ndt_predicted = dt.predict(X_test)\ndt_conf_matrix = confusion_matrix(y_test, dt_predicted)\ndt_acc_score = accuracy_score(y_test, dt_predicted)\nprint(\"confussion matrix\")\nprint(dt_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of DecisionTreeClassifier:\",dt_acc_score*100,'\\n')\nprint(classification_report(y_test,dt_predicted))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature importance","metadata":{}},{"cell_type":"code","source":"imp_feature = pd.DataFrame({'Feature': ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n       'exang', 'oldpeak', 'slope', 'ca', 'thal'], 'Importance': dt.feature_importances_})\nplt.figure(figsize=(10,4))\nplt.title(\"barplot Represent feature importance \")\nplt.xlabel(\"importance \")\nplt.ylabel(\"features\")\nplt.barh(imp_feature['Feature'],imp_feature['Importance'],color = 'blue')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Xgboost-\nXGBoost is a popularly used algorithm which seeks to push the limit of computational resources used in conventional boosted trees. XGBoost is short for “Extreme Gradient Boosting” and is based on the original gradient boosting model\nThe key difference between the two is that XGBoost uses a more regularized model formalization to control over-fitting, giving it better performance.XGBoost uses a combination of bagging and boosting which can reduce variance and bias respectively.","metadata":{}},{"cell_type":"code","source":"m4 = 'Extreme Gradient Boost'\nxgb = XGBClassifier(learning_rate=0.1, n_estimators=500, max_depth=10,gamma=0.6, subsample=0.52,colsample_bytree=0.6,seed=28,  \n                    reg_lambda=2, booster='dart', colsample_bylevel=0.6, colsample_bynode=0.5)\nxgb.fit(X_train, y_train)\nxgb_predicted = xgb.predict(X_test)\nxgb_conf_matrix = confusion_matrix(y_test, xgb_predicted)\nxgb_acc_score = accuracy_score(y_test, xgb_predicted)\nprint(\"confussion matrix\")\nprint(xgb_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Extreme Gradient Boost:\",xgb_acc_score*100,'\\n')\nprint(classification_report(y_test,xgb_predicted))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature importance","metadata":{}},{"cell_type":"code","source":"imp_feature = pd.DataFrame({'Feature': ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n       'exang', 'oldpeak', 'slope', 'ca', 'thal'], 'Importance': xgb.feature_importances_})\nplt.figure(figsize=(10,4))\nplt.title(\"barplot Represent feature importance \")\nplt.xlabel(\"importance \")\nplt.ylabel(\"features\")\nplt.barh(imp_feature['Feature'],imp_feature['Importance'],color = 'blue')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression-\nIt is a classification algorithm in machine learning that uses one or more independent variables to determine an outcome. The outcome is measured with a dichotomous variable meaning it will have only two possible outcomes.\n","metadata":{}},{"cell_type":"code","source":"m1 = 'Logistic Regression'\nlr = LogisticRegression()\nmodel = lr.fit(X_train, y_train)\nlr_predict = lr.predict(X_test)\nlr_conf_matrix = confusion_matrix(y_test, lr_predict)\nlr_acc_score = accuracy_score(y_test, lr_predict)\nprint(\"confussion matrix\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Logistic Regression:\",lr_acc_score*100,'\\n')\nprint(classification_report(y_test,lr_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Naive Bayes-\nIt is a classification algorithm based on Bayes’s theorem which gives an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n","metadata":{}},{"cell_type":"code","source":"m2 = 'Naive Bayes'\nnb = GaussianNB()\nnb.fit(X_train,y_train)\nnbpred = nb.predict(X_test)\nnb_conf_matrix = confusion_matrix(y_test, nbpred)\nnb_acc_score = accuracy_score(y_test, nbpred)\nprint(\"confussion matrix\")\nprint(nb_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Naive Bayes model:\",nb_acc_score*100,'\\n')\nprint(classification_report(y_test,nbpred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KNN-\nIt is a lazy learning algorithm that stores all instances corresponding to training data in n-dimensional space. It is a lazy learning algorithm as it does not focus on constructing a general internal model, instead, it works on storing instances of training data.","metadata":{}},{"cell_type":"code","source":"m5 = 'K-NeighborsClassifier'\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_train, y_train)\nknn_predicted = knn.predict(X_test)\nknn_conf_matrix = confusion_matrix(y_test, knn_predicted)\nknn_acc_score = accuracy_score(y_test, knn_predicted)\nprint(\"confussion matrix\")\nprint(knn_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of K-NeighborsClassifier:\",knn_acc_score*100,'\\n')\nprint(classification_report(y_test,knn_predicted))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM-\nIt is a lazy learning algorithm that stores all instances corresponding to training data in n-dimensional space. It is a lazy learning algorithm as it does not focus on constructing a general internal model, instead, it works on storing instances of training data.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nm7 = 'Support Vector Classifier'\nsvc =  SVC(kernel='rbf', C=2)\nsvc.fit(X_train, y_train)\nsvc_predicted = svc.predict(X_test)\nsvc_conf_matrix = confusion_matrix(y_test, svc_predicted)\nsvc_acc_score = accuracy_score(y_test, svc_predicted)\nprint(\"confussion matrix\")\nprint(svc_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Support Vector Classifier:\",svc_acc_score*100,'\\n')\nprint(classification_report(y_test,svc_predicted))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Comparison","metadata":{}},{"cell_type":"code","source":"model_ev = pd.DataFrame({'Model': ['Logistic Regression','Naive Bayes','Random Forest','Extreme Gradient Boost',\n                    'K-Nearest Neighbour','Decision Tree','Support Vector Machine'], 'Accuracy': [lr_acc_score*100,\n                    nb_acc_score*100,rf_acc_score*100,xgb_acc_score*100,knn_acc_score*100,dt_acc_score*100,svc_acc_score*100]})\nmodel_ev","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mlxtend.classifier import StackingCVClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble technique-Model Stacking-\nStacking, also known as stacked generalization, is an ensemble method where the models are combined using another machine learning algorithm. The basic idea is to train machine learning algorithms with training dataset and then generate a new dataset with these models. Then this new dataset is used as input for the combiner machine learning algorithm.","metadata":{}},{"cell_type":"markdown","source":"# Random forest + KNN + SVM\n","metadata":{}},{"cell_type":"code","source":"scv=StackingCVClassifier(classifiers=[rf,knn,svc],meta_classifier= knn,random_state=42)\nscv.fit(X_train,y_train)\nscv_predicted = scv.predict(X_test)\nscv_conf_matrix = confusion_matrix(y_test, scv_predicted)\nscv_acc_score = accuracy_score(y_test, scv_predicted)\nprint(\"confussion matrix\")\nprint(scv_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of StackingCVClassifier:\",scv_acc_score*100,'\\n')\nprint(classification_report(y_test,scv_predicted))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Xgboost + KNN + SVM","metadata":{}},{"cell_type":"code","source":"scv=StackingCVClassifier(classifiers=[xgb,knn,svc],meta_classifier= xgb,random_state=42)\nscv.fit(X_train,y_train)\nscv_predicted = scv.predict(X_test)\nscv_conf_matrix = confusion_matrix(y_test, scv_predicted)\nscv_acc_score = accuracy_score(y_test, scv_predicted)\nprint(\"confussion matrix\")\nprint(scv_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of StackingCVClassifier:\",scv_acc_score*100,'\\n')\nprint(classification_report(y_test,scv_predicted))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random forest + XGB + SVM","metadata":{}},{"cell_type":"code","source":"scv=StackingCVClassifier(classifiers=[rf,xgb,svc],meta_classifier= knn,random_state=42)\nscv.fit(X_train,y_train)\nscv_predicted = scv.predict(X_test)\nscv_conf_matrix = confusion_matrix(y_test, scv_predicted)\nscv_acc_score = accuracy_score(y_test, scv_predicted)\nprint(\"confussion matrix\")\nprint(scv_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of StackingCVClassifier:\",scv_acc_score*100,'\\n')\nprint(classification_report(y_test,scv_predicted))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Xgboost + KNN + SVM","metadata":{}},{"cell_type":"code","source":"scv=StackingCVClassifier(classifiers=[xgb,knn,svc],meta_classifier= xgb,random_state=42)\nscv.fit(X_train,y_train)\nscv_predicted = scv.predict(X_test)\nscv_conf_matrix = confusion_matrix(y_test, scv_predicted)\nscv_acc_score = accuracy_score(y_test, scv_predicted)\nprint(\"confussion matrix\")\nprint(scv_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of StackingCVClassifier:\",scv_acc_score*100,'\\n')\nprint(classification_report(y_test,scv_predicted))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random forest + KNN + Xgboost","metadata":{}},{"cell_type":"code","source":"scv=StackingCVClassifier(classifiers=[rf,knn,xgb],meta_classifier= xgb,random_state=42)\nscv.fit(X_train,y_train)\nscv_predicted = scv.predict(X_test)\nscv_conf_matrix = confusion_matrix(y_test, scv_predicted)\nscv_acc_score = accuracy_score(y_test, scv_predicted)\nprint(\"confussion matrix\")\nprint(scv_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of StackingCVClassifier:\",scv_acc_score*100,'\\n')\nprint(classification_report(y_test,scv_predicted))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GridSearchCV","metadata":{}},{"cell_type":"code","source":"model = XGBClassifier()\nmodel.fit(X_train, y_train)\n  \n# print prediction results\npredictions = model.predict(X_test)\nprint(classification_report(y_test, predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n# defining parameter range\nparam_grid = {'C': [0.1, 1, 10, 100, 1000], \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel': ['rbf']} \n  \ngrid = GridSearchCV(XGBClassifier(), param_grid, refit = True, verbose = 3)\n  \n# fitting the model for grid search\ngrid.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print best parameter after tuning\nprint(grid.best_params_)\n  \n# print how our model looks after hyper-parameter tuning\nprint(grid.best_estimator_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_predictions = grid.predict(X_test)\n  \n# print classification report\nprint(classification_report(y_test, grid_predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ROC curve","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=500, random_state=0)\n\nmodel = XGBClassifier()\nmodel.fit(X, y)\ny_score = model.predict_proba(X)[:, 1]\nfpr, tpr, thresholds = roc_curve(y, y_score)\n\n# The histogram of scores compared to true labels\nfig_hist = px.histogram(\n    x=y_score, color=y, nbins=50,\n    labels=dict(color='True Labels', x='Score')\n)\n\nfig_hist.show()\n\n\n# Evaluating model performance at various thresholds\ndf = pd.DataFrame({\n    'False Positive Rate': fpr,\n    'True Positive Rate': tpr\n}, index=thresholds)\ndf.index.name = \"Thresholds\"\ndf.columns.name = \"Rate\"\n\nfig_thresh = px.line(\n    df, title='TPR and FPR at every threshold',\n    width=700, height=500\n)\n\nfig_thresh.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig_thresh.update_xaxes(range=[0, 1], constrain='domain')\nfig_thresh.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PR curve for Xgboost","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=500, random_state=0)\n\nmodel = XGBClassifier()\nmodel.fit(X, y)\ny_score = model.predict_proba(X)[:, 1]\n\nprecision, recall, thresholds = precision_recall_curve(y, y_score)\n\nfig = px.area(\n    x=recall, y=precision,\n    title=f'Precision-Recall Curve (AUC={auc(fpr, tpr):.4f})',\n    labels=dict(x='Recall', y='Precision'),\n    width=700, height=500\n)\nfig.add_shape(\n    type='line', line=dict(dash='dash'),\n    x0=0, x1=1, y0=1, y1=0\n)\nfig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig.update_xaxes(constrain='domain')\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PR curve for SVM","metadata":{}},{"cell_type":"markdown","source":"# PR curve for Random Forest","metadata":{}},{"cell_type":"code","source":"X, y = make_classification(n_samples=500, random_state=0)\n\nmodel = RandomForestClassifier(n_estimators=20, random_state=12,max_depth=5)\nmodel.fit(X, y)\ny_score = model.predict_proba(X)[:, 1]\n\nprecision, recall, thresholds = precision_recall_curve(y, y_score)\n\nfig = px.area(\n    x=recall, y=precision,\n    title=f'Precision-Recall Curve (AUC={auc(fpr, tpr):.4f})',\n    labels=dict(x='Recall', y='Precision'),\n    width=700, height=500\n)\nfig.add_shape(\n    type='line', line=dict(dash='dash'),\n    x0=0, x1=1, y0=1, y1=0\n)\nfig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig.update_xaxes(constrain='domain')\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PR curve for KNN","metadata":{}},{"cell_type":"code","source":"X, y = make_classification(n_samples=500, random_state=0)\n\nmodel = KNeighborsClassifier(n_neighbors=10)\nmodel.fit(X, y)\ny_score = model.predict_proba(X)[:, 1]\n\nprecision, recall, thresholds = precision_recall_curve(y, y_score)\n\nfig = px.area(\n    x=recall, y=precision,\n    title=f'Precision-Recall Curve (AUC={auc(fpr, tpr):.4f})',\n    labels=dict(x='Recall', y='Precision'),\n    width=700, height=500\n)\nfig.add_shape(\n    type='line', line=dict(dash='dash'),\n    x0=0, x1=1, y0=1, y1=0\n)\nfig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig.update_xaxes(constrain='domain')\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PR curve for Stacking- Random forest, KNN, SVM","metadata":{}},{"cell_type":"code","source":"X, y = make_classification(n_samples=500, random_state=0)\n\nmodel = StackingCVClassifier(classifiers=[rf,knn,svc],meta_classifier= xgb,random_state=42)\nmodel.fit(X, y)\ny_score = model.predict_proba(X)[:, 1]\n\nprecision, recall, thresholds = precision_recall_curve(y, y_score)\n\nfig = px.area(\n    x=recall, y=precision,\n    title=f'Precision-Recall Curve (AUC={auc(fpr, tpr):.4f})',\n    labels=dict(x='Recall', y='Precision'),\n    width=700, height=500\n)\nfig.add_shape(\n    type='line', line=dict(dash='dash'),\n    x0=0, x1=1, y0=1, y1=0\n)\nfig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig.update_xaxes(constrain='domain')\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}