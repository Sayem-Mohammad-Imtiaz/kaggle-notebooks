{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction: Text Classification with CNNs\nHello people, welcome to this kernel. In this kernel I am going to show you how to create a Convolutional Neural Network using Tensorflow to classify texts.\n\nBefore starting, let's take a look at our table of content\n\n# Table of Content\n1. But CNNs Are For images!?!?\n1. Preparing Environment\n1. Preparing Data\n1. Neural Network Modeling\n1. EXTRA: How To Make Our Model Ready-to-Deploy?\n1. Conclusion\n\n\n# But CNNs Are For Images!?!\nIn deep learning, we generally use Convolutional Neural Networks and their variants to classify image data. So most of the people thinks *we can use them only for image data*.\n\nBut a convolution operator **extracts** features from a data given. And if data has dimension more than one, we can use it with a convolution operator. And if we use **word embeddings** to convert words we can use a Convolutional Neural Network. \n\nLet's start.\n","metadata":{}},{"cell_type":"markdown","source":"# Preparing Environment\nIn this section we'll import libraries and read our data from HDD.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nimport matplotlib.pyplot as plt\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_true = pd.read_csv('../input/fake-and-real-news-dataset/True.csv')\ndata_fake = pd.read_csv('../input/fake-and-real-news-dataset/Fake.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Data\nIn this section we're going to prepare data to use it in our neural network.","metadata":{}},{"cell_type":"code","source":"data_true.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We can drop title, subject and date.\n* Also we need to add a label which will be 1","metadata":{}},{"cell_type":"code","source":"data_true[\"label\"] = 1\ndata_fake[\"label\"] = 0\ndata = pd.concat([data_true,data_fake],0)\ndata.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.loc[:,[\"text\",\"label\"]]\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = data[\"text\"]\ny = data[\"label\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Now we're going to define a function which will clean data.","metadata":{}},{"cell_type":"code","source":"def cleanText(text):\n    cleaned = re.sub(\"[^'a-zA-Z0-9]\",\" \",text)\n    lowered = cleaned.lower().strip()\n    return lowered","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Let's test our function.","metadata":{}},{"cell_type":"code","source":"cleanText(\"Test .* yup *?! okay!.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st = time.time()\nx_cleaned = [cleanText(t) for t in x]\nprint(\"This process took {} seconds\".format(round(time.time()-st,2)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_cleaned[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Now we'll tokenize our data using Tensorflow's tokenizer.","metadata":{}},{"cell_type":"code","source":"st = time.time()\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(x_cleaned)\nx_tokenized = tokenizer.texts_to_sequences(x_cleaned)\nprint(\"This process took {} seconds\".format(round(time.time()-st,2)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_tokenized[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Now we need to pad our sequences, in order to find the true length, I'll use the third quartile of the length array (array which has the lengths of the sequences)","metadata":{}},{"cell_type":"code","source":"length_array = [len(s) for s in x_tokenized]\nSEQUENCE_LENGTH = int(np.quantile(length_array,0.75))\nprint(SEQUENCE_LENGTH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* And let's pad.","metadata":{}},{"cell_type":"code","source":"x_padded = pad_sequences(x_tokenized,maxlen=SEQUENCE_LENGTH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_padded.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Our text data is ready to use, let's split our dataset into train and test sets.","metadata":{}},{"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x_padded,y,test_size=0.2,random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network Modeling\nIn this section I'm going to build and train our convolutional neural network using keras' sequential api.","metadata":{}},{"cell_type":"code","source":"# We've added 1 because or word index has numbers from 1 to end but we've added\n# 0 tokens in padding so our vocab now has len(tokenizer.word_index) + 1\nVOCAB_LENGTH = len(tokenizer.word_index) + 1\nVECTOR_SIZE = 100\n\ndef getModel():\n    \"\"\"\n    Returns a trainable Sigmoid Convolutional Neural Network\n    \"\"\"\n    model = keras.Sequential()\n    model.add(layers.Embedding(input_dim=VOCAB_LENGTH,\n                               output_dim=VECTOR_SIZE,\n                               input_length=SEQUENCE_LENGTH\n                              ))\n    \n    model.add(layers.Conv1D(128,kernel_size=4))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Activation(\"relu\"))\n    model.add(layers.MaxPooling1D(2))\n    \n    model.add(layers.Conv1D(256,kernel_size=4))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Activation(\"relu\"))\n    model.add(layers.MaxPooling1D(2))\n    \n    model.add(layers.Conv1D(512,kernel_size=4))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Activation(\"relu\"))\n    model.add(layers.MaxPooling1D(2))\n    \n    model.add(layers.Flatten())\n    model.add(layers.Dense(1,activation=\"sigmoid\"))\n    \n    model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = getModel()\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 1 epoch and %93 validation accuracy, this is how a convolutional neural network works with text data ","metadata":{}},{"cell_type":"markdown","source":"# EXTRA: How To Make Our Model Ready-to-Deploy?\nBefore finishing this kernel, I wanna show you one more thing, an important one. How to make a model ready to deploy using a web library or framework like Flask or Django.\n\nLet's start.","metadata":{}},{"cell_type":"markdown","source":"* First we'll save weights of our model and pickle our tokenizer.","metadata":{}},{"cell_type":"code","source":"model.save_weights(\"trained_model.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open(\"tokenizer.pickle\",mode=\"wb\") as F:\n    pickle.dump(tokenizer,F)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Also let's save our label map using json library.","metadata":{}},{"cell_type":"code","source":"import json\nlabel_map = {0:\"Fake\",\n             1:\"Real\"\n            }\n\njson.dump(label_map,open(\"label_map.json\",mode=\"w\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* And now we'll write a class which will have a function to predict data.","metadata":{}},{"cell_type":"code","source":"class DeployModel():\n    \n    def __init__(self,weights_path,tokenizer_path,seq_length,label_map_path\n                ):\n        \n        self.model = getModel()\n        self.model.load_weights(weights_path)\n        self.tokenizer = pickle.load(open(tokenizer_path,mode=\"rb\"))\n        self.seq_len = seq_length\n        self.label_map = json.load(open(label_map_path))\n    \n    def _prepare_data(self,text):\n        \n        cleaned = cleanText(text)\n        tokenized = self.tokenizer.texts_to_sequences([cleaned])\n        padded = pad_sequences(tokenized,maxlen=self.seq_len)\n        return padded\n    \n    def _predict(self,text):\n        \n        text = self._prepare_data(text)\n        pred = int(self.model.predict_classes(text)[0])\n        return str(pred)\n    \n    def result(self,text):\n        \n        pred = self._predict(text)\n        return self.label_map[pred]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* And let's create an object using our class.","metadata":{}},{"cell_type":"code","source":"deploy_model = DeployModel(weights_path=\"./trained_model.h5\",\n                           tokenizer_path=\"./tokenizer.pickle\",\n                           seq_length=SEQUENCE_LENGTH,\n                           label_map_path=\"./label_map.json\"\n                          )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_text = x_cleaned[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_text)\nprint(\"\\n\\n===========================\")\nprint(\"Results: \",deploy_model.result(test_text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* And yes, it was real!","metadata":{}}]}