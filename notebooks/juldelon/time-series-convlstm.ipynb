{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-14T06:59:48.946264Z","iopub.execute_input":"2021-06-14T06:59:48.946621Z","iopub.status.idle":"2021-06-14T06:59:53.710564Z","shell.execute_reply.started":"2021-06-14T06:59:48.946534Z","shell.execute_reply":"2021-06-14T06:59:53.709801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"code","source":"def df_formating(df, station_to_drop=[]):    \n    df['day'] = df.date.dt.dayofyear\n    df['hour'] = df.date.dt.hour\n    agg_func = {'dd':np.mean, 'ff':np.mean, 'precip':np.sum, 'hu':np.mean, 't':np.mean, 'td':np.mean}\n    parameter = df.groupby(['number_sta','day','hour']).agg(agg_func)\n    parameter['dd'].fillna(0)\n    parameter['w_x'], parameter['w_y'] = formating_wind(parameter['dd'], parameter['ff'])\n    parameter = parameter.drop(columns=['dd','ff'])\n    inspect = parameter.isna().groupby(level=0).sum()\n    if station_to_drop:\n        to_drop = station_to_drop\n    else:\n        to_drop = inspect.index[inspect.sum(axis=1) != 0].to_list()\n    parameter.drop(index= to_drop, level=0, inplace=True)\n    return parameter, to_drop","metadata":{"execution":{"iopub.status.busy":"2021-06-14T06:59:53.712385Z","iopub.execute_input":"2021-06-14T06:59:53.712732Z","iopub.status.idle":"2021-06-14T06:59:53.72056Z","shell.execute_reply.started":"2021-06-14T06:59:53.712671Z","shell.execute_reply":"2021-06-14T06:59:53.719767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def formating_wind(w_direction, w_intensity):\n    w_direction.fillna(0,inplace=True)\n    w_direction = np.deg2rad(90 - w_direction)\n    w_x = np.cos(w_direction)*w_intensity\n    w_y = np.sin(w_direction)*w_intensity\n    return w_x, w_y","metadata":{"execution":{"iopub.status.busy":"2021-06-14T07:31:45.8828Z","iopub.execute_input":"2021-06-14T07:31:45.883121Z","iopub.status.idle":"2021-06-14T07:31:45.887632Z","shell.execute_reply.started":"2021-06-14T07:31:45.883091Z","shell.execute_reply":"2021-06-14T07:31:45.88679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# station_position = df.groupby('number_sta').mean().loc[:,['lon','lat','height_sta']]\n# station_position","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import cartopy.crs as ccrs\n# import cartopy.feature as cfeature\n# # Coordinates of studied area boundaries (in °N and °E)\n# lllat = 46.25  #lower left latitude\n# urlat = 51.896  #upper right latitude\n# lllon = -5.842  #lower left longitude\n# urlon = 2  #upper right longitude\n# extent = [lllon, urlon, lllat, urlat]\n\n# fig = plt.figure(figsize=(9,5))\n\n# # Select projection\n# ax = plt.axes(projection=ccrs.PlateCarree())\n\n# # Plot the data\n# cond = inspect.sum(axis= 1) != 0\n# plt.scatter(station_position['lon'].loc[cond], station_position['lat'].loc[cond])\n\n# # Add coastlines and borders\n# ax.coastlines(resolution='50m', linewidth=1)\n# ax.add_feature(cfeature.BORDERS.with_scale('50m'))\n\n# # Adjust the plot to the area we defined \n# #/!\\# this line causes a bug of the kaggle notebook and clears all the memory. That is why this line is commented and so\n# # the plot is not completely adjusted to the data\n# # Show only the area we defined\n# ax.set_extent(extent)\n\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def df_to_images(parameter):    \n    images=np.empty((365*24,4,13,6))\n    images[:] = np.nan\n    n_sta = len(parameter.index.unique(level=0))\n    print(n_sta)\n    na = []\n    for i, station in enumerate(parameter.index.unique(level=0)):\n        for day in parameter.index.unique(level=1):\n            for hour in parameter.index.unique(level=2):\n                try:\n                    images[hour*(day-1), int(i/13), i%13,:] = parameter.loc[station, day, hour]\n                except:\n                    na += [(i, day, hour)]\n    return images","metadata":{"execution":{"iopub.status.busy":"2021-06-14T06:59:53.721984Z","iopub.execute_input":"2021-06-14T06:59:53.722604Z","iopub.status.idle":"2021-06-14T06:59:53.732377Z","shell.execute_reply.started":"2021-06-14T06:59:53.722563Z","shell.execute_reply":"2021-06-14T06:59:53.73152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def deal_with_na(images):    \n    images_copy = images.copy()\n    value_modified = 0\n    empty_slice = 0\n    for n in range(365*24):\n        for i in range(4):\n            for j in range(13):\n                for k in range(6):\n                    if np.isnan(np.nanmean(images_copy[n,:,:,k])):\n                        images_copy[n,:,:,k] = images_copy[n-1,:,:,k]\n                        empty_slice += 1/6\n                    elif np.isnan(images_copy[n,i,j,k]):\n                        images_copy[n,i,j,k] = np.nanmean(images_copy[n,:,:,k])\n                        value_modified += 1\n\n    print((value_modified/(365*24*4*13*6))*100)\n    print((empty_slice/(365*24*52))*100)\n    return images_copy","metadata":{"execution":{"iopub.status.busy":"2021-06-14T06:59:53.735346Z","iopub.execute_input":"2021-06-14T06:59:53.735661Z","iopub.status.idle":"2021-06-14T06:59:53.746134Z","shell.execute_reply.started":"2021-06-14T06:59:53.735634Z","shell.execute_reply":"2021-06-14T06:59:53.74521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rep_nan = {}\n# for index in na:\n#     if str(index[1])+' '+str(index[2]) not in rep_nan:\n#         rep_nan[str(index[1])+' '+str(index[2])] = 1\n#     else:\n#         rep_nan[str(index[1])+' '+str(index[2])] += 1\n# for key, value in rep_nan.items():\n#     if value == 52:\n#         print(key)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zone, year, param = 'NW', '2016', 'hu'\nfname = '/kaggle/input/meteonet/'+zone+'_Ground_Stations/'+zone+'_Ground_Stations/'+zone+'_Ground_Stations_'+year+\".csv\"\ndata2016 = pd.read_csv(fname,parse_dates=[4],infer_datetime_format=True)\nyear = '2017'\nfname = '/kaggle/input/meteonet/'+zone+'_Ground_Stations/'+zone+'_Ground_Stations/'+zone+'_Ground_Stations_'+year+\".csv\"\ndata2017 = pd.read_csv(fname,parse_dates=[4],infer_datetime_format=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T06:59:53.750325Z","iopub.execute_input":"2021-06-14T06:59:53.750626Z","iopub.status.idle":"2021-06-14T07:01:06.656551Z","shell.execute_reply.started":"2021-06-14T06:59:53.750601Z","shell.execute_reply":"2021-06-14T07:01:06.655578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data2016, station_to_drop = df_formating(data2016)\nimages2016 = df_to_images(data2016)\nimages2016 = deal_with_na(images2016)\nprint('2016: done')\nnp.save('/kaggle/working/images2016.npy', images2016)\n\ndata2017, _ = df_formating(data2017, station_to_drop)\nimages2017 = df_to_images(data2017)\nimages2017 = deal_with_na(images2017)\nprint('2017: done')\nnp.save('/kaggle/working/images2017.npy', images2017)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T07:31:58.30018Z","iopub.execute_input":"2021-06-14T07:31:58.30049Z","iopub.status.idle":"2021-06-14T07:40:39.32516Z","shell.execute_reply.started":"2021-06-14T07:31:58.30046Z","shell.execute_reply":"2021-06-14T07:40:39.322179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# images2016 = np.load('/kaggle/input/images2016.npy')\n# images2017 = np.load('/kaggle/input/images2017.npy')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"n_filters = 64\nkernel_size = (3,3)\ndropout=0.2\nrecurrent_dropout=0.2\ninput_horizon = 24","metadata":{"execution":{"iopub.status.busy":"2021-06-14T07:41:13.407582Z","iopub.execute_input":"2021-06-14T07:41:13.407926Z","iopub.status.idle":"2021-06-14T07:41:13.412415Z","shell.execute_reply.started":"2021-06-14T07:41:13.407897Z","shell.execute_reply":"2021-06-14T07:41:13.411338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.Sequential([\n    tf.keras.layers.BatchNormalization(input_shape=(input_horizon,4,13,6)),\n    tf.keras.layers.ConvLSTM2D(n_filters, kernel_size, padding='same', dropout=dropout, recurrent_dropout=recurrent_dropout),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(52*6, activation='relu')\n], name='Nostradamus')\n\nloss_fn = tf.keras.losses.MSE\nmodel.compile(optimizer='adam',\n              loss=loss_fn,\n              metrics=['accuracy'])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T07:41:15.143584Z","iopub.execute_input":"2021-06-14T07:41:15.14393Z","iopub.status.idle":"2021-06-14T07:41:17.598779Z","shell.execute_reply.started":"2021-06-14T07:41:15.143901Z","shell.execute_reply":"2021-06-14T07:41:17.597869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"batch_size = 16\nepochs = 10\ntrain_x, train_y, val_x, val_y = [], [], [], []\nfor i in range(330*24):\n    train_x.append(images2016[i:i+input_horizon,:,:,:])\n    train_y.append(images2016[i+input_horizon,:,:,:].flatten())\nfor i in range(330*24 + 24,363*24):\n    val_x.append(images2016[i:i+input_horizon,:,:,:])\n    val_y.append(images2016[i+input_horizon,:,:,:].flatten())\ntrain_x = np.array(train_x)\ntrain_y = np.array(train_y)\nval_x = np.array(val_x)\nval_y = np.array(val_y)\nhistory = model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, validation_data=(val_x, val_y))","metadata":{"execution":{"iopub.status.busy":"2021-06-14T07:41:20.720738Z","iopub.execute_input":"2021-06-14T07:41:20.721057Z","iopub.status.idle":"2021-06-14T07:46:49.680618Z","shell.execute_reply.started":"2021-06-14T07:41:20.721028Z","shell.execute_reply":"2021-06-14T07:46:49.679826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def anticipate(model, input_data, output_horizon):\n    output = []\n    for i in range(output_horizon):\n        next_hour = model.predict(input_data)\n        output += [next_hour]\n        input_data[:-1] = input_data[1:]\n        input_data[-1] = next_hour\n    return output","metadata":{"execution":{"iopub.status.busy":"2021-06-14T07:46:49.682736Z","iopub.execute_input":"2021-06-14T07:46:49.683089Z","iopub.status.idle":"2021-06-14T07:46:49.689897Z","shell.execute_reply.started":"2021-06-14T07:46:49.683051Z","shell.execute_reply":"2021-06-14T07:46:49.688956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output_horizon = 24\n# test_x, test_y = [], []\n# for i in range(300*24):\n#     test_x.append(images2017[i:i+input_horizon,:,:,:])\n#     y = []\n#     for j in range(output_horizon):\n#         y += images2017[i+input_horizon+j,:,:,:].flatten()\n#     test_y.append(y)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T07:46:49.691153Z","iopub.execute_input":"2021-06-14T07:46:49.691662Z","iopub.status.idle":"2021-06-14T07:46:49.720174Z","shell.execute_reply.started":"2021-06-14T07:46:49.691622Z","shell.execute_reply":"2021-06-14T07:46:49.718538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# next_hours_error = np.zeros((100,24))\n# for i in range(100):\n#     out = anticipate(model, test_x[i], output_horizon)","metadata":{},"execution_count":null,"outputs":[]}]}