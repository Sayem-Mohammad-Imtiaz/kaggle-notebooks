{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, learning_curve, train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler, PolynomialFeatures\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (20, 10)\nsns.set_style(\"whitegrid\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's take a look at the dataset","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv(\"../input/california-housing-prices/housing.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create a heatmap to see which features are correlated the most with the label. We'll run it again after creating some interaction (also called synthetic) features.","metadata":{}},{"cell_type":"code","source":"correlation_matrix = dataset.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(correlation_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_matrix[\"median_house_value\"].drop([\"median_house_value\"]).sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import matplotlib.image as mpimg\n\n#california_map = mpimg.imread(\"./california_map.gif\")\n\nmin_x, max_x = dataset[\"longitude\"].min(), dataset[\"longitude\"].max()\nmin_y, max_y = dataset[\"latitude\"].min(), dataset[\"latitude\"].max()\n\np = dataset.plot(kind=\"scatter\",\n                 x=\"longitude\",\n                 y=\"latitude\",\n                 s=dataset[\"population\"]*0.05,\n                 alpha=0.5,\n                 c=dataset[\"median_house_value\"],\n                 cmap=\"jet\")\n\n#p.imshow(california_map, extent=[min_x, max_x, min_y, max_y], alpha=0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\ndef hist_all(dataset):\n    fig, ax = plt.subplots(nrows=int(math.ceil(len(dataset.columns)/2)), ncols=2, figsize=(20, 20))\n    for i, col in enumerate(dataset.columns):\n        a = sns.histplot(dataset[col], ax=ax.flatten()[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist_all(dataset.drop([\"longitude\", \"latitude\"], axis=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scatter_against_median_value(dataset):\n    fig, ax = plt.subplots(nrows=len(dataset.columns), ncols=1, figsize=(20, 60))\n    for i, col in enumerate(dataset.drop([\"median_house_value\"], axis=1).columns):\n        a = sns.scatterplot(data=dataset, x=col, y=\"median_house_value\", ax=ax.flatten()[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scatter_against_median_value(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's worth noting a couple of things:\n\nFirst off, what we have are mostly discrete variables. _ocean\\_proximity_ is an obvious one (since it's a categorical variable), but also the age, number of rooms/bedrooms and households will be integers. Only population and median income are continuous. We'll look at latitude and longitude separately, since they present a different kind of information.\n\nSecondly, we see there are some artifacts in the data. They are visible as horizontal lines in our scatterplots. It seems that some values were rounded to the nearest value. Also, the data seems to be \"capped\" or rounded down to 500k in median house value.\n\nLet's copy the dataset into __dataset_expl__ and try some magic to make it more useful for us. But first, we'll create a baseline by training and evaluating a LinearRegression model, so that we can evaluate our future modifications against it.","metadata":{}},{"cell_type":"markdown","source":"So we are miscalculating the price by (roughly) 1/3rd of the average house value. Not awe inspiring, but it's just the baseline we'll (hopefully) improve on as we dial in our final model shape.\n\nLet's continue by deleting all rows that have __median_house_value__ set to 500k. If that's a lot of rows, me might reconsider!","metadata":{}},{"cell_type":"code","source":"len(dataset[dataset[\"median_house_value\"] == 500_000])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now look at outliers from the other (numerical) columns. We'll use the z-score for this.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=4, ncols=2, figsize=(20, 20))\nfor i, col in enumerate(dataset.drop([\"longitude\", \"latitude\", \"ocean_proximity\"], axis=1).columns):\n    zscore_limit = np.mean(dataset[col]) + 3*(np.std(dataset[col]))\n    graph = sns.histplot(dataset[col], ax=ax.flatten()[i])\n    graph.vlines(zscore_limit, 0, 800, color=\"r\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def boxplot_all(dataset):\n    fig, ax = plt.subplots(nrows=int(math.ceil(len(dataset.columns)/2)), ncols=2, figsize=(20, 20))\n    for i, col in enumerate(dataset.columns):\n        a = sns.boxplot(data=dataset, x=col, ax=ax.flatten()[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"boxplot_all(dataset.drop([\"ocean_proximity\"], axis=1))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So those graphs give us two separate views of outliers.\n\nAs we can see, the boxplot (which uses IQR) is much more \"strict\" in deciding what is an outlier, as opposed to calculating the zscore and drawing the line at 3x standard deviations from the mean. We probably should decide between the two (or a variation, like choosing more/less than 3x the standard deviations for the zscore) separately for each variable. We might also decide not to remove any outliers.\n\nThe outliers don't seem to be concerning, as they do form a continous distribution with the rest of the data. We will not be removing any data at this stage.\n\nLet's try creating synthetic features now. Here are some ideas:\n- median income per household\n- median income per person\n- people per household\n- rooms per household\n- bedrooms per household\n\nWe'll create all fo those, and check the correlation matrix to see if they seem relevant.","metadata":{}},{"cell_type":"code","source":"dataset_synth = dataset.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"But first, we need to impute the missing values to aviod errors","metadata":{}},{"cell_type":"code","source":"dataset_synth[\"ocean_proximity\"] = OrdinalEncoder().fit_transform(dataset_synth[[\"ocean_proximity\"]])\ndataset_synth[:] = SimpleImputer().fit_transform(dataset_synth)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_synth[\"income_per_household\"] = dataset_synth[\"median_income\"] / dataset_synth[\"households\"]\ndataset_synth[\"income_per_person\"] = dataset_synth[\"median_income\"] / dataset_synth[\"population\"]\ndataset_synth[\"population_per_household\"] = dataset_synth[\"population\"] / dataset_synth[\"households\"]\ndataset_synth[\"rooms_per_household\"] = dataset_synth[\"total_rooms\"] / dataset_synth[\"households\"]\ndataset_synth[\"bedrooms_per_household\"] = dataset_synth[\"total_bedrooms\"] / dataset_synth[\"households\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_matrix = dataset_synth.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(correlation_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_matrix[\"median_house_value\"].drop([\"median_house_value\"]).sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ColumnSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, selected_columns):\n        self.selected_columns = selected_columns\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return X[self.selected_columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = list(dataset.drop([\"median_house_value\"], axis=1).columns)\n\nincome_idx = columns.index(\"median_income\")\nhouseholds_idx = columns.index(\"households\")\npopulation_idx = columns.index(\"population\")\nrooms_idx = columns.index(\"total_rooms\")\nbedrooms_idx = columns.index(\"total_bedrooms\")\n\nclass AddSyntheticFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        income_per_household = X[:, income_idx] / X[:, households_idx]\n        income_per_person = X[:, income_idx] / X[:, population_idx]\n        population_per_household = X[:, population_idx] / X[:, households_idx]\n        rooms_per_household = X[:, rooms_idx] / X[:, households_idx]\n        bedrooms_per_household = X[:, bedrooms_idx] / X[:, households_idx]\n        \n        return np.c_[X,\n                     income_per_household, income_per_person,\n                     population_per_household, rooms_per_household,\n                     bedrooms_per_household]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_columns = list(dataset.drop([\"ocean_proximity\", \"median_house_value\"], axis=1).columns)\ncategorical_columns = [\"ocean_proximity\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_num = Pipeline(\n    steps=[\n        (\"select_columns\", ColumnSelector(numerical_columns)),\n        (\"impute\", SimpleImputer(strategy=\"mean\")),\n        (\"add_synth_features\", AddSyntheticFeatures()),\n        (\"scale\", StandardScaler()),\n    ])\n\npipeline_cat = Pipeline(\n    steps=[\n        (\"select_columns\", ColumnSelector(categorical_columns)),\n        (\"ordinal_encode\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=10))\n    ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import FeatureUnion","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocess = FeatureUnion(n_jobs=-1,\n                          transformer_list=[\n                              (\"num_columns\", pipeline_num),\n                              (\"cat_columns\", pipeline_cat)\n                          ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_predictor(model):\n    _predictor = Pipeline(\n        steps=[\n            (\"preprocess\", preprocess),\n            (\"model\", model)\n        ])\n    return _predictor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_pipeline = make_predictor(RandomForestRegressor(random_state=0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gb_pipeline = make_predictor(GradientBoostingRegressor(random_state=0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def score_model(model, X, y):\n    scores = cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\")\n    return np.mean(np.sqrt(np.abs(scores)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a variable to stratify the dataset split by\ndataset[\"median_income_class\"] = pd.cut(dataset[\"median_income\"], bins=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train, dataset_test = train_test_split(dataset, stratify=dataset[[\"median_income_class\"]])\ndataset_train = dataset_train.drop([\"median_income_class\"], axis=1)\ndataset_test = dataset_test.drop([\"median_income_class\"], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will be removing rows which have a price of 500k dollars, because (as we saw in the viz stage) there seems to be a lot of data that is either wrong, or cause by some sort of weird rounding/compression. We do this only for the training stage, though. We will not be removing those elements when testing our final model - it should still give a good, \"generalised\" outcome.","metadata":{}},{"cell_type":"code","source":"X_train = dataset_train[dataset_train[\"median_house_value\"] < 500_000].drop([\"median_house_value\"], axis=1)\ny_train = dataset_train[dataset_train[\"median_house_value\"] < 500_000][\"median_house_value\"].copy()\navg_price_train = y_train.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = dict()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores['rf'] = score_model(rf_pipeline, X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores['gb'] = score_model(gb_pipeline, X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = {\"algo\": scores.keys(),\n              \"score_usd\": [score for score in scores.values()],\n              \"score_vs_avg\": [(score/avg_price_train)*100 for score in scores.values()]}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(results).set_index(\"algo\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"There is a slight edge to a Random Forest, but we'll investigate some more.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import learning_curve\n\ndef draw_learning_curves(model, X, y, label=None): \n    size, train_score, test_score = learning_curve(model, X, y, scoring=\"neg_mean_squared_error\")\n    train_scores_mean = np.mean(np.sqrt(np.abs(train_score)), axis=1)\n    train_scores_std = np.std(np.sqrt(np.abs(train_score)), axis=1)\n    \n    test_scores_mean = np.mean(np.sqrt(np.abs(test_score)), axis=1)\n    test_scores_std = np.std(np.sqrt(np.abs(test_score)), axis=1)\n    \n    _, ax = plt.subplots(nrows=1, ncols=1)\n    \n    ax.plot(size, train_scores_mean, c=\"g\")\n    ax.fill_between(size, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"g\")\n    \n    ax.plot(size, test_scores_mean, c=\"r\")\n    ax.fill_between(size, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"r\")\n    \n    if label is not None:\n        ax.set_title(label)        \n    ax.set_xlabel(\"Training examples\")\n    ax.set_ylabel(\"Score\")   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_learning_curves(model=rf_pipeline, X=X_train, y=y_train, label=\"Random Forest\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_learning_curves(model=gb_pipeline, X=X_train, y=y_train, label=\"Gradient Boosting Regressor\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = dataset_test.drop([\"median_house_value\"], axis=1)\ny_test = dataset_test[\"median_house_value\"].copy()\navg_price_test = y_test.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rf_pipeline.set_params(model__max_depth=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gb_pipeline.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_pipeline.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_rf = cross_val_score(rf_pipeline, X_test, y_test, scoring=\"neg_mean_squared_error\")\nscore_rf = np.mean(np.sqrt(np.abs(score_rf)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_gb = cross_val_score(gb_pipeline, X_test, y_test, scoring=\"neg_mean_squared_error\")\nscore_gb = np.mean(np.sqrt(np.abs(score_gb)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Results on unseen data: \")\nprint(\"Random forest regressor ${:.2f} which is {:.2f}% of average price.\".format(score_rf, (score_rf/avg_price_test)*100))\nprint(\"Gradient boosting regressor ${:.2f} which is {:.2f}% of average price.\".format(score_gb, (score_gb/avg_price_test)*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}