{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"# To create interactive plots\nimport plotly.graph_objects as go\nimport json\nimport pandas as pd\nimport nltk\nfrom os import listdir\nfrom os.path import isfile, join\nimport seaborn as sns\nfrom tqdm import tqdm\nimport glob\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport os\nimport string\nfrom nltk.stem import WordNetLemmatizer\nfrom pattern.en import tag\nfrom nltk.corpus import wordnet as wn\nimport warnings\nimport scipy.sparse as sp \nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom collections import Counter\nfrom nltk.corpus import reuters\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords\nimport nltk.data\nimport math\nimport re\nfrom nltk import word_tokenize, pos_tag\nfrom nltk.corpus import wordnet as wn\nimport numpy as np\nimport pandas as pd\nimport scipy as sc\nstop_words = stopwords.words('english')\nideal_sent_length = 20.0\nstemmer = SnowballStemmer(\"english\")\nimport re\nfrom urllib.request import urlopen\n#import gensim \nimport numpy as np \nfrom time import time  # To time our operations\nfrom collections import defaultdict  # For word frequency\nimport re\nimport spacy  # For preprocessing\n\nimport logging  # Setting up the loggings to monitor gensim\n\nimport spacy\nimport joblib\nfrom tqdm import tqdm\nimport seaborn as sb\nfrom matplotlib import pyplot as plt\n\nwnl = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"path = r\"C:\\bur\\CORD-19-research-challenge\"\nall_json = glob.glob(f'{path}/**/*.json', recursive=True)\nlen(all_json)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"metadata_path = f'{path}/metadata.csv'\n# load metadata\nmetadata = pd.read_csv(metadata_path)\nmetadata.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"class FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            dic={'title':[]} \n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.metadataa = []\n            self.metadata = content['metadata']\n            meta=self.metadata\n            self.abstract = []\n            self.body_text = []\n            \n            \n           \n            # Abstract\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n                \n           \n                tit=meta['title']\n                dic['title'].append(tit)\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n            self.metadataa = tit\n    def __repr__(self):\n        return f'{self.metadataa}...'\nfirst_row = FileReader(all_json[0])\nprint(first_row)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dict_ = {'paper_id': [], 'abstract': [], 'body_text': [],'title':[]}\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) // 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    content = FileReader(entry)\n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n    dict_['title'].append(content.metadataa)\n    \n    \npapers = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text','title'])\npapers.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#append all json files to the corresponding metadata\nfull_df = papers\\\n    .merge(metadata.rename(columns={'sha':'paper_id'}).drop(['abstract','title'], axis=1), \n           on='paper_id', how='left')\n\nfull_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"full_df.to_csv('papers.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import csv\nimport pandas as pd\npapes=pd.read_csv('papers.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"papes.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(papes.title[822])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#for each column ,number of empty values\nfull_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import plotly.graph_objects as go\ncolumn=['title','abstract','body_text','publish_time','authors']\nfor col in column:\n    if full_df[col].isna().sum():\n    \n        # Count the filled and empty values in the column\n        tmp_dict = full_df[col].isna().value_counts().to_dict()\n       \n\n    else:\n        tmp_dict = {False: len(full_df[col]), True: 0}\n        \n    print('The column \"{}\" has:\\n\\n{} filled and\\n{} empty values.'.format(col, tmp_dict[False], tmp_dict[True]))\n\n\n    # Count the unique values in the column\n    nunique = full_df[col].nunique()\n    #duplicated=full_df[col].duplicated\n    dupl= full_df[full_df.duplicated([col])].__len__()\n    \n\n    print('\\n{} unique values are in the column.'.format(nunique))\n    print('\\n{} duplicated values are in the column.'.format(dupl))\n    colors = ['lightslategray',] * 6\n    colors[1] = 'rgb(158,202,225)'\n    colors[3]='rgb(158, 188, 225)'\n    colors[2]='rgb(213, 229, 245)'\n    colors[5]='rgb(122, 163, 240)'\n    colors[4]='rgb(158, 217, 225)'\n    colors[0]='rgb(90, 143, 242)'\n    #colors[2]='rgb(55, 83, 109)'\n    columns=['total','filled','empty','Unique','duplicated' ]\n\n    fig = go.Bar(x=columns, y=[len(full_df[col]),tmp_dict[False],tmp_dict[True],nunique, dupl],#marker_color='lightsalmon'\n                 marker_color=colors)\n    \n    layout = go.Layout( \n                       yaxis_title='Value Count', \n                       xaxis_title='{}'.format(col))\n\n    graph = go.Figure([fig], layout)\n    \n    \n    #graph.update_traces(marker_color='rgb(158,202,225)', marker_line_color='rgb(8,48,107)',marker_line_width=1.5, opacity=0.6)\n    graph.show()\n    graph.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#number of duplicated values \nfull_df[full_df.duplicated(['abstract','body_text','title','paper_id'])].__len__()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"full_df.drop_duplicates(['title'], inplace=True)\nfull_df.dropna(subset=['body_text'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#full_df[full_df.duplicated(['body_text'])].__len__()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(full_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#drop duplicated values\nfull_df=full_df.drop_duplicates(subset=['abstract','body_text','title','paper_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"full_df[full_df.duplicated(['abstract','body_text','title','paper_id'])].__len__()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(full_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"covid_terms =['covid', 'coronavirus disease 19', 'sars cov 2', '2019 ncov', '2019ncov', '2019 n cov', '2019n cov',\n              'ncov 2019', 'n cov 2019', 'coronavirus 2019', 'wuhan pneumonia', 'wuhan virus', 'wuhan coronavirus',\n              'coronavirus 2', 'covid-19', 'SARS-CoV-2', '2019-nCov']\nkeywords = ['sars-cov', 'sars', 'coronavirus', 'ace2', 'coronaviruses', 'ncov', 'covid-19', 'wuhan', 'spike', 'sars-cov-2','corona']\ncovid_terms = [elem.lower() for elem in covid_terms]\ncovid_terms = re.compile('|'.join(covid_terms))\nprint(covid_terms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def checkYear(date):\n    print(int(date[0:4]))\n    return int(date[0:4])\n\ndef checkCovid(row, covid_terms):\n    return bool(covid_terms.search(row['body_text'].lower()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"full_df['is_covid'] =full_df.apply(checkCovid, axis=1, covid_terms=covid_terms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_covid_only = full_df[full_df['is_covid']==True]\ndf_covid_only = df_covid_only.reset_index(drop=True)\nlen(df_covid_only)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import semanticscholar as sch\nfrom IPython.display import display, Latex, HTML, FileLink\nfrom sentence_transformers import SentenceTransformer, models\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def preprocessing(text):\n    # remove mail\n    text = re.sub(r'[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}', 'MAIL', text)\n    # remove doi\n    text = re.sub(r'https\\:\\/\\/doi\\.org[^\\s]+', 'DOI', text)\n    # remove https\n    text = re.sub(r'(\\()?\\s?http(s)?\\:\\/\\/[^\\)]+(\\))?', '\\g<1>LINK\\g<3>', text)\n    # remove single characters repeated at least 3 times for spacing error (e.g. s u m m a r y)\n    text = re.sub(r'(\\w\\s+){3,}', ' ', text)\n    # replace tags (e.g. [3] [4] [5]) with whitespace\n    text = re.sub(r'(\\[\\d+\\]\\,?\\s?){3,}(\\.|\\,)?', ' \\g<2>', text)\n    # replace tags (e.g. [3, 4, 5]) with whitespace\n    text = re.sub(r'\\[[\\d\\,\\s]+\\]', ' ',text)\n     # replace tags (e.g. (NUM1) repeated at least 3 times with whitespace\n    text = re.sub(r'(\\(\\d+\\)\\s){3,}', ' ',text)\n    # replace '1.3' with '1,3' (we need it for split later)\n    text = re.sub(r'(\\d+)\\.(\\d+)', '\\g<1>,\\g<2>', text)\n    # remove all full stops as abbreviations (e.g. i.e. cit. and so on)\n    text = re.sub(r'\\.(\\s)?([^A-Z\\s])', ' \\g<1>\\g<2>', text)\n    # correctly spacing the tokens\n    text = re.sub(r' {2,}', ' ', text)\n    text = re.sub(r'\\.{2,}', '.', text)\n    text=re.sub(r\"[<>()(,)|&©ø\\[\\]\\'\\\";?~*!]\", ' ', text) #remove <>()|&©ø\"',;?~*!\n    text=re.sub(\"(\\\\t)\", ' ', text) #remove escape charecters\n    text=re.sub(\"(\\\\r)\", ' ', text) \n    text=re.sub(\"(\\\\n)\", ' ', text)\n    text= re.sub(\"(\\s+)\",' ',text) #remove multiple space\n    # return lowercase text\n    return text.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_covid_only['preproc_body_text'] = df_covid_only['body_text'].apply(preprocessing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_covid_only['body_text_parags'] = df_covid_only['preproc_body_text'].apply(splitParagraph)\ndf_covid_only.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_covid_only.to_csv('sabrina.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"final_df=pd.DataFrame()\nfinal_df['paper_id']=full_df['paper_id']\nfinal_df['doi']=full_df['doi']\nfinal_df['title']=full_df['title']\nfinal_df['body_text']=full_df['body_text']\nfinal_df.reset_index(drop=True, inplace=True)\nfinal_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"final_df['body_text'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"SUMMARY_LENGTH = 12  # number of sentences in final summary\nstop_words = stopwords.words('english')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"class Summarizer():\n\n    def penn_to_wn(self,tag):\n        \n        if tag.startswith('N'):\n            return 'n'\n \n        if tag.startswith('V'):\n        \n            return 'v'\n \n        if tag.startswith('J'):\n            return 'a'\n \n        if tag.startswith('R'):\n            return 'r'\n        return None \n \n\n    def tagged_to_synset(self,word, tag):\n        wn_tag = self.penn_to_wn(tag)\n    \n        if wn_tag is None:\n            return None\n \n        try:\n        \n            return wn.synsets(word, wn_tag)[0]\n        except:\n            return None\n \n    def sentence_similarity(self,sentence1, sentence2):\n       \n        # Tokenize and tag\n        sentence1 = pos_tag(word_tokenize(sentence1))\n        \n        sentence2 = pos_tag(word_tokenize(sentence2))\n      \n \n        # Get the synsets for the tagged words\n        synsets1 = [self.tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n        \n        synsets2 = [self.tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n       \n \n    # Filter out the Nones\n        synsets1 = [ss for ss in synsets1 if ss]\n      \n        synsets2 = [ss for ss in synsets2 if ss]\n        \n \n        score, count = 0, 0\n        for synset in synsets1:\n            max_sim = 0.0\n            maxx=0\n            for ss in synsets2:\n                \n                sim=wn.wup_similarity(synset, ss)\n                if sim is not None and sim > max_sim:\n                \n                       max_sim = sim\n                   \n           \n        \n            if max_sim is not None and max_sim!=0:\n                score += max_sim\n                count += 1\n \n    # Average the values\n        if count!=0:\n            score /= count\n            return score   \n    def __init__(self, article):\n           \n            self._articles = []\n            #i=1\n           \n            for row in article:\n             #   if i<=5:\n                    title=row[1]\n                    #print(title)\n                    body=row[3].replace('\\n', ' ')\n                    #print(body)\n                    paper_id=row[0]\n                    #doi=row[0]\n                    if title!=''and body!='':\n                        self._articles.append((paper_id,title,body))\n              #      i=i+1\n            \n            \n    def valid_input(self, headline, article_text):\n        return headline != '' and article_text != ''    \n    def normalize_corpus(self,corpus, lemmatize=True):\n        \n        normalized_corpus = []    \n        for text in corpus:\n            if lemmatize:\n                text = self.lemmatize_text(text)\n            else:\n                text = text.lower()\n            text = self.remove_special_characters(text)\n            text = self.remove_stopwords(text)\n            normalized_corpus.append(text)\n        return normalized_corpus\n    def pos_tag_text(self,text):\n        wnl = WordNetLemmatizer()\n        tagged_text = tag(text)\n        tagged_lower_text = [(word.lower(), self.penn_to_wn(pos_tag))\n                             for word, pos_tag in\n                             tagged_text]\n        return tagged_lower_text\n        \n         \n    def score(self,article,query):\n        \"\"\" Assign each sentence in the document a score\"\"\"\n        maxx=0\n        maxxx=0\n        Query=[]\n        Query.append(query)\n        headline = article[1]\n        sentences = self.split_into_sentences(article[2])\n        \n        querry=self.remove_smart_quotes(query)\n        sentencess=self.split_into_sentences(article[2])\n        sentencess.append(querry)\n        \n        #queryy=self.split_into_sentences(query)\n        norm_corpus =self.normalize_corpus(sentences, lemmatize=True)\n        norm_corpuss=self.normalize_corpus(sentencess, lemmatize=True)\n        \n        norm_model_answer =  self.normalize_corpus(Query, lemmatize=True) \n        norm_model_answerquery =  self.normalize_corpus(Query, lemmatize=True) \n        \n        vectorizer, corpus_features = self.build_feature_matrix(norm_corpus,feature_type='frequency')\n        vectorizerq, query_features = self.build_feature_matrix(norm_corpuss,feature_type='frequency')\n        # extract features from model_answer\n        model_answer_features = vectorizer.transform(norm_model_answer)\n        model_answer_featuresquery = vectorizerq.transform(norm_model_answerquery)\n        \n        doc_lengths = [len(doc.split()) for doc in norm_corpus]\n        doc_lengthss = [len(doc.split()) for doc in norm_corpuss] \n        \n        #query_lengths = [len(doc.split()) for doc in norm_query]  \n        avg_dl = np.average(doc_lengths) \n        avg_qr = np.average(doc_lengthss)\n        \n        corpus_term_idfs = self.compute_corpus_term_idfs(corpus_features, norm_corpus)\n        corpus_term_idfsquery = self.compute_corpus_term_idfs(query_features, norm_corpuss)\n        \n        for index, doc in enumerate(Query):\n    \n            doc_features = model_answer_features[index]\n            #doc_featuress = model_answer_featuresquery[index]\n            self.bm25_scores = self.compute_bm25_similarity(doc_features,corpus_features,doc_lengths,avg_dl,corpus_term_idfs,k1=1.5, b=0.75)         \n            maxxx=max(self.bm25_scores)\n            self.semantic_similarity_scores=[]\n     \n        for indexx, doc in enumerate(Query):\n\n                doc_featuress = model_answer_featuresquery[indexx]\n                self.bm25_scoresquery = self.compute_bm25_similarityqr(doc_featuress,query_features,doc_lengthss,avg_qr,corpus_term_idfsquery,k1=1.5, b=0.75)\n            \n                maxx=max(self.bm25_scoresquery)\n        for i, s in enumerate(sentences):\n                score1=self.sentence_similarity(s,query)\n                score2=self.sentence_similarity(query,s)\n                if score1 is not None and score2 is not None:\n                    score=(score1+score2)/2\n                    self.semantic_similarity_scores.append(score)\n                elif score1 is not None and score2 is None:\n                    self.semantic_similarity_scores.append(score1)\n                elif score2 is not None and score1 is None:\n                    self.semantic_similarity_scores.append(score2)\n        doc_index=0\n        sim_score=[]\n        for score_tuple in zip(self.semantic_similarity_scores,self.bm25_scores):\n            sim_score.append((score_tuple[0]+(score_tuple[1]/maxxx))/2)\n        for tuple_ in zip(sentences,sim_score):\n            s=tuple_[0]\n            self._scores[s]=tuple_[1]\n        \n        \n                    \n              \n    def generate_summaries(self,query):\n        \n                self.dict_ = {'task':[],'paper_id':[],'title':[],'summary': [],'score':[],'sentences':[]}\n                jj=1\n                ii=1\n          \n                #tasks=['what is the immune system response to 2019-ncov ?'\n                   \n                 #  ]\n           \n    \n                #for query in tasks:\n                for article in self._articles:\n                    self._scores = Counter()\n                    self.score(article,query)\n                    highest_scoring = self._scores.most_common(SUMMARY_LENGTH)\n                    #totalsentences = splitParagraph(article[2])\n                    #summarylist=[]\n                    #summr=[sent[0] for sent[0] in highest_scoring]\n                   \n                    #for sentence in totalsentences:\n                     #   for sumsen in  summr:\n                      #      if sentence==sumsen:\n                       #         summarylist.append(sentence)        \n                    # Appends highest scoring \"representative\" sentences, returns as a single summary paragraph.\n                    summary=' '.join([sent[0] for sent in highest_scoring])\n                    s=0\n                    for scr in highest_scoring:\n                        s=s+scr[1]\n                    s=s/12    \n                    \n                    \n                    ''' \n                    print('**task**')\n                    print(query)\n                    print(\" **Title: **\")\n                    print(article[1])\n                    print(highest_scoring)\n                    print('**summary**')\n                    print(summary)\n                    print('____________________________________________________________________________________')\n                    '''   \n                    \n                    self.dict_['sentences'].append(highest_scoring)\n                    self.dict_['summary'].append(summary)\n                    self.dict_['title'].append(article[1])\n                    self.dict_['paper_id'].append(article[0])\n                    self.dict_['task'].append(query)\n                    self.dict_['score'].append(s)\n                self.papers = pd.DataFrame(self.dict_, columns=['task','paper_id','title','summary','score','sentences'])\n                return self.papers\n       \n\n    def remove_smart_quotes(self, text):\n       \n       \n        #text=re.sub(\"([\\(\\[].*?[\\)\\]][\\(\\[].*?[\\)\\]]+)\", ' ', text)\n        text=re.sub(\"[\\(\\[].*?[\\)\\]]\", '', text)\n        \n        try:\n            url = re.search(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)', text)\n            repl_url = url.group(3)\n            text = re.sub(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)',repl_url, text)\n        except:\n            pass #there might be emails with no url in them\n        #text=re.sub(\"[\\[]()*?[\\]]\", \"\", text)#remove in-text citation\n        \n        text=re.sub(r\"[<>()(,)|&©ø\\[\\]\\'\\\";?~*!]\", ' ', text) #remove <>()|&©ø\"',;?~*!\n        text=re.sub(\"(\\\\t)\", ' ', text) #remove escape charecters\n        text=re.sub(\"(\\\\r)\", ' ', text) \n        text=re.sub(\"(\\\\n)\", ' ', text)\n        text= re.sub(\"(\\s+)\",' ',text) #remove multiple space\n        \n        text = re.sub(r'[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}', 'MAIL', text)\n    # remove doi\n        text = re.sub(r'https\\:\\/\\/doi\\.org[^\\s]+', 'DOI', text)\n    # remove https\n        text = re.sub(r'(\\()?\\s?http(s)?\\:\\/\\/[^\\)]+(\\))?', '\\g<1>LINK\\g<3>', text)\n    # remove single characters repeated at least 3 times for spacing error (e.g. s u m m a r y)\n        text = re.sub(r'(\\w\\s+){3,}', ' ', text)\n    # replace tags (e.g. [3] [4] [5]) with whitespace\n        text = re.sub(r'(\\[\\d+\\]\\,?\\s?){3,}(\\.|\\,)?', ' \\g<2>', text)\n    # replace tags (e.g. [3, 4, 5]) with whitespace\n        text = re.sub(r'\\[[\\d\\,\\s]+\\]', ' ',text)\n     # replace tags (e.g. (NUM1) repeated at least 3 times with whitespace\n        text = re.sub(r'(\\(\\d+\\)\\s){3,}', ' ',text)\n    # replace '1.3' with '1,3' (we need it for split later)\n        text = re.sub(r'(\\d+)\\.(\\d+)', '\\g<1>,\\g<2>', text)\n    # remove all full stops as abbreviations (e.g. i.e. cit. and so on)\n        text = re.sub(r'\\.(\\s)?([^A-Z\\s])', ' \\g<1>\\g<2>', text)\n    # correctly spacing the tokens\n        text = re.sub(r' {2,}', ' ', text)\n        text = re.sub(r'\\.{2,}', '.', text)\n        text=re.sub(r\"[<>()(,)|&©ø\\[\\]\\'\\\";?~*!]\", ' ', text) #remove <>()|&©ø\"',;?~*!\n        text=re.sub(\"(\\\\t)\", ' ', text) #remove escape charecters\n        text=re.sub(\"(\\\\r)\", ' ', text) \n        text=re.sub(\"(\\\\n)\", ' ', text)\n        text= re.sub(\"(\\s+)\",' ',text) #remove multiple space\n        text=re.sub(\"doi\", ' ',text)\n        text=re.sub(\"bioRxiv\", ' ',text)\n        text=re.sub(\"author\", ' ',text)\n        text=re.sub(\"authors\", ' ',text)\n        text=re.sub(\"authors\", ' ',text)\n        text=re.sub(\"All rights reserved\", ' ',text)\n        text=re.sub(\"preprint\", ' ',text)\n    # return lowercase text\n        return text.lower()\n        \n        \n        \n      \n\n\n    def split_into_sentences(self, text):\n        tok = nltk.data.load('tokenizers/punkt/english.pickle')\n        sentences = tok.tokenize(self.remove_smart_quotes(text))\n        sentences = [sent.replace('\\n', '') for sent in sentences if len(sent) > 20]   \n        words=['author','authors''permissions','doi']\n                                  \n       \n        \n                 \n\n        return sentences\n    def lemmatize_text(self,text):\n    \n        pos_tagged_text = self.pos_tag_text(text)\n        lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n                             else word                     \n                             for word, pos_tag in pos_tagged_text]\n        lemmatized_text = ' '.join(lemmatized_tokens)\n        return lemmatized_text\n\n    def remove_special_characters(self,text):\n        tokens = self.tokenize_text(text)\n        pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n        filtered_tokens = filter(None, [pattern.sub(' ', token) for token in tokens])\n        filtered_text = ' '.join(filtered_tokens)\n        return filtered_text\n\n    def remove_stopwords(self,text):\n        stopword_list = nltk.corpus.stopwords.words('english')\n        tokens = self.tokenize_text(text)\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n        filtered_text = ' '.join(filtered_tokens)    \n        return filtered_text\n\n    \n  \n    def build_feature_matrix(self,documents, feature_type='frequency',\n                         ngram_range=(1, 1), min_df=0.0, max_df=1.0):\n\n        feature_type = feature_type.lower().strip()  \n    \n        if feature_type == 'binary':\n            vectorizer = CountVectorizer(binary=True, min_df=min_df,\n                                     max_df=max_df, ngram_range=ngram_range)\n        elif feature_type == 'frequency':\n            vectorizer = CountVectorizer(binary=False, min_df=min_df,\n                                     max_df=max_df, ngram_range=ngram_range)\n        elif feature_type == 'tfidf':\n            vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, \n                                     ngram_range=ngram_range)\n        else:\n            raise Exception(\"Wrong feature type entered. Possible values: 'binary', 'frequency', 'tfidf'\")\n\n        feature_matrix = vectorizer.fit_transform(documents).astype(float)\n    \n        return vectorizer, feature_matrix\n\n    def compute_corpus_term_idfs(self,corpus_features, norm_corpus):\n    \n        dfs = np.diff(sp.csc_matrix(corpus_features, copy=True).indptr)\n        dfs = 1 + dfs # to smoothen idf later\n        total_docs = 1 + len(norm_corpus)\n        idfs = 1.0 + np.log(float(total_docs) / dfs)\n        return idfs\n    def compute_bm25_similarity(self,doc_features, corpus_features,\n                            corpus_doc_lengths, avg_doc_length,\n                            term_idfs, k1=1.5, b=0.75):\n    # get corpus bag of words features\n        corpus_features = corpus_features.toarray()\n    # convert query document features to binary features\n    # this is to keep a note of which terms exist per document\n        doc_features = doc_features.toarray()[0]\n        doc_features[doc_features >= 1] = 1\n    \n    # compute the document idf scores for present terms\n        doc_idfs = doc_features * term_idfs\n    # compute numerator expression in BM25 equation\n        numerator_coeff = corpus_features * (k1 + 1)\n        numerator = np.multiply(doc_idfs, numerator_coeff)\n    # compute denominator expression in BM25 equation\n        denominator_coeff =  k1 * (1 - b + \n                                (b * (corpus_doc_lengths / \n                                        avg_doc_length)))\n        denominator_coeff = np.vstack(denominator_coeff)\n        denominator = corpus_features + denominator_coeff\n    # compute the BM25 score combining the above equations\n        bm25_scores = np.sum(np.divide(numerator,\n                                   denominator),\n                         axis=1) \n    \n        return bm25_scores\n    def tokenize_text(self,text):\n        tokens = nltk.word_tokenize(text) \n        tokens = [token.strip() for token in tokens]\n        return tokens\n    def compute_bm25_similarityqr(self,doc_features, corpus_features,\n                            corpus_doc_lengths, avg_doc_length,\n                            term_idfs, k1=1.5, b=0.75):\n    # get corpus bag of words features\n        corpus_features = corpus_features.toarray()\n    # convert query document features to binary features\n    # this is to keep a note of which terms exist per document\n        doc_features = doc_features.toarray()[0]\n        doc_features[doc_features >= 1] = 1\n    \n    # compute the document idf scores for present terms\n        doc_idfs = doc_features * term_idfs\n    # compute numerator expression in BM25 equation\n        numerator_coeff = corpus_features * (k1 + 1)\n        numerator = np.multiply(doc_idfs, numerator_coeff)\n    # compute denominator expression in BM25 equation\n        denominator_coeff =  k1 * (1 - b + \n                                (b * (corpus_doc_lengths / \n                                        avg_doc_length)))\n        denominator_coeff = np.vstack(denominator_coeff)\n        denominator = corpus_features + denominator_coeff\n    # compute the BM25 score combining the above equations\n        bm25_scores = np.sum(np.divide(numerator,\n                                   denominator),\n                         axis=1) \n    \n        return bm25_scores\n\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import csv\nimport nltk\ncsv.field_size_limit(100000000)\nresults=pd.DataFrame()\nf= open(\"sabrina.csv\", encoding=\"utf-8-sig\")\nreader = csv.reader(f, delimiter=',')\nnext(reader)\n\nsummaries= Summarizer(reader)\nresults=summaries.generate_summaries('what is the immune system response to covid-19')\nresultss = results.sort_values(by='score', ascending=False)\nresultss.head(50)\n\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from rank_bm25 import BM25Okapi\nimport pandas as pd\ndata = pd.read_csv(\"sabrina2.csv\")\nprint(len(data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from rank_bm25 import BM25Okapi\nenglish_stopwords = list(set(stopwords.words('english')))\n\ndef strip_characters(text):\n    t = re.sub('\\(|\\)|:|,|;|\\.|’|”|“|\\?|%|>|<', '', text)\n    t = re.sub('/', ' ', t)\n    t = t.replace(\"'\",'')\n    return t\n\ndef clean(text):\n    t = text.lower()\n    t = strip_characters(t)\n    return t\n\ndef tokenize(text):\n    words = nltk.word_tokenize(text)\n    return list(set([word for word in words \n                     if len(word) > 1\n                     and not word in english_stopwords\n                     and not (word.isnumeric() and len(word) is not 4)\n                     and (not word.isnumeric() or word.isalpha())] )\n               )\n\ndef preprocess(text):\n    t = clean(text)\n    tokens = tokenize(t)\n    return tokens\n\nclass SearchResults:\n    \n    def __init__(self, \n                 data: pd.DataFrame,\n                 columns = None):\n        self.results = data\n        if columns:\n            self.results = self.results[columns]\n        print( \"self.results\")\n        print( self.results)\n            \n    def __getitem__(self, item):\n        return Paper(self.results.loc[item])\n    \n    def __len__(self):\n        return len(self.results)\n        \n    def _repr_html_(self):\n        return self.results._repr_html_()\n\nSEARCH_DISPLAY_COLUMNS = ['paper_id', 'title', 'abstract', 'body_text']\n\nclass WordTokenIndex:\n    \n    def __init__(self, \n                 corpus: pd.DataFrame, \n                 columns=SEARCH_DISPLAY_COLUMNS):\n        self.corpus = corpus\n        print(\"self.corpus\")\n        print(self.corpus)\n        raw_search_str = self.corpus.abstract.fillna('') + ' ' + self.corpus.title.fillna('') + ' ' + self.corpus.body_text.fillna('')\n        print(\"raw_search_str\")\n        print(raw_search_str)\n        self.index = raw_search_str.apply(preprocess).to_frame()\n        print( \"self.index\")\n        #print( self.index)\n        print( self.index[0])\n        self.index.columns = ['terms']\n        print(\"self.index.columns\")\n        print(self.index.columns)\n        self.index.index = self.corpus.index\n        print(\"self.index.index\")\n        print(self.index.index)\n        self.columns = columns\n        print(\"self.columns\")\n        print(self.columns)\n        return self.columns\n    \n    def search(self, search_string):\n        search_terms = preprocess(search_string)\n        print(\"search_terms\" )\n        print(search_terms )\n        result_index = self.index.terms.apply(lambda terms: any(i in terms for i in search_terms))\n        print(\"result_index\")\n        print(result_index )\n        results = self.corpus[result_index].copy().reset_index().rename(columns={'index':'paper'})\n        print(\"results\")\n        print(results)\n        print(\"SearchResults(results, self.columns + ['paper'])\")\n        print(SearchResults(results, self.columns + ['paper']))\n        return SearchResults(results, self.columns + ['paper'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"class RankBM25Index(WordTokenIndex):\n        \n    def __init__(self, corpus: pd.DataFrame, columns=SEARCH_DISPLAY_COLUMNS):\n        super().__init__(corpus, columns)\n        #print(\"self.index.terms.tolist()\")\n        #print(self.index.terms.tolist())\n        self.bm25 = BM25Okapi(self.index.terms.tolist())\n        print(\"self.bm25\")\n        print(self.bm25)\n        \n    def search(self, search_string):\n        search_terms = preprocess(search_string)\n        doc_scores = self.bm25.get_scores(search_terms)\n        print('doc_scores')\n        print(doc_scores)\n        ind = np.argsort(doc_scores)[::-1]\n        print('ind')\n        print(ind)\n        results = self.corpus.iloc[ind][self.columns]\n        print('results')\n        print(results)\n        results['Score'] = doc_scores[ind]\n        print(\"results['Score']\")\n        print(results['Score'])\n        results['orig_ind'] = ind\n        results['word'] = search_string\n        print(\"results['orig_ind']\")\n        print(results['orig_ind'])\n        results = results[results.Score > 0]\n        print(\"results\")\n        print(results)\n        return SearchResults(results.reset_index(), self.columns + ['Score', 'orig_ind','word'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"bm25_index = RankBM25Index(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\nresults = None\nadded = []\n#for3 word in keywords:\n    #print(word)\n    #print(\"word_result\")\nword_result = bm25_index.search('range of incubation periods for the disease in humans').results\nresults = word_result\n    \ndc = results.sort_values(by='Score', ascending=False)\n\ndc.reset_index(drop=True, inplace=True)\ndc.head(12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dc.to_csv('index.csv', index=False)\nimport csv\nimport nltk\ncsv.field_size_limit(100000000)\nresults=pd.DataFrame()\nf= open(\"index.csv\", encoding=\"utf-8-sig\")\nreader = csv.reader(f, delimiter=',')\nnext(reader)\n\nsummaries= Summarizer(reader)\nresults=summaries.generate_summaries('range of incubation periods for the disease in humans')\nresultss = results.sort_values(by='score', ascending=False)\nresultss.head(50)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#resultss.to_csv('results1.csv', index=False)\nresultss.to_csv('nov.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"resultss.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(resultss.summary[4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nword_cloud=WordCloud(width=400,height=300,background_color='white').generate(resultss.summary[732])\nword_cloud.to_image()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(resultss.summary[732])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nwordcloud = WordCloud().generate(resultss.summary[732])\nplt.imshow(wordcloud, interpolation=\"bilinear\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.imshow(wordcloud, interpolation=\"bilinear\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\nword_cloud=WordCloud(width=400,height=300,background_color='white').generate(resultss.summary[732])\nword_cloud.to_image()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nwordcloud = WordCloud().generate(resultss.summary[77])\nplt.imshow(wordcloud, interpolation=\"bilinear\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\nword_cloud=WordCloud(width=400,height=300,background_color='white').generate(resultss.summary[77])\nword_cloud.to_image()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(resultss.summary[77])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nwordcloud = WordCloud().generate(resultss.summary[355])\nplt.imshow(wordcloud, interpolation=\"bilinear\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\nword_cloud=WordCloud(width=800,height=300,background_color='white').generate(resultss.summary[335])\nword_cloud.to_image()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"word_cloud=WordCloud(width=800,height=300,background_color='white').generate(resultss.summary[544])\nword_cloud.to_image()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nwordcloud = WordCloud().generate(resultss.summary[394])\nplt.imshow(wordcloud, interpolation=\"bilinear\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"word_cloud=WordCloud(width=800,height=300,background_color='white').generate(resultss.summary[438])\nword_cloud.to_image()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"resultss.to_csv('wordnet+bm25.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"csv2=pd.read_csv('w.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"csv2.head()\ncsv2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\nfil=pd.read_csv('results1.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"wordnetID=[]\nwordnetScore=[]\n\nfor i in range(len(fil)):\n    if i<50:\n        wordnetID.append(fil.index[i])\n        wordnetScore.append(fil.score[i])\nprint(wordnetID)\nprint(wordnetScore)\nwordnetBM25_ID=[]\nwordnetBM25_Score=[]\n\nfor i in range(len(csv2)):\n     if i<50:\n        wordnetBM25_ID.append(csv2.index[i])\n        wordnetBM25_Score.append(csv2.score[i])\nprint(wordnetBM25_ID)\nprint(wordnetBM25_Score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(wordnetID, wordnetScore,'g')\nplt.plot(wordnetID, wordnetBM25_Score,'orange')\nplt.xlabel('ID of document')\nplt.ylabel('Score')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}