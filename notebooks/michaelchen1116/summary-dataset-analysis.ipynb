{"cells":[{"metadata":{"papermill":{"duration":0.018388,"end_time":"2021-01-05T21:29:42.15268","exception":false,"start_time":"2021-01-05T21:29:42.134292","status":"completed"},"tags":[],"cell_id":"00000-f33d4ed9-b447-41e3-a361-d8737070852a","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"# Summary dataset analysis\nThe goal of this analysis is to explore the summary dataset, detect abnormalities within this dataset, and filter for relevant data for the later analysis. \n\nThe `summary` dataframe is one of several in the Kaggle Stock Earnings dataset. It contains aggregate data based on the contents of the `earnings` and `prices` files.\n* `earnings` table provides ticker symbol along with the `date` and `release_time` \nwhen `eps_est` (earnings per share estimate) and `eps` (actual earnings per share) being released\n* `prices` table provides information about the stock prices, including columns such as \n`date` , `open`, `high`, `low`, `close`, `close_adjusted`, `volume`, and `split_coefficient`.\n\nA better understanding of this table will facilitate the process of combining those datasets for analysis and modeling.\n\nThis notebook presents the process of exploring the summary dataset and key insights discovered along the way."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-01-05T21:29:42.194051Z","iopub.status.busy":"2021-01-05T21:29:42.19328Z","iopub.status.idle":"2021-01-05T21:29:43.280472Z","shell.execute_reply":"2021-01-05T21:29:43.279851Z"},"papermill":{"duration":1.110444,"end_time":"2021-01-05T21:29:43.28063","exception":false,"start_time":"2021-01-05T21:29:42.170186","status":"completed"},"tags":[],"cell_id":"00001-e5f864d0-3c85-4596-bb99-073d074d54c6","deepnote_to_be_reexecuted":false,"source_hash":"3e13241c","execution_millis":792,"execution_start":1611116782086,"deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"# data analysis, wrangling and preprocessing\nimport numpy as np \nimport pandas as pd\n\n# deal with datetime object\nimport datetime\n\n# data visualization\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.025507,"end_time":"2021-01-05T21:29:43.32811","exception":false,"start_time":"2021-01-05T21:29:43.302603","status":"completed"},"tags":[],"cell_id":"00002-1a70c558-54b2-4d12-bab7-dc19857d0a0f","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"## Loading Data\nWe start by loading the datasets into Pandas DataFrames."},{"metadata":{"execution":{"iopub.execute_input":"2021-01-05T21:29:43.37374Z","iopub.status.busy":"2021-01-05T21:29:43.373041Z","iopub.status.idle":"2021-01-05T21:29:43.42542Z","shell.execute_reply":"2021-01-05T21:29:43.424726Z"},"papermill":{"duration":0.079521,"end_time":"2021-01-05T21:29:43.425532","exception":false,"start_time":"2021-01-05T21:29:43.346011","status":"completed"},"tags":[],"cell_id":"00003-cf794b64-accd-4c48-8ace-1818feebcc97","deepnote_to_be_reexecuted":false,"source_hash":"e62f056b","execution_millis":21,"execution_start":1611116782885,"deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"summary = pd.read_csv('/kaggle/input/us-historical-stock-prices-with-earnings-data/dataset_summary.csv', parse_dates=['stock_from_date','stock_to_date','earnings_from_date','earnings_to_date'])","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.01689,"end_time":"2021-01-05T21:29:43.461131","exception":false,"start_time":"2021-01-05T21:29:43.444241","status":"completed"},"tags":[],"cell_id":"00004-bfb0a994-19cf-472c-abc7-570f1e127d2d","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"## Exploratory Data Analysis\nWe use exploratory data analysis (EDA) to identify trends, anomalies, patterns, and possible relationships within the given data, using plots. It will help us determine if there are columns that don't have enough records and identify possible features we could put into our models or use as keys when we try to combine datasets later on. \n\n* We start off with **univariate analysis**, aiming to understand the distribution of underlying variable. \nThrough this process, we can identify which one of the distributions the variable most closely represents, \nwhich helps with the model selection process later on. \n* Univariate analysis also helps us discover possible outliers that could have a large impact on the \noverall distribution. \n* Since we have multiple datetime obejects, we also have to consider **univariate time series analysis**. \nIn this analysis, we will look at the change of a variable across a time period and their volitility \nand underlying distribution across different granularity of time. \n\n* Then, we can proceed to **bivariate analysis**, where we plan to explore the potentail relationship between two variables. Similiarly, we will look at bivariate analysis across a time period. "},{"metadata":{"papermill":{"duration":0.017123,"end_time":"2021-01-05T21:29:43.496255","exception":false,"start_time":"2021-01-05T21:29:43.479132","status":"completed"},"tags":[],"cell_id":"00005-774ad75b-643f-44a1-b7bd-f32968186c07","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"## Summary table\n\nThese are the following fields in the `summary` dataset.\n\n* `symbol`: This is the ticker of the stock.\n\n* `total_prices`: This is the number of records this particular ticker has in the stock prices dataset. \n\n* `stock_from_date`: The earliest date a price is available for this ticker in the price dataset. \n\n* `stock_to_date`: The latest date a price is available for this ticker in the price dataset.\n\n* `total_earnings`: The number of records corresponding to this particular ticker in the earnings dataset.\n\n* `earnings_from_date`: The earliest date earnings are available for this ticker in the earnings data.\n\n* `earnings_from_date`: The latest date earnings are available for this ticker in the earnings data."},{"metadata":{"papermill":{"duration":0.016998,"end_time":"2021-01-05T21:29:43.530622","exception":false,"start_time":"2021-01-05T21:29:43.513624","status":"completed"},"tags":[],"cell_id":"00006-8c893dd7-6919-4470-90ba-e35684d5de97","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"### Examine the number of stocks in this dataset with unique `symbol`\n\nLet's first examine how many unique stocks are in this dataset."},{"metadata":{"execution":{"iopub.execute_input":"2021-01-05T21:29:43.573115Z","iopub.status.busy":"2021-01-05T21:29:43.57214Z","iopub.status.idle":"2021-01-05T21:29:43.577915Z","shell.execute_reply":"2021-01-05T21:29:43.577256Z"},"papermill":{"duration":0.029959,"end_time":"2021-01-05T21:29:43.578034","exception":false,"start_time":"2021-01-05T21:29:43.548075","status":"completed"},"tags":[],"cell_id":"00007-ac00b6f7-969c-4b28-821e-65263791948b","deepnote_to_be_reexecuted":false,"source_hash":"dcea28c1","execution_millis":2,"execution_start":1611116782912,"deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"unique_symbol = len(summary['symbol'].unique())\nprint(f'The unique number of stocks in the summary dataset is {unique_symbol}')","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-05T21:29:43.623442Z","iopub.status.busy":"2021-01-05T21:29:43.622703Z","iopub.status.idle":"2021-01-05T21:29:43.635559Z","shell.execute_reply":"2021-01-05T21:29:43.634895Z"},"papermill":{"duration":0.038985,"end_time":"2021-01-05T21:29:43.635726","exception":false,"start_time":"2021-01-05T21:29:43.596741","status":"completed"},"tags":[],"cell_id":"00008-28bbef04-3d6f-41cc-9fa5-7e06813e23c5","deepnote_to_be_reexecuted":false,"source_hash":"970e4860","execution_millis":7,"execution_start":1611116782918,"deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"summary.info()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.017937,"end_time":"2021-01-05T21:29:43.673088","exception":false,"start_time":"2021-01-05T21:29:43.655151","status":"completed"},"tags":[],"cell_id":"00009-ce60701e-d7b7-4120-b086-e9d3abd823a4","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"We have the correct data type for each column. \n\nSince we have 7786 entries and there are only 5150 non-null entries in the `earnings_from_date`\nand `earnings_to_date`, these two columns have missing data. \nIt's important to deal with them before our analysis because they will raise errors \nduring the process of combining `summary` dataset with `earnings` dataset later on.\n\nSince `total_prices` and `total_earnings` from the summary table don't have any missing values. \nWe can start our analysis. "},{"metadata":{"papermill":{"duration":0.019981,"end_time":"2021-01-05T21:29:44.337858","exception":false,"start_time":"2021-01-05T21:29:44.317877","status":"completed"},"tags":[],"cell_id":"00016-c146c7ee-e5fa-43fd-a13f-ae099f126221","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"### Examine the distribution of `stock_from_date` across different years:\n\nWe want to find if there is a pattern in the distribution of `stock_from_date` across different years \nand check if there is some abnomalities for this distribution. "},{"metadata":{"execution":{"iopub.execute_input":"2021-01-05T21:29:44.385173Z","iopub.status.busy":"2021-01-05T21:29:44.384379Z","iopub.status.idle":"2021-01-05T21:29:44.916945Z","shell.execute_reply":"2021-01-05T21:29:44.916277Z"},"papermill":{"duration":0.559042,"end_time":"2021-01-05T21:29:44.917062","exception":false,"start_time":"2021-01-05T21:29:44.35802","status":"completed"},"tags":[],"cell_id":"00017-39d9cad8-ad43-4294-9b09-ce46db15c158","deepnote_to_be_reexecuted":false,"source_hash":"fbc52ae3","execution_millis":581,"execution_start":1611116810394,"deepnote_cell_type":"code","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"summary['stock_from_year'] = summary.apply(lambda row: row['stock_from_date'].year, axis=1)\ndf = summary['stock_from_year'].value_counts().rename_axis('year').reset_index(name='counts')\ndf.set_index('year').sort_values(by=['year']).plot(kind='bar', legend=None)\nplt.xlabel('Year')\nplt.ylabel('frequency')\nplt.title('Histogram of stock_from_date column')\nplt.xticks(rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.021083,"end_time":"2021-01-05T21:29:44.959426","exception":false,"start_time":"2021-01-05T21:29:44.938343","status":"completed"},"tags":[],"cell_id":"00018-c47769c9-0193-4d83-ab8f-1c2f330aecf7","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"Based on the plot: \n* A major outlier occurs in 1998. \nThis could be explained by the limitation of the dataset in which it contains stocks around \nthe 20-year period. \n`stock_from_date` column of stocks that were listed before 1998 is censored. \n* The trend of the number of companies being listed on the market across different years. \nIt has a general upward trend until declining in 2019 and 2020. \n* The global financial crisis was the major factor that contribute to the drop-off in the number of IPOs \nin 2008 and 2009. \n* The global pandanmic was the major reason to the drop-off in the number of IPOs in 2019 and 2020.\n"},{"metadata":{"papermill":{"duration":0.038279,"end_time":"2021-01-05T21:29:45.019151","exception":false,"start_time":"2021-01-05T21:29:44.980872","status":"completed"},"tags":[],"cell_id":"00019-4479d8a3-197b-4b79-92f9-09a9288319c6","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"#### Investigate further into the distribution of `stock_from_date` across different quarter:\nWe want to see if seasonality plays a part in the number of IPOs across time. We selected 'quarter' \nas the granularity of time. "},{"metadata":{"execution":{"iopub.execute_input":"2021-01-05T21:29:45.107614Z","iopub.status.busy":"2021-01-05T21:29:45.100755Z","iopub.status.idle":"2021-01-05T21:29:45.457315Z","shell.execute_reply":"2021-01-05T21:29:45.457814Z"},"papermill":{"duration":0.402493,"end_time":"2021-01-05T21:29:45.457972","exception":false,"start_time":"2021-01-05T21:29:45.055479","status":"completed"},"tags":[],"cell_id":"00020-eb23993a-d0cc-44db-b205-ac609f7babd9","deepnote_to_be_reexecuted":false,"source_hash":"e5ab6005","execution_millis":387,"execution_start":1611116783677,"deepnote_cell_type":"code","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"summary['stock_from_quarter'] = summary.apply(lambda row: row['stock_from_date'].quarter, axis=1)\ndf = summary['stock_from_quarter'].value_counts().rename_axis('quarter').reset_index(name='counts')\ndf.set_index('quarter').sort_values(by=['quarter']).plot(kind='bar', legend=None)\nplt.xlabel('quarter')\nplt.ylabel('frequency')\nplt.title('Histogram of stock_from_date column group by quarter')\nplt.xticks(rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.022189,"end_time":"2021-01-05T21:29:45.50244","exception":false,"start_time":"2021-01-05T21:29:45.480251","status":"completed"},"tags":[],"cell_id":"00021-4badcd46-5bec-4dcb-a76a-a860597f5fc2","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"According to the bar chart above, the first quarter seems to have twice as many total_prices as other quarters, which is irregular in the datasets. Our explanation for this to happen is that `stock_from_year` equals to 1998 has a major impact on the distribution of `total_prices` across different quarters.\n\nWe will try to validate this by filtering out the `total_prices` that are in 1998 and see how the bar chart looks without them:"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-05T21:29:45.556357Z","iopub.status.busy":"2021-01-05T21:29:45.555459Z","iopub.status.idle":"2021-01-05T21:29:45.733858Z","shell.execute_reply":"2021-01-05T21:29:45.733317Z"},"papermill":{"duration":0.206907,"end_time":"2021-01-05T21:29:45.73398","exception":false,"start_time":"2021-01-05T21:29:45.527073","status":"completed"},"tags":[],"cell_id":"00022-bb012265-60e1-4d21-a1f9-5c7ed1774a6b","deepnote_to_be_reexecuted":false,"source_hash":"c5cf37ca","execution_millis":139,"execution_start":1611116784084,"deepnote_cell_type":"code","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"summary_ = summary[summary['stock_from_year'] != 1998]\ndf = summary_['stock_from_quarter'].value_counts().rename_axis('quarter').reset_index(name='counts')\ndf.set_index('quarter').sort_values(by=['quarter']).plot(kind='bar', legend=None)\nplt.xlabel('quarter')\nplt.ylabel('frequency')\nplt.title('Histogram of stock_from_date column group by quarter')\nplt.xticks(rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.022938,"end_time":"2021-01-05T21:29:45.780333","exception":false,"start_time":"2021-01-05T21:29:45.757395","status":"completed"},"tags":[],"cell_id":"00023-86c6e91b-142c-4fab-9b1c-a992cc40e6e6","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"Without the interferencing from `total_prices` in 1998, we can see `total_prices` are evenly distributed in different quarters. If we want to use `total_prices` from 1998, we might have to sample some of them to make sure that `total_prices` from 1998 don't weight too much in our analysis."},{"metadata":{"papermill":{"duration":0.022849,"end_time":"2021-01-05T21:29:45.826494","exception":false,"start_time":"2021-01-05T21:29:45.803645","status":"completed"},"tags":[],"cell_id":"00024-515685bb-0d33-45ed-ad55-afc1813fddeb","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"### Examine the distribution of `earnings_from_date` across year:\nWe want to see if there is any irregularities in the distribution of `earnings_from_date` across\ndifferent years."},{"metadata":{"tags":[],"cell_id":"00025-d98781e3-605f-491b-9d85-c32a08937d13","deepnote_to_be_reexecuted":false,"source_hash":"35428c4","execution_millis":6,"execution_start":1611116784227,"deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"summary['earnings_from_date'].min()","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00026-99b5f299-c736-407b-ad42-63946b9015ae","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"The limitation of the dataset from the `summary` dataset is that it only contains earnings \nfor stocks since 2009. That is the earliest time available for analysis was in 2009 compared \nto 1998 (the earliest time for `stock_from_year` in the previous analysis)."},{"metadata":{"tags":[],"cell_id":"00025-6ff9c779-65ec-49b4-af1a-8b983841dd21","deepnote_to_be_reexecuted":false,"source_hash":"46792cd2","execution_millis":6,"execution_start":1611116784235,"deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"# Create a new dataframe, containing the actual IPOs in a year since 2009\n# Sources: https://stockanalysis.com/ipos/statistics/\nyear_range = np.arange(2009,2021,1)\nipos = [79,190,171,157,251,304,206,133,217,255,233,480]\nipo = pd.DataFrame({'year' : year_range, 'ipos':ipos})","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00025-122364ad-8c6e-473b-bae0-8d3874b2a9e3","deepnote_to_be_reexecuted":false,"source_hash":"87ac5b24","execution_millis":194,"execution_start":1611116784245,"deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"summary['earnings_from_year'] = summary.apply(lambda row: row['earnings_from_date'].year, axis=1)\ndf = summary['earnings_from_year'].value_counts().rename_axis('year').reset_index(name='counts')\ndf['year'] = df['year'].astype(int)\ndf = df.set_index('year').sort_values(by=['year'])","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00027-4946cffb-bc41-4e7a-837c-4a23cb5e3332","deepnote_to_be_reexecuted":false,"source_hash":"954e913","execution_millis":316,"execution_start":1611116784447,"deepnote_cell_type":"code","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"comp = pd.merge(ipo, df, on=\"year\")\ncomp['diff'] = comp['ipos'] - comp['counts']\ncomp = comp[['year','diff']]\ncomp = comp.set_index('year')\ncomp = comp.iloc[1:,:]\ncomp.plot(kind='bar', legend=None)\nplt.xticks(rotation=45)\nplt.ylabel('number of IPOs reported')\nplt.title('Difference in Number of IPOs')\nplt.axhline(y=0, linewidth=0.5, color='black')","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.024261,"end_time":"2021-01-05T21:29:46.392639","exception":false,"start_time":"2021-01-05T21:29:46.368378","status":"completed"},"tags":[],"cell_id":"00026-3f2a355d-c081-4ae9-9726-fc75001645a4","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"In order to check the completeness of the summary dataset, we crossed check the\ndata provided with another online source: https://stockanalysis.com/ipos/statistics/.\n\nThe bar chart above shows the different in the number of IPOs the summary data provides and \nthe number of actual IPOs the alternative source provides.\n\nWe filtered out data from 2009 because the data before 2009 is censored. \nWe notice that the data from the `summary` dataset has some inconsistencies with the actual number\nof IPOs throughout the years. \n\nWe have to note that the `summary` dataset is not the complete representation of stocks throughout\nthe years. It's an open question to anyone who are interested in this dataset to investigate this further."},{"metadata":{"papermill":{"duration":0.023787,"end_time":"2021-01-05T21:29:46.440841","exception":false,"start_time":"2021-01-05T21:29:46.417054","status":"completed"},"tags":[],"cell_id":"00027-5a1c156c-3983-4521-8902-11c176c301df","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"#### Investigate further into the distribution of `earnings_from_date` across weekdays:\nWe want to see if there is a general trend of `earnings_from_date` across different time granularity, such as \nweekdays. It's more likely for companies to continue announcing their earnings on a specific \nweekday after making the first announcement on that weekday."},{"metadata":{"execution":{"iopub.execute_input":"2021-01-05T21:29:46.497346Z","iopub.status.busy":"2021-01-05T21:29:46.496686Z","iopub.status.idle":"2021-01-05T21:29:46.905354Z","shell.execute_reply":"2021-01-05T21:29:46.904715Z"},"papermill":{"duration":0.440449,"end_time":"2021-01-05T21:29:46.905468","exception":false,"start_time":"2021-01-05T21:29:46.465019","status":"completed"},"tags":[],"cell_id":"00028-11a95987-bff3-4efd-b6a3-e8ebd392244c","deepnote_to_be_reexecuted":false,"source_hash":"d1c1136b","execution_millis":543,"execution_start":1611116784766,"deepnote_cell_type":"code","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"summary['earnings_from_weekday'] = summary.apply(lambda row: row['earnings_from_date'].weekday(), axis=1)\ndf = summary['earnings_from_weekday'].value_counts().rename_axis('weekday').reset_index(name='counts')\ndf = df.set_index('weekday').sort_values(by=['weekday'])\ndf.rename(index={0.0:'Monday',1.0:'Tuesday',2.0:'Wednesday',3.0:'Thursday',4.0:'Friday',5.0:'Saturday',6.0:'Sunday'},inplace=True)\ndf.plot(kind='bar', legend=None)\nplt.xticks(rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.025482,"end_time":"2021-01-05T21:29:46.956647","exception":false,"start_time":"2021-01-05T21:29:46.931165","status":"completed"},"tags":[],"cell_id":"00029-6f99ec2e-7a9f-408c-b47f-bfde9b7f295b","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"Based on the distribution of `earnings_from_date` across weekdays, \nwe can see that most of the companies in our datasets report their **first** earnings on a Thursday.\nStarting with Monday, more and more companies report their earnings throughout the week. \nWe have also notice that fewer companies reported their earnings on a Friday in the time \nframe of this dataset. \nAnd almost no companies reported their earnings on weekends except few companies on Saturday, \nwhich we may have to look into why they reported their earnings on a Saturday.\n\nSince companies from 2009 may have a huge weight in this plot, \nwe are going to eliminate those companies and see how the distribution look like:"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-05T21:29:47.02397Z","iopub.status.busy":"2021-01-05T21:29:47.022624Z","iopub.status.idle":"2021-01-05T21:29:47.363611Z","shell.execute_reply":"2021-01-05T21:29:47.362936Z"},"papermill":{"duration":0.381309,"end_time":"2021-01-05T21:29:47.363726","exception":false,"start_time":"2021-01-05T21:29:46.982417","status":"completed"},"tags":[],"cell_id":"00030-46a120bc-eb5b-47ad-85e5-3aa7a73b37a6","deepnote_to_be_reexecuted":false,"source_hash":"1cc360d7","execution_millis":483,"execution_start":1611116785316,"deepnote_cell_type":"code","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"summary['earnings_from_year'] = summary.apply(lambda row: row['earnings_from_date'].year, axis=1)\nsummary_ = summary[summary['earnings_from_year'] != 2009]\ndf = summary_['earnings_from_weekday'].value_counts().rename_axis('weekday').reset_index(name='counts')\ndf = df.set_index('weekday').sort_values(by=['weekday'])\ndf.rename(index={0.0:'Monday',1.0:'Tuesday',2.0:'Wednesday',3.0:'Thursday',4.0:'Friday',5.0:'Saturday',6.0:'Sunday'},inplace=True)\ndf.plot(kind='bar', legend=None)\nplt.xticks(rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.026134,"end_time":"2021-01-05T21:29:47.418985","exception":false,"start_time":"2021-01-05T21:29:47.392851","status":"completed"},"tags":[],"cell_id":"00031-a1979848-bb89-440b-a369-ac6d078d2f3c","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"We don't see much change in the distribution of `total_earnings` across weekdays change\nmuch after we filter out companies with `total_earnings` from 2009.\n\nSince `earnings_from_date` column only provides the first earnings release date for\na specific company, we have to use `earnings` dataset to see if there is a pattern in the \nweekday when companies release their earnings."},{"metadata":{"tags":[],"cell_id":"00037-b29c1319-77ae-4f5b-b0b6-47b0777bdd81","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"### Examine the distribution of `total_prices` column\nWe want to understand the distribution of `total_prices` to see if there are any irregularities\nwe have to address."},{"metadata":{"tags":[],"cell_id":"00037-afb2a565-ad3c-423b-8e01-b39adb123d6d","deepnote_to_be_reexecuted":false,"source_hash":"67a06919","execution_millis":187,"execution_start":1611116785804,"deepnote_cell_type":"code","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"summary['total_prices'].hist()\nplt.xlabel('total_prices')\nplt.ylabel('frequency')\nplt.title('Histogram of total_prices column')","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00039-84be4b83-d2a2-49eb-abce-41f93fb74656","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"There are a number of takeaways from this chart:\n\n* `total_prices` is bimodal, with concentrations at 0 and 5500.\n* The mode at 0 is due to the fact these stocks do not have price data.\n* The mode at 5500 is due to the fact that our dataset begins in 1998, \nso securities that have price data before that are censored.\n\nWe identified the `total_price` column to be right-censored.\nThe censoring of the data is a potential issue because the observed value of the \n`total_prices` is partially known. In this case, we don't know the price of stocks before 1998. "},{"metadata":{"tags":[],"cell_id":"00040-3f4f8d46-2f08-400e-8846-8905dc7fe1fc","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"### Examine the distribution of `total_earnings` column\nWe want to understand the distribution of `total_earnings` to see if there are any irregularities\nwe have to address."},{"metadata":{"tags":[],"cell_id":"00041-b1fb37d7-0b10-4a5a-88ff-da68519b0a10","deepnote_to_be_reexecuted":false,"source_hash":"a9f91a3c","execution_millis":186,"execution_start":1611116786007,"deepnote_cell_type":"code","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"summary['total_earnings'].hist()\nplt.xlabel('total_earnings')\nplt.ylabel('frequency')\nplt.title('Histogram of total_earnings column')","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00042-986f0e67-0ea2-4d42-a732-751a512385a2","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"There are a number of takeaways from this chart:\n* `total_earnings` is bimodal with two concentrations at 0 and 45. \n* The peak at 45 represents the number of companies that we have all of their earnings data\nin the time slot of the dataset\n* The peak at 0 says that there are a lot of companies that have very limited number of\nearnings available for our analysis. \n\nWe consider a cut-off point where we can obtain companies with enough earnings for analysis. "},{"metadata":{"papermill":{"duration":0.026261,"end_time":"2021-01-05T21:29:47.471649","exception":false,"start_time":"2021-01-05T21:29:47.445388","status":"completed"},"tags":[],"cell_id":"00032-86634f74-a5fd-4838-92db-dc12a1e4d681","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"### Examine the joint distribution of `earnings_from_date` and `stock_from_date`\nSince we can't make joint distribution from datetime objects, we will transform these two columns into `earnings_years` and `stock_years`, which corresponding to the number of days since."},{"metadata":{"execution":{"iopub.execute_input":"2021-01-05T21:29:47.532823Z","iopub.status.busy":"2021-01-05T21:29:47.532123Z","iopub.status.idle":"2021-01-05T21:29:52.261363Z","shell.execute_reply":"2021-01-05T21:29:52.260881Z"},"papermill":{"duration":4.763451,"end_time":"2021-01-05T21:29:52.261476","exception":false,"start_time":"2021-01-05T21:29:47.498025","status":"completed"},"tags":[],"cell_id":"00033-3e7fac7c-9306-4ac1-9c9d-4ef45106c3f9","deepnote_to_be_reexecuted":false,"source_hash":"e9bc6cca","execution_millis":6611,"execution_start":1611116915894,"deepnote_cell_type":"code","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"summary['earnings_years'] = datetime.datetime.now() - summary['earnings_from_date']\nsummary['earnings_years'] = summary.apply(lambda row: row['earnings_years'].days / 365, axis=1)\nsummary['stock_years'] = datetime.datetime.now() - summary['stock_from_date']\nsummary['stock_years'] = summary.apply(lambda row: row['stock_years'].days / 365, axis=1)\nsns.jointplot(data=summary,x='stock_years',y='earnings_years', kind='kde')","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.027226,"end_time":"2021-01-05T21:29:52.316615","exception":false,"start_time":"2021-01-05T21:29:52.289389","status":"completed"},"tags":[],"cell_id":"00034-a1308f54-2111-48bf-8986-f6769b92c43e","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"One thing to note before going into the takeaways from this plot is that the contour plot goes\ninto the negative territory for a specific region. This is one of the limitations of the kde plot\nbecause of the concentration of data points in that region. \n\nThere are several takeaways from this joint plot:\n* It's been around 12 years since most of the earnings are started to be reported \n* It's been around 25 years since most of the prices are started to be reported in these datasets. \n* The discrepancy in the two bullet points above means that we don't necessarily have \nearnings and price data for a period of time. \n* For our analysis, we need both earnings and price data of a particular stock to measure the \nprice leading up to the earnings announcement and the reaction of the stock prices after the \nearnings announcement. "},{"metadata":{"papermill":{"duration":0.027765,"end_time":"2021-01-05T21:29:52.372054","exception":false,"start_time":"2021-01-05T21:29:52.344289","status":"completed"},"tags":[],"cell_id":"00035-b109c1cd-83fd-400e-bbf7-9cebd427dbbc","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"### Examine joint distribution of `total_prices` and `total_earnings`\nTo find out the distribution for earnings and prices at the same time, \nwe decided to use the joint distribution to plot earnings and prices on the same plot. \nWe decided to use hex plot just because there might be some misunderstanding for negative \nvalues in the previous contour plot.\n\nWe expect a positive relationship between `total_prices` and `total_earnings` for this 20-year period. \nThat is, a company with more `total_earnings` will have more `total_prices`. "},{"metadata":{"execution":{"iopub.execute_input":"2021-01-05T21:29:52.438739Z","iopub.status.busy":"2021-01-05T21:29:52.43801Z","iopub.status.idle":"2021-01-05T21:29:53.061997Z","shell.execute_reply":"2021-01-05T21:29:53.061296Z"},"papermill":{"duration":0.661649,"end_time":"2021-01-05T21:29:53.062116","exception":false,"start_time":"2021-01-05T21:29:52.400467","status":"completed"},"tags":[],"cell_id":"00036-800ba2a0-1d30-4d15-bdd9-29eda600d8e1","deepnote_to_be_reexecuted":false,"source_hash":"fcf42876","execution_millis":539,"execution_start":1611116792583,"deepnote_cell_type":"code","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"g = sns.jointplot(data=summary,x='total_prices',y='total_earnings',kind='hex')\nax = g.ax_joint\nax.add_patch(Rectangle((4500, 35), 1500, 15, fill=False, edgecolor='red', lw=1))\nax.add_patch(Rectangle((0, 0), 1500, 8, fill=False, edgecolor='red', lw=1))\nax.add_patch(Rectangle((200, 10), 1800, 25, fill=False, edgecolor='red', lw=1))\nax.text(1700, 2, \"Group 1\", horizontalalignment='left', size='medium', color='black', weight='semibold')\nax.text(2300, 13, \"Group 2\", horizontalalignment='left', size='medium', color='black', weight='semibold')\nax.text(4300, 38, \"Group 3\", horizontalalignment='right', size='medium', color='black', weight='semibold')","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.028689,"end_time":"2021-01-05T21:29:53.119765","exception":false,"start_time":"2021-01-05T21:29:53.091076","status":"completed"},"tags":[],"cell_id":"00037-396eec77-9772-43c8-b466-d229bb4a6bbd","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"As you can see in the histograms on the top and on the side of the joint distribution, \nthere are a large number of companies that have zero `total_prices` when the `total_earnings` \nis zero (Group 1). These records won't be helpful for our model.\n\nWhile we exclude the records where the `total_earnings` is not zero, `total_prices` caps at \naround 5500. This will useful because we can set a range where we only investigate stocks with \nmore than a certain threshold of `total_prices`. We can filter out record below that threshold \nbecause they lack necessary information.\n\nAccording to the joint distribution, `total_prices` and `total_earnings` are not normally \ndistributed, we can see distinct clusters at the bottom left (Group 1) and top right corners \n(Group 3). \n`total_earnings` has a bimodal distribution with two peaks at 0 and 45. \n`total_prices` has a bimodal distribution with two peaks at 0 and 5500. \n\nMost of the data fill into the Group 3 as we annotated on the plot, which we expect. \nThis is because most of the companies have reported their earnings consistently. \nHowever, the cluster in the left corner is the abnormality we discovered from this plot. \nThose hex boxes don't have enough earnings nor prices available for analysis. \n\nAn interesting trend can be seen from the plot where the `total_earnings` increase as \nthe `total_earnings` increase (Group 2). \nA plausible explanation for this is that those companies went public later than 1998 and \nare on their way to join the majority of companies that have sound records. \nAs time goes on, we expect the cluster at the right upper corner to move further and \nfurther away diagonally.\n\nIn order to get the necessary data with enough records for analysis, \nwe need to filter our data based on total_prices and total_earnings to get the companies that \nlocate at the right upper corner of our joint distribution because they have enough \n`total_earnings` and `total_prices` for further analysis.\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}