{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Prediction Of Heart Disease\n### By: Rahul Kulkarni"},{"metadata":{},"cell_type":"markdown","source":"### Scope and Problem Statement:\nIn this kernel we will try to predict the possibility of a person having heart diesease with the help of various factors such as age, gender,blood pressure etc. We will use various classification models for the purpose of prediction. We will got through various processes such as Data cleaning,EDA,Model fitting and Evaluation.\n\nThis prediction is very useful in the healthcare industry, as an accurate prediction can help the doctors in helping the patient beforehand."},{"metadata":{},"cell_type":"markdown","source":"### Description of the dataset:\n1. **age**: age of the patient\n2. **sex**: 1 = male and 0 = female\n3. **cp(chest pain type)**: 1= typical anginaValue, 2= atypical anginaValue, 3= non-anginal painValue, 4= asymptomatic\n4. **trestps(resting blood pressure)**:  resting blood pressure (in mm Hg on admission to the hospital)\n5. **chol(serum cholestoral in mg/dl)**\n1. **fbs(fasting blood sugar > 120 mg/dl)**: 1 = true, 0 = false\n1. **restecg(resting electrocardiographic results)**: 0= normal, 1= having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), 2= showing probable or definite left ventricular hypertrophy by Estes' criteria\n1. **thalach(maximum heart rate achieved)**\n1. **exang(exercise induced angina)**: 1 = yes, 0 = no\n1. **oldpeak**: ST depression induced by exercise relative to rest\n1. **slope(the slope of the peak exercise ST segment)**: 1= upsloping, 2= flat, 3= downsloping\n1. **ca(number of major vessels (0-3) colored by flourosopy)**\n1. **thal**: 3 = normal; 6 = fixed defect; 7 = reversable defect\n1. **target**: 0= < 50% diameter narrowing, 1= > 50% diameter narrowing"},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nheart_data = pd.read_csv('../input/heart-disease-uci/heart.csv')\nheart_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset is not very large.\n\nLet's make sure that the data types for the features are according to the description."},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_data.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check for missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no missing values.\n\nThere are certain column names that bother me so I will rename them."},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_data.rename(columns={'trestbps':'restbp','thalach':'maxhr','ca':'nmv'},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the statistical properties of the features."},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The features such as 'restbp','chol' and 'maxhr' have large ranges compared to other features. This might cause bias towards these features during modelling. We will handle this later.\n\nLet's have a closer at the features and how they affect the target variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='target',y='age',data=heart_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The box plot doesn't give us good information, so binning ages would be a better option."},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_data['age']  = pd.cut(heart_data['age'],bins=[29,39,49,59,69,79],labels=[1,2,3,4,5],include_lowest=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"age = heart_data.groupby(['age','target'])['target'].count().unstack()\nage['per'] = round((age[1]/(age[0]+age[1]))*100,2)\nage","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the data mid age patients have a higher chance of having heart problems."},{"metadata":{"trusted":true},"cell_type":"code","source":"sex = heart_data.groupby(['sex','target'])['target'].count().unstack()\nsex['per'] = round((sex[1]/(sex[0]+sex[1]))*100,2)\nsex","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Females have a higher chance of having heart problems.\n\nLet's do further analysis by grouping data by age and sex."},{"metadata":{"trusted":true},"cell_type":"code","source":"age_sex = heart_data.groupby(['age','sex','target'])['target'].count().unstack()\nage_sex['per'] = round((age_sex[1]/(age_sex[0]+age_sex[1]))*100,2)\nage_sex","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can club age and sex for further analysis, as this has given us interesting information."},{"metadata":{"trusted":true},"cell_type":"code","source":"cp = heart_data.groupby(['cp','target'])['target'].count().unstack()\ncp['per'] = round((cp[1]/(cp[0]+cp[1]))*100,2)\ncp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, if a person has chest pain, they have a higher chance of heart problems."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='target',y='restbp',data=heart_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both the target classes have the same median. The IQR of '0' label is larger than '1'. There are some outliers, such as restbp of 200. This blood pressure would mean abnormal heart behaviour. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='target',y='chol',data=heart_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both the labels have same ranges. As expected very high cholestrol would result in heart disease, this is visible from the outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"fbs = heart_data.groupby(['fbs','target'])['target'].count().unstack()\nfbs['per'] = round((fbs[1]/(fbs[0]+fbs[1]))*100,2)\nfbs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nothing conclusive can be obtained from fasting blood sugar."},{"metadata":{"trusted":true},"cell_type":"code","source":"restecg = heart_data.groupby(['restecg','target'])['target'].count().unstack()\nrestecg['per'] = round((restecg[1]/(restecg[0]+restecg[1]))*100,2)\nrestecg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ecg result of 1 has a higher chance of heart disease."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='target',y='maxhr',data=heart_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected higher heart rate results in higher chance of heart disease. "},{"metadata":{"trusted":true},"cell_type":"code","source":"ex = heart_data.groupby(['exang','target'])['target'].count().unstack()\nex['per'] = round((ex[1]/(ex[0]+ex[1]))*100,2)\nex","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is surprising to see that no angina results in higher chance of heart disease and angina results in less chance of heart disease."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='target',y='oldpeak',data=heart_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Very low depression results in higher chance of heart disease. A higher ST depression is seen in normal patients."},{"metadata":{"trusted":true},"cell_type":"code","source":"sl = heart_data.groupby(['slope','target'])['target'].count().unstack()\nsl['per'] = round((sl[1]/(sl[0]+sl[1]))*100,2)\nsl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A flat slope results in less chance of heart disease and down slope has a higher chance of disease."},{"metadata":{"trusted":true},"cell_type":"code","source":"nmv = heart_data.groupby(['nmv','target'])['target'].count().unstack()\nnmv['per'] = round((nmv[1]/(nmv[0]+nmv[1]))*100,2)\nnmv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"very low and very high vessels results in heart disease. But mid range vessels have a lower chance of disease."},{"metadata":{"trusted":true},"cell_type":"code","source":"thal = heart_data.groupby(['thal','target'])['target'].count().unstack()\nthal['per'] = round((thal[1]/(thal[0]+thal[1]))*100,2)\nthal","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected irreversible defects have a higher chance of disease and reversible defects have a lower chance.\n\nLet's create a new variable using age and sex."},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_data['age']=heart_data['age'].astype('int')\nheart_data['age_sex'] = heart_data['age']*heart_data['sex']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the correlation between features."},{"metadata":{"trusted":true},"cell_type":"code","source":"c = heart_data.corr()\nplt.figure(figsize=(20,6))\nsns.heatmap(c,annot=True,fmt='f')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe 'cp' and 'maxhr' have a decent positive correlation with our target variables. Features such as 'exang','oldpeak','nmv','thal' and 'age_sex' have a negative correlation."},{"metadata":{},"cell_type":"markdown","source":"# Model Fitting"},{"metadata":{},"cell_type":"markdown","source":"Here we will select the features we will use for prediction. We will select features which have a decent correlation with our target variable. Also we will make use of MinMaxScaler to reduce the range of 'restbp' and 'maxhr' to avoid bias towards them. We make use of training set to find the optimum parameters for our models and make use of KFold to check the performance of our model. The metric which we will use for evaluation will be F1 score. Since we need to reduce the false negatives, F1 score is a better metric than accuracy score. The reason being F1 score includes both precision and recall."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split as tts\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import f1_score as fs\nfrom statistics import mean\nfrom sklearn.preprocessing import MinMaxScaler as MMS\nheart_data['mms_restbp'] = MMS().fit_transform(heart_data[['restbp']])\nheart_data['mms_maxhr'] = MMS().fit_transform(heart_data[['maxhr']])\nx = heart_data[['age_sex','cp','mms_restbp','restecg','mms_maxhr','exang','oldpeak','slope','nmv','thal']]\ny = heart_data['target']\nxtrain,xtest,ytrain,ytest = tts(x,y,test_size=0.2,random_state=100)\nkf = KFold(n_splits=5,shuffle=True,random_state=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K Nearest Neighbors"},{"metadata":{},"cell_type":"markdown","source":"We need to find the optimum number of neighbors for our model. Hence we will compare the F1 scores obtained for various values for our neighbors."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier as KNC\nneighbors,scores_train,scores_test = [i for i in range(1,20)],[],[]\nfor n in neighbors:\n    score_train,score_test = [],[]\n    for train,test in kf.split(xtrain):\n        xtr,xtt = xtrain.iloc[train],xtrain.iloc[test]\n        ytr,ytt = ytrain.iloc[train],ytrain.iloc[test]\n        knc = KNC(n_neighbors=n,weights='distance') #Giving weight to the distance of neighbors\n        knc.fit(xtr,ytr)\n        yhat_train = knc.predict(xtr)\n        yhat_test = knc.predict(xtt)\n        score_train.append(round(fs(ytr,yhat_train),2))\n        score_test.append(round(fs(ytt,yhat_test),2))\n    scores_train.append(mean(score_train))\n    scores_test.append(mean(score_test))\nsns.lineplot(x=neighbors,y=scores_train,color='r')\nsns.lineplot(x=neighbors,y=scores_test,color='b')\nplt.legend(('Train','Test'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The value of optimum neighbors is 8."},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree"},{"metadata":{},"cell_type":"markdown","source":"We need to find the optimum depth for our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier as DTC\ndepths,scores_train,scores_test = [i for i in range(3,20)],[],[]\nfor d in depths:\n    score_train,score_test = [],[]\n    for train,test in kf.split(xtrain):\n        xtr,xtt = xtrain.iloc[train],xtrain.iloc[test]\n        ytr,ytt = ytrain.iloc[train],ytrain.iloc[test]\n        dtc = DTC(max_depth=d)\n        dtc.fit(xtr,ytr)\n        yhat_train = dtc.predict(xtr)\n        yhat_test = dtc.predict(xtt)\n        score_train.append(round(fs(ytr,yhat_train),2))\n        score_test.append(round(fs(ytt,yhat_test),2))\n    scores_train.append(mean(score_train))\n    scores_test.append(mean(score_test))\nsns.lineplot(x=depths,y=scores_train,color='r')\nsns.lineplot(x=depths,y=scores_test,color='b')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Depth value of 3 has the highest F1 score but I am certain that this value would underfit the data. Therefore I believe choosing a value of 6 would be better."},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"We need to find the optimum value of regularization factor for our model. Increasing the regularization strength penalizes \"large\" weight coefficients. Our goal is to prevent that our model picks up \"peculiarities,\" \"noise,\" or \"imagines a pattern where there is none.\""},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression as LR\nC,scores_train,scores_test = [0.001,0.005,0.01,0.05,0.1,0.5],[],[]\nfor c in C:\n    score_train,score_test = [],[]\n    for train,test in kf.split(xtrain):\n        xtr,xtt = xtrain.iloc[train],xtrain.iloc[test]\n        ytr,ytt = ytrain.iloc[train],ytrain.iloc[test]\n        lr = LR(C=c,max_iter=1000)  # C is Inverse of regularization strength\n        lr.fit(xtr,ytr)\n        yhat_train = lr.predict(xtr)\n        yhat_test = lr.predict(xtt)\n        score_train.append(round(fs(ytr,yhat_train),2))\n        score_test.append(round(fs(ytt,yhat_test),2))\n    scores_train.append(mean(score_train))\n    scores_test.append(mean(score_test))\nsns.lineplot(x=C,y=scores_train,color='r')\nsns.lineplot(x=C,y=scores_test,color='b')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The optimum regularization factor was found to be 0.05."},{"metadata":{},"cell_type":"markdown","source":"### Gaussian Naive Bayes"},{"metadata":{},"cell_type":"markdown","source":"This a model where it assumes data to be normally distributed and applies bayes theorem assumptions. There are no parameters to be adjusted in this model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB as GNB\nscore_train,score_test = [],[]\nfor train,test in kf.split(xtrain):\n    xtr,xtt = xtrain.iloc[train],xtrain.iloc[test]\n    ytr,ytt = ytrain.iloc[train],ytrain.iloc[test]\n    gnb = GNB()\n    gnb.fit(xtr,ytr)\n    yhat_train = gnb.predict(xtr)\n    yhat_test = gnb.predict(xtt)\n    score_train.append(round(fs(ytr,yhat_train),2))\n    score_test.append(round(fs(ytt,yhat_test),2))\nprint(mean(score_train),mean(score_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Support Vector Machine"},{"metadata":{},"cell_type":"markdown","source":"Similar to the logistic regression model, we will try to find the optimum value for the regularization factor."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nC,scores_train,scores_test = [0.001,0.005,0.01,0.05,0.1,0.5],[],[]\nfor c in C:\n    score_train,score_test = [],[]\n    for train,test in kf.split(xtrain):\n        xtr,xtt = xtrain.iloc[train],xtrain.iloc[test]\n        ytr,ytt = ytrain.iloc[train],ytrain.iloc[test]\n        svc = SVC(C=c,kernel='linear')\n        svc.fit(xtr,ytr)\n        yhat_train = svc.predict(xtr)\n        yhat_test = svc.predict(xtt)\n        score_train.append(round(fs(ytr,yhat_train),2))\n        score_test.append(round(fs(ytt,yhat_test),2))\n    scores_train.append(mean(score_train))\n    scores_test.append(mean(score_test))\nsns.lineplot(x=C,y=scores_train,color='r')\nsns.lineplot(x=C,y=scores_test,color='b')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The optimum value is 0.1"},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Classifier"},{"metadata":{},"cell_type":"markdown","source":"This model creates various trees and finds the average of all the trees. We will need to find the optimum number of trees to be considered for this model. We will use the maximum depth which we found from the DTC model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier as RFC\nestimators,scores_train,scores_test = [100,150,200,250,300,350,400,450,500],[],[]\nfor e in estimators:\n    score_train,score_test = [],[]\n    for train,test in kf.split(xtrain):\n        xtr,xtt = xtrain.iloc[train],xtrain.iloc[test]\n        ytr,ytt = ytrain.iloc[train],ytrain.iloc[test]\n        rfc = RFC(n_estimators=e,max_depth=6)\n        rfc.fit(xtr,ytr)\n        yhat_train = rfc.predict(xtr)\n        yhat_test = rfc.predict(xtt)\n        score_train.append(round(fs(ytr,yhat_train),2))\n        score_test.append(round(fs(ytt,yhat_test),2))\n    scores_train.append(mean(score_train))\n    scores_test.append(mean(score_test))\nsns.lineplot(x=estimators,y=scores_train,color='r')\nsns.lineplot(x=estimators,y=scores_test,color='b')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The optimum number of trees was found to be 250."},{"metadata":{},"cell_type":"markdown","source":"# Evaluation"},{"metadata":{},"cell_type":"markdown","source":"Now we will test our models on the test data and see the performance with the help of confusion matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score as acs\nfrom sklearn.metrics import confusion_matrix\nmodels,f1score,acscore = ['KNN','DTC','LR','GNB','SVM','RFC'],[],[]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K Nearest Neighbors"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNC(n_neighbors=8,weights='distance')\nknn.fit(xtrain,ytrain)\nyhat_eval = knn.predict(xtest)\nf1 = round(fs(ytest,yhat_eval),2)\nac = round(acs(ytest,yhat_eval),2)\nf1score.append(f1)\nacscore.append(ac)\nprint(f1,ac)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfm = confusion_matrix(ytest,yhat_eval)\nsns.heatmap(cfm,annot=True)\nplt.title('Confusion Matrix KNN')\nplt.xlabel('Predicted Value')\nplt.ylabel('True Label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc = DTC(max_depth=6)\ndtc.fit(xtrain,ytrain)\nyhat_eval = dtc.predict(xtest)\nf1 = round(fs(ytest,yhat_eval),2)\nac = round(acs(ytest,yhat_eval),2)\nf1score.append(f1)\nacscore.append(ac)\nprint(f1,ac)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfm = confusion_matrix(ytest,yhat_eval)\nsns.heatmap(cfm,annot=True)\nplt.title('Confusion Matrix DTC')\nplt.xlabel('Predicted Value')\nplt.ylabel('True Label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LR(C=0.05)\nlr.fit(xtrain,ytrain)\nyhat_eval = lr.predict(xtest)\nf1 = round(fs(ytest,yhat_eval),2)\nac = round(acs(ytest,yhat_eval),2)\nf1score.append(f1)\nacscore.append(ac)\nprint(f1,ac)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfm = confusion_matrix(ytest,yhat_eval)\nsns.heatmap(cfm,annot=True)\nplt.title('Confusion Matrix LR')\nplt.xlabel('Predicted Value')\nplt.ylabel('True Label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gaussian Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb = GNB()\ngnb.fit(xtrain,ytrain)\nyhat_eval = gnb.predict(xtest)\nf1 = round(fs(ytest,yhat_eval),2)\nac = round(acs(ytest,yhat_eval),2)\nf1score.append(f1)\nacscore.append(ac)\nprint(f1,ac)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfm = confusion_matrix(ytest,yhat_eval)\nsns.heatmap(cfm,annot=True)\nplt.title('Confusion Matrix GNB')\nplt.xlabel('Predicted Value')\nplt.ylabel('True Label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Support Vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(kernel='linear',C=0.1)\nsvc.fit(xtrain,ytrain)\nyhat_eval = svc.predict(xtest)\nf1 = round(fs(ytest,yhat_eval),2)\nac = round(acs(ytest,yhat_eval),2)\nf1score.append(f1)\nacscore.append(ac)\nprint(f1,ac)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfm = confusion_matrix(ytest,yhat_eval)\nsns.heatmap(cfm,annot=True)\nplt.title('Confusion Matrix SVC')\nplt.xlabel('Predicted Value')\nplt.ylabel('True Label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RFC(n_estimators=250,max_depth=6)\nrfc.fit(xtrain,ytrain)\nyhat_eval = rfc.predict(xtest)\nf1 = round(fs(ytest,yhat_eval),2)\nac = round(acs(ytest,yhat_eval),2)\nf1score.append(f1)\nacscore.append(ac)\nprint(f1,ac)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfm = confusion_matrix(ytest,yhat_eval)\nsns.heatmap(cfm,annot=True)\nplt.title('Confusion Matrix RFC')\nplt.xlabel('Predicted Value')\nplt.ylabel('True Label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will analyse the metrics for various models and select the best one."},{"metadata":{"trusted":true},"cell_type":"code","source":"Results = pd.DataFrame({'F1 Score':f1score,'Accuracy Score':acscore},index=models)\nResults.sort_values(by=['F1 Score','Accuracy Score'],ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Therefore from the above table it is clear that Logistic Regression was the best model as it generated the highest F1 and Accuracy score."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}