{"cells":[{"metadata":{"_uuid":"c85be89c7642998aa3eaa1417fbfd9cf96a14453"},"cell_type":"markdown","source":"# SMS Spam Detection with Feature Selection"},{"metadata":{"_uuid":"55e0f788a2517ece2b3aae0c4eb48c4e290179ed"},"cell_type":"markdown","source":"In this jupyter notebook I tried to apply the idea of feature selection on a dataset where the features are words instead of numeric/categorical values. I have chosen the spam SMS dataset for its compact size. First I will implement a K-Nearest Neighbor classifier using sklearn without any feature selection. Then, I will perform the feature selection and will see if there is any improvement in the prediction result."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"330df1e9c33e0e47532865e70d42f2b8a10b644d"},"cell_type":"markdown","source":"## Import and overview of the dataset"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/spam.csv', encoding='latin-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d472822a7464e9802478fc1d4e062864410837bb"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"371f5826f03ec7a91fb9f6f02073215b9b66299e"},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7c3e1869c96c6b2f849e950609d46e2e743022f"},"cell_type":"markdown","source":"## Some data clean up"},{"metadata":{"trusted":true,"_uuid":"34ac1a7f2d9ff971fba543ae44bd87763c7e3923"},"cell_type":"code","source":"# Remove garbage columns\ndf.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ac37a40af52ea382fa25c78b304b48d9d7964a1"},"cell_type":"code","source":"# Remove any empty rows\ndf.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0918a162db2b3c1f060219eb229bdc7cabaf01dd"},"cell_type":"code","source":"# Set column names to something meaningful\ndf.columns = ['type', 'sms']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3a1c0aff21734c3c5287ce9ffd2ccc99b4f349f"},"cell_type":"code","source":"# Convert target values to numeric\ndf.loc[df['type'] == 'ham', 'type'] = 0\ndf.loc[df['type'] == 'spam', 'type'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dae1e54a904f6fee3c2ff83657d3c466b599d153"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd41d1bd73a19ff70f9d7b5652f9909d813a6775"},"cell_type":"code","source":"# Remove any html tags from text (just if any) and lowercase the words\nfrom bs4 import BeautifulSoup\ndf['sms'] = df['sms'].apply(lambda x: BeautifulSoup(x.lower(), 'html.parser').get_text())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b466271d9de7e9bae45115794ddda19bbe2c6ec"},"cell_type":"code","source":"# Separating the features and target values\nX = df['sms']\ny = df['type'].astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8af82e75bd423e9a72bde581b1cf36357bcd8f07"},"cell_type":"code","source":"X.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c72a61fe7acf8dcfda7c93888ca7f6059343961a"},"cell_type":"markdown","source":"## Preparing the data for training"},{"metadata":{"trusted":true,"_uuid":"5ccd6e3d4546f1f71cbfd417286a726052c28947"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# We are using 70% of data for training, rest for the testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=111, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a371f0afac53eca44416243f1d2717342ae2b860"},"cell_type":"code","source":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"270d871bddd31fa34bd287ec1fed35a25f9fcec8"},"cell_type":"markdown","source":"## Vectorizing the features"},{"metadata":{"_uuid":"cf79e780b21672ef4c829278725c3e57e8e359a0"},"cell_type":"markdown","source":"The training module does not understand plain texts. It only understands list of features which has to be numeric in type. So, we have to convert the plain text into list of numeric features. This conversion method is known as vectorizing the features. We are going to use sklearn's TfidfVectorizer based on TF-IDF algorithm. For models like spam detection, TfidfVectorizer is quite standard to use. "},{"metadata":{"trusted":true,"_uuid":"cc1e382531f03d07c51b84358bda423673c201ab"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nX_train_vectorized = vectorizer.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c360b076b6ee426e5286f42c0ffc889f26c5acdb"},"cell_type":"code","source":"# Lets have a look at the vectorized features\nvectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6f62c1aef3878fa8d54b89adb222b8f2acab4c9"},"cell_type":"code","source":"# So, we have total 8669 features (or words if its easier to think)\nX_train_vectorized.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"875e0e2bc62e7dd1d447c6923e0c1953d44a6f14"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\n# Create a K-Nearest Neighbors classifier model with 3 neighbors\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train_vectorized, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e031a4995cff332fc689080e71613e292319f539"},"cell_type":"markdown","source":"## Test the accuracy of trained model"},{"metadata":{"trusted":true,"_uuid":"3a0a87663adb3a3fba23833dd0c96e451c2e4994"},"cell_type":"code","source":"# CAUTION: We used the same vectorizer we used for the training data.\n# For test data, we are only doing transform, not fit_transform, as we \n# want to use the same vectorizer which was fitted on train data.\nX_test_vectorized = vectorizer.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9be84742531cde44e0555e464b1dc440c71e569a"},"cell_type":"code","source":"y_pred = knn.predict(X_test_vectorized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfbec501d64142175e5f9ec11f4e33b44661301c"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\naccuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ce54b1d488ab9381b18fe67cc577bc8d6dd31b6"},"cell_type":"markdown","source":"So, using all the features available in the dataset, we have achieved accuracy of >92%. Now, we are going to use feature selection, where we will use only top 100 mostly used features and retrain the model. Lets see if we can achieve any improvement in the prediction result."},{"metadata":{"_uuid":"0b43c4a265b752fffe56d320f8c20ebe5fc58e8b"},"cell_type":"markdown","source":"## Process dataset using feature selection"},{"metadata":{"_uuid":"d4423a1826dad111f767f990acb0927df18a2af9"},"cell_type":"markdown","source":"For our current problem at hand, we can use two different types of feature selection: SelectKBest (selects K best features) or SelectPercentile (select a percentage of original features). Here we are going to use SelectKBest, where K = 100 (we will use 100 best features) with chi2 as the scoring function."},{"metadata":{"trusted":true,"_uuid":"395a2868297b56a79dd63c5b1d3c5d322a0fef0b"},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nskb = SelectKBest(score_func=chi2, k=100)\nfeatures_fit = skb.fit(X_train_vectorized, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e772258a2a28885ae80ba3ebf127a08fa7a5c93"},"cell_type":"code","source":"features_fit.scores_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47e13bd1c616c69c718e18590b500c186001c1fb"},"cell_type":"code","source":"X_train_selected = features_fit.transform(X_train_vectorized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47c635e6946213520cccc13a73b30719c39c393b"},"cell_type":"code","source":"X_train_selected.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0ffc7f7abcb6cbe572c47b4c7ea8d2ea0f03a4f"},"cell_type":"markdown","source":"## Train new model with feature selected data"},{"metadata":{"trusted":true,"_uuid":"29f8050a04dba041624d124cf9df7b6aa876d63c"},"cell_type":"code","source":"knn_selected = KNeighborsClassifier(n_neighbors=3)\nknn_selected.fit(X_train_selected, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fa024244eb86ed8da0189acf20b0bd7a6f392fc"},"cell_type":"code","source":"X_test_selected = features_fit.transform(X_test_vectorized)\ny_pred_selected = knn_selected.predict(X_test_selected)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed86423300e1a0b0755d592627791748baf9c87e"},"cell_type":"code","source":"accuracy_score(y_test, y_pred_selected)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc9d365bdf67e3e12dd2d71436a014622c9f8411"},"cell_type":"markdown","source":"Oohoo! Without doing any changes to the data or algorithm, using as they are, only using feature selection and using best 100 features, we have achieved a accuracy score 95.6% which is more than 3.6% more than the previous one we trained above. "},{"metadata":{"_uuid":"8bac1cb270fb627739599d8c2d6a9a6e9e933ca2"},"cell_type":"markdown","source":"We can try different other training algorithms as well as play with model parameters and number of features to use to see if we can achieve accuracy score as much as 99%. But that is for some other day :)"},{"metadata":{"_uuid":"664fa09b592b92d32214f2a077dc43d428040d21"},"cell_type":"markdown","source":"## Saving the trained model on disk"},{"metadata":{"_uuid":"415872af5a60d654e9ca637ddea695926d3072b9"},"cell_type":"markdown","source":"After we successfully train a model, the thing we want to do is save the model on disk, so that it can be used later. We are going to save our trained model using pickle."},{"metadata":{"trusted":true,"_uuid":"0d0f71185450099e07f0c0ca1b149e9ca1197054"},"cell_type":"code","source":"import pickle\n\npickle.dump(knn_selected, open('knn_selected.model', 'wb'))\n\n# The thing we always miss is that we also have to save the \n# vectorizer and the feature selection model for future use.\n# We have trained our model on those modules and any future \n# data has to be processed by them before we can perform \n# any prediction. Otherwise, we will ended up having a \n# shape mismatch error when we try to predict future data\n# on loaded model using processed by fresh vectorizer and \n# feature selection model\n\n# Save the vectorizer\npickle.dump(vectorizer, open('knn.vect', 'wb'))\n\n# Save the feature selection model\npickle.dump(features_fit, open('feature_selector.feat', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b25993506fa3d5adc9b0bb5f1d3247a6032c8354"},"cell_type":"code","source":"os.listdir(os.getcwd())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c85aa9982258fce804ac00368f349181bd4ea19f"},"cell_type":"markdown","source":"## Load the trained model from disk"},{"metadata":{"trusted":true,"_uuid":"f2ab7b0224584fdd8815719ed4a7cbeae94ca949"},"cell_type":"code","source":"knn_selected_saved = pickle.load(open('knn_selected.model', 'rb'))\nvectorizer_saved = pickle.load(open('knn.vect', 'rb'))\nfeatures_fit_saved = pickle.load(open('feature_selector.feat', 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2cd72a6c39c03f204400845627b72f6d0f262c0"},"cell_type":"markdown","source":"Lets test our loaded model using it to predict on our test data. If we get the same accuracy score of 95.6%, then our model reload is successful."},{"metadata":{"trusted":true,"_uuid":"4094f82708584e87a4fcd38ac5ea2725747535c3"},"cell_type":"code","source":"saved_pred = knn_selected_saved.predict(X_test_selected)\naccuracy_score(y_test, saved_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a532c3b80f3c12ff175f022290fc4f8aeaee0a7"},"cell_type":"markdown","source":"Perfect!! :)"},{"metadata":{"_uuid":"81f7701f2d4919ba3a5f07638c10479b6754414f"},"cell_type":"markdown","source":"## Test loaded model on complete unseen data"},{"metadata":{"trusted":true,"_uuid":"a7793313ec6a8515368068b7559c45427db9d02a"},"cell_type":"code","source":"validation_data = pd.DataFrame.from_dict({\n        'sms': ['Baa, baa, black sheep, have you any wool? Yes sir, yes sir, three bags full! One for the master, And one for the dame, One for the little boy Who lives down the lane']\n    })\n\nvalidation_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33ebd6a263bc592cf301ecf8faafaab65f9d9500"},"cell_type":"code","source":"# Similar data clean up we did earlier on our train/test dataset\nvalidation_data['sms'] = validation_data['sms'].apply(lambda x: BeautifulSoup(x.lower(), 'html.parser').get_text())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6776ce61f704cd1c884c945ac07bd006a7ed4711"},"cell_type":"code","source":"# Note we are processing our data using our loaded vectorizer and \n# feature selection model and only doing transform, not fit_transform\nvalidation_features = vectorizer_saved.transform(validation_data['sms'])\nvalidation_features = features_fit_saved.transform(validation_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea60965492c17265fe5959fe5f65cd08e93a6bb3"},"cell_type":"code","source":"# Lets see if our feature selection worked\nvalidation_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f8b01dce8fc96a66638f2f46ed1ef0afe282dc0"},"cell_type":"code","source":"# Predict on new unseen data\nknn_selected.predict(validation_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5c04d6ce86a3ed58e53f89cc6b7ffe0beebca16"},"cell_type":"code","source":"knn_selected.predict_proba(validation_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a091ff5f3522593ee294665a5999d90946cc699f"},"cell_type":"markdown","source":"So, we can see that our new unseen data was detected as not being spam (I am glad that my favorite childhood rhyme was not detected as spam :)) (equals 0 means not spam remember from above?). Also predict_proba() tells us that it was detected as not spam with 100% confidence."},{"metadata":{"trusted":true,"_uuid":"f0aacccab28a7826cec226f5528e081fae0f8620"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}