{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting a Stroke"},{"metadata":{},"cell_type":"markdown","source":"This is going to be very short notebook to illustrate how **SMOTE** (synthetic minority over-sampling technique) combined with `Random Forest` classifier can improve accuracy for imbalanced classification problem.\n\nWe call classification problem imbalanced when one or more target classes have significantly smaller number of observations. In our case we have:\n - **4860** observations with class `0` (patient had no stroke)\n - **249** observations with class `1` (patient had a stroke)\n \nThis complicates life of the researcher, since ML models will choose path of least resistance and predict that patient had no stroke **at all times** and it will be correct approximately 90% of the time. I will illustrate how `Random Forest` would behave under these circumstances.\n\nPredicting that patient had no stroke is a good thing, but predicting if he will have a stroke might be even more important. As a researcher we have to find the right balance between `true positives` and `false positives` or other characteristics, depends on where our place of interest is in.\n\nSo let's get started"},{"metadata":{},"cell_type":"markdown","source":"## TL;DR"},{"metadata":{},"cell_type":"markdown","source":"`Random Forest` classifier on imbalanced data on average predicted 0-5% `true positives` (patient has a stroke), which is pretty bad, considering we would like to notify a person who has a risk of stroke beforehand.. It had no problem correctly identifying patients with no stroke, of course. With oversampling technique `Random Forest` was able to predict ~90% `true positives` and number of `false negatives` was also decreased."},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        file = os.path.join(dirname, filename)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Loading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.stroke.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Dealing With Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.bmi = df.bmi.fillna(df.bmi.median()) # => 28","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Transforming Categorical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[df.gender != 'Other'] # there is just one row with 'Other' gender. Just drop it\ngender = pd.get_dummies(df.gender)\nhypertension = pd.get_dummies(df.hypertension, prefix='hypertension')\nheart_disease = pd.get_dummies(df.heart_disease, prefix='heart_disease')\never_married = pd.get_dummies(df.ever_married, prefix='married')\nwork_type = pd.get_dummies(df.work_type)\nsmoking_status = pd.get_dummies(df.smoking_status)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_df = pd.concat(\n    [\n        df.age, \n        df.avg_glucose_level, \n        df.bmi,\n        gender, \n        hypertension, \n        heart_disease, \n        ever_married, \n        work_type, \n        smoking_status,\n        df.stroke\n    ],\n    axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Train-Test-Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separating features and target variable AKA dependent variable\nX = clean_df.drop('stroke', axis=1)\ny = clean_df.stroke\n\n# Scaling features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# We're going to use imbalanced dataset first to see what accuracy we can get\nX_train, \\\nX_test, \\\ny_train, \\\ny_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3 Modeling"},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Random Forest on Imbalanced Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=100, n_jobs=4)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\nprint(f'Random Forest accuracy: {accuracy_score(y_test, y_pred)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\nprint(f'TP: {tp}')\nprint(f'FN: {fn}')\nprint(f'TN: {tn}')\nprint(f'FP: {fp}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> So what does it mean? Random Forest was able to predict class '0' pretty well (TN), but failed to identify patients with class '1' (TP). On top of that it falsely identified 79 patients with a stroke. Not good."},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Random Forest 2.0 (with SMOTE)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Oversampling with SMOTE\noversampling = SMOTE()\noversampling = oversampling.fit_resample(X_scaled, y)\nX_smote = oversampling[0]\ny_smote = oversampling[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_smote.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_smote, \\\nX_test_smote, \\\ny_train_smote, \\\ny_test_smote = train_test_split(X_smote, y_smote, test_size=0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_smote = RandomForestClassifier(n_estimators=100, n_jobs=4)\nrf_smote.fit(X_train_smote, y_train_smote)\ny_pred_smote = rf_smote.predict(X_test_smote)\nprint(f'Random Forest accuracy: {accuracy_score(y_test_smote, y_pred_smote)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tn, fp, fn, tp = confusion_matrix(y_test_smote, y_pred_smote).ravel()\nprint(f'TP: {tp}')\nprint(f'FN: {fn}')\nprint(f'TN: {tn}')\nprint(f'FP: {fp}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This looks better. Model was able to correctly identify 1158 patients with stroke and 1120 of without. Though it had it flaws with false positive and false negatives, it did much better job than base model.\n\nBut what we're interested in is to see how our new model works on imbalanced data from first example. Lets try"},{"metadata":{},"cell_type":"markdown","source":"## 3.3 Random Forest 2.0 on Imbalanced Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = rf_smote.predict(X_test)\nprint(f'Random Forest accuracy: {accuracy_score(y_test, y_pred)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\nprint(f'TP: {tp}')\nprint(f'FN: {fn}')\nprint(f'TN: {tn}')\nprint(f'FP: {fp}')\nprint('\\n')\nprint(f'Precision: {tp/(tp+fp)}\\nRecall: {tp/(tp+fn)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pretty good! Now instead of 0 TP we have 73, but we did sacrifice quite a bit on `false positives` (31), that means our model wrongfully marked healthy person having a stroke. Yikes"},{"metadata":{},"cell_type":"markdown","source":"## Conclusion"},{"metadata":{},"cell_type":"markdown","source":"Oversampling worked magic in this example, bringing total accuracy up to the 97%. But since we care more about correctly identifying patients with a stroke (class 1), we should pay attention to the `true positives` and `false negatives`. Those numbers are up. I have to admit that the title of this notebook is a bit clickbaity. If you run through test many times you'll get 95% accuracy on average."},{"metadata":{},"cell_type":"markdown","source":"**PS: if you have questions or recommendations, please feel free to comment. Cheers.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}