{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Fake News detector**\n\nIn this very first NN model I make I try to use some 'sentiment analysis' (with conv1d) to try to detect fake news.\nI developed a Vocabulary class which converts words to indeces, based on frequency of words. This class in particular can be largely improved.\n\nI achieved an interesting result of 98%, which seems to be confirmed testing on titles available online.\n\nHowever the model sill has some issues might be resolved in the future:\n* Fake news seem to use CAPSLOCK much more often, this is not detected\n* Longer titles are more likely to be read as Fake, whereas shorter are read as Real","execution_count":null},{"metadata":{"id":"4lxFCxTdRtmK","outputId":"119a98ba-214c-440c-b3da-bd338eb8759a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, LSTM, Embedding, Flatten\n\nfrom sklearn.compose import ColumnTransformer","execution_count":null,"outputs":[]},{"metadata":{"id":"jBmymYWagKiZ"},"cell_type":"markdown","source":"Inspired from web.","execution_count":null},{"metadata":{"id":"-JvA8JQsgHPQ","trusted":true},"cell_type":"code","source":"import nltk\nfrom re import sub\n\nnltk.download('stopwords')\n\nclass Vocabulary:\n\n  def __init__(self,max_words):\n    self.max_words = max_words\n    self.word2index = {}\n    self.word2count = {}\n    self.index2word = {}\n    self.num_words = 0\n    self.StopWords = set(nltk.corpus.stopwords.words('english'))\n\n  def add_word(self, word):\n    if word not in self.word2count:\n      # First entry of word into vocabulary\n      self.word2count[word] = 1\n      self.num_words += 1\n    else:\n      # Word exists; increase word count\n      self.word2count[word] += 1\n          \n  def add_sentence(self, sentence):\n    sentence = sub(r'[^\\w\\s]','',sentence)\n    for word in sentence.split(' '):\n      to_add = word.lower()\n      if to_add not in self.StopWords:\n        self.add_word(to_add)\n\n  def consolidate(self):\n    self.index2word = {0 : \"NULL\"}\n\n    sortedList = [k for k, v in sorted(self.word2count.items(), key=lambda item: item[1],reverse=True)]\n    for idx in range(1,min(len(sortedList),self.max_words)+1):\n      self.index2word[idx] = sortedList[idx-1]\n    self.word2index = dict({(value,key) for (key,value) in self.index2word.items()})\n\n\n  def to_word(self, index):\n    return self.index2word[index]\n\n  def to_index(self, word):\n    return self.word2index[word]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Useful for later processing","execution_count":null},{"metadata":{"id":"HB89cRkomMZf","trusted":true},"cell_type":"code","source":"def sent2list(sent,word2idx):\n    idxList=[]\n    for word in sent.split(' '):\n        try:\n            idxList.append(word2idx[word.lower()])\n        except:\n            pass\n    return np.array(idxList)","execution_count":null,"outputs":[]},{"metadata":{"id":"OxaC8yDyp-V_","trusted":true},"cell_type":"code","source":"def fake_or_not(y):\n  if y>0.75: return print(\"Fake\")\n  elif y>0.5: return print(\"Probably Fake\")\n  elif y>0.25: return print(\"Probably True\")\n  else: return print(\"True\")\n\nfrom re import sub\n\ndef preprocess(sent, word2idx):\n    sent = sub(r'[^\\w\\s]','',sent)\n    return sent2list(sent, word2idx)","execution_count":null,"outputs":[]},{"metadata":{"id":"zOKdPQbFSB3t","outputId":"9875136b-c94c-410e-a56a-8694a826fcad","trusted":true},"cell_type":"code","source":"Fake = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/Fake.csv\")\nReal = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/True.csv\")\nFake[\"Fake\"]=1\nReal[\"Fake\"]=0\ndata = pd.concat([Fake,Real])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"OFJPbhLazCUk"},"cell_type":"markdown","source":"**Create word to index conversion and viceversa**","execution_count":null},{"metadata":{"id":"F9xqhLk1cFjX","trusted":true},"cell_type":"code","source":"num_words = 20000\n\nvoc = Vocabulary(num_words)\n\nfor sentence in data.text.values:\n  voc.add_sentence(sentence)\n\nvoc.consolidate()\n\nidx2word = voc.index2word\nword2idx = voc.word2index","execution_count":null,"outputs":[]},{"metadata":{"id":"YvWx3gZ7x4xG","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ny = data['Fake'].values\nX = data[['title','text']].values\n\nX[:,0] = [sent2list(sent,word2idx) for sent in X[:,0]]\nX[:,1] = [sent2list(sent,word2idx) for sent in X[:,1]]\n\nX_train, X_test, y_train, y_test = train_test_split(X,y)","execution_count":null,"outputs":[]},{"metadata":{"id":"eH2YupWwiVX4","trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\nmax_words = 700\n#X_temp=np.array(X_temp.shape[0],max)\nX_temp = pad_sequences(X_train[:,1],maxlen=max_words)\nX_train = X_temp\nX_temp = pad_sequences(X_test[:,1],maxlen = max_words)\nX_test = X_temp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finished preprocessing, start the model","execution_count":null},{"metadata":{"id":"UsT3zjCljaip","outputId":"783e1746-31d0-4a5b-e83a-3096d4adce7d","trusted":true},"cell_type":"code","source":"from keras.layers.convolutional import Conv1D,MaxPooling1D\n\nmodel = Sequential()\nmodel.add(Embedding(num_words+1,100,input_length=max_words))\nmodel.add(LSTM(32, dropout=0.9, return_sequences=True))\nmodel.add(Conv1D(filters=32,kernel_size=3,padding='same',activation='relu'))\nmodel.add(MaxPooling1D())\nmodel.add(Flatten())\nmodel.add(Dropout(0.9))\nmodel.add(Dense(1,activation='sigmoid'))\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"tu4HngJLkkqR","outputId":"7f3afdd0-e847-471d-a5f4-91fe10535b71","trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\nmodel.fit(X_train,y_train,batch_size=512,epochs=15,validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"About 99% accuracy on test set","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Random title from NYT","execution_count":null},{"metadata":{"id":"Aqc4yHF7mqDe","outputId":"87b90c08-b3c1-41cb-e529-3eaf475240f4","trusted":true},"cell_type":"code","source":"text = 'With a flood of unemployment claims continuing to overwhelm many state agencies, economists say the job losses may be far worse than government tallies indicate. The Labor Department said Thursday that 3.8 million workers filed for unemployment benefits last week, bringing the six-week total to 30 million. But researchers say that as the economy staggers under the weight of the coronavirus pandemic, millions of others have lost jobs but have yet to see benefits. A study by the Economic Policy Institute found that roughly 50 percent more people than counted as filing claims in a recent four-week period may have qualified for benefits — with the difference representing those who were stymied in applying or didn’t even try because the process was too formidable. “The problem is even bigger than the data suggest,” said Elise Gould, a senior economist with the institute, a left-leaning research group. “We’re undercounting the economic pain.” Alexander Bick of Arizona State University and Adam Blandin of Virginia Commonwealth University found that 42 percent of those working in February had lost their jobs or suffered a reduction in earnings. By April 18, they found, up to eight million workers were unemployed but not reflected in the weekly claims data. The difficulties at the state level largely flow from the sheer volume of claims, which few agencies were prepared to handle. Many were burdened by aging computer systems that were hard to reconfigure for new federal guidelines. “We’ve known that the state unemployment insurance systems were not up to the task, yet those investments were not made,” Ms. Gould said. “The result is that the state systems are buckling under the weight of these claims.” The crush of claims is a major reason — but not the only one — that states are backlogged. Frustrated applicants who refile their applications, some as many as 20 times, slow the system as processors weed out duplicates. Some applications are missing i formation. New York analyzed a million claims and found many had been delayed because of a missing employer identification number. In such cases, each applicant has to be called back. Callers looking for updates also flood the system, increasing the wait for those who need to correct a mistake.'\nsentTest = preprocess(text, word2idx)\n\nsentTest = sentTest.reshape(1,sentTest.shape[0])\nsentTest\nsentTest = pad_sequences(sentTest, maxlen = max_words)\n\ny = model.predict(sentTest)\ny, fake_or_not(y)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}