{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Read dataset**","metadata":{}},{"cell_type":"code","source":"%%time\ntrain = pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/train.csv')\ntest = pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Preview dataset**","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def num_of_words(df):\n    df['word_count'] = df['tweet'].apply(lambda x : len(str(x).split(\" \")))\n    print(df[['tweet','word_count']].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_of_words(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_of_words(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see that word counts in every tweet has been calculated above.","metadata":{}},{"cell_type":"code","source":"def num_of_chars(df):\n    df['char_count'] = df['tweet'].str.len() ## this also includes spaces\n    print(df[['tweet','char_count']].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_of_chars(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_of_chars(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def avg_word(sentence):\n    words = sentence.split()    \n    return (sum(len(word) for word in words)/len(words))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def avg_word_length(df):\n    df['avg_word'] = df['tweet'].apply(lambda x: avg_word(x))\n    print(df[['tweet','avg_word']].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_word_length(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_word_length(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nset(stopwords.words('english'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can count the number of stopwords as follows-","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstop = stopwords.words('english')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stop_words(df):\n    df['stopwords'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x in stop]))\n    print(df[['tweet','stopwords']].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def hash_tags(df):\n    df['hashtags'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n    print(df[['tweet','hashtags']].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hash_tags(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hash_tags(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def num_numerics(df):\n    df['numerics'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n    print(df[['tweet','numerics']].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_numerics(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_numerics(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def num_uppercase(df):\n    df['upper_case'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n    print(df[['tweet','upper_case']].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_uppercase(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_uppercase(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n          'This is the first document.',\n          'This document is the second document.',\n          'And this is the third one.',\n          'Is this the first document?',\n         ]\n\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.toarray())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\nX2 = vectorizer2.fit_transform(corpus)\nprint(vectorizer2.get_feature_names())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X2.toarray())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import HashingVectorizer\ncorpus = [\n          'This is the first document.',\n          'This document is the second document.',\n          'And this is the third one.',\n          'Is this the first document?',\n         ]\nvectorizer = HashingVectorizer(n_features=2**4)\nX = vectorizer.fit_transform(corpus)\nprint(X.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lower_case(df):\n    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n    print(df['tweet'].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lower_case(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lower_case(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def punctuation_removal(df):\n    df['tweet'] = df['tweet'].str.replace('[^\\w\\s]','')\n    print(df['tweet'].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"punctuation_removal(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"punctuation_removal(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstop = stopwords.words('english')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stop_words_removal(df):\n    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n    print(df['tweet'].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words_removal(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words_removal(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[:10]\nfreq","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq = list(freq.index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def frequent_words_removal(df):    \n    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n    print(df['tweet'].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frequent_words_removal(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frequent_words_removal(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[-10:]\nfreq","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq = list(freq.index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rare_words_removal(df):\n    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n    print(df['tweet'].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rare_words_removal(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rare_words_removal(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from textblob import TextBlob","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def spell_correction(df):\n    return df['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spell_correction(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spell_correction(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokens(df):\n    return TextBlob(df['tweet'][1]).words","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem import PorterStemmer\nst = PorterStemmer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stemming(df):\n    return df['tweet'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stemming(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stemming(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from textblob import Word","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lemmatization(df):\n    df['tweet'] = df['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n    print(df['tweet'].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemmatization(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemmatization(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from textblob import TextBlob","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combination_of_words(df):\n    return (TextBlob(df['tweet'][0]).ngrams(2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combination_of_words(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combination_of_words(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def term_frequency(df):\n    tf1 = (df['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n    tf1.columns = ['words','tf']\n    return tf1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"term_frequency(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"term_frequency(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf1 = (train['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\ntf1.columns = ['words','tf']\ntf1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf2 = (test['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\ntf2.columns = ['words','tf']\ntf2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf1 = (train['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\ntf1.columns = ['words','tf']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i,word in enumerate(tf1['words']):\n    tf1.loc[i, 'idf'] = np.log(train.shape[0]/(len(train[train['tweet'].str.contains(word)])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf1['tfidf'] = tf1['tf'] * tf1['idf']\ntf1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n stop_words= 'english',ngram_range=(1,1))\ntrain_vect = tfidf.fit_transform(train['tweet'])\ntrain_vect","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nbow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\ntrain_bow = bow.fit_transform(train['tweet'])\ntrain_bow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4.7 Model","metadata":{}},{"cell_type":"code","source":"def polarity_subjectivity(df):\n    return df['tweet'][:5].apply(lambda x: TextBlob(x).sentiment)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"polarity_subjectivity(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentiment_analysis(df):\n    df['sentiment'] = df['tweet'].apply(lambda x: TextBlob(x).sentiment[0] )\n    return df[['tweet','sentiment']].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment_analysis(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nmax_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(train_model['tweet'].values)\nX = tokenizer.texts_to_sequences(train_model['tweet'].values)\nX = pad_sequences(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment_analysis(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"polarity_subjectivity(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_model = train[['tweet','sentiment']]\ntrain_model.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nX_train, X_test, Y_train, Y_test = train_test_split(X,train[['sentiment']], test_size = 0.33, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nimport re","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nmax_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(train_model['tweet'].values)\nX = tokenizer.texts_to_sequences(train_model['tweet'].values)\nX = pad_sequences(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm_out = 196\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, 128,input_length = 17))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X,train[['sentiment']], test_size = 0.33, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils import to_categorical\ntrain_labels = to_categorical(Y_train)\nbatch_size = 32\nmodel.fit(X_train, train_labels, epochs = 7, batch_size=batch_size, verbose = 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_size = 32\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\ntest_labels = to_categorical( Y_test)\nscore,acc = model.evaluate(X_test, test_labels, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = {'No' : 1, 'tweet' : \"this project is horrible\"}\ntestline = pd.DataFrame(data, index=[0])\nsentiment_analysis(testline)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test = pd.read_csv('/kaggle/input/xarvio/data_Field_Manager.csv')\nrating = list(x_test[\"Rating\"])\nreviews =list(x_test[\"Review\"]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_review = []\nfor i in range (len(reviews)):\n    data = {'No' : 1, 'tweet' : reviews[i]}  \n    testline = pd.DataFrame(data, index=[0])\n    score = sentiment_analysis(testline)\n    #print((score[\"sentiment\"]+1)*2.5 , rating[i])\n    predicted_review.append(float(score[\"sentiment\"]+1)*2.5)\n    #print(score*5 , rating[i], reviews[i])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = {'Review' : reviews, 'Rating' : rating , 'Prediction' : predicted_review}\n\nResult = pd.DataFrame(data)\nResult","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Go to Top](#0)\t","metadata":{}}]}