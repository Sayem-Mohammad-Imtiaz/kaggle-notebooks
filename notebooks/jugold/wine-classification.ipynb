{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import most usefull libs\nimport pandas as pd\nimport numpy as np\n\n\n#read data for model\n\nwine_path=(\"../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")\ndataset=pd.read_csv(wine_path)\ndataset['quality_label'] = dataset.quality.apply(lambda q: 'low' if q <= 5 else 'medium' if q <= 7 else 'high')\n\ndataset=pd.get_dummies(dataset)\ndataset.head ()\n#print(type(dataset))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check for missing values in the  columns\ncols_missing=[col for col in dataset.columns\n             if dataset[col].isnull().any()]\nif cols_missing == []:\n    print(\"Keine Missing Values\")\n    \n#schauen ob der Datensatz kategorische Daten hat\n\nlist_cols_objects= (dataset.dtypes==\"object\")\nobject_cols= list(list_cols_objects[list_cols_objects].index)\n\nif object_cols == []:\n    print(\"Keine kategorischen Werte\")\nelse :\n    print(\"Kategorische Werte gefunden\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.model_selection import train_test_split\n\n#Werte in X und y splitten, in feature und target und preprocessor vorbereiten, dabei brauchen wir keinen OneHotEncoder oder SampleImputer\n\nX= dataset.copy(deep=True)\ncolomns_for_x=dataset.columns [0:11]\nX= dataset[colomns_for_x]\ncolumns_for_y= dataset.columns[12:]\ny= dataset[columns_for_y]\n\n\nX_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.1)\n\nsc= StandardScaler()\nX_train = sc.fit_transform(X_train) #hier am besten den Durschnitt von allem oder nur von X_train\nX_valid= sc.transform(X_valid)\ninput_shape = [X.shape[1]]\nprint(\"Input shape: {}\".format(input_shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"#preprocessor auf X anwenden um die Werte zu standartisieren --> Vermutung auf bessere Performance\npreprocessor= make_column_transformer(\n                                    (StandardScaler(),\n                                    make_column_selector(dtype_include=np.number)),\n                                   )\nX = preprocessor.fit_transform(X)\nprint(\"fertig\")\ninput_shape = [X.shape[1]]\nprint(\"Input shape: {}\".format(input_shape))\npd.DataFrame(X[:10,:]).head()\"\"\" #aus Exercise: Stochastic Gradient Descent könnte man auch nehmen","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nclassification_model=keras.Sequential([\n    layers.Dense(100, activation=\"relu\", input_shape=input_shape), #100\n    layers.Dense(200, activation=\"relu\"), #200\n    layers.Dense(64,activation=\"relu\"),#64\n    layers.Dense(3, activation='softmax') #Notiz: softmax oder sigmoid hat das Model extrem verbessern --> warum recherchieren!!!\n])\nprint(\"finished\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#model compilen und loss function (Ziel was trainiert werden sol z.B. Regression oder Klassifikation und Optimizer definiernén)\nclassification_model.compile(\n    optimizer=\"adam\",\n    loss=\"categorical_crossentropy\",\n    metrics=\"accuracy\",\n    \n)\nprint(\"finished\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = classification_model.fit(\n            X, y,\n            batch_size=60,\n            epochs=10,\n            \n)\n\n# Check your answer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nhistory_df = pd.DataFrame(history.history)\n# Start the plot at epoch 5. You can change this to get a different view.\nhistory_df.loc[5:, ['loss']].plot();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred= classification_model.predict(X_valid)\nprint(y_pred[0:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_rounded = np.argmax(y_pred,axis=1)[:] #Returns the indices of the maximum values along an axis.\ny_valid.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ny_valid_keine_Indices=y_valid.iloc[:,:].values #Indizes und labels aus dem altes df löschen\n\ny_valid = np.argmax(y_valid_keine_Indices,axis=1)[:] #Returns the indices of the maximum values along an axis.\nprint(y_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(20):\n    print(\"Predicted %d ---> Expected %d\"%(y_pred_rounded[i],y_valid[i]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shap\nshap.initjs()\n\n# rather than use the whole training set to estimate expected values, we summarize with\n# a set of weighted kmeans, each weighted by the number of points they represent.\nX_train_summary = shap.kmeans(X_train, 10)\n\nexplainer = shap.KernelExplainer(classification_model.predict, X_train_summary)\nshap_values = explainer.shap_values(X_valid)\nshap.summary_plot(shap_values, X_valid)\n\n\nprint(\"DONE\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}