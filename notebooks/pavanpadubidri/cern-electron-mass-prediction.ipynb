{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport seaborn as sns\nsns.set_style(style='darkgrid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '/kaggle/input/cern-electron-collision-data/dielectron.csv'\ndf = pd.read_csv(path)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis\n\nCheck for missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since only 0.085% of the data is missing, let us drop those rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features `[Run, Event]` are the run number and event numbers which does not contribute to the target variable and can be dropped."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=['Run', 'Event'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A correlation matrix helps in understanding which features directly affect the target variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df.corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(corr, annot=True, fmt='0.1f');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the heatmap, it can be observed that target variable, `M` is dependent directly on `[E1, pt1, E2, pt2]`. \\\\\nFrom the correlation matrix, it can be observed that some features correlated to each other. Let us visualize some of the correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10), tight_layout=True)\nr,c = 3, 3\nplt.subplot(r,c,1)\nsns.scatterplot(x=df['E1'], y=df['pt1']);\nplt.subplot(r,c,2)\nsns.scatterplot(x=df['px1 '], y=df['px2']);\nplt.subplot(r,c,3)\nsns.scatterplot(x=df['phi1'], y=df['py1']);\nplt.subplot(r,c,4)\nsns.scatterplot(x=df['py1'], y=df['py2']);\nplt.subplot(r,c,5)\nsns.scatterplot(x=df['pz1'], y=df['pz2']);\nplt.subplot(r,c,6)\nsns.scatterplot(x=df['pt1'], y=df['pz2']);\nplt.subplot(r,c,7)\nsns.scatterplot(x=df['E2'], y=df['pt2']);\nplt.subplot(r,c,8)\nsns.scatterplot(x=df['pz2'], y=df['eta2']);\nplt.subplot(r,c,9)\nsns.scatterplot(x=df['phi2'], y=df['py2']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression\n\nSince the target is floating value, linear regression can be applied on the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(columns='M')\ny = df[['M']].squeeze()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before applying linear regression, let us check the data distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scaling the data helps in increasing the performance of the regression model. Let us split the data into train and test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"xTr, xTs, yTr, yTs = train_test_split(X, y, test_size = 0.25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applying minmax scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\nxTr = scaler.fit_transform(xTr)\nxTs = scaler.transform(xTs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fit a linear model and check the MSE and R2 for test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = LinearRegression()\nreg.fit(xTr, yTr)\nyPred = reg.predict(xTs)\nprint(f'MSE: {mean_squared_error(yTs, yPred):0.2f}, R2: {r2_score(yTs, yPred):0.4f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A graph of actual vs predicted gives how well the model fits the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nxyMin, xyMax = yTs.min(), yTs.max()\nsns.lineplot(x=[xyMin, xyMax], y=[xyMin, xyMax], color='red')\nsns.scatterplot(x=yTs, y=yPred);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The given dataset is not linear on the target variable, since MSE is a large number, and R2 is small."},{"metadata":{},"cell_type":"markdown","source":"## Random forest\n\nSince the data is not linear, to design a more complex model, random forest of decision trees can be applied."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = RandomForestRegressor()\nreg.fit(xTr, yTr)\nyPred = reg.predict(xTs)\nprint(f'MSE: {mean_squared_error(yTs, yPred):0.2f}, R2: {r2_score(yTs, yPred):0.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nxyMin, xyMax = yTs.min(), yTs.max()\nsns.lineplot(x=[xyMin, xyMax], y=[xyMin, xyMax], color='red')\nsns.scatterplot(x=yTs, y=yPred);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using random forest, the model performs better on test data with relatively small MSE and higher R2. Since the model performed well, let us understand which features are important in predicting the target variable and their ranking."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nfeature_importance = reg.feature_importances_\nidx = np.argsort(-feature_importance, )\nsns.barplot(x=df.columns[idx] ,y=feature_importance[idx]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`[pt2, pt1, E2, E1, pz2, eta1, eta2, px2, py1, py2, px1, Event, phi2, phi1, Run, Q2, Q1]` is the order of importance of the features in predicting the target `M`."},{"metadata":{},"cell_type":"markdown","source":"## Fully Connected Neural Network\n\nEventhough with random forest MSE reduced sigificantly, and model predicts with relatively high accuracy, neural network can be built for the model since 100k data is available with only 16 fetures."},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Building a FC NN model with 2 hidden layers. One with 32 units another with 8 units. The model has 817 trainable parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.backend.clear_session()\nmodel = tf.keras.Sequential([\n            tf.keras.layers.Dense(units=32, input_shape=(xTr.shape[1],), activation='relu', name='FC1'), \n            tf.keras.layers.Dense(units=8, activation='relu', name='FC2'), \n            tf.keras.layers.Dense(units=1, name='Output')\n])\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.002), loss=tf.keras.losses.mean_squared_error)\ninit = 0\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Writing a custom callback function to log MSE of validation data every 10 epochs."},{"metadata":{"trusted":true},"cell_type":"code","source":"class LogMetrics(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        if epoch % 10 == 9:\n            val_loss = mean_squared_error(yTs, self.model.predict(xTs))\n            print(f'Epoch: {epoch+1}/{self.params[\"epochs\"]} - val_loss: {val_loss:0.4f} ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Passing early stopping callback to avoid overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"xtr, xval, ytr, yval = train_test_split(xTr, yTr, test_size=0.1)\nepochs = 500\nhist = model.fit(xtr, ytr, validation_data=(xval, yval), epochs=epochs, verbose=0, batch_size=128, \n                 callbacks=[LogMetrics(), tf.keras.callbacks.EarlyStopping(min_delta=0.1, patience=5)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A graph of traing loss vs valiation loss gives insight on model overfitting/ underfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nsplot = sns.lineplot(data=hist.history);\nsplot.set(xscale=\"log\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yPred = model.predict(xTs)\nprint(f'MSE: {mean_squared_error(yTs, yPred):0.2f}, R2: {r2_score(yTs, yPred):0.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.set_style(style='darkgrid')\nxyMin, xyMax = yTs.min(), yTs.max()\nsns.lineplot(x=[xyMin, xyMax], y=[xyMin, xyMax], color='red')\nsns.scatterplot(x=yTs, y=yPred.squeeze());","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}