{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import nltk\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#To check the encoding of the CSV in order to avoid issue while reading CSV using Pandas.\n\nimport chardet\nwith open('/kaggle/input/sms-spam-collection-dataset/spam.csv', 'rb') as rawdata:\n    result = chardet.detect(rawdata.read(100000))\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv',encoding='Windows-1252')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Remove the unwanted columns from dataset and change the label v1 and v2 to label and message respectively\ndata=data[['v1','v2']]\ndata.columns=['label','message']\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport re\n\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=[]\nps=PorterStemmer()\n\n\nfor i in range(0,len(data)):\n    \n    review=re.sub('[^a-zA-Z]',' ', data['message'][i])\n    review.lower()\n    reviews=review.split(' ')\n    review=[ps.stem(word) for word in reviews if  not word in set(stopwords.words('english'))]\n    \n    \n    review=' '.join(review)\n    corpus.append(review)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#using Bag Of Word\n\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer()\n\nX=cv.fit_transform(corpus).toarray()\nX.shape\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=pd.get_dummies(data['label'])\ny=y.iloc[:,1].values\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Train Test Split\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train , X_test, y_train , y_test=train_test_split(X,y,test_size=0.3,random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Selection Naive Bayes\n\nfrom sklearn.naive_bayes import MultinomialNB\n\nmodel=MultinomialNB().fit(X_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nconfusionM=confusion_matrix(y_test,y_pred)\nconfusionM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP=confusionM[0][0]\nFP=confusionM[0][1]\nFN=confusionM[1][0]\nTN=confusionM[1][1]\n\n\n\naccuracy=((TP + TN)/(TP+FP+FN+TN)) * 100\nprint(accuracy)\n\n#Precision tells us how many of the correctly predicted cases actually turned out to be positive.\nprecision=(TP/(TP +FP) ) * 100\nprint(precision)\n\n#Recall tells us how many of the actual positive cases we were able to predict correctly with our model.\nrecall= (TP / (TP + FN)) * 100\nprint(recall)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Or we can use direct library avaiable to get accuracy of the prediction.\nfrom sklearn.metrics import accuracy_score\n\naccuracyscore=accuracy_score(y_test,y_pred)\naccuracyscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#apply LogisticRegression classfier\nfrom sklearn.linear_model import LogisticRegression\nlg = LogisticRegression(class_weight='balanced').fit(X_train, y_train)\nprint (lg.coef_)\nprint('training set score obtained Logistic Regression: {:.2f}'.format(lg.score(X_train, y_train)))\nprint('test set score obtained Logistic Regression: {:.2f}'.format(lg.score(X_test, y_test)))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = lg.predict(X_test)\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\n\nlogit_roc_auc = roc_auc_score(y_test, lg.predict_proba(X_test)[: ,1])\nfpr, tpr, thresholds = roc_curve(y_test, lg.predict_proba(X_test)[:,1])\n# print(thresholds)\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}