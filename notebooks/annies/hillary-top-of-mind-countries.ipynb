{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n# The data comes both as CSV files and a SQLite database\n\n\n### reference site\n#http://cpsievert.github.io/LDAvis/reviews/reviews.html\n\nlibrary(readr)\nlibrary(tm)\nlibrary(SnowballC)\nlibrary(Matrix)\nlibrary(lda)\nlibrary(LDAvis)\nlibrary(servr)\n\n# read data\nemails <- read_csv(\"../input/Emails.csv\")\n\ntxt = paste(emails$ExtractedSubject,emails$ExtractedBodyText);\n\n\n# pre-processing\ntxt <- gsub(\"'\", \"\", txt)  # remove apostrophes\ntxt <- gsub(\"[[:punct:]]\", \" \", txt)  # replace punctuation with space\ntxt <- gsub(\"[[:cntrl:]]\", \" \", txt)  # replace control characters with space\ntxt <- gsub(\"[0-9]\", \" \", txt)        # remove digits as only noise\ntxt <- gsub(\"\\\\b\\\\w{1,3}\\\\b\", \" \", txt) # remove words shorter than 3 characters \ntxt <- gsub(\"^[[:space:]]+\", \"\", txt) # remove whitespace at beginning of documents\ntxt <- gsub(\"[[:space:]]+$\", \"\", txt) # remove whitespace at end of documents\n\n\ntxt <- tolower(txt)  # force to lowercase\n\n# tokenize on space and output as a list:\ndoc.list <- strsplit(txt, \"[[:space:]]+\")\n\n# compute the table of terms:\nterm.table <- table(unlist(doc.list))\nterm.table <- sort(term.table, decreasing = TRUE)\n\n# stop words\nstop_words = c('a','about','above','across','after','again','against','all','almost','alone','along','already','also','although','always','among','an','and','another','any','anybody','anyone','anything','anywhere','are','area','areas','around','as','ask','asked','asking','asks','at','away','b','back','backed','backing','backs','be','became','because','become','becomes','been','before','began','behind','being','beings','best','better','between','big','both','but','by','c','came','can','cannot','case','cases','certain','certainly','clear','clearly','come','could','d','did','differ','different','differently','do','does','done','down','down','downed','downing','downs','during','e','each','early','either','end','ended','ending','ends','enough','even','evenly','ever','every','everybody','everyone','everything','everywhere','f','face','faces','fact','facts','far','felt','few','find','finds','first','for','four','from','full','fully','further','furthered','furthering','furthers','g','gave','general','generally','get','gets','give','given','gives','go','going','good','goods','got','great','greater','greatest','group','grouped','grouping','groups','h','had','has','have','having','he','her','here','herself','high','high','high','higher','highest','him','himself','his','how','however','i','if','important','in','interest','interested','interesting','interests','into','is','it','its','itself','j','just','k','keep','keeps','kind','knew','know','known','knows','l','large','largely','last','later','latest','least','less','let','lets','like','likely','long','longer','longest','m','made','make','making','man','many','may','me','member','members','men','might','more','most','mostly','mr','mrs','much','must','my','myself','n','necessary','need','needed','needing','needs','never','new','new','newer','newest','next','no','nobody','non','noone','not','nothing','now','nowhere','number','numbers','o','of','off','often','old','older','oldest','on','once','one','only','open','opened','opening','opens','or','order','ordered','ordering','orders','other','others','our','out','over','p','part','parted','parting','parts','per','perhaps','place','places','point','pointed','pointing','points','possible','present','presented','presenting','presents','problem','problems','put','puts','q','quite','r','rather','really','right','right','room','rooms','s','said','same','saw','say','says','second','seconds','see','seem','seemed','seeming','seems','sees','several','shall','she','should','show','showed','showing','shows','side','sides','since','small','smaller','smallest','so','some','somebody','someone','something','somewhere','state','states','still','still','such','sure','t','take','taken','than','that','the','their','them','then','there','therefore','these','they','thing','things','think','thinks','this','those','though','thought','thoughts','three','through','thus','to','today','together','too','took','toward','turn','turned','turning','turns','two','u','under','until','up','upon','us','use','used','uses','v','very','w','want','wanted','wanting','wants','was','way','ways','we','well','wells','went','were','what','when','where','whether','which','while','who','whole','whose','why','will','with','within','without','work','worked','working','works','would','x','y','year','years','yet','you','young','younger','youngest','your','yours','z')\n\n# remove terms that are stop words or occur fewer than 5 times:\ndel <- names(term.table) %in% stop_words | term.table < 5\nterm.table <- term.table[!del]\nvocab <- names(term.table)\n\n# now put the documents into the format required by the lda package:\nget.terms <- function(x) {\n    index <- match(x, vocab)\n    index <- index[!is.na(index)]\n    rbind(as.integer(index - 1), as.integer(rep(1, length(index))))\n}\ndocuments <- lapply(doc.list, get.terms)\n\n\n\n# Compute some statistics related to the data set:\nD <- length(documents)\nW <- length(vocab)  \ndoc.length <- sapply(documents, function(x) sum(x[2, ])) \nN <- sum(doc.length)  \nterm.frequency <- as.integer(term.table)\n\n# MCMC and model tuning parameters:\nK <- 20\nG <- 1000\nalpha <- 0.02\neta <- 0.02\n\n# Fit the model:\nset.seed(357)\nt1 <- Sys.time()\nfit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab, \n                                   num.iterations = G, alpha = alpha, \n                                   eta = eta, initial = NULL, burnin = 0,\n                                   compute.log.likelihood = TRUE)\nt2 <- Sys.time()\nt2 - t1  \n\n\n### Visualizing the fitted model with LDAvis\ntheta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))\nphi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))\n\nresults <- list(phi = phi,\n                theta = theta,\n                doc.length = doc.length,\n                vocab = vocab,\n                term.frequency = term.frequency)\n\n\n# create the JSON object to feed the visualization:\njson <- createJSON(phi = results$phi, \n                   theta = results$theta, \n                   doc.length = results$doc.length, \n                   vocab = results$vocab, \n                   term.frequency = results$term.frequency)\n\nserVis(json, out.dir = './', open.browser = FALSE)\nsystem(\"mv index.html results.html\")\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}