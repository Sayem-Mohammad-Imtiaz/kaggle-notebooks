{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introdução\n\nFeature engineering é o processo no qual é possível transformar, extrair e criar novas features a partir dos dados disponíveis, com o objetivo de melhorar o desempenho de algoritmos de aprendizado de máquina. Isso faz com que seja essencial conhecer essas técnicas, para aqueles que pretendem participar em competições de ciência de dados.\n\nComo será mostrado nesse notebook, esse procedimento pode aumentar significamente a precisão até mesmo dos modelos mais simples. O objetivo desse projeto será gerar um modelo para predizer a temperatura máxima em um dia qualquer no futuro. \n\nPorém, só será utilizada uma feature fornecida nos dados: a data na qual a temperatura foi registrada. Além disso, o modelo utilizado será uma regressão linear simples ou com regularização. A data foi escolhida, pois muitas competições utilizam séries temporais, logo as técnicas abordadas nesse material poderão ser úteis em uma variedade de projetos."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"## 0 - Importando os módulos necessários"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1 - Examinando os dados\nNesse notebook, será utilizado o dataset de clima da Austrália, que possui 10 anos de observações diárias do clima em diversas regiões do país.\n\nLink do dataset: https://www.kaggle.com/jsphyg/weather-dataset-rattle-package"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lendo os dados\ndf = pd.read_csv('../input/weather-dataset-rattle-package/weatherAUS.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Informações básicas sobre o conjunto de dados\nprint('Formato do DataFrame: ', df.shape)\nprint('Número de registros: ', df.shape[0], '\\n')\nprint('Localizações: ', df['Location'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Informações sobre valores nulos e tipos de dados\nprint(df.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para simplificar a análise, vamos utilizar apenas a coluna que representa a temperatura máxima (MaxTemp) em Sydney, realizando regressões tentar capturar as tendências dessa feature em relação ao tempo."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convertendo 'Date' para DateTime\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Selecionando Sydney\ndf_sidney = df[df['Location'] == 'Sydney']\n\n# Definindo a data como index e selecionando apenas a coluna MaxTemp\n# Note que [[\"MaxTemp\"]] foi usado para obter um DataFrame. Se [\"MaxTemp\"]\n# fosse utilizado, a operacao retornaria um objeto Series do pandas.\ndf_sidney = df_sidney.set_index('Date')[[\"MaxTemp\"]].dropna().sort_index()\n\nprint(df_sidney.head(), '\\n')\nprint(type(df_sidney))\nprint(df_sidney.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sidney.plot(figsize=(20, 10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nota-se que a temperatura possui uma relação forte com a data. Portanto, vamos extrair features a partir da data para tentar prever a temperatura em datas futuras.\n\nAntes disso, precisamos separar um conjunto de validação, como estamos tentando prever o clima em datas futuras, não podemos extrair observações em datas aleatórias.\nPara obter um conjunto de validação robusto, vamos treinar o model com observações até o final de 2015. Os dados a partir de 2016 serão utilizados para validação."},{"metadata":{},"cell_type":"markdown","source":"## 2 - Tratando os dados e preparando funções"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separando treino e validação\ntrain_df = df_sidney[:'2015-12-31']\nvalid_df = df_sidney['2016-01-01':]\n\nfig, ax = plt.subplots(figsize=(20, 10))\n\nax.plot(train_df.index, train_df['MaxTemp'], color='b', label='Treino')\nax.plot(valid_df.index, valid_df['MaxTemp'], color='r', label='Validação')\nax.legend()\nax.set_title('Divisão entre treino e validação')\nax.set_xlabel('Data')\nax.set_ylabel('Temperatura Máxima (MaxTemp)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Resetando os índices para extrair features da data\ntrain_df.reset_index(inplace=True)\nvalid_df.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mostrar_predicoes(train_df_features, valid_df_features, model = None):\n    \"\"\"\n    Recebe os DataFrames de treino e validacao, treina o modelo,\n    e gera uma visualizacao grafica das predicoes para esses conjuntos de dados.\n    \"\"\"\n    if model == None:\n        # Instanciando a regressão linear\n        model = LinearRegression()\n    \n    # Obtendo as features e alvos a partir dos dados\n    train_target = train_df_features['MaxTemp'].values\n    train_data = train_df_features.drop(['MaxTemp', 'Date'], axis=1).values\n    \n    valid_target = valid_df_features['MaxTemp'].values\n    valid_data = valid_df_features.drop(['MaxTemp', 'Date'], axis=1).values\n    \n    # Treinando o modelo\n    model.fit(train_data, train_target)\n    \n    # Computando as predições e erro de validação\n    predictions_train = model.predict(train_data)\n    predictions_valid = model.predict(valid_data)\n    validation_mse = mean_squared_error(valid_target, predictions_valid)\n    \n    fig, ax = plt.subplots(figsize=(20, 10))\n    ax.plot(train_df['Date'], train_df['MaxTemp'], color='b', marker='.', linestyle='', markersize=2, alpha=0.4)\n    ax.plot(valid_df['Date'], valid_df['MaxTemp'], color='r', marker='.', linestyle='', markersize=2, alpha=0.4)\n    \n    ax.plot(train_df['Date'], predictions_train, color='b', label='Treino')\n    ax.plot(valid_df['Date'], predictions_valid, color='r', label='Validação')\n    ax.legend()\n    \n    ax.set_title('Predições de temperatura em função do tempo')\n    \n    plt.show()\n    \n    print(\"MSE (Erro quadrático médio): \", validation_mse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3 - Extraindo features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copiando os conjuntos de treino e validação\ntrain_1 = train_df.copy()\nvalid_1 = valid_df.copy()\n\n# Extraindo o mês\ntrain_1['mes'] = train_1['Date'].dt.month\nvalid_1['mes'] = valid_1['Date'].dt.month\n    \nmostrar_predicoes(train_1, valid_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gráficos semelhantes ao que foi criado acima serão utilizados nesse notebook para demonstrar como as predições se ajustam às features fornecidas.\nNele, os pontos representam os dados reais do clima de Sydney, enquanto as linhas contínuas representam as predições obtidas pela regressão linear.\n\nNessa primeira visualização, notamos que o gráfico obtido está muito distante dos dados reais, afinal, fornecemos apenas uma feature (mês) para um modelo de regressão linear simples. Para tentar melhorar isso, vamos extrair novas informações da data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copiando dos conjuntos anteriores\ntrain_2 = train_1.copy()\nvalid_2 = valid_1.copy()\n\n# Extraindo o dia\ntrain_2['dia'] = train_2['Date'].dt.day\nvalid_2['dia'] = valid_2['Date'].dt.day\n\n# Extraindo o ano\ntrain_2['ano'] = train_2['Date'].dt.day\nvalid_2['ano'] = valid_2['Date'].dt.day\n\nmostrar_predicoes(train_2, valid_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Percebe-se que o modelo ficou mais complexo, mas ainda não obtemos melhoria na performance do modelo (na verdade o erro aumentou). Logo, temos que fazer algum tratamento adicional nesses dados. Uma opção é aplicar one-hot encoding nas features, mesmo com elas sendo numéricas."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copiando dos conjuntos só com o mês\ntrain_3 = train_1.copy()\nvalid_3 = valid_1.copy()\n\n# Aplicando one-hot encoding\ntrain_3 = pd.get_dummies(train_3, columns=['mes'])\nvalid_3 = pd.get_dummies(valid_3, columns=['mes'])\ntrain_3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predições usando apenas o mês\nmostrar_predicoes(train_3, valid_3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nesse caso, já foi possível observar uma melhoria significante no desempenho do modelo. Isso acontece, porque ele pode tratar cada mês separadamente durante a regressão linear. Dessa forma, cada mês está devolvendo um valor diferente, enquanto o modelo anterior fazia com que a predição fosse o produto do número do mês por um valor somado a uma constante (linear).\n\n- Sem one-hot encoding: MaxTemp = c * mês + constante.\n- Com one-hot encoding: MaxTemp = c1 * é_janeiro + c2 * é_fevereiro + c3 * é_março + ... + constante"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copiando dos conjuntos anteriores\ntrain_3 = train_2.copy()\nvalid_3 = valid_2.copy()\n\n# Aplicando one-hot encoding\ntrain_3 = pd.get_dummies(train_3, columns=['dia', 'mes', 'ano'])\nvalid_3 = pd.get_dummies(valid_3, columns=['dia', 'mes', 'ano'])\ntrain_3.head()\n\n# Mostrando as predicoes usando dia, mês e ano\nmostrar_predicoes(train_3, valid_3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Novamente, o modelo se tornou mais complexo, mas o erro aumentou. Isso foi um caso de overfitting, que pode ser resolvido selecionando melhor as features utilizadas ou usando outros métodos, como regularização."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Realizando uma regressão linear com regularização (Regressão de Ridge)\nridge_model = Ridge()\nmostrar_predicoes(train_3, valid_3, ridge_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No modelo acima, usamos a regressão de Ridge, que é basicamente uma regressão linear simples com regularização L2. Observamos uma redução na complexidade do modelo, reduzindo o ruído causado pelos dias, com isso também obtemos um erro menor que as tentativas anteriores.\n\nLeia mais sobre isso nesse link: https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/"},{"metadata":{},"cell_type":"markdown","source":"## 4 - Features polinomiais"},{"metadata":{},"cell_type":"markdown","source":"Uma desvantagem da regressão linear é a fato dela só extrair relações lineares das features fornecidas, mas isso pode ser contornado adicionando features polinomiais ao modelo (transformando ele em uma regressão polinomial), com isso, conseguimos extrair relações mais complexas dos mesmos dados. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_4 = train_2.copy()\nvalid_4 = valid_2.copy()\n\n# Instanciando o gerador de features do sklearn\npoly = PolynomialFeatures(degree=2, include_bias=False)\n\n# Adicionando as features geradas conjunto de treino e validação\ntrain_4 = train_4.join(pd.DataFrame(poly.fit_transform(train_4[['dia', 'mes', 'ano']])))\nvalid_4 = valid_4.join(pd.DataFrame(poly.fit_transform(valid_4[['dia', 'mes', 'ano']])))\n\n# Removendo colunas que ficaram duplicadas\ntrain_4.drop(['dia', 'mes', 'ano'], axis=1, inplace=True)\nvalid_4.drop(['dia', 'mes', 'ano'], axis=1, inplace=True)\n\n# Observe que o sklearn opera utilizando o numpy, logo, os valores gerados pelo fit_transform\n# serão arrays do numpy. Para contornar isso, é possível usar pd.DataFrame() pra gerar um novo\n# DataFrame a partir desse array, mas ele não possuirá nomes nas colunas, apenas números sequenciais\ntrain_4.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mostrando as predições geradas pela regressão polinomial\nmostrar_predicoes(train_4, valid_4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Uma observação importante é que não é faz sentido utilizar one-hot encoding seguido de geração de features polinomiais, pois todos os valores serão 0 ou 1, e esses números elevados a qualquer potências são eles mesmos.\n\nOutra questão a prestar atenção na regressão polinomial é o grau da mesma, o gráfico acima foi feito com uma de grau 2. Podemos usar graus maiores para gerar modelos mais complexos, mas números muito altos também podem causar overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_4 = train_2.copy()\nvalid_4 = valid_2.copy()\n\n# Instanciando o gerador de features do sklearn\npoly = PolynomialFeatures(degree=4, include_bias=False)\n\n# Adicionando as features geradas conjunto de treino e validacao\ntrain_4 = train_4.join(pd.DataFrame(poly.fit_transform(train_4[['dia', 'mes', 'ano']])))\nvalid_4 = valid_4.join(pd.DataFrame(poly.fit_transform(valid_4[['dia', 'mes', 'ano']])))\n\ntrain_4.drop(['dia', 'mes', 'ano'], axis=1, inplace=True)\nvalid_4.drop(['dia', 'mes', 'ano'], axis=1, inplace=True)\n\n# Mostrando as predições geradas pela regressão polinomial\nmostrar_predicoes(train_4, valid_4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Utilizando features polinomiais de grau 4 obtemos o menor erro até agora, mas ainda podemos tentar aplicar regularização, por exemplo com Ridge e Lasso."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Regressão de Ridge com features polinomiais\nridge_model = Ridge()\nmostrar_predicoes(train_4, valid_4, ridge_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Regressão de Lasso com features polinomiais\nlasso_model = Lasso()\nmostrar_predicoes(train_4, valid_4, lasso_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Com isso, observamos que nem sempre a regularização proporciona um desempenho melhor para o modelo."},{"metadata":{},"cell_type":"markdown","source":"## Conclusão\n\nA partir dessa análise, é claro como extração de features e seu tratamento adequado melhoram consideravelmente o desempenho de um modelo. Também vale a pena destacar que modelos diferentes funcionam melhor com outros tipos de features (por exemplo modelos baseados em árvores).\n\nEm casos de dados com séries temporais, extrair informações da data fornece ao modelo um comportamento em relação a periodicidade, como foi o caso desse notebook, afinal a temperatura segue um padrão com o decorrer dos meses. O mesmo poderia ser aplicado, por exemplo, às vendas de uma loja. Nesse caso, existiriam outras informações úteis, como saber se o dia é um feriado.\n\nÉ importante ressaltar que tudo que foi apresentado é apenas uma parte pequena do campo da engenharia de features. Cada tipo de dado possui múltiplos tratamentos e codificações possíveis, com resultados diferentes. Porém, espero que esse material tenha te ajudado a compreender mais sobre esse campo tão amplo."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}