{"cells":[{"metadata":{"id":"XcdHI4JC_eKG"},"cell_type":"markdown","source":"![alt text](https://cdn.discordapp.com/attachments/693138332166914077/698275109416206486/bootcamp.png)"},{"metadata":{"id":"riairREzDZsQ"},"cell_type":"markdown","source":"# GENETIC VARIANT CLASSIFICATION"},{"metadata":{"id":"OfOml2QBDgKR"},"cell_type":"markdown","source":"Clinic Variant is a public resource containing annotations about human genetic variants. These variants are (usually manually) classified by clinical laboratories on a categorical spectrum ranging from **benign, likely benign, uncertain significance, likely pathogenic, and pathogenic.** Variants that have conflicting classifications (from laboratory to laboratory) can cause confusion when clinicians or researchers try to interpret whether the variant has an impact on the disease of a given patient."},{"metadata":{"id":"iiaoBN6yD49m"},"cell_type":"markdown","source":"The objective is to predict whether a Clinic Variant  will have conflicting classifications. This is presented here as a binary classification problem, where each record in the dataset is a genetic variant.\n"},{"metadata":{"id":"f93FD_BcCStq"},"cell_type":"markdown","source":"![alt text](https://cdn.discordapp.com/attachments/693138332166914077/698141745719935036/concordant_variant.png)"},{"metadata":{"id":"no9fNOyuD2Uj"},"cell_type":"markdown","source":"![alt text](https://cdn.discordapp.com/attachments/693138332166914077/698141746902597662/conflicting_variant.png)"},{"metadata":{"id":"9G0g_J30HnHK"},"cell_type":"markdown","source":"# **Distribution of Target**"},{"metadata":{"id":"kMCcSwiGELxp"},"cell_type":"markdown","source":"![alt text](https://cdn.discordapp.com/attachments/693138332166914077/698141755178221629/target_histogram.png)"},{"metadata":{"id":"u7nWOFgD1rdP"},"cell_type":"markdown","source":"**The CLASS distribution is skewed a bit to the 0 class, meaning there are fewer variants with conflicting submissions.**"},{"metadata":{"id":"PbdsH7-j0iOI"},"cell_type":"markdown","source":"Since our target variable is binary (categorical), we will use both old and new models. These can be listed as follows:\n\n\n\n*   Logistic Regression\n*   XGBoost Classifier\n*   KNeighbors Classifier\n*   Decision Tree Classifier\n*   LigthGBM Classifier\n*   Gradient Boosting Classifier\n*   Hist Gradient Boosting Classifier\n\n"},{"metadata":{"id":"_vZD25VZIH1Z"},"cell_type":"markdown","source":"# **Models Histogram**"},{"metadata":{"id":"EFlitntpEd4a"},"cell_type":"markdown","source":"![alt text](https://cdn.discordapp.com/attachments/693138332166914077/698263762855002164/Ekran_Resmi_2020-04-10_23.08.13.png)"},{"metadata":{"id":"cVTFW6Wy5wBD","trusted":true,"collapsed":true},"cell_type":"code","source":"#Import Libraries\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport missingno as msno\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import base\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPRegressor, MLPClassifier\n!pip install category_encoders\n!pip install rfpimp\nfrom category_encoders import BinaryEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,roc_auc_score,classification_report,confusion_matrix\nfrom IPython.display import Image\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"id":"gjtFsLr4TNXE","trusted":true},"cell_type":"code","source":"#Read The Dataset\n\nurl = \"https://drive.google.com/uc?id=1TiEhIjpjB6KUxvqqgVeps9SkKTff1hvY\"\n\ndata = pd.read_csv(url)","execution_count":null,"outputs":[]},{"metadata":{"id":"pxieQJBcb5S9","outputId":"ecc2e692-cab8-4de1-a1b0-cb11cedb998c","trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"cVd6BCvnEiNE","outputId":"be5cf0f3-f7eb-4a65-b508-1a340dace303","trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"2iDtfRJ1ccTh","outputId":"f4821ea9-faa7-416c-f799-98e5e52e4fb8","trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"0jb5TEIHeBi9","trusted":true},"cell_type":"code","source":"#Inserting KEY\n\n#In this data, we define a column with a unique value of 65188 as a key.\n\ndata.insert(0,\"KEY\",data.CLNHGVS)","execution_count":null,"outputs":[]},{"metadata":{"id":"sF-kM038AKjX","outputId":"60bf6d90-9edf-4078-c382-77c32989e56c","trusted":true},"cell_type":"code","source":"data['KEY'].nunique()==len(data)","execution_count":null,"outputs":[]},{"metadata":{"id":"0nfaypsvsGmg","outputId":"0a304e06-9247-4c9a-a224-ce035b4a6540","trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"IIXJW0y7IlNC"},"cell_type":"markdown","source":"## **Histogram of Binary Target Categories**"},{"metadata":{"id":"fAglh1IzfPed","trusted":true},"cell_type":"code","source":"%matplotlib inline\n# Histogram of the target categories\ndef histogram(df,feature):\n    ncount = len(df)\n    ax = sns.countplot(x = feature, data=df ,palette=\"hls\")\n    sns.set(font_scale=1)\n    ax.set_xlabel('Target Segments')\n    plt.xticks(rotation=90)\n    ax.set_ylabel('Number of Observations')\n    fig = plt.gcf()\n    fig.set_size_inches(12,5)\n    # Make twin axis\n    ax2=ax.twinx()\n    # Switch so count axis is on right, frequency on left\n    ax2.yaxis.tick_left()\n    ax.yaxis.tick_right()\n    # Also switch the labels over\n    ax.yaxis.set_label_position('right')\n    ax2.yaxis.set_label_position('left')\n    ax2.set_ylabel('Frequency [%]')\n    for p in ax.patches:\n        x=p.get_bbox().get_points()[:,0]\n        y=p.get_bbox().get_points()[1,1]\n        ax.annotate('{:.2f}%'.format(100.*y/ncount), (x.mean(), y), \n                ha='center', va='bottom') # set the alignment of the text\n    # Use a LinearLocator to ensure the correct number of ticks\n    ax.yaxis.set_major_locator(ticker.LinearLocator(11))\n    # Fix the frequency range to 0-100\n    ax2.set_ylim(0,100)\n    ax.set_ylim(0,ncount)\n    # And use a MultipleLocator to ensure a tick spacing of 10\n    ax2.yaxis.set_major_locator(ticker.MultipleLocator(10))\n    # Need to turn the grid on ax2 off, otherwise the gridlines end up on top of the bars\n    ax2.grid(None)\n    plt.title('Histogram of Binary Target Categories', fontsize=20, y=1.08)\n    plt.show()\n    plt.savefig('target_histogram.png')\n    del ncount, x, y","execution_count":null,"outputs":[]},{"metadata":{"id":"XwdnTggpgHWq","outputId":"a685b281-61a1-4404-c5bd-3cb15fb740ca","trusted":true},"cell_type":"code","source":"histogram(data,\"CLASS\")","execution_count":null,"outputs":[]},{"metadata":{"id":"t_HFDlfY3G4_"},"cell_type":"markdown","source":"**The CLASS distribution is skewed a bit to the 0 class, meaning there are fewer variants with conflicting submissions.**\n\n"},{"metadata":{"id":"3hCEHvYbFHP6"},"cell_type":"markdown","source":"### **Information function about missing values**"},{"metadata":{"id":"qvuihyawOEG0","trusted":true},"cell_type":"code","source":"def MissingUniqueStatistics(df):\n  \n  total_entry_list = []\n  total_missing_value_list = []\n  missing_value_ratio_list = []\n  data_type_list = []\n  unique_values_list = []\n  number_of_unique_values_list = []\n  variable_name_list = []\n  \n  for col in df.columns:\n\n    variable_name_list.append(col)\n    missing_value_ratio = round((df[col].isna().sum()/len(df[col])),4)\n    total_entry_list.append(df[col].shape[0] - df[col].isna().sum())\n    total_missing_value_list.append(df[col].isna().sum())\n    missing_value_ratio_list.append(missing_value_ratio)\n    data_type_list.append(df[col].dtype)\n    unique_values_list.append(list(df[col].unique()))\n    number_of_unique_values_list.append(len(df[col].unique()))\n\n  data_info_df = pd.DataFrame({'Variable':variable_name_list,'#_Total_Entry':total_entry_list,\\\n                           '#_Missing_Value':total_missing_value_list,'%_Missing_Value':missing_value_ratio_list,\\\n                           'Data_Type':data_type_list,'Unique_Values':unique_values_list,\\\n                           '#_Uniques_Values':number_of_unique_values_list})\n  \n  return data_info_df.sort_values(by=\"#_Missing_Value\",ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"4tAU0EpsT3ys","outputId":"e4ad52e9-9de2-4d68-fbca-b27452f8cd6d","trusted":true},"cell_type":"code","source":"data_info = MissingUniqueStatistics(data)\ndata_info = data_info.set_index(\"Variable\")\ndata_info","execution_count":null,"outputs":[]},{"metadata":{"id":"maji-N35HcmK"},"cell_type":"markdown","source":"### When we analyze the missing value statistics, we see that our 9 columns are larger than 99%. These columns do not contain information. That's why we drop these columns."},{"metadata":{"id":"0A5ofluyH2pa","trusted":true},"cell_type":"code","source":"drop_list = list(data_info[data_info[\"%_Missing_Value\"] >= 0.99].index)\n\ndata.drop(drop_list,axis = 1,inplace = True) # --> MAIN DF CHANGED","execution_count":null,"outputs":[]},{"metadata":{"id":"1VeNlsBMKwys"},"cell_type":"markdown","source":"### When we continue to analyze the missing data statistics, we see that there are 3 different columns with numerical data, but the data type is an object."},{"metadata":{"id":"4MN5TLgMRPq8","outputId":"c8c1e2cb-ca4c-44de-9999-4533854d1481","trusted":true},"cell_type":"code","source":"data[\"Protein_position\"].unique() , data[\"CDS_position\"].unique()  , data[\"cDNA_position\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"id":"tpXf6SEFRTn8"},"cell_type":"markdown","source":"### We see that some data is separated by \"-\". And these numbers are very close to each other. Based on this information, we can only use the first number."},{"metadata":{"id":"LpSWA1arScz2","trusted":true},"cell_type":"code","source":"def value_correction(df,columns):\n  \n  for col in columns:\n\n    value_correction = pd.DataFrame(df[col][df[col].notnull()].str.split(\"-\").tolist(),columns=[\"X\",\"Y\"])\n    value_correction[\"X\"][value_correction[\"X\"]==\"?\"] = value_correction[\"Y\"]\n    key = df[[col,\"KEY\"]][df[col].notnull()][\"KEY\"]\n\n    counter = 0\n\n    for i in key.index:\n\n      df[col][i] = value_correction[\"X\"][counter]\n      counter += 1\n\n    df[col] = df[col].astype(float)\n  return df","execution_count":null,"outputs":[]},{"metadata":{"id":"FYVAcfNnTGb4","trusted":true},"cell_type":"code","source":"data = value_correction(data,[\"CDS_position\",\"cDNA_position\",\"Protein_position\"]) # --> MAIN DF CHANGED","execution_count":null,"outputs":[]},{"metadata":{"id":"kJ_sW5tuTUqx"},"cell_type":"markdown","source":"# Showing the correlation map of the dataset"},{"metadata":{"id":"wkmZZvm_TTLF","outputId":"45b5fa8f-21d6-4aa6-a2af-b600937119d7","trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(20, 9))\nsns.heatmap(data.corr(), annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"gb-GHKxfTyjY"},"cell_type":"markdown","source":"When we analyze this map, we see that the correlation of cDNA_position, CDS_position, Protein_position columns with each other is 1.And more than 85% of the CDS_position values ​​are filled with 3 times the Protein_position value.Also, the value of cDNA_position is 50 numbers larger than CDS_position.\n\nTherefore, we dropped the cDNA_position and CDS_position columns."},{"metadata":{"id":"K8tuKG7jXXwM","trusted":true},"cell_type":"code","source":"data.drop([\"CDS_position\",\"cDNA_position\"],axis = 1, inplace = True) # --> MAIN DF CHANGED","execution_count":null,"outputs":[]},{"metadata":{"id":"Xb_qZoL3YAkw"},"cell_type":"markdown","source":"When we continue to analyze the table of missing values ​​statistics, we see that we have two columns with fractional numbers but the data type is an object.These are \"EXON\" and \"INTRON\" columns.\n\nWe make the data type of these columns \"float\".\n\n"},{"metadata":{"id":"HXLtOhaYkE72","outputId":"288e6926-1864-444c-e605-3e5171d9c557","trusted":true},"cell_type":"code","source":"data[[\"EXON\",\"INTRON\"]][data[\"INTRON\"].notnull() & data[\"EXON\"].notnull()].head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"HiPy2PBEYxMo","trusted":true},"cell_type":"code","source":"def convert_to_float(df,columns):\n  \n  for col in columns:\n       \n    convert_to_float = pd.DataFrame(df[col][df[col].notnull()].str.split(\"/\").tolist(),columns=[\"Numerator\",\"Denominator\"])\n    convert_to_float = convert_to_float.astype(\"float\")\n    convert_to_float[\"Result\"] = convert_to_float[\"Numerator\"] / convert_to_float[\"Denominator\"]\n    key =df[[col,\"KEY\"]][df[col].notnull()][\"KEY\"]\n\n    counter = 0\n    for i in key.index:\n\n      df[col][i] = convert_to_float[\"Result\"][counter]\n      counter += 1\n    df[col] = df[col].astype(float)\n\n  return df","execution_count":null,"outputs":[]},{"metadata":{"id":"xaCJtvN2ZGVT","trusted":true},"cell_type":"code","source":"data = convert_to_float(data,[\"INTRON\",\"EXON\"]) # --> MAIN DF CHANGED","execution_count":null,"outputs":[]},{"metadata":{"id":"X2K7I_kIavI-","outputId":"6df94f90-e5d8-415c-8e30-0e25c9febe94","trusted":true},"cell_type":"code","source":"MissingUniqueStatistics(data)","execution_count":null,"outputs":[]},{"metadata":{"id":"zfwhKnGoUcJj"},"cell_type":"markdown","source":"# Showing missing values on the bar graphic"},{"metadata":{"id":"n16Ruzy3Asa6","outputId":"fbd51576-8c1d-42b7-eb1e-bc3ed74cb868","trusted":true},"cell_type":"code","source":"msno.bar(data,color='#79ccb3',sort='descending')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"W1DNNwzQKFrP","outputId":"8a9a5757-9cec-40c9-a9fd-8c6ad48a7e54","trusted":true},"cell_type":"code","source":"msno.matrix(data,color=(0.45,0.45,0.64),figsize=(27, 10), width_ratios=(10, 0))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"nF7luujs1juH","outputId":"af3b9c62-9f96-48d2-b96b-1a7fc3a482e8","trusted":true},"cell_type":"code","source":"msno.dendrogram(data);","execution_count":null,"outputs":[]},{"metadata":{"id":"yd7moZF3xH9K","outputId":"2e2360b1-c597-4958-948a-f615f3317483","trusted":true},"cell_type":"code","source":"msno.heatmap(data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"ZcBFb4FwbHmd"},"cell_type":"markdown","source":"We have a heapmap showing the relationship of missing values ​​with each other. On this map, we see that the \"EXON\" and \"INTRON\" columns are inversely related to each other. Therefore, we fill in the missing values ​​of the column \"EXON\" with the values ​​of the column \"INTRON\"."},{"metadata":{"id":"LIdbnTlMb5Tx","outputId":"6b769bc2-1e18-4ccf-d5a0-29a2b7a8f5de","trusted":true},"cell_type":"code","source":"data[\"EXON\"][data[\"EXON\"].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"id":"5HIq-1CCb6-1","outputId":"8b23863f-2e46-4f70-b3b2-1beaf8579622","trusted":true},"cell_type":"code","source":"data[\"INTRON\"][data[\"INTRON\"].notnull()]","execution_count":null,"outputs":[]},{"metadata":{"id":"gx3Dui5nb9gb","trusted":true},"cell_type":"code","source":"data[\"EXON\"][data[\"EXON\"].isnull()] = data[\"INTRON\"][data[\"INTRON\"].notnull()] # --> MAIN DF CHANGED","execution_count":null,"outputs":[]},{"metadata":{"id":"VO8bYr53cI0E"},"cell_type":"markdown","source":"Then we drop the \"INTRON\" column."},{"metadata":{"id":"2OWpkpVBcKS0","trusted":true},"cell_type":"code","source":"data.drop([\"INTRON\"], axis = 1, inplace = True) # --> MAIN DF CHANGED","execution_count":null,"outputs":[]},{"metadata":{"id":"o1SlesHK21b3","outputId":"133e9ac2-5e8f-4274-9753-a716210b11ba","trusted":true},"cell_type":"code","source":"data_info = MissingUniqueStatistics(data)\ndata_info = data_info.set_index(\"Variable\")\ndata_info","execution_count":null,"outputs":[]},{"metadata":{"id":"LYdXkwktcaUf","outputId":"264f4971-e067-4ffa-ea56-6e8c19ea7c6d","trusted":true},"cell_type":"code","source":"#Creating new column for add of variable type\ndata_info[\"Variable_Type\"] = [\"Ordinal\",\"Ordinal\",\"Nominal\",\"Nominal\",\"Nominal\",\"Nominal\",\"Nominal\",\"Continuous\",\"Nominal\",\n                              \"Continuous\",\"Continuous\",\"Continuous\",\"Continuous\",\"Continuous\",\"Nominal\",\"Nominal\",\"Nominal\",\n                              \"Nominal\",\"Nominal\",\"Nominal\",\"Nominal\",\"Ordinal\",\"Nominal\",\"Nominal\",\"Nominal\",\"Nominal\",\"Nominal\",\"Nominal\",\n                              \"Continuous\",\"Continuous\",\"Continuous\",\"Nominal\",\"Nominal\",\"Cardinal\",\"Ordinal\"]\ndata_info","execution_count":null,"outputs":[]},{"metadata":{"id":"HV4Zr5sSOQGL"},"cell_type":"markdown","source":"## **Data Controlling**"},{"metadata":{"id":"qjH90I4lmFzT","outputId":"477e3e25-caf1-4c08-c213-715ab172c638","trusted":true},"cell_type":"code","source":"# 1- Row Uniqueness (Drop Duplicates) \nlen(data_info.index) == data_info.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"5jKg_4dBmQ38","outputId":"37650ae1-33f4-47da-c631-1b54a64d9f9e","trusted":true},"cell_type":"code","source":"# 2- Column Uniqueness (Drop Singletons)\nnumerical_columns = list(data_info.loc[(data_info.loc[:,\"Variable_Type\"]==\"Cardinal\") |\n                                       (data_info.loc[:,\"Variable_Type\"]==\"Continuous\")].index)\nlen(numerical_columns), numerical_columns","execution_count":null,"outputs":[]},{"metadata":{"id":"HIBvrds6mmay","outputId":"9821d45b-321f-4e7e-bfbc-4507ef191640","trusted":true},"cell_type":"code","source":"categorical_columns = list(data_info.loc[(data_info.loc[:,\"Variable_Type\"]==\"Nominal\") |\n                                       (data_info.loc[:,\"Variable_Type\"]==\"Ordinal\")].index)\nlen(categorical_columns), categorical_columns","execution_count":null,"outputs":[]},{"metadata":{"id":"W-YHWcJ7mpsX","trusted":true},"cell_type":"code","source":"def ZeroVarianceFinder(df, numerical_columns):\n  \n  import pandas as pd\n  import numpy as np\n\n  zerovariance_numerical_features=[]\n  for col in numerical_columns:\n      try:\n          if pd.DataFrame(df[col]).describe().loc['std'][0] == 0.00 or \\\n          np.isnan(pd.DataFrame(df[col]).describe().loc['std'][0]):\n              zerovariance_numerical_features.append(col)\n      except:\n          print(\"Error:\",col)\n  return zerovariance_numerical_features","execution_count":null,"outputs":[]},{"metadata":{"id":"YRFy2OubGPFV","outputId":"24dd0e83-a6e7-4844-ea83-bbe501f367df","trusted":true},"cell_type":"code","source":"zerovariance_numerical_features = ZeroVarianceFinder(data,numerical_columns)\nzerovariance_numerical_features","execution_count":null,"outputs":[]},{"metadata":{"id":"Uq9kxh7YGS9-","outputId":"43a2fca0-6178-4c81-8d22-20f89e53b36d","trusted":true},"cell_type":"code","source":"singleton_categorical_features=[]\nfor col in categorical_columns:\n    if len(data[col].unique()) <= 1:\n        singleton_categorical_features.append(col)\nlen(singleton_categorical_features), singleton_categorical_features","execution_count":null,"outputs":[]},{"metadata":{"id":"C-AVoUIQ4bEm"},"cell_type":"markdown","source":"# **Train/Test Split**"},{"metadata":{"id":"nwv1joLfaMxe","trusted":true},"cell_type":"code","source":"y = data.loc[:,\"CLASS\"]","execution_count":null,"outputs":[]},{"metadata":{"id":"ztn8KbJBaR5t","trusted":true},"cell_type":"code","source":"x1 = data.iloc[:,1:15]\nx2 = data.iloc[:,16:]\nx = pd.concat([x1,x2],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"h1ijKkMJaWRu","outputId":"f9cec39e-d33b-4436-a4a6-2ea6c9ad5ae7","trusted":true},"cell_type":"code","source":"X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.33,random_state=42)\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"AGNUXqVW24qQ"},"cell_type":"markdown","source":"#### **Histogram of Binary Target Categories for Train**"},{"metadata":{"id":"521ds9z628WI","outputId":"d7859708-0078-4e5f-b919-43a07026924a","trusted":true},"cell_type":"code","source":"histogram(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"xWvQG1Qy3Vyq"},"cell_type":"markdown","source":"After the separation of the train and test data we made, we see that the target distribution does not deteriorate."},{"metadata":{"id":"-MszY0-X3gIj"},"cell_type":"markdown","source":"#### **Histogram of Binary Target Categories for Test**\n"},{"metadata":{"id":"8OqjE7oZ3jgu","outputId":"0a24e3e6-d096-472e-ce9c-65ee3abc365c","trusted":true},"cell_type":"code","source":"histogram(X_test,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"RWUCstER4YpJ"},"cell_type":"markdown","source":"## **Outlier Detection**"},{"metadata":{"id":"ZW-ECqk76To7"},"cell_type":"markdown","source":"### Finding sparse columns"},{"metadata":{"id":"gjd7G6e84fOB","outputId":"3c815e1b-b8f0-4c34-d8d8-1eaa3a65221d","trusted":true},"cell_type":"code","source":"df_train = data.copy()\nnumerical_columns_remains = numerical_columns\n        \nsparse_columns = []\nfor col in numerical_columns_remains:\n    if (df_train[col].quantile(0.01)==df_train[col].quantile(0.25)==df_train[col].mode()[0]):\n        sparse_columns.append(col)\n\nsparse_columns_2 = []\nfor col in numerical_columns_remains:\n    if (df_train[col].quantile(0.01)==df_train[col].quantile(0.25)):\n        sparse_columns_2.append(col)\n\nlen(numerical_columns_remains), len(sparse_columns), len(sparse_columns_2)","execution_count":null,"outputs":[]},{"metadata":{"id":"wqv5U4Ii3wO7"},"cell_type":"markdown","source":"## **Visualization Before Cleaning Outlier**"},{"metadata":{"id":"3oaBDRD83_T1","outputId":"1e9f1309-6997-492d-983a-c2b68091d7d7","trusted":true},"cell_type":"code","source":"from pylab import rcParams\n\ndef box_plot(x,y,data):\n\n  rcParams['figure.figsize'] = 20, 10\n  fig, axs = plt.subplots(2,5)\n  plt.tight_layout()\n  fig.subplots_adjust(top=0.7)\n  sns.set(style=\"ticks\", palette=\"rainbow\")\n\n  j = 0\n  k = 0\n  for i in range(len(y)):\n    sns.boxplot(x=x, y=y[i], data=data,ax=axs[j,k])\n    if(k==4):\n      k = 0\n      j += 1\n    else:\n      k += 1\n\n  plt.tight_layout()\n  plt.show()\n\nbox_plot(Y_train,numerical_columns,X_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"7VXxESG8_E5r"},"cell_type":"markdown","source":"## **Cleaning Outliers for Train Dataset**\n\n"},{"metadata":{"id":"97aD8CEG5K9C","trusted":true},"cell_type":"code","source":"\"\"\"\nAlgorithm 'HER(Hard-Edges Method)' applies induction to the elements of a value line which are:\n\n    - lower than the 1th quantile to that quantile and\n    - upper than the 99th quantile to that quantile.\n    \nMain aim is to diminish negative effects of outlier values on analytical operations being performed.\n\"\"\"\n\ndef HardEdgeReduction(df,numerical_columns,sparse_columns,upper_quantile=0.99,lower_quantile=0.01):\n    \n    import pandas as pd\n\n    import psutil, os, gc, time\n    print(\"HardEdgeReduction process has began:\\n\")\n    proc = psutil.Process(os.getpid())\n    gc.collect()\n    mem_0 = proc.memory_info().rss\n    start_time = time.time()\n    \n    # Do outlier cleaning in only one loop\n    epsilon = 0.0001 # for zero divisions\n\n    # Define boundaries that we will use for Reduction operation\n\n    df_outlier_cleaned = df.copy()\n\n\n    print(\"Detected outliers will be replaced with edged quantiles/percentiles: 1% and 99%!\\n\")\n    print(\"Total number of rows is: %s\\n\"%df_outlier_cleaned.shape[0])\n\n    outlier_boundries_dict={}\n\n    for col in numerical_columns:\n\n        if col in sparse_columns:\n\n            # First ignore the 'sparse' data points:\n            nonsparse_data = pd.DataFrame(df_outlier_cleaned[df_outlier_cleaned[col] !=\\\n                                                             df_outlier_cleaned[col].mode()[0]][col]) \n            \n            #we used only mode to catch sparse points, since we know/proved it is enough to do that.\n\n            # Find Outlier Thresholds:\n            # Note: All columns are right-skewed\n            # For lower threshold (left-hand-side)\n            if nonsparse_data[col].quantile(lower_quantile) < df_outlier_cleaned[col].mode()[0]: #Unexpected case\n                lower_bound_sparse = nonsparse_data[col].quantile(lower_quantile)\n            else:\n                lower_bound_sparse = df_outlier_cleaned[col].mode()[0]\n            \n            # For upper threshold (right-hand-side)\n            if nonsparse_data[col].quantile(upper_quantile) < df_outlier_cleaned[col].mode()[0]: #Unexpected case\n                upper_bound_sparse = df_outlier_cleaned[col].mode()[0]\n            else:\n                upper_bound_sparse = nonsparse_data[col].quantile(upper_quantile)\n\n            outlier_boundries_dict[col]=(lower_bound_sparse,upper_bound_sparse)\n\n            # Inform user about the cardinality of Outlier existence:\n            number_of_outliers = len(df_outlier_cleaned[(df_outlier_cleaned[col] < lower_bound_sparse) |\\\n                                                        (df_outlier_cleaned[col] > upper_bound_sparse)][col])\n            print(\"Sparse: Outlier number in {} is equal to: \".format(col),round(number_of_outliers/(nonsparse_data.shape[0] -\n                                                                                       nonsparse_data.isnull().sum()),2))\n\n            # Replace Outliers with Edges --> 1% and 99%:\n            if number_of_outliers > 0:\n\n                # Replace 'left-hand-side' outliers with its 1% quantile value\n                df_outlier_cleaned.loc[df_outlier_cleaned[col] < lower_bound_sparse,col] = lower_bound_sparse - epsilon # --> MAIN DF CHANGED\n\n                # Replace 'right-hand-side' outliers with its 99% quantile value\n                df_outlier_cleaned.loc[df_outlier_cleaned[col] > upper_bound_sparse,col] = upper_bound_sparse + epsilon # --> MAIN DF CHANGED\n\n        else:\n            # Find Edges:\n            number_of_outliers = len(df_outlier_cleaned[(df_outlier_cleaned[col] < \\\n                                                         df_outlier_cleaned[col].quantile(lower_quantile))|\\\n                                                        (df_outlier_cleaned[col] > \\\n                                                         df_outlier_cleaned[col].quantile(upper_quantile))]\\\n                                     [col])\n            print(\"Other: Outlier number in {} is equal to: \".format(col),round(number_of_outliers/(df[col].shape[0] -\n                                                                                       df[col].isnull().sum()),2)) \n\n            # Replace 'Standard' outliers:\n            if number_of_outliers > 0:\n                # Replace all outliers with its %99 quartile\n                lower_bound_sparse = df_outlier_cleaned[col].quantile(lower_quantile)\n                df_outlier_cleaned.loc[df_outlier_cleaned[col] < \\\n                                       lower_bound_sparse,col] \\\n                = lower_bound_sparse  - epsilon\n\n                upper_bound_sparse = df_outlier_cleaned[col].quantile(upper_quantile)\n                df_outlier_cleaned.loc[df_outlier_cleaned[col] > \\\n                                       upper_bound_sparse,col] \\\n                = upper_bound_sparse  + epsilon\n\n            outlier_boundries_dict[col]=(lower_bound_sparse,upper_bound_sparse)\n\n\n    print('HardEdgeReduction process has been completed!')\n    print(\"--- in %s minutes ---\" % ((time.time() - start_time)/60))\n\n    return df_outlier_cleaned, outlier_boundries_dict\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"LlRZeZQG5O85","outputId":"d65e23c5-024c-4b4b-9746-5e0651632a81","trusted":true},"cell_type":"code","source":"X_train, outlier_boundries_dict = HardEdgeReduction(X_train,numerical_columns,sparse_columns)","execution_count":null,"outputs":[]},{"metadata":{"id":"M7EWrHJF5QD4","outputId":"79f18795-d0ec-4764-c83d-a93195e64498","trusted":true},"cell_type":"code","source":"outlier_boundries_dict","execution_count":null,"outputs":[]},{"metadata":{"id":"_8GnSl5P_ARm"},"cell_type":"markdown","source":"## **Cleaning Outliers for Test Dataset**"},{"metadata":{"id":"ghSsPF50-5n5","outputId":"a90703ce-aea7-44ad-c578-f897a6a29fc3","trusted":true},"cell_type":"code","source":"# Do outlier cleaning in only one loop\nepsilon = 0.0001 # for zero divisions\n\n# Define boundaries that we will use for Reduction operation\nupper_quantile = 0.99\nlower_quantile = 0.01\n\ndf_test_outlier_cleaned = X_test.copy()\n\nprint(\"Detected outliers will be replaced with edged quantiles/percentiles: 1% and 99%!\\n\")\nprint(\"Total number of rows is: %s\\n\"%df_test_outlier_cleaned.shape[0])\n\nfor col in numerical_columns_remains:\n\n      lower_bound = outlier_boundries_dict[col][0]\n      upper_bound = outlier_boundries_dict[col][1]\n        \n      # Inform user about the cardinality of Outlier existence:\n      number_of_outliers = len(df_test_outlier_cleaned[(df_test_outlier_cleaned[col] < lower_bound) |\\\n                                                        (df_test_outlier_cleaned[col] > upper_bound)][col])\n      print(\"Outlier number in {} is equal to: \".format(col), round(number_of_outliers/\n            (df_test_outlier_cleaned[col].shape[0] - df_test_outlier_cleaned[col].isnull().sum()),2))\n\n      # Replace Outliers with Edges --> 1% and 99%:\n      if number_of_outliers > 0:\n\n          # Replace 'left-hand-side' outliers with its 1% quantile value\n          df_test_outlier_cleaned.loc[df_test_outlier_cleaned[col] < lower_bound,col] = lower_bound  - epsilon # --> MAIN DF CHANGED\n          \n          # Replace 'right-hand-side' outliers with its 99% quantile value\n          df_test_outlier_cleaned.loc[df_test_outlier_cleaned[col] > upper_bound,col] = upper_bound  + epsilon # --> MAIN DF CHANGED\n        \n","execution_count":null,"outputs":[]},{"metadata":{"id":"GYm4Y-rN44La"},"cell_type":"markdown","source":"## **Visualization After Cleaning Outlier**"},{"metadata":{"id":"-xvLnXZw5CSa","outputId":"126fba04-ea9d-4ba7-a642-40cca145b775","trusted":true},"cell_type":"code","source":"box_plot(Y_train,numerical_columns,X_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"kqVHpHvHjIER","trusted":true},"cell_type":"code","source":"X_test = df_test_outlier_cleaned","execution_count":null,"outputs":[]},{"metadata":{"id":"H7xB_cKuJAM-"},"cell_type":"markdown","source":"# **Imputation for Missing Values**"},{"metadata":{"id":"mu-QSl2nE6Dq"},"cell_type":"markdown","source":"<img src=\"https://cdn.discordapp.com/attachments/693138332166914077/698142092865830922/Missing_Value.png\">"},{"metadata":{"id":"BeLeKDHjGtbX"},"cell_type":"markdown","source":"## **Specifying a range for missing values**"},{"metadata":{"id":"tJsqsz_Y_dZ5","outputId":"e6a6e461-48b6-49d7-ad0e-c551fee4cb8b","trusted":true},"cell_type":"code","source":"Zero_MR_variables_list = list(data_info[data_info['%_Missing_Value']==0].index)\nLow_MR_variables_list = list(data_info[(data_info['%_Missing_Value']>0)&\n                                       (data_info['%_Missing_Value']<=0.05)].index)\nModerate_MR_variables_list = list(data_info[(data_info['%_Missing_Value']>0.05)&\\\n                                                      (data_info['%_Missing_Value']<=0.25)].index)\nHigh_MR_variables_list = list(data_info[(data_info['%_Missing_Value']>0.25)&\\\n                                                  (data_info['%_Missing_Value']<=0.50)].index)\nExtreme_MR_variables_list = list(data_info[(data_info['%_Missing_Value']>0.50)&\n                                           (data_info['%_Missing_Value']<=0.95)].index)\nDrop_MR_variables_list = list(data_info[data_info['%_Missing_Value']>0.95].index)\n\nlen(Zero_MR_variables_list),len(Low_MR_variables_list),len(Moderate_MR_variables_list),len(High_MR_variables_list),\\\nlen(Extreme_MR_variables_list),\\\nlen(Zero_MR_variables_list)+len(Low_MR_variables_list)+len(Moderate_MR_variables_list)+len(High_MR_variables_list)+\\\nlen(Extreme_MR_variables_list) == len(data_info)","execution_count":null,"outputs":[]},{"metadata":{"id":"kCj2tiUONnRt"},"cell_type":"markdown","source":"# **Simple Imputer for Low Missing Values**"},{"metadata":{"id":"TJtAUQhx6lMR"},"cell_type":"markdown","source":"Filling in the null values ​​of columns with a missing value statistic less than 0.05\n\nIt does this according to the frequency of the data."},{"metadata":{"id":"IdHu2QVFIAr5","outputId":"7d9051b9-8a1f-4391-a67b-a405128fd174","trusted":true},"cell_type":"code","source":"Low_MR_variables_list","execution_count":null,"outputs":[]},{"metadata":{"id":"ACjXGkopMbcx","trusted":true},"cell_type":"code","source":"def SimpleImputer(df,data_info,variable_list):\n  for col in variable_list:\n    \n    if(col in numerical_columns):\n      \n      print(\"Total null values: {}\".format(df[[str(col)]].isnull().sum()))\n\n      average = float(df[col].mean())\n      std = float(df[col].std())\n      count_nan = int(df[col].isnull().sum())\n      rand = np.random.normal(loc=average,scale=std,size =count_nan)\n      slice_col = pd.Series(df[col].copy())\n      slice_col[pd.isnull(slice_col)] = rand\n      df[col] = slice_col\n\n      print(\"Numerical variable {} have been imputed.\".format(col))\n\n    else:\n\n      print(\"Total null values: {}\".format(df[[str(col)]].isnull().sum()))\n      df.loc[df.loc[:,col].isnull(),col] = np.random.choice(sorted(list(df.loc[:,col].dropna().unique())),\n                                                            size=int(df.loc[df.loc[:,col].isnull(),col].shape[0]),\n                                                            p=[pd.Series(df.groupby(col).size()/df.loc[:,col].dropna().shape[0]).iloc[i] for i in \n                                                               np.arange(0,len(df.loc[:,col].dropna().unique()))])\n      \n      print(\"Categorical variable {} have been imputed.\".format(col))","execution_count":null,"outputs":[]},{"metadata":{"id":"TkwIcWi4O1Dz","outputId":"60812b44-2189-4450-b28b-e41985d09d40","trusted":true},"cell_type":"code","source":"SimpleImputer(X_train, data_info, Low_MR_variables_list)","execution_count":null,"outputs":[]},{"metadata":{"id":"jUSUlL75d48h","outputId":"66352654-12d1-49eb-a90c-1adeab6238fb","trusted":true},"cell_type":"code","source":"SimpleImputer(X_test,data_info,Low_MR_variables_list)","execution_count":null,"outputs":[]},{"metadata":{"id":"Q7-bAy3ZO5Tk","outputId":"f78d6974-7587-4291-f980-eb1145a8ec95","trusted":true},"cell_type":"code","source":"MissingUniqueStatistics(X_train.loc[:,Low_MR_variables_list])","execution_count":null,"outputs":[]},{"metadata":{"id":"v4AZoEaCeE6z","outputId":"4ec4eda2-ece2-475c-9117-abb630a135d1","trusted":true},"cell_type":"code","source":"MissingUniqueStatistics(X_test.loc[:,Low_MR_variables_list])","execution_count":null,"outputs":[]},{"metadata":{"id":"muB83XY01yTp"},"cell_type":"markdown","source":"## **STRING CONVERSION**"},{"metadata":{"id":"iajmD0oR9woe"},"cell_type":"markdown","source":"Categorizes empty cells and converts them into numerical variables by using mean encoding."},{"metadata":{"id":"cmf2YsiIkKYp"},"cell_type":"markdown","source":"## MEAN ENCODING"},{"metadata":{"id":"yYYHE45a0lP-","trusted":true},"cell_type":"code","source":"class KFoldTargetEncoderTrain(base.BaseEstimator,\n                               base.TransformerMixin):\n    def __init__(self,colnames,targetName,\n                  n_fold=5, verbosity=True,\n                  discardOriginal_col=False):\n        self.colnames = colnames\n        self.targetName = targetName\n        self.n_fold = n_fold\n        self.verbosity = verbosity\n        self.discardOriginal_col = discardOriginal_col\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self,X):\n        assert(type(self.targetName) == str)\n        assert(type(self.colnames) == str)\n        assert(self.colnames in X.columns)\n        assert(self.targetName in X.columns)\n        \n        mean_of_target = X[self.targetName].mean()\n        kf = KFold(n_splits = self.n_fold,\n                   shuffle = False, random_state=2020)\n        col_mean_name = self.colnames + '_' + 'Kfold_Target_Enc'\n        X[col_mean_name] = np.nan\n        for tr_ind, val_ind in kf.split(X):\n            X_tr, X_val = X.iloc[tr_ind], X.iloc[val_ind]\n            X.loc[X.index[val_ind], col_mean_name] = \\\n            X_val[self.colnames].map(X_tr.groupby(self.colnames)\n                                     [self.targetName].mean())\n            X[col_mean_name].fillna(mean_of_target, inplace = True)\n        if self.verbosity:\n            encoded_feature = X[col_mean_name].values\n            print('Correlation between the new feature, {} and, {} is {}.'\\\n                  .format(col_mean_name,self.targetName,\n                          np.corrcoef(X[self.targetName].values,\n                                      encoded_feature)[0][1]))\n        if self.discardOriginal_col:\n            X = X.drop(self.targetName, axis=1)\n        return X","execution_count":null,"outputs":[]},{"metadata":{"id":"gm5wM4Og0lL0","trusted":true},"cell_type":"code","source":"def StringConverterTrain(df,target_name,variable_list):\n    for col in variable_list:\n      targetc = KFoldTargetEncoderTrain(col,target_name,n_fold=4)\n      new_train = targetc.fit_transform(df)\n    return new_train","execution_count":null,"outputs":[]},{"metadata":{"id":"Gc1gQkNj20zK","outputId":"b91ad459-2530-4726-a73d-f30479f3705d","trusted":true},"cell_type":"code","source":"nominal_variable = list(data_info[data_info[\"Variable_Type\"]==\"Nominal\"].index)\nnominal_lst = [item for item in Moderate_MR_variables_list+High_MR_variables_list+Extreme_MR_variables_list if item in nominal_variable]\nnominal_lst","execution_count":null,"outputs":[]},{"metadata":{"id":"kTab7bR6iHrS","outputId":"24afa056-14c7-4621-f95f-66d06544e4b2","trusted":true},"cell_type":"code","source":"df_trial = pd.concat([X_train,Y_train],axis=1).copy()\ndf_output_train = StringConverterTrain(df=df_trial,target_name=\"CLASS\",variable_list=nominal_lst)","execution_count":null,"outputs":[]},{"metadata":{"id":"_EiO8NOt_7AQ","outputId":"d5858f93-6d8e-4310-b075-4f981258517c","trusted":true},"cell_type":"code","source":"for item in nominal_lst:\n  print(df_output_train.loc[:,[item+\"_Kfold_Target_Enc\"]].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"id":"g_kkUyuTrEir","trusted":true},"cell_type":"code","source":"for item in nominal_lst:\n  X_train[item] = df_output_train[item+\"_Kfold_Target_Enc\"]","execution_count":null,"outputs":[]},{"metadata":{"id":"74AL7MHYzs6N"},"cell_type":"markdown","source":"# Mean Encoding for nominal variables(non missing value)"},{"metadata":{"id":"ir1y2TA2zNEk","outputId":"c19af6ea-f4ee-4f3d-ff96-20fc8830be43","trusted":true},"cell_type":"code","source":"nominal_encoding_variable_lst =[\"Consequence\",\"REF\",\"ALT\",\"CLNDISDB\",\"CLNDN\",\"Allele\",\"Feature\",\"SYMBOL\"]\n\ndf_encoding = pd.concat([X_train,Y_train],axis=1).copy()\ndf_encoding_train = StringConverterTrain(df=df_encoding,target_name=\"CLASS\",variable_list=nominal_encoding_variable_lst)","execution_count":null,"outputs":[]},{"metadata":{"id":"2nJECL8m0B9y","outputId":"8cef9212-a47f-44c4-a928-6ff047bf288a","trusted":true},"cell_type":"code","source":"for item in nominal_encoding_variable_lst:\n  print(df_encoding_train.loc[:,[item+\"_Kfold_Target_Enc\"]].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"id":"amfwMDjH0Ty8","trusted":true},"cell_type":"code","source":"for item in nominal_encoding_variable_lst:\n  X_train[item] = df_encoding_train[item+\"_Kfold_Target_Enc\"]","execution_count":null,"outputs":[]},{"metadata":{"id":"jpGU3b1X0bGV","outputId":"e226f3ae-34f9-4c21-f58a-8d4b9e382a22","trusted":true},"cell_type":"code","source":"MissingUniqueStatistics(X_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"dzV91_wsARJf"},"cell_type":"markdown","source":"# **String Converter for Test Dataset**"},{"metadata":{"id":"ygYR0OhwrMlO","trusted":true},"cell_type":"code","source":"df_output_test = X_test.copy()\nmean_of_target = df_output_train['CLASS'].copy().mean()\ntarget_mean_list = nominal_lst                                                 \nfor col in target_mean_list:\n    df_output_test[col] = df_output_test[col].map(df_output_train.groupby(col)[col+'_Kfold_Target_Enc'].mean())\n    df_output_test[col].fillna(mean_of_target, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"vRm-vQJ7rQbi","outputId":"f54a10df-28b2-46cb-93c6-ca59afc5f6b5","trusted":true},"cell_type":"code","source":"for item in nominal_lst:\n  print(df_output_test.loc[:,[item]].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"id":"8cORcIWSrQuB","trusted":true},"cell_type":"code","source":"X_test[nominal_lst] = df_output_test[nominal_lst]","execution_count":null,"outputs":[]},{"metadata":{"id":"rG2KFoghMrGW","trusted":true},"cell_type":"code","source":"X_test_encoder = X_test.copy()\nmean_of_target = df_encoding_train['CLASS'].copy().mean()\ntarget_mean_list = nominal_encoding_variable_lst                                                 \nfor col in target_mean_list:\n    X_test_encoder[col+'_Kfold_Target_Enc'] = X_test_encoder[col].map(df_encoding_train.groupby(col)[col+'_Kfold_Target_Enc'].mean())\n    X_test_encoder[col+'_Kfold_Target_Enc'].fillna(mean_of_target, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"N_RKW8Q1M79f","outputId":"a1a98610-3032-4c45-861a-1b79f08dcdeb","trusted":true},"cell_type":"code","source":"for item in nominal_encoding_variable_lst:\n  X_test[item] = X_test_encoder[item+\"_Kfold_Target_Enc\"]\n  \nMissingUniqueStatistics(X_test[nominal_encoding_variable_lst])","execution_count":null,"outputs":[]},{"metadata":{"id":"a7wc4AGQzJKo"},"cell_type":"markdown","source":"Our column \"CLNHGVS\" in the table has 65188 values ​​and all values ​​are unique. Therefore, we drop this column as it does not carry any information."},{"metadata":{"id":"AmxTVuZZgrfs","trusted":true},"cell_type":"code","source":"X_train.drop(\"CLNHGVS\", axis = 1, inplace = True)\nX_test.drop(\"CLNHGVS\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"63llg9xmEMZN"},"cell_type":"markdown","source":"# Modal Based Imputation"},{"metadata":{"id":"0VdPBxQX-g5W","trusted":true},"cell_type":"code","source":"def MBI(df,columns,train_or_test,lst_numerical):\n\n  data_binary_encoded=df.copy()\n  le=LabelEncoder()\n\n  for col in columns:\n    \n    if(train_or_test == \"test\"):\n\n      le.fit(X_train[col].copy().astype(str))\n      data_binary_encoded[col]=le.transform(df[col].copy().astype(str))\n\n    else:\n\n      data_binary_encoded[col] = le.fit_transform(df[col].copy().astype(str))\n\n  data_scaled=data_binary_encoded.copy()\n\n  for col in numerical_columns:\n\n    scaler = StandardScaler()\n\n    if(train_or_test == \"test\"):\n\n      scaler.fit(np.array(X_train.loc[:,col]).reshape(-1,1))\n      data_scaled.loc[:,col] = scaler.transform(np.array(data_scaled.loc[:,col]).reshape(-1,1))\n\n    else:\n      data_scaled.loc[:,col] = scaler.fit_transform(np.array(data_scaled.loc[:,col]).reshape(-1,1))\n\n  for col in lst_numerical:\n\n    target_dropped_fullcases = data_scaled.drop(col,axis=1).loc[:,list(set(Zero_MR_variables_list+Low_MR_variables_list)-\n                                                                                  set([\"CLASS\",\"KEY\",\"CLNHGVS\"]))].copy()\n    \n    target = data_scaled.loc[:,col]\n    null_mask = target.isna()\n    print(col)\n\n    if(col in numerical_columns):\n      \n      mlp = MLPRegressor(hidden_layer_sizes=(100,10,),\n                        activation='tanh',\n                        solver='adam',\n                        learning_rate='adaptive',\n                        max_iter=1000,\n                        learning_rate_init=0.01,\n                        alpha=0.01,\n                        early_stopping = False)\n    else:\n      mlp = MLPClassifier(hidden_layer_sizes=(100,10,),\n                        activation='tanh',\n                        solver='adam',\n                        learning_rate='adaptive',\n                        max_iter=1000,\n                        learning_rate_init=0.01,\n                        alpha=0.01,\n                        early_stopping = False)\n    \n    mlp.fit(target_dropped_fullcases[~null_mask],target[~null_mask])\n    data_scaled.loc[null_mask,col] = mlp.predict(target_dropped_fullcases[null_mask])\n\n  print(data_scaled.loc[:,lst_numerical].isnull().sum());\n  return data_scaled","execution_count":null,"outputs":[]},{"metadata":{"id":"2jDg272TBLOb","outputId":"431f4a5a-1676-47b6-ac6e-7466dbe964c8","trusted":true},"cell_type":"code","source":"lst_numerical = [item for item in Moderate_MR_variables_list if item in numerical_columns]\nlst_numerical.append(\"SIFT\")\nlst_numerical.append(\"PolyPhen\")\nlst_numerical","execution_count":null,"outputs":[]},{"metadata":{"id":"9Qt3MGxW6Fkn","trusted":true},"cell_type":"code","source":"encoding_col_list =[\"CHROM\",\"CLNVC\",\"Feature_type\",\"BIOTYPE\",\"IMPACT\"]\n","execution_count":null,"outputs":[]},{"metadata":{"id":"rAz5prgp6NAT"},"cell_type":"markdown","source":"# **Scaling**"},{"metadata":{"id":"kkc-oF6-IrHR","outputId":"fb2e998d-353c-4a33-d9f4-1676a125d5db","trusted":true},"cell_type":"code","source":"X_train_scaled = MBI(X_train,encoding_col_list,\"train\",lst_numerical)","execution_count":null,"outputs":[]},{"metadata":{"id":"fc_EZLmk9zJN","outputId":"0fac0a80-53d4-422d-c0ca-c88b8e93157d","trusted":true},"cell_type":"code","source":"X_train_scaled","execution_count":null,"outputs":[]},{"metadata":{"id":"gcZqn30qG6c2","outputId":"1cdc42db-af4a-4100-fdc7-a1c9a142ae6e","trusted":true},"cell_type":"code","source":"X_test_scaled = MBI(X_test,encoding_col_list,\"test\",lst_numerical)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZelQC1BoDMOt","trusted":true},"cell_type":"code","source":"def Label_Encoder(df,columns,train_or_test):\n  le = LabelEncoder()\n  for col in columns:\n    if(train_or_test == \"test\"):\n\n      le.fit(X_train_scaled[col].copy().astype(str))\n      df[col] = le.transform(df[col].copy().astype(str))\n\n    else:\n      df[col] = le.fit_transform(df[col].copy().astype(str))\n\n  return df","execution_count":null,"outputs":[]},{"metadata":{"id":"SHfOuqoKEoEb","trusted":true},"cell_type":"code","source":"X_test_scaled = Label_Encoder(X_test_scaled,[\"SIFT\",\"PolyPhen\"],\"test\")","execution_count":null,"outputs":[]},{"metadata":{"id":"4yJ_rBdOELrM","trusted":true},"cell_type":"code","source":"X_train_scaled = Label_Encoder(X_train_scaled,[\"SIFT\",\"PolyPhen\"],\"train\")","execution_count":null,"outputs":[]},{"metadata":{"id":"n98JKir3NEm8"},"cell_type":"markdown","source":"# Feature Importance"},{"metadata":{"id":"lHNYFrbLSzMh","trusted":true},"cell_type":"code","source":"rnd_clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\nrnd_clf.fit(X_train_scaled, Y_train)\n\nfeatures = X_train_scaled.columns\nimportances = rnd_clf.feature_importances_\nindices = np.argsort(importances)","execution_count":null,"outputs":[]},{"metadata":{"id":"LjiYKW9JcHF7","outputId":"0034a095-ba36-4efb-c8a2-0adc74d3ffd6","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nfeat_importances = pd.Series(importances, index=features)\nfeat_importances.nlargest(len(indices)).plot(kind='bar',color = '#79CCB3');","execution_count":null,"outputs":[]},{"metadata":{"id":"tPl46DQNBmbZ"},"cell_type":"markdown","source":"# MODELLING"},{"metadata":{"id":"yg3k4htVk8pk","trusted":true},"cell_type":"code","source":"# Creating an empty Dataframe with Scores\ndf_accur_roc_score = pd.DataFrame(columns=['Roc_Auc_Score'])","execution_count":null,"outputs":[]},{"metadata":{"id":"U_TZGZiGhyR7"},"cell_type":"markdown","source":"## **Logistic Regression**"},{"metadata":{"id":"8YtKfI6X8BUP","outputId":"eba949e0-6ffb-40c9-d87b-be6b04ad3956","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(random_state=0)\nclf.fit(X_train_scaled, Y_train)\n\ny_preds = clf.predict_proba(X_test_scaled)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score.loc['Logistic_regression'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='Logistic R. AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"v6TQdKIIDRIz"},"cell_type":"markdown","source":"## **XGBoost Classifier**"},{"metadata":{"id":"OW4x5od4Db9T","outputId":"478bda3a-3502-4713-ded7-8f9b8c62caaa","trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nxgb_model = xgb.XGBClassifier(n_estimators=150,random_state=0,learning_rate=0.25,eta=0.4,booster=\"gbtree\",base_score=0.8,colsample_bylevel=0.9009229642844634,gamma=0.49967765132613584,\n                        max_depth=6,min_child_weight=7,reg_lambda=0.27611902459972926,subsample=0.9300916052594785)\n\nxgb_model.fit(X_train_scaled, Y_train)\n\ny_preds = xgb_model.predict_proba(X_test_scaled)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score.loc['XGBoost_Classifier'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='XGBoost Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"QiS-IaEjCkmX"},"cell_type":"markdown","source":"## **KNeighbors Classifier**"},{"metadata":{"id":"76-aWk9eBlqK","outputId":"1947759b-ae26-445c-9b71-fc9ccda0e0b7","trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(X_train_scaled,Y_train)\ny_preds = knn.predict_proba(X_test_scaled)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score.loc['KNeighborsClassifier'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='KNeighbors Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"6qQ0niv8CuTR"},"cell_type":"markdown","source":"## **Decision Tree Classifier**"},{"metadata":{"id":"pTq6BjW4CjF7","outputId":"c930716a-4e5c-46d7-9ff6-fee5f48dbfe1","trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nreg_dtr = DecisionTreeClassifier(random_state=0)\nreg_dtr.fit(X_train_scaled,Y_train)\n\ny_preds = reg_dtr.predict_proba(X_test_scaled)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score.loc['DecisionTreeClassifier'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='DecisionTree Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"3YpQcNsWEBwG"},"cell_type":"markdown","source":"##  **LightGBM Classifier**"},{"metadata":{"id":"naYIhd9zEFDT","outputId":"72ea18ae-1054-4dea-92bf-cc750396589a","trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(\n        max_depth=6,\n        n_estimators=100,random_state=0,learning_rate=0.1,eta=0.4,base_score=0.8,colsample_bylevel=0.9009229642844634,gamma=0.49967765132613584,\n                        min_child_weight=9,reg_lambda=0.27611902459972926,subsample=0.9300916052594785,min_samples_split=2,min_samples_leaf=0.1)\n\nlgbm.fit(X_train_scaled, Y_train)\n\ny_preds = lgbm.predict_proba(X_test_scaled)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score.loc['LGBMClassifier'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='LGBM Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"3BbFYDeME6es"},"cell_type":"markdown","source":"## Gradient Boosting Classifier"},{"metadata":{"id":"G6c8ClkZE62x","outputId":"615865b2-12e9-43ce-f411-ec083ea338b5","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\ngradient_boosting_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n                                                   max_depth=7, random_state=0).fit(X_train_scaled, Y_train)\n\ny_preds = gradient_boosting_clf.predict_proba(X_test_scaled)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score.loc['GradientBoostingClassifier'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='GradientBoosting Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"eWGlBmBsAebE"},"cell_type":"markdown","source":"## **Hist Gradient Boosting Classifier**"},{"metadata":{"id":"wuhWDlYKAc3c","outputId":"10d2bb5e-7605-4ec7-b944-3ac48577c700","trusted":true},"cell_type":"code","source":"from sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\n\nhist_gradient_boosting_clf = HistGradientBoostingClassifier(learning_rate=0.25,\n                                                   max_depth=4, random_state=0).fit(X_train_scaled, Y_train)\n\ny_preds = hist_gradient_boosting_clf.predict_proba(X_test_scaled)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score.loc['HistGradientBoostingClassifier'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.plot(fpr, tpr, label='HistGradientBoosting Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\nplt.title('ROC Curve')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"j_wmV50DMGGw","outputId":"bb6a920b-9b34-4f23-c96f-fbebfed0d3db","trusted":true},"cell_type":"code","source":"df_accur_roc_score\n","execution_count":null,"outputs":[]},{"metadata":{"id":"noROaJxIK0Xe"},"cell_type":"markdown","source":"# **Visualization Model Outputs**"},{"metadata":{"id":"hbIHPmAYK5WK","outputId":"99eb5506-5176-442b-94dd-87f9fc42ffa4","trusted":true},"cell_type":"code","source":"df_accur_roc_score.sort_values(by=['Roc_Auc_Score'],ascending=False).plot(kind='bar', y='Roc_Auc_Score',figsize=(20,8),color='#79ccb3', rot=0,title=\"Model outputs by roc score before feature importance\");","execution_count":null,"outputs":[]},{"metadata":{"id":"41hwwxYtRsE0"},"cell_type":"markdown","source":"# **Changing Dataset using Feature Importance** "},{"metadata":{"id":"2PQ5Nbkbs6mz","outputId":"3a7e7090-9679-4949-a9c1-c18772a8439d","trusted":true},"cell_type":"code","source":"lst_importance_drop = []\n\nfor item in range(0,feat_importances.shape[0]):\n  \n  if(feat_importances[item] < 0.004):\n    lst_importance_drop.append(features[item])\n\nX_train_importance = X_train_scaled.drop(lst_importance_drop,axis=1)\nX_test_importance = X_test_scaled.drop(lst_importance_drop,axis=1)\n\nlst_importance_drop","execution_count":null,"outputs":[]},{"metadata":{"id":"AM1OsjPvRqSH","trusted":true},"cell_type":"code","source":"# Creating an empty Dataframe with Scores\ndf_accur_roc_score_importance = pd.DataFrame(columns=['Roc_Auc_Score'])","execution_count":null,"outputs":[]},{"metadata":{"id":"WFnE-ywQSVxl"},"cell_type":"markdown","source":"### **Logistic Regression**"},{"metadata":{"id":"zGYyOZHIDMqR","outputId":"13c3c97a-a6a3-4a86-c416-9ffb96488add","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(random_state=0)\nclf = logreg.fit(X_train_importance, Y_train)\n\ny_preds = clf.predict_proba(X_test_importance)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score_importance.loc['Logistic_regression'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='Logistic Regression AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"7k7Y1pnOSRbw"},"cell_type":"markdown","source":"### **XGBoost Classifier**"},{"metadata":{"id":"2tScYj_pEGtM","outputId":"062b5047-7773-4a85-d773-3162efb95cb3","trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nxgb_model = xgb.XGBClassifier(n_estimators=150,random_state=0,learning_rate=0.1,eta=0.4,booster=\"gbtree\",base_score=0.8,colsample_bylevel=0.9009229642844634,gamma=0.49967765132613584,\n                        max_depth=6,min_child_weight=7,reg_lambda=0.27611902459972926,subsample=0.9300916052594785)\n\nxgb_model.fit(X_train_importance, Y_train)\ny_preds = xgb_model.predict_proba(X_test_importance)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score_importance.loc['XGBoost_Classifier'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='XGBoost Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"FzqnInW4SimH"},"cell_type":"markdown","source":"### **KNeighbors Classifier**"},{"metadata":{"id":"DE-yecq7S0LB","outputId":"baf0fcc5-b122-4e24-a99a-b52a7b82df9c","trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(X_train_importance,Y_train)\ny_preds = knn.predict_proba(X_test_importance)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score_importance.loc['KNeighborsClassifier'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='KNeighbors Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"Fuhx_UW6TECi"},"cell_type":"markdown","source":"### **Decision Tree Classifier**"},{"metadata":{"id":"hX4PPKPgTPFM","outputId":"9230cce0-f99e-42fb-8d83-946fee8f9a38","trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nreg_dtr = DecisionTreeClassifier(random_state=0)\nreg_dtr.fit(X_train_importance,Y_train)\n\ny_preds = reg_dtr.predict_proba(X_test_importance)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score_importance.loc['DecisionTreeClassifier'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='DecisionTree Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"3sSVowSeTfvN"},"cell_type":"markdown","source":"### **LightGBM Classifier**"},{"metadata":{"id":"-tS7z5FMHpLS","outputId":"2add56da-dade-4c8e-c750-16caaa4c88d1","trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(\n        max_depth=6,\n        n_estimators=100,random_state=0,learning_rate=0.25,eta=0.4,base_score=0.8,colsample_bylevel=0.9009229642844634,gamma=0.49967765132613584,\n                        min_child_weight=7,reg_lambda=0.27611902459972926,subsample=0.9300916052594785,min_sample_split=2)\n\nlgbm.fit(X_train_importance, Y_train)\n\ny_preds = lgbm.predict_proba(X_test_importance)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score_importance.loc['LGBMClassifier'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='LGBM Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"wc9vZHarTo99"},"cell_type":"markdown","source":"### **Gradient Boosting Classifier**"},{"metadata":{"id":"7WBLI2KLKXAf","outputId":"1b94558c-8445-4531-8c99-a0aa7c0494dc","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\ngradient_boosting_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n                                                   max_depth=5, random_state=0).fit(X_train_importance, Y_train)\n\n\ny_preds = gradient_boosting_clf.predict_proba(X_test_importance)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score_importance.loc['GradientBoostingClassifier'] = [auc_score]\n\nplt.subplots(figsize=(8, 6))\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label='GradientBoosting Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"JLIeJsoyAAaG"},"cell_type":"markdown","source":"### **Hist Gradient Boosting Classifier**"},{"metadata":{"id":"CZ4svM3__-Wx","outputId":"a61a8748-aefd-4ae6-f35d-bef2f6224813","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import HistGradientBoostingClassifier\n\nhist_gradient_boosting_clf = HistGradientBoostingClassifier(learning_rate=0.1,\n                                                   max_depth=7, random_state=0).fit(X_train_importance, Y_train)\n\ny_preds = hist_gradient_boosting_clf.predict_proba(X_test_importance)\npreds = y_preds[:,1]\n\nfpr, tpr, _ = metrics.roc_curve(Y_test, preds)\n\nauc_score = metrics.auc(fpr, tpr)\ndf_accur_roc_score_importance.loc['HistGradientBoostingClassifier'] = [auc_score]\nplt.subplots(figsize=(8, 6))\nplt.plot(fpr, tpr, label='HistGradientBoosting Classifier AUC = {:.2f}'.format(auc_score))\nplt.plot([0,1],[0,1],'r--')\nplt.title('ROC Curve')\n\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"oN15Nb07Owgm","outputId":"9ce20cb1-7998-43f7-fca0-23e14bca75bb","trusted":true},"cell_type":"code","source":"df_accur_roc_score_importance.sort_values(by=['Roc_Auc_Score'],ascending=False).plot(kind='bar', y='Roc_Auc_Score',figsize=(20,8),color='#79ccb3', rot=0,title=\"Model outputs by roc score before feature importance\");","execution_count":null,"outputs":[]},{"metadata":{"id":"Cve1t94HP7_1"},"cell_type":"markdown","source":"### **Visualization for After Feature Importance and Before Feature Importance**"},{"metadata":{"id":"76bhgZV0URRQ","outputId":"c67724aa-ec0d-4645-ccb2-6a15dd0f26ff","trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize = (9,10))\nsns.barplot(x=df_accur_roc_score_importance.Roc_Auc_Score,y=df_accur_roc_score_importance.index,color='red',alpha = 0.5,label='After Feature Importance' )\nsns.barplot(x=df_accur_roc_score.Roc_Auc_Score,y=df_accur_roc_score.index,color='blue',alpha = 0.7,label='Before Feature Importance')\n\nax.legend(frameon = True)\nax.set(xlabel='Scores', ylabel='Models',title = \"Auc Score \")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"dqMZzMW6Umwo","trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\n\nfig = go.Figure(data=[\n    go.Bar(name='Before Feature Importance', y=df_accur_roc_score.Roc_Auc_Score, x=df_accur_roc_score.index,text=round(df_accur_roc_score.Roc_Auc_Score,3),textposition='auto'),\n    go.Bar(name='After Feature Importance', y=df_accur_roc_score_importance.Roc_Auc_Score, x=df_accur_roc_score_importance.index,text=round(df_accur_roc_score_importance.Roc_Auc_Score,3),textposition='auto',)\n    \n])\nfig.update_layout(barmode='group')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"Rn8MWPasjLtp","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Clinvar_Conflicting.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":4}