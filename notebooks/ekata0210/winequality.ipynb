{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"winequality = pd.read_csv(\"../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *Data Visualization*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets have a look at top 5 columns in our dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"winequality.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets see if there is any missing data in our dataset. This step is crucial in any data analysis. Not having many values for any feature might make our assumptions totally wrong. And we do not want to take any risk with Wine. Do we?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"winequality.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from the above execution that there are no-null values. We can safely start now with checking various relations among different features using data visualization techniques","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#SNS is very popular library in python, It is very easy to plot and infer relations between two parameters using this\nimport seaborn as sns \nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen above every data is in numerical form, this makes our analysis much easier as we do not have to deal with strings. We will directly jump into heatmap","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"winequality.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(winequality.corr(), annot=True, fmt= '.1f',ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Greater the number inside the box, higher the dependency between the two.\n\nWith the above understanding, let us see how is quality related to otehr factors\n\nWhile alcohol, sulphates and citric acidity positively influence final wine quality\nVolatile acidity almost negatively influences wine quality.\n\nFew other conclusions that can be derived from heat map\n1. Free sulphur dioxide and total sulphur dioxide are related (as expected)\n2. Fixed acidity, citrus acidity and residual sugar influences pH\nLet us start to plot each of these variables against ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (20,6))\nsns.regplot(x= winequality['alcohol'], y = winequality['quality'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"From scatterplot it can be seen that alcohol positively influences wine quality. Alcohol percentage > 11.5 generally gives us good review","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (10,6)) \nsns.barplot(y= winequality['fixed acidity'], x = winequality['quality'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The bar-graph depicts what we observed from heatmap. There is no much dependency between wine_quality and fixed acidity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (20,6))\nsns.barplot(x= winequality['quality'], y = winequality['sulphates'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(winequality['quality'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Its clear from the above graph how our wine quality is distributed over different ratings","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# *Preparing data for machine learning*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here we categorise wine quality into 3 segments. From the above graph it is clear that most wines are in the rating 5-6, so we consider this as average rating. Anything below 5 will be a bad rating and any other rating will be good.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"quality = winequality[\"quality\"].values\ncategory = []\nfor num in quality:\n    if num<5:\n        category.append(\"Bad\")\n    elif num == 5 or num == 6:\n        category.append(\"Average\")\n    else:\n        category.append(\"Good\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the above assumption, we replace numerical quality data in our main dataset to the one with categorised rating","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating new dataset for prediction\ncategory = pd.DataFrame(data=category, columns=[\"category\"])\nwinedata = pd.concat([winequality,category],axis=1)\nwinedata.drop(columns=\"quality\",axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"winedata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X= winedata.iloc[:,:-1].values\ny= winedata.iloc[:,-1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabelencoder_y =LabelEncoder()\ny= labelencoder_y.fit_transform(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  *Machine Learning Models using scikit library*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here we first split our data into training and testing. Training data will contain 80% while testing data will be 20% of main dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We train our data on \n1. Random Forest Classifier\n2. KNN\n3. Logistic Regression\n4. DecisionTree\n5. Naive Bayes\n\nAt the end, we compare how each model will perform on our data and finalize on the model based on performance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,accuracy_score\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrandom_result = RandomForestClassifier(n_estimators=250)\nrandom_result.fit(X_train, y_train)\nres_forest = random_result.predict(X_test)\nprint(classification_report(y_test, res_forest))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn_result = KNeighborsClassifier()\nknn_result.fit(X_train,y_train)\nres_knn=knn_result.predict(X_test)\nprint(classification_report(y_test, res_knn))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr_result = LogisticRegression()\nlr_result.fit(X_train, y_train)\nres_logRes = lr_result.predict(X_test)\nprint(classification_report(y_test, res_logRes))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nDecTree_res = DecisionTreeClassifier()\nDecTree_res.fit(X_train,y_train)\nres_DecTree = DecTree_res.predict(X_test)\nprint(classification_report(y_test, res_DecTree))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nNaiBay_res = GaussianNB()\nNaiBay_res.fit(X_train,y_train)\nres_NaiBay=NaiBay_res.predict(X_test)\nprint(classification_report(y_test, res_NaiBay))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_result = pd.DataFrame({'models': [\"Random Forest\",\"KNN\",\"LogisticRegression\",\"DecisionTree\", \"NaiveBayes\"],\n                           'accuracy_score': [accuracy_score(y_test,res_forest),accuracy_score(y_test,res_knn), accuracy_score(y_test,res_logRes), \n                                              accuracy_score(y_test,res_DecTree), accuracy_score(y_test,res_NaiBay)]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (6,6))\nsns.barplot(x= final_result['models'], y = final_result['accuracy_score'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# * Conclusion*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Naive Bayes performed the worst while KNN, Logistic Regression faired slightly better\nDecision Tree had accuracy score 0.81 which made it better than Naive Bayes but not so good as KNN and Logistic Regression\nRandom forest with the accuracy of 0.89 emerged clearly as the best one","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}