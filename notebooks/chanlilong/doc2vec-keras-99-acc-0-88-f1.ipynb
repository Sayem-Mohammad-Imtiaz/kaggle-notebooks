{"cells":[{"metadata":{},"cell_type":"markdown","source":"### <center> In this Notebook, the task is a binary classification of fraudulent behaviour by analysing text </center>\n\n#### Here is what ive used:\n#### 1. Doc2Vec\n#### 2. textclean\n#### 3. Keras","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import the data and fill NAs\n\n- Notice that ived replaced \"benefits\" with \"Adequete benefits\".\n- This is because doc2vec will intepret this as a vector. \n- This vector will contain information on its general context","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\ndf1=pd.read_csv(\"/kaggle/input/real-or-fake-fake-jobposting-prediction/fake_job_postings.csv\",index_col=\"job_id\")\ndf1[\"location\"]=df1[\"location\"].fillna(\"LOC\")\ndf1[\"department\"]=df1[\"department\"].fillna(\"DEPART\")\ndf1[\"salary_range\"]=df1[\"salary_range\"].fillna(\"0-0\")\ndf1[\"company_profile\"]=df1[\"company_profile\"].fillna(\"No Description\")\ndf1[\"description\"]=df1[\"description\"].fillna(\"No Description\")\ndf1[\"requirements\"]=df1[\"requirements\"].fillna(\"No Description\")\ndf1[\"benefits\"]=df1[\"benefits\"].fillna(\"Adequete benefits\")\ndf1[\"employment_type\"]=df1[\"employment_type\"].fillna(\"Other\")\ndf1[\"required_experience\"]=df1[\"required_experience\"].fillna(\"Not Applicable\")\ndf1[\"required_education\"]=df1[\"required_education\"].fillna(\"Bachelor's Degree\")\ndf1[\"industry\"]=df1[\"industry\"].fillna(\"None\")\ndf1[\"function\"]=df1[\"function\"].fillna(\"None\")\ndf1.head()\n# df1.industry.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Doc2Vec\nmodel=Doc2Vec.load(\"/kaggle/input/doc2vec-english-binary-file/doc2vec.bin\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ive converted the relavant columns to categorical here","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#=========[CLEAN DATA]==============#\n# !pip install textcleaner==0.4.26\nimport string\n\n#=========[CLEAN PANDAS]==============#\n# employment_type\trequired_experience\trequired_education\tindustry\tfunction\nfrom sklearn.preprocessing import LabelEncoder\ndf1[\"location\"]=LabelEncoder().fit_transform(df1[\"location\"])\ndf1[\"department\"]=LabelEncoder().fit_transform(df1[\"department\"])\ndf1[\"salary_range\"]=LabelEncoder().fit_transform(df1[\"salary_range\"])\ndf1[\"employment_type\"]=LabelEncoder().fit_transform(df1[\"salary_range\"])\ndf1[\"required_experience\"]=LabelEncoder().fit_transform(df1[\"salary_range\"])\ndf1[\"required_education\"]=LabelEncoder().fit_transform(df1[\"salary_range\"])\ndf1[\"industry\"]=LabelEncoder().fit_transform(df1[\"salary_range\"])\ndf1[\"function\"]=LabelEncoder().fit_transform(df1[\"salary_range\"])\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We text the text cleaner here\n- ive used this text cleaner because it does not remove stop words\n- all words are required in order for doc2vec to capture the relavant context as implied by the name\n- you can try removing stop words and see how it affects accuraccy (from my tests, it reduces accuracy if stopwords are taken out)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install clean-text\nfrom cleantext import clean\n\nprint(\"#==============[BEFORE]======================#\")\nprint(df1[\"company_profile\"].iloc[0])\nprint(\"#==============[AFTER]======================#\")\ntext=clean(df1[\"company_profile\"].iloc[0],no_punct=True)\nprint(text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This cell converts the text to the word2doc embeddings\n- ived saved the dataframe in a .npy file as this cell will take awhile to run\n- you can uncomment the lines below to try for yourself","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_to_embeddings(text):\n    try:\n        text=clean(text,no_punct=True)\n    except:\n        text=\" \"\n    return model.infer_vector(text.split())\n\n\n\n#==========[IVED SAVED THIS PORTION IN .NPY FILE]=======================#\n# df1[\"title\"]=df1[\"title\"].apply(convert_to_embeddings)\n# df1[\"company_profile\"]=df1[\"company_profile\"].apply(convert_to_embeddings)\n# df1[\"description\"]=df1[\"description\"].apply(convert_to_embeddings)\n# df1[\"requirements\"]=df1[\"requirements\"].apply(convert_to_embeddings)\n# df1[\"benefits\"]=df1[\"benefits\"].apply(convert_to_embeddings)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We then normalize the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\n\nswag=np.load(\"/kaggle/input/df1tonpy1/data.npy\",allow_pickle=True)\n\ntraining_data_text=np.hstack([np.vstack(swag[:,0]),np.vstack(swag[:,4]),np.vstack(swag[:,5]),np.vstack(swag[:,6]),np.vstack(swag[:,7])])\ntraining_data_text.shape\n\ntraining_data=np.hstack([training_data_text,swag[:,1:3],swag[:,8:]])\n\n\ntraining_data=scaler.fit_transform(training_data)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split the data into test and train set\n- 0.1 split was used","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X=training_data[:,:-1]\nY=training_data[:,-1]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here we define the model\n- Noticed that ived add in dropout for regularization\n- the starting few codes are meant to reduce EOM errors on my gpu\n- BatchNorm is the most important layer here\n    - Batch Norm will cause the accuracy to go up significantly, \n    - You can test this by yourself\n    - This is because it reduces the covariate shift problem (Think of a classifier trained on black cats but the test set is on ginger cats)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)\n    \nfrom tensorflow.keras.layers import Dense,Input,Dropout,BatchNormalization\nfrom tensorflow.keras.models import Sequential\n\nmodel2=Sequential()\nmodel2.add(Input(shape=(X.shape[1])))\nmodel2.add(BatchNormalization())\nmodel2.add(Dense(128,activation=tf.nn.selu))\nmodel2.add(Dropout(0.5))\nmodel2.add(Dense(64,activation=tf.nn.selu))\nmodel2.add(Dropout(0.2))\nmodel2.add(Dense(32,activation=tf.nn.selu))\nmodel2.add(Dense(1,activation=tf.nn.sigmoid))\n\n\nmodel2.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['accuracy'])\n\nmodel2.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The training is slightly overfitted here, but as you can see, it achieves impressive results\n### -0.99 Accuraccy\n### -0.98 Val Accuaraccy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model2.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=80)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix\n\npred=model2.predict(X_test)\npred=np.array([1 if row>=0.5 else 0 for row in pred])\nprint(classification_report(y_test,pred))\nsns.heatmap(confusion_matrix(y_test,pred),annot=True)\nplt.show()\n\npred=model2.predict(X_train)\npred=np.array([1 if row>=0.5 else 0 for row in pred])\nprint(classification_report(y_train,pred))\nsns.heatmap(confusion_matrix(y_train,pred),annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loss function confirms overfitting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history[\"val_loss\"])\nplt.plot(history.history[\"loss\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}