{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install pyspark\nimport os\nfrom time import time\n\nimport numpy as np\nimport pyLDAvis\nfrom nltk.corpus import stopwords\nfrom pyspark import SparkContext\nfrom pyspark.ml.clustering import LDA\nfrom pyspark.ml.feature import CountVectorizer, Tokenizer, StopWordsRemover, IDF\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import explode, size\nfrom pyspark.sql.types import StringType","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\n\n\ndef init_spark():\n    # SparkContext.setSystemProperty('spark.local.dir', '/home/dhimal/spark')\n    spark = SparkSession.builder \\\n        .master(\"local\") \\\n        .config(\"spark.executor.memory\", \"64g\") \\\n        .config(\"spark.driver.memory\", \"64g\") \\\n        .config(\"spark.memory.offHeap.enabled\", True) \\\n        .config(\"spark.memory.offHeap.size\", \"100g\") \\\n        .appName(\"hw3\") \\\n        .getOrCreate()\n    return spark\n\n\ndef read_json_files(root_path, spark, num):\n    json_dir = root_path + \"document_parses/pdf_json/\"\n    filenames = os.listdir(json_dir)\n\n    all_json = [json_dir + filename for filename in filenames]\n    all_json = all_json[:num]\n\n    data = spark.read.json(all_json, multiLine=True)\n    data.createOrReplaceTempView(\"data\")\n    return data\n\n\ndef get_body_text(spark, data):\n    body_text_only_data = spark.sql(\n        \"\"\"\n            SELECT\n                body_text.text AS body_text,\n                paper_id\n            FROM data\n            \"\"\")\n    return body_text_only_data\n\n\ndef topic_render(topic, wordNumbers, vocabArray):  # specify vector id of words to actual words\n    terms = topic[1]\n    result = []\n    for i in range(wordNumbers):\n        term = vocabArray[terms[i]]\n        result.append(term)\n    return result\n\n\ndef clean_up_sentences(sentence):\n    matches = [word for word in sentence.split(' ') if word.isalnum()]\n    matches = [word.lower() for word in matches]\n    matches = [word for word in matches if word not in stop_words]\n    matches = [word for word in matches if len(word) >= 4]\n    return matches\n\n\ndef format_data_to_pyldavis(cleaned_DataFrame, cvmodel, lda_transformed, lda_model):\n    counts = cleaned_DataFrame.select((explode(cleaned_DataFrame.filtered)).alias(\"tokens\")).groupby(\"tokens\").count()\n    wc = {i['tokens']: i['count'] for i in counts.collect()}\n    wc = [wc[x] for x in cvmodel.vocabulary]\n\n    data = {'topic_term_dists': np.array(lda_model.topicsMatrix().toArray()).T,\n            'doc_topic_dists': np.array(\n                [x.toArray() for x in lda_transformed.select([\"topicDistribution\"]).toPandas()['topicDistribution']]),\n            'doc_lengths': [x[0] for x in cleaned_DataFrame.select(size(cleaned_DataFrame.filtered)).collect()],\n            'vocab': cvmodel.vocabulary,\n            'term_frequency': wc}\n\n    return data\n\n\ndef clean_up(document):\n    cleaned = [clean_up_sentences(w) for w in document]\n    joined = [' '.join(w) for w in cleaned]\n    return joined\n\n\ndef main():\n    timeStamp = str(int(time()))\n    # todo\n    num = 100\n    #out_file_name = '../out/output-' + timeStamp + \"-\" + str(num) + '.txt'\n    #os.mkdir\n    #out_file = open(out_file_name, 'w')\n\n    start = time()\n    root_path = '../input/CORD-19-research-challenge/'\n    spark = init_spark()\n    json_files = read_json_files(root_path, spark, num)\n    data = get_body_text(spark, json_files)\n    print(\"data reading done\")\n\n    # clean the data\n    word_clean_up_F = F.udf(lambda x: clean_up(x), StringType())\n    data = data.withColumn(\"body_text_cleaned\", word_clean_up_F(\"body_text\"))\n    data = data.select(\"body_text_cleaned\")\n    print(\"data processing done\")\n\n    tokenizer = Tokenizer(inputCol=\"body_text_cleaned\", outputCol=\"words\")\n    token_DataFrame = tokenizer.transform(data)\n    token_DataFrame = token_DataFrame.select(\"words\")\n    print(\"data tokenizing done\")\n\n    # Remove stopwords\n    remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n    cleaned_DataFrame = remover.transform(token_DataFrame)\n    cleaned_DataFrame = cleaned_DataFrame.select('filtered')\n\n    # Count vectorizer\n    cv_tmp = CountVectorizer(inputCol=\"filtered\", outputCol=\"count_features\")\n    cvmodel = cv_tmp.fit(cleaned_DataFrame)\n    count_dataframe = cvmodel.transform(cleaned_DataFrame)\n    count_dataframe = count_dataframe.select('count_features')\n\n    # TF-IDF Vectorizer\n    tfidf = IDF(inputCol=\"count_features\", outputCol=\"features\")\n    tfidfmodel = tfidf.fit(count_dataframe)\n    tfidf_dataframe = tfidfmodel.transform(count_dataframe).select(\"features\")\n\n    print(\"Ready to fit with the LDA model\")\n    # Fit the LDA Model\n    num_topics = 10\n    max_iterations = 10\n    lda_start = time()\n    lda = LDA(seed=1, optimizer=\"em\", k=num_topics, maxIter=max_iterations)\n    lda_model = lda.fit(tfidf_dataframe)\n    lda_transformed = lda_model.transform(tfidf_dataframe)\n    lda_end = time()\n    print(\"LDA complete\")\n    # joblib.dump(lda_model, 'lda.csv')\n\n    # Get terms per topic\n    topics = lda_model.topicsMatrix()\n    vocabArray = cvmodel.vocabulary\n\n    wordNumbers = 15  # number of words per topic\n    topicIndices = lda_model.describeTopics(maxTermsPerTopic=wordNumbers).rdd.map(tuple)\n\n    topics_final = topicIndices.map(lambda topic: topic_render(topic, wordNumbers, vocabArray)).collect()\n\n    for topic in range(len(topics_final)):\n        print(\"Topic \" + str(topic) + \":\")\n        #print(\"Topic \" + str(topic) + \":\", file=out_file)\n        print(topics_final[topic])\n        #print(topics_final[topic], file=out_file)\n\n    print(\"Full runtime : {} min. \".format((time() - start) / 60))\n    print(\"LDA runtime : {} min. \".format((lda_end - lda_start) / 60))\n    # print(\"Check\" + out_file.name)\n\n    # cleaned_DataFrame.write.csv('cleaned_DataFrame' + timeStamp + \"-\" + str(num) + '.csv')\n    # cvmodel.save('cvmodel' + timeStamp + \"-\" + str(num) + '.csv')\n    # lda_transformed.write.csv('lda_transformed' + timeStamp + \"-\" + str(num) + '.csv')\n    # lda_model.write.csv('lda_model' + timeStamp + \"-\" + str(num) + '.csv')\n    cleaned_DataFrame.cache()\n    lda_transformed.cache()\n\n    # Data Visualization\n    data = format_data_to_pyldavis(cleaned_DataFrame, cvmodel, lda_transformed, lda_model)\n    print(\"Preparing data with pyLDAvis ...\")\n    py_lda_prepared_data = pyLDAvis.prepare(**data)\n    file_name = '../out/data-viz-' + timeStamp + '.html'\n    print(\"Saving pyLDAvis html page ...\")\n    pyLDAvis.save_html(py_lda_prepared_data, file_name)\n    # pyLDAvis.show(py_lda_prepared_data)\n    spark.stop()\n\n\nif __name__ == '__main__':\n    main()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}