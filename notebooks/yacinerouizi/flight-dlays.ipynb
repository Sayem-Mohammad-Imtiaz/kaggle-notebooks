{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\n\nairlines = pd.read_csv('../input/flight-delays/airlines.csv')\nairports = pd.read_csv('../input/flight-delays/airports.csv')\nflights = pd.read_csv('../input/flight-delays/flights.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning the Data and Get the Big Picture"},{"metadata":{"trusted":true},"cell_type":"code","source":"airlines","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airports.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flights.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flights.info(show_counts=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Drop columns that have a lot of missing values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_drop = ['CANCELLATION_REASON', 'AIR_SYSTEM_DELAY', 'SECURITY_DELAY', \n                  \"AIRLINE_DELAY\", \"LATE_AIRCRAFT_DELAY\", \"WEATHER_DELAY\"]\n\nflights = flights.drop(columns_to_drop, axis=1, errors='ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how many flights have been canceled or diverted"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(flights['DIVERTED'].value_counts())\nprint('*' * 30)\nprint(flights['CANCELLED'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Flights that are 'CANCELLED' or 'DIVERTED' should be removed because they are outliers (rare cases)"},{"metadata":{"trusted":true},"cell_type":"code","source":"flights = flights[flights['CANCELLED'] == 0]   # keep only non-concelled flights\nflights = flights[flights['DIVERTED'] == 0]    # keep only non-diverted flights\nassert len(flights) == 5819079 - 89884 - 15187 # all flights - conceled - diverted (= 5714008)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Correlations**\n\nLet's look at how much each attributes correlates with the arrival delay:"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = flights.corr()\ncorr_matrix['ARRIVAL_DELAY'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's keep only useful attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"flights = flights[['MONTH', 'DAY', 'AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', \n                    'SCHEDULED_DEPARTURE', 'DEPARTURE_TIME', 'DEPARTURE_DELAY', \n                    'SCHEDULED_ARRIVAL', 'ARRIVAL_TIME', 'ARRIVAL_DELAY', 'ELAPSED_TIME', ]]\nflights.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check for null values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"flights.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Summary of the numerical attributes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"flights.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Distribution of the arrival delay attribute**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nflights.ARRIVAL_DELAY.hist(ax=ax, bins=1000, range=(-10, 1000))\nax.set_xscale('log')\nplt.ylim(0, 150000)\nplt.xlabel('Delay (minutes)')\nplt.ylabel('Number of flights')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploration with Regard to the Mean Delay"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inspired from this kaggle kernel: https://www.kaggle.com/fabiendaniel/predicting-flight-delays-tutorial\nimport seaborn as sns \n\ndef delay_by_attribute(attribute, df=flights, figsize=(10, 7)):\n    # Delay with less than 10 min are mapped to 0 otherwise they are mapped to 1\n    delay_type = lambda x: 0 if x < 10 else 1\n    flights['DELAY_TYPE'] = flights['DEPARTURE_DELAY'].apply(delay_type)\n    \n    plt.figure(1, figsize=figsize)\n    ax = sns.countplot(y=attribute, hue='DELAY_TYPE', data=df)\n    \n    plt.xlabel('Flight count', fontsize=16, weight='bold')\n    plt.ylabel(attribute, fontsize=16, weight='bold')\n    plt.title(f'Delay by {attribute}', weight='bold')\n    L = plt.legend()\n    L.get_texts()[0].set_text('small delay (t < 10 min)')\n    L.get_texts()[1].set_text('large delay (t > 10 min)')\n    plt.grid(True)\n    plt.show()\n\ndelay_by_attribute('AIRLINE')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the proportion between small and large delay is related to the airline, for example: the airline **UA** (United Air Lines Inc.) almost 50% of their flights have a large delay, on the other hand, the airline **DL** (Delta Air Lines Inc.) ~25% of their flights have a large delay."},{"metadata":{"trusted":true},"cell_type":"code","source":"delay_by_attribute('MONTH')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"delay_by_attribute('DAY')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the correlation matrix and the plots above we can see that delays are not correlated with months nor days."},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pd.merge(flights[['ORIGIN_AIRPORT', 'DELAY_TYPE']], \n                  airports[['IATA_CODE', 'STATE']], \n                  left_on='ORIGIN_AIRPORT', right_on='IATA_CODE')\n\ndelay_by_attribute('STATE', df=result, figsize=(10, 15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Statistiques of outliers**\n\nif we consider delays of more than 10 minutes to be significant delays, than let's see how much in percentage these delays represent of the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_of_large_delays = (flights.ARRIVAL_DELAY > 10).sum()\npercent_of_large_delays = np.round(nb_of_large_delays * 100 / len(flights), 2)\nprint('There are {} flights with large delays (more than 10min), which represent {}% of the flights'\n      .format(nb_of_large_delays, percent_of_large_delays))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of flights that have more than 150min (2.5h) delay\nnb_of_rare_delays = (flights.ARRIVAL_DELAY > 150).sum()\npercent_of_rare_delays = np.round(nb_of_rare_delays * 100 / len(flights), 1)\n\n# percent of rare delays with regard to large delays\npercent_rare_large = np.round((nb_of_rare_delays * 100 / nb_of_large_delays), 1)\nprint(\n    'There are {} flights with rare delays (> 5h) which represent {}% of all flights, which also represent {}% of large delays'\n     .format(nb_of_rare_delays, percent_of_rare_delays, percent_rare_large))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <table style=\"width:50%; border: 1px solid black; border-collapse: collapse;\">\n  <tr>\n    <th></th>\n    <th>All flights</th>\n    <th>Large delays (> 10min)</th>\n    <th>Rare delays (> 150min)</th>\n  </tr>\n    <tr>\n    <td>All flights</td>\n    <td>100%</td>\n    <td>/</td>\n    <td>/</td>\n  </tr>\n  <tr>\n    <td>Large delays (> 10min)</td>\n    <td>22%</td>\n    <td>100%</td>\n    <td>/</td>\n  </tr>\n  <tr>\n    <td>Rare delays (> 150min)</td>\n    <td>1.3%</td>\n    <td>5.8%</td>\n    <td>100%</td>\n  </tr>\n</table> "},{"metadata":{},"cell_type":"markdown","source":"We can think of delays longer than 2.5 hours as outliers, so we need to remove them from the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keep only delays less than 150min\nflights = flights[flights.ARRIVAL_DELAY < 150]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split the Dataset"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"flights['ORIGIN_AIRPORT'] = flights['ORIGIN_AIRPORT'].astype(str)\nflights['DESTINATION_AIRPORT'] = flights['DESTINATION_AIRPORT'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(flights, test_size=0.2, random_state=42)\n\nX_train = train_set.drop(\"ARRIVAL_DELAY\", axis=1)\ny_train = train_set['ARRIVAL_DELAY'].copy()\n\nX_test = test_set.drop('ARRIVAL_DELAY', axis=1)\ny_test = test_set['ARRIVAL_DELAY'].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoding Categorical Attributes"},{"metadata":{},"cell_type":"markdown","source":"I chose embedding to convert categorical attributes to numbers: ordinal encoding is not good since the model will assume that two nearby values are more familiar than to distant values, which is obviously not the case for our categorical attributes. Also, our categorical attributes have a large number of categories (629 for airports), so one-hot encoding will result in a large number of features (more than 1000). This will affect performance and training."},{"metadata":{"trusted":true},"cell_type":"code","source":"dest_airport = X_train['DESTINATION_AIRPORT'].unique()\norig_airport = X_train['ORIGIN_AIRPORT'].unique()\nairports = np.unique(np.hstack([dest_airport, orig_airport]))\nairlines = X_train['AIRLINE'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nimport tensorflow as tf \n\nclass EmbeddingTransorfmer(BaseEstimator, TransformerMixin):\n    def __init__(self, vocab, n_oov_buckets=10, embedding_dim=10):\n        self.n_oov_buckets = n_oov_buckets\n        self.vocab = vocab\n        self.embedding_dim = embedding_dim\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        vocab = tf.constant(self.vocab)\n        indices = tf.range(len(vocab), dtype=tf.int64)\n        table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n        table = tf.lookup.StaticVocabularyTable(table_init, self.n_oov_buckets)\n        \n        embedding_dim = self.embedding_dim\n        embedding_matrix = tf.random.uniform([len(vocab) + self.n_oov_buckets, embedding_dim])\n        cat_indices = table.lookup(tf.constant(X))\n        return tf.nn.embedding_lookup(embedding_matrix, cat_indices).numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing the Input Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\nnum_pipeline = Pipeline([\n    ('std_scaler', StandardScaler()),\n])\n\nX_train_num = X_train.drop(['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT'], axis=1, errors='ignore')\nnum_attribs = list(X_train_num)\n\nfull_pipeline = ColumnTransformer([\n    ('num', num_pipeline, num_attribs),\n    ('cat1', EmbeddingTransorfmer(airports), 'ORIGIN_AIRPORT'),\n    ('cat2', EmbeddingTransorfmer(airports), 'DESTINATION_AIRPORT'),\n    ('cat3', EmbeddingTransorfmer(airlines), 'AIRLINE')\n])\n\nX_train_prepared = full_pipeline.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training and Evaluation"},{"metadata":{},"cell_type":"markdown","source":"## Ridge Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n\nridge_reg = Ridge(alpha=0.01, random_state=42)\nridge_reg.fit(X_train_prepared, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try to predict some data"},{"metadata":{"trusted":true},"cell_type":"code","source":"some_data = X_train.iloc[:5]\nsome_labels = y_train.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\", ridge_reg.predict(some_data_prepared))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Labels:\", list(some_labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the predictions are not exactly accurate! Let's measure the RMSE on the whole training set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = ridge_reg.predict(X_train_prepared)\nridge_mse = mean_squared_error(y_train, predictions)\nridge_rmse = np.sqrt(ridge_mse)\n'{:.2f} min'.format(ridge_rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The typical error the model make is 12.37min which is not very good. \n\nUnfortunatly, I couldn't use other models because training took too long, so let's try fine-tuning the ridge regression model."},{"metadata":{},"cell_type":"markdown","source":"## Fine Tuning Using Grid Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'alpha': [0.01, 0.1, 1, 10, 100]},\n    {'solver': ['cholesky', 'lsqr']}\n]\n\ngrid_search = GridSearchCV(ridge_reg, param_grid, cv=4,\n                          scoring='neg_mean_squared_error',\n                          return_train_score=True,\n                          verbose=2)\ngrid_search.fit(X_train_prepared, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sqrt(-grid_search.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation on the Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = grid_search.best_estimator_\n\nX_test_prepared = full_pipeline.transform(X_test)\ntest_predictions = model.predict(X_test_prepared)\ntest_mse = mean_squared_error(y_test, test_predictions)\ntest_rmse = np.sqrt(test_mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# References\n\n* [Predicting flight delays [Tutorial]](https://www.kaggle.com/fabiendaniel/predicting-flight-delays-tutorial)\n* [Flight_Delay_Prediction](https://www.kaggle.com/hrishikeshmalkar/flight-delay-prediction)\n* https://github.com/Djinny/Formation-Data-Scientist/tree/master/Pr%C3%A9dire%20le%20retard%20d'avions\n* https://github.com/xmontamat/OC_DataScience/tree/master/OC_Project4_Flights_delay"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}