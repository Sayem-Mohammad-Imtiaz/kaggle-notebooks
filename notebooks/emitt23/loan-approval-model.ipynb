{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loan Approval Model","metadata":{}},{"cell_type":"markdown","source":"**Purpose:** The purpose of this notebook is to work with machine learning techniques to make the best model for approving or denying loan applications.\n\n**Result:** Able to produce a model with approximately 80% accuracy.","metadata":{}},{"cell_type":"markdown","source":"## Table of Contents\n\n* **[Analyzing the Data](#Analyzing-the-Data)**\n* **[Cleaning the Data](#Cleaning-the-Data)**\n* **[Modeling the Data](#Modeling-the-Data)**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:00.709476Z","iopub.execute_input":"2021-05-20T16:45:00.709924Z","iopub.status.idle":"2021-05-20T16:45:00.73424Z","shell.execute_reply.started":"2021-05-20T16:45:00.709823Z","shell.execute_reply":"2021-05-20T16:45:00.733129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ntest = pd.read_csv('/kaggle/input/loan-prediction-problem-dataset/test_Y3wMUE5_7gLdaTN.csv')\ntrain = pd.read_csv('/kaggle/input/loan-prediction-problem-dataset/train_u6lujuX_CVtuZ9i.csv')\ntrain_original = train.copy()\ntest_original = test.copy()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:00.735526Z","iopub.execute_input":"2021-05-20T16:45:00.736002Z","iopub.status.idle":"2021-05-20T16:45:01.718903Z","shell.execute_reply.started":"2021-05-20T16:45:00.735967Z","shell.execute_reply":"2021-05-20T16:45:01.718004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analyzing the Data","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:01.723152Z","iopub.execute_input":"2021-05-20T16:45:01.723499Z","iopub.status.idle":"2021-05-20T16:45:01.755063Z","shell.execute_reply.started":"2021-05-20T16:45:01.723467Z","shell.execute_reply":"2021-05-20T16:45:01.754254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plt.hist(train['ApplicantIncome']) #Will need to do a log transformation on this column\n#plt.hist(train['CoapplicantIncome']) #Will need to do a log transformation on this column\n#plt.hist(train['LoanAmount']) #Will need to do a log transformation on this column\n#plt.hist(train['Loan_Amount_Term'])\n#plt.hist(train['Credit_History'])","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:01.756287Z","iopub.execute_input":"2021-05-20T16:45:01.756721Z","iopub.status.idle":"2021-05-20T16:45:01.759916Z","shell.execute_reply.started":"2021-05-20T16:45:01.756688Z","shell.execute_reply":"2021-05-20T16:45:01.759057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Will need to apply log transformations on the test columns ApplicantIncome, CoapplicantIncome and LoanAmount.","metadata":{}},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:01.761048Z","iopub.execute_input":"2021-05-20T16:45:01.76144Z","iopub.status.idle":"2021-05-20T16:45:01.811652Z","shell.execute_reply.started":"2021-05-20T16:45:01.76141Z","shell.execute_reply":"2021-05-20T16:45:01.810737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have some missing data in LoanAmount, Loan_Amount_Term and Credit_History that will have to get cleaned up. ","metadata":{}},{"cell_type":"code","source":"plt.title('Genders')\ntrain['Gender'].value_counts().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:01.812672Z","iopub.execute_input":"2021-05-20T16:45:01.813338Z","iopub.status.idle":"2021-05-20T16:45:01.996782Z","shell.execute_reply.started":"2021-05-20T16:45:01.8133Z","shell.execute_reply":"2021-05-20T16:45:01.995873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Working with what looks like about three times as many Males than Females in the train dataset.","metadata":{}},{"cell_type":"code","source":"plt.title('Marital Status')\ntrain['Married'].value_counts().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:01.998041Z","iopub.execute_input":"2021-05-20T16:45:01.99863Z","iopub.status.idle":"2021-05-20T16:45:02.142389Z","shell.execute_reply.started":"2021-05-20T16:45:01.998571Z","shell.execute_reply":"2021-05-20T16:45:02.1413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Married is essentially a boolean column split up into yes and no answers. Looks like more of the train dataset includes people who are married.","metadata":{}},{"cell_type":"code","source":"plt.title('Dependents')\ntrain['Dependents'].value_counts().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:02.145952Z","iopub.execute_input":"2021-05-20T16:45:02.146449Z","iopub.status.idle":"2021-05-20T16:45:02.296155Z","shell.execute_reply.started":"2021-05-20T16:45:02.1464Z","shell.execute_reply":"2021-05-20T16:45:02.295072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dependents column looks to be a column illustrating the amount of people that an independent claims. Most of the independents applying for loans look to have 0 dependents.","metadata":{}},{"cell_type":"code","source":"plt.title('Education')\ntrain['Education'].value_counts().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:02.297983Z","iopub.execute_input":"2021-05-20T16:45:02.298267Z","iopub.status.idle":"2021-05-20T16:45:02.43581Z","shell.execute_reply.started":"2021-05-20T16:45:02.298237Z","shell.execute_reply":"2021-05-20T16:45:02.434662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the education column what we see is a little vague. No data description is given of this column. We see that there are more graduates than not, but we are not sure if this means people that graduate high school or university. The assumption would be that a graduate would mean from university, but we cannot be certain.","metadata":{}},{"cell_type":"code","source":"plt.title('Self-Employed')\ntrain['Self_Employed'].value_counts().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:02.437112Z","iopub.execute_input":"2021-05-20T16:45:02.437431Z","iopub.status.idle":"2021-05-20T16:45:02.561306Z","shell.execute_reply.started":"2021-05-20T16:45:02.437402Z","shell.execute_reply":"2021-05-20T16:45:02.560306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This column is straight forward and it looks like most loan applicants are not self-employed.","metadata":{}},{"cell_type":"code","source":"plt.title('Property Area')\ntrain['Property_Area'].value_counts().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:02.562783Z","iopub.execute_input":"2021-05-20T16:45:02.563086Z","iopub.status.idle":"2021-05-20T16:45:02.692869Z","shell.execute_reply.started":"2021-05-20T16:45:02.563055Z","shell.execute_reply":"2021-05-20T16:45:02.691809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We seem to have a pretty equal distribution of property area, semiurban being the most popular area.","metadata":{}},{"cell_type":"code","source":"train['Loan_Status'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:07.428574Z","iopub.execute_input":"2021-05-20T16:45:07.42898Z","iopub.status.idle":"2021-05-20T16:45:07.436939Z","shell.execute_reply.started":"2021-05-20T16:45:07.428945Z","shell.execute_reply":"2021-05-20T16:45:07.436035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our last and most important column for our machine learning is the loan status column. This is the column that we are looking too predict. From here it looks like a good majority of applicants in the train dataset got there loan application approved.","metadata":{}},{"cell_type":"markdown","source":"From what is seen above it might be helpful to convert some of these columns to boolean. However we will do some categorical encoding, so it might be better to just leave as is.","metadata":{}},{"cell_type":"markdown","source":"## Cleaning the Data","metadata":{}},{"cell_type":"markdown","source":"For cleaning the data there are a couple of tasks that will need to be done in order to make sure that the model is running the best it can.\n* Clean up the missing values\n* Check for any duplicates\n* Make a separate train dataset where the columns that needed a logistic regression earlier are modified.","metadata":{}},{"cell_type":"markdown","source":"### Cleaning Missing Values","metadata":{}},{"cell_type":"markdown","source":"So the process does not have to be done twice for train and test, the datasets will be merged while it is cleaned and then split back up after the data is put together like wanted.","metadata":{}},{"cell_type":"code","source":"ntrain = train.shape[0]\nntest = test.shape[0]\ny = train.Loan_Status.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['Loan_Status'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))\n\n\n#Credit to: https://stackoverflow.com/questions/18172851/deleting-dataframe-row-in-pandas-based-on-column-value for helping me understand the correct way to drop my columns","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:02.823545Z","iopub.execute_input":"2021-05-20T16:45:02.823872Z","iopub.status.idle":"2021-05-20T16:45:02.836256Z","shell.execute_reply.started":"2021-05-20T16:45:02.823839Z","shell.execute_reply":"2021-05-20T16:45:02.835105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have a dataset with the combination of rows for train and test and the loan_status column dropped.","metadata":{}},{"cell_type":"code","source":"#Checking percentage of data that is na\nall_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(7)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:02.837723Z","iopub.execute_input":"2021-05-20T16:45:02.838061Z","iopub.status.idle":"2021-05-20T16:45:02.981039Z","shell.execute_reply.started":"2021-05-20T16:45:02.83803Z","shell.execute_reply":"2021-05-20T16:45:02.979945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because we have such a low percentage of missing values in each row it can be seen that none of these rows will need to be dropped, just cleaned up.","metadata":{}},{"cell_type":"code","source":"all_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:02.982448Z","iopub.execute_input":"2021-05-20T16:45:02.982824Z","iopub.status.idle":"2021-05-20T16:45:03.002507Z","shell.execute_reply.started":"2021-05-20T16:45:02.982791Z","shell.execute_reply":"2021-05-20T16:45:03.001214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data.LoanAmount.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:03.004465Z","iopub.execute_input":"2021-05-20T16:45:03.004977Z","iopub.status.idle":"2021-05-20T16:45:03.023298Z","shell.execute_reply.started":"2021-05-20T16:45:03.00493Z","shell.execute_reply":"2021-05-20T16:45:03.021982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above I just wanted to quickly confirm that zero was not being counted as an NaN value or that No's were considered NaN's. As seen with the describe for Loan Amount column the lowest amount that there is, is nine which means that zeros are not apart of our range of values.","metadata":{}},{"cell_type":"markdown","source":"#### Cleaning Numeric Variables","metadata":{}},{"cell_type":"markdown","source":"Now that we are setup to start cleaning the missing data we will split up the data into numeric and categorical variables.","metadata":{}},{"cell_type":"code","source":"Numeric_Columns = all_data.select_dtypes(include=np.number) #Creating dataset for just the numeric columns\nNumeric_Columns.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:03.02466Z","iopub.execute_input":"2021-05-20T16:45:03.025159Z","iopub.status.idle":"2021-05-20T16:45:03.043406Z","shell.execute_reply.started":"2021-05-20T16:45:03.025123Z","shell.execute_reply":"2021-05-20T16:45:03.04211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For this part of the process I am going to use simple imputer to quickly and effectively deal with my numeric missing data","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nmy_imputer = SimpleImputer()\nimputed_num = pd.DataFrame(my_imputer.fit_transform(Numeric_Columns)) #Using simple imputer to fill in missing values\n\nimputed_num.columns = Numeric_Columns.columns #Making column headings the same for imputed_num as Numeric_Columns\nidentification = all_data['Loan_ID'] #Making a column for Loan_ID to merge numerical and categorical data\nimputed_num = imputed_num.join(identification)\nimputed_num.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:03.045117Z","iopub.execute_input":"2021-05-20T16:45:03.04554Z","iopub.status.idle":"2021-05-20T16:45:03.497314Z","shell.execute_reply.started":"2021-05-20T16:45:03.045493Z","shell.execute_reply":"2021-05-20T16:45:03.496028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Cleaning Categorical Variables","metadata":{}},{"cell_type":"markdown","source":"For cleaning the categorical variables I am going to use get_dummies to effectively clean. This will also prove effective for my model.","metadata":{}},{"cell_type":"code","source":"Categorical_Columns = all_data.select_dtypes(exclude=np.number)\nCategorical_Columns.drop(['Loan_ID'], axis=1, inplace=True)\nCategorical_Columns.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:03.498596Z","iopub.execute_input":"2021-05-20T16:45:03.498937Z","iopub.status.idle":"2021-05-20T16:45:03.523722Z","shell.execute_reply.started":"2021-05-20T16:45:03.498903Z","shell.execute_reply":"2021-05-20T16:45:03.522607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imputed_categorical = pd.get_dummies(data=Categorical_Columns) #Using get_dummies to clean up categorical data\nimputed_categorical.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:03.525372Z","iopub.execute_input":"2021-05-20T16:45:03.525816Z","iopub.status.idle":"2021-05-20T16:45:03.552886Z","shell.execute_reply.started":"2021-05-20T16:45:03.525767Z","shell.execute_reply":"2021-05-20T16:45:03.551626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imputed_categorical = imputed_categorical.join(identification)\nimputed_categorical.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:03.554492Z","iopub.execute_input":"2021-05-20T16:45:03.554978Z","iopub.status.idle":"2021-05-20T16:45:03.574705Z","shell.execute_reply.started":"2021-05-20T16:45:03.554933Z","shell.execute_reply":"2021-05-20T16:45:03.573701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Merging Numerical Dataset and Categorical Dataset Together","metadata":{}},{"cell_type":"code","source":"merged_data = imputed_num.merge(imputed_categorical, on='Loan_ID') #Using similar id columns to merge data\nmerged_data.drop(['Loan_ID'], axis=1, inplace=True) #Need to drop Loan_ID now that we have successfully merged our data together\nmerged_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:03.576224Z","iopub.execute_input":"2021-05-20T16:45:03.576629Z","iopub.status.idle":"2021-05-20T16:45:03.616664Z","shell.execute_reply.started":"2021-05-20T16:45:03.576585Z","shell.execute_reply":"2021-05-20T16:45:03.615502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data_nan = (merged_data.isnull().sum() / len(merged_data)) * 100\nall_data_nan = all_data_nan.drop(all_data_nan[all_data_nan == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_nan})\nmissing_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:03.622367Z","iopub.execute_input":"2021-05-20T16:45:03.622683Z","iopub.status.idle":"2021-05-20T16:45:03.637009Z","shell.execute_reply.started":"2021-05-20T16:45:03.622651Z","shell.execute_reply":"2021-05-20T16:45:03.635666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As can be seen above we no longer have any missing values. We are going to go ahead and split our data back up into our train and test datasets and then do the same process and make a separate dataset that contains our logarithmic columns.","metadata":{}},{"cell_type":"code","source":"X_tra = merged_data[:ntrain]\nX_test = merged_data[ntrain:]\nX_tra.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:03.639799Z","iopub.execute_input":"2021-05-20T16:45:03.640223Z","iopub.status.idle":"2021-05-20T16:45:03.657069Z","shell.execute_reply.started":"2021-05-20T16:45:03.640181Z","shell.execute_reply":"2021-05-20T16:45:03.655822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logarithmic Cleaning","metadata":{}},{"cell_type":"code","source":"log_train = train_original\nlog_test = test_original\nlog_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:03.658431Z","iopub.execute_input":"2021-05-20T16:45:03.659031Z","iopub.status.idle":"2021-05-20T16:45:03.688017Z","shell.execute_reply.started":"2021-05-20T16:45:03.658983Z","shell.execute_reply":"2021-05-20T16:45:03.68686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The columns that we are replacing with logarithm values are:\n* ApplicantIncome\n* CoapplicantIncome\n* LoanAmount\n\nFirst we will start again by merging the log_train and log_test and then perform the transformations.","metadata":{}},{"cell_type":"code","source":"log_train.drop(log_train.loc[log_train['ApplicantIncome']==0].index, inplace=True)\nlog_train.drop(log_train.loc[log_train['CoapplicantIncome']==0].index, inplace=True)\nlog_train.drop(log_train.loc[log_train['LoanAmount']==0].index, inplace=True)\n\nlog_test.drop(log_test.loc[log_test['ApplicantIncome']==0].index, inplace=True)\nlog_test.drop(log_test.loc[log_test['CoapplicantIncome']==0].index, inplace=True)\nlog_test.drop(log_test.loc[log_test['LoanAmount']==0].index, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:03.689713Z","iopub.execute_input":"2021-05-20T16:45:03.690312Z","iopub.status.idle":"2021-05-20T16:45:03.710391Z","shell.execute_reply.started":"2021-05-20T16:45:03.690264Z","shell.execute_reply":"2021-05-20T16:45:03.70931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlogtrain = log_train.shape[0]\nnlogtest = log_test.shape[0]\ny_log = log_train.Loan_Status.values\nall_data_log = pd.concat((log_train, log_test)).reset_index(drop=True)\nall_data_log.drop(['Loan_Status'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data_log.shape))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:03.711693Z","iopub.execute_input":"2021-05-20T16:45:03.711995Z","iopub.status.idle":"2021-05-20T16:45:03.722792Z","shell.execute_reply.started":"2021-05-20T16:45:03.711967Z","shell.execute_reply":"2021-05-20T16:45:03.721639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The log transformation will not be insightful if we have values equal to zero, so if that is the case we will need to drop them.","metadata":{}},{"cell_type":"code","source":"#Perform log transformations\nall_data_log['ApplicantIncome_log'] = np.log(all_data_log['ApplicantIncome'])\nall_data_log['CoapplicantIncome_log'] = np.log(all_data_log['CoapplicantIncome'])\nall_data_log['LoanAmount_log'] = np.log(all_data_log['LoanAmount'])\nall_data_log.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:03.724381Z","iopub.execute_input":"2021-05-20T16:45:03.724688Z","iopub.status.idle":"2021-05-20T16:45:03.749584Z","shell.execute_reply.started":"2021-05-20T16:45:03.724659Z","shell.execute_reply":"2021-05-20T16:45:03.748537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data_log.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:03.751105Z","iopub.execute_input":"2021-05-20T16:45:03.751531Z","iopub.status.idle":"2021-05-20T16:45:03.766298Z","shell.execute_reply.started":"2021-05-20T16:45:03.751486Z","shell.execute_reply":"2021-05-20T16:45:03.765374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have a lot less rows to work with, but we are able to get log transformations that will be useful. Let's verify that we have a approximately normal distribution now by taking a look at the plots.","metadata":{}},{"cell_type":"code","source":"plt.hist(all_data_log['ApplicantIncome_log']) ","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:03.767735Z","iopub.execute_input":"2021-05-20T16:45:03.768163Z","iopub.status.idle":"2021-05-20T16:45:03.922304Z","shell.execute_reply.started":"2021-05-20T16:45:03.76812Z","shell.execute_reply":"2021-05-20T16:45:03.921087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(all_data_log['CoapplicantIncome_log']) ","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:03.923685Z","iopub.execute_input":"2021-05-20T16:45:03.924113Z","iopub.status.idle":"2021-05-20T16:45:04.083801Z","shell.execute_reply.started":"2021-05-20T16:45:03.924064Z","shell.execute_reply":"2021-05-20T16:45:04.08266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(all_data_log['LoanAmount_log'])","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:04.085198Z","iopub.execute_input":"2021-05-20T16:45:04.085485Z","iopub.status.idle":"2021-05-20T16:45:04.238354Z","shell.execute_reply.started":"2021-05-20T16:45:04.085456Z","shell.execute_reply":"2021-05-20T16:45:04.237309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can verify that these are now approximately normally distributed. From here we will go ahead and drop the original columns associated with the log transformations and then follow the same steps as with the all_data dataset with how we will clean.","metadata":{}},{"cell_type":"code","source":"all_data_log.drop(['ApplicantIncome'], axis=1, inplace=True)\nall_data_log.drop(['CoapplicantIncome'], axis=1, inplace=True)\nall_data_log.drop(['LoanAmount'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:04.239527Z","iopub.execute_input":"2021-05-20T16:45:04.239806Z","iopub.status.idle":"2021-05-20T16:45:04.247298Z","shell.execute_reply.started":"2021-05-20T16:45:04.239778Z","shell.execute_reply":"2021-05-20T16:45:04.246577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data_log.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:04.248161Z","iopub.execute_input":"2021-05-20T16:45:04.248417Z","iopub.status.idle":"2021-05-20T16:45:04.278292Z","shell.execute_reply.started":"2021-05-20T16:45:04.248391Z","shell.execute_reply":"2021-05-20T16:45:04.277384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Numeric_Columns_log = all_data_log.select_dtypes(include=np.number) #Creating dataset for just the numeric columns\nNumeric_Columns_log.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:04.279556Z","iopub.execute_input":"2021-05-20T16:45:04.279858Z","iopub.status.idle":"2021-05-20T16:45:04.299804Z","shell.execute_reply.started":"2021-05-20T16:45:04.279827Z","shell.execute_reply":"2021-05-20T16:45:04.298469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imputed_num_log = pd.DataFrame(my_imputer.fit_transform(Numeric_Columns_log)) #Using simple imputer to fill in missing values\n\nimputed_num_log.columns = Numeric_Columns_log.columns #Making column headings the same for imputed_num as Numeric_Columns\nidentification_log = all_data_log['Loan_ID'] #Making a column for Loan_ID to merge numerical and categorical data\nimputed_num_log = imputed_num_log.join(identification_log)\nimputed_num_log.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:04.301293Z","iopub.execute_input":"2021-05-20T16:45:04.301724Z","iopub.status.idle":"2021-05-20T16:45:04.330218Z","shell.execute_reply.started":"2021-05-20T16:45:04.301677Z","shell.execute_reply":"2021-05-20T16:45:04.32907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Categorical_Columns_log = all_data_log.select_dtypes(exclude=np.number)\nCategorical_Columns_log.drop(['Loan_ID'], axis=1, inplace=True)\nCategorical_Columns_log.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:04.331958Z","iopub.execute_input":"2021-05-20T16:45:04.332379Z","iopub.status.idle":"2021-05-20T16:45:04.351899Z","shell.execute_reply.started":"2021-05-20T16:45:04.332333Z","shell.execute_reply":"2021-05-20T16:45:04.350799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imputed_categorical_log = pd.get_dummies(data=Categorical_Columns_log) #Using get_dummies to clean up categorical data\nimputed_categorical_log.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:04.35347Z","iopub.execute_input":"2021-05-20T16:45:04.353894Z","iopub.status.idle":"2021-05-20T16:45:04.379334Z","shell.execute_reply.started":"2021-05-20T16:45:04.353848Z","shell.execute_reply":"2021-05-20T16:45:04.378326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imputed_categorical_log = imputed_categorical_log.join(identification_log)\nimputed_categorical_log.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:04.380596Z","iopub.execute_input":"2021-05-20T16:45:04.380901Z","iopub.status.idle":"2021-05-20T16:45:04.3997Z","shell.execute_reply.started":"2021-05-20T16:45:04.380871Z","shell.execute_reply":"2021-05-20T16:45:04.398774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_data_log = imputed_num_log.merge(imputed_categorical_log, on='Loan_ID') #Using similar id columns to merge data\nmerged_data_log.drop(['Loan_ID'], axis=1, inplace=True) #Need to drop Loan_ID now that we have successfully merged our data together\nmerged_data_log.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:04.401117Z","iopub.execute_input":"2021-05-20T16:45:04.40153Z","iopub.status.idle":"2021-05-20T16:45:04.426794Z","shell.execute_reply.started":"2021-05-20T16:45:04.401483Z","shell.execute_reply":"2021-05-20T16:45:04.426059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data_log_nan = (merged_data_log.isnull().sum() / len(merged_data_log)) * 100\nall_data_log_nan = all_data_log_nan.drop(all_data_log_nan[all_data_log_nan == 0].index).sort_values(ascending=False)[:30]\nmissing_data_log = pd.DataFrame({'Missing Ratio' :all_data_log_nan})\nmissing_data_log.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:04.42771Z","iopub.execute_input":"2021-05-20T16:45:04.428128Z","iopub.status.idle":"2021-05-20T16:45:04.439571Z","shell.execute_reply.started":"2021-05-20T16:45:04.428098Z","shell.execute_reply":"2021-05-20T16:45:04.438877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_tra_log = merged_data[:nlogtrain]\nX_test_log = merged_data[nlogtrain:]\nX_tra_log.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:04.440471Z","iopub.execute_input":"2021-05-20T16:45:04.440881Z","iopub.status.idle":"2021-05-20T16:45:04.462665Z","shell.execute_reply.started":"2021-05-20T16:45:04.440846Z","shell.execute_reply":"2021-05-20T16:45:04.461628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling the Data","metadata":{}},{"cell_type":"markdown","source":"Now we are into the fun part working with building the best model we can for this data. A couple methods will be done. We are going to be using train_test_split and trying a couple different models like LogisticRegression, DecisionTreeClassifier, RandomForest and XGBoost. We will start with our original dataset and then like with the cleaning do the same methods with our log transformation dataset to see if that improved our model at all.","metadata":{}},{"cell_type":"markdown","source":"### Base Modeling with train_test_split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_cv, y_train, y_cv = train_test_split(X_tra, y, test_size =.3)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:04.464335Z","iopub.execute_input":"2021-05-20T16:45:04.464772Z","iopub.status.idle":"2021-05-20T16:45:04.47872Z","shell.execute_reply.started":"2021-05-20T16:45:04.464699Z","shell.execute_reply":"2021-05-20T16:45:04.477658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlog_model = LogisticRegression()\nlog_model.fit(X_train, y_train)\nLogisticRegression()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:04.480885Z","iopub.execute_input":"2021-05-20T16:45:04.481191Z","iopub.status.idle":"2021-05-20T16:45:04.548871Z","shell.execute_reply.started":"2021-05-20T16:45:04.481161Z","shell.execute_reply":"2021-05-20T16:45:04.54786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nlog_pred_cv = log_model.predict(X_cv)\naccuracy_score(y_cv, log_pred_cv)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:04.550125Z","iopub.execute_input":"2021-05-20T16:45:04.550411Z","iopub.status.idle":"2021-05-20T16:45:04.5607Z","shell.execute_reply.started":"2021-05-20T16:45:04.550382Z","shell.execute_reply":"2021-05-20T16:45:04.559789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like our first model is about 80-85% accurate which is not bad at all, but we will work to do see if a better model can be achieved starting with a DecisionTreeClassifier.","metadata":{}},{"cell_type":"code","source":"from sklearn import tree\n\ntree_model = tree.DecisionTreeClassifier()\ntree_model.fit(X_train, y_train)\ntree.DecisionTreeClassifier()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:04.561971Z","iopub.execute_input":"2021-05-20T16:45:04.562284Z","iopub.status.idle":"2021-05-20T16:45:04.61414Z","shell.execute_reply.started":"2021-05-20T16:45:04.562255Z","shell.execute_reply":"2021-05-20T16:45:04.613009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tree_pred_cv = tree_model.predict(X_cv)\naccuracy_score(y_cv, tree_pred_cv)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:04.615326Z","iopub.execute_input":"2021-05-20T16:45:04.615605Z","iopub.status.idle":"2021-05-20T16:45:04.624505Z","shell.execute_reply.started":"2021-05-20T16:45:04.615578Z","shell.execute_reply":"2021-05-20T16:45:04.623526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not as accurate as the logistic regression model, but we will see how a random forest classifier handles the dataset.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nforest_model = RandomForestClassifier()\nforest_model.fit(X_train, y_train)\nRandomForestClassifier()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:04.625711Z","iopub.execute_input":"2021-05-20T16:45:04.626056Z","iopub.status.idle":"2021-05-20T16:45:04.873638Z","shell.execute_reply.started":"2021-05-20T16:45:04.626025Z","shell.execute_reply":"2021-05-20T16:45:04.87263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forest_pred_cv = forest_model.predict(X_cv)\naccuracy_score(y_cv, forest_pred_cv)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:04.874921Z","iopub.execute_input":"2021-05-20T16:45:04.875217Z","iopub.status.idle":"2021-05-20T16:45:04.897463Z","shell.execute_reply.started":"2021-05-20T16:45:04.875188Z","shell.execute_reply":"2021-05-20T16:45:04.896562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This model did better than the decision tree, but still not as good as the logistic regression. We will try one more method which is XGBoost.","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxgb_model = XGBClassifier(n_estimators=50, max_depth=4)\nxgb_model.fit(X_train, y_train)\nXGBClassifier()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:04.898577Z","iopub.execute_input":"2021-05-20T16:45:04.898889Z","iopub.status.idle":"2021-05-20T16:45:05.046109Z","shell.execute_reply.started":"2021-05-20T16:45:04.898861Z","shell.execute_reply":"2021-05-20T16:45:05.045344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_pred_cv = xgb_model.predict(X_cv)\naccuracy_score(y_cv, xgb_pred_cv)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:05.047108Z","iopub.execute_input":"2021-05-20T16:45:05.047498Z","iopub.status.idle":"2021-05-20T16:45:05.064124Z","shell.execute_reply.started":"2021-05-20T16:45:05.047467Z","shell.execute_reply":"2021-05-20T16:45:05.063226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our xgb did not do as well as the logistic regression model, so we will go ahead and just try to beef up the logistic regression model the best it can be. This starts with seeing if our log transformations helped the model.","metadata":{}},{"cell_type":"markdown","source":"### Log Dataset Modeling","metadata":{}},{"cell_type":"code","source":"X_train_log, X_cv_log, y_train_log, y_cv_log = train_test_split(X_tra_log, y_log, test_size =.3)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:05.067942Z","iopub.execute_input":"2021-05-20T16:45:05.068305Z","iopub.status.idle":"2021-05-20T16:45:05.074996Z","shell.execute_reply.started":"2021-05-20T16:45:05.068272Z","shell.execute_reply":"2021-05-20T16:45:05.073805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_model.fit(X_train_log, y_train_log)\nLogisticRegression()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:05.078245Z","iopub.execute_input":"2021-05-20T16:45:05.078693Z","iopub.status.idle":"2021-05-20T16:45:05.102893Z","shell.execute_reply.started":"2021-05-20T16:45:05.078661Z","shell.execute_reply":"2021-05-20T16:45:05.101969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_pred_cv_log = log_model.predict(X_cv_log)\naccuracy_score(y_cv_log, log_pred_cv_log)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:05.104163Z","iopub.execute_input":"2021-05-20T16:45:05.104596Z","iopub.status.idle":"2021-05-20T16:45:05.115821Z","shell.execute_reply.started":"2021-05-20T16:45:05.10454Z","shell.execute_reply":"2021-05-20T16:45:05.114556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As it appears the making of new columns for log transformations was not very influential for finding a good model.","metadata":{}},{"cell_type":"markdown","source":"#### Final Model","metadata":{}},{"cell_type":"markdown","source":"It seems like our basic logistic regression model actually appeared to do the best for this dataset. Before calling it quits a few things will be checked out to see if any slight improvements can be made to our Logistic Regression model.","metadata":{}},{"cell_type":"code","source":"#Credit for ideas on improvement: https://stackoverflow.com/questions/38077190/how-to-increase-the-model-accuracy-of-logistic-regression-in-scikit-python\n#Credit for ideas on improvement: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n\n#log_model = LogisticRegression()\n#log_model.fit(X_train, y_train)\n#LogisticRegression()\n\n#log_pred_cv = log_model.predict(X_cv)\n#accuracy_score(y_cv, log_pred_cv)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:05.117185Z","iopub.execute_input":"2021-05-20T16:45:05.117495Z","iopub.status.idle":"2021-05-20T16:45:05.126245Z","shell.execute_reply.started":"2021-05-20T16:45:05.117467Z","shell.execute_reply":"2021-05-20T16:45:05.125238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_model1 = LogisticRegression(random_state=42)\nlog_model1.fit(X_train, y_train)\n\nlog_pred_cv1 = log_model1.predict(X_cv)\naccuracy_score(y_cv, log_pred_cv1)\n#We get about the same results","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:05.127659Z","iopub.execute_input":"2021-05-20T16:45:05.12818Z","iopub.status.idle":"2021-05-20T16:45:05.18279Z","shell.execute_reply.started":"2021-05-20T16:45:05.128148Z","shell.execute_reply":"2021-05-20T16:45:05.181842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_model2 = LogisticRegression(random_state=42, solver='lbfgs')\nlog_model2.fit(X_train, y_train)\n\nlog_pred_cv2 = log_model2.predict(X_cv)\naccuracy_score(y_cv, log_pred_cv2)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:05.184174Z","iopub.execute_input":"2021-05-20T16:45:05.184579Z","iopub.status.idle":"2021-05-20T16:45:05.235038Z","shell.execute_reply.started":"2021-05-20T16:45:05.184535Z","shell.execute_reply":"2021-05-20T16:45:05.233784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Our final model will be our log_model2 and we want to now use it to predict based on the information in our test dataset\nfinal_model = log_model2\npred_test = final_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:05.236838Z","iopub.execute_input":"2021-05-20T16:45:05.237371Z","iopub.status.idle":"2021-05-20T16:45:05.250006Z","shell.execute_reply.started":"2021-05-20T16:45:05.237322Z","shell.execute_reply":"2021-05-20T16:45:05.248723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Make new dataframe for our identification and loan_status predictions\nsubmission = pd.DataFrame()\n\n#Putting data into submission dataframe\nsubmission['Loan_Status'] = pred_test\nsubmission['Loan_ID'] = test_original['Loan_ID']\n\n#Makes more sense to put in terms of 'Y' and 'N' instead of 1 and 0 when talking about a loan being approved\nsubmission['Loan_Status'].replace(0, 'N', inplace=True)\nsubmission['Loan_Status'].replace(1, 'Y', inplace=True)\n\nsubmission.Loan_Status.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:45:05.251066Z","iopub.execute_input":"2021-05-20T16:45:05.251342Z","iopub.status.idle":"2021-05-20T16:45:05.270861Z","shell.execute_reply.started":"2021-05-20T16:45:05.251316Z","shell.execute_reply":"2021-05-20T16:45:05.269881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this we are predicting that the test dataset will have 305 accepted loans and 62 denied loans. Thinking back to the amount of loans that were approved and denied in the train dataset there were about 68% of applications that were accepted. With our model there are about 83% of applications that are accepted. This could be in part to either the fact that we are 80% accurate or the fact that there are simply more applicants in the test dataset that would qualify for a loan than in the train dataset.","metadata":{}}]}