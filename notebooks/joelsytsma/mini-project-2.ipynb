{"cells":[{"metadata":{},"cell_type":"markdown","source":"The original plan for this assignment was to do a key driver analysis between lots of different variables and look for unexpected correlations. I did quite a bit of research and exploration work looking into how I could do that. I kept a work log about what I did exploring this:\n\n1)\tReading about linear regression https://en.wikipedia.org/wiki/Regression_analysis\n2)\tReading about mediation https://en.wikipedia.org/wiki/Mediation_(statistics)\n3)\tReading about identifying predictor variables https://blog.minitab.com/blog/adventures-in-statistics-2/how-to-identify-the-most-important-predictor-variables-in-regression-models\n4)\tReading about a key driver analysis https://measuringu.com/key-drivers/\n5)\tReading about non-linear regression https://en.wikipedia.org/wiki/Nonlinear_regression\n6) Decided that key driver analysis is what I’m interested in.\n7) Found library called RelativeImp that I’m going to explore https://pypi.org/project/relativeImp/\n\nHowever, after consultation with Brock, I decided on something different. Doing a scatterplot of countries happiness score and their ranking. When I was in the middle of this effort it became clear that scatter plot was the wrong way to look at this data and that linear regression was a better way of looking at the data and getting predictions. The end result of these iterations is before you now. I run through 3 different linear regressions.\n\n1) Comparing happiness score to a countries happiness ranking. \n2) Comparing a countries happiness score to the percentage of their lands that is forested.\n3) Comparing a countries happiness score to their military spending.\n\nThe end result was a very strong correlation between happiness score and ranking (r=.98). And an insignificant but positive correlation for forested % and happiness score (r= .02) and also for military spending and happiness (r=.08). Enjoy the report!","attachments":{}},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nimport sklearn.metrics as sm\nfrom numpy.polynomial.polynomial import polyfit\nfrom sklearn.metrics import r2_score\nfrom scipy import stats\nimport math\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"slowly...painfully. I reallized that the world happiness CSVs containing very similar data had slightly different names for the same data on different years. I tried for a long time to programmitacally loop through every column and extract columns with similar data and move those columns to a new dataframe. However, after spending some time I realized that I could either a) make this my assignment and build an ML model that looked for similar data and did what I was asking or b) do it manually and move on to the real part of the assingment. The below code is my attempt to loop through the DF columns for each CSV and extract the relevant one programmatically.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#happiness2015 = pd.read_csv(\"../input/world-happiness/2015.csv\")\n#happiness2016 = pd.read_csv(\"../input/world-happiness/2016.csv\")\n#happiness2017 = pd.read_csv(\"../input/world-happiness/2017.csv\")\n#happiness2018 = pd.read_csv(\"../input/world-happiness/2018.csv\")\n#happiness2019 = pd.read_csv(\"../input/world-happiness/2019.csv\")\n#happinessList= [happiness2015,happiness2016,happiness2017,happiness2018,happiness2019] \n#happinessMash = pd.DataFrame()\n#(above) putting all these dictionaries in a list so I can iterate through and do the same action on all of them\n#year= 2015\n# (above) setting variable to equal the year that represents the first year of the data set I'm looking at.\n#for column in happinessList:\n#    columnList= list(column.columns)\n#    #for header in columnList:\n#        #if 'Country'in header:\n#            #print(year,header)\n#            #x= column[header]\n#            #print(x)\n#        #if 'Score' in header:\n#            #print(year, header)\n#(above) I realized that the different years have different column names, so now I need to clean that up. I'm thinking I will just keep rank and score.\n#after a couple hours trying to iterate through the column names and then dropping the unselected columns, I'm realizing there may be an easier way here\n#    column['Year']= year\n#    happiness2015 = happiness2015['Country']\n#    year = year+1\n#adding the year variable to the data set so that when I combine these dataframes they are able to report the year of each data\n\n#happinessMash","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I stopped trying to create a new CSV of all the happiness scores programmatically and instead manually manipulating these dataframes to only include the columns I wanted."},{"metadata":{},"cell_type":"markdown","source":"I want to have a way for each CSV to know what year it represents so that when I join them together they can report on the year."},{"metadata":{"trusted":true},"cell_type":"code","source":"happiness2015 = pd.read_csv(\"../input/world-happiness/2015.csv\")\nhappiness2016 = pd.read_csv(\"../input/world-happiness/2016.csv\")\nhappiness2017 = pd.read_csv(\"../input/world-happiness/2017.csv\")\nhappiness2018 = pd.read_csv(\"../input/world-happiness/2018.csv\")\nhappiness2019 = pd.read_csv(\"../input/world-happiness/2019.csv\")\nhappinessList= [happiness2015,happiness2016,happiness2017,happiness2018,happiness2019] \n#(above) putting all these dictionaries in a list so I can iterate through and do the same action on all of them\nyear= 2015\n# (above) setting variable to equal the year that represents the first year of the data set I'm looking at.\nfor column in happinessList:\n    column['Year']= year\n    year = year+1\n#adding the year variable to the data set so that when I combine these dataframes they are able to report the year of each data\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Changing the columns and re-ordering the column names so that they are always in the same order."},{"metadata":{"trusted":true},"cell_type":"code","source":"#reordering the column names and only including data I want in the final piece of data.\n\nhappiness2015= happiness2015[['Year','Country','Happiness Rank','Happiness Score']]\nhappiness2016= happiness2016[['Year','Country','Happiness Rank','Happiness Score']]\nhappiness2017= happiness2017[['Year','Country','Happiness.Rank','Happiness.Score']]\nhappiness2018= happiness2018[['Year','Country or region','Overall rank','Score']]\nhappiness2019= happiness2019[['Year','Country or region','Overall rank','Score']]\n\n#changing the column names to match. I originally just changed the column names by reassigning the column names. df = df.columns ['before':'after']\n#but this didn't change the table at a deep enough level. I kep on getting an index reference error. So I tried this and it seemed to work.\nhappiness2017_2 = happiness2017.set_axis(['Year', 'Country', 'Happiness Rank', 'Happiness Score'], axis=1, inplace=False)\nhappiness2018_2 = happiness2018.set_axis(['Year', 'Country', 'Happiness Rank', 'Happiness Score'], axis=1, inplace=False)\nhappiness2019_2 = happiness2019.set_axis(['Year', 'Country', 'Happiness Rank', 'Happiness Score'], axis=1, inplace=False)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm now going to combine all my cleaned datasets into one dataframe that I can use for my analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"#putting the cleaned datasets in a list\nhappinessList_2 = [happiness2015, happiness2016, happiness2017_2, happiness2018_2, happiness2019_2]\n#combining all the happiness dataframes into one\nh_mash= pd.concat(happinessList_2).reset_index(drop=True)\nh_mash\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#converting the happinesse rank and happiness score to a numpy so I can start to plot the intersection of these two\nX= h_mash[['Happiness Rank','Happiness Score']].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Beginning to plot my the numpy arrary I created so that I can eventually start my clustering. I'm borrowing this method heavily from https://www.tutorialspoint.com/machine_learning_with_python/machine_learning_with_python_clustering_algorithms_hierarchical.htm and so don't fully undestand what it is doing here."},{"metadata":{"trusted":true},"cell_type":"code","source":"#labeling every dot on the plot\nlabels = range(0, 781)\n#setting the width and height of the display\nplt.figure(figsize = (20, 7))\n#I'm honestly not sure what this is doing. I've messed with the numbers to see what happens but I get unexpected errors.\nplt.subplots_adjust(bottom = 0.1)\n#Assigning the values to each axis\nplt.scatter(X[:,0],X[:,1], label = 'True Position')\n#setting up a for loop that applies the label to every dot that is plotted. I've tried to make this only label every 50th dot but to no avail.\nfor label, x, y in zip(labels, X[:, 0], X[:, 1]):\n   plt.annotate(\n      label,xy = (x, y), xytext = (-3, 3),textcoords = 'offset points', ha = 'right', va = 'bottom')\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay. So after plotting the relationship between ranking and happiness score it is pretty clear that it has a linear regression (surprise, surprise). I'm going to plot that regression line real fast. I think now that I've abandoned cluster analysis there is less need to set up such a robust plotting like I did with the above code."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a numpy array of every x value. In this case the happiness rank\nhr = h_mash['Happiness Rank'].to_numpy()\n#Creating a numpy array of every y value. In this case the happiness score.\nhs = h_mash['Happiness Score'].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using the scypy library to define everything about the line of best fit. I love libraries!\nslope, intercept, r_value, p_value, std_err = stats.linregress(hr,hs)\n#setting where the line should draw\ndef linefitline(b):\n    return intercept + slope * b\nline1 = linefitline(hr)\n\n#plotting the line we just made\nplt.figure(figsize = (20, 7))\nplt.scatter(hr,hs)\nplt.plot(hr,line1, c = 'g')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have the line of best fit. We need to plot the average of the y intercept so that we can calculate the difference between each plot point and that average line. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#drawing the figure large\nplt.figure(figsize = (20, 7))\n#setting the line to draw the length of the x2 array and at the height of the mean of the y data.\nline2 = np.full(len(hr),[hs.mean()])\n\naverage = hs.mean()\nprint('the average happiness score is', average)\n#drawing the scatter plot again\nplt.scatter(hr,hs)\nplt.plot(hr,line2, c = 'r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now to calculate the sum of squares between the line of best fit and the y axis mean line. This is another instance of libraries coming through. I'm not 100% sure what all the code is doing but I know that the output is a sum of the squares for each line.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculating the difference between each point and the line of best fit\ndifferences_line1 = linefitline(hr)-hs\nline1sum = 0\nfor i in differences_line1:\n    line1sum = line1sum + (i*i)\n\n#calculating the difference between each point and y intercept line\ndifferences_line2 = line2 - hs\nline2sum = 0\nfor i in differences_line2:\n    line2sum = line2sum + (i*i)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculating the r2 score. Looking at the plot line we can expect this value to be very close to 1 since the best fit line seems to be a very good fit."},{"metadata":{"trusted":true},"cell_type":"code","source":"r2 = r2_score(hs, linefitline(hr))\nprint('The rsquared value is: ' + str(r2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Out of curiousity I want see if % of forested land is correlated to happiness score. Looking at the CSV for the forest CSV I have, it is apparent that the only year with both forest % and happiness data is 2015. So I'm going to isolate that data from the two datasets."},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"forest = pd.read_csv(\"../input/forest-area-of-land-area/forest_area.csv\")\nforest.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the below code I am doing an inner join and merging together the two dataframes by country. I am doing this because the world happiness CSV has reports on countries that the forest data set does not report. By doing an inner join I am only going to include countries that have data for both. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Setting the forest dataframe to only show the information that I'm interested in. Country name and the year 2015\nforest = forest[['CountryName','2015']]\n#Taking that truncated dataframe and renaming the column names so that I can join the two dataframes on this key column\nsimpleF = forest.set_axis(['Country','Forest Percent'], axis=1, inplace=False)\n#Setting the happiness report to only show the data I'm interested in. Mainly happiness score.\nsimpleHR= happiness2015[['Country', 'Happiness Score']]\n\n#Creating a new dataframe that is the forest and happiness dataframes merged.\nFandHR = pd.merge(left=simpleF, right=simpleHR, left_on='Country', right_on='Country')\n#Printing that to new dataframe to verify the merge worked.\nprint(FandHR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that I've merged the data sets I can start extracting data from it to set up my scatter plot chart. This is what I do in the next bit of code. I'm creating two numpy arrays that I will use to create a scatter plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"f= FandHR['Forest Percent'].to_numpy()\nhr2015= FandHR['Happiness Score'].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the setup work is done and now I'm just going to plot the line of best fit and y-intercept line and then get the r-squared value. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#using the scypy library to define everything about the line of best fit. I love libraries!\nslope, intercept, r_value, p_value, std_err = stats.linregress(hr2015,f)\n#setting where the line should draw\ndef linefitline(b):\n    return intercept + slope * b\nline1 = linefitline(hr2015)\n\n#plotting the line we just made\nplt.figure(figsize = (20, 7))\n#using the numpy arrays I've set up earlier to plot a graph.\nplt.scatter(hr2015,f)\nplt.plot(hr2015,line1, c = 'g')\n#describing the graph\nplt.title('Forests and happiness')\nplt.ylabel('Percentage of the country that is forest')\nplt.xlabel('Happiness Score')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drawing the figure large\nplt.figure(figsize = (20, 7))\n#setting the line to draw the length of the x2 array and at the height of the mean of the y data.\nline2 = np.full(len(hr2015),[f.mean()])\n\naverage = f.mean()\nprint('the average forest percentage score is', average)\n#drawing the scatter plot again\nplt.scatter(hr2015,f)\nplt.plot(hr2015,line2, c = 'r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculating the difference between each point and the line of best fit\ndifferences_line1 = linefitline(hr2015)-f\nline1sum = 0\nfor i in differences_line1:\n    line1sum = line1sum + (i*i)\n\n#calculating the difference between each point and y intercept line\ndifferences_line2 = line2 - f\nline2sum = 0\nfor i in differences_line2:\n    line2sum = line2sum + (i*i)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While there is slight positive correlation between % of forests and world happiness score, it is not significant. Still interesting though!"},{"metadata":{"trusted":true},"cell_type":"code","source":"r2 = r2_score(f, linefitline(hr2015))\nprint('The rsquared value is: ' + str(r2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I want to do the same thing I did with forests but this time I'm going to compare military spending of a country with happiness."},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"Military = pd.read_csv(\"../input/military-expenditure-of-countries-19602019/Military Expenditure.csv\")\nMilitary.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The below code is formatting the military dataframe to look like the happiness dataframe and then inner joining the two dataframes on the columns year and country."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating new DF that only reference the columns that have matching data in the happiness report\nMilitary2015 = Military[['Name','2015']]\nMilitary2016 = Military[['Name','2016']]\nMilitary2017 = Military[['Name','2017']]\nMilitary2018 = Military[['Name','2018']]\n#adding the year column to each of the new DFs\nyear = 2015\nMilitaryList = [Military2015, Military2016, Military2017,Military2018]\nfor column in MilitaryList:\n    column['Year']= year\n    year = year+1\n#renaming the columns so that I cna do a join\nM2015= Military2015.set_axis(['Country', 'Military Spending', 'Year',], axis=1, inplace=False)\nM2016= Military2016.set_axis(['Country', 'Military Spending', 'Year',], axis=1, inplace=False)\nM2017= Military2017.set_axis(['Country', 'Military Spending', 'Year',], axis=1, inplace=False)\nM2018= Military2018.set_axis(['Country', 'Military Spending', 'Year',], axis=1, inplace=False)\n\n#stacking all the DFs I created to create a long DF that I can join with the happiness CSV\nML = [M2015, M2016, M2017, M2018]\nM_mash= pd.concat(ML).reset_index(drop=True)\nM_mash = M_mash[['Year', 'Country', 'Military Spending']]\n#doing an inner join with happiness DF I created way back in box three of this notebook.\nMandHS = pd.merge(left=M_mash, right=h_mash, left_on=['Year','Country'], right_on=['Year','Country'])\n\nMandHS","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm interested in only seeing countries that report military spending. So, I'm dropping all rows without data."},{"metadata":{"trusted":true},"cell_type":"code","source":"MandHS2= MandHS[['Military Spending','Happiness Score']]\nMandHS2 = MandHS2.apply (pd.to_numeric, errors='coerce')\nMandHS2 = MandHS2.dropna()\nMandHS2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"HappScore= MandHS2['Happiness Score'].to_numpy()\nMilSpend= MandHS2['Military Spending'].to_numpy()\n\n#using the scypy library to define everything about the line of best fit. I love libraries!\nslope, intercept, r_value, p_value, std_err = stats.linregress(HappScore,MilSpend)\n#setting where the line should draw\ndef linefitline(b):\n    return intercept + slope * b\nline2 = linefitline(HappScore)\n\n#plotting the line we just made\nplt.figure(figsize = (20, 7))\n#using the numpy arrays I've set up earlier to plot a graph.\nplt.scatter(HappScore,MilSpend)\nplt.plot(HappScore,line2, c = 'g')\n#describing the graph\nplt.title('Happiness and Military spending')\nplt.ylabel('Military Spending')\nplt.xlabel('Happiness Score')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Outliers! Let's see who thoe countries are that spend so much money so that they are skewing the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers = MandHS.sort_values(by='Military Spending', ascending=False)\noutliers.reset_index(drop=True,inplace=True)\noutliers.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normally, I'd do something like find all countries that fit outside of a certain number of standard deviations of the mean and drop them (I plotted out the normal distribution of military spend below).\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"variance = np.var(MilSpend)\nmu = np.mean(MilSpend)\n\nsigma = math.sqrt(variance)\nx = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\nplt.plot(x, stats.norm.pdf(x, mu, sigma))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" However it's clear that four plots at the top are the USA across all four year. The four dots in the middle are China across all four years. So I think I'm just going to drop those 8 outliers and do the plot again."},{"metadata":{"trusted":true},"cell_type":"code","source":"#I'm doing this the easy way by just dropping the first 8 rows of my sorted database rather and creating a new DF from that.\n#This is instead of going through each row and looking for \"China\" or \"US\"\n#The easy way means I need to drop each non-number again.\nnoUSAorChina = outliers.iloc[8:]\nMandHS3= noUSAorChina[['Military Spending','Happiness Score']]\nMandHS3 = MandHS3.apply (pd.to_numeric, errors='coerce')\nMandHS3 = MandHS3.dropna()\nMandHS3\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting a new graph without China or the USA. Still a positive correlation! Now to calculate R squared score."},{"metadata":{"trusted":true},"cell_type":"code","source":"NOHappScore= MandHS3['Happiness Score'].to_numpy()\nNOMilSpend= MandHS3['Military Spending'].to_numpy()\n\n#using the scypy library to define everything about the line of best fit. I love libraries!\nslope, intercept, r_value, p_value, std_err = stats.linregress(NOHappScore,NOMilSpend)\n#setting where the line should draw\ndef linefitline(b):\n    return intercept + slope * b\nline2 = linefitline(NOHappScore)\n\n#plotting the line we just made\nplt.figure(figsize = (20, 7))\n#using the numpy arrays I've set up earlier to plot a graph.\nplt.scatter(NOHappScore,NOMilSpend)\nplt.plot(NOHappScore,line2, c = 'g')\n#describing the graph\nplt.title('No China or USA: Happiness and Military spending')\nplt.ylabel('No China or USA: Military Spending')\nplt.xlabel('No China or USA: Happiness Score')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drawing the figure large\nplt.figure(figsize = (20, 7))\n#setting the line to draw the length of the x2 array and at the height of the mean of the y data.\nline4 = np.full(len(NOHappScore),[NOMilSpend.mean()])\n\naverage = NOMilSpend.mean()\nprint('the average military spend without the USA or China is:', average)\n#drawing the scatter plot again\nplt.scatter(NOHappScore,NOMilSpend)\nplt.plot(NOHappScore,line4, c = 'r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2 = r2_score(NOMilSpend, linefitline(NOHappScore))\nprint('The rsquared value is: ' + str(r2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nothing significant. But still interesting that military spending has a positive correlation with happiness score!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}