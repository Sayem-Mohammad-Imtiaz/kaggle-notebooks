{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#gun-violence dataset import into notebook\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the required libraries \nfrom plotly.offline import init_notebook_mode, iplot\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom wordcloud import WordCloud\nfrom textblob import TextBlob \nimport plotly.graph_objects as go\nimport seaborn as sns\nimport pandas as pd\nimport string, os, random\nimport calendar\nfrom PIL import Image \nimport numpy as np\n\nfrom itertools import chain\n\nimport folium \nfrom folium import plugins \n\n\ninit_notebook_mode(connected=True)\npunc = string.punctuation\nfrom datetime import datetime\n\n# Load the dataset\ngun_violence= pd.read_csv(\"../input/gun-violence-data/gun-violence-data_01-2013_03-2018.csv\")\n\n\n### Fix a row in the dataset \n### According to the author of this dataset, one particular incident is missing from the dataset \n### I have manually added this incident\n### Related Thread: https://www.kaggle.com/jameslko/gun-violence-data/discussion/55307\nmissing_row = ['sban_1', '2017-10-01', 'Nevada', 'Las Vegas', 'Mandalay Bay 3950 Blvd S', 59, 489, 'https://en.wikipedia.org/wiki/2017_Las_Vegas_shooting', 'https://en.wikipedia.org/wiki/2017_Las_Vegas_shooting', '-', '-', '-', '-', '-', '36.095', 'Hotel', \n               '-115.171667', 47, 'Route 91 Harvest Festiva; concert, open fire from 32nd floor. 47 guns seized; TOTAL:59 kill, 489 inj, number shot TBD,girlfriend Marilou Danley POI', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-']\ngun_violence.loc[len(gun_violence)] = missing_row\n\n# Create some additional features\ngun_violence['date'] = pd.to_datetime(gun_violence['date'])\ngun_violence['year'] = gun_violence['date'].dt.year\ngun_violence['month'] = gun_violence['date'].dt.month\ngun_violence['monthday'] = gun_violence['date'].dt.day\ngun_violence['weekday'] = gun_violence['date'].dt.weekday\ngun_violence['loss'] = gun_violence['n_killed'] + gun_violence['n_injured']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Gun Violence Dimensions:\",gun_violence.shape)\ngun_violence.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#display last 4 elements in the dataset\ngun_violence.tail(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check to see if there are spaces in column names\ngun_violence.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gun_violence.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Useful information about the data\ngun_violence.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gun_violence['year'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"#  number of Gun Violence Incidents by year\n# function to aggregate and return keys and values\ndef create_stack_bar_data(col):\n    aggregated = gun_violence[col].value_counts()\n    x_values = aggregated.index.tolist()\n    y_values = aggregated.values.tolist()\n    return x_values, y_values\n\nx1, y1 = create_stack_bar_data('year')\nx1 = x1[:-1]\ny1 = y1[:-1]\ntrace1 = go.Bar(x=x1, y=y1, opacity=0.75, name=\"year count\", marker=dict(color=['rgba(10, 220, 150, 0.6)', 'rgba(10, 220, 150, 0.6)', 'rgba(10, 220, 150, 0.6)', 'rgba(10, 220, 150, 0.6)', 'rgba(222,45,38,0.8)']))\nlayout = dict(height=400, title='Fig.1: Gun Violence Incidents by year', legend=dict(orientation=\"h\"));\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selecting Sample of the data that will be used (i.e. Months of 2018)\ngun_violence_2017 = gun_violence[(gun_violence['year']==2017)]\nprint(\"Gun Violence Data Sample Dimension:\", gun_violence.shape,\"(i.e. Months of 2018)\")#dataset dimension\ngun_violence_2017.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Measures of the numerical data (i.e. Gun Violence in 2018)\ngun_violence_2017.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gun_violence_2017.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Above code isnt working"},{"metadata":{"trusted":true},"cell_type":"code","source":"extract =gun_violence_2017[['participant_gender','incident_id','participant_type','participant_age','participant_status','gun_stolen','gun_type','incident_characteristics']] \n\nextract['participant_type'] =  np.where(extract['participant_type']!='',extract['participant_type'],'NaN')\nextract['participant_gender'] =  np.where(extract['participant_gender']!='',extract['participant_gender'],'NaN')\nextract['participant_age'] =  np.where(extract['participant_age']!='',extract['participant_age'],'NaN')\nextract['participant_status'] =  np.where(extract['participant_status']!='',extract['participant_status'],'NaN')\nextract['gun_stolen'] =  np.where(extract['gun_stolen']!='',extract['gun_stolen'],'NaN')\nextract['gun_type'] =  np.where(extract['gun_type']!='',extract['gun_type'],'NaN')\nextract['incident_characteristics'] =  np.where(extract['gun_type']!='',extract['incident_characteristics'],'NaN')\n#  removing special characters  regex\nextract['participant_gender']=extract['participant_gender'].replace('[0-9||]', '',regex=True)\nextract['participant_gender']=extract['participant_gender'].replace('[::]', ',',regex=True)\n\nextract['participant_type']=extract['participant_type'].replace('[0-9||]', '',regex=True)\nextract['participant_type']=extract['participant_type'].replace('[::]', ',',regex=True)\n\nextract['participant_status']=extract['participant_status'].replace('[0-9||]', '',regex=True)\nextract['participant_status']=extract['participant_status'].replace('[::]', ',',regex=True)\n\nextract['participant_type']=extract['participant_type'].replace('[0-9||]', '',regex=True)\nextract['participant_type']=extract['participant_type'].replace('[::]', ',',regex=True)\n\nextract['gun_stolen']=extract['gun_stolen'].replace('[0-9||]', '',regex=True)\nextract['gun_stolen']=extract['gun_stolen'].replace('[::]', ',',regex=True)\n\nextract['gun_type']=extract['gun_type'].replace('[0-9||]', '',regex=True)\nextract['gun_type']=extract['gun_type'].replace('[::]', ',',regex=True)\n\nextract['incident_characteristics']=extract['incident_characteristics'].replace('[||]', ';',regex=True)\n\n\nextract['participant_age']=extract['participant_age'].replace('[::]', ',',regex=True)\nextract['participant_age']=extract['participant_age'].replace('[||]', ',',regex=True)\nextract.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extract = extract.dropna(axis=0)\n\nextract.isnull().sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert to list\nextract['participant_gender'] = extract['participant_gender'].str.split(',') \nextract['participant_type'] = extract['participant_type'].str.split(',') \nextract['participant_age'] = extract['participant_age'].str.split(',')\nextract['participant_status'] = extract['participant_status'].str.split(',')\nextract['gun_stolen'] = extract['gun_stolen'].str.split(',')\nextract['gun_type'] = extract['gun_type'].str.split(',')\nextract['incident_characteristics'] = extract['incident_characteristics'].str.split(';')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extract.head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert list of pd.Series then stack it\ngun_violence_gender = (extract\n .set_index(['incident_id'])['participant_gender']\n .apply(pd.Series)\n .stack()\n .reset_index()\n .rename(columns={0:'participant_gender'}))\n\ngun_violence_type = (extract\n .set_index(['incident_id'])['participant_type']\n .apply(pd.Series)\n .stack()\n .reset_index()                  \n .rename(columns={0:'participant_type'}))\n\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gun_violence_age = (extract\n .set_index(['incident_id'])['participant_age']\n .apply(pd.Series)\n .stack()\n .reset_index()                 \n .rename(columns={0:'participant_age'}))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gun_violence_participant_status = (extract\n .set_index(['incident_id'])['participant_status']\n .apply(pd.Series)\n .stack()\n .reset_index()                  \n .rename(columns={0:'participant_status'}))\n\ngun_violence_gun_stolen = (extract\n .set_index(['incident_id'])['gun_stolen']\n .apply(pd.Series)\n .stack()\n .reset_index()                  \n .rename(columns={0:'gun_stolen'}))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gun_violence_gun_type = (extract\n .set_index(['incident_id'])['gun_type']\n .apply(pd.Series)\n .stack()\n .reset_index()                   \n .rename(columns={0:'gun_type'}))\n\ngun_violence_gun_incident_characteristics = (extract\n .set_index(['incident_id'])['incident_characteristics']\n .apply(pd.Series)\n .stack()\n .reset_index()                  \n .rename(columns={0:'incident_characteristics'}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gun_violence_age=gun_violence_age[gun_violence_age['participant_age'] != '']\ngun_violence_age=gun_violence_age[gun_violence_age['participant_age'] != '-']\ngun_violence_age=gun_violence_age[gun_violence_age['participant_age'].astype(int) > 6]\ngun_violence_age","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gun_violence_gun_incident_characteristics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# new_extract = pd.concat([gun_violence_gender, gun_violence_type,gun_violence_age,gun_violence_gun_stolen,gun_violence_participant_status,gun_violence_gun_type,gun_violence_gun_incident_characteristics],axis=1, join='inner')\n# new_extract\n new_extract =  pd.merge(gun_violence_type,gun_violence_gender,how='inner', left_on=['incident_id','level_1'], right_on = ['incident_id','level_1'],)\n new_extract =  pd.merge(new_extract,gun_violence_age,how='inner', left_on=['incident_id','level_1'], right_on = ['incident_id','level_1'],)\n new_extract =  pd.merge(new_extract,gun_violence_gun_stolen,how='inner', left_on=['incident_id','level_1'], right_on = ['incident_id','level_1'],)\n new_extract =  pd.merge(new_extract,gun_violence_participant_status,how='inner', left_on=['incident_id','level_1'], right_on = ['incident_id','level_1'],)\n new_extract =  pd.merge(new_extract,gun_violence_gun_incident_characteristics,how='left', left_on=['incident_id','level_1'], right_on = ['incident_id','level_1'],)\n new_extract =  pd.merge(new_extract,gun_violence_gun_type,how='left', left_on=['incident_id','level_1'], right_on = ['incident_id','level_1'],)\n    \n# newextract = newextract.merge(gun_violence_gun_stolen,left_on='incident_id', right_on='incident_id', suffixes=(False, False))\n# newextract = newextract.join(gun_violence_gun_type.set_index('incident_id'),how='inner')\n# newextract = newextract.join(gun_violence_gun_incident_characteristics.set_index('incident_id'),how='inner')\n# newextract = newextract[(newextract['participant_gender']!='')]\n new_extract","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_extract['participant_type'] =  np.where(new_extract['participant_type']!='',new_extract['participant_type'],'Unidentified')\nnew_extract['participant_type'] =  np.where(new_extract['participant_type']!='-',new_extract['participant_type'],'Unidentified')\n\nnew_extract['participant_gender'] =  np.where(new_extract['participant_gender']!='',new_extract['participant_gender'],'Unidentified')\nnew_extract['participant_gender'] =  np.where(new_extract['participant_gender']!='-',new_extract['participant_gender'],'Unidentified')\n\nnew_extract","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_extract = new_extract.dropna()\nnew_extract = new_extract.reset_index(drop=True)\nnew_extract","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Suspect = new_extract[new_extract['participant_type'] != 'Victim']\nSuspect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Suspectcount =Suspect['participant_gender'].value_counts()\nSuspectcount","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  number of Gun Violence Incidents by year\n# function to aggregate and return keys and values\ndef create_stack_bar_data(col):\n    aggregated = Suspect[col].value_counts()\n    x_values = aggregated.index.tolist()\n    y_values = aggregated.values.tolist()\n    return x_values, y_values\n\nx1, y1 = create_stack_bar_data('participant_gender')\n\ntrace = go.Pie(labels=x1, values=y1,title= 'Gender of Suspect')\niplot([trace], filename='basic_pie_chart')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2014 time series\ntemp = gun_violence[gun_violence['year'] == 2014].groupby('date').agg({'state' : 'count', 'n_killed' : 'sum', 'n_injured' : 'sum'}).reset_index().rename(columns={'state' : 'incidents'})\n\n\ntrace1 = go.Scatter(x = temp.date, y = temp.incidents, name='Total Incidents', mode = \"lines\", marker = dict(color = '#c5d9f9'))\ntrace2 = go.Scatter(x = temp.date, y = temp.n_killed, name=\"Total Killed\", mode = \"lines\", marker = dict(color = '#ff9f87'))\ntrace3 = go.Scatter(x = temp.date, y = temp.n_injured, name=\"Total Injured\", mode = \"lines\", marker = dict(color = '#e8baff'))\n\ndata = [trace1, trace2, trace3]\nlayout = dict(height=350, title = 'Gun Violence Incidents - 2014', legend=dict(orientation=\"h\", x=-.01, y=1), xaxis= dict(title='Date Time', ticklen= 1))\nfig = dict(data = data, layout = layout)\niplot(fig)\n\n\n\n# 2015 time series\ntemp = gun_violence[gun_violence['year'] == 2015].groupby('date').agg({'state' : 'count', 'n_killed' : 'sum', 'n_injured' : 'sum'}).reset_index().rename(columns={'state' : 'incidents'})\n\n\ntrace1 = go.Scatter(x = temp.date, y = temp.incidents, name='Total Incidents', mode = \"lines\", marker = dict(color = '#c5d9f9'))\ntrace2 = go.Scatter(x = temp.date, y = temp.n_killed, name=\"Total Killed\", mode = \"lines\", marker = dict(color = '#ff9f87'))\ntrace3 = go.Scatter(x = temp.date, y = temp.n_injured, name=\"Total Injured\", mode = \"lines\", marker = dict(color = '#e8baff'))\n\ndata = [trace1, trace2, trace3]\nlayout = dict(height=350, title = 'Gun Violence Incidents - 2015', legend=dict(orientation=\"h\", x=-.01, y=1), xaxis= dict(title='Date Time', ticklen= 1))\nfig = dict(data = data, layout = layout)\niplot(fig)\n\n\n\n\n\n# 2016 time series\ntemp = gun_violence[gun_violence['year'] == 2016].groupby('date').agg({'state' : 'count', 'n_killed' : 'sum', 'n_injured' : 'sum'}).reset_index().rename(columns={'state' : 'incidents'})\n\n\ntrace1 = go.Scatter(x = temp.date, y = temp.incidents, name='Total Incidents', mode = \"lines\", marker = dict(color = '#c5d9f9'))\ntrace2 = go.Scatter(x = temp.date, y = temp.n_killed, name=\"Total Killed\", mode = \"lines\", marker = dict(color = '#ff9f87'))\ntrace3 = go.Scatter(x = temp.date, y = temp.n_injured, name=\"Total Injured\", mode = \"lines\", marker = dict(color = '#e8baff'))\n\ndata = [trace1, trace2, trace3]\nlayout = dict(height=350,title = 'Gun Violence Incidents - 2016', legend=dict(orientation=\"h\", x=-.01, y=1), xaxis= dict(title='Date Time', ticklen= 1))\nfig = dict(data = data, layout = layout)\niplot(fig)\n\n\n# 2017 time series\ntemp = gun_violence[gun_violence['year'] == 2017].groupby('date').agg({'state' : 'count', 'n_killed' : 'sum', 'n_injured' : 'sum'}).reset_index().rename(columns={'state' : 'incidents'})\n\n\ntrace1 = go.Scatter(x = temp.date, y = temp.incidents, name='Total Incidents', mode = \"lines\", marker = dict(color = '#c5d9f9'))\ntrace2 = go.Scatter(x = temp.date, y = temp.n_killed, name=\"Total Killed\", mode = \"lines\", marker = dict(color = '#ff9f87'))\ntrace3 = go.Scatter(x = temp.date, y = temp.n_injured, name=\"Total Injured\", mode = \"lines\", marker = dict(color = '#e8baff'))\n\ndata = [trace1, trace2, trace3]\nlayout = dict(height=350,title = 'Gun Violence Incidents - 2017', legend=dict(orientation=\"h\", x=-.01, y=1), xaxis= dict(title='Date Time', ticklen= 1))\nfig = dict(data = data, layout = layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"states_df = gun_violence['state'].value_counts()\n\nstatesdf = pd.DataFrame()\nstatesdf['state'] = states_df.index\nstatesdf['counts'] = states_df.values\n\nscl = [[0.0, 'rgb(242,240,247)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\\\n            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']]\n\nstate_to_code = {'District of Columbia' : 'dc','Mississippi': 'MS', 'Oklahoma': 'OK', 'Delaware': 'DE', 'Minnesota': 'MN', 'Illinois': 'IL', 'Arkansas': 'AR', 'New Mexico': 'NM', 'Indiana': 'IN', 'Maryland': 'MD', 'Louisiana': 'LA', 'Idaho': 'ID', 'Wyoming': 'WY', 'Tennessee': 'TN', 'Arizona': 'AZ', 'Iowa': 'IA', 'Michigan': 'MI', 'Kansas': 'KS', 'Utah': 'UT', 'Virginia': 'VA', 'Oregon': 'OR', 'Connecticut': 'CT', 'Montana': 'MT', 'California': 'CA', 'Massachusetts': 'MA', 'West Virginia': 'WV', 'South Carolina': 'SC', 'New Hampshire': 'NH', 'Wisconsin': 'WI', 'Vermont': 'VT', 'Georgia': 'GA', 'North Dakota': 'ND', 'Pennsylvania': 'PA', 'Florida': 'FL', 'Alaska': 'AK', 'Kentucky': 'KY', 'Hawaii': 'HI', 'Nebraska': 'NE', 'Missouri': 'MO', 'Ohio': 'OH', 'Alabama': 'AL', 'Rhode Island': 'RI', 'South Dakota': 'SD', 'Colorado': 'CO', 'New Jersey': 'NJ', 'Washington': 'WA', 'North Carolina': 'NC', 'New York': 'NY', 'Texas': 'TX', 'Nevada': 'NV', 'Maine': 'ME'}\nstatesdf['state_code'] = statesdf['state'].apply(lambda x : state_to_code[x])\n\ndata = [ dict(\n        type='choropleth',\n        colorscale = scl,\n        autocolorscale = False,\n        locations = statesdf['state_code'],\n        z = statesdf['counts'],\n        locationmode = 'USA-states',\n        text = statesdf['state'],\n        marker = dict(\n            line = dict (\n                color = 'rgb(255,255,255)',\n                width = 2\n            ) ),\n        colorbar = dict(\n            title = \"Gun Violence Incidents\")\n        ) ]\n\nlayout = dict(\n        title = 'State wise number of Gun Violence Incidents',\n        geo = dict(\n            scope='usa',\n            projection=dict( type='albers usa' ),\n            showlakes = True,\n            lakecolor = 'rgb(255, 255, 255)'),\n             )\n\n# data = [ dict(\n#         type='choropleth',\n#         colorscale = scl,\n#         autocolorscale = False,\n#         locations = statesdf['state_code'],\n#         z = statesdf['counts'],\n#         locationmode = 'USA-states',\n#         text = statesdf['state'],\n#         marker = dict(\n#             line = dict (\n#                 color = 'rgb(255,255,255)',\n#                 width = 2\n#             ) ),\n#         colorbar = dict(\n#             title = \"Gun Violence Incidents\")\n#         ) ]\n\n# layout = dict(\n#         title = 'State wise number of Gun Violence Incidents',\n#         geo = dict(\n#             scope='usa',\n#             projection=dict( type='albers usa' ),\n#             showlakes = True,\n#             lakecolor = 'rgb(255, 255, 255)'),\n#              )\n    \nfig = dict( data=data, layout=layout )\niplot( fig, filename='d3-cloropleth-map' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Prominent Age of Gun Violence Suspects \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_user_mapping(txt):\n    if txt == \"NA\":\n        return {}\n    mapping = {}\n    for d in txt.split(\"||\"):\n        try:\n            key = d.split(\"::\")[0]\n            val = d.split(\"::\")[1]\n            if key not in mapping:\n                mapping[key] = val\n        except:\n            pass\n    return mapping\n\ngun_violence['participant_type'] = gun_violence['participant_type'].fillna(\"NA\")\ngun_violence['participant_type_map'] = gun_violence['participant_type'].apply(lambda x : get_user_mapping(x))\ngun_violence['participant_age'] = gun_violence['participant_age'].fillna(\"NA\")\ngun_violence['participant_age_map'] = gun_violence['participant_age'].apply(lambda x : get_user_mapping(x))\ngun_violence['participant_gender'] = gun_violence['participant_gender'].fillna(\"NA\")\ngun_violence['participant_gender_map'] = gun_violence['participant_gender'].apply(lambda x : get_user_mapping(x))\n\n## Finding the Suspect Age Groups\nsuspect_age_groups = {}\nfor i, row in gun_violence.iterrows():\n    suspects = []\n    for k,v in row['participant_type_map'].items():\n        if \"suspect\" in v.lower():\n            suspects.append(k)\n    for suspect in suspects:\n        if suspect in row['participant_age_map']:\n            ag = row['participant_age_map'][suspect]\n            if ag not in suspect_age_groups:\n                suspect_age_groups[ag] = 0 \n            else:\n                suspect_age_groups[ag] += 1\n\ntrace1 = go.Bar(x=list(suspect_age_groups.keys()), y=list(suspect_age_groups.values()), opacity=0.75, name=\"month\", marker=dict(color='rgba(200, 20, 160, 0.6)'))\nlayout = dict(height=400, title='Suspects Age - Distribution', xaxis=dict(range=[0, 100]), legend=dict(orientation=\"h\"));\nfig = go.Figure(data=[trace1], layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n Prominent Age of Gun Violence Victims\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"victim_age_groups = {}\nfor i, row in gun_violence.iterrows():\n    victims = []\n    for k,v in row['participant_type_map'].items():\n        if \"victim\" in v.lower():\n            victims.append(k)\n    for victim in victims:\n        if victim in row['participant_age_map']:\n            ag = row['participant_age_map'][victim]\n            if ag not in victim_age_groups:\n                victim_age_groups[ag] = 0 \n            else:\n                victim_age_groups[ag] += 1\n                \ntrace1 = go.Bar(x=list(victim_age_groups.keys()), y=list(victim_age_groups.values()), opacity=0.75, name=\"month\", marker=dict(color='brown'))\nlayout = dict(height=400, title='Victims Age - Distribution', xaxis=dict(range=[0, 100]), legend=dict(orientation=\"h\"));\nfig = go.Figure(data=[trace1], layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\nstatedate=gun_violence[['incident_id','state','city_or_county','monthday','weekday','loss']]\nstatedate\n\n\nnew_extract2 = pd.merge(statedate,new_extract,how='inner',left_on=['incident_id'], right_on = ['incident_id'],suffixes=(False,False))\nnew_extract2\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef create_label_encoder_dict(df):\n    from sklearn.preprocessing import LabelEncoder\n    \n    label_encoder_dict = {}\n    for column in df.columns:\n        # Only create encoder for categorical data types\n        if not np.issubdtype(df[column].dtype, np.number) and column != 'participant_age':\n            label_encoder_dict[column]= LabelEncoder().fit(df[column])\n    return label_encoder_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data =new_extract2[['state','participant_type','participant_gender','participant_age','gun_stolen','participant_status','incident_characteristics','weekday']]\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def func(row):\n    if row['weekday'] == 0:\n        return 'Monday'\n    elif row['weekday'] ==1:\n        return 'Tuesday'\n    elif row['weekday'] ==2:\n        return 'Wednesday'\n    elif row['weekday'] ==3:\n        return 'Thursday'\n    elif row['weekday'] ==4:\n        return 'Friday'\n    elif row['weekday'] ==5:\n        return 'Saturday'\n    elif row['weekday'] ==6:\n        return 'Sunday'\n    else:\n        return 'Not a Vaild Date'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['weekday'] = data.apply(func, axis=1)\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlabel_encoders = create_label_encoder_dict(data)\nprint(\"Encoded Values for each Label\")\nprint(\"=\"*32)\nfor column in label_encoders:\n    print(\"=\"*32)\n    print('Encoder(%s) = %s' % (column, label_encoders[column].classes_ ))\n    print(pd.DataFrame([range(0,len(label_encoders[column].classes_))], columns=label_encoders[column].classes_, index=['Encoded Values']  ).T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # Apply each encoder to the data set to obtain transformed values\ndata2 = data.copy() # create copy of initial data set\nfor column in data2.columns:\n    if column in label_encoders:\n        data2[column] = label_encoders[column].transform(data2[column])\n        \nprint(\"Transformed data set\")\nprint(\"=\"*32)\ndata2.head(15)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# separate our data into dependent (Y) and independent(X) variables\nX_data = data2[['state','participant_type','participant_gender','participant_age','gun_stolen','participant_status','incident_characteristics']]\nY_data = data2['weekday'] # actually weekday column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = MLPClassifier()\n\nreg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"help(MLPClassifier)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.n_layers_ # Number of layers utilized\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predicted = reg.predict(X_test)\ntest_predicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data3 = X_test.copy()\ndata3['predicted_weekday']=test_predicted\ndata3['predicted_weekday_en']=label_encoders['weekday'].inverse_transform(test_predicted)\ndata3['weekday']=data3['weekday']=y_test\ndata3['weekday_en']=label_encoders['weekday'].inverse_transform(y_test)\ndata3.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Evaluation**\n\nBuilding a Confusion Matrix\nNB. Data should be split in training and test data. The model built should be evaluated using unseen or test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"k=(reg.predict(X_test) == y_test) # Determine how many were predicted correctly\n\nk.value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ncm=confusion_matrix(y_test, reg.predict(X_test), labels=y_test.unique())\ncm\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    import itertools\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,16))\nplot_confusion_matrix(cm,data['weekday'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With Preprocessing\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_min_max_scaler_dict(df):\n    from sklearn.preprocessing import MinMaxScaler\n    min_max_scaler_dict = {}\n    for column in df.columns:\n        # Only create encoder for categorical data types\n        if np.issubdtype(df[column].dtype, np.number):\n            min_max_scaler_dict[column]= MinMaxScaler().fit(pd.DataFrame(df[column]))\n    return min_max_scaler_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scalerdata= data2.copy()\n\ndel scalerdata['weekday']\nscalerdata\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmin_max_scalers = create_min_max_scaler_dict(scalerdata)\nprint(\"Min Max Values for each Label\")\nprint(\"=\"*32)\nmin_max_scalers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#retrieving a scacler\nstate_scaler=min_max_scalers['state']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state_scaler\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state_scaler.data_max_ ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state_scaler.data_min_ ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state_scaler.data_range_ ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame([\n    {\n        'column':col,\n        'min':min_max_scalers[col].data_min_[0], \n        'max':min_max_scalers[col].data_max_[0], \n        'range':min_max_scalers[col].data_range_[0] }  for col in min_max_scalers])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply each scaler to the data set to obtain transformed values\ndata3 = data2.copy() # create copy of initial data set\nfor column in data3.columns:\n    if column in min_max_scalers:\n        data3[column] = min_max_scalers[column].transform(pd.DataFrame(data3[column]))\n\nprint(\"Transformed data set\")\nprint(\"=\"*32)\ndata3.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data3.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# separate our data into dependent (Y) and independent(X) variables\nX2_data = data3[['state','participant_type','participant_gender','participant_age','gun_stolen','participant_status','incident_characteristics']]\nY2_data = data3['weekday'] # actually weekday column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split \nX2_train, X2_test, y2_train, y2_test = train_test_split(X2_data, Y2_data, test_size=0.30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier, MLPRegressor\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create an instance of linear regression\nreg2 = MLPRegressor()\nreg2.fit(X2_train,y2_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg2.n_layers_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions using the testing set\ntest2_predicted = reg2.predict(X2_test)\ntest2_predicted=test2_predicted.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data4 = X2_test.copy()\ndata4['predicted_weekday']=test2_predicted\ndata4['predicted_weekday_en']=label_encoders['weekday'].inverse_transform(test2_predicted)\ndata4['weekday']=data3['weekday']=y2_test\ndata4['weekday_en']=label_encoders['weekday'].inverse_transform(y2_test)\ndata4.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k=(reg.predict(X2_test) == y2_test) # Determine how many were predicted correctly\nk.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y2_test, reg.predict(X2_test), labels=y2_test.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,16))\nplot_confusion_matrix(cm,data['weekday'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Decision tREE\ntreedata2 =  new_extract2[['city_or_county','participant_gender','participant_type','state','participant_age','weekday','incident_characteristics','gun_type']]\ntreedata2['incident_type'] = treedata2['incident_characteristics'].apply(lambda x: 'True' if x == 'Mass Shooting (4+ victims injured or killed excluding the subject/suspect/perpetrator, one location)' else 'False')\ntreedata2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_label_encoder_dict(df):\n    from sklearn.preprocessing import LabelEncoder\n    \n    label_encoder_dict = {}\n    for column in df.columns:\n        # Only create encoder for categorical data types\n        if not np.issubdtype(df[column].dtype, np.number):\n            label_encoder_dict[column]= LabelEncoder().fit(df[column])\n    return label_encoder_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoders = create_label_encoder_dict(treedata2)\nprint(\"Encoded Values for each Label\")\nprint(\"=\"*32)\nfor column in label_encoders:\n    print(\"=\"*32)\n    print('Encoder(%s) = %s' % (column, label_encoders[column].classes_ ))\n    print(pd.DataFrame([range(0,len(label_encoders[column].classes_))], columns=label_encoders[column].classes_, index=['Encoded Values']  ).T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply each encoder to the data set to obtain transformed values\ndata5 = treedata2.copy() # create copy of initial data set\nfor column in data5.columns:\n    if column in label_encoders:\n        data5[column] = label_encoders[column].transform(data5[column])\n\nprint(\"Transformed data set\")\nprint(\"=\"*32)\ndata5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# separate our data into dependent (Y) and independent(X) variables\nX_data = data5[['city_or_county','participant_type', 'participant_age','state','weekday','gun_type']]\nY_data = data5['incident_type']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = DecisionTreeClassifier(max_depth=2, criterion='entropy') # Change the max_depth to 10 or another number\nclf.fit(X_data, Y_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame([ \"%.2f%%\" % perc for perc in (clf.feature_importances_ * 100) ], index = X_data.columns, columns = ['Feature Significance in Decision Tree'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import graphviz\ndot_data = tree.export_graphviz(clf,out_file=None, \n                                feature_names=X_data.columns, \n                         class_names=label_encoders[Y_data.name].classes_,  \n                         filled=True, rounded=True,  proportion=True,\n                                node_ids=True, #impurity=False,\n                         special_characters=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graph = graphviz.Source(dot_data) \ngraph","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tree_to_code(tree, feature_names, label_encoders={}):\n    from sklearn.tree import _tree\n\n    '''\n    Outputs a decision tree model as a Python function\n    \n    Parameters:\n    -----------\n    tree: decision tree model\n        The decision tree to represent as a function\n    feature_names: list\n        The feature names of the dataset used for building the decision tree\n    '''\n\n    tree_ = tree.tree_\n    feature_name = [\n        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n        for i in tree_.feature\n    ]\n    print (\"def decision_tree({}):\" .format(\", \".join(feature_names)))\n\n    def recurse(node, depth):\n        indent = \"  \" * depth\n        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n            name = feature_name[node]\n            threshold = tree_.threshold[node]\n            print (\"{}if {} <= {}:\".format(indent, name, threshold))\n            recurse(tree_.children_left[node], depth + 1)\n            print(\"{}else:  # if {} > {}\".format(indent, name, threshold)) \n            recurse(tree_.children_right[node], depth + 1)\n        else:\n            #print(node)\n            \n            name = tree_.feature[node] \n            if name in label_encoders:\n                if isinstance(label_encoders[name] , LabelEncoder) or True:\n                    print ( \"{}-return {}\".format(indent, label_encoders[name].inverse_transform(tree_.value[node])))\n                    return\n            print (\"{}return {} # Distribution of samples in node\".format(indent, tree_.value[node]))\n\n    recurse(0, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Decision Tree Rules\")\nprint(\"=\"*32)\ntree_to_code(clf, X_data.columns, label_encoders)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Encoded Values for each Label\")\nprint(\"=\"*32)\nfor column in label_encoders:\n    print(\"=\"*32)\n    print('Encoder(%s) = %s' % (column, label_encoders[column].classes_ ))\n    print(pd.DataFrame([range(0,len(label_encoders[column].classes_))], columns=label_encoders[column].classes_, index=['Encoded Values']  ).T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k=(clf.predict(X_data) == Y_data) # Determine how many were predicted correctly\nk.value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(Y_data, clf.predict(X_data), labels=Y_data.unique())\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    import itertools\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(cm,data5['incident_type'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# separate our data into dependent (Y) and independent(X) variables\n# separate our data into dependent (Y) and independent(X) variables\nX3_data = data3[['weekday','participant_type','participant_gender','participant_age','gun_stolen','participant_status','incident_characteristics']]\nY3_data = data3['state'] # actually weekday column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split \nX3_train, X3_test, y3_train, y3_test = train_test_split(X3_data, Y3_data, test_size=0.30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import linear model package (has several regression classes)\nfrom sklearn import linear_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create an instance of linear regression\nreg = linear_model.LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.coef_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Regression Coefficients\")\npd.DataFrame(reg.coef_,index=X_train.columns,columns=[\"Coefficient\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Intercept\nreg.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions using the testing set\ntest_predicted = reg.predict(X_test)\ntest_predicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata6 = X_test.copy()\ndata6['predicted_state_level']=test_predicted\ndata6['state_level']=y_test\ndata6.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, r2_score\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(y_test, test_predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Variance score: %.2f' % r2_score(y_test, test_predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"help(reg.score)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.score(X_test,y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.fit(data2[X3_train.columns])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.components_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.n_features_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.n_components_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X3_test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X3_test =X3_test.fillna(X3_test.mean())\nX_reduced = pca.transform(X3_test)\nX_reduced","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(X_reduced, y3_test,  color='black')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.scatter(X_reduced, y3_test,  color='black')\nplt.plot(X_reduced, test_predicted, color='blue',linewidth=1)\nplt.plot(X_reduced, test_predicted, color='red',linewidth=1)\n\n#plt.xticks(())\n#plt.yticks(())\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(y3_test, test_predicted, 'ro-')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.std(np.abs(y3_test-test_predicted))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(y3_test, test_predicted, 'ro-')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.std(np.abs(y_test-test_predicted))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data4=pd.DataFrame({'actual':y_test,'pred':test_predicted})\ndata4.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data4.sort_values('actual').plot(kind='line',x='actual',y='pred')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.corr()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}