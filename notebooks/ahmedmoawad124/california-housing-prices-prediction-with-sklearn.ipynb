{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Solving the California Housing Prices Prediction problem using:\n## 1. Linear Regression\n## 2. Lasso Regression\n## 3. Ridge Regression\n## 4. ElasticNet Regression\n## 5. Support Vector Regressor (SVR)\n## 6. Nearest Nieghbors regression\n## 7. Decision trees\n## 8. SGD Regressor\n## 9. Neural Network\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sklearn\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neural_network import MLPRegressor\n\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Reading the dataset (California Housing Prices)"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_dataset = pd.read_csv('../input/california-housing-prices/housing.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing the first 5 columns \nhousing_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing random 5 samples\nhousing_dataset.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the shape of the data\nhousing_dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_dataset.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the missing data\nhousing_dataset = housing_dataset.dropna()\n\n# the shape after dropping the missing data\nhousing_dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_dataset.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exporing the categorical data\nhousing_dataset['ocean_proximity'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting categorical values to numeric values using one-hot encoding\nhousing_dataset = pd.get_dummies(housing_dataset, columns= ['ocean_proximity'])\n\n# Another techinque:\n'''\nocean_proximity = ['NEAR BAY', '<1H OCEAN', 'INLAND', 'NEAR OCEAN', 'ISLAND']\nlabel_encoding = preprocessing.LabelEncoder()\nlabel_encoding = label_encoding.fit(ocean_proximity)\nhousing_dataset['ocean_proximity'] = label_encoding.transform(housing_dataset['ocean_proximity'])\nlabel_encoding.classes_\n'''\n\n# Showing the data after Converting categorical values to numeric values\nhousing_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Original data frame had 10 columns, we now have 14 columns\nhousing_dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing the correlation between data\nhousing_dataset_correlation = housing_dataset.corr()\nhousing_dataset_correlation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# housing dataset correlation in heat map\nplt.figure(figsize=(15,12))\nsns.heatmap(housing_dataset_correlation, annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting the data\nX = housing_dataset.drop('median_house_value', axis = 1)  # Features\nY = housing_dataset['median_house_value']                 # Target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data into traing and testing data\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=45)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Using Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_regressor = LinearRegression(normalize = True, fit_intercept = False, copy_X = True).fit(x_train, y_train)\n# Normailzation scales all numeric features to be between 0 and 1. \n# Having features in the same scale can vastly improve tne performance of your ML model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training score : \", linear_regressor.score(x_train, y_train))\n# R-Square is a measure of how well our linear mogel captures the underlying variation in our training data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = linear_regressor.predict(x_test)\n\nprint(\"testing score : \", r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_regressor.intercept_ , linear_regressor.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Hyper-Parameters Tuning\n'''\nlinear_regressor_parameter = {'normalize': [True, False], 'fit_intercept': [True, False]}\nlinear_regressor_grid_search = GridSearchCV(LinearRegression(), linear_regressor_parameter, cv = 2)\nlinear_regressor_grid_search.fit(X, Y)\nprint('The best score',linear_regressor_grid_search.best_score_)\nprint('The best parameters',linear_regressor_grid_search.best_params_)\n'''\n\n# The result:\n\n# The best parameters {'fit_intercept': False, 'normalize': True}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Using Lasso Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_regressor = Lasso(alpha = 1, fit_intercept= True, normalize= False, max_iter = 20000).fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training score : \", lasso_regressor.score(x_train, y_train))\n# R-Square is a measure of how well our linear mogel captures the underlying variation in our training data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = lasso_regressor.predict(x_test)\nprint(\"testing score : \", r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_regressor.intercept_ , lasso_regressor.coef_ , lasso_regressor. n_iter_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyper-Parameters Tuning\n'''\nlasso_regressor_parameter = {'alpha': [0.2,0.4,0.6,0.8,1], 'normalize': [True, False], 'fit_intercept': [True, False]}\nlasso_regressor_grid_search = GridSearchCV(Lasso(max_iter = 400000), lasso_regressor_parameter, cv = 2)\nlasso_regressor_grid_search.fit(X, Y)\nprint('The best score',lasso_regressor_grid_search.best_score_)\nprint('The best parameters',lasso_regressor_grid_search.best_params_)\n'''\n\n# The result:\n\n# The best parameters {'alpha': 1, 'fit_intercept': True, 'normalize': False}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Using Ridge Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_regressor = Ridge(alpha = 0.4, fit_intercept= True, normalize= False, max_iter = 20000).fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training score : \", ridge_regressor.score(x_train, y_train))\n# R-Square is a measure of how well our linear mogel captures the underlying variation in our training data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = ridge_regressor.predict(x_test)\nprint(\"testing score : \", r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_regressor.intercept_ , ridge_regressor.coef_ , ridge_regressor. n_iter_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyper-Parameters Tuning\n'''\nridge_regressor_parameter = {'alpha': [0.2,0.4,0.6,0.8,1], 'normalize': [True, False], 'fit_intercept': [True, False]}\nridge_regressor_grid_search = GridSearchCV(Ridge(max_iter = 400000), ridge_regressor_parameter, cv = 2)\nridge_regressor_grid_search.fit(X, Y)\nprint('The best score',ridge_regressor_grid_search.best_score_)\nprint('The best parameters',ridge_regressor_grid_search.best_params_)\n'''\n\n# The result:\n\n# The best parameters {'alpha': 0.4, 'fit_intercept': True, 'normalize': False}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Using ElasticNet Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"elastic_regressor = ElasticNet(alpha = 1, l1_ratio = 1, normalize = False, fit_intercept= True, max_iter = 20000).fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training score : \", elastic_regressor.score(x_train, y_train))\n# R-Square is a measure of how well our linear mogel captures the underlying variation in our training data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = elastic_regressor.predict(x_test)\nprint(\"testing score : \", r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elastic_regressor.intercept_ , elastic_regressor.coef_ , elastic_regressor. n_iter_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyper-Parameters Tuning\n'''\nelastic_regressor_parameter = {'alpha': [0.2,0.4,0.6,0.8,1], 'l1_ratio': [0,0.2,0.5,0.8,1] , 'normalize': [True, False], 'fit_intercept': [True, False]}\nelastic_regressor_grid_search = GridSearchCV(ElasticNet(max_iter = 400000), elastic_regressor_parameter, cv = 2)\nelastic_regressor_grid_search.fit(X, Y)\nprint('The best score',elastic_regressor_grid_search.best_score_)\nprint('The best parameters',elastic_regressor_grid_search.best_params_)\n'''\n\n# The result:\n\n# The best parameters {'alpha': 1, 'fit_intercept': True, 'l1_ratio': 1, 'normalize': False}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Using Support Vector Regressor (SVR)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVR tries to fit as many points as possiple into a margine surrounding the best fit line\nsvr_regressor = SVR(kernel='linear', epsilon = 0.2, C = 1).fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training score : \", svr_regressor.score(x_train, y_train))\n# R-Square is a measure of how well our linear mogel captures the underlying variation in our training data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = svr_regressor.predict(x_test)\nprint(\"testing score : \", r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyper-Parameters Tuning\n'''\nsvr_regressor_parameter = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'epsilon': [0.05,0.1,0.2,0.3], 'C': [0.2,0.5,0.8,1]}\nsvr_regressor_grid_search = GridSearchCV(SVR(), svr_regressor_parameter, cv = 2)\nsvr_regressor_grid_search.fit(X, Y)\nprint('The best score',svr_regressor_grid_search.best_score_)\nprint('The best parameters',svr_regressor_grid_search.best_params_)\n'''\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Using Nearest Nieghbors regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Nearest Nieghbors regression uses training data to find what is most similar to the current sample\n# Average y-values of K nearest nieghbors\n\nknn_regressor = KNeighborsRegressor(n_neighbors = 10).fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training score : \", knn_regressor.score(x_train, y_train))\n# R-Square is a measure of how well our linear mogel captures the underlying variation in our training data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = knn_regressor.predict(x_test)\nprint(\"testing score : \", r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyper-Parameters Tuning\n'''\nknn_regressor_parameter = {'n_neighbors': [3,5,8,10,15,20,25]}\nknn_regressor_grid_search = GridSearchCV(KNeighborsRegressor(), knn_regressor_parameter, cv = 2)\nknn_regressor_grid_search.fit(X, Y)\nprint('The best score',knn_regressor_grid_search.best_score_)\nprint('The best parameters',knn_regressor_grid_search.best_params_)\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Using Decision trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision trees set up a tree structure on training data which helps make decisions based on rules\n\ntree_regressor = DecisionTreeRegressor(max_depth = 7).fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training score : \", tree_regressor.score(x_train, y_train))\n# R-Square is a measure of how well our linear mogel captures the underlying variation in our training data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = tree_regressor.predict(x_test)\nprint(\"testing score : \", r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyper-Parameters Tuning\n'''\ntreen_regressor_parameter = {'max_depth': [2,3,4,5,6,7,8,9,10]}\ntree_regressor_grid_search = GridSearchCV(DecisionTreeRegressor(), tree_regressor_parameter, cv = 2)\ntree_regressor_grid_search.fit(X, Y)\nprint('The best score',tree_regressor_grid_search.best_score_)\nprint('The best parameters',tree_regressor_grid_search.best_params_)\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Using SGD Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is an iterative model where you use multiple iteration to find \n# the best linear model that fit your underlyning data.\n\n# It works well with standarized features\n# I will standarize the all features except the categorical features\nfeatures_not_categorical_columns = ['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households','median_income']\nfeatures_not_categorical_train = x_train[features_not_categorical_columns]\nx_train_scaled = x_train.copy()\nscaler = StandardScaler()\nfeatures_not_categorical_train_scaled = scaler.fit_transform(features_not_categorical_train)\nx_train_scaled[features_not_categorical_columns] = features_not_categorical_train_scaled\n\nsgd_regressor = SGDRegressor(max_iter = 100000, tol = 1e-3).fit(x_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training score : \", sgd_regressor.score(x_train_scaled, y_train))\n# R-Square is a measure of how well our linear mogel captures the underlying variation in our training data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_not_categorical_test = x_test[features_not_categorical_columns]\nx_test_scaled = x_test.copy()\nscaler = StandardScaler()\nfeatures_not_categorical_test_scaled = scaler.fit_transform(features_not_categorical_test)\nx_test_scaled[features_not_categorical_columns] = features_not_categorical_test_scaled\n\ny_pred = sgd_regressor.predict(x_test_scaled)\nprint(\"testing score : \", r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9. Using Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"# It do well with standarized features\n\nnn_regressor = MLPRegressor(activation = 'relu', hidden_layer_sizes = (32,64,128,64,8), solver= 'lbfgs', max_iter= 20000).fit(x_train_scaled, y_train)\n\n# 1- hidden_layer_sizes = (No. of units or neurons in 1st hidden layer, No. of units or neurons in 2nd hidden layer, .... )\n# 2- Activation function for the hidden layer: {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default=’relu’  \n# 3- solver for weight optimization : {‘lbfgs’, ‘sgd’, ‘adam’}, default=’adam’\n# 4- learning_rate_initdouble, default=0.001, The initial learning rate used. \n# It controls the step-size in updating the weights. Only used when solver=’sgd’ or ‘adam’.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training score : \", nn_regressor.score(x_train_scaled, y_train))\n# R-Square is a measure of how well our linear mogel captures the underlying variation in our training data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = nn_regressor.predict(x_test_scaled)\nprint(\"testing score : \", r2_score(y_test, y_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}