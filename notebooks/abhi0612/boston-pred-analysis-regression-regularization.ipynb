{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import datasets\nboston= datasets.load_boston()\nboston.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data= pd.DataFrame(boston.data, columns=boston.feature_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['MEDV']=boston.target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"The Boston Housing Dataset\n\nThe Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. The following describes the dataset columns:\n\nCRIM - per capita crime rate by town\nZN - proportion of residential land zoned for lots over 25,000 sq.ft.\nINDUS - proportion of non-retail business acres per town.\nCHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\nNOX - nitric oxides concentration (parts per 10 million)\nRM - average number of rooms per dwelling\nAGE - proportion of owner-occupied units built prior to 1940\nDIS - weighted distances to five Boston employment centres\nRAD - index of accessibility to radial highways\nTAX - full-value property-tax rate per $10,000\nPTRATIO - pupil-teacher ratio by town\nB - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\nLSTAT - % lower status of the population\nMEDV - Median value of owner-occupied homes in $1000's"},{"metadata":{"trusted":false},"cell_type":"code","source":"#data Wrangling\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.isna().any","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for columns in data:\n    plt.figure()\n    sns.distplot(data[columns], color='red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for columns in data:\n    plt.figure()\n    sns.boxplot(y=data[columns], color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Bivariate Analysis\ndata.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation of MEDV with other features\ndata.corr()['MEDV'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Heatmap\nplt.figure(figsize=(10,8))\nsns.heatmap(data.corr(), annot=True, linecolor='white', linewidths='0.1', square=True)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#PairPlot\nsns.pairplot(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Regplot\nfor columns in data:\n    plt.figure()\n    sns.regplot(x=columns, y='MEDV' , data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Splitting the dataset\nX= data.iloc[:,:-1]\nY= data.iloc[:,-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Data Preprocessing\n#1 - No missing value is there\ndata.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test= train_test_split(X, Y, test_size=0.25, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Linear Regression- taking all the variables\nfrom sklearn.linear_model import LinearRegression\nlr_reg= LinearRegression(normalize=True)\n\nlr_reg.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#coffecient values\ncoff= pd.DataFrame(lr_reg.coef_, index=X_train.columns).sort_values(by=[0], ascending=False)\ncoff.rename(columns={0:'coff'})\n\nprint('linear intercept is {}'.format(lr_reg.intercept_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Y_pred= lr_reg.predict(X_test)\nplt.scatter(Y_test, Y_pred)\n#less scattered it will be more better it is","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Evaluating metrics for linear regression\nfrom sklearn import metrics\nprint('R2 score for linear reg is {}'.format(metrics.r2_score(Y_test, Y_pred)))\nprint('MSE for linear reg is {}'.format(metrics.mean_squared_error(Y_test, Y_pred)))\nprint('RMSE for linear reg is {}'.format(np.sqrt(metrics.mean_squared_error(Y_test, Y_pred))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets use backward elimination to build model\nfrom scipy.special import factorial\nimport statsmodels.formula.api as sm\n\nones= pd.Series(np.ones(shape=(506,1), dtype=int).ravel())\nX1= pd.concat([ones, X], axis=1, )\n\nX1_opt=X1.iloc[:,[0,1,2,4,5,6,8,9,10,11,12,13]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_OLS= sm.OLS(endog=Y, exog=X1_opt)\nreg_OLS= reg_OLS.fit()\nreg_OLS.summary()\n#thus 'Age' and 'Indus' has very high p value. So they are statistically insignificant--thus removing them","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Thus using Backward elimination the R2 value is 0.741 and adj R2 is 0.735 "},{"metadata":{},"cell_type":"raw","source":"=========================================================================================="},{"metadata":{"trusted":false},"cell_type":"code","source":"#Decision tree\nfrom sklearn.tree import DecisionTreeRegressor\ndtree= DecisionTreeRegressor(criterion='mse', random_state=1)\n\ndtree.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('R2 score for decision tree is {}'.format(metrics.r2_score(Y_test, dtree.predict(X_test))))\nprint('RMSE for decision tree is {}'.format(np.sqrt(metrics.mean_squared_error(Y_test, dtree.predict(X_test)))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.Series(dtree.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n#Looks like only two columns has high dependency - LSTAT, RM\n\nsns.barplot(x=X_train.columns, y=dtree.feature_importances_)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#lets draw the tree to analyse better\nfrom sklearn.externals.six import StringIO\nfrom IPython.display import Image\nfrom sklearn.tree import export_graphviz\nfrom sklearn import tree\nimport pydot","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Create DOT data\ndot_data= StringIO()\nexport_graphviz(dtree, out_file=dot_data, feature_names=X_train.columns.tolist(), rounded=True, filled=True)\n\n#Draw Graph\ngraph= pydot.graph_from_dot_data(dot_data.getvalue())\n\n#Show Graph\nImage(graph[0].create_png())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Using grid search CV for best parameter for decision tree\nfrom sklearn.model_selection import GridSearchCV\ndt= DecisionTreeRegressor(random_state=5)\nparams= [{'max_depth':[4,8,12,16,20], \"max_leaf_nodes\":range(2,20)}]\ngrid= GridSearchCV(dt, param_grid=params, refit=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"grid.fit(X_train, Y_train)\ngrid_predictions= grid.predict(X_test)\n\nprint('Accuracy Score:{}'.format(metrics.r2_score(Y_test, grid_predictions)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Best hyperparameter is {}'.format(grid.best_params_))\nprint('\\n')\nprint('Best Estimator is :')\ngrid.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Using hyperparameter- max-depth=4, max_leaf_nodes=7\ndtree1= DecisionTreeRegressor(criterion='mse', max_depth= 4, max_leaf_nodes= 7, random_state=1)\ndtree1.fit(X_train, Y_train)\nmetrics.r2_score(Y_test, dtree1.predict(X_test))\nnp.sqrt(metrics.mean_squared_error(Y_test, dtree1.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Create DOT data\ndot_data= StringIO()\nexport_graphviz(grid.best_estimator_, out_file=dot_data, feature_names=X_train.columns.tolist(),class_names=['MEDV'], rounded=True, filled=True)\n\n#Draw Graph\ngraph= pydot.graph_from_dot_data(dot_data.getvalue())\n\n#Show Graph\nImage(graph[0].create_png())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Important features in Decision tree\npd.DataFrame(grid.best_estimator_.feature_importances_, index=X_train.columns, columns=['Importance'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#using only LSTAT and RM to predict the MEDV value\nX_train_dt= X_train[['LSTAT', 'RM']]\nX_test_dt= X_test[['LSTAT', 'RM']]\n\ndtr= DecisionTreeRegressor(criterion='mse', max_depth=4, max_leaf_nodes=7, random_state=1)\ndtr.fit(X_train_dt, Y_train)\n\n\nprint('The R2 score is {}'.format(metrics.r2_score(Y_test, dtr.predict(X_test_dt))))\nprint('The RMSE is {}'.format(np.sqrt(metrics.mean_squared_error(Y_test, dtr.predict(X_test_dt)))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"====================================================================================================================="},{"metadata":{"trusted":false},"cell_type":"code","source":"#Random Forest\n#Taking max_depth = 4 using Decision tree\n\nfrom sklearn.ensemble import RandomForestRegressor\nrf= RandomForestRegressor(random_state=1,max_depth=4)\nparams1= [{'n_estimators': range(10,100)}]\ngrid1= GridSearchCV(rf, param_grid=params1, refit=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"grid1.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"grid_predictions1= grid1.predict(X_test)\n\nprint('Accuracy Score: {}'.format(metrics.r2_score(Y_test, grid_predictions1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('The best value for n_estimator is {}'.format(grid1.best_params_))\npd.DataFrame(grid1.best_estimator_.feature_importances_, index=X_train.columns, columns=['Importance'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('One of the estimator is {}'.format(grid1.best_estimator_.estimators_[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Create DOT data\ndot_data= StringIO()\nexport_graphviz(grid1.best_estimator_.estimators_[1], out_file=dot_data, feature_names=X_train.columns.tolist(),class_names=['MEDV'], rounded=True, filled=True)\n\n#Draw Graph\ngraph= pydot.graph_from_dot_data(dot_data.getvalue())\n\n#Show Graph\nImage(graph[0].create_png())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"============================================================================================================================="},{"metadata":{"trusted":false},"cell_type":"code","source":"#Regularization\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"lr= LinearRegression()\nlr.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(lr.coef_)\nprint('\\n')\nprint('The R2 score for train data is {}'.format(lr.score(X_train, Y_train)))\nprint('The R2 for test data is {}'.format(lr.score(X_test, Y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Ridge Regression\nrr1= Ridge(alpha=0.01)\nrr1.fit(X_train, Y_train)\nprint(rr1.coef_)\nprint('\\n')\nprint('The R2 score for train data is {}'.format(rr1.score(X_train, Y_train)))\nprint('The R2 score for train data is {}'.format(rr1.score(X_test, Y_test)))\n\nsns.barplot(X_train.columns, rr1.coef_)\nplt.xticks(rotation=90)\n\n#looks like there is no change in coff wrt Linear Reg as alpha value is very small","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rr100= Ridge(alpha=100)\nrr100.fit(X_train, Y_train)\nprint(rr100.coef_)\nprint('\\n')\nprint('The R2 score for train data is {}'.format(rr100.score(X_train, Y_train)))\nprint('The R2 score for train data is {}'.format(rr100.score(X_test, Y_test)))\n\nsns.barplot(X_train.columns, rr100.coef_)\nplt.xticks(rotation=90)\n\n#looks like there is no change in coff wrt Linear Reg as alpha value is very small","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plotting the coff values for LR, Ridge(0.01), Ridge(100)\nplt.plot(lr.coef_ ,linestyle='none' , marker='+', color='blue', markersize=10, label='lin reg')\nplt.plot(rr1.coef_ ,linestyle='none' , marker='o', color='green', label=' Ridge(0.01)')\nplt.plot(rr100.coef_ ,linestyle='none' , marker='*', color='red', label= 'Ridge(100)')\nplt.xlabel(boston.feature_names)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Zipping the coff values\nlist(zip(X_train.columns, rr100.coef_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Lasso regression\nlasso1= Lasso(alpha=0.01)\nlasso1.fit(X_train, Y_train)\n\nlasso1.coef_\nprint('The R2 score for train data is {}'.format(lasso1.score(X_train, Y_train)))\nprint('The R2 score for test data is {}'.format(lasso1.score(X_test, Y_test)))\n\nsns.barplot(X_train.columns, lasso1.coef_)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"lasso2= Lasso(alpha=1)\nlasso2.fit(X_train, Y_train)\n\nlasso2.coef_\nprint('The R2 score for train data is {}'.format(lasso2.score(X_train, Y_train)))\nprint('The R2 score for test data is {}'.format(lasso2.score(X_test, Y_test)))\n\nsns.barplot(X_train.columns, lasso2.coef_)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"list(zip(X_train.columns, lasso2.coef_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(lr.coef_, linestyle='none', marker='*', color='blue', label='lin reg')\nplt.plot(lasso1.coef_, linestyle='none', marker='o',color='red', label='lasso(0.01)')\nplt.plot(lasso2.coef_, linestyle='none', marker='+', color='green', label='lasso(1)')\nplt.xlabel(X_train.columns)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('no of features where coff is not zero: {}'.format(sum((lasso2.coef_!=0))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_score=[]\ntest_score= []\n\nfor i in [0.01,0.1,1,2,3,4,5,6,7,8,9,10]:\n    lasso= Lasso(alpha=i)\n    lasso.fit(X_train, Y_train)\n    \n    train_sc= lasso.score(X_train, Y_train)\n    test_sc= lasso.score(X_test, Y_test)\n    \n    train_score.append(train_sc)\n    test_score.append(test_sc)\n    \nprint(train_score)\nprint(test_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure()\nplt.plot([0.01,0.1,1,2,3,4,5,6,7,8,9,10], train_score, marker='*', color='green')\nplt.plot([0.01,0.1,1,2,3,4,5,6,7,8,9,10], test_score, marker='+', color='red')\nplt.xlabel('x-values')\nplt.ylabel('accuracy score')\nplt.plot()\n\n#at x=1 the accuracy value is almost sme for train and test data","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Elastic net Regression\nfrom sklearn.linear_model import ElasticNet\nElastic1= ElasticNet(alpha=1)\n\nElastic1.fit(X_train, Y_train)\ntrain_score= Elastic1.score(X_train, Y_train)\ntest_score= Elastic1.score(X_test, Y_test)\n\nprint('Train score={}'.format(train_score))\nprint('Test score={}'.format(test_score))\nprint('No of features used={}'.format(np.sum(Elastic1.coef_!=0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_score=[]\ntest_score= []\n\nfor i in [0.01,0.1,1,2,3,4,5,6,7,8,9,10]:\n    elastic= ElasticNet(alpha=i)\n    elastic.fit(X_train, Y_train)\n    \n    train_sc= elastic.score(X_train, Y_train)\n    test_sc= elastic.score(X_test, Y_test)\n    \n    train_score.append(train_sc)\n    test_score.append(test_sc)\n    \nprint(train_score)\nprint(test_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure()\nplt.plot([0.01,0.1,1,2,3,4,5,6,7,8,9,10], train_score, marker='*', color='green')\nplt.plot([0.01,0.1,1,2,3,4,5,6,7,8,9,10], test_score, marker='+', color='red')\nplt.xlabel('x-values')\nplt.ylabel('accuracy score')\nplt.plot()\n\n#at x=2 the accuracy value is almost sme for train and test data","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Using Grid search CV for Elatic net regression\nfrom sklearn.model_selection import GridSearchCV\nelasticnet= ElasticNet()\nparam_elastic={'alpha':[0.01,0.1,1,2,3,4,5,6,7,8,9,10,100]}\ngrid_elastic= GridSearchCV(estimator=elasticnet, param_grid=param_elastic, cv=3, refit= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"grid_elastic.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_elastic.best_params_)\nmetrics.r2_score(Y_test, grid_elastic.best_estimator_.predict(X_test))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}