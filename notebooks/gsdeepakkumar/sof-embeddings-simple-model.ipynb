{"cells":[{"metadata":{},"cell_type":"markdown","source":"References :\n\n1.https://www.kaggle.com/opanichev/tf2-0-qa-feature-extraction\n\n2.https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-qiqc\n\n3.https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n\n4.https://towardsdatascience.com/hacking-scikit-learns-vectorizers-9ef26a7170af\n\n5.https://towardsdatascience.com/benchmarking-python-nlp-tokenizers-3ac4735100c5\n\n6.[Abhiskek Thakur's youtube channel](https://www.youtube.com/user/abhisheksvnit)\n\n7.https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"},{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/shujian/single-rnn-with-4-folds-clr\n\nhttps://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go\n\nhttps://www.analyticsvidhya.com/blog/2020/03/pretrained-word-embeddings-nlp/#:~:text=Pretrained%20Word%20Embeddings%20are%20the,a%20form%20of%20Transfer%20Learning.\n\nhttps://www.kaggle.com/hung96ad/pytorch-starter"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom wordcloud import WordCloud,STOPWORDS\nimport re\nimport operator\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBED_PATH='../input/glove-reddit-comments/GloVeReddit120B/GloVe.Reddit.120B.300D.txt'\nMAX_LEN=300\nMAX_FEATURES=50000\nBATCH_SIZE=32\nNUM_EPOCHS=10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nfrom sklearn import model_selection\nfrom tqdm import tqdm\nfrom html import unescape\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom nltk.tokenize import word_tokenize,wordpunct_tokenize,regexp_tokenize\nfrom sklearn import metrics\nimport random\ntqdm.pandas()\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader,Dataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def seed_torch(seed=45):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset consists of stack overflow questions collected between 2016-2020."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Read the data,\ndf=pd.read_csv(\"../input/60k-stack-overflow-questions-with-quality-rate/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Number of rows:{df.shape[0]} and Number of columns:{df.shape[1]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum() # no null values in any column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to solve the task of classifying the questions based on their quality.We have been provided with three columns - title,body and questions tag.The target variable is named as column Y.Lets check that column."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Y'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A quick look at the data description provides and idea about what these exactly mean,\n\n1.HQ: High-quality posts with 30+ score and without a single edit.\n\n2.LQ_EDIT: Low-quality posts with a negative score and with multiple community edits. However, they still remain open after the edits.\n\n3.LQ_CLOSE: Low-quality posts that were closed by the community without a single edit.\n\nLets take a look at few sample questions from each category."},{"metadata":{"trusted":true},"cell_type":"code","source":"hq_data=df.loc[df['Y']=='HQ',]['Body'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hq_data[0:4]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A look at the first 4 question description suggest that the question is very exact and provides lot of background information."},{"metadata":{"trusted":true},"cell_type":"code","source":"lq_data=df.loc[df['Y']=='LQ_EDIT',]['Body'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lq_data[0:4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"From the above 4 questions we can see that they are very generic and do not furnish any reproducible examples."},{"metadata":{"trusted":true},"cell_type":"code","source":"lq_close=df.loc[df['Y']=='LQ_CLOSE',]['Body'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lq_close[0:4]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above questions are low quality questions which are closed by the community.Similar to LQ_EDIT,these questions do not provide any reproducible example."},{"metadata":{},"cell_type":"markdown","source":"We find that the questions are filled with lot of html tags,links to images and code blocks.Lets do some cleaning on the data and visualize each of the categories using wordcloud to get and understanding on how these questions differ by each category."},{"metadata":{"trusted":true},"cell_type":"code","source":"#REFERENCE https://www.kaggle.com/aashita/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    #create a set of common html tags,\n    html_tags={'<p>','</p>','<code>','</code>','<pre>','</pre>','<div>','</div>','<br/>','<title>','</title>','<body>','</body>','title'}\n    stopwords.union(html_tags)\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_wordcloud(df.loc[df['Y']=='HQ',]['Body'], title=\"Questions in HQ category\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of the common words are Java,Android,LinkedIn,Optionals"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_wordcloud(df.loc[df['Y']=='LQ_EDIT',]['Body'], title=\"Questions in LQ_EDIT category\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The common words here are URLS,Saturday(?!),want,form,table."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_wordcloud(df.loc[df['Y']=='LQ_CLOSE',]['Body'], title=\"Questions in LQ_CLOSE category\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of the common words here are integer,inside,new,already,repeating."},{"metadata":{},"cell_type":"markdown","source":"From the above three wordcloud,we understand that there is a difference in the pattern and the kind of words on how the questions are being posted which required for our model to classify the groups."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'''Average length of the question body for category HQ {np.mean(df.loc[df['Y']=='HQ',]['Body'].apply(lambda x:len(x.split())))}''')\nprint(f'''Average length of the question body for category LQ_EDIT {np.mean(df.loc[df['Y']=='LQ_EDIT',]['Body'].apply(lambda x:len(x.split())))}''')\nprint(f'''Average length of the question body for category LQ_CLOSE {np.mean(df.loc[df['Y']=='LQ_CLOSE',]['Body'].apply(lambda x:len(x.split())))}''')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the categories are equally balanced,we can use simple kfold to split our data for modelling."},{"metadata":{"trusted":true},"cell_type":"code","source":"##mapping the targets as integer values for modelling,\ntarget_dict={'HQ':0,'LQ_EDIT':1,'LQ_CLOSE':2}\n\ndf.loc[:,'Y']=df['Y'].map(target_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Split the data into train and validation set:\n### reference : Abhiskek Thakur - https://www.youtube.com/user/abhisheksvnit\ndf['kfold']=-1\nkf=model_selection.KFold(n_splits=5,random_state=40,shuffle=True)\nfor fold_,(trn_,val_) in enumerate(kf.split(df)):\n    print(f'Fold {fold_} Training {len(trn_)} Validation {len(val_)}')\n    print(\"\")\n    df.loc[val_,'kfold']=fold_\n    print(f'Split of categories for fold {fold_}')\n    print(df.loc[df['kfold']==fold_,'Y'].value_counts())\n    print(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Simple Model"},{"metadata":{},"cell_type":"markdown","source":"Lets build a simple baseline model to classify the groups based on the body of the question.Though I am sure the model will not provide the best accuracy,always starting with a baseline model will help to build our approach for future analysis.\n\nWe will use our own custom function to clean and tokenize and create a count vectorizer which has unigram tokens.Lets see a sample first.\n\nMultinomial Naive Bayes model is used over the vectorizer with log-loss."},{"metadata":{"trusted":true},"cell_type":"code","source":"spacy.load('en')\nlemma=spacy.lang.en.English()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    text=str(text)\n    #cleaning URLs\n    text=re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n    #cleaning html elements,\n    text=re.sub(r'<.*?>', '', text)\n    #replace carriage return with space\n    text=text.replace(\"\\n\",\" \").replace(\"\\r\",\" \")\n    #replace punctuations with space,\n    punct='?!.,\"#$%\\'()*+-/:;=@[\\\\]^_`{|}~<>&'\n    for p in punct:\n        text=text.replace(p,\" \")\n    #replace single quote with empty character,\n    text=text.replace(\"'`\",'')\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef my_tokenizer(doc):\n    tokens = lemma(doc)\n    return([token.lemma_ for token in tokens])\n\ndef reg_tokenize(doc):\n    doc=clean_text(doc)\n    doc=regexp_tokenize(doc,pattern='\\s+',gaps=True) # gaps=True,since we want to find separators between tokens.\n    return doc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample=list(df['Body'])[0:3]\ncount_vect=CountVectorizer(tokenizer=reg_tokenize,\n                           token_pattern=None,\n                           ngram_range=(1,1),\n                           stop_words='english')\ncount_vect.fit(sample)\nprint(sample)\nprint(\"\\n\\n\")\nprint(count_vect.vocabulary_)\nprint(\"\")\nprint(len(count_vect.vocabulary_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample=list(df['Body'])[0:3]\ncount_vect=CountVectorizer(tokenizer=my_tokenizer,\n                           token_pattern=None,\n                           ngram_range=(1,1),\n                           stop_words='english')\ncount_vect.fit(sample)\nprint(sample)\nprint(\"\\n\\n\")\nprint(count_vect.vocabulary_)\nprint(\"\")\nprint(len(count_vect.vocabulary_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above two examples on tokenization,we will use the regexp_tokenizer along with cleaned sentence for our model for simplicity."},{"metadata":{"trusted":true},"cell_type":"code","source":"##Reference :sklearn documentation\n#https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f\nlosses=[]\nfor fold_ in range(5):\n    #initialize train data,\n    x_train=df[df.kfold!=fold_].reset_index(drop=True)\n    #initialize valid data,\n    x_valid=df[df.kfold==fold_].reset_index(drop=True)\n    #print(f'Shape of x_train {x_train.shape} Shape of x_valid {x_valid.shape}')\n    #initialize count vectorizer,\n    count_vect=CountVectorizer(tokenizer=reg_tokenize,\n                           token_pattern=None,\n                           ngram_range=(1,1),\n                           stop_words='english')\n    #print(\"Fitting the count vectorizer\")\n    count_vect.fit(x_train['Body'])\n    \n    x_train_counts=count_vect.transform(x_train['Body'])\n    \n    x_valid_counts=count_vect.transform(x_valid['Body'])\n    #print(type(x_train_counts),type(x_valid_counts))\n    #transform train and test question body,\n    \n    \n    clf=MultinomialNB()\n    \n    clf.fit(x_train_counts,x_train['Y'].values)\n    \n    predicted_class=clf.predict_proba(x_valid_counts)\n    \n    loss=metrics.log_loss(x_valid['Y'].values,predicted_class)\n    losses.append(loss)\n    \n    print(f'Fold {fold_} Log loss {loss}')\n\nprint(\"Average Log Loss\",np.mean(losses))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With count vectors and Naive Bayes model we get a average log loss value of 3.18"},{"metadata":{},"cell_type":"markdown","source":"## Using Word Embeddings"},{"metadata":{},"cell_type":"markdown","source":"We are using GloVe Reddit 300 dimension word embeddings to train our model since our model has lot of escape sequences,http links,code blocks.Let us check how much of our vocabulary is covered by the embeddings."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reference https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings\ndef build_vocab(sentences,verbose=True):\n    #initialize empty dictionary\n    vocab={}\n    for sentence in tqdm(sentences): ## loop over each sentence\n        for word in sentence: #for each word in the sentence create a vocab id\n            try:\n                vocab[word]+=1\n            except KeyError:\n                vocab[word]=1\n    return vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences=df['Body'].progress_apply(lambda x:x.split()).values\nvocab=build_vocab(sentences)\nprint({k:vocab[k] for k in list(vocab)[:5]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import embeddings :\n#Reference:https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n# embeddings_index={}\n# f=open(EMBED_PATH)\n# for line in tqdm(f):\n#     values=line.split()\n#     word=values[0]\n#     coefs=np.asarray(values[1:],dtype='float32')\n#     embeddings_index[word]=coefs\n# f.close()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word,*arr): return word,np.array(arr,dtype='float32')\n\ndef load_embeddings(EMBED_PATH):\n    embedding_index=dict(get_coefs(*o.strip().split(\" \")) for o in tqdm(open(EMBED_PATH)))\n    return embedding_index","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"embedding_index=load_embeddings(EMBED_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_coverage(vocab,embedding_index):\n    a={}\n    oov={}\n    k=0\n    i=0\n    for word in tqdm(vocab):\n        #print(\"\\nWord\",word)\n        #print(\"\\nVocab Word\",vocab[word])\n        try:\n            a[word]=embedding_index[word] ## check if the word is present in the embedding matrix\n            k+=vocab[word]\n            #print(\"\\n K value\",k)\n        except:\n            oov[word]=vocab[word]\n            i+=vocab[word]\n            #print(\"\\n i value\",i)\n            pass\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n    sortedx=sorted(oov.items(),key=operator.itemgetter(1))[::-1]\n    \n    return sortedx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oov=check_coverage(vocab,embedding_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oov[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us clean the text a bit and then try to check the coverage."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Body']=df['Body'].apply(lambda x:clean_text(x))\nsentences=df['Body'].progress_apply(lambda x:x.split()).values\nvocab=build_vocab(sentences)\noov=check_coverage(vocab,embedding_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oov[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_char_num(text):\n    text=re.sub('[0-9]{5,}','#####',text)\n    text=re.sub('[0-9]{4}','####',text)\n    text=re.sub('[0-9]{3}','###',text)\n    text=re.sub('[0-9]{2}','##',text)\n    #replace same repeating chacters with one eg.yyyyy to y\n    text=re.sub(r'[^\\w\\s]|(.)(?=\\1)', '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Body']=df['Body'].apply(lambda x:clean_char_num(x))\nsentences=df['Body'].progress_apply(lambda x:x.split()).values\nvocab=build_vocab(sentences)\noov=check_coverage(vocab,embedding_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"oov[:100]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"word_mapping={'Eror':'Error',\n        'ArayList':'Arraylist',\n        'Scaner':'Scanner',\n        'botstrap':'bootstrap',\n        'sucesfuly':'successfully',\n        'arays':'arrays',\n        'bufer':'buffer',\n        'calback':'callback',\n        'ApCompatActivity':'app compact activity',\n        'Pasword':'password',\n        'inerHTML':'inner html',\n        'clasName':'classname',\n        'TypeEror':'type error',\n        'maloc':'malloc',\n        'ApData':'appdata',\n        'foter':'footer',\n        'Bolean':'boolean',\n        'ThreadPolExecutor':'thread pool executor',\n        'styleshet':'stylesheet',\n        'tolbar':'toolbar',\n        'Colections':'collections',\n        '1px':'one pixel',\n        'SESION':'session',\n        'Arays':'arrays',\n        'BuferedReader':'buffered reader',\n        'getAplicationContext':'get application context',\n        '0px':'pixel',\n        'NulPointerException':'null pointer exception',\n        'SqlComand':'sql command',\n        'dispatchMesage':'dispatch message',\n        'MesageBox':'messagebox',\n        'DefaultBuildOperationExecutor':'default build operation executor',\n        'MethodAndArgsCaler':'method and args caller',\n        'AdWithValue':'ad with value',\n        'notebok':'notebook',\n        'debuger':'debugger',\n        'hadop':'hadoop',\n        'Fluter':'flutter',\n        '5px':'pixel',\n        'claspath':'classpath',\n        'NativeMethodAcesorImpl':'native method accessor impl',\n        'MyClas':'myclass',\n        'iHealAp':'app',\n        'AbstractAutowireCapableBeanFactory':'abstract autowire capable bean factory',\n        'PASWORD':'password',\n        'SqlConection':'sql connection',\n        'MyAp':'my app',\n        'SyntaxEror':'syntax error',\n        'ClasLoader':'class loader',\n        'SpringAplication':'spring application',\n        'Tols':'Tools',\n        'INER':'Inner',\n        'Botstrap':'bootstrap',\n        'adEventListener':'ad event lister',\n        'ValueEror':'value error',\n        'NString':'n string',\n        'adreses':'addresses',\n        'handleMesage':'handle message',\n        'getMesage':'get message',\n        'ViewControler':'view controller',\n        'apcompat':'app compact',\n        'Tolbar':'toolbar',\n        'ArayAdapter':'array adapter',\n        'JButon':'j button',\n        'UIViewControler':'ui view controller',\n        'Atempt':'attempt',\n        'custable0':'table',\n        'programaticaly':'programatically',\n        'midleware':'middleware',\n        'Extent1':'extent',\n        'opensl':'open ssl',\n        'myap':'myapp',\n        'ApComponent':'app component',\n        'AbstractBeanFactory':'abstract bean factory',\n        'stder':'std error',\n        'Acept':'accept',\n        'buton1':'button',\n        'wordpres':'word press',\n        'Nulable':'nullable',\n        'iphonesimulator':'iphone simulator',\n        'NonNul':'non null',\n        'DelegatingMethodAcesorImpl':'delegating method',\n        'JSONAray':'json array',\n        'acesing':'accessing',\n        'lokup':'lookup',\n        'nulptr':'null pointer',\n        'HtpClient':'http client',\n        'Loger':'logger',\n        'ToInt':'to int',\n        'Aplications':'applications',\n        'acesible':'accessible',\n        'ViewRotImpl':'view',\n        'alocator':'allocator',\n        'ContentValues':'content values',\n        'Iluminate':'illuminate',\n        'adClas':'add class',\n        'asoc':'associate',\n        'Runable':'runnable',\n        '0xF':'F',\n        'contentValues':'content values',\n        'findviewbyid':'find view by id',\n         'activitythread':'activity thread',\n         'araylist':'array list',\n       'oncreate':'on create',\n     'getelementbyid':'get element by id',\n     'savedinstancestate':'saved instance state',\n     'setext':'text',\n     'editext':'edit text',\n     'mainactivity':'main activity',\n     'getext':'get text',\n    'getstring':'get string'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"known=[]\nunknown=[]\nfor word,key in word_mapping.items():\n    #print(word)\n    sent=[k for k in key.split(\" \")]\n    for s in sent:\n        if s in embeddings_index :\n            #print(\"True\")\n            known.append(key)\n        else:\n            unknown.append(key)\nprint(\"Total words present in the embedding\",len(known))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace_words(text,word_mapping):\n    return ' '.join(word_mapping[t] if t in word_mapping else t for t in text.split(' '))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Body']=df['Body'].apply(lambda x:replace_words(x,word_mapping))\nsentences=df['Body'].progress_apply(lambda x:x.split()).values\nvocab=build_vocab(sentences)\noov=check_coverage(vocab,embedding_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oov[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After replacing texts we find that the total vocabulary coverage is only 35 %.We will try to create a model using this as for now."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_embed_matrix(word_index,embedding_index):\n    num_words=min(MAX_FEATURES,len(word_index))\n    \n    embed_matrix=np.random.normal(0,1,(num_words,MAX_LEN))\n    \n    for word,i in word_index.items():\n        if i>=MAX_FEATURES:continue\n        embedding_vector=embedding_index.get(word)\n        if embedding_vector is not None:embed_matrix[i]=embedding_vector\n    \n    return embed_matrix\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SOF_DATASET:\n    def __init__(self,Body,Y):\n        self.Body=Body\n        self.Y=Y\n        \n    def __len__(self):\n        return len(self.Body)\n    \n    def __getitem__(self,item):\n        body=self.Body[item,:]\n        target=self.Y[item]\n        \n        return {\n            \"body\":torch.tensor(body,dtype=torch.long),\n            \"target\":torch.tensor(target,dtype=torch.float)\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NNet(nn.Module):\n    def __init__(self):\n        super(NNet,self).__init__()\n        \n        hidden_size=128\n        \n        self.embedding=nn.Embedding(num_embeddings=MAX_FEATURES,embedding_dim=300) \n        \n        self.embedding.weight=nn.Parameter(torch.tensor(embed_matrix,dtype=torch.float32))\n        \n        self.embedding.weight.requires_grad=False\n        \n        self.embedding_dropout=nn.Dropout2d(0.3)\n        \n        self.lstm=nn.LSTM(input_size=300,hidden_size=hidden_size,bidirectional=True,batch_first=True)\n        \n        self.linear=nn.Linear(512,16)\n        \n        self.relu=nn.ReLU()\n        \n        self.dropout=nn.Dropout(0.3)\n        \n        self.softmax=nn.Softmax()\n        \n        self.out=nn.Linear(16,3)\n        \n    def forward(self,x):\n        x=self.embedding(x)\n        \n        #print(x.shape)\n        \n        #x=torch.squeeze(self.embedding_dropout(torch.unsqeeze(x,0))) ##https://stackoverflow.com/questions/57237352/what-does-unsqueeze-do-in-pytorch\n        \n        x,_=self.lstm(x)  #(output (#shape (seq_len,batch_size,num_dir*hidden_size)),h_n,c_n)\n        \n        avg_pool=torch.mean(x,1)\n        \n        max_pool,_=torch.max(x,1) #returns max_values,indices\n        \n        concat=torch.cat((avg_pool,max_pool),1)\n        \n        concat=self.relu(self.linear(concat))\n        \n        concat=self.dropout(concat)\n        #https://stackoverflow.com/questions/58122505/suppress-use-of-softmax-in-crossentropyloss-for-pytorch-neural-net\n        out=self.softmax(self.out(concat))\n        \n        #print(out.shape)\n        #print(out)\n        \n        #out=torch.argmax(out,dim=1)\n        \n        #print(out)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer=Tokenizer(num_words=MAX_FEATURES)\n    \ntokenizer.fit_on_texts(df.Body.values.tolist())\n\nembed_matrix=create_embed_matrix(tokenizer.word_index,embedding_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using {device}\")\nmodel=NNet().to(device)\noptimizer=optim.Adam(model.parameters(),lr=3e-3)\nscheduler=ReduceLROnPlateau(optimizer,verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(outputs, targets):\n    \"\"\"\n    Defining categorical cross-entropy loss function.\n    #https://discuss.pytorch.org/t/categorical-cross-entropy-loss-function-equivalent-in-pytorch/85165/3\n    \"\"\"\n    \n    return nn.NLLLoss()(torch.log(outputs), targets)\n\n\ndef accuracy(preds, y):\n    \"\"\"\n    Returns accuracy per batch\n    \"\"\"\n\n    ind= torch.argmax(preds,dim= 1)\n    correct = (ind == y).float()\n    acc = correct.sum()/float(len(correct))\n    return acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold_ in range(5):\n    \n    #initialize train data,\n    df_train=df[df.kfold!=fold_].reset_index(drop=True)\n    \n    #initialize valid data,\n    df_valid=df[df.kfold==fold_].reset_index(drop=True)\n    \n    #tokenize the sentences,\n    xtrain=tokenizer.texts_to_sequences(df_train.Body.values)\n    xvalid=tokenizer.texts_to_sequences(df_valid.Body.values)\n    \n    #pad the sequences,\n    xtrain=pad_sequences(xtrain,maxlen=MAX_LEN)\n    xvalid=pad_sequences(xvalid,maxlen=MAX_LEN)\n    \n    \n    x_train=SOF_DATASET(Body=xtrain,Y=df_train['Y'])\n    \n    x_train_loader=torch.utils.data.DataLoader(x_train,batch_size=BATCH_SIZE,shuffle=False)\n    \n    x_valid=SOF_DATASET(Body=xvalid,Y=df_valid['Y'])\n    \n    x_valid_loader=torch.utils.data.DataLoader(x_valid,batch_size=BATCH_SIZE,shuffle=False)\n    \n    model.train()\n    avg_train_loss=0.0\n    avg_train_accuracy=0.0\n    print(f\"----Starting training fold {fold_+1}--------\")\n    total_step = len(x_train_loader)\n    for epoch in range(NUM_EPOCHS):\n    \n        for i,data in enumerate(x_train_loader):\n\n            body=data['body']\n\n            targets=data['target']\n\n            targets=Variable(targets).long()\n\n            body=torch.tensor(body,dtype=torch.long).cuda()\n\n            targets=torch.tensor(targets,dtype=torch.long).cuda()\n\n            outputs=model(body)\n\n            loss=loss_fn(outputs,targets)\n\n            acc=accuracy(outputs,targets)\n\n            optimizer.zero_grad()\n\n            loss.backward()\n\n            optimizer.step()\n\n            scheduler.step(loss)\n            \n            if (i+1) % 500 == 0:\n                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                       .format(epoch+1, NUM_EPOCHS, i+1, total_step, loss.item()))\n\n\n    model.eval()\n    avg_val_loss=0.0\n    avg_val_accuracy=0.0\n    print(f\"--------Starting Validation {fold_+1}------\")\n    for i,data in enumerate(x_valid_loader):\n        epoch_loss=0\n        epoch_acc=0\n        body=data['body']\n\n        targets=data['target']\n\n        body=torch.tensor(body,dtype=torch.long).cuda()\n\n        targets=torch.tensor(targets,dtype=torch.long).cuda()\n\n        outputs=model(body)\n\n        loss=loss_fn(outputs,targets)\n\n        epoch_loss+=loss.item()\n\n        acc=accuracy(outputs,targets)\n\n        avg_val_loss+=loss.item()\n\n        avg_val_accuracy+=acc.item()\n\n\n\n\n\n    print(f\"Fold {fold_} Validation NLLoss {avg_val_loss/len(x_valid_loader):.3f} Validation accuracy {(avg_val_accuracy/len(x_valid_loader))*100:.2f} %\")\n\n\n        \n        \n        \n        \n    \n    \n    ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}