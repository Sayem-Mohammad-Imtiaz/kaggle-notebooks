{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"../input/twitter-sentiment-analysis-hatred-speech/train.csv\")\ntest=pd.read_csv(\"../input/twitter-sentiment-analysis-hatred-speech/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(\"id\",axis=1)\ntrain.isnull().value_counts()\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['label']==1].value_counts()   #negative_tweets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['label']==0].value_counts()    #positive_tweets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='label',data=train)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlength_train = train['tweet'].str.len().plot.hist(color = 'blue', figsize = (6, 4))\nlength_test = test['tweet'].str.len().plot.hist(color = 'pink', figsize = (6, 4))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c=CountVectorizer(stop_words='english')\nword=c.fit_transform(train.tweet)\nsummation=word.sum(axis=0)\nprint(summation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq=[(word,summation[0,i]) for word,i in c.vocabulary_.items()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq=sorted(freq,key=lambda x:x[1],reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frequency = pd.DataFrame(freq, columns=['word', 'freq'])\n\nprint(frequency)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frequency.head(20).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = 'blue')\nplt.title(\"20 most frequently used words in twitter\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\n#w=WordCloud(background_color=\"white\",height=500,width=500).generate(dict(frequency)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.figure(figsize=(10,8))\n#plt.imshow(w)\n#plt.title(\"WordCloud - Words of importance\", fontsize = 20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_words=' '.join([text for text in train['tweet'][train['label'] == 0]])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neg_words=' '.join(text for text in train['tweet'][train['label']==1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p=WordCloud(background_color=\"white\",width=500,height=500).generate(pos_words)\nplt.imshow(p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n=WordCloud(background_color=\"white\",width=500,height=500).generate(neg_words)\nplt.imshow(n) #imageshow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef ext_hash(x):\n    hashtags=[]\n    for i in x:\n        ht = re.findall(r\"#(\\w+)\", i)  #find a in b is findall(a,b)\n        hashtags.append(ht)\n\n    return hashtags\nprint(\"the positive hashtags\")\next_hash(train[\"tweet\"][train [\"label\"]==0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"the negative hashtags\")\next_hash(train[\"tweet\"][train [\"label\"]==1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport nltk\n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\ntrain_corpus = []\n\nfor i in range(0, 31962):\n  review = re.sub('[^a-zA-Z]', ' ', train['tweet'][i])\n  review = review.lower()\n  review = review.split()\n  \n  ps = PorterStemmer()\n  \n  # stemming\n  review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n  \n  # joining them back with space\n  review = ' '.join(review)\n  train_corpus.append(review)\n\ntest_corpus = []\n\nfor i in range(0, 17197):\n  review = re.sub('[^a-zA-Z]', ' ', test['tweet'][i])\n  review = review.lower()\n  review = review.split()\n  \n  ps = PorterStemmer()\n  \n  # stemming\n  review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n  \n  # joining them back with space\n  review = ' '.join(review)\n  test_corpus.append(review)\n\n# creating bag of words\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(max_features = 2500)\nx = cv.fit_transform(train_corpus).toarray()\ny = train.iloc[:, 1]\n\nprint(x.shape)\nprint(y.shape)\n\n# creating bag of words\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(max_features = 2500)\nx_test = cv.fit_transform(test_corpus).toarray()\n\nprint(x_test.shape)\n\n# splitting the training data into train and valid sets\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.25, random_state = 42)\n\nprint(x_train.shape)\nprint(x_valid.shape)\nprint(y_train.shape)\nprint(y_valid.shape)\n\n# standardization\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_valid = sc.transform(x_valid)\nx_test = sc.transform(x_test)\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\n\nmodel = RandomForestClassifier()\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_valid)\n\nprint(\"Training Accuracy :\", model.score(x_train, y_train))\nprint(\"Validation Accuracy :\", model.score(x_valid, y_valid))\n\n# calculating the f1 score for the validation set\nprint(\"F1 score :\", f1_score(y_valid, y_pred))\n\n# confusion matrix\ncm = confusion_matrix(y_valid, y_pred)\nprint(cm)\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_valid)\n\nprint(\"Training Accuracy :\", model.score(x_train, y_train))\nprint(\"Validation Accuracy :\", model.score(x_valid, y_valid))\n\n# calculating the f1 score for the validation set\nprint(\"f1 score :\", f1_score(y_valid, y_pred))\n\n# confusion matrix\ncm = confusion_matrix(y_valid, y_pred)\nprint(cm)\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier()\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_valid)\n\nprint(\"Training Accuracy :\", model.score(x_train, y_train))\nprint(\"Validation Accuracy :\", model.score(x_valid, y_valid))\n\n# calculating the f1 score for the validation set\nprint(\"f1 score :\", f1_score(y_valid, y_pred))\n\n# confusion matrix\ncm = confusion_matrix(y_valid, y_pred)\nprint(cm)\n\nfrom sklearn.svm import SVC\n\nmodel = SVC()\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_valid)\n\nprint(\"Training Accuracy :\", model.score(x_train, y_train))\nprint(\"Validation Accuracy :\", model.score(x_valid, y_valid))\n\n# calculating the f1 score for the validation set\nprint(\"f1 score :\", f1_score(y_valid, y_pred))\n\n# confusion matrix\ncm = confusion_matrix(y_valid, y_pred)\nprint(cm)\n\nfrom xgboost import XGBClassifier\n\nmodel = XGBClassifier()\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_valid)\n\nprint(\"Training Accuracy :\", model.score(x_train, y_train))\nprint(\"Validation Accuracy :\", model.score(x_valid, y_valid))\n\n# calculating the f1 score for the validation set\nprint(\"f1 score :\", f1_score(y_valid, y_pred))\n\n# confusion matrix\ncm = confusion_matrix(y_valid, y_pred)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}