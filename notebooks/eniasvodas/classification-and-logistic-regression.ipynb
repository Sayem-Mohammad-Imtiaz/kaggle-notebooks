{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-30T14:00:50.489412Z","iopub.execute_input":"2021-07-30T14:00:50.489882Z","iopub.status.idle":"2021-07-30T14:00:50.506368Z","shell.execute_reply.started":"2021-07-30T14:00:50.489846Z","shell.execute_reply":"2021-07-30T14:00:50.505113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Introduction & Statistical Prediction","metadata":{}},{"cell_type":"markdown","source":"There are many instances where we would like to have knowledge about the future. From tomorrow's weather forecast to the possibility of having to deal with a serious illness, we would like to have an 'informed guess' about that future event. Prediction is a statement about a future event. It can be seen as a means of transferring knowledge about a sample of the population to the whole population. A statistical technique used for prediction is called Regression Analysis, where we estimate the relationships between a dependent variable (response) and one or more independent variables (features, predictors). Good predictions can be considered those with as small an error in their estimation as possible. \n\nThere are many different techniques to predict future events for different types of problems. The response variable might be continuous, binary or with multiple categories, we might want to focus on our model's predictive capabilities, or care more about its interpretability. Here we would like to present a method for predicting a binary response, whether or not a patient has breast cancer. This can be done with the Logistic Regression method.\n\nBut in order to get there, we would like to show the motivation behind this type of regression. So we will start with another example, one of Linear Regression, the most common for of Regression Analysis. We assume the reader has some knowledge in Linear Algebra and Regression as well as basic knowledge in Python.","metadata":{}},{"cell_type":"markdown","source":"## A Linear Regression example","metadata":{}},{"cell_type":"markdown","source":"We import necessary libraries and create the data, concerning the Percentage Score a student received in a test and the Hours each student spent studying for that test. It would be logical to expect a linear relationship between these variables, meaning an increase in the hours studying for a test would result in an increase in a student score for that test.","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\n# Create the data\nx = np.array([2.5,5.1,3.2,8.5,3.5,1.5,9.2,5.5,8.3,2.7,7.7,5.9,\n              4.5,3.3,1.1,8.9,2.5,1.9,6.1,7.4,2.7,4.8,3.8,6.9,7.8])\n\ny = np.array([21,47,27,75,30,20,88,60,81,25,85,62,41,42,17,95,30,24,67,69,30,54,35,76,86])","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:00:50.510639Z","iopub.execute_input":"2021-07-30T14:00:50.511488Z","iopub.status.idle":"2021-07-30T14:00:50.521284Z","shell.execute_reply.started":"2021-07-30T14:00:50.511425Z","shell.execute_reply":"2021-07-30T14:00:50.519669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the Linear Regression Model\nregressor = LinearRegression()\nregressor.fit(x.reshape(-1,1), y)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:00:50.527375Z","iopub.execute_input":"2021-07-30T14:00:50.527905Z","iopub.status.idle":"2021-07-30T14:00:50.542749Z","shell.execute_reply.started":"2021-07-30T14:00:50.527866Z","shell.execute_reply":"2021-07-30T14:00:50.541596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the results\nplt.scatter(x, y)\nm, b = np.polyfit(x, y, 1)\nplt.plot(x, m*x+b,color=\"red\")\nplt.xlabel(\"Hours Studied\")\nplt.ylabel(\"Percentage Score\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:00:50.544705Z","iopub.execute_input":"2021-07-30T14:00:50.545073Z","iopub.status.idle":"2021-07-30T14:00:50.764269Z","shell.execute_reply.started":"2021-07-30T14:00:50.545032Z","shell.execute_reply":"2021-07-30T14:00:50.76293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the image above, we can see our two variables, Y as a Percentage Score that a student received in a test, and X as the Hours that particular student spent studying. We can see a linear relationship between the two variables, and our Linear Regression line seems to fit our data very well.","metadata":{}},{"cell_type":"code","source":"# Print intercept, coefficient and R squared as a measure of our model's fit to the data\nprint('intercept:', regressor.intercept_)\nprint('slope:',regressor.coef_)\nprint('R squared:',regressor.score(x.reshape(-1,1),y))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:00:50.766129Z","iopub.execute_input":"2021-07-30T14:00:50.766572Z","iopub.status.idle":"2021-07-30T14:00:50.776326Z","shell.execute_reply.started":"2021-07-30T14:00:50.766533Z","shell.execute_reply":"2021-07-30T14:00:50.774979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here our intercept can be interpreted as the expected Percentage Score a student would get in the test, if they did not study at all $(X=0)$. Our coefficient can be interpreted as the expected change (increase) in a student's Percentage Score for every Hourly increase in their study time for that particular test. We can see that our model has fit the data very well, where a 95.3% of the variability can be described from our Linear Regression model.","metadata":{}},{"cell_type":"markdown","source":"Now we consider turning our dependent variable into a binary response, where if a student received a score above 50 they get a passing grade, value of '1', otherwise they get a failing grade, value of '0'.","metadata":{}},{"cell_type":"code","source":"# Our dependent variable turned into a binary response\ny = np.array([0,0,0,1,0,0,1,1,1,0,1,1,0,0,0,1,0,0,1,1,0,1,0,1,1])\n\n# Fit the Linear Regression Model\n\nregressor = LinearRegression()\nregressor.fit(x.reshape(-1,1), y)\n\n# Plot the results\nplt.scatter(x, y)\nm, b = np.polyfit(x, y, 1)\nplt.plot(x, m*x+b,color=\"red\")\nplt.xlabel(\"Hours Studied\")\nplt.ylabel(\"Probability of Passing\")\nplt.show()\n\n# Print intercept, coefficient and R squared as a measure of our model's fit to the data\nprint('intercept:',regressor.intercept_)\nprint('slope:',regressor.coef_)\nprint('R squared:',regressor.score(x.reshape(-1,1),y))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:00:50.778037Z","iopub.execute_input":"2021-07-30T14:00:50.778385Z","iopub.status.idle":"2021-07-30T14:00:50.979489Z","shell.execute_reply.started":"2021-07-30T14:00:50.778351Z","shell.execute_reply":"2021-07-30T14:00:50.978315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can clearly see that if our dependent variable Y is a binary variable, Linear Regression fails to fit our data well. The line that is produced does not really make sense, as we can see estimates in the y-axis that are not in the [0,1] interval and thus cannot be interpreted as probabilities. We would like to model these probabilities with a function that gives values between 0 and 1. So it is time to consider another Regression model, that of Logistic Regression.","metadata":{}},{"cell_type":"markdown","source":"## Classification and Regression","metadata":{}},{"cell_type":"markdown","source":"Classification is the problem of identifying in which category an observation belongs to. This could be a diagnosis, whether or not a patient has a disease, based on some observed characteristics of the patient. Or we could have multiple possible categories and not just two. For example, we could have a problem of classifying a set of images of animals, which may be different breeds of dogs, such as German Shepherd, Golden Retriever, Bulldog and so on.\n\nIf we generalize the idea of a category with two events to an idea of “one vs all” we could use the same method of our first example (the patient) to our second example (dog images) where we split a multi class classification problem into one binary classification problem per class. We consider one of the possible classes vs all the rest, and we do that for all classes.\n\nWe would like to analyze each individual observation into a set of quantifiable properties (explanatory variables), known as features. Another way would be by comparing a new observation to previous observations based on a distance function. Whatever method is considered, an algorithm that implements classification (binary or multiple) is called a ‘classifier’.  These classifiers need training data to understand how the given input variables (features)  are related to the class (eg. a patient has or does not have a disease). In other words, we could call this process supervised learning and classification is a type of supervised learning.\n\nSupervised learning is the process of making an algorithm learn to map an input to a particular output. This algorithm can then help make predictions for new data points. One of the disadvantages of supervised learning can easily be thought of, and that is if we provide bad examples/bad data to our learning algorithm, then it is going to make inaccurate predictions, but there are always tools to help us measure our model’s predictive acccuracy.\n\nThere are many classification methods, such as Logistic Regression, k Nearest Neighbours, Decision Trees, Support Vector Machines and others. We will focus on Logistic Regression: what is it, how we can implement it and how we can interpret the results.\n\nOne definition of Regression can be that “it is a statistical method that attempts to determine the strength and character of the relationship between one dependent variable (Y) and a series of other variables (X independent variables)”. The most common form of Regression Analysis is Linear Regression, where we assume a linear relationship between these variables (Y and X): $Y=a+bX$, and we try to fit a line to our data that best describes them, according to a criterion that we set. \n","metadata":{}},{"cell_type":"markdown","source":"## Logistic Resgression approach","metadata":{}},{"cell_type":"markdown","source":"Logistic Regression is a Regression model, since the output is a prediction for the probability that a data point belongs to the category/label considered as ‘success’. Now when we consider a threshold value in order to classify our new data point, it becomes a classification techique. \n\nLogistic Regression assumes another kind of function, a sigmoid function, that would fit our data points better.","metadata":{}},{"cell_type":"markdown","source":"But how to build a Logistic Regression model?","metadata":{}},{"cell_type":"markdown","source":"As we saw in the Linear Regression example, when we coded our dependent variable Y into 0 and 1, we had trouble fitting that particular Regression line. It became obvious that we need to model our probabilities with a function that produces values between 0 and 1. A function that fits this criterion is a logistic function with a so-called sigmoid curve, as shown in the graph below:","metadata":{}},{"cell_type":"markdown","source":"![image.png](attachment:ca6db485-3f25-4378-a686-4bc62cb06e71.png)","metadata":{},"attachments":{"ca6db485-3f25-4378-a686-4bc62cb06e71.png":{"image/png":"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"}}},{"cell_type":"markdown","source":"If we apply this approach to our previous example, we will have a curve like the following:","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nsns.regplot(x=x, y=y, logistic=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:00:50.98138Z","iopub.execute_input":"2021-07-30T14:00:50.981868Z","iopub.status.idle":"2021-07-30T14:00:59.784458Z","shell.execute_reply.started":"2021-07-30T14:00:50.981815Z","shell.execute_reply":"2021-07-30T14:00:59.783521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can clearly see now that, if we used a logistic function, our prediction line would be much better than before.","metadata":{}},{"cell_type":"markdown","source":"The logistic regression function has the form:\n$$p(X) = \\frac{exp(\\beta_0 + \\beta_1*X)}{1+exp(\\beta_0 + \\beta_1*X)}$$\nwhich can be written as equation (1) below:\n$$\\frac{p(X)}{1-p(X)} = exp(\\beta_0 + \\beta_1*X) \\tag{1}\\label{eq:1} $$\nwhere the left-hand side is called **the odds of an event X** and takes values between $0$ and $\\infty$, which in our case indicate very low and very high probabilites, respectively. \n\nThough, if we consider the odds of an event as:\n$$\\text{odds of event happening} = 0.9/(1—0.9)=0.9/0.1$$,\nor 9-to-1, the odds of the opposite of p would be: \n$$\\text{odds of event not happening} = 0.1/(1-0.1)=0.1/0.9$$, \nor 0.11, which is **not** the opposite of what we found above. \n\nIn order to correct for this asymmetry, we take the natural logarithm of the function $ln$ and have the result:\n$$log(\\frac{p(X)}{1-p(X)}) = \\beta_0 + \\beta_1*X$$\n\nNow, the results above would become $ln(0.9/0.1)=2.217$ and $ln(0.1/0.9)= - 2.217$, and so the logarithm of the odds of our event happening is exactly the opposite to the logarithm of the odds of our event not happening. This logarithm of odds is called a logit function. We can also see that the logit is linearly connected with our independent variable X. However, the relationship between $p(X)$ and $X$ is not a straight line, we do not have the same interpretation for our parameters here, as we did in Linear Regression example, i.e. a one-unit increase in $X$ does not correspond to $\\beta_1$ change in $p(X)$. In this Logistic Regression model, a one-unit increase in $X$ changes the log-odds by $\\beta_1$, or it multiplies the odds by $e^{\\beta_1}$. But regardless, if $\\beta_1$ is positive, then increasing $X$ will increase $p(X)$, and if $\\beta_1$ is negative, then increasing $X$ will decrease $p(X)$.","metadata":{}},{"cell_type":"markdown","source":"In order to estimate our parameters, $\\beta_0$ and $\\beta_1$, we will use the Maximum Likelihood Estimation approach: we seek estimates for $\\beta_0$ and $\\beta_1$ such that our predicted probability $\\hat{p}(x_i)$ of success for data point $i$ corresponds as closely as possible to the observed value of that data point.\n\nIn order to get the best possible estimates, we calculate the log-likelihood function (LLF) and maximize it:\n\n$$LLF = \\sum\\limits_{i=1}^{n}[y_i*(log(p(x_i)) + (1-y_i)*log(1-p(x_i))]$$\n\nWhen $y_i = 0 => LLF = log(1-p(x_i))$, and if $p(x_i)$ is close to $y_i = 0$ then $log(1-p(x_i))$ is close to 0 as well, which is the outcome we want.\n\nWhen $y_i = 1 => LLF = y_i*log(p(x_i))$, and if $p(x_i)$ is close to $y_i = 1$ then $log(p(x_i)) = 0$ and if $p(x_i)$ is far from 1, then $log(p(x_i))$ is a large negative number.","metadata":{}},{"cell_type":"markdown","source":"After we maximize our LLF, we obtain the best weights of function $p(x_i))$. Let's say for example, that we computed $b_0, b_1$ for the logit $f(x_1) = b_0 + b_1 x_1$, then we can compute the probability $p(x_1) = 1 / (1+exp(-f(x_1)))$ and according to $p(x_1)$ we can classify that observation:\n\nIf $p(x_1) > 0/5$ then predicted output is 1.\n\nIf $p(x_1) < 0.5$ then predicted output is 0.","metadata":{}},{"cell_type":"markdown","source":"Here we only considered one variable $x_1$, but this can be generalized in the case where we would have a matrix instead of one variable.\n\nOne of the problems we face in machine learning models is to overfit them, i.e. our model has learnt the training data \"too well\", which can be a bad thing. An overfitted model can then have poor performance when applied in data other than the ones used to train it.\n\nOne method to account for overfitting is called Regularization. Regularization tries to reduce the complexity of our model, and it penalizes large coefficients.\n* L1 regularization penalty : $|b_0| + |b_1| + ... + |b_r|$, where $r$ is the number of features/independent variables we have considered.\n* L2 regularization penalty : $b_0^2 + b_1^2 + ... + b_r^2$\n* Elastic-net, which is a linear combination of L1 and L2 methods.\nRegularization can improve performance on data other than the ones used to train our model.","metadata":{}},{"cell_type":"markdown","source":"After applying all the above, we check our model's performance. This can be done by comparing our actual values and the ones we got as predicted values and then count the correct and incorrect predictions. This will be done with the 'test set', a number of observations that we left out of the computation process. We will compare those values with the ones we got from our model and see how accurate we are, or not. \n\n$$ \\begin{bmatrix}\n     TP & FP \\\\\n     FN & TN \\\\   \n   \\end{bmatrix} $$\n  \nwhere \n* TP(True Positives): correctly predicted ones, \n* TN(True Negatives): correctly predicted zeroes, \n* FN(False Negatives): incorrectly predicted zeroes,\n* FP(False Positives): incorrectly predicted ones","metadata":{}},{"cell_type":"markdown","source":"## A Logistic Regression Example","metadata":{}},{"cell_type":"markdown","source":"Let us consider an example more thoroughly, in order to better understand what has been presented so far. ","metadata":{}},{"cell_type":"markdown","source":"## Our data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\ndataset = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:00:59.785761Z","iopub.execute_input":"2021-07-30T14:00:59.786226Z","iopub.status.idle":"2021-07-30T14:00:59.806668Z","shell.execute_reply.started":"2021-07-30T14:00:59.786189Z","shell.execute_reply":"2021-07-30T14:00:59.805664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset used in our example contains 569 observations for patients with possibility of breast cancer, and can be found here:\nhttps://www.kaggle.com/uciml/breast-cancer-wisconsin-data\nWe have our target variable ‘Diagnosis’ with two categories: ‘M’ for “Malignant” and ‘B’ for “Benign” for the tumor, one ID feature,which we will drop from our dataset and 28 other features,  which we will use in order to predict the class that a patient belongs to. We also know that our independent variables have continuous values. There is also a column, ‘Unnamed: 32’ , with no values whatsoever, which we will also drop. All the above can be seen in the following outputs:\n","metadata":{}},{"cell_type":"code","source":"dataset = dataset.drop([\"Unnamed: 32\",\"id\"],axis=1)\ndataset.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:00:59.808978Z","iopub.execute_input":"2021-07-30T14:00:59.809462Z","iopub.status.idle":"2021-07-30T14:00:59.821666Z","shell.execute_reply.started":"2021-07-30T14:00:59.809414Z","shell.execute_reply":"2021-07-30T14:00:59.820749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.isnull().values.any()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:00:59.823615Z","iopub.execute_input":"2021-07-30T14:00:59.824219Z","iopub.status.idle":"2021-07-30T14:00:59.840699Z","shell.execute_reply.started":"2021-07-30T14:00:59.824147Z","shell.execute_reply":"2021-07-30T14:00:59.839786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.groupby([\"diagnosis\"]).diagnosis.count()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:00:59.842104Z","iopub.execute_input":"2021-07-30T14:00:59.842609Z","iopub.status.idle":"2021-07-30T14:00:59.856022Z","shell.execute_reply.started":"2021-07-30T14:00:59.842563Z","shell.execute_reply":"2021-07-30T14:00:59.855174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that we have 357 cases for 'Benign' and 212 for 'Malignant' tumor and that there are no missing cases.","metadata":{}},{"cell_type":"markdown","source":"We would like to see the correlations between our variables, and if they are strong or not.import matplotlib.pyplot as plt\nimport seaborn as sns\ndataset.groupby([\"diagnosis\"]).diagnosis.count()\nplt.figure(figsize=(15, 12))\nmatrix = np.triu(dataset.corr())\nsns.heatmap(dataset.corr(), annot=True, linewidth=.8, mask=matrix, cmap=\"rocket\")","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(15, 12))\nmatrix = np.triu(dataset.corr())\nsns.heatmap(dataset.corr(), annot=True, linewidth=.8, mask=matrix, cmap=\"rocket\")","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:00:59.857391Z","iopub.execute_input":"2021-07-30T14:00:59.85792Z","iopub.status.idle":"2021-07-30T14:01:03.24079Z","shell.execute_reply.started":"2021-07-30T14:00:59.857871Z","shell.execute_reply":"2021-07-30T14:01:03.23981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there are some  variables that are strongly positively correlated with each other, as well as some that are negatively correlated, for example ‘smoothness_se’ is negatively correlated with 3 other variables.\n\nWe make the decision to not remove any of our features in this Logistic Regression example, and so we move on with our analysis.","metadata":{}},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"We set the features and the target variable.","metadata":{}},{"cell_type":"code","source":"X = dataset.drop([\"diagnosis\"],axis=1)\ny = dataset.iloc[:,0:1].values","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:01:03.242085Z","iopub.execute_input":"2021-07-30T14:01:03.242589Z","iopub.status.idle":"2021-07-30T14:01:03.249468Z","shell.execute_reply.started":"2021-07-30T14:01:03.242536Z","shell.execute_reply":"2021-07-30T14:01:03.248342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We encode our target variable, meaning that we want to have '0' and '1' values for 'Benign' and 'Malignant' tumor cases respectively.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y.ravel())","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:01:03.251034Z","iopub.execute_input":"2021-07-30T14:01:03.251413Z","iopub.status.idle":"2021-07-30T14:01:03.265793Z","shell.execute_reply.started":"2021-07-30T14:01:03.251378Z","shell.execute_reply":"2021-07-30T14:01:03.264281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We split the dataset into training and test set. On the training set we will 'train' our regression model and on the test set we will see how it will perform. We will predict the values of the test set and then compare our results with the real values of the test set. We set 80% of our dataset rows as the training set and the remaining 20% as the test set. We also define a 'random_state = 0' for reproducibility, in order to always get the same result.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:01:03.267372Z","iopub.execute_input":"2021-07-30T14:01:03.267732Z","iopub.status.idle":"2021-07-30T14:01:03.281433Z","shell.execute_reply.started":"2021-07-30T14:01:03.267696Z","shell.execute_reply":"2021-07-30T14:01:03.280197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we stated previously, Logistic Regression also performs Regularization on our parameters, one form of penalty for large coefficients. In this example, we have many different independent variables in different units of measurement, which can pose a problem if we want to select the most important features later in our analysis. We decide to standardize our variables, in order to overcome this problem and so that all independent variables will be in the same scale.\n\nWe have to be careful though, not to standardize all our data at the same time. We should standardize our train set first, with its own mean and variance and our test set separately, with its own mean and variance. That is done because the test set is supposed to be data that we have not yet received, so if we standardize all our dataset, in the computation of mean and variance, we consider values that should not be there.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:01:03.282893Z","iopub.execute_input":"2021-07-30T14:01:03.28324Z","iopub.status.idle":"2021-07-30T14:01:03.302589Z","shell.execute_reply.started":"2021-07-30T14:01:03.283205Z","shell.execute_reply":"2021-07-30T14:01:03.301229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We fit our Logistic Regression classifier.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression().fit(X_train,y_train)\n\ny_pred = classifier.predict(X_test)\nprint('intercept:',classifier.intercept_)\nprint('coefficients:',classifier.coef_)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:01:03.304287Z","iopub.execute_input":"2021-07-30T14:01:03.304619Z","iopub.status.idle":"2021-07-30T14:01:03.342599Z","shell.execute_reply.started":"2021-07-30T14:01:03.304587Z","shell.execute_reply":"2021-07-30T14:01:03.339546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We print the odds of our coefficients, for easier interpretation\nprint('odds of coefficients:',np.exp(classifier.coef_))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:01:03.344784Z","iopub.execute_input":"2021-07-30T14:01:03.345364Z","iopub.status.idle":"2021-07-30T14:01:03.357017Z","shell.execute_reply.started":"2021-07-30T14:01:03.34531Z","shell.execute_reply":"2021-07-30T14:01:03.355361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the intercept and the coefficients of our Logistic Regression classifier. These are interpreted in a different manner than in Linear Regression, because here we have calculated the logit of odds and not the probability. If we want the odds we have to convert, by taking the exponents of the values presented above. For example, our first coefficient $0.34$, which refers to the independent variable/feature 'radius_mean' can be interpreted as: for every one-unit increase in 'radius_mean', the odds that the patient represented in the observation is in the target class (“1 - Malignant”) are over $exp(0.34 = 1.403)$ as large as the odds that they won’t be in the target class, **if** all other variables are held constant. To convert this to a probability, we have $1.403 / (1+1.403) = 0.584$, or a 58.4% increase in probability for the patient to be in class '1 - Malignant' for every one-unit increase in the 'radius_mean' variable, **if** all other variables are held constant. \n\nIn the case of a negative coefficient (or when the odds of that coefficient are less than 1), we simply take $1/ \\text{coefficient}$. For example our sixth coeffiecient $(-0.4458)$ with odds $exp(-0.4458) = 0.64$ will become $1/0.64 = 1.56$, meaning for every one-unit increase in our 'compactness_mean' variable the odds of the patient represented in the observation **not** being in the target class (\"1 - Malignant\") are 1.56 as likely as the odds of the patient being in the target class, **if** all other variables are held constant. ","metadata":{}},{"cell_type":"markdown","source":"We import some metrics, to better understand our model's accuracy.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:01:03.360596Z","iopub.execute_input":"2021-07-30T14:01:03.361968Z","iopub.status.idle":"2021-07-30T14:01:03.370915Z","shell.execute_reply.started":"2021-07-30T14:01:03.361904Z","shell.execute_reply":"2021-07-30T14:01:03.369278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfm = confusion_matrix(y_test,y_pred)\nprint('Precision', cfm[0, 0] / sum(cfm[:, 0]))\nprint('Recall', cfm[0, 0] / sum(cfm[0, :]))\nprint('Specificity', cfm[1, 1] / sum(cfm[1, :]))\ncfm","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:01:03.375226Z","iopub.execute_input":"2021-07-30T14:01:03.376989Z","iopub.status.idle":"2021-07-30T14:01:03.398775Z","shell.execute_reply.started":"2021-07-30T14:01:03.376278Z","shell.execute_reply":"2021-07-30T14:01:03.39737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the matrix we can see that\n* True Positives: 65, \n* True Negatives: 45, \n* False Negatives: 2,\n* False Positives: 2","metadata":{}},{"cell_type":"code","source":"accuracy = accuracy_score(y_test, y_pred)\naccuracy","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:01:03.403412Z","iopub.execute_input":"2021-07-30T14:01:03.403973Z","iopub.status.idle":"2021-07-30T14:01:03.416276Z","shell.execute_reply.started":"2021-07-30T14:01:03.403923Z","shell.execute_reply":"2021-07-30T14:01:03.414422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred,target_names=['M', 'B']))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:01:03.418942Z","iopub.execute_input":"2021-07-30T14:01:03.420276Z","iopub.status.idle":"2021-07-30T14:01:03.437483Z","shell.execute_reply.started":"2021-07-30T14:01:03.420195Z","shell.execute_reply":"2021-07-30T14:01:03.436203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that our model has done very well, considering we only got two False Positives and two False Negatives and an accuracy of 96.5%. As for our Classification Report, the 'precision' label refers to our classifier's ability to not label an instance positive while it is actually negative, i.e. what percent of our predictors were correct (97%), the 'recall' label refers to the ability of our classifier to find all positive instances, i.e. what percent of the positive cases did it find (97%), and lastly, the 'f1-scre' label refers to a harmonic mean of our 'precision' and 'recall' labels with best score 1 and worst 0, and can be interpreted as what percent of positive predictions were correct (97%).","metadata":{}},{"cell_type":"markdown","source":"## ROC Curve and AUC","metadata":{}},{"cell_type":"markdown","source":"Receiver Operating Characteristic Curve (ROC) can be used to determine the diagnostic ability of a binary classifier. It is constructed by plotting the true positive rate (TPR) against the false positive rate (FPR).\n$$TPR = TP / (TP + FN)$$ and $$FPR = FP / (FP + TN)$$\n\nThe ROC curve shows the trade-off between sensitivity (or TPR) and specificity (1 – FPR). Classifiers that give curves closer to the top-left corner indicate a better performance. A random classifier would give points lying along the diagonal (FPR = TPR)(our baseline).","metadata":{}},{"cell_type":"markdown","source":"Area Under the Curve (AUC) is used to compare different classifiers, where we summarize the performance of each classifier into a single measurement. AUC measures the entire two-dimensional area underneath the ROC curve. It can be interpreted as the probability that the model ranks a random positive example more highly than a random negative example.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\nimport sklearn.metrics as metrics\ny_test_pred_prob=classifier.predict_proba(X_test)[:,1]\nmetrics.roc_auc_score(y_test, y_test_pred_prob)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:01:03.439487Z","iopub.execute_input":"2021-07-30T14:01:03.440317Z","iopub.status.idle":"2021-07-30T14:01:03.456794Z","shell.execute_reply.started":"2021-07-30T14:01:03.440233Z","shell.execute_reply":"2021-07-30T14:01:03.45571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr, tpr,thresholds=roc_curve(y_test,y_test_pred_prob)\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel(\"False Possitive rate\")\nplt.ylabel(\"True Positive rate\")\nplt.title(\"Area Under ROC Curve\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:01:03.458222Z","iopub.execute_input":"2021-07-30T14:01:03.458595Z","iopub.status.idle":"2021-07-30T14:01:03.641875Z","shell.execute_reply.started":"2021-07-30T14:01:03.458561Z","shell.execute_reply":"2021-07-30T14:01:03.640519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above results we can see that our model has performed very well, with an AOC of 99.3%, close to a perfect diagnostic ability for our classifier.","metadata":{}},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"In this notebook we have presented the motivation behind Logistic Regression as a method of Classification, where we would like to predict the class that a future observation/patient would belong to, according to some of their characteristics. We have shown how to implement Logistic Regression in Python, what libraries and code to use in order to implement this analysis. Also we interpret the results of our analysis and how accurate our Classification technique actually is in this dataset. ","metadata":{}},{"cell_type":"markdown","source":"## References","metadata":{}},{"cell_type":"markdown","source":"1.  Dataset: https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\n2. Terminology on Regerssion Analysis: https://en.wikipedia.org/wiki/Regression_analysis\n3. https://realpython.com/logistic-regression-python/\n4. http://faculty.cas.usf.edu/mbrannick/regression/Logistic.html\n5. Code for the Heatmap graph from: https://www.kaggle.com/d4rklucif3r/breast-cancer-wisconsin-eda-97-80/notebook\n6. Classification Report explained: https://muthu.co/understanding-the-classification-report-in-sklearn/","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}