{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction:\n\n![VW cars](https://wallpapercave.com/wp/wp2170192.jpg)\n\nIn this analysis, I have done a basic EDA of features and I have selected k-best features out of both linear features and from polynomial features and have applied regression on top of it to find the maximum r_squared value that I am able to acheive from the data.\n\n1. Introduction\n2. Importing dataset and exploration\n3. Exploratory data analysis\n4. Pre-processing for modeling\n5. Modeling\n6. Backward selection for variable selection on linear regression\n7. Polynomial features for modeling\n8. Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Importing the packages needed for the analysis. I usually like to import the packages in the alphabetical order, so that it is easy for reviewing if needed","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.svm import SVR\n\nimport statsmodels.api as sm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max.columns', None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing dataset and exploration","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There are many files in the input folder for each of the car brands. We will import the file that is with VW naming on it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_vw = pd.read_csv(\"/kaggle/input/used-car-dataset-ford-and-mercedes/vw.csv\")\nprint(data_vw.shape)\ndata_vw.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seeing if there are any missing values in the records","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_vw.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice :) it is a nice and clean data, very good one to work with!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_vw.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory data analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data_vw[\"transmission\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the cars on the dataset are with manual transmission with very few cars in automatic and seim automatic transmission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data_vw[\"model\"].value_counts() / len(data_vw))\nsns.countplot(y = data_vw[\"model\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Top 3 cars are Golf, Polo and Tiguan on the dataset constuite 64% of all the VW cars, with all other cars contributing to 36%","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data_vw[\"fuelType\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(y = data_vw[\"year\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5),facecolor='w') \nsns.barplot(x = data_vw[\"year\"], y = data_vw[\"price\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The recently manufactured cars (year = 2018, 2019) are sold for more average price when compared to the cars that are manufactured earlier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = data_vw[\"transmission\"], y = data_vw[\"price\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10),facecolor='w') \nsns.scatterplot(data_vw[\"mileage\"], data_vw[\"price\"], hue = data_vw[\"year\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5),facecolor='w') \nsns.scatterplot(data_vw[\"mileage\"], data_vw[\"price\"], hue = data_vw[\"fuelType\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data_vw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I am computing a age field, subtracting 2020 from the year field and dropping the year field","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_vw[\"age_of_car\"] = 2020 - data_vw[\"year\"]\ndata_vw = data_vw.drop(columns = [\"year\"])\ndata_vw.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-processing for modeling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I like to use pd.get_dummies option over OHE in SKLearn to get the one hot encoded variables for the categorical variables. It is usually tidy on the dataset and the column names are preserved.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_vw_expanded = pd.get_dummies(data_vw)\ndata_vw_expanded.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applying the standard scalar option to standardize all the variables in the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"std = StandardScaler()\ndata_vw_expanded_std = std.fit_transform(data_vw_expanded)\ndata_vw_expanded_std = pd.DataFrame(data_vw_expanded_std, columns = data_vw_expanded.columns)\nprint(data_vw_expanded_std.shape)\ndata_vw_expanded_std.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(data_vw_expanded_std.drop(columns = ['price']), data_vw_expanded_std[['price']])\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Selecting best features for model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Since ther are 40 variables in the dataset after the one hot encoding, I am using SelectKBest option from sklearn to select the best features from the dataset for applying the regression.\n\nFor this, I am executing the SelectKBest() on f_regression by taking into consideration from 3 variables to 40 variables to see where we get the best score. \n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"column_names = data_vw_expanded.drop(columns = ['price']).columns\n\nno_of_features = []\nr_squared_train = []\nr_squared_test = []\n\nfor k in range(3, 40, 2):\n    selector = SelectKBest(f_regression, k = k)\n    X_train_transformed = selector.fit_transform(X_train, y_train)\n    X_test_transformed = selector.transform(X_test)\n    regressor = LinearRegression()\n    regressor.fit(X_train_transformed, y_train)\n    no_of_features.append(k)\n    r_squared_train.append(regressor.score(X_train_transformed, y_train))\n    r_squared_test.append(regressor.score(X_test_transformed, y_test))\n    \nsns.lineplot(x = no_of_features, y = r_squared_train, legend = 'full')\nsns.lineplot(x = no_of_features, y = r_squared_test, legend = 'full')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get score of 0.88 around 23 variables befor the curve stablizes. Hence keeping k as 23 selecting 23 best variables from the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"selector = SelectKBest(f_regression, k = 23)\nX_train_transformed = selector.fit_transform(X_train, y_train)\nX_test_transformed = selector.transform(X_test)\ncolumn_names[selector.get_support()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def regression_model(model):\n    \"\"\"\n    Will fit the regression model passed and will return the regressor object and the score\n    \"\"\"\n    regressor = model\n    regressor.fit(X_train_transformed, y_train)\n    score = regressor.score(X_test_transformed, y_test)\n    return regressor, score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_performance = pd.DataFrame(columns = [\"Features\", \"Model\", \"Score\"])\n\nmodels_to_evaluate = [LinearRegression(), Ridge(), Lasso(), SVR(), RandomForestRegressor(), MLPRegressor()]\n\nfor model in models_to_evaluate:\n    regressor, score = regression_model(model)\n    model_performance = model_performance.append({\"Features\": \"Linear\",\"Model\": model, \"Score\": score}, ignore_index=True)\n\nmodel_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best score we are getting is on a RandomForestRegressor() with a score of 0.9513","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Backward selection for variable selection on linear regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Fitting a linear regression model and checking the model parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor = sm.OLS(y_train, X_train).fit()\nprint(regressor.summary())\n\nX_train_dropped = X_train.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"while True:\n    if max(regressor.pvalues) > 0.05:\n        drop_variable = regressor.pvalues[regressor.pvalues == max(regressor.pvalues)]\n        print(\"Dropping \" + drop_variable.index[0] + \" and running regression again because pvalue is: \" + str(drop_variable[0]))\n        X_train_dropped = X_train_dropped.drop(columns = [drop_variable.index[0]])\n        regressor = sm.OLS(y_train, X_train_dropped).fit()\n    else:\n        print(\"All p values less than 0.05\")\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"8 variables are dropped because p value is higher than our alpha level of 0.05. We fit the model with the remaining variables and see the summary below. \n\nWe can see a slight improvement over the linear regression in our earlier step with SKLearn fit which yielded a r_squared value of 0.87, this vies us a r_square value of 0.89","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(regressor.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fitting on polynomial features\n\nI would like to explore the dataset a bit further to see if a polynomial variable model is performing better on the same models. \n\nI am using PolynomialFeatures() to engineer polynomial features from the dataset. \nWe have around 820 features from PolynomialFeatures(), so again using SelectKBest to see how much is our optimum feature set size","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"poly = PolynomialFeatures()\nX_train_transformed_poly = poly.fit_transform(X_train)\nX_test_transformed_poly = poly.transform(X_test)\n\nprint(X_train_transformed_poly.shape)\n\nno_of_features = []\nr_squared = []\n\nfor k in range(10, 277, 5):\n    selector = SelectKBest(f_regression, k = k)\n    X_train_transformed = selector.fit_transform(X_train_transformed_poly, y_train)\n    regressor = LinearRegression()\n    regressor.fit(X_train_transformed, y_train)\n    no_of_features.append(k)\n    r_squared.append(regressor.score(X_train_transformed, y_train))\n    \nsns.lineplot(x = no_of_features, y = r_squared)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above graph we can see that we are hitting 0.93 score around 110 features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"selector = SelectKBest(f_regression, k = 110)\nX_train_transformed = selector.fit_transform(X_train_transformed_poly, y_train)\nX_test_transformed = selector.transform(X_test_transformed_poly)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models_to_evaluate = [LinearRegression(), Ridge(), Lasso(), SVR(), RandomForestRegressor(), MLPRegressor()]\n\nfor model in models_to_evaluate:\n    regressor, score = regression_model(model)\n    model_performance = model_performance.append({\"Features\": \"Polynomial\",\"Model\": model, \"Score\": score}, ignore_index=True)\n\nmodel_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion:\nI got maximum r^2 score of 0.955 for polynomian data on RandomForest regressor. \n\nAs next steps, I can concentrate on individual features, and make some transformations such as log transforms on each of the features to make the model perform even better.\n\nPlease upvote the notebook if you liked it, and leave me a feedback if you think something could have been better.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# References:\n1. https://medium.com/@mayankshah1607/machine-learning-feature-selection-with-backward-elimination-955894654026","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}