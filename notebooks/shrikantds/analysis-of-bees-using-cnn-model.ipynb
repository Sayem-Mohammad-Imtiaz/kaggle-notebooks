{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nimport skimage\nimport skimage.io\nimport skimage.transform\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sys\nimport random\nfrom sklearn.model_selection import train_test_split\n\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Conv2D,MaxPool2D,Flatten,Dropout,BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\nnp.random.seed(42)\n# Global variables\nimg_folder='../input/bee_imgs/bee_imgs/'\nimg_width=100\nimg_height=100\nimg_channels=3\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"### Read Bee data\n"},{"metadata":{"trusted":true,"_uuid":"d8bb095f39f391b21e3d57df63b526dbaddb5f90"},"cell_type":"code","source":"bees=pd.read_csv('../input/bee_data.csv')\nbees.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16d8d805639d8faa3d8e91d5d3d73b1bb0946fba"},"cell_type":"code","source":"bees.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66e8f9ff01aaa524b93825ad60b759efc7de7ec5"},"cell_type":"code","source":"bees=pd.read_csv('../input/bee_data.csv', \n                index_col=False,  \n                parse_dates={'datetime':[1,2]},\n                dtype={'subspecies':'category', 'health':'category','caste':'category'})\n\ndef read_or_skip(file):\n    \"\"\"This function is to supress imageio exception if file doesn't exist\"\"\"\n    try:\n        img = skimage.io.imread(img_folder + file)\n        img = skimage.transform.resize(img, (img_width, img_height), mode='reflect')\n        return img[:,:,:img_channels]\n    except:\n        #print('Skipping %s. %s' %(file, sys.exc_info()[1]))\n        return None\n\nbees['img'] = bees['file'].apply(read_or_skip)\nbees.dropna(inplace=True)\n\n# Print sample data without img array\nbees.drop('img',axis=1).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f907660855de39a164383b4e9e31dff550df19a2"},"cell_type":"code","source":"bees.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12e1e97d09d2b1024925aa6229835170848c0da9"},"cell_type":"code","source":"bees.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"933f962f2455395487b8c01ae0703e9f8a8e2188"},"cell_type":"markdown","source":" ### Bee data EDA\n ####  Distribution of bees by categories"},{"metadata":{"trusted":true,"_uuid":"c8823f1e8cfefff40001f4d93d505cda237dc91c"},"cell_type":"code","source":"f, ax = plt.subplots(nrows=2,ncols=2,figsize=(12,8))\nbees['subspecies'].value_counts().plot(kind='bar',ax=ax[0,0])\nax[0,0].set_ylabel('Count')\nax[0,0].set_title('Subspecies')\n\n\nbees['location'].value_counts().plot(kind='bar',ax = ax[0,1])\nax[0,1].set_title('Location')\nax[0,1].set_ylabel('Count')\n\nbees['caste'].value_counts().plot(kind='bar',ax = ax[1,0])\nax[1,0].set_title('Caste')\nax[1,0].set_ylabel('Count')\n\nbees['health'].value_counts().plot(kind='bar',ax = ax[1,1])\nax[1,1].set_title('Health')\nax[1,1].set_ylabel('Count')\n\nf.subplots_adjust(hspace=0.7)\nf.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d3b7842d20c08e540ef5b5f18f47ac3c4ae25b4"},"cell_type":"markdown","source":"### 3.2 Look at Bees images\n\n#### Subspecies of Bee\n"},{"metadata":{"trusted":true,"_uuid":"2466ba149fe4a6c29db4e1710cf3e6d34fea1890"},"cell_type":"code","source":"import imageio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"575601ac72ac44d5d7db512a77531a195e81a040"},"cell_type":"code","source":"# Select first X subspecies titles \ncolumns=7\nsubspecies = bees['subspecies'].unique()[:7]\nf, ax = plt.subplots(nrows=1,ncols=7, figsize=(12,3))\ni=0\n\n# Draw the first found bee of given subpecies\nfor s in subspecies:\n    if s == 'healthy': continue\n    file=img_folder + bees[bees['subspecies']==s].iloc[0]['file']\n    im=imageio.imread(file)\n    ax[i].imshow(im, resample=True)\n    ax[i].set_title(s, fontsize=8)\n    i+=1\n    \nplt.suptitle(\"Subspecies of Bee\")\nplt.tight_layout()\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d7b7efe380023c424b8a7e296ec4e71a1a76021"},"cell_type":"markdown","source":"### Healthy Bees"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"89a96757a5ba832356dd1287c1a9ab23076fbac4"},"cell_type":"code","source":"healthy = bees[bees['health'] == 'healthy'].iloc[:5]\n\nf, ax = plt.subplots(nrows=1,ncols=5, figsize=(12,3))\n# Read image of original size from disk, because bees['img'] contains resized numpy array\nfor i in range(0,5): \n    file = img_folder + healthy.iloc[i]['file']\n    ax[i].imshow(imageio.imread(file))\n\nplt.suptitle(\"Healthy Bees\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bb1e7ad88ff7cc6e104d794d6a13c5dc371c5aa"},"cell_type":"markdown","source":"### Sick Bees"},{"metadata":{"trusted":true,"_uuid":"76beccff1c16bd748450edcceec1c8279b0f1893"},"cell_type":"code","source":"healths_cat = bees['health'].cat.categories\nhealths_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d981cb246c8b9e298a79be0762b85d26b032005f"},"cell_type":"code","source":"healths_cat.size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2410f6e6428406d0d5b9dde96354e7a1a8c73398"},"cell_type":"code","source":"f, ax = plt.subplots(1, healths_cat.size-1, figsize=(12,3))\ni=0\n\nfor c in healths_cat:\n    if c == 'healthy': continue\n    bee = bees[bees['health'] == c].iloc[0]\n    f = bee['file']\n    f_path= img_folder + f\n    ax[i].imshow(imageio.imread(f_path))\n    ax[i].set_title(bee['health'], fontsize=8)\n    i += 1\nplt.suptitle(\"Sick Bees\")    \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"441f76eece790a936cd0c354088f1ea3096e9f39"},"cell_type":"markdown","source":"### 4. CNN Model for Bee subspecies detection\n#### 4.1 Prepare data and train Bee subspecies detection CNN"},{"metadata":{"trusted":true,"_uuid":"d444ef412c99e52122d97bec7687ee90d96ae0e2"},"cell_type":"code","source":"# Prepare train and test data\nlabels = pd.get_dummies(bees.subspecies, drop_first=True)\nX = np.stack(bees.img)\ntrain_data, test_data, train_labels, test_labels = train_test_split(X, labels)\n# Build and train CNN model\nmodel1=Sequential()\nmodel1.add(Conv2D(5, kernel_size=3, input_shape=(img_width, img_height,3), activation='relu'))\nmodel1.add(MaxPool2D(2))\nmodel1.add(Conv2D(10, kernel_size=3, activation='relu'))\nmodel1.add(Flatten())\nmodel1.add(Dense(labels.columns.size, activation='softmax'))\nmodel1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\ntraining = model1.fit(train_data, train_labels, validation_split=0.2, epochs=20, batch_size=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"564a8e83ddc6067ff21cd5280a227b81aaba2089"},"cell_type":"markdown","source":"### 4.2 Evaluate bee subspecies detection model\n"},{"metadata":{"trusted":true,"_uuid":"809bcfa11a6324df7e243654246090c91dae6306"},"cell_type":"code","source":"## Trained model analysis and evaluation\nf, ax = plt.subplots(2,1, figsize=(5,5))\nax[0].plot(training.history['loss'])\nax[0].set_title('Detect kind of Bee: loss')\nax[0].set_xlabel('Epoch')\nax[0].set_ylabel('Loss')\n\n# Accuracy\nax[1].plot(training.history['acc'])\nax[1].set_title('Detect kind of Bee: accuracy')\nax[1].set_xlabel('Epoch')\nax[1].set_ylabel('Accuracy')\nplt.tight_layout()\nplt.show()\n\n# Accuracy by subspecies\ntest_pred = model1.predict(test_data)\nacc_by_subspecies = np.logical_and((test_pred > 0.5), test_labels).sum()/test_labels.sum()\nacc_by_subspecies.plot(kind='bar', title='Subspecies prediction accuracy')\nplt.ylabel('Accuracy')\nplt.show()\n\n# Loss function and accuracy\ntest_res = model1.evaluate(test_data, test_labels)\nprint('Evaluation: loss function: %s, accuracy:' % test_res[0], test_res[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84e0a63c29f0bfdf0f10a29203c1a79892c0976a"},"cell_type":"markdown","source":"### 5. CNN model for Bee health detection\n#### 5.1 Prepare data and train Bee health detection modelÂ¶\n"},{"metadata":{"trusted":true,"_uuid":"ccf037acb45c1037f7ecc46a686fa1c9c1caccc1"},"cell_type":"code","source":"# Prepare train and test data\nlabels = pd.get_dummies(bees.health)\nX = np.stack(bees.img)\ntrain_data, test_data, train_labels, test_labels = train_test_split(X, labels)\n\n# Data augmentation - a little bit rotate, zoom and shift input images.\ngenerator = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\ngenerator.fit(train_data)\n\n# Split train data to train and validation\ntrain_data, train_data_val, train_labels, train_labels_val = train_test_split(train_data, \n                                                                              train_labels,\n                                                                              test_size=0.1)  \n# Build and train CNN model\nmodel2 = Sequential()\nmodel2.add(Conv2D(6, kernel_size=3, input_shape=(img_width, img_height,3), activation='relu'))\nmodel2.add(MaxPool2D(2))\nmodel2.add(Conv2D(12, kernel_size=3, activation='relu'))\nmodel2.add(Flatten())\nmodel2.add(Dense(labels.columns.size, activation='softmax'))\nmodel2.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# We'll stop training if no improvement after some epochs\nearlystopper = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n\n# Train\ntraining = model2.fit_generator(generator.flow(train_data,train_labels, batch_size=20),\n                               epochs = 20,\n                               validation_data=(train_data_val, train_labels_val),\n                               steps_per_epoch=20,  # batch_size\n                               callbacks=[earlystopper])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53c569652f57799ae27e0e458024a69c609ecaa6"},"cell_type":"markdown","source":"### 5.2 Evaluate Bee health detection model"},{"metadata":{"trusted":true,"_uuid":"8e54fcbb74584424b5d1ab3a80ed0549e9139f75"},"cell_type":"code","source":"f, ax = plt.subplots(2,1, figsize=(5,5))\n\n# Loss function\nax[0].plot(training.history['loss'])\nax[0].set_title('Detect Bee health: loss')\nax[0].set_xlabel('Epoch')\nax[0].set_ylabel('Loss')\n\n# Accuracy\nax[1].plot(training.history['acc'])\nax[1].set_title('Detect Bee health: accuracy')\nax[1].set_xlabel('Epoch')\nax[1].set_ylabel('Accuracy')\nplt.tight_layout()\nplt.show()\n\n# Prediction accuracy by health status\ntest_pred = model2.predict(test_data)\nacc_by_health = np.logical_and((test_pred > 0.5), test_labels).sum()/test_labels.sum()\nacc_by_health.plot(kind='bar', title='Health prediction accuracy')\nplt.ylabel('Accuracy')\nplt.tight_layout()\nplt.show()\n\ntest_res = model2.evaluate(test_data, test_labels)\nprint('Evaluation: loss function: %s, accuracy:' % test_res[0], test_res[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b8a8fc41708490a2ab6776d55f750bb47141db7"},"cell_type":"markdown","source":"### 6. Visualization of Conv2D layers\n\nLet's look how our models process images. Our models contains Conv2D layers with kernels inside. We are going to convolve a sample image through kernels and see how does it look before and after each kernel. For each kernel visualize: kernel itself, input image, output image. No idea how to interprete these results, let's do it for fun :)\n\nFunction for Conv2D layers visualization:\n"},{"metadata":{"trusted":true,"_uuid":"d81bf48b928d065bb2fa0f18d10d1907c7c4ae1e"},"cell_type":"code","source":"# Common function for visualization of kernels\ndef visualize_layer_kernels(img, conv_layer, title):\n    \"\"\"\n    Displays how input sample image looks after convolution by each kernel\n    :param img: Sample image array\n    :param conv_layer: Layer of Conv2D type\n    :param title: Text to display on the top \n    \"\"\"\n    # Extract kernels from given layer\n    weights1 = conv_layer.get_weights()\n    kernels = weights1[0]\n    kernels_num = kernels.shape[3]\n    \n    # Each row contains 3 images: kernel, input image, output image\n    f, ax = plt.subplots(kernels_num, 3, figsize=(7, kernels_num*2))\n\n    for i in range(0, kernels_num):\n        # Get kernel from the layer and draw it\n        kernel=kernels[:,:,:3,i]\n        ax[i][0].imshow((kernel * 255).astype(np.uint8), vmin=0, vmax=255)\n        ax[i][0].set_title(\"Kernel %d\" % i, fontsize = 9)\n        \n        # Get and draw sample image from test data\n        ax[i][1].imshow((img * 255).astype(np.uint8), vmin=0, vmax=255)\n        ax[i][1].set_title(\"Before\", fontsize=8)\n        \n        # Filtered image - apply convolution\n        img_filt = scipy.ndimage.filters.convolve(img, kernel)\n        ax[i][2].imshow((img_filt * 255).astype(np.uint8), vmin=0, vmax=255)\n        ax[i][2].set_title(\"After\", fontsize=8)\n        \n    plt.suptitle(title)\n    plt.tight_layout()\n    plt.subplots_adjust(top=0.93)\n    plt.show()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98584e71128939fb7fccef85a900cd36c9cb8c37"},"cell_type":"code","source":"# Take sample image to visualize convolutoin\nidx = random.randint(0,len(test_data)-1)\nimg = test_data[idx,:,:,:]\n# Take 1st convolutional layer and look at it's filters\nconv1 = model1.layers[0]\nimg = visualize_layer_kernels(img, conv1, \"Subspecies CNN. Layer 0\")\n\n# Take sample image to visualize convolutoin\nidx = random.randint(0,len(test_data)-1)\nimg = test_data[idx,:,:,:]\n# Take another convolutional layer and look at it's filters\nconv2 = model1.layers[2]\nres = visualize_layer_kernels(img, conv2, \"Subspecies CNN. Layer 2\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74ce394e8a57e84ced06f98bde96520e4d047764"},"cell_type":"markdown","source":"### 6.2 Visualize convolutions in Bee health CNN"},{"metadata":{"trusted":true,"_uuid":"8048ac621fd225be7de1333a16a64b2df5b6d552"},"cell_type":"code","source":"# Take sample image to visualize convolutoin\nidx = random.randint(0,len(test_data)-1)\nimg = test_data[idx,:,:,:]\n# Take 1st convolutional layer and look at it's filters\nconv1 = model2.layers[0]\nvisualize_layer_kernels(img, conv1, \"Health CNN layer 0\")\n\n# Take sample image to visualize convolutoin\nidx = random.randint(0,len(test_data)-1)\nimg = test_data[idx,:,:,:]\n# Take another convolutional layer and look at it's filters\nconv2 = model2.layers[2]\nvisualize_layer_kernels(img, conv2, \"Health CNN layer 2\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a0e1a1b2fbb08e71d48a8474a0f4a0b8165f5f8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}