{"cells":[{"metadata":{"id":"sOLKXRD0GOYl","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"###############################################################################################################################\n#\n#\tVariational Auto Encoder Built using Keras and celebA dataset is used \n#\tfor training.\n#\t\n#\tCreated by : B V P Sai Kumar \n#\tgithub : https://github.com/kumararduino\n#\twebsite : https://kumarbasaveswara.in\n#\tlinkedin : https://www.linkedin.com/in/kumar15412304/\n#\n#\n#\tCredits:\n#\tDataset : https://www.kaggle.com/jessicali9530/celeba-dataset\n#\tArticle : https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf\n#\t\tThis Article gave me a clear glance about how Variational_Auto_Encoders work\n# Article on KL_Divergence : https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained\n#\t\n#\n#\n###############################################################################################################################","execution_count":null,"outputs":[]},{"metadata":{"id":"lMaCTHUODmXU","colab_type":"text"},"cell_type":"markdown","source":"Import Necessary packages"},{"metadata":{"id":"n7Qcr3BWuXNc","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import gc\nimport psutil\nimport multiprocessing as mp\nimport copy\nmp.cpu_count()\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport keras\nfrom keras.models import Model,Sequential\nfrom keras.layers import Dense,Conv2D,MaxPooling2D,Dropout,BatchNormalization,Lambda,Activation,Input,Flatten,Reshape,Conv2DTranspose\nimport keras.backend as K\nfrom keras.layers.merge import add\nfrom sklearn.model_selection import train_test_split\nimport os\nimport glob\nfrom time import time,asctime\nfrom random import randint as r\nimport random","execution_count":null,"outputs":[]},{"metadata":{"id":"GrulpW1TD-aU","colab_type":"text"},"cell_type":"markdown","source":"Load file names of all pics"},{"metadata":{"id":"StjAD_cCvZ5l","colab_type":"code","outputId":"04682243-e90a-4dcc-87d4-f7853a152ecf","colab":{"base_uri":"https://localhost:8080/","height":94},"trusted":true},"cell_type":"code","source":"# os.chdir(\"faces\")\nimgs = glob.glob(\"../input/img_align_celeba/img_align_celeba/*.jpg\")\nprint(len(imgs))","execution_count":null,"outputs":[]},{"metadata":{"id":"YdIhw6nuEBtt","colab_type":"text"},"cell_type":"markdown","source":"create Input vector of 200000 images each of shape (32,32,3) and scale them by dividing them by 255.save this as Y_data and also normalize the Y_data and save it as Z_data to train."},{"metadata":{"trusted":true},"cell_type":"code","source":"imgs[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"TbCYhpWPCIB7","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"train_y = []\ntrain_y2 = []\nfor _ in range(0,100000):\n  if _%20000 == 0:\n    print(\"{} / 100000\".format(_))\n  img = cv2.imread(imgs[_])\n  img = cv2.resize(img,(32,32),interpolation = cv2.INTER_AREA)\n  train_y.append(img.astype(\"float32\")/255.0)\nfor _ in range(100000,200000):\n  if _%20000 == 0:\n    print(\"{} / 200000\".format(_))\n  img = cv2.imread(imgs[_])\n  img = cv2.resize(img,(32,32),interpolation = cv2.INTER_AREA)\n  train_y2.append(img.astype(\"float32\")/255.0)\ntrain_y = np.array(train_y)\ntrain_y2 = np.array(train_y2)\nY_data = np.vstack((train_y,train_y2))\nprint(psutil.virtual_memory())\ndel train_y,train_y2\ngc.collect()\nprint(psutil.virtual_memory())\nZ_data = copy.deepcopy(Y_data)\nZ_data = (Z_data - Z_data.mean())/Z_data.std()","execution_count":null,"outputs":[]},{"metadata":{"id":"FwZ3vkCpEciP","colab_type":"text"},"cell_type":"markdown","source":"This is the test data"},{"metadata":{"id":"s2bhja5qCHzk","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"test_Y = []\nfor _ in range(200000,202599):\n  if _%5000 == 0:\n    print(\"{} / 100000\".format(_))\n  img = cv2.imread(imgs[_])\n  img = cv2.resize(img,(32,32),interpolation = cv2.INTER_AREA)\n  test_Y.append(img.astype(\"float32\")/255.0)\n  \ntest_Y = np.array(test_Y)\nmean = test_Y.mean()\nstd = test_Y.std()\ntest_Z = (test_Y - mean)/std","execution_count":0,"outputs":[]},{"metadata":{"id":"Sn4aeJp6Egq6","colab_type":"text"},"cell_type":"markdown","source":"Variational Auto Encoder has a sampler \nIn VAE, the encoder outputs two vectors.one is mean and the other is standard_deviation.sample from these two\nare taken as a final vector that can be done using the **sampler** function."},{"metadata":{"id":"orb_K5T77-7U","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"def sampler(layers):\n  std_norm = K.random_normal(shape=(K.shape(layers[0])[0], 128), mean=0, stddev=1)\n  return layers[0] + layers[1]*std_norm","execution_count":0,"outputs":[]},{"metadata":{"id":"yJMas-okFD-n","colab_type":"text"},"cell_type":"markdown","source":"Building the **Encoder** part"},{"metadata":{"id":"cERc9buwBaTB","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"stride = 2\ninp = Input(shape = (32,32,3))\nx = inp\nx = Conv2D(32,(2,2),strides = stride,activation = \"relu\",padding = \"same\")(x)\nx = Conv2D(64,(2,2),strides = stride,activation = \"relu\",padding = \"same\")(x)\nx = Conv2D(128,(2,2),strides = stride,activation = \"relu\",padding = \"same\")(x)\nshape = K.int_shape(x)\nx = Flatten()(x)\nx = Dense(256,activation = \"relu\")(x)\nmean_layer = Dense(128,activation = \"relu\")(x)\nsd_layer = Dense(128,activation = \"relu\")(x)\nlatent_vector = Lambda(sampler)([mean_layer,sd_layer])\nencoder = Model(inp,latent_vector,name = \"VAE_Encoder\")\nencoder.summary()\n","execution_count":0,"outputs":[]},{"metadata":{"id":"uIuoo_-eFJXl","colab_type":"text"},"cell_type":"markdown","source":"Building the **Decoder** part"},{"metadata":{"id":"u7qUl9JfBaQz","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"decoder_inp = Input(shape = (128,))\nx = decoder_inp\nx = Dense(shape[1]*shape[2]*shape[3],activation = \"relu\")(x)\nx = Reshape((shape[1],shape[2],shape[3]))(x)\nx = (Conv2DTranspose(32,(3,3),strides = stride,activation = \"relu\",padding = \"same\"))(x)\nx = (Conv2DTranspose(16,(3,3),strides = stride,activation = \"relu\",padding = \"same\"))(x)\nx = (Conv2DTranspose(8,(3,3),strides = stride,activation = \"relu\",padding = \"same\"))(x)\noutputs = Conv2DTranspose(3, (3,3), activation = 'sigmoid', padding = 'same', name = 'decoder_output')(x)\ndecoder = Model(decoder_inp,outputs,name = \"VAE_Decoder\")\ndecoder.summary()","execution_count":0,"outputs":[]},{"metadata":{"id":"9-6BNcyaFRji","colab_type":"text"},"cell_type":"markdown","source":"Connecting the Encoder and Decoder to make the **Auto Encoder**"},{"metadata":{"id":"i8KbxzMxBaOI","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"autoencoder = Model(inp,decoder(encoder(inp)),name = \"Variational_Auto_Encoder\")\nautoencoder.summary()","execution_count":0,"outputs":[]},{"metadata":{"id":"ierKRKACFYbI","colab_type":"text"},"cell_type":"markdown","source":"This is the loss function used by the VAE.It is calculating KL_Divergence loss.KL-Divergence is explained clearly in this article\n[KL-Divergence](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)"},{"metadata":{"id":"aHGAu5oGBaMZ","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"def vae_loss(input_img, output):\n\t# compute the average MSE error, then scale it up, ie. simply sum on all axes\n\treconstruction_loss = K.sum(K.square(output-input_img))\n\t# compute the KL loss\n\tkl_loss = - 0.5 * K.sum(1 + sd_layer - K.square(mean_layer) - K.square(K.exp(sd_layer)), axis=-1)\n\t# return the average loss over all images in batch\n\ttotal_loss = K.mean(reconstruction_loss + kl_loss)    \n\treturn total_loss","execution_count":0,"outputs":[]},{"metadata":{"id":"E2R5_XH6BaKV","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"autoencoder.compile(optimizer = \"adam\",loss = vae_loss,metrics = [\"accuracy\"])","execution_count":0,"outputs":[]},{"metadata":{"id":"Rm6u84OSF1OW","colab_type":"text"},"cell_type":"markdown","source":"Training the VAE"},{"metadata":{"id":"YrIay3BKBaIa","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"autoencoder.fit(Z_data,Y_data,batch_size = 200,epochs = 15,validation_split = 0.5)","execution_count":0,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder.fit(Z_data,Y_data,batch_size = 32,epochs = 30,validation_split = 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder.fit(Z_data,Y_data,batch_size = 200,epochs = 100,validation_split = 0.35)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder.fit(Z_data,Y_data,batch_size = 150,epochs = 30,validation_split = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder.fit(Z_data,Y_data,batch_size = 32,epochs = 200,validation_split = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder.fit(Z_data,Y_data,batch_size = 200,epochs = 2000,validation_split = 0)","execution_count":null,"outputs":[]},{"metadata":{"id":"5O_1hWqzBaGJ","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"pred = autoencoder.predict(test_Z)","execution_count":0,"outputs":[]},{"metadata":{"id":"DycMIFGgF4To","colab_type":"text"},"cell_type":"markdown","source":"Displaying the input,normalized input,VAE output "},{"metadata":{"id":"hTqpsXWbBaD_","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"temp = r(0,2599)\nprint(temp)\nplt.subplot(1,3,1)\nplt.imshow(test_Y[temp])\nplt.subplot(1,3,2)\nplt.imshow(test_Z[temp])\nplt.subplot(1,3,3)\nplt.imshow(pred[temp])","execution_count":0,"outputs":[]},{"metadata":{"id":"QTeMxYm6F99-","colab_type":"text"},"cell_type":"markdown","source":"Generating a new face by passing a random normal sample of size (32,32,3) and observing the output"},{"metadata":{"id":"EVpaLlSbB9UQ","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"gen = np.random.normal(size = (1,32,32,3))\ngen_sample = autoencoder.predict(gen)\nplt.subplot(1,2,1)\nplt.imshow(gen[0])\nplt.subplot(1,2,2)\nplt.imshow(gen_sample[0])","execution_count":0,"outputs":[]},{"metadata":{"id":"7TmgLCJqGHZu","colab_type":"text"},"cell_type":"markdown","source":"Saving the model weights"},{"metadata":{"id":"V1Mqdk3IB83a","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"autoencoder.save_weights(\"VAE-weights-\"+str(r(0,3653))+\".h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for _ in range(10):\n    img = np.random.normal(size = (9,32,32,3))\n    pred = autoencoder.predict(img)\n    op = np.vstack((np.hstack((pred[0],pred[1],pred[2])),np.hstack((pred[3],pred[4],pred[5])),np.hstack((pred[6],pred[7],pred[8]))))\n    print(op.shape)\n    op = cv2.resize(op,(288,288),interpolation = cv2.INTER_AREA)\n    cv2.imwrite(\"generated\"+str(r(0,9999))+\".jpg\",(op*255).astype(\"uint8\"))","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Variational Auto Encoder.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":1}