{"cells":[{"metadata":{},"cell_type":"markdown","source":"Ridge regression uses the same least-squares criterion, but with one difference. During the training phase, it adds a penalty for feature weights. Let's see the comparison. ","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn import linear_model\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/housesalesprediction/kc_house_data.csv\")\nprint(data.head())\nprint(data.shape)\nprint(data.dtypes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now i will get rid of the variables not required in the modeling: id and date.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(['id', 'date'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets do some Exploratory Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.distplot( a = data[\"price\"], hist = True, kde = True, \n             kde_kws={\"color\": \"g\", \"alpha\":0.3, \"linewidth\": 5, \"shade\":True}\n            )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x = \"price\", y = \"sqft_living\", data = data, fit_reg = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe the relation of price with different variables in one go too!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"p = sns.pairplot(data[['sqft_lot','sqft_above','price','sqft_living','bedrooms']], palette='afmhot',height=1.4)\np.set(xticklabels=[])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Price seems to have directrelation with most of the variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.price.values\ndata = data.drop(['price'], axis = 1)\nX = data.to_numpy()\ncolnames = data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\nlinridge = Ridge(alpha = 20.0).fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training R Squared : {}\".format(linridge.score(X_train, y_train)))\nprint(\"Testing R Squared : {}\".format(linridge.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets perform Regularization and find the best R-squared for different alpha values! ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nclf = Ridge().fit(X_train_scaled, y_train)\nR_squared = clf.score(X_test_scaled, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"R_squared","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see, the R-Squared has gone down. The normalization is not effective if we have broad range of variables being proved by this example. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Ridge Regression: Effect of alpha regularization paramater')\nfor this_alpha in [0, 1,  10, 20, 30, 50, 100, 250, 500]:\n    linridge = Ridge(alpha = this_alpha).fit(X_train, y_train)\n    r2_train = linridge.score(X_train, y_train)\n    r2_test = linridge.score(X_test, y_test)\n    num_coeff_bigger = np.sum(abs(linridge.coef_) > 1.0)\n    print('Alpha = {}\\n\\\n    num abs(coeff) > 1.0: {}, \\\n    r-squared training: {}, rsquared test: {}\\n '.format(this_alpha, num_coeff_bigger, r2_train, r2_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"at Alpha = 20, the R-Squared Test is the highest. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The LASSO Regression uses L1 regularization type to reduce error. LASSO is said to be used with lesser variables but with medium/large effects. Ridge instead is used with many variables but small/medium sized effects. Default alpha is 1.0 in Lasso","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nlinlasso = Lasso(alpha= 1.0, max_iter = 10000).fit(X_train_scaled, y_train)\nprint(\"Training R Squared : {}\".format(linlasso.score(X_train_scaled, y_train)))\nprint(\"Testing R Squared : {}\".format(linlasso.score(X_test_scaled, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finding the best parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Lasso Regression: Effect of alpha regularization paramater')\nfor alpha in [0.5, 1, 2,3,4,5,10,20,50]:\n    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)\n    r2_train = linlasso.score(X_train_scaled, y_train)\n    r2_test = linlasso.score(X_test_scaled, y_test)\n    \n    print('Alpha = {}\\n\\\n    Features kept: {}, r-squared training: {},\\\n    r-squared testing: {}\\n'.format(alpha, np.sum(linlasso.coef_ !=0), r2_train, r2_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alpha = 5 gives the highest R-Squared on test data. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"DEPLOYING POLYNOMIAL REGRESSION\nAdding extra polynomial features allows us a much richer set of complex functions that we can use to fit to the data. So you can think of this intuitively as allowing polynomials to be fit to the training data instead of simply a straight line, but using the same least-squares criterion that minimizes mean squared error.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree = 2)\nX_F1_poly = poly.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression().fit(X_train, y_train)\nprint('poly degree 2 Linear model R squared training:{}'.format(linreg.score(X_train, y_train)))\nprint('poly degree 2 Linear model R squared test:{}'.format(linreg.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see much improved R-Squared with polynomial model as it allows a better fit","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}