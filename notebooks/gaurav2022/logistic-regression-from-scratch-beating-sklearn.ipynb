{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n \nplt.rcParams['figure.facecolor'] = 'white'\nnp.random.seed(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class LogisticRegression():\n \n  def __init__(self, learning_rate = 0.01, epochs=500, plot =False):\n    self.learning_rate = learning_rate\n    self.epochs = epochs\n    self.weights = None\n    self.bias = None\n    self.cost = np.zeros((self.epochs))\n    self.plot = plot\n \n  def sigmoid(self,z):\n    return (1/(1+np.exp(-z)))\n \n  def predict_prob(self,x):\n    z = np.dot(x,self.weights) + self.bias             # for dot product (500,10)*(10,)  + 1\n    return (self.sigmoid(z))\n\n\n  def cost_function(self, x, y, y_hat):\n    c = (1/len(y))* sum(-y*np.log(y_hat) - (1-y)*np.log(1-y_hat))\n    return c\n    \n\n  def plot_function(self,x,y, y_hat,Cost):\n    w = np.linspace(-3,3, self.epochs)\n    #w = np.linspace(-self.cost[0], self.cost[0], self.epochs)\n    line = np.dot(x,self.weights) + self.bias\n\n    color = ['red' if i == 0 else 'green' for i in y]\n    plt.figure(figsize = (12,4))\n\n    plt.subplot(1,2,1)\n    plt.scatter(x,y)\n    plt.plot(x,y_hat, color = 'red')\n    plt.title('Gradient Decent trying hard to fit line')\n \n    plt.subplot(1,2,2)\n    plt.plot(w, w**2)\n    plt.scatter(self.weights[0] ,Cost, color = 'red')\n    plt.xlabel('weights')\n    plt.ylabel('cost')\n    plt.title('cost function')\n    plt.show()\n  \n \n  def fit(self,x,y):\n    \n    # x.shape = (500,10)   500 samples and 10 features\n    n_samples,n_features = x.shape      \n    \n    self.weights = np.zeros((n_features,))               # shape = (10,)\n    self.bias = np.zeros(1)                              #shape = (1,)\n        \n \n    for epoch in range(self.epochs): \n      \n      y_hat = self.predict_prob(x)       #shape = (500,)\n \n      self.cost[epoch] = self.cost_function(x, y, y_hat)\n      \n \n      if epoch % 100 == 0:\n        print('epoch:',epoch,'cost:',self.cost[epoch])\n        \n        if self.plot:\n          self.plot_function(x,y, y_hat, self.cost[epoch])\n \n      # for dot product (10,500)*(500,)\n      dw = np.dot(x.T, (y_hat - y))         #shape = (10,)\n      db = sum(y_hat - y)                     #shape = (1,)\n \n      self.weights -= self.learning_rate * dw\n      self.bias -= self.learning_rate * db\n  \n \n    self.intercept_ = self.bias\n    self.coef_ = self.weights\n    self.cost_ = self.cost\n    \n  def predict(self,x):\n    y_hat = self.predict_prob(x)\n    y_pred = [1 if i>0.5 else 0 for i in y_hat ]\n    return y_pred\n \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x1 = np.linspace(-10,-1,50)\nx2 = np.linspace(1,10,50)\nx = np.hstack((x1,x2))\ny = x  + np.random.normal(1,2,size= x.shape)\n\nz = np.zeros(y.shape)\nz[y>0] = 1\n\n\ncolor= ['red' if i == 0 else 'green' for i in z]\nplt.figure(figsize = (8,4))\n\nplt.subplot(1,2,1)\nplt.scatter(x,y, color = color)\n\nplt.subplot(1,2,2)\nplt.scatter(x,z)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = LogisticRegression(plot = True)\nm.fit(x[:,np.newaxis], z)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression as lr\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\n\ndataset = load_breast_cancer()\n\nX = dataset.data\ny = dataset.target\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\\\n                X, y, test_size=0.3, random_state=42)\n\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training\nmy_regressor = LogisticRegression()\nmy_regressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(my_regressor.cost_)\nplt.show","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sklearn_regressor = lr()\nsklearn_regressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training prediction\nmy_pred_train = my_regressor.predict(X_train)\nsklearn_pred_train = sklearn_regressor.predict(X_train)\n\n#testing prediction\nmy_pred_test = my_regressor.predict(X_test)\nsklearn_pred_test = sklearn_regressor.predict(X_test)\n\n\nprint('Comparing scores:\\n')\n\nprint('my prediction train accuracy:', accuracy_score(y_train, my_pred_train))\nprint('sklearn prediction train accuracy:', accuracy_score(y_train, sklearn_pred_train))\nprint()\nprint('my prediction test accuracy:', accuracy_score(y_test, my_pred_test))\nprint('sklearn prediction test accuracy:', accuracy_score(y_test, sklearn_pred_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}