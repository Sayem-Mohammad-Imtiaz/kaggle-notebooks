{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n \nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";  \nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":89,"outputs":[{"output_type":"stream","text":"['glove6b', 'imdb-review-dataset']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn import preprocessing\nfrom keras.preprocessing import sequence\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Embedding, SpatialDropout1D, MaxPooling1D, Embedding, Conv1D, Flatten, Dropout\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, LSTM\nfrom keras.optimizers import Adam\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import f1_score, confusion_matrix\nfrom sklearn.naive_bayes import MultinomialNB\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\n\n# load data\ninput_file = \"../input/imdb-review-dataset/imdb_master.csv\"\n\n# comma delimited is the default\ndata = pd.read_csv(input_file, header = 0, encoding='ISO-8859-1', engine='python')","execution_count":112,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"------------------------------------------------------------------------------------------------------------------------------------------------------\n------------------------------------------------------------------------------------------------------------------------------------------------------\n*Naive Bayes*"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rem_sw(df):\n    # Downloading stop words\n    stop_words = set(stopwords.words('english'))\n\n    # Removing Stop words from training data\n    count = 0\n    for sentence in df:\n        sentence = [word for word in sentence.lower().split() if word not in stop_words]\n        sentence = ' '.join(sentence)\n        df.loc[count] = sentence\n        count+=1\n    return(df)","execution_count":113,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rem_punc(df):\n    count = 0\n    for s in df:\n        cleanr = re.compile('<.*?>')\n        s = re.sub(r'\\d+', '', s)\n        s = re.sub(cleanr, '', s)\n        s = re.sub(\"'\", '', s)\n        s = re.sub(r'\\W+', ' ', s)\n        s = s.replace('_', '')\n        df.loc[count] = s\n        count+=1\n    return(df)","execution_count":114,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemma(df):\n\n    lmtzr = WordNetLemmatizer()\n\n    count = 0\n    stemmed = []\n    for sentence in df:    \n        word_tokens = word_tokenize(sentence)\n        for word in word_tokens:\n            stemmed.append(lmtzr.lemmatize(word))\n        sentence = ' '.join(stemmed)\n        df.iloc[count] = sentence\n        count+=1\n        stemmed = []\n    return(df)","execution_count":115,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stemma(df):\n\n    stemmer = SnowballStemmer(\"english\") #SnowballStemmer(\"english\", ignore_stopwords=True)\n\n    count = 0\n    stemmed = []\n    for sentence in df:\n        word_tokens = word_tokenize(sentence)\n        for word in word_tokens:\n            stemmed.append(stemmer.stem(word))\n        sentence = ' '.join(stemmed)\n        df.iloc[count] = sentence\n        count+=1\n        stemmed = []\n    return(df)","execution_count":116,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_master = pd.read_csv(\"../input/imdb-review-dataset/imdb_master.csv\", encoding='latin-1', index_col = 0)\n\nimdb_train = df_master[[\"review\", \"label\"]][df_master.type.isin(['train'])].reset_index(drop=True)\nimdb_test = df_master[[\"review\", \"label\"]][df_master.type.isin(['test'])].reset_index(drop=True)\n\nimdb_train['review'] = rem_sw(imdb_train['review'])\n\nimdb_test['review'] = rem_sw(imdb_test['review'])\n\nimdb_train['review'] = rem_punc(imdb_train['review'])\n\nimdb_test['review'] = rem_punc(imdb_test['review'])\n\nimdb_train['review'] = lemma(imdb_train['review'])\nimdb_train['review'] = stemma(imdb_train['review'])\n\nimdb_test['review'] = lemma(imdb_test['review'])\nimdb_test['review'] = stemma(imdb_test['review'])","execution_count":118,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imdb_unsup = df_master[[\"review\", \"label\"]][df_master.label.isin(['unsup'])].reset_index(drop=True)\n\n# Cleaning Unlabelled data\n\nimdb_unsup['review'] = rem_sw(imdb_unsup['review'])\nimdb_unsup['review'] = rem_punc(imdb_unsup['review'])\nimdb_unsup['review'] = lemma(imdb_unsup['review'])\nimdb_unsup['review'] = stemma(imdb_unsup['review'])\n\n# Vectorizing unlabelled reviews set\nvect = CountVectorizer(stop_words = 'english', analyzer='word')\nvect_pos = vect.fit_transform(imdb_unsup.review)\n\n# Creating a dataframe for the high frequency words for unlabelled reviews set\ndf_freq = pd.DataFrame(vect_pos.sum(axis=0), columns=list(vect.get_feature_names()), index = ['frequency']).T\n\n# Removing high frequency and low frequency data for more accuracy\nword_list = df_freq.nlargest(100, 'frequency').index\nword_list = word_list.append(df_freq.nsmallest(43750, 'frequency').index)\n\n# Removing unwanted words based on word_list from unlabelled data\ncount = 0\nfor sentence in imdb_unsup['review']:\n    sentence = [word for word in sentence.lower().split() if word not in word_list]\n    sentence = ' '.join(sentence)\n    imdb_unsup.loc[count, 'review'] = sentence\n    count+=1","execution_count":119,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################################## Preparing dataframe for model ##############################\n\n# Creating df_algo dataframe which will be used for hypothesis testing\ndf_algo = pd.concat([imdb_train, imdb_test], keys=['train', 'test'])\ndf_algo = df_algo.reset_index(col_level=1).drop(['level_1'], axis=1)\n\n# Cleaning the dataset\ndf_algo['review'] = rem_sw(df_algo['review'])\ndf_algo['review'] = rem_punc(df_algo['review'])\ndf_algo['review'] = lemma(df_algo['review'])\ndf_algo['review'] = stemma(df_algo['review'])\n\n# df_algo = pd.read_csv(\"clean_algo.csv\", encoding='latin-1', index_col = 0) # Uncomment this line to load from csv","execution_count":120,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################################### Removing non feature words ###############################\nfrom sklearn.preprocessing import LabelEncoder\nimport operator\n\n# Creating the feature word_list\n# Selecting 14440 feature selected words based on 80-20 rule\nle = LabelEncoder()\nword_list = get_feature(df_algo[['review', 'label']], 14440)\n\n# Removing non prefered words from training and test combined data\ncount = 0\nfor sentence in df_algo['review']:\n    sentence = [word for word in sentence.lower().split() if word in word_list]\n    sentence = ' '.join(sentence)\n    df_algo.loc[count, 'review'] = sentence\n    count+=1","execution_count":125,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"################################## Splitting with feature selection data ###############################a\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# Vectorising the required data\nvect_algo = TfidfVectorizer(stop_words='english', analyzer='word')\nvect_algo.fit(df_algo.review)\nXf_train = vect_algo.transform(df_algo[df_algo['level_0'].isin(['train'])].review)\nXf_test = vect_algo.transform(df_algo[df_algo['level_0'].isin(['test'])].review)\n\n# Encoding target data\n# Creating an object and fitting on target strings\n\nyf_train = le.fit_transform(df_algo[df_algo['level_0'].isin(['train'])].label)\nyf_test = le.fit_transform(df_algo[df_algo['level_0'].isin(['test'])].label)","execution_count":128,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################################### Naive Bayes #########################################\nfrom sklearn.metrics import accuracy_score\n# Fit the Naive Bayes classifier model to the object\nclf = MultinomialNB()\nclf.fit(Xf_train, yf_train)\n\n# predict the outcome for testing data\npredictions = clf.predict(Xf_test)\n\n# check the accuracy of the model\naccuracy = accuracy_score(yf_test, predictions)\nprint(\"Observation: Naive Bayes Classification gives an accuracy of %.2f%% on the testing data\" %(accuracy*100))","execution_count":130,"outputs":[{"output_type":"stream","text":"Observation: Naive Bayes Classification gives an accuracy of 1.13% on the testing data\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"------------------------------------------------------------------------------------------------------------------------------------------------------\n------------------------------------------------------------------------------------------------------------------------------------------------------\n*Word embeddings*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","execution_count":36,"outputs":[{"output_type":"stream","text":"[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 10056875027996068974\n, name: \"/device:XLA_CPU:0\"\ndevice_type: \"XLA_CPU\"\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 13749474256287560615\nphysical_device_desc: \"device: XLA_CPU device\"\n, name: \"/device:XLA_GPU:0\"\ndevice_type: \"XLA_GPU\"\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 2871300394165162002\nphysical_device_desc: \"device: XLA_GPU device\"\n, name: \"/device:GPU:0\"\ndevice_type: \"GPU\"\nmemory_limit: 15856546612\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 8950815957514336975\nphysical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"   Unnamed: 0  type     ...      label         file\n0           0  test     ...        neg      0_2.txt\n1           1  test     ...        neg  10000_4.txt\n2           2  test     ...        neg  10001_1.txt\n3           3  test     ...        neg  10002_3.txt\n4           4  test     ...        neg  10003_3.txt\n\n[5 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>type</th>\n      <th>review</th>\n      <th>label</th>\n      <th>file</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>test</td>\n      <td>Once again Mr. Costner has dragged out a movie...</td>\n      <td>neg</td>\n      <td>0_2.txt</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>test</td>\n      <td>This is an example of why the majority of acti...</td>\n      <td>neg</td>\n      <td>10000_4.txt</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>test</td>\n      <td>First of all I hate those moronic rappers, who...</td>\n      <td>neg</td>\n      <td>10001_1.txt</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>test</td>\n      <td>Not even the Beatles could write songs everyon...</td>\n      <td>neg</td>\n      <td>10002_3.txt</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>test</td>\n      <td>Brass pictures (movies is not a fitting word f...</td>\n      <td>neg</td>\n      <td>10003_3.txt</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 1} ) \nsess = tf.Session(config=config) \nkeras.backend.set_session(sess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indexes_train = np.where(data['type'] == 'train')\nindexes_test = np.where(data['type'] == 'test')\n\nX_train = data['review'][indexes_train[0]].values\nY_train = data['label'][indexes_train[0]].values\n\nX_test = data['review'][indexes_test[0]].values\nY_test = data['label'][indexes_test[0]].values\n\nindex_unsup = np.where(Y_train == 'unsup')\nY_train = np.delete(Y_train, index_unsup)\nX_train = np.delete(X_train, index_unsup)","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = preprocessing.LabelEncoder()\nle.fit(Y_train)\n\nY_train_encod = le.transform(Y_train) \nY_test_encod = le.transform(Y_test) \n\nX_train, Y_train_encod = shuffle(X_train, Y_train_encod)","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = 10000\nmaxlen = 100\nembedding_dimenssion = 100\n\nVALIDATION_SPLIT = 0.1\nCLASSES = 1\nNB_EPOCH = 20\nBATCH_SIZE = 64\nOPTIMIZER = Adam(lr=0.001)\n\n# Tokenization and encoding text corpus\ntk = Tokenizer(num_words=max_features)\ntk.fit_on_texts(X_train)\nX_train_en = tk.texts_to_sequences(X_train)\nX_test_en = tk.texts_to_sequences(X_test)\n\nword2index = tk.word_index\nindex2word = tk.index_word","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = sequence.pad_sequences(X_train_en, maxlen=maxlen)\nX_test_new = sequence.pad_sequences(X_test_en, maxlen=maxlen)\n\nglove_dir = ''.join(['../input/glove6b/glove.6B.', str(embedding_dimenssion),'d.txt'])\n\nembeddings_index = {}\n\nwith open(glove_dir, encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        embedding = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = embedding \n        \nprint('Found {:,} word vectors in GloVe.'.format(len(embeddings_index)))","execution_count":41,"outputs":[{"output_type":"stream","text":"Found 400,000 word vectors in GloVe.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((max_features, embedding_dimenssion))\n\nfor word, i in word2index.items():\n    if i >= max_features:\n        break\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Embedding(max_features, embedding_dimenssion, input_length=maxlen,\n                    weights=[embedding_matrix], trainable=False))\nmodel.add(LSTM(125, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.summary()\n\n\nmodel.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n\nmodel.fit(X_train_new, Y_train_encod, batch_size=BATCH_SIZE, epochs=10, validation_split=VALIDATION_SPLIT, verbose=1)\n\nscores = model.evaluate(X_test_new, Y_test_encod)\nprint('losses: {}'.format(scores[0]))\nprint('TEST accuracy: {}'.format(scores[1]))","execution_count":44,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_4 (Embedding)      (None, 100, 100)          1000000   \n_________________________________________________________________\nlstm_4 (LSTM)                (None, 125)               113000    \n_________________________________________________________________\ndense_4 (Dense)              (None, 1)                 126       \n=================================================================\nTotal params: 1,113,126\nTrainable params: 113,126\nNon-trainable params: 1,000,000\n_________________________________________________________________\nTrain on 22500 samples, validate on 2500 samples\nEpoch 1/10\n22500/22500 [==============================] - 80s 4ms/step - loss: 0.6083 - acc: 0.6608 - val_loss: 0.4820 - val_acc: 0.7700\nEpoch 2/10\n22500/22500 [==============================] - 77s 3ms/step - loss: 0.4760 - acc: 0.7690 - val_loss: 0.4074 - val_acc: 0.8172\nEpoch 3/10\n22500/22500 [==============================] - 77s 3ms/step - loss: 0.4283 - acc: 0.8011 - val_loss: 0.3737 - val_acc: 0.8312\nEpoch 4/10\n22500/22500 [==============================] - 77s 3ms/step - loss: 0.4023 - acc: 0.8154 - val_loss: 0.3546 - val_acc: 0.8428\nEpoch 5/10\n22500/22500 [==============================] - 77s 3ms/step - loss: 0.3814 - acc: 0.8277 - val_loss: 0.3615 - val_acc: 0.8368\nEpoch 6/10\n22500/22500 [==============================] - 77s 3ms/step - loss: 0.3670 - acc: 0.8352 - val_loss: 0.3369 - val_acc: 0.8540\nEpoch 7/10\n22500/22500 [==============================] - 77s 3ms/step - loss: 0.3524 - acc: 0.8420 - val_loss: 0.3329 - val_acc: 0.8568\nEpoch 8/10\n22500/22500 [==============================] - 77s 3ms/step - loss: 0.3379 - acc: 0.8508 - val_loss: 0.3224 - val_acc: 0.8696\nEpoch 9/10\n22500/22500 [==============================] - 77s 3ms/step - loss: 0.3281 - acc: 0.8554 - val_loss: 0.3196 - val_acc: 0.8660\nEpoch 10/10\n22500/22500 [==============================] - 77s 3ms/step - loss: 0.3140 - acc: 0.8614 - val_loss: 0.3307 - val_acc: 0.8552\n25000/25000 [==============================] - 67s 3ms/step\nlosses: 0.3337736030101776\nTEST accuracy: 0.85368\n","name":"stdout"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}