{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport seaborn as sns\nimport plotly.express as px\nfrom itertools import product\nimport warnings\nimport statsmodels.api as sm\nplt.style.use('seaborn-darkgrid')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the dataset\nbitstamp = pd.read_csv(\"/kaggle/input/bitcoin-historical-data/bitstampUSD_1-min_data_2012-01-01_to_2020-09-14.csv\")\nbitstamp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bitstamp.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the Timestamp column from string to datetime\nbitstamp['Timestamp'] = [datetime.fromtimestamp(x) for x in bitstamp['Timestamp']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bitstamp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Dataset Shape: ',  bitstamp.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bitstamp.set_index(\"Timestamp\").Weighted_Price.plot(figsize=(10,7), title=\"Bitcoin Weighted Price\", color='green')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data PreProcessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculating missing values in the dataset\nmissing_values = bitstamp.isnull().sum()\nmissing_per = (missing_values/bitstamp.shape[0])*100\nmissing_table = pd.concat([missing_values,missing_per], axis=1, ignore_index=True) \nmissing_table.rename(columns={0:'Total Missing Values',1:'Missing %'}, inplace=True)\nmissing_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#testing missing value methods on a subset\n\na = bitstamp.set_index('Timestamp')\na = a['2019-11-01 00:10:00':'2019-11-02 00:10:00']\n\na['ffill'] = a['Weighted_Price'].fillna(method='ffill') # Imputation using ffill/pad\na['bfill'] = a['Weighted_Price'].fillna(method='bfill') # Imputation using bfill/pad\na['interp'] = a['Weighted_Price'].interpolate()         # Imputation using interpolation\n\na.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Imputation using Linear Interpolation method\n\nTime series data has a lot of variations against time. Hence, imputing using backfill and forward fill isn't the best possible solution to address the missing value problem. A more apt alternative would be to use interpolation methods, where the values are filled with incrementing or decrementing values.\nLinear interpolation is an imputation technique that assumes a linear relationship between data points and utilises non-missing values from adjacent data points to compute a value for a missing data point."},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to impute missing values using interpolation\ndef fill_missing(df):\n    df['Open'] = df['Open'].interpolate()\n    df['Close'] = df['Close'].interpolate()\n    df['High'] = df['High'].interpolate()\n    df['Low'] = df['Low'].interpolate()\n    df['Weighted_Price'] = df['Weighted_Price'].interpolate()\n    df['Volume_(BTC)'] = df['Volume_(BTC)'].interpolate()\n    df['Volume_(Currency)'] = df['Volume_(Currency)'].interpolate()\n\n\n    print(df.head())\n    print(\"\\n\")\n    print(df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fill_missing(bitstamp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No Null values in the final output."},{"metadata":{},"cell_type":"markdown","source":"## Data Visualisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#created a copy \nbitstamp_non_indexed = bitstamp.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bitstamp = bitstamp.set_index('Timestamp')\nbitstamp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Resampling data\nhourly_data = bitstamp.resample('1H').mean()\nhourly_data = hourly_data.reset_index()\nhourly_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#daily resampling\nbitstamp_daily = bitstamp.resample(\"24H\").mean() \nbitstamp_daily.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bitstamp_daily.reset_index(inplace=True)\n\ntrace1 = go.Scatter(\n    x = bitstamp_daily['Timestamp'],\n    y = bitstamp_daily['Open'].astype(float),\n    mode = 'lines',\n    name = 'Open'\n)\n\ntrace2 = go.Scatter(\n    x = bitstamp_daily['Timestamp'],\n    y = bitstamp_daily['Close'].astype(float),\n    mode = 'lines',\n    name = 'Close'\n)\ntrace3 = go.Scatter(\n    x = bitstamp_daily['Timestamp'],\n    y = bitstamp_daily['Weighted_Price'].astype(float),\n    mode = 'lines',\n    name = 'Weighted Avg'\n)\n\nlayout = dict(\n    title='Historical Bitcoin Prices with the Slider ',\n    xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1,\n                     label='1m',\n                     step='month',\n                     stepmode='backward'),\n                dict(count=6,\n                     label='6m',\n                     step='month',\n                     stepmode='backward'),\n                dict(count=12,\n                     label='1y',\n                     step='month',\n                     stepmode='backward'),\n                dict(count=36,\n                     label='3y',\n                     step='month',\n                     stepmode='backward'),\n                dict(count=60,\n                     label='5y',\n                     step='month',\n                     stepmode='backward'),\n                dict(step='all')\n            ])\n        ),\n        rangeslider=dict(\n            visible = True\n        ),\n        type='date'\n    )\n)\n\ndata = [trace1,trace2,trace3]\nfig = dict(data=data, layout=layout)\niplot(fig, filename = \"Time Series with Rangeslider\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trace1 = go.Scatter(\n    x = bitstamp_daily['Timestamp'],\n    y = bitstamp_daily['Volume_(Currency)'].astype(float),\n    mode = 'lines',\n    name = 'Currency',\n    marker = dict(\n            color='#FFBB33')\n)\n\nlayout = dict(\n    title='Currency(USD) Volume traded in Bitcoin with the slider',\n    xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1,\n                     label='1m',\n                     step='month',\n                     stepmode='backward'),\n                dict(count=6,\n                     label='6m',\n                     step='month',\n                     stepmode='backward'),\n                dict(count=12,\n                     label='1y',\n                     step='month',\n                     stepmode='backward'),\n                dict(count=36,\n                     label='3y',\n                     step='month',\n                     stepmode='backward'),\n                dict(count=60,\n                     label='5y',\n                     step='month',\n                     stepmode='backward'),\n                dict(step='all')\n            ])\n        ),\n        rangeslider=dict(\n            visible = True\n        ),\n        type='date'\n    )\n)\n\ndata = [trace1]\nfig = dict(data=data, layout=layout)\niplot(fig, filename = \"Time Series with Rangeslider\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BTC Volume vs USD visualization\ntrace = go.Scattergl(\n    y = bitstamp_daily['Volume_(BTC)'].astype(float),\n    x = bitstamp_daily['Weighted_Price'].astype(float),\n    mode = 'markers',\n    marker = dict(\n        line = dict(width = 1),\n        color='#00FF00'\n    )\n)\nlayout = go.Layout(\n    title='BTC Volume v/s USD',\n    xaxis=dict(\n        title='Weighted Price',\n        titlefont=dict(\n            family='Times New Roman, monospace',\n            size=18\n        )\n    ),\n    yaxis=dict(\n        title='Volume BTC',\n        titlefont=dict(\n            family='Times New Roman, monospace',\n            size=18\n    )))\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename='compare_webgl')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Time Series Decomposition & Statistical Tests\nWe can decompose a time series into trend, seasonal amd remainder components.The seasonal_decompose in statsmodels is used to implements the decomposition."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import kpss\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fill_missing(bitstamp_daily)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decomposition = sm.tsa.seasonal_decompose(bitstamp_daily.Weighted_Price,period=1)\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nax, fig = plt.subplots(figsize=(12,8), sharex=True)\n\nplt.subplot(411)\nplt.plot(bitstamp_daily.Weighted_Price, label='Original',color='b')\nplt.title(\"Observed\",loc=\"left\", alpha=0.75, fontsize=18)\n\nplt.subplot(412)\nplt.plot(trend, label='Trend',color='g')\nplt.title(\"Trend\",loc=\"left\", alpha=0.75, fontsize=18)\n\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality',color='r')\nplt.title(\"Seasonal\",loc=\"left\", alpha=0.75, fontsize=18)\n\nplt.subplot(414)\nplt.plot(residual, label='Residuals',color='c')\nplt.title(\"Residual\",loc=\"left\", alpha=0.75, fontsize=18)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Post time series decomposition we don't observe any seasonality. Also, there is no constant mean, variance and covariance, hence the series is Non Stationary."},{"metadata":{},"cell_type":"markdown","source":"Stastical tests can be performed for checking if the time series is stationary or not. One such test is Dickey-Fuller Test. The DF tests a null hypothesis that a unit root is present in an autoregressive model. If the value is less then <0.05 then the data is stationary."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Dicky-Fuller stationarity test: p=%f\" % sm.tsa.adfuller(bitstamp_daily[\"Weighted_Price\"])[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Rolling windows\nA rolling mean, or moving average, is a transformation method which helps average out noise from data. It works by simply splitting and aggregating the data into windows according to function, such as mean(), median(), count(), etc. For this dataset, we’ll use a rolling mean for 3, 7 and 30 days."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = bitstamp_daily.set_index(\"Timestamp\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.reset_index(drop=False, inplace=True)\n\nlag_features = [\"Open\", \"High\", \"Low\", \"Close\",\"Volume_(BTC)\"]\nwindow1 = 3\nwindow2 = 7\nwindow3 = 30\n\ndf_rolled_3d = df[lag_features].rolling(window=window1, min_periods=0)\ndf_rolled_7d = df[lag_features].rolling(window=window2, min_periods=0)\ndf_rolled_30d = df[lag_features].rolling(window=window3, min_periods=0)\n\ndf_mean_3d = df_rolled_3d.mean().shift(1).reset_index()\ndf_mean_7d = df_rolled_7d.mean().shift(1).reset_index()\ndf_mean_30d = df_rolled_30d.mean().shift(1).reset_index()\n\ndf_std_3d = df_rolled_3d.std().shift(1).reset_index()\ndf_std_7d = df_rolled_7d.std().shift(1).reset_index()\ndf_std_30d = df_rolled_30d.std().shift(1).reset_index()\n\nfor feature in lag_features:\n    df[f\"{feature}_mean_lag{window1}\"] = df_mean_3d[feature]\n    df[f\"{feature}_mean_lag{window2}\"] = df_mean_7d[feature]\n    df[f\"{feature}_mean_lag{window3}\"] = df_mean_30d[feature]\n    \n    df[f\"{feature}_std_lag{window1}\"] = df_std_3d[feature]\n    df[f\"{feature}_std_lag{window2}\"] = df_std_7d[feature]\n    df[f\"{feature}_std_lag{window3}\"] = df_std_30d[feature]\n\ndf.fillna(df.mean(), inplace=True)\n\ndf.set_index(\"Timestamp\", drop=False, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"month\"] = df.Timestamp.dt.month\ndf[\"week\"] = df.Timestamp.dt.week\ndf[\"day\"] = df.Timestamp.dt.day\ndf[\"day_of_week\"] = df.Timestamp.dt.dayofweek\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df[df.Timestamp < \"2020\"]\ndf_valid = df[df.Timestamp >= \"2020\"]\n\nprint('train shape :', df_train.shape)\nprint('validation shape :', df_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ARIMA Model"},{"metadata":{},"cell_type":"markdown","source":"ARIMA is an acronym that stands for AutoRegressive Integrated Moving Average. It is a class of model that captures a suite of different standard temporal structures in time series data."},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install pmdarima","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pmdarima as pm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exogenous_features = ['Open_mean_lag3',\n       'Open_mean_lag7', 'Open_mean_lag30', 'Open_std_lag3', 'Open_std_lag7',\n       'Open_std_lag30', 'High_mean_lag3', 'High_mean_lag7', 'High_mean_lag30',\n       'High_std_lag3', 'High_std_lag7', 'High_std_lag30', 'Low_mean_lag3',\n       'Low_mean_lag7', 'Low_mean_lag30', 'Low_std_lag3', 'Low_std_lag7',\n       'Low_std_lag30', 'Close_mean_lag3', 'Close_mean_lag7',\n       'Close_mean_lag30', 'Close_std_lag3', 'Close_std_lag7',\n       'Close_std_lag30', 'Volume_(BTC)_mean_lag3', 'Volume_(BTC)_mean_lag7',\n       'Volume_(BTC)_mean_lag30', 'Volume_(BTC)_std_lag3',\n       'Volume_(BTC)_std_lag7', 'Volume_(BTC)_std_lag30', 'month', 'week',\n       'day', 'day_of_week']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = pm.auto_arima(df_train.Weighted_Price, exogenous=df_train[exogenous_features], trace=True,\n                      error_action=\"ignore\", suppress_warnings=True)\nmodel.fit(df_train.Weighted_Price, exogenous=df_train[exogenous_features])\n\nforecast = model.predict(n_periods=len(df_valid), exogenous=df_valid[exogenous_features])\ndf_valid[\"Forecast_ARIMAX\"] = forecast","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_valid[[\"Weighted_Price\", \"Forecast_ARIMAX\"]].plot(figsize=(14, 7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\nprint(\"RMSE of Auto ARIMAX:\", np.sqrt(mean_squared_error(df_valid.Weighted_Price, df_valid.Forecast_ARIMAX)))\nprint(\"\\nMAE of Auto ARIMAX:\", mean_absolute_error(df_valid.Weighted_Price, df_valid.Forecast_ARIMAX))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Facebook Prophet\n"},{"metadata":{},"cell_type":"markdown","source":"Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality effects.\nIt works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well."},{"metadata":{"trusted":true},"cell_type":"code","source":"from fbprophet import Prophet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Resampling originial data to day level and forward fill the missing values\ndaily_data = bitstamp.resample(\"24H\").mean() \nfill_missing(daily_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming the column names accroding to Prophet's requirements\ndaily_data_fb = daily_data.reset_index()[['Timestamp','Weighted_Price']].rename({'Timestamp':'ds','Weighted_Price':'y'}, axis=1)\ndaily_data_fb.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split_date = \"2020-01-01\"\ntrain_filt = daily_data_fb['ds'] <= split_date\ntest_filt = daily_data_fb['ds'] > split_date\n\ntrain_fb = daily_data_fb[train_filt]\ntest_fb = daily_data_fb[test_filt]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train data shape :\", train_fb.shape)\nprint(\"test data shape :\", test_fb.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_fbp = Prophet()\nfor feature in exogenous_features:\n    model_fbp.add_regressor(feature)\n\nmodel_fbp.fit(df_train[[\"Timestamp\", \"Weighted_Price\"] + exogenous_features].rename(columns={\"Timestamp\": \"ds\", \"Weighted_Price\": \"y\"}))\n\nforecast = model_fbp.predict(df_valid[[\"Timestamp\", \"Weighted_Price\"] + exogenous_features].rename(columns={\"Timestamp\": \"ds\"}))\nforecast.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - **yhat** : the predicted forecast\n - **yhat_lower** : the lower border of the prediction\n - **yhat_upper**: the upper border of the prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_valid[\"Forecast_Prophet\"] = forecast.yhat.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot Our Predictions\nfig1 = model_fbp.plot(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"model_fbp.plot_components(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_valid[[\"Weighted_Price\", \"Forecast_Prophet\"]].plot(figsize=(14, 7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_mae = mean_absolute_error(df_valid['Weighted_Price'], df_valid['Forecast_Prophet'])\ntest_rmse = np.sqrt(mean_squared_error(df_valid['Weighted_Price'], df_valid['Forecast_Prophet']))\n\nprint(f\"Prophet's MAE : {test_mae}\")\nprint(f\"Prophet's RMSE : {test_rmse}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import ensemble\nfrom sklearn import metrics\nfrom sklearn.model_selection import RandomizedSearchCV\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nplt.style.use('fivethirtyeight')\n\nfrom datetime import datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train Test Split\nX_train, y_train = df_train[exogenous_features], df_train.Weighted_Price\nX_test, y_test = df_valid[exogenous_features], df_valid.Weighted_Price","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = xgb.XGBRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyper Parameter Optimization\nparams={\n \"learning_rate\"    : [0.10,0.20,0.30],\n \"max_depth\"        : [1, 3, 4, 5, 6, 7],\n \"n_estimators\"     : [int(x) for x in np.linspace(start=100, stop=1000, num=10)],\n \"min_child_weight\" : [int(x) for x in np.arange(3, 10, 1)],\n \"gamma\"            : [0.0, 0.2 , 0.4, 0.6],\n \"subsample\"        : [0.5, 0.6, 0.7, 0.8, 0.9, 1],\n \"colsample_bytree\" : [0.5, 0.7, 0.9, 1],\n \"colsample_bylevel\": [0.5, 0.7, 0.9, 1],  \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model  = RandomizedSearchCV(    \n                reg,\n                param_distributions=params,\n                n_iter=20,\n                n_jobs=-1,\n                cv=5,\n                verbose=3,\n                )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Model Best Parameters : {model.best_params_}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Predicted_Weighted_Price'] = model.predict(X_train)\n\ndf_train[['Weighted_Price','Predicted_Weighted_Price']].plot(figsize=(15, 5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_valid['Forecast_XGBoost'] = model.predict(X_test)\n\noverall_data = pd.concat([df_train, df_valid], sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_valid[['Weighted_Price','Forecast_XGBoost']].plot(figsize=(15, 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"overall_data[['Weighted_Price','Forecast_XGBoost']].plot(figsize=(15, 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_mae = mean_absolute_error(df_train['Weighted_Price'], df_train['Predicted_Weighted_Price'])\ntrain_rmse = np.sqrt(mean_squared_error(df_train['Weighted_Price'], df_train['Predicted_Weighted_Price']))\n\nprint(f\"train MAE : {train_mae}\")\nprint(f\"train RMSE : {train_rmse}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_mae = mean_absolute_error(df_valid['Weighted_Price'], df_valid['Forecast_XGBoost'])\ntest_rmse = np.sqrt(mean_squared_error(df_valid['Weighted_Price'], df_valid['Forecast_XGBoost']))\n\nprint(f\"test MAE : {test_mae}\")\nprint(f\"test RMSE : {test_rmse}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTM"},{"metadata":{},"cell_type":"markdown","source":"Long Short Term Memory networks are a special kind of RNN, capable of learning long-term dependencies.\nLSTMs are explicitly designed to avoid the long-term dependency problem. \nAlso, they don't suffer from problems like vanishing/exploding gradient descent."},{"metadata":{"trusted":true},"cell_type":"code","source":"price_series = bitstamp_daily.reset_index().Weighted_Price.values\nprice_series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"price_series.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range = (0, 1))\nprice_series_scaled = scaler.fit_transform(price_series.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"price_series_scaled, price_series_scaled.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, test_data = price_series_scaled[0:2923], price_series_scaled[2923:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.shape, test_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def windowed_dataset(series, time_step):\n    dataX, dataY = [], []\n    for i in range(len(series)- time_step-1):\n        a = series[i : (i+time_step), 0]\n        dataX.append(a)\n        dataY.append(series[i+ time_step, 0])\n        \n    return np.array(dataX), np.array(dataY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, y_train = windowed_dataset(train_data, time_step=100)\nX_test, y_test = windowed_dataset(test_data, time_step=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reshape inputs to be [samples, timesteps, features] which is requred for LSTM\n\nX_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n\nprint(X_train.shape) \nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create LSTM Model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialising the LSTM\nregressor = Sequential()\n\n# Adding the first LSTM layer and some Dropout regularisation\nregressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\nregressor.add(Dropout(0.5))\n\n# Adding a second LSTM layer and some Dropout regularisation\nregressor.add(LSTM(units = 50, return_sequences = True))\nregressor.add(Dropout(0.5))\n\n# Adding a third LSTM layer and some Dropout regularisation\nregressor.add(LSTM(units = 50, return_sequences = True,))\nregressor.add(Dropout(0.5))\n\n# Adding a fourth LSTM layer and some Dropout regularisation\nregressor.add(LSTM(units = 50))\nregressor.add(Dropout(0.5))\n\n# Adding the output layer\nregressor.add(Dense(units = 1))\n\n# Compiling the model\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the LSTM to the Training set\nhistory = regressor.fit(X_train, y_train, validation_split=0.2, epochs = 100, batch_size = 32, verbose=1, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,7))\nplt.plot(history.history[\"loss\"], label= \"train loss\")\nplt.plot(history.history[\"val_loss\"], label= \"validation loss\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prediction\ntrain_predict = regressor.predict(X_train)\ntest_predict = regressor.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#transformation to original form\ny_train_inv = scaler.inverse_transform(y_train.reshape(-1, 1))\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\ntrain_predict_inv = scaler.inverse_transform(train_predict)\ntest_predict_inv = scaler.inverse_transform(test_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prediction on Training data\nplt.figure(figsize=(12,7))\nplt.plot(y_train_inv.flatten(), marker='.', label=\"Actual\")\nplt.plot(train_predict_inv.flatten(), 'r', marker='.', label=\"Predicted\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#Prediction on Test data\nplt.figure(figsize=(12,7))\nplt.plot(y_test_inv.flatten(), marker='.', label=\"Actual\")\nplt.plot(test_predict_inv.flatten(), 'r', marker='.', label=\"Predicted\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_RMSE = np.sqrt(mean_squared_error(y_train, train_predict))\ntrain_MAE = np.sqrt(mean_absolute_error(y_train, train_predict))\nLSTM_RMSE = np.sqrt(mean_squared_error(y_test, test_predict))\nLSTM_MAE = np.sqrt(mean_absolute_error(y_test, test_predict))\n\n\nprint(f\"Train RMSE: {train_RMSE}\")\nprint(f\"Train MAE: {train_MAE}\")\n\nprint(f\"Test RMSE: {LSTM_RMSE}\")\nprint(f\"Test MAE: {LSTM_MAE}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Comparison"},{"metadata":{"trusted":true},"cell_type":"code","source":"arimax_rmse = np.sqrt(mean_squared_error(df_valid['Weighted_Price'], df_valid['Forecast_ARIMAX']))\nfbp_rmse = np.sqrt(mean_squared_error(df_valid['Weighted_Price'], df_valid['Forecast_Prophet']))\nxgb_rmse = np.sqrt(mean_squared_error(df_valid['Weighted_Price'], df_valid['Forecast_XGBoost']))\n\narimax_mae = mean_absolute_error(df_valid['Weighted_Price'], df_valid['Forecast_ARIMAX'])\nfbp_mae = mean_absolute_error(df_valid['Weighted_Price'], df_valid['Forecast_Prophet'])\nxgb_mae = mean_absolute_error(df_valid['Weighted_Price'], df_valid['Forecast_XGBoost'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"ARIMAX RMSE :\", arimax_rmse)\nprint(\"FB Prophet RMSE :\", fbp_rmse)\nprint(\"XGBoost RMSE :\", xgb_rmse)\nprint(\"LSTM RMSE :\", LSTM_RMSE)\n\nprint(\"\\nARIMAX MAE :\", arimax_mae)\nprint(\"FB Prophet MAE :\", fbp_mae)\nprint(\"XGBoost MAE :\", xgb_mae)\nprint(\"LSTM MAE :\", LSTM_MAE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8,5))\nX = ['Arimax','Prophet','XGBoost','LSTM']\nY = [arimax_rmse,fbp_rmse,xgb_rmse,LSTM_RMSE*100]\nax = sns.barplot(x=X,y=Y,palette='cool')\nax.set(xlabel ='Model',ylabel ='RMSE score')\nplt.show()\nprint('*Note: We have multiplied Rmse score of lstm model by 100 so that it can be visualised')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8,5))\nX = ['Arimax','Prophet','XGBoost','LSTM']\nY = [arimax_mae,fbp_mae,xgb_mae,LSTM_MAE*100]\nax = sns.barplot(x=X,y=Y,palette='viridis')\nax.set(xlabel ='Model',ylabel ='MAE score')\nplt.show()\nprint('*Note: We have multiplied MAE score of lstm model by 100 so that it can be visualised')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trace1 = go.Scatter(\n    x = df_valid['Timestamp'],\n    y = df_valid['Weighted_Price'],\n    mode = 'lines',\n    name = 'Weighted Price'\n)\n\ntrace2 = go.Scatter(\n    x = df_valid['Timestamp'],\n    y = df_valid['Forecast_ARIMAX'],\n    mode = 'lines',\n    name = 'ARIMA Forecast'\n)\ntrace3 = go.Scatter(\n    x = df_valid['Timestamp'],\n    y = df_valid['Forecast_Prophet'],\n    mode = 'lines',\n    name = 'Prophet Forecast'\n)\ntrace4 = go.Scatter(\n    x = df_valid['Timestamp'],\n    y = df_valid['Forecast_XGBoost'],\n    mode = 'lines',\n    name = 'XGBoost Forecast'\n)\ntrace5 = go.Scatter(\n    x = df_valid['Timestamp'],\n    y = df_valid['Weighted_Price'],\n    mode = 'lines',\n    name = 'Forecast_LSTM'\n)\n\n\nlayout = dict(\n    title='Model Comparison ',\n    xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1,\n                     label='1m',\n                     step='month',\n                     stepmode='backward'),\n                dict(count=6,\n                     label='6m',\n                     step='month',\n                     stepmode='backward'),\n                dict(step='all')\n            ])\n        ),\n        rangeslider=dict(\n            visible = True\n        ),\n        type='date'\n    )\n)\n\ndata = [trace1,trace2,trace3,trace4,trace5]\nfig = dict(data=data, layout=layout)\niplot(fig, filename = \"Time Series with Rangeslider\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}