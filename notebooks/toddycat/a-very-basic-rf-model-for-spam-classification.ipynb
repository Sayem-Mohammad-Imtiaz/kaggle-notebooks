{"cells":[{"metadata":{"_cell_guid":"7f0a4357-d20f-4af6-be89-e431bca62e2d","_uuid":"d9539bcfd045e0135870c14133d83c7a5016506d"},"cell_type":"markdown","source":"**Importing Libraries**","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2baa567eb4d808bb08ce080bf61266724df82f4a"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom collections import OrderedDict\nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23b30a44a2865f105d1286eb9310dc99ef5e0af9"},"cell_type":"markdown","source":"**     Import Data**","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"96ebd2136ac92aa3c2859060529520657a482908"},"cell_type":"code","source":"df = pd.read_csv(\"../input/spam.csv\",encoding='latin-1')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9627956a4eea3216f3603b28baea54508db0232"},"cell_type":"markdown","source":"Lets rename the columns and drop the unwanted ones","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"f3b58defd85fc71817c6fd089dc1a32dfb50be6d"},"cell_type":"code","source":"del(df[\"Unnamed: 2\"])\ndel(df[\"Unnamed: 3\"])\ndel(df[\"Unnamed: 4\"])\ndf = df.rename(columns = {\"v1\" : \"Label\", \"v2\" : \"Message\" })\ndf['Flag'] = df.Label.map({'ham':0, 'spam':1})\ndf = df.drop(['Label'],axis =1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66aa88b74a2e150e88508c5de4afd135340d3f64"},"cell_type":"markdown","source":"Lets check the count of Messages which Flagged as Spam or Ham","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"2a7e6f9c94bd5742349004f12bd49ab1b1bc35cf"},"cell_type":"code","source":"Count = pd.value_counts(df['Flag'], sort = True).sort_index()\nCount.plot(kind = 'bar')\nplt.title(\"Spam - Ham histogram\")\nplt.xlabel(\"Flag\")\nplt.ylabel(\"Frequency\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9cccbbf8a0820e40b21c6dd690aa7f1f79fc39dc"},"cell_type":"markdown","source":"You can see that the data is imbalanced with count of 0s = 4825 and 1s = 745.  We can deal with imbalanced data using various techniques like undersampling,oversampling,SMOTE etc. I am going with Undersampling here.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"93ceb3c431e9a4b474bff461a00466007b04ec92"},"cell_type":"code","source":"data_X = df.loc[:, df.columns != 'Flag']\ndata_Y = df.loc[:,df.columns == 'Flag']\n\nnumberofrecords_spam = len(df[df.Flag == 1])\nspam_indices = np.array(df[df.Flag == 1].index)\n\nnotspam_indices = df[df.Flag == 0].index\n\nrandom_notspam_indices = np.random.choice(notspam_indices,numberofrecords_spam,replace = False)\nrandom_notspam_indices = np.array(random_notspam_indices)\n\nunder_sample_indices = np.concatenate([spam_indices,random_notspam_indices])\n\nunder_sample_data = df.iloc[under_sample_indices,:]\n\nX_undersample = under_sample_data.loc[:,df.columns != 'Flag']\nY_undersample = under_sample_data.loc[:,df.columns == 'Flag']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b3d6ef9cfb31c8000e708f7e0b08d99b1e85f182"},"cell_type":"code","source":"print(\"Number of spam messages: \" , len(under_sample_data[under_sample_data.Flag == 1]))\nprint(\"Number of ham messages : \" , len(under_sample_data[under_sample_data.Flag == 0]))\nprint(\"Total messages : \", len(under_sample_data))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afac64a7337f040108a7b8b103bac461d5301620"},"cell_type":"markdown","source":"I have chosen here 1:1 ratio for my undersampling.  Now we can split the data into train and test.\n","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"d3eba5454d44a4d5a8dfe17f6e45932d704b0079"},"cell_type":"code","source":"X_train,X_test,Y_train,Y_test = train_test_split(data_X['Message'],data_Y,test_size = 0.3, random_state = 0)\n\nprint(\"Number of datapoints in training : \" , len(X_train))\nprint(\"Number of datapoints in testing : \", len(X_test))\n\nX_train_undersample,X_test_undersample,Y_train_undersample,Y_test_undersample = train_test_split(X_undersample['Message'],Y_undersample,test_size = 0.3, random_state = 0)\n\nprint(\"Number of datapoints in Undersampled training data : \" , len(X_train_undersample))\nprint(\"Number of datapoints in Undersampled testing data : \", len(X_test_undersample))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15293539ef657c4522fb53559edcdad24b343135"},"cell_type":"markdown","source":"Now lets extract the features from the text. Please note for simplicity i am not performing any stop words removal, word normalization,standardization etc and i am converting the features into vectors you can also do TF-IDF.","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"95ff197b8c2ac02c0651eb328ec415dd69801758"},"cell_type":"code","source":"vect = CountVectorizer()\nvect.fit(X_train)\n\n#Print first five features\nprint(vect.get_feature_names()[0:5])\n\nX_train_csr = vect.transform(X_train)\nX_test_csr = vect.transform(X_test)\nX_train_undersample_csr = vect.transform(X_train_undersample)\nX_test_undersample_csr = vect.transform(X_test_undersample)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1327b5827389a4ccad9e80056a34a26ac85d6558"},"cell_type":"markdown","source":"We created training and test data. Now lets run a basic RF model. Here i am tuning only Number of trees and number of features selected for the tree and going to use OOB score for tuning. Lets see how it turns out with the undersampled data. ","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"ac736b45e6c6d911c4e62365f6c424585e65fb94"},"cell_type":"code","source":"RANDOM_STATE = 123\n\n\nensemble_clfs = [\n    (\"RandomForestClassifier, max_features='sqrt'\",\n        RandomForestClassifier(warm_start=True, oob_score=True,\n                               max_features=\"sqrt\",\n                               random_state=RANDOM_STATE)),\n    (\"RandomForestClassifier, max_features='log2'\",\n        RandomForestClassifier(warm_start=True, max_features='log2',\n                               oob_score=True,\n                               random_state=RANDOM_STATE)),\n    (\"RandomForestClassifier, max_features=None\",\n        RandomForestClassifier(warm_start=True, max_features=None,\n                               oob_score=True,\n                               random_state=RANDOM_STATE))\n]\n\n#Map a classifier name to a list of pairs\nerror_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n\n#Range of `n_estimators` values to explore.\nmin_estimators = 15\nmax_estimators = 175\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfor label, clf in ensemble_clfs:\n    for i in range(min_estimators, max_estimators + 1):\n        clf.set_params(n_estimators=i)\n        clf.fit(X_train_undersample_csr, Y_train_undersample.values.ravel())\n\n        #Record the OOB error for each `n_estimators=i` setting.\n        oob_error = 1 - clf.oob_score_\n        error_rate[label].append((i, oob_error))\n\n#Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\nfor label, clf_err in error_rate.items():\n    xs, ys = zip(*clf_err)\n    plt.plot(xs, ys, label=label)\n\nplt.xlim(min_estimators, max_estimators)\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"OOB error rate\")\nplt.legend(loc=\"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49ad866934d5a1fdbee31b8cbb40c93e1911388b"},"cell_type":"markdown","source":"You can see that from the plot the error rate is minimum for RF model with close to 155 trees and when features are selected using Square root. Since we are looking at very basic RF model and RF works on bootstrap sampling i am going with these parameters for final model. Lets test it on our Undersampled test data.","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"abda241b16b12eff6214092b38892aef9cc02220"},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators= 155,max_features= 'sqrt',oob_score=True)\nclf.fit(X_test_undersample_csr,Y_test_undersample)\nprint(\"OOB Score for undersampled test data = \",clf.oob_score_)\n\n#Lets check the F1 score for the model created using undersampled training data on undersampled test data\nrf = RandomForestClassifier(n_estimators= 155, max_features= 'sqrt',oob_score= True)\nrf.fit(X_train_undersample_csr,Y_train_undersample)\nY_pred = rf.predict(X_test_undersample_csr)\nprint('F1 score for the undersampled test data = ', f1_score(Y_test_undersample,Y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1a9095f959008934786b2bbb9140d3b0c762c58"},"cell_type":"markdown","source":"OOB score and F1 scores looks good. Now lets check how our model performs on the whole test data","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"8e9873f559b1fc94fba7863af735691bee28ac38"},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators= 155,max_features= 'sqrt',oob_score=True)\nclf.fit(X_test_csr,Y_test)\nprint(\"OOB Score for test data = \",clf.oob_score_)\n\n#Lets check the F1 score for the model created using undersampled training data on undersampled test data\nrf = RandomForestClassifier(n_estimators= 155, max_features= 'sqrt',oob_score= True)\nrf.fit(X_train_undersample_csr,Y_train_undersample)\nY_pred = rf.predict(X_test_csr)\nprint('F1 score for the undersampled test data = ', f1_score(Y_test,Y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4a7f24512e9ca09a56014d5b045dc7a52e7115c"},"cell_type":"markdown","source":"OOB score and F1 score looks really good when tested on our test data. \n\nThis is a very basic model using RF you can imporve this model by using basic nlp techniques when preparing the data and also tuning the other parameters for RF and also checking other classification models. \n\nThank you\n\n\n\n\n\nnote: some parts of the code were referred from scikit learn library and kernel of joparga3 :).\n","outputs":[],"execution_count":null}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}