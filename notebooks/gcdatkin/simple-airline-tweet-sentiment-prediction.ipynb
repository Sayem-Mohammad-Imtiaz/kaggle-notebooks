{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Task for Today  \n\n***\n\n## Airline Tweet Sentiment Prediction  \n\nGiven *tweets about airline experiences*, let's try to classify the **sentiment** of a given tweet (positive/neutral/negative).  \n  \nWe will use a TensorFlow ANN (with a GRU) to make our predictions."},{"metadata":{},"cell_type":"markdown","source":"# Getting Started"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport re\nimport emoji\nfrom nltk.stem import PorterStemmer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/twitter-airline-sentiment/Tweets.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"confidence_threshold = 0.6\n\ndata = data.drop(data.query(\"airline_sentiment_confidence < @confidence_threshold\").index, axis=0).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df = pd.concat([data['text'], data['airline_sentiment']], axis=1)\ntweets_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df.isna().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df['airline_sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_ordering = ['negative', 'neutral', 'positive']\n\ntweets_df['airline_sentiment'] = tweets_df['airline_sentiment'].apply(lambda x: sentiment_ordering.index(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emoji.demojize('@AmericanAir right on cue with the delaysðŸ‘Œ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ps = PorterStemmer()\n\ndef process_tweet(tweet):\n    new_tweet = tweet.lower()\n    new_tweet = re.sub(r'@\\w+', '', new_tweet) # Remove @s\n    new_tweet = re.sub(r'#', '', new_tweet) # Remove hashtags\n    new_tweet = re.sub(r':', ' ', emoji.demojize(new_tweet)) # Turn emojis into words\n    new_tweet = re.sub(r'http\\S+', '',new_tweet) # Remove URLs\n    new_tweet = re.sub(r'\\$\\S+', 'dollar', new_tweet) # Change dollar amounts to dollar\n    new_tweet = re.sub(r'[^a-z0-9\\s]', '', new_tweet) # Remove punctuation\n    new_tweet = re.sub(r'[0-9]+', 'number', new_tweet) # Change number values to number\n    new_tweet = new_tweet.split(\" \")\n    new_tweet = list(map(lambda x: ps.stem(x), new_tweet)) # Stemming the words\n    new_tweet = list(map(lambda x: x.strip(), new_tweet)) # Stripping whitespace from the words\n    if '' in new_tweet:\n        new_tweet.remove('')\n    return new_tweet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets = tweets_df['text'].apply(process_tweet)\n\nlabels = np.array(tweets_df['airline_sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get size of vocabulary\nvocabulary = set()\n\nfor tweet in tweets:\n    for word in tweet:\n        if word not in vocabulary:\n            vocabulary.add(word)\n\nvocab_length = len(vocabulary)\n\n# Get max length of a sequence\nmax_seq_length = 0\n\nfor tweet in tweets:\n    if len(tweet) > max_seq_length:\n        max_seq_length = len(tweet)\n\n# Print results\nprint(\"Vocab length:\", vocab_length)\nprint(\"Max sequence length:\", max_seq_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=vocab_length)\ntokenizer.fit_on_texts(tweets)\n\nsequences = tokenizer.texts_to_sequences(tweets)\n\nword_index = tokenizer.word_index\n\nmodel_inputs = pad_sequences(sequences, maxlen=max_seq_length, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_inputs.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(model_inputs, labels, train_size=0.7, random_state=22)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 32\n\n\ninputs = tf.keras.Input(shape=(max_seq_length,))\n\nembedding = tf.keras.layers.Embedding(\n    input_dim=vocab_length,\n    output_dim=embedding_dim,\n    input_length=max_seq_length\n)(inputs)\n\n\n# Model A (just a Flatten layer)\nflatten = tf.keras.layers.Flatten()(embedding)\n\n# Model B (GRU with a Flatten layer)\ngru = tf.keras.layers.GRU(units=embedding_dim)(embedding)\ngru_flatten = tf.keras.layers.Flatten()(gru)\n\n# Both A and B are fed into the output\nconcat = tf.keras.layers.concatenate([flatten, gru_flatten])\n\noutputs = tf.keras.layers.Dense(3, activation='softmax')(concat)\n\n\nmodel = tf.keras.Model(inputs, outputs)\n\ntf.keras.utils.plot_model(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n\nbatch_size = 32\nepochs = 100\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    validation_split=0.2,\n    batch_size=batch_size,\n    epochs=epochs,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau()\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps://youtu.be/-2NNe6qiIg8"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}