{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Task for Today  \n\n***\n\n## Automobile Accident Severity Prediction  \n\nGiven *data about accidents in the US*, let's try to predict the **severity** of a given accident.  \n  \nWe will use a TensorFlow ANN to make our predictions."},{"metadata":{},"cell_type":"markdown","source":"# Getting Started"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/us-accidents/US_Accidents_June20.csv', nrows=400000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_columns = ['End_Lat', 'End_Lng', 'Number', 'Wind_Chill(F)', 'Precipitation(in)']\n\ndata = data.drop(null_columns, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.dropna(axis=0).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total missing values:\", data.isna().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Unnecessary Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"{column: len(data[column].unique()) for column in data.columns if data.dtypes[column] == 'object'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unneeded_columns = ['ID', 'Description', 'Street', 'City', 'Zipcode', 'Country']\n\ndata = data.drop(unneeded_columns, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_years(df, column):\n    return df[column].apply(lambda date: date[0:4])\n\ndef get_months(df, column):\n    return df[column].apply(lambda date: date[5:7])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Start_Time_Month'] = get_months(data, 'Start_Time')\ndata['Start_Time_Year'] = get_years(data, 'Start_Time')\n\ndata['End_Time_Month'] = get_months(data, 'End_Time')\ndata['End_Time_Year'] = get_years(data, 'End_Time')\n\ndata['Weather_Timestamp_Month'] = get_months(data, 'Weather_Timestamp')\ndata['Weather_Timestamp_Year'] = get_years(data, 'Weather_Timestamp')\n\n\ndata = data.drop(['Start_Time', 'End_Time', 'Weather_Timestamp'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"def onehot_encode(df, columns, prefixes):\n    df = df.copy()\n    for column, prefix in zip(columns, prefixes):\n        dummies = pd.get_dummies(df[column], prefix=prefix)\n        df = pd.concat([df, dummies], axis=1)\n        df = df.drop(column, axis=1)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"{column: len(data[column].unique()) for column in data.columns if data.dtypes[column] == 'object'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = onehot_encode(\n    data,\n    columns=['Side', 'County', 'State', 'Timezone', 'Airport_Code', 'Wind_Direction', 'Weather_Condition'],\n    prefixes=['SI', 'CO', 'ST', 'TZ', 'AC', 'WD', 'WC']\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_binary_column(df, column):\n    if column == 'Source':\n        return df[column].apply(lambda x: 1 if x == 'MapQuest' else 0)\n    else:\n        return df[column].apply(lambda x: 1 if x == 'Day' else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Source'] = get_binary_column(data, 'Source')\n\ndata['Sunrise_Sunset'] = get_binary_column(data, 'Sunrise_Sunset')\ndata['Civil_Twilight'] = get_binary_column(data, 'Civil_Twilight')\ndata['Nautical_Twilight'] = get_binary_column(data, 'Nautical_Twilight')\ndata['Astronomical_Twilight'] = get_binary_column(data, 'Astronomical_Twilight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting/Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data['Severity'].copy()\nX = data.drop('Severity', axis=1).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = y - 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.astype(np.float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\n\nX = scaler.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = tf.keras.Input(shape=(X.shape[1],))\nx = tf.keras.layers.Dense(64, activation='relu')(inputs)\nx = tf.keras.layers.Dense(64, activation='relu')(x)\noutputs = tf.keras.layers.Dense(4, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs, outputs)\n\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nbatch_size = 32\nepochs = 20\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    validation_split=0.2,\n    batch_size=batch_size,\n    epochs=epochs,\n    callbacks=[\n        tf.keras.callbacks.ReduceLROnPlateau(),\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test Accuracy:\", model.evaluate(X_test, y_test, verbose=0)[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps://youtu.be/hB6Wx7HX0c4"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}