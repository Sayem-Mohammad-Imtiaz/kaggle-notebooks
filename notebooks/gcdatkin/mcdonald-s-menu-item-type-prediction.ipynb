{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Task for Today  \n\n***\n\n## McDonald's Menu Item Type Prediction  \n\nGiven *data about McDonald's menu items*, let's try to predict the **type** of a given item.  \n  \nWe will use a TensorFlow ANN with two inputs to make our predictions."},{"metadata":{},"cell_type":"markdown","source":"# Getting Started"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport re\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/nutrition-facts/menu.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoding Label Column"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Category'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder = LabelEncoder()\n\ndata['Category'] = label_encoder.fit_transform(data['Category'])\n\nlabel_mappings = dict(enumerate(label_encoder.classes_))\nlabel_mappings","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoding Item Column"},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = data['Item'].copy()\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(names)\n\nnames = tokenizer.texts_to_sequences(names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_length = len(tokenizer.word_index) + 1\n\nmax_seq_length = np.max(list(map(lambda x: len(x), names)))\n\nprint(\"Vocabulary length:\", vocab_length)\nprint(\"Max sequence length:\", max_seq_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = pad_sequences(names, maxlen=max_seq_length, padding='post')\nnames","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop('Item', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning Serving Size Feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Serving Size']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Serving Size'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"units = []\n\ndef get_grams(serving):\n    units.append(0)\n    return np.float(re.search(r'(?<=\\()[\\d]+', serving).group(0))\n\ndef get_ml(serving):\n    units.append(1)\n    return np.float(re.search(r'(?<=\\()[\\d]+', serving).group(0))\n\ndef get_fl_oz(serving):\n    units.append(2)\n    return np.float(re.search(r'^[\\d.]+', serving).group(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_units(serving):\n    if ' g)' in serving:\n        return get_grams(serving)\n    \n    elif ' ml)' in serving:\n        return get_ml(serving)\n    \n    else:\n        return get_fl_oz(serving)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Serving Size'] = data['Serving Size'].apply(get_units)\ndata['Serving Units'] = units","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def onehot_encode(df, column, prefix):\n    df = df.copy()\n    dummies = pd.get_dummies(df[column], prefix=prefix)\n    df = pd.concat([df, dummies], axis=1)\n    df = df.drop(column, axis=1)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = onehot_encode(data, 'Serving Units', 'units')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting/Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data['Category'].copy()\nX = data.drop('Category', axis=1).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\n\nX = scaler.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names_train, names_test, X_train, X_test, y_train, y_test = train_test_split(names, X, y, train_size=0.7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = len(y.unique())\nprint(\"Number of classes:\", num_classes)\n\nname_feature_length = names.shape[1]\nprint(\"Name feature length:\", name_feature_length)\n\nother_feature_length = X.shape[1]\nprint(\"Other feature length:\", other_feature_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Name features\nname_input = tf.keras.Input(shape=(name_feature_length,), name=\"name_input\")\n\nname_embedding = tf.keras.layers.Embedding(\n    input_dim=vocab_length,\n    output_dim=64,\n    input_length=name_feature_length,\n    name=\"name_embedding\"\n)(name_input)\n\nname_flatten = tf.keras.layers.Flatten(name=\"name_flatten\")(name_embedding)\n\n\n# Other features\nother_input = tf.keras.Input(shape=(other_feature_length,), name=\"other_input\")\n\ndense_1 = tf.keras.layers.Dense(64, activation='relu', name=\"dense_1\")(other_input)\ndense_2 = tf.keras.layers.Dense(64, activation='relu', name=\"dense_2\")(dense_1)\n\n\n# Combined\nconcat = tf.keras.layers.concatenate([name_flatten, dense_2], name=\"concatenate\")\n\noutputs = tf.keras.layers.Dense(num_classes, activation='softmax', name=\"output_layer\")(concat)\n\n\n# Create model\nmodel = tf.keras.Model(inputs=[name_input, other_input], outputs=outputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.summary())\ntf.keras.utils.plot_model(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nepochs = 100\n\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    [names_train, X_train],\n    y_train,\n    validation_split=0.2,\n    batch_size=batch_size,\n    epochs=epochs,\n    callbacks=[\n        tf.keras.callbacks.ReduceLROnPlateau()\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.evaluate([names_test, X_test], y_test, verbose=0)\n\nprint(\"Model Accuracy:\", results[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps://youtu.be/YFvFoTZLKlA"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}