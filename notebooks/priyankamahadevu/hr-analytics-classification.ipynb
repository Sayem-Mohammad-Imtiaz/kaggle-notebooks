{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import packages\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport scipy.stats as stats\nfrom matplotlib import pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read .csv file\ndf = pd.read_csv('/kaggle/input/hr-analytics/HR_comma_sep.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Exploration and Visualization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Employees left\nleft = df[df.left==1]\nleft.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Employees retained\nretained = df[df.left==0]\nretained.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Mean of features groupby Left**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('left').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insights from above:\n\nThe \"satisfaction_level\" seems to be relatively low (0.44) in employees leaving firms compared to retained ones (0.66). The \"Avergae_monthly_hours\" spent by employees leaving the firm is higher compared to retained ones. The \"Promotion_last_5years\" employees who received promotion are likey to be retained in the firm.\n"},{"metadata":{},"cell_type":"markdown","source":"**Impact of salries on employee retention**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df.salary,df.left).plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot we could observe that employees with hgh salary are likey to retain in the firm."},{"metadata":{},"cell_type":"markdown","source":"**Corelation between department and employee retention**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df.Department, df.left).plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plot we could see there is slight a employee retained has slight correlation with department but its not major."},{"metadata":{},"cell_type":"markdown","source":"**Check for correlation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.corr(), xticklabels=df.corr().columns, yticklabels=df.corr().columns, \n            annot=True, linewidth=4.8, cmap=\"autumn\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the correlation check we could observe that fetaures last_evaluation, number_project, work_accident features has weak correlation with 'left'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating new dataframe with features useful for building model\ndf2 = df[['satisfaction_level', 'average_montly_hours', 'promotion_last_5years', 'salary']]\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating (n-1) dummy variables for Salary categorical feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"dummies = pd.get_dummies(df.salary, prefix=\"salary\")\ndf3 = pd.concat([df2,dummies], axis=1)\ndf3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping categorical feature 'salary' as dummies are alreday being created.\n# Dropping 'salary_high' for to avoid multicollinerity\ndf4 = df3.drop(['salary', 'salary_high'], axis='columns')\ndf4.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Building Logistic Regression Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df4\ny = df.left","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nreg = LogisticRegression()\nreg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Accuracy of a Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.score(X_test,y_test)\nprint(\"Accuracy on Training set: \",reg.score(X_train,y_train))\nprint(\"Accuracy on Testing set: \",reg.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression Model Error Table"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\ny_pred = reg.predict(X_test)\nprint(\"\\t\\tError Table\")\nprint('Mean Absolute Error     : ', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error      : ', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error : ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint('R Squared Error         : ', metrics.r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Building Decision Tree classifier Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\nmodeltree = tree.DecisionTreeClassifier()\nmodeltree.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Model Accuracy on train data:', modeltree.score(X_train,y_train))\nprint('Model Accuracy on test data :', modeltree.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = modeltree.predict(X_test)\nprint(\"\\t\\tError Table\")\nprint('Mean Absolute Error     : ', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error      : ', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error : ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint('R Squared Error         : ', metrics.r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest Classifier Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\nrandommodel = RandomForestClassifier()\nrandommodel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Random Model Accuracy on train data:', randommodel.score(X_train,y_train))\nprint('Random model Accuracy on test data: ', randommodel.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = randommodel.predict(X_test)\nprint('\\t\\tError Table')\nprint('Mean Absolute Error       :', metrics.mean_absolute_error(y_test,y_pred))\nprint('Mean Squared Error        :', metrics.mean_squared_error(y_test,y_pred))\nprint('Root Mean Squared Error   :', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\nprint('Mean Absolute Error       :', metrics.r2_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Area Under-Receiving Operating Characteristic Curve Evaluation Metric**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\n\n# Getting predicted probabilities\ny_score1 = reg.predict_proba(X_test)[:,1]\ny_score2 = modeltree.predict_proba(X_test)[:,1]\ny_score3 = randommodel.predict_proba(X_test)[:,1]\n# Creating true and false positive rate\nfalse_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_test, y_score1)\nfalse_positive_rate2, true_positive_rate2, threshold2 = roc_curve(y_test, y_score2)\nfalse_positive_rate3, true_positive_rate3, threshold3 = roc_curve(y_test, y_score3)\n\nreg_roc_auc    = roc_auc_score(y_test, y_score1)\ntree_roc_auc   = roc_auc_score(y_test, y_score2)\nrandom_roc_auc = roc_auc_score(y_test, y_score3)\n\nprint('roc_auc_score for Logistic Regression: ', reg_roc_auc)\nprint('roc_auc_score for DecisionTree: ', tree_roc_auc)\nprint('roc_auc_score for RandomForest: ', random_roc_auc)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Ploting ROC Curve**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9, 6))\n\n# Plot Logistic Regression ROC\nplt.plot(false_positive_rate1,true_positive_rate1,linestyle= '--',label='Logistic Regression(area = %0.3f)'\n         % reg_roc_auc)\n\n# Plot Decision Tree ROC\nplt.plot(false_positive_rate2,true_positive_rate2,linestyle= '--',label='Decision Tree (area = %0.3f)'\n         % tree_roc_auc)\n\n# Plot Random Forest ROC\nplt.plot(false_positive_rate3,true_positive_rate3,linestyle= '--',label='Random Forest (area = %0.3f)'\n         % random_roc_auc)\n\n# Plot Base Rate ROC\nplt.plot([0,1], [0,1],linestyle= '--',label='Base Rate')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Area Under-Receiving Operating Characteristic Graph')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**\n\nThe higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.\n\nIt is evident from the plot that the AUC for the Decision Tree ROC curve(area=0.977) is higher than that for the Logistic(area=0.775). But almost equal to Random Forest ROC curves(area=0.966). \n\nTherefore, we can say that Decision Tree did a better job of classifying the positive class in the dataset.\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}