{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TEXT CLASSIFICATION USING PYTORCH \n\n- As most of the users are prefering pytorch over tensorflow and I am also finding pytorch more of like a maths that we learn as basic of machine learning. In tensorflow the beauty of Maths is abstract, So I am here trying to understand/Learn pytorch as much possible as I can. I am mostly working on TEXTUAL data. As my area of focus is NLP. \n\n- In this tutorial I am gonna build a text classfier using Linear Model. \n- I am also gonna keep updating in future notebooks . \n- I hope this will help anyone who's trying to use/ learn pytorch. \n\n\n","metadata":{}},{"cell_type":"markdown","source":"![PytorchGIF](https://images.ctfassets.net/rc8q7tcpu9y3/7JrqmBOgQcEQWGuKEiSuow/4f2e59badb8cecb518fb38752feebf61/Facebook-PyTorch-Conference-Experience-Design-Brand-Logo-Construction.gif)\nsource : https://images.ctfassets.net/rc8q7tcpu9y3/7JrqmBOgQcEQWGuKEiSuow/4f2e59badb8cecb518fb38752feebf61/Facebook-PyTorch-Conference-Experience-Design-Brand-Logo-Construction.gif","metadata":{}},{"cell_type":"markdown","source":"### What is torch?\nThe torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities.(copied from documentation) ","metadata":{}},{"cell_type":"code","source":"# importing torch \n# nn is a module in pytorch package which helps and provide us various \nimport torch\nimport torch.nn as nn \nimport numpy as np \nimport pandas as pd \n\n# train_test_split\n# TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# scipy to convert .. sparse matrix to dense \nimport scipy \n# improting optimizers \nfrom torch import optim\n\n# for ploting loss and train& val prediction accuracy \nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\ntorch.__version__","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's look in data\n\n- here we got the data in two different files. so I used df1 & df2 to import the data \n- then I used pandas concat() function doing axis=0 so the rows will get add from second dataset(df2)\n- deleted the df1 and df2 \n","metadata":{}},{"cell_type":"markdown","source":"\n <img align=centre src = \"https://i.pinimg.com/originals/28/18/74/2818749911c12b7f854d45e250c0b6d1.gif\" width=\"1000\" height=\"700\">\n\n","metadata":{}},{"cell_type":"code","source":"df1 = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')\ndf2 = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv')\ndf1['label'] = 1\ndf2['label'] = 0\ndf = pd.concat([df1, df2], axis=0)\n\ndel df1 \ndel df2\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Shape of the dataset: {df.shape}')\nprint(f'\\nSum of nulls:\\n{df.isna().sum()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text preprocessing \n","metadata":{}},{"cell_type":"code","source":"# Text preprocessing \n\ndef normalise_text(text):\n    \n    text = text.str.lower()\n    text = text.str.replace(r\"\\#\", \"\" )\n    text = text.str.replace(r'http\\S+', \"URL\")\n    text = text.str.replace(r\"@\", \"\")\n    text = text.str.replace(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\",\" \")\n    text = text.str.replace(\"\\s{2,}\",\" \")\n    \n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']= df['title']+\" \"+df['text']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = normalise_text(df['text'])\ndel df['title']\ndel df['subject']\ndel df['date']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Test Splitting","metadata":{}},{"cell_type":"code","source":"# let's split the test and train \nX_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating vectors for the sentence \nvectorizer = TfidfVectorizer(ngram_range=(1,1), max_features=10000)\n\n# Learn vocabulary from training texts and vectorize training texts.\nX_train = vectorizer.fit_transform(X_train)\n\n# Vectorize test texts.\nX_test = vectorizer.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CONVERTING INTO TENSORS \n- Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the modelâ€™s parameters.\n\n- Tensors are similar to NumPyâ€™s ndarrays, except that tensors can run on GPUs or other specialized hardware to accelerate computing. If youâ€™re familiar with ndarrays, youâ€™ll be right at home with the Tensor API. If not, follow along in this quick API walkthrough.","metadata":{}},{"cell_type":"code","source":"#X_train = torch.tensor(scipy.sparse.csr_matrix.todense(X_train)).float()\n#X_test = torch.tensor(scipy.sparse.csr_matrix.todense(X_test)).float()\n\nX_train = torch.tensor(X_train.todense()).float()\nX_test = torch.tensor(X_test.todense()).float()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = torch.tensor(y_train.values)\ny_test = torch.tensor(y_test.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building Model ","metadata":{}},{"cell_type":"code","source":"model = nn.Sequential(\n                nn.Linear(X_train.shape[1],128 ),\n                nn.ReLU(),\n                nn.Dropout(0.1),\n                nn.Linear(128, df['label'].nunique()),\n                nn.LogSoftmax(dim=1)\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss, Optimzer","metadata":{}},{"cell_type":"code","source":"# defining the loss \ncriterion = nn.NLLLoss()\n\n# Forward pass, get our logits\nlogps = model(X_train)\n\n# Calculate the loss with the logits and the labels\nloss = criterion(logps, y_train)\n\n\nloss.backward()\n\n# Optimizers require the parameters to optimize and a learning rate\noptimizer = optim.Adam(model.parameters(), lr=0.002)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"%%time\ntrain_losses = []\ntest_losses = []\ntest_accuracies = []\n\nepochs = 100\nfor e in range(epochs):\n    optimizer.zero_grad()\n\n    output = model.forward(X_train)\n    loss = criterion(output, y_train)\n    loss.backward()\n    train_loss = loss.item()\n    train_losses.append(train_loss)\n    \n    optimizer.step()\n    \n    \n    # Turn off gradients for validation, saves memory and computations\n    with torch.no_grad():\n        model.eval()\n        log_ps = model(X_test)\n        test_loss = criterion(log_ps, y_test)\n        test_losses.append(test_loss)\n\n        ps = torch.exp(log_ps)\n        top_p, top_class = ps.topk(1, dim=1)\n        equals = top_class == y_test.view(*top_class.shape)\n        test_accuracy = torch.mean(equals.float())\n        test_accuracies.append(test_accuracy)\n\n    model.train()\n    if (e+1)%10==0:\n        print(f\"Epoch: {e+1}/{epochs}.. \",\n              f\"Training Loss: {train_loss:.3f}.. \",\n              f\"Test Loss: {test_loss:.3f}.. \",\n              f\"Test Accuracy: {test_accuracy:.3f}\")\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's Plot the Learning and Loss Curve","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nax = plt.subplot(121)\nplt.xlabel('epochs')\nplt.ylabel('negative log likelihood loss')\nplt.plot(train_losses, label='Training loss')\nplt.plot(test_losses, label='Validation loss')\nplt.legend(frameon=False);\nplt.subplot(122)\nplt.xlabel('epochs')\nplt.ylabel('test accuracy')\nplt.plot(test_accuracies);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ˜ŽðŸ˜ŽDO UPVOTE FOR CHANDLERðŸ˜ŽðŸ˜Ž \n![Chandler](https://media.tenor.com/images/b7b886de0e04e771361f730414f52919/tenor.gif)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}