{"cells":[{"metadata":{},"cell_type":"markdown","source":"# SENTIMENT ANALYSIS \n\nIn this hands-on project, we will train a Naive Bayes classifier and logistic regression to predict sentiment from thousands of Twitter tweets. This project could be practically used by any company with social media presence to automatically predict customer's sentiment (i.e.: whether their customers are happy or not). The process could be done automatically without having humans manually review thousands of tweets and customer reviews."},{"metadata":{},"cell_type":"markdown","source":"**Now we will perform Twitter sentiment analysis**\n* Perform exploratory data analysis and plot word-cloud\n* Apply python libraries to import and visualize dataset\n* Evaluate the performance of trained Naïve Bayes Classifier model using confusion matrices.\n* Train Naïve Bayes classifier models using Scikit-Learn to preform classification\n* Understand the difference between prior probability, posterior probability and likelihood.\n* Understand the theory and intuition behind Naïve Bayes classifiers\n* Perform tokenization to tweet text using Scikit Learn\n* Understand the concept of count vectorization (tokenization)\n* Perform text data cleaning such as removing punctuation and stop words"},{"metadata":{},"cell_type":"markdown","source":"#  UNDERSTAND THE PROBLEM STATEMENT AND BUSINESS CASE\n\nTwitter is one of the platforms widely used by people to express their opinions and showcase sentiments on various occasions. Sentiment analysis is an approach to analyze data and retrieve sentiment that it embodies.\nThe tweet format is very small, which generates a whole new dimension of problems like the use of slang, abbreviations, etc. This article reports on the exploration and preprocessing of data, transforming data into a proper input format and classify the user’s perspective via tweets into positive(non-racist) and negative (racist) by building supervised learning models using Python and NLTK library."},{"metadata":{},"cell_type":"markdown","source":"# IMPORT LIBRARIES AND DATASETS"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import seaborn as sns \nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_df = pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/train.csv')\ntwitter_test = pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EXPLORE DATASET"},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_df.describe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_df = twitter_df.drop(['id'],axis = 1)\ntwitter_test = twitter_test.drop(['id'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(twitter_df.isnull(),yticklabels = False,cbar = False , cmap = \"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I don't find any null values "},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_df.hist(bins = 40,figsize = (14,5),color = 'r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(twitter_df['label'],label = 'count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_df['length'] = twitter_df['tweet'].apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_df['length'].plot(bins = 100,kind = 'hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_df.describe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_df[twitter_df['length']==11]['tweet'].iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_df[twitter_df['length']==84]['tweet'].iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"positive = twitter_df[twitter_df['label']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative = twitter_df[twitter_df['label']==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"positive","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = twitter_df['tweet'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences_as_one_string = \" \".join(sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install Wordcloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install WordCloud","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PLOT THE WORDCLOUD"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd \n%pylab\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nplt.figure(figsize(20,20))\nplt.imshow(WordCloud().generate(sentences_as_one_string))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PERFORM DATA CLEANING - REMOVE PUNCTUATION FROM TEXT"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nstring.punctuation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test = 'Good morning beautiful people :)... I am having fun learning Machine learning and AI!!'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_punc_rem = [char for char in Test if char not in string.punctuation]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_punc_rem","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_punc_rem_join = ''.join(Test_punc_rem)\nTest_punc_rem_join","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PERFORM DATA CLEANING - REMOVE STOPWORDS"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk # Natural Language tool kit \n\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk # Natural Language tool kit \n\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re                                  # library for regular expression operations\nimport string                              # for string operations\n\nfrom nltk.corpus import stopwords          # module for stop words that come with NLTK\nfrom nltk.stem import PorterStemmer        # module for stemming\nfrom nltk.tokenize import TweetTokenizer ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_punc_rem_join_clean = [word for word in Test_punc_rem_join.split() if word.lower() not in stopwords.words('english')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_punc_rem_join_clean ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PERFORM COUNT VECTORIZATION (TOKENIZATION)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_data = ['This is the first paper.','This document is the second paper.','And this is the third one.','Is this the first paper?']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = vectorizer.fit_transform(sample_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.toarray())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CREATE A PIPELINE TO REMOVE PUNCTUATIONS, STOPWORDS AND PERFORM COUNT VECTORIZATION"},{"metadata":{"trusted":true},"cell_type":"code","source":"def message_cleaning(message):\n    Test_punc_rem = [char for char in Test if char not in string.punctuation]\n    Test_punc_rem_join = ''.join(Test_punc_rem)\n    Test_punc_rem_join_clean = [word for word in Test_punc_rem_join.split() if word.lower() not in stopwords.words('english')]\n    return Test_punc_rem_join_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's test the newly added function\ntwitter_df_clean = twitter_df['tweet'].apply(message_cleaning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(twitter_df_clean[5]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(twitter_df['tweet'][5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n# Define the cleaning pipeline we defined earlier\nvectorizer = CountVectorizer(analyzer = message_cleaning)\ntwitter_countvectorizer = CountVectorizer(analyzer = message_cleaning,dtype = 'uint8').fit_transform(twitter_df['tweet']).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_countvectorizer.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = twitter_countvectorizer\ny = twitter_df['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# UNDERSTAND THE THEORY AND INTUITION BEHIND NAIVE BAYES"},{"metadata":{},"cell_type":"markdown","source":"Bayes’ Theorem is a formula that tells us how to update the probabilities of a hypothesis when given an event occurs. In other words it shows the probability of a hypothesis given an event. The image shown above gives a solid summary of Bayes’ formula and each of the components. I am not going to dive deep into Bayes’ theorem as I want to focus on Naive Bayes but Data Skeptic did a great mini-podcast that explains the intuition behind Bayesian updating. I recommend checking it out before moving on to get a better grasp on the concept.\n\n\n## ****Naive Bayes Classifier****\n\n\nNow that we have an understanding for the Bayesian framework we can move to Naive Bayes. Naive Bayes is a classification algorithm used for binary or multi-class classification. The classification is carried out by calculating the posterior probabilities and finding the hypothesis with the highest probability using MAP. Basically, it is finding the probability of given feature being associated with a label and assigning the label with the highest probability. It is referred to as naive because it assumes all features are independent, which is rarely the case in real life."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Things to Remember\n* Easy to understand and fast to implement\n* Need less training data than logistic regression\n* Performs well for categorical input values\n* “Zero Frequency” or if a categorical variable has a category in the test set that is not present in the training set, the model will assign a 0% probability to this category making it unable to make a prediction. This can be fixed by using a smoothing method such as Laplace estimation. Laplace estimation assigns a small non-zero probability to data not in the train set. This is extremely relevant for text classification. For example if one word does not appear in the train set you do not want the classifier to lower the probability of the entire document to 0.\n* Assumption of independent predictors"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Naive Bayes vs Logistic Regression\nNaive Bayes is often compared to another classification algorithm, Logistic Regression. Logistic Regression is a linear classification model that learn the probability of a sample belonging to a class and tries to find the optimal decision boundary that separates the classes.\nThe main difference between the two is that Naive Bayes is a Generative Model and Logistic Regression is a Discriminative Model. A Generative Model is one that tries to recreate the model that generated the data by estimating the assumptions and distributions of the model. It then uses this to predict the unseen data. For example Naive Bayes models the joint probability of feature X and feature Y and tries to predict the posterior probability based off that model. A Discriminative model is built based only on the observed data and includes less assumptions on the distribution of the data. However, it is very reliant on the quality of the data. For example Logistic Regression directly models posterior probability by learning the input to output mapping by minimizing error."},{"metadata":{},"cell_type":"markdown","source":"# Compare and Contrast\n*  Naive Bayes assumes all features to be independent so if variables are correlated the predictions will be poor. Logistic Regression is better at handling correlation.\n* Naive Bayes works well on small training samples with high dimensionality (given features are independent) as it makes assumptions on prior probabilities. This is why it is commonly used for text classification.\n* Logistic Regression works much better than Naive Bayes on large data sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nNB_classifier = MultinomialNB()\nNB_classifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny_predict_test = NB_classifier.predict(x_test)\ncm = confusion_matrix(y_test,y_predict_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(cm,annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_predict_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.35)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Logistic Regression\nlog_reg_model = LogisticRegression()\nlog_reg_model.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scoring\ntrain_prediction = log_reg_model.predict(x_train)\ntest_prediction = log_reg_model.predict(x_test)\naccuracy_train = accuracy_score(train_prediction, y_train)\naccuracy_test = accuracy_score(test_prediction, y_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Score on training set: {accuracy_train}\")\nprint(f\"Score on test set: {accuracy_test}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Here I got almost same result though there are much more techniques this model for learning purpose ."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}