{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Kensho Derived Wikimedia Data - Explicit Topic Models\n\nThe Kensho Derived Wikimedia Data (KDWD) provides a large English corpus that is also an encyclopedia.  Let's use it to make some explicit topic models."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from collections import Counter\nimport itertools\nimport json\nimport os\nimport random\nimport re\n\nimport joblib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy.sparse\nimport seaborn as sns\nfrom tqdm import tqdm\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('max_colwidth', 160)\npd.set_option('display.max_colwidth', 60)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"sns.set()\nsns.set_context('notebook')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All of the KDWD files have one \"thing\" per line. Below we'll hard code the number of lines in the `link_annotated_text.json` file and specify sections names that we want to skip when creating our topic model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_KLAT_LINES = 5_343_564\nKDWD_PATH = os.path.join(\n    '/kaggle', \n    'input', \n    'kensho-derived-wikimedia-data'\n)\nSKIP_SECTION_NAMES = [\n    'see also', \n    'external links', \n    'further reading', \n    'references', \n    'bibliography'\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each page is potentially a topic.  Let's read in all the page meta-data and then filter it down later."},{"metadata":{"trusted":true},"cell_type":"code","source":"page_df = pd.read_csv(\n    os.path.join(KDWD_PATH, \"page.csv\"),\n    keep_default_na=False, # dont read the page title \"NaN\" as a null\n) \npage_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate in links and out links\nThe first thing we want to do is count the incoming and outgoing links to/from each page. This will help us select pages that will make good topics."},{"metadata":{"trusted":true},"cell_type":"code","source":"class KdwdLinkAnnotatedText:\n    \n    def __init__(self, file_path):\n        self.file_path = file_path\n        self.num_lines = NUM_KLAT_LINES\n\n    def __iter__(self):\n        with open(self.file_path) as fp:\n            for line in tqdm(fp, total=self.num_lines):\n                page = json.loads(line)\n                yield page ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_path = os.path.join(KDWD_PATH, \"link_annotated_text.jsonl\")\nklat = KdwdLinkAnnotatedText(file_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out_links = Counter()\nin_links = Counter()\nfor page in klat:\n    for section in page['sections']:\n        out_links[page['page_id']] += len(section['target_page_ids'])\n        in_links.update(section['target_page_ids'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"page_df = pd.merge(\n    page_df, \n    pd.DataFrame(in_links.most_common(), columns=['page_id', 'in_links']), \n    how='left').fillna(0.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"page_df = pd.merge(\n    page_df, \n    pd.DataFrame(out_links.most_common(), columns=['page_id', 'out_links']), \n    how='left').fillna(0.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"page_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del out_links\ndel in_links","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Parameterize Explicit Topic Models\n\nHere we set the parameters of several topic models.  \n\n## Topic Page Samples\n\nThe `topic_defs` dictionary determines which pages we'll use as topics.  We set thresholds for the minimum number of incoming links, the minimum number of outgoing links, and the minimum number of page views. To change the number of topics in a model we vary the `min_in_links` parameter and keep the other two fixed.\n\n## Topic-Word Distributions\n\nThe `model_defs` dictionary sets parameters related to the way we associate words with the topic pages above.  This will be done with a [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and a [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html).  We expose several keyword arguments for the CountVectorizer here, but we only vary `intros_only` and `ngram_range`.  The first determines if we pass all the words on a page to the CountVectorizer or just those in the Introduction section. The second determines \"the lower and upper boundary of the range of n-values for different word n-grams\".  We also change the defualt CountVectorizer regex pattern used for tokenization from `(?u)\\b\\w\\w+\\b)` to `(?u)\\b[^\\d\\W]{2,25}\\b`.  This excludes digits and non-word characters (see https://docs.python.org/3/library/re.html) and captures tokens with between 2 and 25 characters ([see the analysis of this regex in the amazing regex101 tool](https://regex101.com/r/kYmeS3/2)).\n\n## Models\n\nHere we briefly describe the model variations we use, \n\n * `base_large`: use unigrams from whole page with large topic sample size\n * `base_medium`: use unigrams from whole page with medium topic sample size\n * `base_small`: use unigrams from whole page with small topic sample size\n \n \n * `intros_large`: use unigrams from introduction section with large topic sample size\n * `intros_medium`: use unigrams from introduction section with medium topic sample size\n * `intros_small`: use unigrams from introduction section with small topic sample size\n \n \n * `intros_small_12`: use unigrams and bigrams from introduction section with small topic sample size\n * `intros_small_13`: use unigrams, bigrams, and trigrams from introduction section with small topic sample size"},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_defs = {\n    'large': {\n        'min_in_links': 50,\n        'min_out_links': 20,\n        'min_views': 100,\n    },\n    'medium': {\n        'min_in_links': 500,\n        'min_out_links': 20,\n        'min_views': 100,\n    },\n    'small': {\n        'min_in_links': 5000,\n        'min_out_links': 20,\n        'min_views': 100,\n    },\n}\n\nmodel_defs = {\n    'base_large': {\n        'topics': 'large',\n        'intros_only': False,\n        'min_df': 3,\n        'max_df': 0.98,\n        'ngram_range': (1,1),\n        'stop_words': ENGLISH_STOP_WORDS,\n        'max_features': 1_000_000,\n        'token_pattern': r\"(?u)\\b[^\\d\\W]{2,25}\\b\",\n    },\n    'base_medium': {\n        'topics': 'medium',\n        'intros_only': False,\n        'min_df': 3,\n        'max_df': 0.98,\n        'ngram_range': (1,1),\n        'stop_words': ENGLISH_STOP_WORDS,\n        'max_features': 1_000_000,\n        'token_pattern': r\"(?u)\\b[^\\d\\W]{2,25}\\b\",\n    },\n    'base_small': {\n        'topics': 'small',\n        'intros_only': False,\n        'min_df': 3,\n        'max_df': 0.98,\n        'ngram_range': (1,1),\n        'stop_words': ENGLISH_STOP_WORDS,\n        'max_features': 1_000_000,\n        'token_pattern': r\"(?u)\\b[^\\d\\W]{2,25}\\b\",\n    },    \n    'intros_large': {\n        'topics': 'large',\n        'intros_only': True,\n        'min_df': 3,\n        'max_df': 0.98,\n        'ngram_range': (1,1),\n        'stop_words': ENGLISH_STOP_WORDS,\n        'max_features': 1_000_000,\n        'token_pattern': r\"(?u)\\b[^\\d\\W]{2,25}\\b\",\n    },\n    'intros_medium': {\n        'topics': 'medium',\n        'intros_only': True,\n        'min_df': 3,\n        'max_df': 0.98,\n        'ngram_range': (1,1),\n        'stop_words': ENGLISH_STOP_WORDS,\n        'max_features': 1_000_000,\n        'token_pattern': r\"(?u)\\b[^\\d\\W]{2,25}\\b\",\n    },\n    'intros_small': {\n        'topics': 'small',\n        'intros_only': True,\n        'min_df': 3,\n        'max_df': 0.98,\n        'ngram_range': (1,1),\n        'stop_words': ENGLISH_STOP_WORDS,\n        'max_features': 1_000_000,\n        'token_pattern': r\"(?u)\\b[^\\d\\W]{2,25}\\b\",\n    },\n    'intros_small_12': {\n        'topics': 'small',\n        'intros_only': True,\n        'min_df': 3,\n        'max_df': 0.98,\n        'ngram_range': (1,2),\n        'stop_words': ENGLISH_STOP_WORDS,\n        'max_features': 1_000_000,\n        'token_pattern': r\"(?u)\\b[^\\d\\W]{2,25}\\b\",\n    },\n    'intros_small_13': {\n        'topics': 'small',\n        'intros_only': True,\n        'min_df': 3,\n        'max_df': 0.98,\n        'ngram_range': (1,3),\n        'stop_words': ENGLISH_STOP_WORDS,\n        'max_features': 1_000_000,\n        'token_pattern': r\"(?u)\\b[^\\d\\W]{2,25}\\b\",\n    },\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Topic Page Samples"},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_dfs = {}\nfor model_name, model_params in model_defs.items():\n    if model_params['topics'] in topic_dfs:\n        continue\n    else:\n        topic_params = topic_defs[model_params['topics']]\n        mask1 = page_df['in_links'] >= topic_params['min_in_links']\n        mask2 = page_df['out_links'] >= topic_params['min_out_links']\n        mask3 = page_df['views'] > topic_params['min_views']\n        topic_dfs[model_params['topics']] = page_df[mask1 & mask2 & mask3].copy().reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_dfs['small']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_dfs['medium']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_dfs['large']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Generator\n\nThe function below will iterare through the `link_annotated_text.jsonl` file and yield the topic pages in `topic_df`."},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen_text(klat, topic_df, intros_only):\n    keep_page_ids = set(topic_df['page_id'].values)\n    for page in klat:\n        if page['page_id'] not in keep_page_ids:\n            continue\n        sections = [\n            section for section in page['sections']\n            if (section['name'] is not None) and (section['name'].lower() not in SKIP_SECTION_NAMES)]\n        if intros_only:\n            yield sections[0]['text']\n        else:\n            yield ' '.join([section['text'] for section in sections])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key = 'small'\nnext(iter(gen_text(klat, topic_dfs[key], intros_only=False)))[0:1000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Count Vectorizer Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"cvs = {}\nfor model_name, model_params in model_defs.items():\n    cvs[model_name] = CountVectorizer(\n        min_df=model_params['min_df'],\n        max_df=model_params['max_df'],\n        stop_words=model_params['stop_words'],\n        ngram_range=model_params['ngram_range'],\n        max_features=model_params['max_features'],\n        token_pattern=model_params['token_pattern'],\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Count Vectorizer Matrices"},{"metadata":{"trusted":true},"cell_type":"code","source":"Xcvs = {}\nfor model_name, model_params in model_defs.items():\n    topic_df = topic_dfs[model_params['topics']]\n    Xcvs[model_name] = cvs[model_name].fit_transform(\n        gen_text(klat, topic_df, model_params['intros_only'])\n    )\n    print('model_name={}'.format(model_name), flush=True)\n    print('len(vocabulary_)={}'.format(len(cvs[model_name].vocabulary_)), flush=True)\n    print('(num_docs, num_tokens)={}'.format(Xcvs[model_name].shape), flush=True)\n    print('deleting {} stop words'.format(len(cvs[model_name].stop_words_)), flush=True)\n    del cvs[model_name].stop_words_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TF-IDF Matrices"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidfs = {}\nXtfidfs = {}\nfor model_name, model_params in model_defs.items():\n    print('model_name={}'.format(model_name))\n    tfidfs[model_name] = TfidfTransformer()\n    Xtfidfs[model_name] = tfidfs[model_name].fit_transform(Xcvs[model_name])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Explicit Topic Model Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ExplicitTopicModel:\n    \n    def __init__(self, cv, Xtfidf, topic_df):\n        self.cv = cv\n        self.Xtfidf = Xtfidf\n        self.topic_df = topic_df\n        self.tokenize = cv.build_analyzer()\n        self.feature_names = np.array(cv.get_feature_names())\n        print('creating explicit topic model (topics={}, tokens={})'.format(\n            Xtfidf.shape[0], Xtfidf.shape[1]))\n        \n    def topn_topics_from_text(self, text, topn=10, thresh=0.0):\n        tokens = self.tokenize(text)\n        return self.topn_topics_from_tokens(tokens, topn=topn, thresh=thresh)\n\n    def topn_topics_from_tokens(self, tokens, topn=10, thresh=0.0):\n        topic_vector = self.topic_vec_from_tokens(tokens)\n        return self.topn_topics_from_topic_vec(topic_vector, topn=topn, thresh=thresh)\n\n    def topn_topics_from_topic_vec(self, topic_vector, topn=10, thresh=0.0):\n        topic_indxs = np.argsort(-topic_vector)[:topn]\n        top_topics_df = self.topic_df.iloc[topic_indxs].copy()\n        topic_scores = topic_vector[topic_indxs]\n        top_topics_df['score'] = topic_scores\n        return top_topics_df[top_topics_df['score']>thresh].copy()\n        \n    def topic_vec_from_tokens(self, tokens):\n        token_indices = [\n            self.cv.vocabulary_[token] for token in tokens \n            if token in self.cv.vocabulary_]\n        norm = max(1, len(token_indices))\n        topic_vector = np.array(self.Xtfidf[:, token_indices].sum(axis=1)).squeeze() / norm\n        return topic_vector\n    \n    def explain_topic_for_text(self, text, topic_title):\n        text_tokens = self.tokenize(text)\n        topic_tokens_df = self.topn_tokens_from_topic(topic_title, topn=1000)\n        explanation = topic_tokens_df[topic_tokens_df['token'].isin(text_tokens)]\n        return explanation.sort_values('score', ascending=False)\n    \n    def topn_tokens_from_topic(self, topic_title, topn=10):\n        indx = self.topic_df.index[self.topic_df['title']==topic_title][0]\n        token_vector = self.Xtfidf.getrow(indx).toarray().squeeze()\n        token_indxs = np.argsort(-token_vector)[:topn]\n        tokens = pd.DataFrame(\n            zip(self.feature_names[token_indxs], token_vector[token_indxs]),\n            columns=['token', 'score'])\n        return tokens\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Instances of Explicit Topic Model Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"etms = {}\nfor model_name, model_params in model_defs.items():\n    etms[model_name] = ExplicitTopicModel(\n        cvs[model_name], \n        Xtfidfs[model_name], \n        topic_dfs[model_params['topics']]\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Topic Plotting Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_etm_results(etms, model_defs, text):\n    fig, axes = plt.subplots(4 , 2, figsize=(14, 17), sharex=True)\n    results = pd.DataFrame()\n    for ax, model_name in zip(axes.flatten(), model_defs):\n        etm = etms[model_name]\n        text_topics_df = etm.topn_topics_from_text(text, topn=10)\n        text_topics_df['model'] = model_name\n        results = pd.concat([results, text_topics_df])\n        g = sns.barplot(\n            x='score', y='title', color='orange', alpha=0.7, \n            data=text_topics_df, ax=ax).set_title(model_name)\n    plt.tight_layout()\n    return results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Example 1: Topics"},{"metadata":{"trusted":true},"cell_type":"code","source":"text1 = \"\"\"\nThe canine - which was two months old when it died - has been\nremarkably preserved in the permafrost of the Russian region, with its\nfur, nose and teeth all intact.  DNA sequencing has been unable to determine\nthe species.  Scientists say that could mean the specimen represents an\nevolutionary link between wolves and modern dogs.\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = plot_etm_results(etms, model_defs, text1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Example 1: Explanation"},{"metadata":{"trusted":true},"cell_type":"code","source":"etm = etms['intros_small_12']\ntopics = etm.topn_topics_from_text(text1, topn=10)\nfor topic_title in topics['title']:\n    explanation = etm.explain_topic_for_text(text1, topic_title)\n    print(topic_title)\n    print(explanation)\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Example 2: Topics"},{"metadata":{"trusted":true},"cell_type":"code","source":"text2 = \"\"\"\nU.S. intelligence cannot say conclusively that Saddam Hussein\nhas weapons of mass destruction, an information gap that is complicating\nWhite House efforts to build support for an attack on Saddam's Iraqi regime.\nThe CIA has advised top administration officials to assume that Iraq has\nsome weapons of mass destruction.  But the agency has not given President\nBush a \"smoking gun,\" according to U.S. intelligence and administration\nofficials.\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = plot_etm_results(etms, model_defs, text2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Example 2: Explanation"},{"metadata":{"trusted":true},"cell_type":"code","source":"etm = etms['intros_small_12']\ntopics = etm.topn_topics_from_text(text2, topn=10)\nfor topic_title in topics['title']:\n    explanation = etm.explain_topic_for_text(text2, topic_title)\n    print(topic_title)\n    print(explanation)\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Example 3: Topics"},{"metadata":{"trusted":true},"cell_type":"code","source":"text3 = \"\"\"\nThe development of T-cell leukaemia following the otherwise\nsuccessful treatment of three patients with X-linked severe combined\nimmune deficiency (X-SCID) in gene-therapy trials using haematopoietic\nstem cells has led to a re-evaluation of this approach.  Using a mouse\nmodel for gene therapy of X-SCID, we find that the corrective therapeutic\ngene IL2RG itself can act as a contributor to the genesis of T-cell\nlymphomas, with one-third of animals being affected.  Gene-therapy trials\nfor X-SCID, which have been based on the assumption that IL2RG is minimally\noncogenic, may therefore pose some risk to patients.\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = plot_etm_results(etms, model_defs, text3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Example 3: Explanation"},{"metadata":{"trusted":true},"cell_type":"code","source":"etm = etms['intros_small_12']\ntopics = etm.topn_topics_from_text(text3, topn=10)\nfor topic_title in topics['title']:\n    explanation = etm.explain_topic_for_text(text3, topic_title)\n    print(topic_title)\n    print(explanation)\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Example 4: Topics"},{"metadata":{"trusted":true},"cell_type":"code","source":"text4 = \"\"\"\nShare markets in the US plummeted on Wednesday, with losses accelerating \nafter the World Health Organization declared the coronavirus outbreak a pandemic.\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = plot_etm_results(etms, model_defs, text4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Example 4: Explanation"},{"metadata":{"trusted":true},"cell_type":"code","source":"etm = etms['intros_small_12']\ntopics = etm.topn_topics_from_text(text4, topn=10)\nfor topic_title in topics['title']:\n    explanation = etm.explain_topic_for_text(text4, topic_title)\n    print(topic_title)\n    print(explanation)\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Review and Output"},{"metadata":{"trusted":true},"cell_type":"code","source":"cvs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtfidfs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_dfs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for key, model in cvs.items():\n    file_name = \"cv_{}.joblib\".format(key)\n    joblib.dump(model, file_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for key, df in topic_dfs.items():\n    file_name = \"topic_df_{}.csv\".format(key)\n    df.to_csv(file_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for key, mat in Xtfidfs.items():\n    file_name = \"xtfidf_{}.npz\".format(key)\n    scipy.sparse.save_npz(file_name, mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_key = \"intros_small\"\ntopic_key = model_key.split('_')[1]\nprint('model_key={}'.format(model_key))\nprint('topic_key={}'.format(topic_key))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = joblib.load(\"cv_{}.joblib\".format(model_key))\nXtfidf = scipy.sparse.load_npz(\"xtfidf_{}.npz\".format(model_key))\ntopic_df = pd.read_csv(\"topic_df_{}.csv\".format(topic_key), index_col=\"page_id\")\netm = ExplicitTopicModel(cv, Xtfidf, topic_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"etm.topn_topics_from_text(text3, topn=10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}