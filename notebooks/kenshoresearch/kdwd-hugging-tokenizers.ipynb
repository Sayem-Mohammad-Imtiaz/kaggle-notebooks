{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Kensho Derived Wikimedia Dataset - Checking out Hugging Face Tokenizers\nHugging Face [recently announced](https://twitter.com/huggingface/status/1215746098201014272?lang=en) fast [Rust](https://www.rust-lang.org/) implementations of its tokenizers. Lets see what kind of performance we can get out of the new [huggingface tokenizers package](https://github.com/huggingface/tokenizers) compared to the tokenizers included in the [huggingface transformers package](https://github.com/huggingface/transformers)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from collections import Counter\nimport json\nimport os\nfrom pprint import pprint\nimport string\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom tqdm import tqdm\n\nsns.set()\nsns.set_context('talk')\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All of the KDWD files have one \"thing\" per line.  We'll hard code the number of lines in the files we're going to use so we can have nice progress bars when streaming through them."},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_KLAT_LINES = 5_343_564\nkdwd_path = os.path.join(\"/kaggle/input\", \"kensho-derived-wikimedia-data\")\nvocab_path = os.path.join(\"/kaggle/input\", \"hugging-face-tokenizer-vocabs\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Install the Hugging Face tokenizers and transformers packages"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"!pip install tokenizers\n!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Example Usage of Tokenizers"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tokenizers    # Rust implementations\nimport transformers  # Python implementations","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets create some tokenizer classes and see how they work.  We'll use Bert Word Pieces as our benchmark."},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_file = os.path.join(vocab_path, \"bert-base-uncased-vocab.txt\")\nrust_bert_wp = tokenizers.BertWordPieceTokenizer(vocab_file)\npyth_bert_wp = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\npprint(\"Rust tokenizer class: {}\".format(rust_bert_wp))\nprint()\npprint(\"Python tokenizer class: {}\".format(pyth_bert_wp))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `BertWordPieceTokenizer` class from the [huggingface tokenizers package](https://github.com/huggingface/tokenizers) works like this,  "},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded = rust_bert_wp.encode(\"Do you feel like I feel?\")\npprint(\"encoded={}\".format(encoded))\npprint(\"encoded.tokens={}\".format(encoded.tokens))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `BertTokenizer` class from the [huggingface transformers package](https://github.com/huggingface/transformers) works like this,  "},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens = pyth_bert_wp.convert_ids_to_tokens(pyth_bert_wp.encode(\"Do you feel like I feel?\"))\npprint(\"tokens={}\".format(tokens))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Speed Test \nLets create a class to iterate through the link annotated text of the Kensho Derived Wikimedia Dataset (KDWD). "},{"metadata":{"trusted":true},"cell_type":"code","source":"class KdwdLinkAnnotatedText:\n    def __init__(self, file_path, max_pages):\n        self.num_lines = NUM_KLAT_LINES\n        self.file_path = file_path\n        self.max_pages = max_pages\n        self.pages_to_parse = min(self.num_lines, self.max_pages)\n    def __iter__(self):\n        with open(self.file_path) as fp:\n            for ii_line, line in enumerate(fp):\n                if ii_line == self.pages_to_parse:\n                    break\n                yield json.loads(line)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_PAGES = 500","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see how long it takes to tokenize some pages from our Wikipedia sample. We'll use both huggingface tokenizers and a simple function that splits on whitespace, lowercases, and removes punctuation. "},{"metadata":{"trusted":true},"cell_type":"code","source":"table = str.maketrans('', '', string.punctuation)\ndef simple_tokenizer(text):\n    tokens = [tok.lower().strip() for tok in text.split()]\n    tokens = [tok.translate(table) for tok in tokens]\n    tokens = [tok for tok in tokens if tok != \"\"]\n    return tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_path = os.path.join(kdwd_path, \"link_annotated_text.jsonl\")\nklat = KdwdLinkAnnotatedText(file_path, max_pages=NUM_PAGES)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To begin we'll see how long it take to simply iterate through the pages. "},{"metadata":{"trusted":true},"cell_type":"code","source":"t0 = time.time()\nfor page in tqdm(klat, total=klat.pages_to_parse, desc='just iteration'):\n    for section in page['sections']:\n        first = section['text'][0]\ndt_iter = time.time() - t0\nprint(\"dt: {}\".format(dt_iter))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we'll count unigrams produced by our 3 tokenizers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"unigrams_simple = Counter()\nunigrams_hf_rust = Counter()\nunigrams_hf_pyth = Counter()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simple Tokenizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"t0 = time.time()\nfor page in tqdm(klat, total=klat.pages_to_parse, desc='simple tokenizer'):\n    for section in page['sections']:\n        tokens = simple_tokenizer(section['text'])\n        unigrams_simple.update(tokens)\ndt_simple = time.time() - t0\nprint(\"dt: {}\".format(dt_simple))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hugging Face - Rust Tokenizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"t0 = time.time()\nfor page in tqdm(klat, total=klat.pages_to_parse, desc='hugging face Rust tokenizer'):\n    for section in page['sections']:\n        encoded = rust_bert_wp.encode(section['text'])\n        unigrams_hf_rust.update(encoded.tokens)\ndt_hf_rust = time.time() - t0\nprint(\"dt: {}\".format(dt_hf_rust))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hugging Face - Python Tokenizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"t0 = time.time()\nfor page in tqdm(klat, total=klat.pages_to_parse, desc='hugging face Python tokenizer'):\n    for section in page['sections']:\n        tokens = pyth_bert_wp.convert_ids_to_tokens(pyth_bert_wp.encode(section['text']))\n        unigrams_hf_pyth.update(tokens)\ndt_hf_pyth = time.time() - t0\nprint(\"dt: {}\".format(dt_hf_pyth))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = [\"just iteration\", \"simple\", \"hugging rust\", \"hugging python\"]\ntimes = np.array([dt_iter, dt_simple, dt_hf_rust, dt_hf_pyth])\nrates = np.array([\n    sum(unigrams_simple.values()) / dt_simple,\n    sum(unigrams_hf_rust.values()) / dt_hf_rust,\n    sum(unigrams_hf_pyth.values()) / dt_hf_pyth,\n])\nyy = np.arange(len(labels)) \n\nwidth = 0.5\nfigsize = (16, 8)\nfig, axes = plt.subplots(1, 2, figsize=figsize, sharey=True)\n\nax = axes[0]\nrects1 = ax.barh(yy, times, width) \nax.set_yticks(yy)\nax.set_yticklabels(labels)\nax.set_xlabel('seconds')\nax.set_ylabel('Tokenizer')\nax.set_title('Total Parse Time')\n\nax = axes[1]\nrects2 = ax.barh(yy[1:], rates/1000, width, color=\"orange\") \nax.set_xlabel('Thousands of Tokens / s')\nax.set_title('Token Parse Rate')\n\nfig.suptitle('Tokenizer Performance on {} Wikipedia Pages'.format(NUM_PAGES));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\nNote execution time may vary between runs, but we can get a sense of how large the differences are. Iteration takes a negligible amount of time compared to any of the tokenizers.  In all experiments the simple parser is the fastest but does the least (e.g. no unicode normalization, cant recover original string ...) and  the Hugging Face Rust implementation is more than a factor of 10 faster that the Python implementation. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"times: {}\".format(list(zip(labels, times))))\n\n# normalize by Hugging Face Python\nprint(\"times normalized by Hugging Face Python: {}\".format(times/times[3]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"rates: {}\".format(list(zip(labels[1:], rates))))\n\n# normalize by Hugging Face Python\nprint(\"rates normalized by Hugging Face Python: {}\".format(rates/rates[2]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check Tokens"},{"metadata":{"trusted":true},"cell_type":"code","source":"unigrams_simple.most_common(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unigrams_hf_rust.most_common(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unigrams_hf_pyth.most_common(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}