{"cells":[{"metadata":{},"cell_type":"markdown","source":"The aim is to use a pretrained convnet to correctly classify 5 types of crop. Transfer learning is especially useful here as the pretrained model can extract useful features from the images, and the trainable part of the neural net can then extract some meaning from those features! Using this technique, we achieve a remarkable 97.5% accuracy on the test set.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Imports","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport tensorflow as tf\nimport keras\nfrom keras.applications import VGG19\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport cv2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We first want to load and preprocess all the images with their correct labels. The only real preprocessing steps are to resize the image to be 224x224 (if necessary) and to scale the numbers in the array to be between 0 and 1.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the images into a dataframe\n\ndf = []\n\ncrops = ['jute', 'maize', 'rice', 'sugarcane', 'wheat']\n\n# Useful dict for switching between crop names and labels\ncrop_to_label = {}\nfor i, crop in enumerate(crops):\n    crop_to_label[crop] = i\n    \nlabel_to_crop = {value:key for (key, value) in crop_to_label.items()}\n\nfor crop in crops:\n    subdir = '../input/agriculture-crop-images/kag2/' + crop\n    for path in os.listdir(subdir):\n        df.append([os.path.join(subdir, path), crop])\n    \ndf = pd.DataFrame(df, columns=['path', 'label'])\ndf = df.sample(frac=1, random_state=0).reset_index(drop=True) # shuffle the rows\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_image(path):\n    \"\"\"Helper function to read, resize and rescale an image from its path\"\"\"\n    im = plt.imread(path)\n    im = cv2.resize(im, (224,224), interpolation=cv2.INTER_CUBIC)    \n    return im/255.\n\n# Test on first image in dataset\nim = preprocess_image(df.loc[0, 'path'])\nlabel = df.loc[0, 'label']\n\n# Get the image dimensions to a variable\nimg_size, _, channels = im.shape\n\n# Show the image with its label\nplt.title(label)\nplt.imshow(im)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now preprocess all the images, and one-hot encode a label for them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_examples = len(df.index)\nn_classes = len(crops)\n\n# Initialize X and y\nX = np.zeros(shape=(n_examples, img_size, img_size, channels))\ny = np.zeros(shape=(n_examples, n_classes))\n\n# Loop through dataset to set values of X and y\nfor i, idx in enumerate(df.index):\n    path, label = df.loc[idx, :]\n    X[i, :, :, :] = preprocess_image(path)\n    y[i, crop_to_label[label]] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, split X and y into train, and dev sets in proportions 80-20 respectively. We will then be ready to train a model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Training","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The model architecture consists of two hidden FC layers of size 1000 and 128 sitting on top of the VGG19 model, with a softmax layer to give the final prediction. ","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"num_classes = 5\n\nmodel = Sequential()\nvgg = VGG19(input_shape=(img_size,img_size,channels),include_top=False,weights = 'imagenet',pooling='avg')\nmodel.add(vgg)\nmodel.add(Dense(1000, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.layers[0].trainable = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hyperparameters to set:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 16\nepochs = 30\nlearning_rate = 1e-3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use an Adam optimizer, with a uniform learning rate decay.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=keras.optimizers.Adam(lr=learning_rate, decay=learning_rate/epochs), loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's fit the model to our training data!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x=X_train, y=y_train, validation_data=(X_dev, y_dev), epochs=epochs, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's first plot the loss and the accuracy over each epoch for the train and dev sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 10))\n\nplt.subplot(2, 2, 1)\nplt.title('Training accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.plot(history.history['accuracy'])\n\nplt.subplot(2, 2, 2)\nplt.title('Validation accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.plot(history.history['val_accuracy'])\n\nplt.subplot(2, 2, 3)\nplt.title('Training loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.plot(history.history['loss'])\n\nplt.subplot(2, 2, 4)\nplt.title('Validation loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.plot(history.history['val_loss'])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The loss of both decreases and plateaus, which shows our model is fitting the data well.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/testssss/testdata.csv', index_col=0)\n\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = np.zeros(shape=(len(test_df.index), img_size, img_size, channels))\ny_test = np.zeros(shape=(len(test_df.index), 5))\n\nfor i, idx in enumerate(test_df.index):\n    path, crop = test_df.loc[idx, ['testpath', 'crop']]\n    X_test[i, :, :, :] = preprocess_image(path)\n    y_test[i, crop_to_label[crop]] = 1\n    \nplt.imshow(X_test[0, :, :, :])\nplt.title(label_to_crop[np.argmax(y_test[0, :])] + ' - Model predicts ' + label_to_crop[model.predict_classes(np.array([X_test[0,:, :, :]]))[0]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Test accuracy - {}%'.format(model.evaluate(X_test, y_test)[1] * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfortunately, despite getting ~95-100% accuracy scores on the training and dev sets, its generalisation to new data is not as good. We see this also with some new images from the web.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_predict(path):\n    im = np.array([preprocess_image(path)])\n    prediction = model.predict_classes(im)\n    return np.vectorize(label_to_crop.get)(prediction)[0], model.predict(im)[0, prediction[0]]\n\nplt.figure(figsize=(20, 20))\n\nfor n, path in enumerate(os.listdir('../input/new-images')): \n    prediction, confidence = model_predict('../input/new-images/' + path)\n    plt.subplot(2, 2, n+1)\n    plt.title('Model predicts {}, confidence={}'.format(prediction, confidence))\n    plt.imshow(preprocess_image('../input/new-images/' + path))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model correctly identifies three out of the four. The picture of the sugarcane here is quite different to the ones in the dataset, so we can see that our algorithm is unsure! In order to generalise well to new data, realistically we need a much bigger dataset. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"****PLEASE UPVOTE THIS NOTEBOOK IF YOU FOUND IT USEFUL!****","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(np.array([list(test_df['testpath'].values), list(model.predict_classes(X_test))]).T, columns=['pathname', 'label'])\nsubmission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}