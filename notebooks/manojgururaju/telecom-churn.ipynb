{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Telecom Churn Case Study"},{"metadata":{},"cell_type":"markdown","source":"### 1. Data Understanding and Cleaning\n\nLet's first have a look at the dataset and understand the size, attribute names etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom sklearn import linear_model\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom pprint import pprint\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix, roc_auc_score\nimport os\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Housing.csv\ntelecom = pd.read_csv(\"../input/telecom-customer/Telecom_customer churn.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summary of the dataset: 99999 rows, 226 columns\ntelecom.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking columns which have missing values\n\ntelecom.isnull().mean().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns with more than 50% missing values\n\ncolumn_missing_data = telecom.loc[:,telecom.isnull().mean() >= 0.5 ]\nprint(\"Number of columns with missing data {}\".format(len(column_missing_data.columns)))\ncolumn_missing_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Droping columns with more than 50% missing values\ntelecom = telecom.loc[:, telecom.isnull().mean() <= .5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Droping date columns since there will be no time series analysis\n\ndate_cols = telecom.columns[telecom.columns.str.contains(pat = 'date')]\ntelecom = telecom.drop(date_cols, axis = 1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping Mobile Number\n\ntelecom = telecom.drop('mobile_number', axis = 1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking percentage of missing values in dataset\n\ntelecom.isnull().mean().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# All the column names with missing values\ntelecom.loc[:,telecom.isnull().mean() > 0].columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting missing values\nplt.figure(figsize=(20, 5))\nsns.heatmap(telecom.isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove Columns which have only 1 unique Value\n\ncol_list = telecom.loc[:,telecom.apply(pd.Series.nunique) == 1]\ntelecom = telecom.drop(col_list, axis = 1)\ntelecom.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"telecom.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Storing column names before imputing\n\ncol_name = telecom.columns\ncol_name","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Since we have outliers in most of the columns we will do imputation of missing values using median"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imputing median values using SimpleImputer\nfrom sklearn.impute import SimpleImputer\n\nimp_mean = SimpleImputer( strategy='median') \nimp_mean.fit(telecom)\ntelecom = imp_mean.transform(telecom)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom= pd.DataFrame(telecom)\ntelecom.columns = col_name\ntelecom.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 5))\nsns.heatmap(telecom.isnull())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All missing values imputed"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming columns\n\ntelecom.rename(columns={'jun_vbc_3g': 'vbc_3g_6', \n                        'jul_vbc_3g': 'vbc_3g_7', \n                        'aug_vbc_3g': 'vbc_3g_8', \n                        'sep_vbc_3g': 'vbc_3g_9'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Deriving total recharge amount for month 6 and 7"},{"metadata":{"trusted":true},"cell_type":"code","source":"total_rech_amt_6_7 = telecom[['total_rech_amt_6','total_rech_amt_7']].sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting top 30 percent subscribers for churn prediction\n\np70 = np.percentile(total_rech_amt_6_7, 70.0)\n\ntele_top30 = telecom[total_rech_amt_6_7 > p70]\ntele_top30.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Deriving churn flag using month 9 data"},{"metadata":{"trusted":true},"cell_type":"code","source":"tele_top30['total_usage_9'] = tele_top30[['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9']].sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tele_top30['churn'] = tele_top30['total_usage_9'].apply(lambda x: 1 if x==0 else 0 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tele_top30.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping month 9 columns "},{"metadata":{"trusted":true},"cell_type":"code","source":"mon_9_cols = tele_top30.columns[tele_top30.columns.str.contains(pat = '_9')]\nmon_9_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tele_top30.drop(mon_9_cols, axis=1, inplace = True)\ntele_top30.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Deriving New columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting age on network to years from days\n\ntele_top30['aon_yr'] = round(tele_top30['aon']/365,2)\ntele_top30.drop('aon',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Derving new total columns by combing month 6 and 7"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_list = tele_top30.columns[tele_top30.columns.str.contains('_6|_7')]\nlen(col_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_col_list = col_list.str[:-2].unique()\nlen(unique_col_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_col_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in unique_col_list:\n    col_new_name = col+\"_6_7\"\n    col_6_name = col+\"_6\"\n    col_7_name = col+\"_7\"\n    tele_top30[col_new_name] = tele_top30[[col_6_name,col_7_name]].sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tele_top30.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tele_top30.drop(col_list, axis=1, inplace=True)\ntele_top30.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tele_top30.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tele_top30.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see presence of outliers, hence performing capping operation"},{"metadata":{"trusted":true},"cell_type":"code","source":"tele_top30['churn'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Churn column is also imbalanced, so we will not cap it, we will balance the dataset later"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Storing churn data in new dataframe\nChurn = pd.DataFrame(tele_top30['churn'])\n\n# Dropping churn column from tele_top30 before capping operation\ntele_top30 = tele_top30.drop(['churn'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Derving 25th and 75th percentile\n\nQ1=tele_top30.quantile(0.25)\nQ3=tele_top30.quantile(0.75)\n\n# Deriving Inter Quartile Range\nIQR=Q3-Q1\n\n# Derving the Upper limit and Lower limit\nLL = Q1 - 3*IQR\nUL = Q3 + 3*IQR ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Capping the data using Upper Limit and Lower Limit\n\nq = [LL,UL]\ntele_top30 = tele_top30.clip(LL,UL,axis=1)\nprint(tele_top30.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tele_top30.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After capping many columns have only 1 unique value. So dropping these columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing columns which have only one value after capping operation\n\ncol_list = tele_top30.loc[:,tele_top30.apply(pd.Series.nunique) == 1]\ntele_top30 = tele_top30.drop(col_list, axis = 1)\ntele_top30.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding churn column to tele_top30\n\ntele_top30 = pd.concat([tele_top30,Churn], axis=1)\ntele_top30.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the correlation matrix using seaborn heatmap\n\ncorr_mat = tele_top30.corr()\nplt.figure(figsize=(20, 10))\nsns.heatmap(corr_mat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see high correlation between month 6,7 and 8 features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding the pairs of most correlated features\n\nabs(corr_mat).unstack().sort_values(ascending = False).drop_duplicates().head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the jointplot to check correlation\n\nsns.jointplot(x = 'total_rech_amt_6_7', y = 'arpu_6_7', data=tele_top30, kind='reg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the jointplot to check correlation\n\nsns.jointplot(x = 'total_rech_amt_8', y = 'arpu_8', data=tele_top30, kind='reg', color = [255/255,152/255,150/255])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding highest correlated features with churn\n\ncorr_tgt = abs(corr_mat[\"churn\"]).sort_values(ascending = False)\ntop_features = corr_tgt.loc[((corr_tgt > 0.2) & (corr_tgt != 1))]\ntop_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting absolute correlation value of churn with all other varibales\n\nplt.figure(figsize=(20,5))\ncorr_tgt.sort_values(ascending = False).plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the imbalance in churn feature\n\ntele_top30['churn'].value_counts()*100.0 /len(tele_top30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(3, 4))\nsns.countplot('churn', data=tele_top30)\nplt.title('Churn distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset is higly imbalnced "},{"metadata":{},"cell_type":"markdown","source":"# Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"Interpretable_Model_df = tele_top30","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = Interpretable_Model_df.pop('churn')\nX = Interpretable_Model_df\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cols = X.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling the data using standard scaler\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the test train split\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Balancing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Balancing the dataSet using SMOTE method\n\nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE(sampling_strategy='auto', random_state=100)\nX_train_bal, y_train_bal = sm.fit_sample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train_bal.shape)\nprint(y_train_bal.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(3, 4))\nsns.countplot(y_train_bal)\nplt.title('Churn distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data is now balanced"},{"metadata":{},"cell_type":"markdown","source":"# 1. Interpretable Models - Without PCA"},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection using Lasso Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\n\nC = [100, 10, 1, 0.5, 0.1, 0.01, 0.001]\n\nfor c in C:\n    lassoclf = LogisticRegression(penalty='l1', solver='liblinear', C=c).fit(X_train_bal, y_train_bal)\n    model = SelectFromModel(lassoclf, prefit=True)\n    X_lasso = model.transform(X_train_bal)\n    print('C Value - ',c, ' selects',X_lasso.shape[1],' no. of Features')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Selecting c = 0.001 to have 18 important features"},{"metadata":{"trusted":true},"cell_type":"code","source":"lassoclf = LogisticRegression(penalty='l1', solver='liblinear', C=.001).fit(X_train_bal, y_train_bal)\nmodel = SelectFromModel(lassoclf, prefit=True)\nX_train_lasso = model.transform(X_train_bal)\npos = model.get_support(indices=True)\nselected_features = list(Interpretable_Model_df.columns[pos])\nprint(selected_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_lasso = pd.DataFrame(X_train_lasso)\nX_train_lasso.columns = selected_features\nX_train_lasso","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Interpretable Model 1 - Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining common code\n\ndef print_all_scores(y_test, test_prediction, y_train, train_prediction):\n    print('Precision on test set:\\t'+str(round(precision_score(y_test,test_prediction) *100,2))+\"%\")\n    print('Recall on test set:\\t'+str(round(recall_score(y_test,test_prediction) *100,2))+\"%\")\n    print(\"Training Accuracy: \"+str(round(accuracy_score(y_train,train_prediction) *100,2))+\"%\")\n    print(\"Test Accuracy: \"+str(round(accuracy_score(y_test,test_prediction) *100,2))+\"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression Base Model - Default parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a base logistic regression model\nlr = LogisticRegression(random_state=100)\n\n# Lookin at the parameters used by our base model\nprint('Parameters currently in use:\\n')\npprint(lr.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the model\nlr.fit(X_train_lasso, y_train_bal)\n\n# Predicting values\nX_test_lasso = pd.DataFrame(data=X_test).iloc[:, pos]\nX_test_lasso.columns = selected_features\npredictions = lr.predict(X_test_lasso)\ntrain_pred = lr.predict(X_train_lasso)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy, precision, recall/sensitivity of the model\nprint_all_scores(y_test,predictions, y_train_bal, train_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interpreatation of scores\n\n1. Precision on test set:\t31.29%\n2. Recall on test set:\t78.9%\n3. Training Accuracy: 83.51%\n4. Test Accuracy: 82.39%\n\n\n- Precision score is low and recall score is good, which means the model has a tendency for classifying most of the customers to churn, even though the customers are not likely to churn ( actual churn / total predicted churn ). In this case, As most of the customers are classified that they would churn hence, the model would capture all the customers who would likely churn.\n- Observing test and training score, we can be sure that model is not overfitted\n\n<b> Verdict: Many customers who would not likely to churn would recive offers, which would lead to loss of revenue for the organization </b>"},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression - Hyperparameter Tuning"},{"metadata":{},"cell_type":"markdown","source":"#### Creating a hypertuned logistic regression model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialising logistic Regression\nlog_reg = LogisticRegression(random_state = 100)\n\n# Creating hyper parameter grid\nparameter_grid = {'solver': ['newton-cg', 'lbfgs','liblinear','sag'],\n                  'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n                  'C': [100, 10, 1.0, 0.1, 0.01]}\n\ngs = GridSearchCV(estimator=log_reg, param_grid=parameter_grid, n_jobs=-1, cv=3, scoring='accuracy', error_score=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model\ngrid_result = gs.fit(X_train_lasso, y_train_bal)\n\n# Finding the best model\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fitting the final model with the best parameters obtained from grid search.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialising hyper tuned logistic Regression\nlog_reg_ht = LogisticRegression(C= 1.0, penalty= 'l2', solver= 'liblinear', random_state = 100)\n\n# Fitting the model\nlog_reg_ht.fit(X_train_lasso, y_train_bal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the labels\ntrain_pred = log_reg_ht.predict(X_train_lasso)\ntest_pred = log_reg_ht.predict(X_test_lasso)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy, precision, recall/sensitivity of the model\nprint_all_scores(y_test,test_pred,y_train_bal,train_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interpreatation of scores\n\nEven with the hyperparametres tunning there is some improvement with our model\n\n1. Precision on test set:\t36.69%\n2. Recall on test set:\t72.39%\n3. Training Accuracy: 83.51%\n4. Test Accuracy: 86.18%\n\n\n- Precision score is still low and recall score is good, which means the model has a tendency for classifying most of the customers to churn, even though the customers are not likely to churn ( actual churn / total predicted churn ). In this case, As most of the customers are classified that they would churn hence, the model would capture all the customers who would likely churn.\n- Observing test and training score, we can be sure that model is not overfitted\n\n<b> Verdict: Many customers who would not likely to churn would recive offers, which would lead to loss of revenue for the organization </b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# To get the weights of all the variables\nweights = pd.Series(log_reg_ht.coef_[0],\n                 index=selected_features)\nweights.sort_values(ascending = False).plot(kind = 'bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Interpretable Model 2 - Random Forrest"},{"metadata":{},"cell_type":"markdown","source":"### Random Forrest Base Model - Default Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the random forest with default parameters.\nrfc = RandomForestClassifier(random_state = 100)\n\n# Lookin at the parameters used by our base model\nprint('Parameters currently in use:\\n')\npprint(rfc.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the model\nrfc.fit(X_train_lasso,y_train_bal)\n\n# Making predictions\npredictions = rfc. predict(X_test_lasso)\ntrain_pred = rfc. predict(X_train_lasso)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy, precision, recall/sensitivity of the model\nprint_all_scores(y_test,predictions,y_train_bal,train_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interpreatation of scores\n\nEven with the hyperparametres tunning there is significant improvement with our random forest model\n\n1. Precision on test set:\t43.19%\n2. Recall on test set:\t72.02%\n3. Training Accuracy: 87.78%\n4. Test Accuracy: 88.88%\n\n\n- Precision score is better than logistic regression and recall score is good too, This model would identify most of the customers who are about churn and also the model would relativly less likely to miss classify non churn customers leading to significant loss in revenue\n- Observing test and training score, we can be sure that model is not overfitted which would likely to expect in the tree based models\n\n<b> Verdict: some customers who would not likely to churn would recive offers, which would lead to loss of revenue for the organization to some extent, If we can improve our precision it would be more benificial and this model would take more computational resorce and time consuming hence it can't make prediction quickly</b>"},{"metadata":{},"cell_type":"markdown","source":"### Random Forrest - Hyperparameter Tuning"},{"metadata":{},"cell_type":"markdown","source":"#### Creating a hyperparameter grid"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth = [int(x) for x in np.linspace(10, 50, num = 5)]\nmax_depth.append(None)\n\n# Create the random parameter grid\nparameter_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)],\n                  'max_features': ['auto', 'sqrt'],\n                  'max_depth': max_depth,\n                  'min_samples_split': [100, 500, 1000],\n                  'min_samples_leaf': [50, 250, 500],\n                  'bootstrap': [True, False]}\n\npprint(parameter_grid)\n\n# Searching across different combinations for best model parameters\nrf_random = RandomizedSearchCV(estimator = rfc, param_distributions = parameter_grid, n_iter = 100, \n                               cv = 3, verbose=2, random_state=100, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the random search model\nrf_random.fit(X_train_lasso, y_train_bal)\n\n# Finding the best parameters\nrf_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Building the model around the random parameter obtained"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [15, 20, 25],\n    'min_samples_leaf': range(40, 50, 60),\n    'min_samples_split': range(80, 100, 120),\n    'n_estimators': [800, 1000, 1200], \n    'max_features': ['sqrt'],\n    'bootstrap': [False]\n}\n\n# Create a based model\nrf = RandomForestClassifier(random_state = 100)\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1,verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model\ngrid_result = grid_search.fit(X_train_lasso, y_train_bal)\n\n# Finding the best model\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fitting the final model with the best parameters obtained from grid search.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model with the best hyperparameters\n\nrfc = RandomForestClassifier(bootstrap=False,\n                             max_depth=20,\n                             min_samples_leaf=40, \n                             min_samples_split=80,\n                             max_features='sqrt',\n                             n_estimators=800)\n\n# Fit\nrfc.fit(X_train_lasso, y_train_bal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict\ntrain_pred = rfc.predict(X_train_lasso)\ntest_pred = rfc.predict(X_test_lasso)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy, precision, recall/sensitivity of the model\nprint_all_scores(y_test,predictions,y_train_bal,train_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interpreatation of scores\n\nThere is no significant improvement in the model\n\n1. Precision on test set:\t43.19%\n2. Recall on test set:\t72.02%\n3. Training Accuracy: 92.83%\n4. Test Accuracy: 88.88%\n\n\n- Precision score is low and recall score is good, which means the model has a tendency for classifying most of the customers to churn, even though the customers are not likely to churn. In this case, As most of the customers are classified that they would churn hence, the model would capture all the customers who would likely churn.\n- Observing test and training score, we can be sure that model is not overfitted\n\n<b> Verdict: Many customers who would not likely to churn would recive offers, which would lead to loss of revenue for the organization upto some extent </b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# To get the weights of all the variables\nweights = pd.Series(rfc.feature_importances_,\n                 index=selected_features)\nweights.sort_values(ascending = False).plot(kind = 'bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Interpretable Model 3 - Using XgBoost"},{"metadata":{},"cell_type":"markdown","source":"### XgBoost Base Model - Default Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model on training data with default hyperparameters\nxgb = XGBClassifier()\n\n# Lookin at the parameters used by our base model\nprint('Parameters currently in use:\\n')\npprint(xgb.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model\nxgb.fit(X_train_lasso,y_train_bal)\n\n# Making predictions\npredictions = xgb.predict(X_test_lasso)\ntrain_pred = xgb.predict(X_train_lasso)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy, precision, recall/sensitivity of the model\nprint_all_scores(y_test,predictions,y_train_bal,train_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# AUC Score\nprint(\"AUC Score on test set:\\t\" +str(round(roc_auc_score(y_test,predictions) *100,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interpreatation of scores\n\n1. Precision on test set:\t62.38%\n2. Recall on test set:\t54.11%\n3. Training Accuracy: 83.51%\n4. Test Accuracy: 92.39%\n\n\n- Precision score is better and recall score is not good, which means the model has a tendency for classifying most of the customers will not churn, even though the customers are likely to churn. This model is not good for the objective we have. As we dont need our model to predict customers who are about to churn as this would impact the business more.\n- Observing test and training score, looks like model is not overfitted and performs better with test data\n\n<b> Verdict: This model is not suitable as we would miss 50% of customers who would likely to churn </b>"},{"metadata":{},"cell_type":"markdown","source":"### XgBoost - Hyperparameter Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyperparameter tuning with XGBoost\n\n# specify range of hyperparameters\nparam_grid = {'learning_rate': [0.2, 0.6], \n             'subsample': [0.3, 0.6, 0.9]}          \n\n\n# specify model\nxgb_ht = XGBClassifier(max_depth=2, n_estimators=200)\n\n# set up GridSearchCV()\ngs = GridSearchCV(estimator = xgb_ht, param_grid = param_grid, scoring= 'roc_auc', \n                        cv = 3, verbose = 1, return_train_score=True, n_jobs = -1)     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model\ngrid_result = gs.fit(X_train_lasso,y_train_bal) \n\n# Finding the best model\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Fitting the final model with the best parameters obtained from grid search."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model with the best hyperparameters\nxgb_ht = XGBClassifier(max_depth=2, n_estimators=200, learning_rate = 0.6, subsample = 0.9)\n\n# Fit\nxgb_ht.fit(X_train_lasso, y_train_bal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict\ntrain_pred = xgb_ht.predict(X_train_lasso)\ntest_pred = xgb_ht.predict(X_test_lasso)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy, precision, recall/sensitivity of the model\nprint_all_scores(y_test,predictions,y_train_bal,train_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interpreatation of scores\n\nThis model has no significant improvement apart from the the training accuracy\n\n1. Precision on test set:\t62.38%\n2. Recall on test set:\t54.11%\n3. Training Accuracy: 95.92%\n4. Test Accuracy: 92.88%\n\n\n- Precision score is better and recall score is not good, which means the model has a tendency for classifying most of the customers will not churn, even though the customers are likely to churn. This model is not good for the objective we have. As we dont need our model to predict customers who are about to churn as this would impact the business more.\n- Observing test and training score, looks like model is not overfitted and performs better with test data\n\n<b> Verdict: This model is not suitable as we would miss 50% of customers who would likely to churn </b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# To get the weights of all the variables\nweights = pd.Series(xgb_ht.feature_importances_,\n                 index=selected_features)\nweights.sort_values(ascending = False).plot(kind = 'bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. High Accuracy Models - Using PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_roc( y_test_churn, y_pred_churn ):\n    fpr, tpr, thresholds = metrics.roc_curve(  y_test_churn, y_pred_churn,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score(  y_test_churn, y_pred_churn )\n    print(\"ROC score: {}\".format(auc_score))\n    plt.figure(figsize=(6, 6))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return fpr, tpr, thresholds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Manually finding what would be ideal number of components*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running pca with default parameters.\npca = PCA(random_state=100)\n\n# Fitting the model\npca.fit(X_train_bal)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using Screeplot for identifying the component size"},{"metadata":{"trusted":true},"cell_type":"code","source":"# cumulative variance\nvar_cumu = np.cumsum(pca.explained_variance_ratio_)\n\n# code for Scree plot\nfig = plt.figure(figsize=[12,8])\nplt.vlines(x=30, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\nplt.hlines(y=0.95, xmax=30, xmin=0, colors=\"g\", linestyles=\"--\")\nplt.plot(var_cumu)\nplt.ylabel(\"Cumulative variance explained\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the grapgh we can infer that 30 compenents would be ideal"},{"metadata":{},"cell_type":"markdown","source":"#### Using incremental PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initializing the PCA model\npca_inc = IncrementalPCA(n_components=30)\n\n# Fitting the model\ndf_train_pca_inc = pca_inc.fit_transform(X_train_bal)\n\n# Looking at the shape\ndf_train_pca_inc.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_pca_inc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Verifying there is no correlation exist after PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plottong correlation\n\ncorrmat = np.corrcoef(df_train_pca_inc.transpose())\nplt.figure(figsize=[15,5])\nsns.heatmap(corrmat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the transformation on test\n\ndf_test_pca_inc = pca_inc.transform(X_test)\ndf_test_pca_inc.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1 High Accuracy Model 1 - Using Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression Base Model - Default parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a base logistic regression model\nlr = LogisticRegression(random_state=100)\n\n# Lookin at the parameters used by our base model\nprint('Parameters currently in use:\\n')\npprint(lr.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the model\nlr.fit(df_train_pca_inc, y_train_bal)\n\n# Predicting values\npredictions = lr.predict(df_test_pca_inc)\ntest_pred = lr.predict(df_train_pca_inc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy, precision, recall/sensitivity of the model\nprint_all_scores(y_test,predictions,y_train_bal,train_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression - Hyperparameter Tuning"},{"metadata":{},"cell_type":"markdown","source":"#### Creating a hypertuned logistic regression model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialising logistic Regression\nlog_reg = LogisticRegression(random_state = 100)\n\n# Creating hyper parameter grid\nparameter_grid = {'solver': ['newton-cg', 'lbfgs','liblinear','sag'],\n                  'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n                  'C': [100, 10, 1.0, 0.1, 0.01]}\n\ngs = GridSearchCV(estimator=log_reg, param_grid=parameter_grid, n_jobs=-1, cv=3, scoring='accuracy', error_score=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model\ngrid_result = gs.fit(df_train_pca_inc, y_train_bal)\n\n# Finding the best model\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fitting the final model with the best parameters obtained from grid search.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialising hyper tuned logistic Regression\nlog_reg_ht = LogisticRegression(C= 0.1, penalty= 'l2', solver= 'newton-cg', random_state = 100)\n\n# Fitting the model\nlog_reg_ht.fit(df_train_pca_inc, y_train_bal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the labels\ntrain_pred = log_reg_ht.predict(df_train_pca_inc)\ntest_pred = log_reg_ht.predict(df_test_pca_inc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy, precision, recall/sensitivity of the model\nprint_all_scores(y_test,predictions,y_train_bal,train_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interpreatation of scores for Logistic regression ( default and hyperparametre tuned )\n\n1. Precision on test set:\t32.18%\n2. Recall on test set:\t79.63%\n3. Training Accuracy: 83.59%\n4. Test Accuracy: 82.94% \n\n\n- Precision score is low and recall score is good, which means the model has a tendency for classifying most of the customers to churn, even though the customers are not likely to churn ( actual churn / total predicted churn ). In this case, As most of the customers are classified that they would churn hence, the model would capture all the customers who would likely churn.\n- Observing test and training score, we can be sure that model is not overfitted\n\n<b> Verdict: Many customers who would not likely to churn would recive offers, which would lead to loss of revenue for the organization </b>"},{"metadata":{},"cell_type":"markdown","source":"## 2.2 High Accuracy Model 2 - Random Forrest"},{"metadata":{},"cell_type":"markdown","source":"### Random Forrest Base Model - Default Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the random forest with default parameters.\nrfc = RandomForestClassifier(random_state = 100)\n\n# Lookin at the parameters used by our base model\nprint('Parameters currently in use:\\n')\npprint(rfc.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the model\nrfc.fit(df_train_pca_inc,y_train_bal)\n\n# Making predictions\npredictions = rfc.predict(df_test_pca_inc)\ntrain_pred = rfc.predict(df_train_pca_inc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy, precision, recall/sensitivity of the model\nprint_all_scores(y_test,predictions,y_train_bal,train_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forrest - Hyperparameter Tuning"},{"metadata":{},"cell_type":"markdown","source":"## Note: Random Forest with hyper tuning would take more than 1 hour for execution"},{"metadata":{},"cell_type":"markdown","source":"#### Creating a hyperparameter grid"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth = [int(x) for x in np.linspace(10, 30, num = 3)]\n\n# Create the random parameter grid\nparameter_grid = {'n_estimators': [int(x) for x in np.linspace(start = 600, stop = 1000, num = 5)],\n                  'max_features': ['auto', 'sqrt'],\n                  'max_depth': max_depth,\n                  'min_samples_split': [500, 1000],\n                  'min_samples_leaf': [250, 500],\n                  'bootstrap': [True, False]}\n\npprint(parameter_grid)\n\n# Searching across different combinations for best model parameters\nrf_random = RandomizedSearchCV(estimator = rfc, param_distributions = parameter_grid, n_iter = 50, \n                               cv = 3, verbose=2, random_state=100, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the random search model\nrf_random.fit(df_train_pca_inc, y_train_bal)\n\n# Finding the best parameters\nrf_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Building the model around the random parameter obtained"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [15, 20, 25],\n    'min_samples_leaf': range(200, 300, 50),\n    'min_samples_split': range(400, 600, 100),\n    'n_estimators': [800, 1000, 1200], \n    'max_features': ['auto'],\n    'bootstrap': [False]\n}\n\n# Create a based model\nrf = RandomForestClassifier(random_state = 100)\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1,verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model\ngrid_result = grid_search.fit(df_train_pca_inc, y_train_bal)\n\n# Finding the best model\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model with the best hyperparameters\n\nrfc = RandomForestClassifier(bootstrap=False,\n                             max_depth=20,\n                             min_samples_leaf=200, \n                             min_samples_split=400,\n                             max_features='auto',\n                             n_estimators=800)\n\n# Fit\nrfc.fit(df_train_pca_inc, y_train_bal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict\ntrain_pred = rfc.predict(df_train_pca_inc)\ntest_pred = rfc.predict(df_test_pca_inc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy, precision, recall/sensitivity of the model\nprint_all_scores(y_test,predictions,y_train_bal,train_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interpreatation of scores for Random Forest ( deafault and hypertuned model )\n\n\n1. Precision on test set:\t43.19%\n2. Recall on test set:\t72.02%\n3. Training Accuracy: 92.83%\n4. Test Accuracy: 88.88%\n\n\n- Precision score is low and recall score is good, which means the model has a tendency for classifying most of the customers to churn, even though the customers are not likely to churn. In this case, As most of the customers are classified that they would churn hence, the model would capture all the customers who would likely churn.\n- Observing test and training score, we can be sure that model is not overfitted\n\n<b> Verdict: Many customers who would not likely to churn would recive offers, though it would be better than losing the customer </b>"},{"metadata":{},"cell_type":"markdown","source":"## 2.3 High Accuracy Model 3 - Using XgBoost"},{"metadata":{},"cell_type":"markdown","source":"### XgBoost Base Model - Default Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model on training data with default hyperparameters\nxgb = XGBClassifier()\n\n# Lookin at the parameters used by our base model\nprint('Parameters currently in use:\\n')\npprint(xgb.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model\nxgb.fit(df_train_pca_inc,y_train_bal)\n\n# Making predictions\npredictions = xgb.predict(df_test_pca_inc)\ntrain_pred = xgb.predict(X_train_lasso)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy, precision, recall/sensitivity of the model\nprint_all_scores(y_test,predictions,y_train_bal,train_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# AUC Score\nprint(\"AUC Score on test set:\\t\" +str(round(roc_auc_score(y_test,predictions) *100,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XgBoost - Hyperparameter Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyperparameter tuning with XGBoost\n\n# specify range of hyperparameters\nparam_grid = {'learning_rate': [0.2, 0.6], \n             'subsample': [0.3, 0.6, 0.9]}          \n\n\n# specify model\nxgb_ht = XGBClassifier(max_depth=2, n_estimators=200)\n\n# set up GridSearchCV()\ngs = GridSearchCV(estimator = xgb_ht, param_grid = param_grid, scoring= 'roc_auc', \n                        cv = 3, verbose = 1, return_train_score=True, n_jobs = -1)     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model\ngrid_result = gs.fit(df_train_pca_inc,y_train_bal) \n\n# Finding the best model\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model with the best hyperparameters\nxgb_ht = XGBClassifier(max_depth=2, n_estimators=200, learning_rate = 0.6, subsample = 0.9)\n\n# Fit\nxgb_ht.fit(df_train_pca_inc, y_train_bal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict\ntrain_pred = xgb_ht.predict(df_train_pca_inc)\ntest_pred = xgb_ht.predict(df_test_pca_inc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy, precision, recall/sensitivity of the model\nprint_all_scores(y_test,predictions,y_train_bal,train_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interpreatation of scores\n\nThis model has no significant improvement apart from the the training accuracy\n\n1. Precision on test set:\t62.38%\n2. Recall on test set:\t54.11%\n3. Training Accuracy: 95.92%\n4. Test Accuracy: 92.88%\n\n\n- Precision score is better and recall score is not good, which means the model has a tendency for classifying most of the customers will not churn, even though the customers are likely to churn. This model is not good for the objective we have. As we dont need our model to predict customers who are about to churn as this would impact the business more.\n- Observing test and training score, looks like model is not overfitted and performs better with test data\n\n<b> Verdict: This model is not suitable as we would miss 50% of customers who would likely to churn </b>"},{"metadata":{},"cell_type":"markdown","source":"## Conclusion:\n\nBased on the model's performance, Model built with <b><i> Random forest using PCA  </i> </b> helps us in identyfing upto 70% of the customers who are about to churn, though we have identified some customers would churn even though they are not would not cause much harm as providing the offers to them also helps in keeping the revenue up rather than losing the customers\n\nPCA helps us in reducing the dimensions which would further reduce the model building"},{"metadata":{},"cell_type":"markdown","source":"## Significant features that would help in identifying the churn"},{"metadata":{},"cell_type":"markdown","source":"### As per the model the following features are important\n\n1. arpu_8\n2. loc_og_t2t_mou_8\n3. loc_og_t2m_mou_8\n4. loc_og_t2f_mou_8\n5. spl_og_mou_8\n6. total_og_mou_8\n7. loc_ic_t2f_mou_8\n8. std_ic_t2f_mou_8\n9. total_ic_mou_8\n10. ic_others_8\n11. total_rech_num_8\n12. max_rech_amt_8\n13. last_day_rch_amt_8\n14. vol_2g_mb_8\n15. aon_yr\n16. arpu_6_7\n17. roam_og_mou_6_7\n18. std_og_mou_6_7"},{"metadata":{},"cell_type":"markdown","source":"### According to EDA\n\n1. arpu_8          \n2. total_rech_amt_8\n3. total_ic_mou_8  \n4. total_og_mou_8  \n\nThe above mentioned features are the top features which are corelated with Churn"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}