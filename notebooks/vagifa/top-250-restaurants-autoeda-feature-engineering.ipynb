{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 style='text-align:center; background: lightgreen; font-size: 3vw'>Top 250 Restaurants  AutoEDA & Feature Engineering</h1>"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, I am going to perform exploratory data analysis using two AutoEDA libraries that have recently caught my attention:\n1. `pandas-profiling`\n2. `dataprep`\n\nThen, I am going to perform some feature engineering to see if we are able to produce interesting features that may be used for modelling!\n\nHowever, before we actually begin our EDA and feature engineering, it is important to understand the origin of the data and the context of the data.\nIf you want to do so, here are the links explaining the context and data collection process of the 3 datasets:\n\n1. Future 50: https://www.restaurantbusinessonline.com/future-50-2020\n2. Top 250: https://www.restaurantbusinessonline.com/top-500-2020\n3. Independence 100: https://www.restaurantbusinessonline.com/top-100-independents-2020\n\n\n<h2> Here is a quick overview of the context of each dataset: </h2>\n\n1. Future 50: a measure of the fastest-growing restaurant concepts with annual sales between 20 million and 50 million dollars a year.\n2. Top 250: a measure of the largest restaurant concepts by U.S. systemwide sales, based on results from the 2019 calendar year.\n3. independence 100: a measure of the highest-grossing independent restaurants. Only restaurant concepts with no more than five locations are\nconsidered  \"independents” for the purpose of this list "},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"!pip install dataprep","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom pandas_profiling import ProfileReport\nfrom dataprep.eda import plot,plot_correlation\nfrom scipy.stats import pearsonr,spearmanr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"top_250 = pd.read_csv('/kaggle/input/restaurant-business-rankings-2020/Top250.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>EDA Process</h1>\n\n1. Use `pandas_profiling` to get a general overview of the data, and to look for features that may prove interesting for exploration\n\n2. Focus in on key features identified in the first step to gather interestings insights using `dataprep.eda`\n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<h1 style='text-align:center; background: lightgreen; font-size: 2vw'>Top 250 - EDA</h1>"},{"metadata":{},"cell_type":"markdown","source":"Before we actually being our EDA process, it's important to get a \"feel\" for the dataset. Let's view the schema for the dataset to do so:\n\n    Rank: Position in ranking\n    Restaurant: Name of restaurant\n    Content: Description, only for certain restaurants\n    Sales: in 2019 (in million dollars)\n    YOY_sales: Year on year sales increase in %\n    Units: Number of premises in US\n    YOY_units: Year on year premises increase in %\n    Headquarters: Place of the restaurant's headquarters\n    Segement_Category: Menu type and / or industry segment\n\n\n\n\n\nNow, let's get a general overview of the dataset using the `ProfileReport` class from `pandas_profiling`"},{"metadata":{"trusted":true},"cell_type":"code","source":"ProfileReport(top_250)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, so `pandas-profiling` has given us some really interesting insights about the data! While there is a lot, and I do recommend you to look over them yourselves, Here are the key points:\n\n1. **We have 9 features, with 6 being categorical and 3 being numeric.**\n\n   **Insight**: They make require further investigation, possibly using feature engineering tools to encode the values into numeric values that \n   can be interpreted by the model(or just use CatBoost :] )\n   \n\n2. **We have missing values in the Content and Headquarters features**\n\n\n3. **We have some strong linear(and non-linear) correlations**\n\n   In the interactions section, there seems to be high **linear** correlation between the Sales and Units features, according to the Pearson correlation coefficient.\n   However, note that just because something does not have a high Pearson correlation coefficient, it does not mean that it is not correlated. How so?\n>    change the filter from Pearson's R to Spearman's P\n\n   If you have 2 numeric features that are not linearly correlated, and if one(or both) of your features are ordinal features(E.g ranking, hierarchical class),\n   Then you can measure the strength and relationship between them using a correlation statistic.   \n   The most common is Spearman's Rank,which considers the ranks of the values for the two variables\n   \n   Spearman’s correlation is equivalent to calculating the Pearson correlation coefficient on the ranked data. So ρ will always be a value between -1 and 1. The further away ρ is from zero, the stronger the relationship between the two variables. The sign of ρ corresponds to the direction of the relationship. If it is positive, then as one variable increases, the other tends to increase. If it is negative, then as one variable increases, the other tends to decrease.\n\n You might want to use Spearman’s correlation if your data have a non-linear relationship (like an exponential relationship) or you have one or more outliers. However,  Spearman’s correlation is only appropriate if the relationship between your variables is monotonic, meaning that as one variable increases, the other tends to either increase or decrease (not both)\n \n![](http://sites.utexas.edu/sos/files/2017/06/Monotonic_both.png) \n\nSource: [UT Austin](http://sites.utexas.edu/sos/guided/inferential/numeric/bivariate/rankcor/)\n\n**Why does Spearman's correlation coefficient work with our data?**\n\n1. We have a non-linear relationship between Rank and Sales\n2. We have one ordinal feature(Rank)\n3. We have a monotonic relationship between 2 features(as Rank increases, Sales increases)"},{"metadata":{},"cell_type":"markdown","source":"# Comparing correlations(Do not always rely on the Pearson correlation coefficient!)\n\nFirst, let's plot `Rank` against `Sales`"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(top_250,x='Sales',y='Rank')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's compare the different coefficients:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pearson_correlation,_ = pearsonr(top_250['Sales'],top_250['Rank'])\nprint('The Pearson correlation coefficient is ' + str(pearson_correlation))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spearman_correlation,_ = spearmanr(top_250['Sales'],top_250['Rank'])\nprint('The Spearman correlation coefficient is ' + str(spearman_correlation))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So now it is clear to see that you should not drop a feature just because it has a low Pearson correlation.\nAlways make sure to check the Spearman coefficient, but only when:\n1. There is a non-linear relationship between at 2 features, and 1 feature is an ordinal feature\n2. The data is non-parametric(not assumed to come from prescribed models that are determined by a small number of parameters)\n\nRemeber than Spearman considers the ranks of the features, so essentially it is calculating the Pearson coefficient on ranked data."},{"metadata":{},"cell_type":"markdown","source":"# Now that we have done correlation 101, it's time to get dirty with data!"},{"metadata":{},"cell_type":"markdown","source":"First, I am going to drop the `Headquarters` and `Content` features, as they simply have too many missing values to be of any real use"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_250 = top_250.drop(columns=['Headquarters','Content'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am going to encode `YOY_Sales` and `YOY_Units` so that they are numerical in order to be able to discover insights about them"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_250['YOY_Sales'] = top_250['YOY_Sales'].apply(lambda x: x.strip('%')).astype(np.float64)\ntop_250['YOY_Units'] = top_250['YOY_Units'].apply(lambda x: x.strip('%')).astype(np.float64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's call `the plot_correlation` method of from `dataprep.eda` to gain some valuable insights:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_correlation(top_250)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we can see that `YOY_Sales` and `YOY_Units` are strongly correlated. Let's further examine this correlation:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(top_250,x='YOY_Sales',y='YOY_Units')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So here we can see a clear linear relationship between the two features. This could potentially be a useful feature for the modelling process\n\n**What can we confidently say:**\n1. There is a clear linear correlation between YOY_Sales and YOY_Units, so the more sales a business makes, the more it can expand."},{"metadata":{},"cell_type":"markdown","source":"# Analysing the categorical feature: Segment Category"},{"metadata":{},"cell_type":"markdown","source":"Let's analyse the `Segment Category` feature and see if we can gain useful insights from it as well:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(top_250,\"Segment_Category\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's break down our observations:\n\n1. We can see that the most popular segment category is a varied menu, and mexican being the 2nd most frequent\n2. From the word cloud, we see that the words \"casual\",\"service\",\"dining\" and \"quick\" are the most frequent works found in the segment category description. This shows us that most business have a common target of providing customers with a quick and casual service.\n3. Most restaurants used casual in their segment category description; also, not how \"quick\" and \"service\" have equal word frequencies; they probably are seen in a sentence together. Let us verify this: "},{"metadata":{"trusted":true},"cell_type":"code","source":"top_250.loc[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our assumption holds true!"},{"metadata":{},"cell_type":"markdown","source":"# Extended Analysis of numerical features"},{"metadata":{},"cell_type":"markdown","source":"Let's begin by exploring the sales feature in more depth:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(top_250,'Sales')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's similarly break down our analysis:\n1. Firstly, this feature is left skewed. This should be accounted for when modelling. Possibly a log/sqrt transformation may be required.\n2. The Normal q-q plot is a plot that helps us assess if a set of data plausibly came from some theoretical distribution such as a Normal or exponential. Essentially, it helps us determine of data came from a normal distrubution or not, and shows us if our data is skewed. So we can say that the normal distribution is not present in our feature and it is indeed a left tailed distribution with a heavy tail.\n\nTo read more, follow [this link](https://data.library.virginia.edu/understanding-q-q-plots/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(top_250,'Units')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar to Sales, we witness a non-normal and left-tailed distribution for Units"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"After our analysis, let's take away some key insights:\n\n1. We have linear relationships between key features, as well as non-linear relationships as well.\n2. We have data of different scales, so we should rescale features if using linear models.\n3. We should also transform a few features using log transformations and other transformations to try transform our data into a normal distribution.\n4. We should also encode the Segment category to see if it positiely benefits our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}