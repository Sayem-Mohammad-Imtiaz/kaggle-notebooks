{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Step 1: Importing Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data analysis tools\nimport pandas as pd\nimport numpy as np\n\n# Data Visualization Tools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Data Pre-Processing Libraries\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport category_encoders as ce\n\n# For Train-Test Split\nfrom sklearn.model_selection import train_test_split\n\n# Libraries for various Algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Metrics Tools\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import accuracy_score, f1_score\n\n#For Receiver Operating Characteristic (ROC)\nfrom sklearn.metrics import roc_curve ,roc_auc_score, auc\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 2: Loading the Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 3: Understanding the Structure of the Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 4: Data Pre-Processing\n\n   # a) Treating Missing Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> No Missing Values in the dataset. Hence no treatment for missing values required</b>\n\n\n# b) Finding and removing all the duplicated values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df[df.duplicated()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> The dataset does not have any duplicate values.</b>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# c) Checking for Imbalance\n\n<b>From the graph, it is clear that the class distribution is Imbalanced. The dataset has 80% samples of class 0 (employee is not leaving the job) and 20% samples of class 1(employee has decided to leave the job).</b>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df[\"Attrition\"])\nplt.xlabel(\"Class\")\nplt.ylabel(\"frequency\")\nplt.title(\"Checking imbalance\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 6: Defining the Tatget and Predictor Variables and Standard Scaling\n\n<b> If a featureâ€™s variance is more than the variance of other features, that particular feature might dominate other features in the dataset. This could affect the accuracy of predictions. Hence, we need to scale all the features to a standard centred scale. For this purpose, we use StandardScaler() method.</b>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df.drop('Attrition',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"em = pd.get_dummies(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"em.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Attrition']=LabelEncoder().fit_transform(df['Attrition'])\ny=df['Attrition']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(em,y, test_size=0.30, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 7: Fitting the dataset to various models\n\n<b>We will fit the dataset to various models and find out the best fit model among these.\n\nVarious models used in this notebook are:\n    \n\n1)  Logistic Regression\n\n2)  KNN                \n\n3)  Naive-Bayes       \n\n4)  SVM                   \n\n5)  Decision Tree         \n\n6)  Gradient Boosting     \n\n7)  Random Forest         \n\n8)  AdaBoost             \n\n9)  XGBoost    </b>           \n\n# 1) Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the model\n\nlogistic_Regression = LogisticRegression(max_iter=3000,random_state=0,class_weight=\"balanced\",solver = \"saga\")\nlogistic_Regression.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the model to the x_test\n\ny_pred = logistic_Regression.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding Accuracy\n\nlog = accuracy_score(y_pred,y_test)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\n\ncmlr=confusion_matrix(y_pred,y_test)\nprint(cmlr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification Report that computes various\n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(y_pred,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the ROC Curve\n\n\nprob_lr=logistic_Regression.predict_proba(x_test)\nauc_lr = roc_auc_score(y_test,prob_lr[:,1])\nfprlr,tprlr,_ = roc_curve(y_test,prob_lr[:,1])\nroc_auc=auc(fprlr,tprlr)\nplt.plot(fprlr,tprlr,label = \"AUC = %.2f\" % auc_lr)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for Logistic Regression\")\nplt.plot([0,1],[0,1],\"--\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2) KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the model\n\nknn = KNeighborsClassifier(n_neighbors=35)\nknn.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the model to the x_test\n\npred_knn = knn.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding Accuracy\n\nKNN = accuracy_score(pred_knn,y_test)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\n\ncm_knn=confusion_matrix(pred_knn,y_test)\nprint(cm_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification Report that computes various\n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(pred_knn,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the ROC Curve\n\nprob_knn= knn.predict_proba(x_test)\nauc_knn = roc_auc_score(y_test,prob_knn[:,1])\nfprknn,tprknn,_= roc_curve(y_test,prob_knn[:,1])\nroc_auc_knn=auc(fprknn,tprknn)\nplt.plot(fprknn,tprknn,label = \"AUC = %.2f\" % auc_knn)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for KNN\")\nplt.plot([0,1],[0,1],\"--\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3) Naive-Bayes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the model\n\ngnb=GaussianNB()\ngnb.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the model to the x_test\n\npred_gnb = gnb.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding Accuracy\n\nGNB = accuracy_score(pred_gnb,y_test)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\n\ncm_gnb=confusion_matrix(pred_gnb,y_test)\nprint(cm_gnb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification Report that computes various \n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(pred_gnb,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the ROC Curve\n\nprob_gnb= gnb.predict_proba(x_test)\nauc_gnb = roc_auc_score(y_test,prob_gnb[:,1])\nfprgnb,tprgnb,_= roc_curve(y_test,prob_gnb[:,1])\nroc_auc_gnb=auc(fprgnb,tprgnb)\nplt.plot(fprgnb,tprgnb,label = \"AUC = %.2f\" % auc_gnb)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for Naive-Bayes\")\nplt.plot([0,1],[0,1],\"--\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4) SVM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the model\n\nsvc = SVC(probability=True)\nsvc.fit(x_train,y_train)\n\n# Applying the model to the x_test\npred_svc = svc.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding Accuracy\n\nSVC = accuracy_score(pred_svc,y_test)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\n\ncm_svc=confusion_matrix(pred_svc,y_test)\nprint(cm_svc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification Report that computes various \n#metrics like Precision, Recall and F1 Score\n\nprint(classification_report(pred_svc,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the ROC Curve\n\nprob_svc= svc.predict_proba(x_test)\nauc_svc = roc_auc_score(y_test,prob_svc[:,1])\nfprsvc,tprsvc,_= roc_curve(y_test,prob_svc[:,1])\nroc_auc_svc=auc(fprsvc,tprsvc)\nplt.plot(fprsvc,tprsvc,label = \"AUC = %.2f\" % auc_svc)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for SVM\")\nplt.plot([0,1],[0,1],\"--\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5) Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the model\n\ndtree_en = DecisionTreeClassifier()\nclf = dtree_en.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the model to the x_test\n\npred_dt = clf.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding Accuracy\n\nDTREE = accuracy_score(pred_dt,y_test)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\n\ncm_dt=confusion_matrix(y_test,pred_dt)\nprint(cm_dt)\n\n# Classification Report that computes various \n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(y_test,pred_dt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the ROC Curve\n\nprob_dt= dtree_en.predict_proba(x_test)\nauc_dt = roc_auc_score(y_test,prob_dt[:,1])\nfprdt,tprdt,_= roc_curve(y_test,prob_dt[:,1])\nroc_auc_dt=auc(fprdt,tprdt)\nplt.plot(fprdt,tprdt,label = \"AUC = %.2f\" % auc_dt)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for Decision Tree\")\nplt.plot([0,1],[0,1],\"--\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6) Gradient Boosting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the model\n\nGBC=GradientBoostingClassifier(n_estimators=150)\nGBC.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the model to the x_test\n\nY_predict=GBC.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding Accuracy\n\ngbc = accuracy_score(y_test,Y_predict)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\n\ncm_gbc=confusion_matrix(y_test,Y_predict)\nprint(cm_gbc)\n\n# Classification Report that computes various \n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(y_test,Y_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the ROC Curve\n\nprob_GBC= GBC.predict_proba(x_test)\nauc_GBC = roc_auc_score(y_test,prob_GBC[:,1])\nfprGBC,tprGBC,_= roc_curve(y_test,prob_GBC[:,1])\nroc_auc_GBC=auc(fprGBC,tprGBC)\nplt.plot(fprGBC,tprGBC,label = \"AUC = %.2f\" % auc_GBC)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for Gradient Boosting\")\nplt.plot([0,1],[0,1],\"--\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7) Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the model\n\nrfc = RandomForestClassifier(n_estimators=30,criterion='gini',random_state=1,max_depth=10)\nrfc.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the model to the x_test\n\npred_rf= rfc.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding Accuracy\n\nRFC = accuracy_score(y_test,pred_rf)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\n\ncm_rf=confusion_matrix(pred_rf,y_test)\nprint(cm_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification Report that computes various \n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(pred_rf,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the ROC Curve\n\nprob_rfc= rfc.predict_proba(x_test)\nauc_rfc = roc_auc_score(y_test,prob_rfc[:,1])\nfprrfc,tprrfc,_= roc_curve(y_test,prob_rfc[:,1])\nroc_auc_rfc=auc(fprrfc,tprrfc)\nplt.plot(fprrfc,tprrfc,label = \"AUC = %.2f\" % auc_rfc)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for Random Forest\")\nplt.plot([0,1],[0,1],\"--\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8) AdaBoost (Entropy-Decision Tree)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the model. Base model is chosen to be Decision Tree\n\nmodel = DecisionTreeClassifier(criterion='entropy',max_depth=1,random_state=0)\nadaboost = AdaBoostClassifier(n_estimators=80, base_estimator=model,random_state=0)\nadaboost.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the model to the x_test\n\npred = adaboost.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding Accuracy\n\nada = accuracy_score(y_test,pred)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\n\ncm_ada=confusion_matrix(pred,y_test)\nprint(cm_ada)\n\n# Classification Report that computes various \n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(pred,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the ROC Curve\n\nprob_adaboost= adaboost.predict_proba(x_test)\nauc_adaboost = roc_auc_score(y_test,prob_adaboost[:,1])\nfpradaboost,tpradaboost,_= roc_curve(y_test,prob_adaboost[:,1])\nroc_auc_adaboost=auc(fpradaboost,tpradaboost)\nplt.plot(fpradaboost,tpradaboost,label = \"AUC = %.2f\" % auc_adaboost)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for AdaBoost (Entropy-Decision Tree)\")\nplt.plot([0,1],[0,1],\"--\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9) XGBoost ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the model\n\nxgb =  XGBClassifier(learning_rate =0.000001,n_estimators=1000,max_depth=5,min_child_weight=1,\n                     subsample=0.8,colsample_bytree=0.8,nthread=4,scale_pos_weight=1,seed=27)\nxgb.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the model to the x_test\n\n\npredxg = xgb.predict(x_test)\n\n# Finding Accuracy\nxg = accuracy_score(y_test,predxg)*100\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\n\ncm_xg=confusion_matrix(predxg,y_test)\nprint(cm_xg)\n\n# Classification Report that computes various \n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(predxg,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the ROC Curve\n\nprob_xgb= xgb.predict_proba(x_test)\nauc_xgb = roc_auc_score(y_test,prob_xgb[:,1])\nfprxgb,tprxgb,_= roc_curve(y_test,prob_xgb[:,1])\nroc_auc_xgb=auc(fprxgb,tprxgb)\nplt.plot(fprxgb,tprxgb,label = \"AUC = %.2f\" % auc_xgb)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for XGBoost\")\nplt.plot([0,1],[0,1],\"--\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 8: Choosing the Best model\n\nThere are various ways to evaluate a classification model. Some of them are:\n \n1) Accuracy\n    \n2) AUC\n    \n3) ROC\n    \n4) f1 Score\n\n\nI am evaluating with all these metrics in order to find the best fit model\n\n# Confusion Matrix\n\nA confusion matrix is an N X N matrix, where N is the number of classes being predicted. Confusion Matrix gives us a matrix as output and describes the complete performance of the model.\n\nThe correct predictions falls on the diagonal line of the matrix.\n\n4 important terms in Confusion Matrix:\n\n<b>True Positives</b>  : We predict YES and the actual output is also YES.\n\n<b>True Negatives</b>  : We predict NO and the actual output is NO.\n\n<b>False Positives(Type I Error)</b> : We predict YES but the actual output is NO.\n\n<b>False Negatives(Type II error)</b> : We predict NO but the actual output is YES.\n\n<b>The Confusion matrix in itself is not a performance measure, but almost all of the performance metrics are based on Confusion Matrix.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1) Accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy values for all the models\nprint(\"1)  Logistic Regression    :\",round(log, 2))\nprint(\"2)  KNN                    :\",round(KNN, 2))\nprint(\"3)  Naive-Bayes            :\",round(GNB, 2))\nprint(\"4)  SVM                    :\",round(SVC, 2))\nprint(\"5)  Decision Tree          :\",round(DTREE, 2))\nprint(\"6)  Gradient Boosting      :\",round(gbc, 2))\nprint(\"7)  Random Forest          :\",round(RFC, 2))\nprint(\"8)  AdaBoost               :\",round(ada, 2))\nprint(\"9)  XGBoost                :\",round(xg, 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nHere, <b>Gradient Boosting has the highest accuracy rate.</b> But during Data visualization step, we observed that the <b>class distribution is Imbalanced</b>. The dataset has 80% samples of class 0 (the employee is not leaving their job) and 20% samples of class 1(The employee has decided to leave their job). This is the reason why most of the models are getting accuracy above 90% by simply predicting every training sample belonging to class 0. But, when we apply this model to a new test-set, then the <b>test accuracy would drop to less than 60%.</b>\n\n<b>In this case, Accuracy metric proves to be a poor indicator of model performance. Therefore, we need to consider other metrics before deciding the best model.</b>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2) Area Under Curve (AUC)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Area Under the Curve(AUC) of all the models\nprint('Area under the curve for Logistic Regression :',round(roc_auc, 2))\nprint('Area under the curve for KNN                 :',round(roc_auc_knn, 2))\nprint('Area under the curve for Naive-Bayes         :',round(roc_auc_gnb, 2))\nprint('Area under the curve for SVM                 :',round(roc_auc_svc, 2))\nprint('Area under the curve for Decision Tree       :',round(roc_auc_dt, 2))\nprint('Area under the curve for Gradient Boosting   :',round(roc_auc_GBC, 2))\nprint('Area under the curve for Random Forest       :',round(roc_auc_rfc, 2))\nprint('Area under the curve for AdaBoost            :',round(roc_auc_adaboost, 2))\nprint('Area under the curve for XGBoost             :',round(roc_auc_xgb, 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The area under the curve (AUC), is an aggregated measure of performance of a binary classifier on all possible threshold values. AUC calculates the area under the ROC curve, and therefore it is between 0 and 1.<b> For any classifier, the higher the AUC of a model the better it is.\n\n\nHere, <b>Gradient Boosting </b> have the highest AUC value. Hence, based on the AUC values, Gradient Boosting is the best fit model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3) ROC Curve","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#ROC Curve for all models\nplt.figure(figsize = (20,10))\nplt.plot(fprlr,tprlr,label = \"Logistic Regression\")\nplt.plot(fprknn,tprknn,label = \"KNN\")\nplt.plot(fprgnb,tprgnb,label = \"Naive-Bayes\")\nplt.plot(fprsvc,tprsvc,label = \"SVM\")\nplt.plot(fprdt,tprdt,label = \"Decision Tree\")\nplt.plot(fprGBC,tprGBC,label = \"Gradient Boosting\",color='black')\nplt.plot(fprrfc,tprrfc,label = \"Random Forest\",color='yellow')\nplt.plot(fpradaboost,tpradaboost,label = \" AdaBoost\")\nplt.plot(fprxgb,tprxgb,label = \"XGBoost\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.legend(loc=\"lower right\", fontsize=10)\nplt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The <b>Receiver Operating Characteristic (ROC)</b> curve is plot which shows the performance of a binary classifier as function of its cut-off threshold. ROC curve is one of the most effective evaluation metrics because it visualizes the accuracy of predictions for a whole range of cutoff values. It essentially shows the true positive rate (TPR) against the false positive rate (FPR) for all possible threshold values. <b>A model is said to be the best model when the ROC is close to the upper left corner.</b>\n\nLooking at the ROC curve plot above, the <b>Naive-Bayes, has the curve that is closest to the upper left corner. Hence, based on the ROC plot, Naive-Bayes is the best fit model.</b>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4) F1-Score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# f1_score of all models\nprint(\"1)  Logistic Regression    :\",round(f1_score(y_pred,y_test), 2))\nprint(\"2)  KNN                    :\",round(f1_score(pred_knn,y_test), 2))\nprint(\"3)  Naive-Bayes            :\",round(f1_score(pred_gnb,y_test), 2))\nprint(\"4)  SVM                    :\",round(f1_score(pred_svc,y_test), 2))\nprint(\"5)  Decision Tree          :\",round(f1_score(pred_dt,y_test), 2))\nprint(\"6)  Gradient Boosting      :\",round(f1_score(Y_predict,y_test), 2))\nprint(\"7)  Random Forest          :\",round(f1_score(pred_rf,y_test), 2))\nprint(\"8)  AdaBoost               :\",round(f1_score(pred,y_test), 2))\nprint(\"9)  XGBoost                :\",round(f1_score(predxg,y_test), 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Precision</b>           - It is the number of True Positive divided by the number of positive results predicted by the classifier.\n\n<b>Recall/ Sensitivity</b> - It is the number of True Positives divided by the number of all relevant samples\n\n<b>F1 Score</b>            - F1 Score is the Harmonic Mean between precision and recall.\n\nF1 Score tells how precise the classifier is (how many values it classifies correctly).\n\n<b>The greater the F1 Score, the better is the performance of our model.</b>\n\n\nHere, <b>Naive-Bayes</b> has the highest f1_score. Hence, based on the f1_score, Naive-Bayes is the best fit model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 5)Type I Error","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Accessing the False Positives of all models from their confusion Matrix\nprint(\"1)  Logistic Regression    :\",cmlr[0][1])\nprint(\"2)  KNN                    :\",cm_knn[0][1])\nprint(\"3)  Naive-Bayes            :\",cm_gnb[0][1])\nprint(\"4)  SVM                    :\",cm_svc[0][1])\nprint(\"5)  Decision Tree          :\",cm_dt[0][1])\nprint(\"6)  Gradient Boosting      :\",cm_gbc[0][1])\nprint(\"7)  Random Forest          :\",cm_rf[0][1])\nprint(\"8)  AdaBoost               :\",cm_ada[0][1])\nprint(\"9)  XGBoost                :\",cm_xg[0][1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"False Positives(Type I Error) occurs when we incorrectly reject a true hypothesis.<b>Lower the value of False Positives, better is the model</b>. This is because, while predicting, <b>if we predict that an employee is not going to leave the job, but later he/she actually leaves the job, then this kind of wrong prediction could further increase Attrition Rate to an alarming range.</b>\n\nThe False Positives(Type I Error) for all the models can be accessed from the confusion matrix.\n\n\n<b>Gradient Boosting algorithm has the least number of False Positives(Type I Error). Hence, based on the False Positives(Type I Error), Gradient Boosting is the best fit model.</b>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Step 9: Finalizing the Best Model\n\nAfter all the comparison using 5 different metrics:\n\nwhen considering the metrics AUC, ROC and Type I error, Gradient Boosting is found to be the best model.\n\nwhen considering the metrics Accuracy and F1 Score, Random Forest is found to be the best model.\n\n# Finally, <u><b>Gradient Boosting</b></u> algorithm proves to be the best model for the Employee Attrition.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}