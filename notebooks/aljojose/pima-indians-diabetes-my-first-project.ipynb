{"nbformat_minor":0,"metadata":{"_change_revision":0,"language_info":{"file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","nbconvert_exporter":"python","name":"python","version":"3.6.0","pygments_lexer":"ipython3"},"_is_fork":false,"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"cells":[{"outputs":[],"metadata":{"_cell_guid":"4b99af97-db32-4f0e-592b-4f26399f89f2","_uuid":"0a38a154dd23f471fdfa51f65e63c08be913cc9a"},"execution_count":null,"cell_type":"markdown","source":"Aljo Jose - 04 June 2017\n# Database - Pima Indians Diabetes\n**Motivation - Help medical professionals to make diagnosis easier by bridging gap between huge datasets and human knowledge. Apply machine learning techniques for given classification in a dataset that describes a population that is under a high risk of the onset of diabetes.**"},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"584f29ee-0e73-bfde-5a4b-c9f9aa581761","_uuid":"ea3522c693c6434bce4c9f14806fe2c451e910c2"},"execution_count":null,"cell_type":"code","source":"# Importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns"},{"outputs":[],"metadata":{"_cell_guid":"14382e04-10b0-6bcd-4849-636773acd6a3","_uuid":"d4422323efe7220de96bb50341d079e4b0e99164"},"execution_count":null,"cell_type":"markdown","source":"##1.1 Load dataset and  summarize "},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"e673ce84-3ac0-f987-d45a-874e17594b28","_uuid":"7b53284d367d2a4adda97ece9d043dfd573ac5d0"},"execution_count":null,"cell_type":"code","source":"# Loading dataset and view a few records.\ndataset = pd.read_csv('../input/diabetes.csv')\ndataset.shape"},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"23e9f21a-33e1-dfd1-b51d-6a5182e2fd9c","_uuid":"5940b608354d419c57bfce14d4f0efaec098c9af"},"execution_count":null,"cell_type":"code","source":"dataset.head()"},{"outputs":[],"metadata":{"_cell_guid":"225defc7-6fe8-a10d-26c9-8e0ea7bc3331","_uuid":"ccb01abd57d2bc5b5ea1efe0430300c58cb6ba26"},"execution_count":null,"cell_type":"markdown","source":"Some of the columns ( SkinThickness, Insulin) having incorrect 0. Let's try to find columns having 0 values."},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"832ca30f-84c5-e9b9-c840-1af2deabe45f","_uuid":"ab882f3fe26150013526e38c3d96122b38ab632d"},"execution_count":null,"cell_type":"code","source":"dataset.describe()"},{"outputs":[],"metadata":{"_cell_guid":"4cdc5396-d26b-7441-6004-17d4fa39cc67","_uuid":"1cecbf2767b1ba0c77313836dae3ba16a815a19f"},"execution_count":null,"cell_type":"markdown","source":"Attributes Glucose,  BloodPressure, SkinThickness, Insulin, BMI having non-realistic value (0). We can try to expose dataset with and without handling 0 value and observe performance."},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"70abdcfd-779f-e195-65b3-b0c4b4ed6be5","_uuid":"5810ed0edf6318cd69283d10a105fff83b6658c6"},"execution_count":null,"cell_type":"code","source":"dataset.info()"},{"outputs":[],"metadata":{"_cell_guid":"8a17e19a-46de-ecce-5bf2-81ca0789b502","_uuid":"7918e84c1023edd2a28f6b13fe4b19db5d446da2"},"execution_count":null,"cell_type":"markdown","source":"Observed columns DiabetesPedigreeFunction and BMI having float datatype. All others are of integer type."},{"outputs":[],"metadata":{"_cell_guid":"3a693ba8-5f89-cda4-1ad1-28cae336f66b","_uuid":"00348dec156b11ae11e68e92ef0bdda8e28967a9"},"execution_count":null,"cell_type":"markdown","source":"## 1.2 Visualize dataset"},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"4fc55ac1-9e41-70df-9b76-216ea4696c92","_uuid":"8bc261e2a8f45e9cea9c40d5b2e005ee7fbf2cf2"},"execution_count":null,"cell_type":"code","source":"# Histogram\ndataset.hist(figsize=(10,8))"},{"outputs":[],"metadata":{"_cell_guid":"dad715a9-3ddd-744d-3db4-3e21fd48f696","_uuid":"c1d74e84d8b2f9f29c832802ed7d02dcae959c00"},"execution_count":null,"cell_type":"markdown","source":"Attributes BMI, BloodPressure, Glucose are found to be normally distributed.\n<br>BMI and BloodPressure nearly have Gaussian distribution.\n<br>Age, DiabetesPedigreeFunction, Insulin, Pregnancies found to be exponentially distributed."},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"dc23d6a0-d425-fb95-a113-dceebeef902b","_uuid":"44081a9d9fe872a46acf1434ca2adbe6532f4290"},"execution_count":null,"cell_type":"code","source":"# Box plot\ndataset.plot(kind= 'box' , subplots=True, layout=(3,3), sharex=False, sharey=False, figsize=(10,8))"},{"outputs":[],"metadata":{"_cell_guid":"0e53b394-f5e3-eb53-aaf8-19d517d3d256","_uuid":"e36cf9f11c04e39071fd5ee803ab896299d70ee0"},"execution_count":null,"cell_type":"markdown","source":"Observed that spread of attributes is quite different. Attributes Age, Insulin appear to be quite skewed towards smaller values. <br>Scaling on dataset can be applied during data pre-processing."},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"f1106d8b-cfb3-d46a-c10c-bc0a8b5368df","_uuid":"0a7e5f47b2d77db9b8e3424b4b7a96ae95bc04d2"},"execution_count":null,"cell_type":"code","source":"# Correlation plot\nCorr=dataset[dataset.columns].corr() \nsns.heatmap(Corr, annot=True)"},{"outputs":[],"metadata":{"_cell_guid":"94579afc-8d21-020a-f235-fb27f9918afb","_uuid":"fdedce55f6acfed6aeb093ee44c4c663f3d5df1b"},"execution_count":null,"cell_type":"markdown","source":"Observed that attributes BloodPressure, SkinThickness are not much related to outcome.  <br> Feature extraction can be tried to observe performance."},{"outputs":[],"metadata":{"_cell_guid":"db570a9f-cb9d-0a50-eea1-e7d2faf2e5bb","_uuid":"9b46eda26bf12c5a63ab3fb99d9e74433449976e"},"execution_count":null,"cell_type":"markdown","source":"## 1.3 Data Pre-processing"},{"outputs":[],"metadata":{"_cell_guid":"2a5a6882-5e0b-674e-2ac3-ebc183ad789f","_uuid":"9261877aba5320ca17a77547ffe90e67e6783eca"},"execution_count":null,"cell_type":"markdown","source":"Note : Replaced 0 values by mean, but no performance improvement was observed while evaluating models. <br> Dropped rows with 0 values, performance seems to be improved. But dataset reduces to half. <br> Hence commented below lines."},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"93337685-ffab-784c-512a-f90ac419782c","_uuid":"118f08120481efebebdfaf5c19e1a8fa83a828ae"},"execution_count":null,"cell_type":"code","source":"# Data preprocessing - replace zeroes with mean or drop records with 0 values.\n#attributes_to_replace_zero =list(dataset.columns[1:6])      # list all column names. \n#dataset[attributes_to_replace_zero] = dataset[attributes_to_replace_zero].replace(0, np.NaN)\n#dataset.fillna(dataset.mean(), inplace=True) \n#dataset.dropna(inplace=True)"},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"a6429ad0-65ed-49f8-ab53-fa962faa16ad","_uuid":"9218b51c0648f80e4ad71e60f363ea36ad669b34"},"execution_count":null,"cell_type":"code","source":"# Split into Input and Output.\nattributes = list(dataset.columns[:8])\nX = dataset[attributes].values \ny= dataset['Outcome'].values"},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"d02436bb-3f48-8d29-d1f0-5ede441571db","_uuid":"832d64f1a60f213c8d93496bf58fbfa0baa5a70c"},"execution_count":null,"cell_type":"code","source":"# Scale input dataset.\nfrom sklearn.preprocessing import StandardScaler \nsc_X = StandardScaler() \nX = sc_X.fit_transform(X) "},{"outputs":[],"metadata":{"_cell_guid":"8a02fd4b-0dcc-dc26-a1c7-2a4689fb2455","_uuid":"75f34b8f203326fa95494ad8f9d1141bfd88ebb4"},"execution_count":null,"cell_type":"markdown","source":"Note : Normalization reduced performance while evaluating models. Hence code disabled."},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"de5a714d-9bc1-1d53-4872-a47331bbb440","_uuid":"de3d5fee8a01d868de1ee22fbaba1d0e5c86f9fa"},"execution_count":null,"cell_type":"code","source":"#Performacne reduces with normalizer\n#from sklearn import preprocessing\n#X = preprocessing.normalize(X)"},{"outputs":[],"metadata":{"_cell_guid":"1a721343-2d04-8ffd-48c6-681b64cf13f8","_uuid":"5f7c46877b01ac0c01dd13520113771c487e7d0a"},"execution_count":null,"cell_type":"markdown","source":"Split into train and test sets."},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"b19052a4-f7b8-4f24-dc31-ae93dfbb8d45","_uuid":"6d40b9d9504283af4bc9fe33e9f6462cac97f4b4"},"execution_count":null,"cell_type":"code","source":"# Split into train and test sets.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state =0)"},{"outputs":[],"metadata":{"_cell_guid":"048d2e66-f74a-e330-dba1-28ac9b3b8588","_uuid":"ec414244723d329f19348c6c39967e445d4afff2"},"execution_count":null,"cell_type":"markdown","source":"Note : Applied feature selection, but not much change in performance. So code lines disabled."},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"a19c5b15-8f82-43c8-522e-83ccc16f9d22","_uuid":"e4e5ad5016168df7b25933f4e6b7a30dcf261407"},"execution_count":null,"cell_type":"code","source":"# Applying Kernel PCA ( Not much change in performance)\n#from sklearn.decomposition import PCA\n#pca = PCA(n_components = 6)\n#X_train = pca.fit_transform(X_train)\n#X_test = pca.transform(X_test)\n#explained_variance = pca.explained_variance_ratio_"},{"outputs":[],"metadata":{"_cell_guid":"e4b3209a-52ee-dc1c-a330-92554c4f5384","_uuid":"a37498f7921d560706d8e49e0f8f74fbb421dc2a"},"execution_count":null,"cell_type":"markdown","source":"# 1.4  Evaluate models."},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"7cd1271b-82a6-48ab-7846-b487b4519a2e","_uuid":"b6b6f4e442071b5823944e83e7a3518b5078dae1"},"execution_count":null,"cell_type":"code","source":"\n# Import suite of algorithms.\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\nfrom sklearn.calibration import CalibratedClassifierCV"},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"4725023a-38a4-5fd6-8cf4-d2c5f838b3b4","_uuid":"7c55fbd98a087f8094d6321ab865aa2367648711"},"execution_count":null,"cell_type":"code","source":"# Create objects of required models.\nmodels = []\nmodels.append((\"LR\",LogisticRegression()))\nmodels.append((\"GNB\",GaussianNB()))\nmodels.append((\"KNN\",KNeighborsClassifier()))\nmodels.append((\"DecisionTree\",DecisionTreeClassifier()))\nmodels.append((\"LDA\",  LinearDiscriminantAnalysis()))\nmodels.append((\"QDA\",  QuadraticDiscriminantAnalysis()))\nmodels.append((\"AdaBoost\", AdaBoostClassifier()))\nmodels.append((\"SVM Linear\",SVC(kernel=\"linear\")))\nmodels.append((\"SVM RBF\",SVC(kernel=\"rbf\")))\nmodels.append((\"Random Forest\",  RandomForestClassifier()))\nmodels.append((\"Bagging\",BaggingClassifier()))\nmodels.append((\"Calibrated\",CalibratedClassifierCV()))\nmodels.append((\"GradientBoosting\",GradientBoostingClassifier()))\nmodels.append((\"LinearSVC\",LinearSVC()))\nmodels.append((\"Ridge\",RidgeClassifier()))"},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"6f1d26e4-ae7f-b96d-80b7-1f114a65313e","_uuid":"af9c0d0e56928ef28b351781b1b819d7c28c718f"},"execution_count":null,"cell_type":"code","source":"\n# Find accuracy of models.\nresults = []\nfor name,model in models:\n    kfold = KFold(n_splits=10, random_state=0)\n    cv_result = cross_val_score(model,X_train,y_train, cv = kfold,scoring = \"accuracy\")\n    results.append(tuple([name,cv_result.mean(), cv_result.std()]))\n  \nresults.sort(key=lambda x: x[1], reverse = True)    \nfor i in range(len(results)):\n    print('{:20s} {:2.2f} (+/-) {:2.2f} '.format(results[i][0] , results[i][1] * 100, results[i][2] * 100))\n\n"},{"outputs":[],"metadata":{"_cell_guid":"2d6061a7-b5de-25f6-6f09-dedc474b6d20","_uuid":"92158ea5e87aec5b89d9c0ca1a30960644795ee8"},"execution_count":null,"cell_type":"markdown","source":"#1.5 Optimize peformance of best model."},{"outputs":[],"metadata":{"_cell_guid":"8fb78822-ad36-1b0f-26a3-d2ef6dfbb759","_uuid":"ae9a17fdc3e4406fd946f3ef0c346ca26e47762d"},"execution_count":null,"cell_type":"markdown","source":"Linear classifiers performs well. So I have added all possible classifiers to suite of algorithms.\n<br> SVM Linear seems performs best. Now let us try to find the optimistic parameters for SVM using GridSearchCV. "},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"636203a0-cd78-c489-ec48-28c3660f46e9","_uuid":"1753c69b440a7554edc68652beb6f1e642999544"},"execution_count":null,"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nmodel = SVC()\nparamaters = [\n             {'C' : [0.01, 0.1, 1, 10, 100, 1000], 'kernel' : ['linear']}                                       \n             ]\ngrid_search = GridSearchCV(estimator = model, \n                           param_grid = paramaters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_ \nbest_parameters = grid_search.best_params_  \n\nprint('Best accuracy : ', grid_search.best_score_)\nprint('Best parameters :', grid_search.best_params_  )"},{"outputs":[],"metadata":{"_cell_guid":"58ad9ab9-0a9b-7849-9e1b-ddf3fccbdf99","_uuid":"483b3d852ab927ff622bb68bcd4c64f520128cc3"},"execution_count":null,"cell_type":"markdown","source":"#1.6 Finalize model "},{"outputs":[],"metadata":{"_cell_guid":"2e05be7d-46e8-e6d5-6bfd-b93ac7cda9e9","_uuid":"8f19b9a01168be26be73ed7486955bef981d2172"},"execution_count":null,"cell_type":"markdown","source":"**Selected SVM  model with parameters C= 0.01 and kernel = 'linear'.**"},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"faff9a85-7427-cc4d-bc67-6e23af154a44","_uuid":"749771b84bf8c681d375d5bee8fc2715561eaaf4"},"execution_count":null,"cell_type":"code","source":"\n# Predict output for test set. \nfinal_model = SVC(C = 0.1, kernel = 'linear')\nfinal_model.fit(X_train, y_train)\ny_pred = final_model.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncf = confusion_matrix(y_test, y_pred)\nprint(cf)\nprint(accuracy_score(y_test, y_pred) * 100) \n\n#Update. 19Jun2017. Included classiifcation report.\nfrom sklearn.metrics import classification_report\nreport = classification_report(y_test, y_pred)\nprint(report)"},{"outputs":[],"metadata":{"_cell_guid":"6f3956cb-bfb3-ddd6-493a-040b6d948ac6","_uuid":"c61f211998ab99c0121ab369cfa7e6227e7b6000"},"execution_count":null,"cell_type":"markdown","source":"**Conclusion :- Observed accuracy of 82.46% on test set using SVM linear model.**"},{"outputs":[],"metadata":{"_cell_guid":"bd946b0f-1c95-4f67-9e8f-09f5e92a204c","_uuid":"c589a500ba62ef6c78e8b407b648fd72a5df2802"},"execution_count":null,"cell_type":"markdown","source":"***Thank you very much for reading the post. This work is just an attempt to apply the concepts I learned. Please suggest improvements if any.***"}],"nbformat":4}