{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Topic Modelling Using Non-Negative Matrix Factorisation","metadata":{}},{"cell_type":"markdown","source":"**About the Dataset:**\n    \n    This dataset is a collection of posts from the subreddit r/VaccineMyths. \n    The dataset is unfiltered and may contain phrases that may be considered offensive by some. \n\n    The dataset has eight columns:\n        title - relevant for posts\n        score - relevant for posts - based on impact, number of comments\n        id - unique id for posts/comments\n        url - relevant for posts - url of post thread\n        commns_num - relevant for post - number of comments to this post\n        created - date of creation\n        body - relevant for posts/comments - text of the post or comment\n        timestamp - timestamp\n\n    For this notebook, I will be running the dataset through Non-Negative Matrix Factorisation (NMF) Topic Modelling. NMF is an unsupervised dimensionality-reduction technique by decomposing vectors into lower dimensional representation.","metadata":{}},{"cell_type":"markdown","source":"### **Import Libraries**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom datetime import date, timedelta\nfrom collections import Counter\nfrom operator import itemgetter\nimport os\nimport warnings\n\n#Visualisation Library\nimport matplotlib.pyplot as plt\nimport cufflinks as cf\nimport seaborn as sns\nfrom wordcloud import WordCloud \n\n#Preprocessing Libraries\nfrom nltk.corpus import stopwords\nimport string\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom gensim.utils import simple_preprocess\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#Topic Modelling Libraries\n\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer, TfidfTransformer\nfrom gensim.models.coherencemodel import CoherenceModel\nfrom gensim.corpora.dictionary import Dictionary\nfrom gensim.models.nmf import Nmf\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-08T00:58:39.759634Z","iopub.execute_input":"2021-06-08T00:58:39.760056Z","iopub.status.idle":"2021-06-08T00:58:44.733074Z","shell.execute_reply.started":"2021-06-08T00:58:39.759967Z","shell.execute_reply":"2021-06-08T00:58:44.731878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\ncf.go_offline()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T00:58:44.735013Z","iopub.execute_input":"2021-06-08T00:58:44.735722Z","iopub.status.idle":"2021-06-08T00:58:44.789951Z","shell.execute_reply.started":"2021-06-08T00:58:44.735663Z","shell.execute_reply":"2021-06-08T00:58:44.788904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\", category=DeprecationWarning)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T00:58:44.792553Z","iopub.execute_input":"2021-06-08T00:58:44.793546Z","iopub.status.idle":"2021-06-08T00:58:44.800677Z","shell.execute_reply.started":"2021-06-08T00:58:44.793496Z","shell.execute_reply":"2021-06-08T00:58:44.79964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Import the Dataset**","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv('../input/reddit-vaccine-myths/reddit_vm.csv', error_bad_lines=False);\ndataset.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-08T00:58:44.802586Z","iopub.execute_input":"2021-06-08T00:58:44.803348Z","iopub.status.idle":"2021-06-08T00:58:44.854921Z","shell.execute_reply.started":"2021-06-08T00:58:44.803295Z","shell.execute_reply":"2021-06-08T00:58:44.853895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T00:58:44.856209Z","iopub.execute_input":"2021-06-08T00:58:44.856655Z","iopub.status.idle":"2021-06-08T00:58:44.887779Z","shell.execute_reply.started":"2021-06-08T00:58:44.856589Z","shell.execute_reply":"2021-06-08T00:58:44.886562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"#Creation of StopWords\n\nstop_words = stopwords.words('english') #Call the StopWords Function from the Library\nstop_words.extend(['from', 'subject', 're', 'edu', 'use', 'op']) #I am removing some extra words like 'OP' and 'Edu' by extending the stopwords list\nstop_words[0:5] #Sample of StopWords","metadata":{"execution":{"iopub.status.busy":"2021-06-08T00:58:44.889492Z","iopub.execute_input":"2021-06-08T00:58:44.889922Z","iopub.status.idle":"2021-06-08T00:58:44.91247Z","shell.execute_reply.started":"2021-06-08T00:58:44.889876Z","shell.execute_reply":"2021-06-08T00:58:44.911221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Functions to Preprocess \n\ndef pre_process(s):\n    s = s.str.lower()\n    s = s.str.replace(r'(?i)\\brt\\b', \"\")\n    s = s.str.replace(' via ',\"\") \n    s = s.replace(r'@\\w+', \"\", regex = True)\n    s = s.replace(r'http\\S+', '', regex = True)\n    s = s.replace(r'www.[^ ]+', '', regex = True)\n    s = s.replace(r'[0-9]+', '', regex = True)\n    s = s.replace(r'''[¬!\"#$%&()*+,-./:;<=>?@[\\]’^'_`\\{|}~]''', '', regex=True)\n    return s\n\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef lemmatizing(words):\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef final_text(words):\n     return ' '.join(words)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-08T00:58:44.914034Z","iopub.execute_input":"2021-06-08T00:58:44.914644Z","iopub.status.idle":"2021-06-08T00:58:44.924872Z","shell.execute_reply.started":"2021-06-08T00:58:44.914593Z","shell.execute_reply":"2021-06-08T00:58:44.92348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Call the Functions on the Data (Plotting)**","metadata":{}},{"cell_type":"code","source":"dataset.body = pre_process(dataset['body']) \ndataset = dataset.dropna(subset = ['body']) #Drop Empty Rows\n\ndataset['token'] = remove_stopwords(dataset['body']) \ndataset['token'] = dataset['token'].apply(lambda x: lemmatizing(x)) \ndataset['body_combined_text'] = dataset['token'].apply(lambda x: final_text(x))\n\ndataset[dataset['body_combined_text'].duplicated(keep=False)].sort_values('body_combined_text').head() #View Duplicates\ndataset = dataset.drop_duplicates(['body_combined_text']) #Remove Duplicates\n\n#Plot Comments Per Month\n\ndataset['date'] = pd.to_datetime(dataset['timestamp'])\ndataset['Hour'] = dataset['date'].apply(lambda x: x.hour)\ndataset['Month'] = dataset['date'].apply(lambda x: x.month)\ndataset['Month'] = dataset['Month'].replace({1:'Jan',2:'Feb',3:'Mar',4:'Apr',5:'May',6:'Jun', 7:'Jul',8:'Aug',9:'Sep',10:'Oct',11:'Nov',12:'Dec'})\ndataset_month = dataset.Month.value_counts().reindex(['Jan', 'Feb', 'Mar', 'Apr', \"May\", 'Jun','Jul', \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"])\ndataset_month.iplot()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T00:58:44.928773Z","iopub.execute_input":"2021-06-08T00:58:44.929334Z","iopub.status.idle":"2021-06-08T00:58:48.602635Z","shell.execute_reply.started":"2021-06-08T00:58:44.92929Z","shell.execute_reply":"2021-06-08T00:58:48.601505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Most Common Words in the Dataset**","metadata":{}},{"cell_type":"code","source":"a = dataset['token']\na = [x for i in a for x in i]\ntop_20 = pd.DataFrame(Counter(a).most_common(20), columns=['word', 'freq']) #Check Word Frequency via Dataframe sorted by mos frequent.\nprint(top_20)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T00:58:48.604721Z","iopub.execute_input":"2021-06-08T00:58:48.60516Z","iopub.status.idle":"2021-06-08T00:58:48.622211Z","shell.execute_reply.started":"2021-06-08T00:58:48.605115Z","shell.execute_reply":"2021-06-08T00:58:48.621204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_of_unique_words = len(set(a)) #Check Number of Unique Words in the Dataset.\nprint(\"There are \" + str(no_of_unique_words) + \" unique words in the dataset.\")","metadata":{"execution":{"iopub.status.busy":"2021-06-08T00:58:48.623621Z","iopub.execute_input":"2021-06-08T00:58:48.623928Z","iopub.status.idle":"2021-06-08T00:58:48.639739Z","shell.execute_reply.started":"2021-06-08T00:58:48.62389Z","shell.execute_reply":"2021-06-08T00:58:48.638509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Calculating the number of topics (k)**\n\nNMF is an unsupervised machine learning techniques. All we need as input are the dataset and the number of topics (k). For this notebook, I will\ncalculate the coherence score to determine the appropriate number of topics. As such, I am running NMF against the range number of topics I have picked. As you can see below, I made a range of 5, 55. The k with the highest number of coherence score is the optimum number of k for this given dataset.","metadata":{}},{"cell_type":"code","source":"#Functions For Analysit\n\ndef top_words(topic, n_top_words):\n    return topic.argsort()[:-n_top_words - 1:-1]  \n\n\ndef topic_table(model, feature_names, n_top_words):\n    topics = {}\n    for topic_idx, topic in enumerate(model.components_):\n        t = (topic_idx)\n        topics[t] = [feature_names[i] for i in top_words(topic, n_top_words)]\n    return pd.DataFrame(topics)\n\ncoherence_scores = []\n\ndef find_cv(): #Find Number of K\n    for num in topic_nums:\n        nmf = Nmf(corpus=corpus, num_topics=num, id2word=dictionary,normalize=True)\n        cm = CoherenceModel(model=nmf, texts=texts, dictionary=dictionary, coherence='c_v')\n        coherence_scores.append(round(cm.get_coherence(), 5))\n\ndef compare_cv():\n    for m, cv in zip(topic_nums, coherence_scores):\n        print(\"K =\", m, \" CV: \", round(cv, 2))\n    scores = list(zip(topic_nums, coherence_scores))\n    best_cv = sorted(scores, key=itemgetter(1), reverse=True)[0][0]\n    print('\\n')\n    return best_cv","metadata":{"execution":{"iopub.status.busy":"2021-06-08T00:58:48.641511Z","iopub.execute_input":"2021-06-08T00:58:48.641956Z","iopub.status.idle":"2021-06-08T00:58:48.654962Z","shell.execute_reply.started":"2021-06-08T00:58:48.641908Z","shell.execute_reply":"2021-06-08T00:58:48.654189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_nums = list(np.arange(5, 55 + 1, 3)) #The range that we will run NMF on\ntexts = dataset.token\ndictionary = Dictionary(texts) #create dictionary \ndictionary.filter_extremes(no_below=5, no_above=0.8, keep_n=2000) \ncorpus = [dictionary.doc2bow(text) for text in texts] ","metadata":{"execution":{"iopub.status.busy":"2021-06-08T00:58:48.656324Z","iopub.execute_input":"2021-06-08T00:58:48.656669Z","iopub.status.idle":"2021-06-08T00:58:48.782993Z","shell.execute_reply.started":"2021-06-08T00:58:48.65664Z","shell.execute_reply":"2021-06-08T00:58:48.781967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Call the Functions\nfind_cv()  \n#Store the return value of compare_cv function to best_cv to use in the NMF\nbest_cv = compare_cv()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-08T00:58:48.784231Z","iopub.execute_input":"2021-06-08T00:58:48.784555Z","iopub.status.idle":"2021-06-08T00:59:42.909023Z","shell.execute_reply.started":"2021-06-08T00:58:48.784524Z","shell.execute_reply":"2021-06-08T00:59:42.907738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Topic Modeling using NMF**","metadata":{}},{"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer(min_df=3, max_df=0.85, max_features=5000, ngram_range=(1, 2), preprocessor=' '.join)\ntfidf = tfidf_vectorizer.fit_transform(texts)\ntfidf_fn = tfidf_vectorizer.get_feature_names()\nnmf = NMF(n_components= best_cv , init='nndsvd', solver='cd',random_state=42).fit(tfidf)\ndocweights = nmf.transform(tfidf_vectorizer.transform(texts))\n\nn_top_words = 10\n\ntopic_df = topic_table(nmf, tfidf_fn, n_top_words).T\ntopic_df\n\ntopic_df['topics'] = topic_df.apply(lambda x: [' '.join(x)], axis=1) \ntopic_df['topics'] = topic_df['topics'].str[0] \n\ntopic_df = topic_df['topics'].reset_index()\ntopic_df.columns = ['topic_num', 'topics']\n\ntopic_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T00:59:42.910509Z","iopub.execute_input":"2021-06-08T00:59:42.910859Z","iopub.status.idle":"2021-06-08T00:59:43.801084Z","shell.execute_reply.started":"2021-06-08T00:59:42.910821Z","shell.execute_reply":"2021-06-08T00:59:43.799854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"docweights = nmf.transform(tfidf_vectorizer.transform(texts))\n\nn_top_words = 10\n\ntopic_df = topic_table(nmf, tfidf_fn, n_top_words).T #Creates a Topic Table to see the Top Words in each topic number.\ntopic_df","metadata":{"execution":{"iopub.status.busy":"2021-06-08T00:59:43.802376Z","iopub.execute_input":"2021-06-08T00:59:43.802689Z","iopub.status.idle":"2021-06-08T00:59:43.937935Z","shell.execute_reply.started":"2021-06-08T00:59:43.80266Z","shell.execute_reply":"2021-06-08T00:59:43.9368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_df['topics'] = topic_df.apply(lambda x: [' '.join(x)], axis=1) #Joins all words in one column for easy interpretation of topics.\ntopic_df['topics'] = topic_df['topics'].str[0]  \n\ntopic_df = topic_df['topics'].reset_index()\ntopic_df.columns = ['topic_num', 'topics']\n\ntopic_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T00:59:43.939232Z","iopub.execute_input":"2021-06-08T00:59:43.939567Z","iopub.status.idle":"2021-06-08T00:59:43.956795Z","shell.execute_reply.started":"2021-06-08T00:59:43.939534Z","shell.execute_reply":"2021-06-08T00:59:43.955706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['topic_num'] = docweights.argmax(axis=1)\ndataset = dataset.merge(topic_df[['topic_num','topics']],\"left\") #Merge the Topic dataset to our main dataset to find which Reddit post belongs to which topic\ncolumns = ['score', 'url', 'comms_num', 'created', 'body_combined_text', 'date', 'Hour', 'Month']\ndataset.drop(columns = columns, inplace = True) #Removes Unncessary Columns\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T00:59:43.958541Z","iopub.execute_input":"2021-06-08T00:59:43.959126Z","iopub.status.idle":"2021-06-08T00:59:44.002823Z","shell.execute_reply.started":"2021-06-08T00:59:43.959086Z","shell.execute_reply":"2021-06-08T00:59:44.001549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"A = tfidf_vectorizer.transform(texts)\nW = nmf.components_\nH = nmf.transform(A)\n\nprint('A = {} x {}'.format(A.shape[0], A.shape[1]))\nprint('W = {} x {}'.format(W.shape[0], W.shape[1]))\nprint('H = {} x {}'.format(H.shape[0], H.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2021-06-08T00:59:44.004254Z","iopub.execute_input":"2021-06-08T00:59:44.00463Z","iopub.status.idle":"2021-06-08T00:59:44.118545Z","shell.execute_reply.started":"2021-06-08T00:59:44.004594Z","shell.execute_reply":"2021-06-08T00:59:44.117708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Calculating Residuals**\n\nA residual of zero means that the topic has approximated the text perfectly. We will visualise which topic number has the lowest residual score and create a wordcloud out of that topic number.","metadata":{}},{"cell_type":"code","source":"r = np.zeros(A.shape[0])\n\nfor row in range(A.shape[0]):\n    r[row] = np.linalg.norm(A[row, :] - H[row, :].dot(W), 'fro')\n\nsum_sqrt_res = round(sum(np.sqrt(r)), 3)\nprint(sum_sqrt_res)\n\ndataset['resid'] = r","metadata":{"execution":{"iopub.status.busy":"2021-06-08T00:59:44.119468Z","iopub.execute_input":"2021-06-08T00:59:44.119738Z","iopub.status.idle":"2021-06-08T00:59:44.496663Z","shell.execute_reply.started":"2021-06-08T00:59:44.119712Z","shell.execute_reply":"2021-06-08T00:59:44.495457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resid_data = dataset[['topic_num','resid']].groupby('topic_num').mean().sort_values(by='resid') \n\n\nresid_data.iplot( kind = 'bar', title = 'Average Residuals by Topic', xTitle = 'Topic Number', yTitle = 'Residuals')","metadata":{"execution":{"iopub.status.busy":"2021-06-08T00:59:44.498738Z","iopub.execute_input":"2021-06-08T00:59:44.499655Z","iopub.status.idle":"2021-06-08T00:59:44.609186Z","shell.execute_reply.started":"2021-06-08T00:59:44.499597Z","shell.execute_reply":"2021-06-08T00:59:44.60799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resid_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T00:59:44.611222Z","iopub.execute_input":"2021-06-08T00:59:44.612139Z","iopub.status.idle":"2021-06-08T00:59:44.623308Z","shell.execute_reply.started":"2021-06-08T00:59:44.612102Z","shell.execute_reply":"2021-06-08T00:59:44.622196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def word_cloud(df_weights, n_top_words=20, is_print=True, is_plot=True):\n    s_word_freq = pd.Series(df_weights['count'])\n    s_word_freq.index = df_weights['word']\n    di_word_freq = s_word_freq.to_dict()\n    cloud = WordCloud(width=1600, height=800, background_color='white').generate_from_frequencies(di_word_freq)\n    plt.figure(1,figsize=(13, 10))\n    if is_print:\n        print(df_weights.iloc[:n_top_words,:])\n    if is_plot:\n        plt.imshow(cloud)\n        plt.axis('off')\n        plt.show()\n    return cloud","metadata":{"execution":{"iopub.status.busy":"2021-06-08T01:00:00.779084Z","iopub.execute_input":"2021-06-08T01:00:00.77986Z","iopub.status.idle":"2021-06-08T01:00:00.787729Z","shell.execute_reply.started":"2021-06-08T01:00:00.779802Z","shell.execute_reply":"2021-06-08T01:00:00.786713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['joined'] = dataset['token'].apply(lambda x: final_text(x))\nfrequent_NN = pd.Series(' '.join(dataset['joined']).split()).value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-08T01:00:02.757685Z","iopub.execute_input":"2021-06-08T01:00:02.758217Z","iopub.status.idle":"2021-06-08T01:00:02.787377Z","shell.execute_reply.started":"2021-06-08T01:00:02.758185Z","shell.execute_reply":"2021-06-08T01:00:02.786566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = CountVectorizer(max_df = 0.6, min_df = 10, max_features=None, ngram_range=(1,4))\nX = cv.fit_transform(dataset['joined'])\ncvec = cv.fit(dataset.joined)\nbag_of_words = cvec.transform(dataset.joined)\nfeature_names = cvec.get_feature_names()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T01:00:04.173727Z","iopub.execute_input":"2021-06-08T01:00:04.174253Z","iopub.status.idle":"2021-06-08T01:00:04.865804Z","shell.execute_reply.started":"2021-06-08T01:00:04.174221Z","shell.execute_reply":"2021-06-08T01:00:04.864865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer = TfidfTransformer()\ntfidf = transformer.fit_transform(bag_of_words)\nword_cnts = np.asarray(bag_of_words.sum(axis=0)).ravel().tolist()  # for each word in column, sum all row counts\ndf_cnts = pd.DataFrame({'word': feature_names, 'count': word_cnts})\ndf_cnts = df_cnts.sort_values('count', ascending=False)\nweights = np.asarray(tfidf.mean(axis=0)).ravel().tolist()\ndf_weights = pd.DataFrame({'word': feature_names, 'weight': weights})\ndf_weights = df_weights.sort_values('weight', ascending=False)\n\ndf_weights = df_weights.merge(df_cnts, on='word', how='left')\ndf_weights = df_weights[['word', 'count', 'weight']]","metadata":{"execution":{"iopub.status.busy":"2021-06-08T01:00:06.447779Z","iopub.execute_input":"2021-06-08T01:00:06.44829Z","iopub.status.idle":"2021-06-08T01:00:06.467543Z","shell.execute_reply.started":"2021-06-08T01:00:06.448259Z","shell.execute_reply":"2021-06-08T01:00:06.466483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_no_0 = dataset[dataset['topic_num'] == 0]","metadata":{"execution":{"iopub.status.busy":"2021-06-08T01:01:33.383767Z","iopub.execute_input":"2021-06-08T01:01:33.384133Z","iopub.status.idle":"2021-06-08T01:01:33.390691Z","shell.execute_reply.started":"2021-06-08T01:01:33.384103Z","shell.execute_reply":"2021-06-08T01:01:33.38896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_no_0","metadata":{"execution":{"iopub.status.busy":"2021-06-08T01:01:40.359778Z","iopub.execute_input":"2021-06-08T01:01:40.360135Z","iopub.status.idle":"2021-06-08T01:01:40.388519Z","shell.execute_reply.started":"2021-06-08T01:01:40.360104Z","shell.execute_reply":"2021-06-08T01:01:40.387165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Word Cloud of Topic O**","metadata":{}},{"cell_type":"code","source":"topic_no_0 = dataset[dataset['topic_num'] == 0] #Filter to get only those comments which belongs to Topic 0\nfrequent_NN = pd.Series(' '.join(topic_no_0['joined']).split()).value_counts()  #Creates a new dataframe that joins and count the frequency of the number\n","metadata":{"execution":{"iopub.status.busy":"2021-06-08T01:03:21.532032Z","iopub.execute_input":"2021-06-08T01:03:21.532476Z","iopub.status.idle":"2021-06-08T01:03:21.542386Z","shell.execute_reply.started":"2021-06-08T01:03:21.532442Z","shell.execute_reply":"2021-06-08T01:03:21.541298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = CountVectorizer(max_df = 0.2, min_df = 2, max_features=None, ngram_range=(1,2))\nX = cv.fit_transform(topic_no_0['joined'])\ncvec = cv.fit(topic_no_0.joined)\nbag_of_words = cvec.transform(topic_no_0.joined)\nfeature_names = cvec.get_feature_names()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-08T01:03:56.025191Z","iopub.execute_input":"2021-06-08T01:03:56.02555Z","iopub.status.idle":"2021-06-08T01:03:56.058928Z","shell.execute_reply.started":"2021-06-08T01:03:56.025521Z","shell.execute_reply":"2021-06-08T01:03:56.057943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer = TfidfTransformer()\ntfidf = transformer.fit_transform(bag_of_words)\n\nword_counts = np.asarray(bag_of_words.sum(axis=0)).ravel().tolist()\ndf_cnts = pd.DataFrame({'word': feature_names, 'count': word_counts})\ndf_cnts = df_cnts.sort_values('count', ascending=False)\nweights = np.asarray(tfidf.mean(axis=0)).ravel().tolist()\n\n\ndf_weights = pd.DataFrame({'word': feature_names, 'weight': weights})\ndf_weights = df_weights.sort_values('weight', ascending=False)\ndf_weights","metadata":{"execution":{"iopub.status.busy":"2021-06-08T01:03:58.770856Z","iopub.execute_input":"2021-06-08T01:03:58.771213Z","iopub.status.idle":"2021-06-08T01:03:58.79678Z","shell.execute_reply.started":"2021-06-08T01:03:58.771184Z","shell.execute_reply":"2021-06-08T01:03:58.795789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_weights = df_weights.merge(df_cnts, on='word', how='left')\ndf_weights = df_weights[['word', 'count', 'weight']]\ndf_weights\ncloud_all = word_cloud(df_weights, is_print=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T01:04:03.497049Z","iopub.execute_input":"2021-06-08T01:04:03.497449Z","iopub.status.idle":"2021-06-08T01:04:06.618895Z","shell.execute_reply.started":"2021-06-08T01:04:03.497406Z","shell.execute_reply":"2021-06-08T01:04:06.618104Z"},"trusted":true},"execution_count":null,"outputs":[]}]}