{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include=\"O\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.heatmap(df.isnull(), cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column=df.select_dtypes(include=['object'])\nfor col in column:\n    display(df[col].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring the dataset by each feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = df['gender'], data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.gender.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = df['gender'], hue = df['status'], data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* By visualizing we can see that males have higher of getting placed as compared to females","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x = df['gender'], y = df['salary'], data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('gender')['salary'].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Males tends to have higher salary as compared to females.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = df['hsc_b'], hue = df['hsc_s'], data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 15))\nax=plt.subplot(221)\nsns.boxplot(x='status',y='ssc_p',data=df)\nax.set_title('Secondary school percentage')\nax=plt.subplot(222)\nsns.boxplot(x='status',y='hsc_p',data=df)\nax.set_title('Higher Secondary school percentage')\nax=plt.subplot(223)\nsns.boxplot(x='status',y='degree_p',data=df)\nax.set_title('UG Degree percentage')\nax=plt.subplot(224)\nsns.boxplot(x='status',y='mba_p',data=df)\nax.set_title('MBA percentage')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(x = df['gender'], y = df['salary'], hue = df['workex'], data=df)\nplt.title(\"Gender vs Salary based on work experience\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It can be seen that people with work experience have higher chance to get placed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['salary'], bins=50, hist=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['sl_no', 'salary'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['gender'] = df.gender.map({\"M\" : 0, \"F\" : 1})\ndf['ssc_b'] = df.ssc_b.map({\"Other\" : 0, \"Central\" : 1})\ndf['hsc_s'] = df.hsc_s.map({\"Commerce\" : 0, \"Science\" : 1, \"Arts\" : 2})\ndf['degree_t'] = df.degree_t.map({\"Comm&Mgmt\" : 0, \"Sci&Tech\" : 1, \"Others\" : 2})\ndf['workex'] = df.workex.map({\"No\" : 0, \"Yes\" :1})\ndf['specialisation'] = df.specialisation.map({\"Mkt&Fin\" : 0, \"Mkt&HR\" : 1})\ndf['status'] = df.status.map({\"Not Placed\" : 0, \"Placed\" : 1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['ssc_b'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['hsc_b'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation\n\n* Correlation is a statistical term which show that how strongly the variables as correlated\n\n* Positive correlation : If the value of one feature increases the value of other feature also increases\n\n* Negative correlation : If the value of one feature increases the value of other feature decreases","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a correlation matrix\n\nplt.figure(figsize=(10,10))\n\nsns.heatmap(df.corr(), annot=True, cmap=\"RdYlGn\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection\n\n* Feature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in.\n\n*  Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seperating our variables into Independent and Dependent variables\n\nX = df[['ssc_p', 'hsc_p', 'hsc_s', 'degree_p', 'degree_t','workex', 'specialisation', 'mba_p', 'etest_p']] # Indepepndent variables\n\ny = df['status'] # Dependent variables\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building Machine Learning Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n\ndef print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        pred = clf.predict(X_train)\n        print(\"Train Result:\\n===========================================\")\n        print(f\"accuracy score: {accuracy_score(y_train, pred):.4f}\\n\")\n        print(f\"Classification Report: \\n \\tPrecision: {precision_score(y_train, pred)}\\n\\tRecall Score: {recall_score(y_train, pred)}\\n\\tF1 score: {f1_score(y_train, pred)}\\n\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, clf.predict(X_train))}\\n\")\n        \n    elif train==False:\n        pred = clf.predict(X_test)\n        print(\"Test Result:\\n===========================================\")        \n        print(f\"accuracy score: {accuracy_score(y_test, pred)}\\n\")\n        print(f\"Classification Report: \\n \\tPrecision: {precision_score(y_test, pred)}\\n\\tRecall Score: {recall_score(y_test, pred)}\\n\\tF1 score: {f1_score(y_test, pred)}\\n\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data into train test split\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying Decision Tree Algorithm\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(random_state=42)\ntree.fit(X_train, y_train)\n\nprint_score(tree, X_train, y_train, X_test, y_test, train=True)\nprint_score(tree, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=200,criterion='gini',max_depth= 4, max_features= 'auto',random_state=42)\nrf.fit(X_train, y_train)\n\nprint_score(rf, X_train, y_train, X_test, y_test, train=True)\nprint_score(rf, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier()\nxgb.fit(X_train, y_train)\n\nprint_score(xgb, X_train, y_train, X_test, y_test, train=True)\nprint_score(xgb, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting ROC Curve","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_probs = rf.predict_proba(X_test)[:,1]\ndtree_probs = tree.predict_proba(X_test)[:,1]\nxgb_probs = xgb.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('roc_auc_score for Random forest: ', roc_auc_score(y_test, rf_probs))\nprint('roc_auc_score for Decision Tree: ', roc_auc_score(y_test, dtree_probs))\nprint('roc_auc_score for XGBoost: ', roc_curve(y_test, xgb_probs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ROC Curve\nfrom sklearn.metrics import roc_curve\ny_pred_prob1 = rf.predict_proba(X_test)[:,1]\nfpr1 , tpr1, thresholds1 = roc_curve(y_test, rf_probs)\n\ny_pred_prob2 = tree.predict_proba(X_test)[:,1]\nfpr2 , tpr2, thresholds2 = roc_curve(y_test, dtree_probs)\n\n\ny_pred_prob3 = xgb.predict_proba(X_test)[:,1]\nfpr3 , tpr3, thresholds3 = roc_curve(y_test, xgb_probs)\n\nplt.plot([0,1],[0,1], 'k--')\nplt.plot(fpr1, tpr1, label= \"Random Forest\")\nplt.plot(fpr2, tpr2, label= \"Decision Tree\")\nplt.plot(fpr3, tpr3, label= \"Xgboost\")\n\nplt.legend()\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.title('Receiver Operating Characteristic')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the graph we can clearly see that Random Forest classifier is performing better than rest of the algorithms**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}