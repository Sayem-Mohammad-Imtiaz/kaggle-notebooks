{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport nltk\nimport string\nimport spacy\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom numpy.random import seed\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout \nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.random import set_seed\n\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\npd.set_option('display.max_columns', None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -m spacy download en_vectors_web_lg\n!python -m spacy link en_vectors_web_lg en_vectors_web_lg_link","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load('en_vectors_web_lg_link') #('en_core_web_sm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/sms-spam-collection-dataset/spam.csv', encoding ='windows-1251')[['v1', 'v2']]\ndata.columns = ['label', 'msg']\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splits","metadata":{}},{"cell_type":"code","source":"df_else, validation_df  = train_test_split(data,\n                                test_size=0.25,\n                                random_state = 101)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train, df_test  = train_test_split(df_else,\n                                test_size=0.25,\n                                random_state = 101)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation ","metadata":{}},{"cell_type":"code","source":"def del_punct(text):\n    chars = []\n    for char in text:\n        if char not in string.punctuation:\n            chars.append(char)\n        else:\n            chars.append(' ')\n    return ''.join(chars)\n\ndef text_preparation(text: str) -> str:\n    text = text.lower()\n    text = del_punct(text)\n    doc = nlp(text)\n    text = ' '.join([\n            token.lemma_ \n                for token in doc \n                if token.text not in nlp.Defaults.stop_words \n        ])\n    \n    text = re.sub(r'\\d+', ' somenumbers ', text)\n    text = re.sub(r'\\s+', ' ', text)\n    \n    return text\n     \ndef processing(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df['msg_len'] = df['msg'].apply(len)\n    df['msg'] = df['msg'].apply(text_preparation)\n    return df\n\ndf_train_p = processing(df_train)\ndf_test_p = processing(df_test)\ndf_train_p.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"datasets = {\n    'train'        : df_train, \n    'test'         : df_test, \n    'train + test' : df_else, \n    'validation'   : validation_df\n}\n\nfor dataset_name, dataset in datasets.items():\n    print('\\n' + dataset_name + ':')\n    display(pd.DataFrame(dataset['label'].value_counts()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(df_train_p, hue='label', height = 7, aspect = 2)\ng.map(sns.kdeplot, 'msg_len')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Spam messages tend to be longer","metadata":{}},{"cell_type":"code","source":"def text_for_cloud(label):\n    text = ' '.join(df_train_p['msg'][df_train_p['label'] == label].to_list())\n    text = text.replace('somenumbers', '')\n    return text\n\nfor label in ['spam', 'ham']:\n    wordcloud = WordCloud(\n        max_font_size=500,\n        max_words=100,\n        background_color=\"white\"\n    ).generate(text_for_cloud(label))\n\n    plt.figure(figsize=(12, 8))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.title(f'{label.capitalize()} messages wordcloud', fontsize=20)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Bag of Words (lemmatized)","metadata":{}},{"cell_type":"code","source":"bow_transformer = CountVectorizer(max_features = 1500).fit(df_train_p['msg'])\nbow_train = bow_transformer.transform(df_train_p['msg'])\nbow_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_transformer = TfidfTransformer().fit(bow_train)\ntrain_tfidf = tfidf_transformer.transform(bow_train)\n\nbow_test = bow_transformer.transform(df_test_p['msg'])\ntest_tfidf = tfidf_transformer.transform(bow_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = pd.DataFrame.sparse.from_spmatrix(train_tfidf)\nX_train.columns = bow_transformer.get_feature_names()\nX_train['msg_len'] = df_train_p['msg_len'].values\n\nX_test = pd.DataFrame.sparse.from_spmatrix(test_tfidf) \nX_test.columns = bow_transformer.get_feature_names()\nX_test['msg_len'] = df_test_p['msg_len'].values\n\ny_train = df_train_p['label']\ny_test = df_test_p['label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling","metadata":{}},{"cell_type":"code","source":"scaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train_sc = scaler.transform(X_train.values)\nX_test_sc = scaler.transform(X_test.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dimensionality reduction using autoencoder","metadata":{}},{"cell_type":"code","source":"input_width = len(X_train.columns)\ninput_width","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dim_red_analysis(n_epochs):\n    seed(101)\n    set_seed(101)\n\n    encoder = Sequential()\n    encoder.add(Dense(units = 256, activation = 'relu', input_shape = [input_width]))\n    encoder.add(Dropout(0.2))\n    encoder.add(Dense(units = 16, activation = 'relu'))\n    encoder.add(Dense(units = 2, activation = 'relu'))\n\n    decoder = Sequential()\n    decoder.add(Dense(units = 16, activation = 'relu', input_shape = [2]))\n    decoder.add(Dense(units = 256, activation = 'relu'))\n    decoder.add(Dense(units = input_width, activation = 'relu'))\n\n    autoencoder = Sequential([encoder, decoder])\n\n    autoencoder.compile(loss = 'mse', optimizer = SGD(lr = 10))\n    \n    autoencoder.summary()\n\n    autoencoder.fit(\n        X_train_sc, \n        X_train_sc, \n        epochs = n_epochs,\n        validation_data=(X_test, X_test)\n    )\n    \n    if n_epochs > 1:\n        histo = pd.DataFrame(autoencoder.history.history)\n        for metric in ['loss', 'val_loss']:\n            plt.title(metric)\n            histo[metric].plot()\n            plt.show()\n        \n    encoded_2dim = encoder.predict(X_train_sc)\n    encoded_2dim = pd.DataFrame(encoded_2dim)\n    encoded_2dim['y'] = df_train['label'].values\n\n    plt.figure(figsize = (12, 8))\n    sns.scatterplot(data = encoded_2dim, x = 0, y = 1, hue = 'y', palette = 'magma')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dim_red_analysis(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dim_red_analysis(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dim_red_analysis(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dim_red_analysis(83)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see, that classes are clearly separable with just small overlapping","metadata":{}},{"cell_type":"markdown","source":"# Modelling with TF-IDF","metadata":{}},{"cell_type":"code","source":"def eval_result(model, X_test, y_test):\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        pred = model.predict(X_test)\n        print(classification_report(y_test, pred, target_names = ['Ham', 'Spam']))\n        display(pd.DataFrame(confusion_matrix(y_test, pred), \n                         columns = ['Predicted Ham', 'Predicted Spam'],\n                         index = ['Ham', 'Spam']))\n        \n        print(f'Accuracy: {round(accuracy_score(y_test, pred), 5)}')\n        if hasattr(model, 'feature_importances_'):\n            features = pd.DataFrame({\n                'Variable'  :X_test.columns,\n                'Importance':model.feature_importances_\n            })\n            features.sort_values('Importance', ascending=False, inplace=True)\n            display(features.head(20))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_model = MultinomialNB()\nnb_model.fit(X_train, y_train)\neval_result(nb_model, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtc = DecisionTreeClassifier(random_state = 1)\ndtc.fit(X_train, y_train)\neval_result(dtc, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc = RandomForestClassifier(n_jobs = -1, random_state = 1)\nrfc.fit(X_train, y_train)\neval_result(rfc, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbc = GradientBoostingClassifier(random_state = 1)\ngbc.fit(X_train, y_train)\neval_result(gbc, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word2Vec","metadata":{}},{"cell_type":"code","source":"X_train_v = pd.DataFrame([nlp(msg).vector for msg in df_train['msg']])\nX_test_v = pd.DataFrame([nlp(msg).vector for msg in df_test['msg']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_v.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc = RandomForestClassifier(n_jobs = -1, random_state = 1)\nrfc.fit(X_train_v, y_train)\neval_result(rfc, X_test_v, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbc = GradientBoostingClassifier(random_state = 1)\ngbc.fit(X_train_v, y_train)\neval_result(gbc, X_test_v, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"sid = SentimentIntensityAnalyzer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_sa = pd.DataFrame([sid.polarity_scores(msg) for msg in df_train['msg']])\nX_test_sa = pd.DataFrame([sid.polarity_scores(msg) for msg in df_test['msg']])\nX_train_sa","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc = RandomForestClassifier(n_jobs = -1, random_state = 1)\nrfc.fit(X_train_sa, y_train)\neval_result(rfc, X_test_sa, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbc = GradientBoostingClassifier(random_state = 1)\ngbc.fit(X_train_sa, y_train)\neval_result(gbc, X_test_sa, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Topic modelling","metadata":{}},{"cell_type":"code","source":"LDA = LatentDirichletAllocation(n_components=12,random_state=1)\nLDA.fit(bow_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index,topic in enumerate(LDA.components_):\n    print(f'The top 10 words for topic #{index}')\n    print([bow_transformer.get_feature_names()[i] for i in topic.argsort()[-10:]])\n    print('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_results_train = pd.DataFrame(LDA.transform(bow_train))\ntopic_results_test = pd.DataFrame(LDA.transform(bow_test))\nprint(f'train shape: {topic_results_train.shape}, test shape: {topic_results_test.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc = RandomForestClassifier(n_jobs = -1, random_state = 1)\nrfc.fit(topic_results_train, y_train)\neval_result(rfc, topic_results_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbc = GradientBoostingClassifier(random_state = 1)\ngbc.fit(topic_results_train, y_train)\neval_result(gbc, topic_results_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word2Vec + TF-IDF + Sentiment data","metadata":{}},{"cell_type":"code","source":"X_train_full = pd.concat([X_train, X_train_v, X_train_sa], axis=1)\nX_test_full = pd.concat([X_test, X_test_v, X_test_sa], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_full.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc = RandomForestClassifier(n_jobs = -1, random_state = 1)\nrfc.fit(X_train_full, y_train)\neval_result(rfc, X_test_full, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbc = GradientBoostingClassifier(random_state = 1)\ngbc.fit(X_train_full, y_train)\neval_result(gbc, X_test_full, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Didn't use topic modelling, because it worsens the result. The model of choice is GBC.","metadata":{}},{"cell_type":"markdown","source":"# Validation","metadata":{}},{"cell_type":"code","source":"df_val_p = processing(validation_df)\n\nbow_val = bow_transformer.transform(df_val_p['msg'])\nval_tfidf = tfidf_transformer.transform(bow_val)\n\nX_val = pd.DataFrame.sparse.from_spmatrix(val_tfidf)\nX_val['msg_len'] = df_val_p['msg_len'].values\n\ny_val = df_val_p['label']\n\nX_val_v = pd.DataFrame([nlp(msg).vector for msg in validation_df['msg']])\nX_val_sa = pd.DataFrame([sid.polarity_scores(msg) for msg in validation_df['msg']])\nX_val_full = pd.concat([X_val, X_val_v, X_val_sa], axis=1)\nX_val_full.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_result(gbc, X_val_full, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}