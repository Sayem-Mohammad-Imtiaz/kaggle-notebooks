{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport warnings\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score, learning_curve, ShuffleSplit\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures, OneHotEncoder, OrdinalEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.compose import make_column_transformer, make_column_selector, ColumnTransformer\nfrom sklearn.feature_selection import SelectPercentile, SelectorMixin\nfrom sklearn.base import TransformerMixin, BaseEstimator\n\npd.set_option('display.max_columns', None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['stroke'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['age'] == 0.08]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (14, 8))\nplt.hist(x = df['age'], bins = 40)    #, bins = 'fd')\nplt.title('Age distribution')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(df, hue='stroke', height = 7, aspect = 2)\ng.map(sns.kdeplot, 'age')\nplt.title('Age distribution with class')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(df, hue='stroke', height = 7, aspect = 2)\ng.map(sns.kdeplot, 'avg_glucose_level')\nplt.title('Glucose level distribution with class')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(df, hue='stroke', height = 7, aspect = 2)\ng.map(sns.kdeplot, 'bmi')\nplt.title('Body Mass Index distribution with class')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(\n    data = df[['age', 'avg_glucose_level', 'bmi', 'stroke']],\n    hue = 'stroke',  \n    palette = 'magma',\n    height = 4\n)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Effect of smoking","metadata":{}},{"cell_type":"code","source":"for status in df['smoking_status'].unique():\n    df_temp = df[df['smoking_status'] == status]\n    df_temp = df_temp.groupby(\"stroke\")['id'].count()\n    df_temp.plot.pie(autopct=\"%.1f%%\")\n    plt.ylabel('#')\n    plt.title('Strokes among ' + status)\n    plt.show() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Marriage status difference","metadata":{}},{"cell_type":"code","source":"for status in df['ever_married'].unique():\n    df_temp = df[df['ever_married'] == status]\n    df_temp = df_temp.groupby(\"stroke\")['id'].count()\n    df_temp.plot.pie(autopct=\"%.1f%%\")\n    plt.ylabel('#')\n    plt.title('Strokes among ' + status)\n    plt.show() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Worktype","metadata":{}},{"cell_type":"code","source":"for status in df['work_type'].unique():\n    df_temp = df[df['work_type'] == status]\n    df_temp = df_temp.groupby(\"stroke\")['id'].count()\n    df_temp.plot.pie(autopct=\"%.1f%%\")\n    plt.ylabel('#')\n    plt.title('Strokes among ' + status)\n    plt.show() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gender","metadata":{}},{"cell_type":"code","source":"for status in df['gender'].unique():\n    df_temp = df[df['gender'] == status]\n    df_temp = df_temp.groupby(\"stroke\")['id'].count()\n    df_temp.plot.pie(autopct=\"%.1f%%\")\n    plt.ylabel('#')\n    plt.title('Strokes among ' + status)\n    plt.show() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df.drop('bmi', axis= 1).isnull().any(axis=1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"N/A values are only in bmi columns","metadata":{}},{"cell_type":"markdown","source":"# Split","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df.drop('stroke', axis = 1), \n                                                    df['stroke'], \n                                                    test_size=0.3, \n                                                    random_state=101)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"markdown","source":"### Columns preprocessing","metadata":{}},{"cell_type":"code","source":"class DataPreprocessor(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, binning_age = False, binning_glucose_level = False, binning_bmi = False):\n        self.binning_age = binning_age\n        self.binning_glucose_level = binning_glucose_level\n        self.binning_bmi = binning_bmi\n\n        \n    def __binning_func(self, X, column_to_bin):\n        X = X.copy()\n        min_value = self.columns_data[column_to_bin]['min']\n        max_value = self.columns_data[column_to_bin]['max']\n        bins = np.linspace(min_value,max_value, 8)\n        X[column_to_bin] = pd.cut(X[column_to_bin], bins=bins,  include_lowest=True)\n        X[column_to_bin] = X[column_to_bin].astype('str')\n        return X\n\n    def fit(self, X, y = None):\n        self.columns_data = dict()\n        \n        detection_list = []\n        if self.binning_age:\n            detection_list.append('age')\n        \n        if self.binning_glucose_level:\n            detection_list.append('avg_glucose_level')\n        \n        if self.binning_bmi:\n            detection_list.append('bmi')\n    \n        for col in detection_list:\n            self.columns_data[col] = {\n                'min': min(X[col]),\n                'max': max(X[col]),\n            }\n        return self\n\n    def transform(self, X, y = None):\n        X_copy = X.copy()\n        \n        if 'id' in X_copy.columns:\n            X_copy.drop('id', axis = 1, inplace = True)\n\n        if self.binning_age:\n            X_copy = self.__binning_func(X_copy, 'age')\n\n        if self.binning_glucose_level:\n            X_copy = self.__binning_func(X_copy, 'avg_glucose_level')\n\n        if self.binning_bmi:\n            X_copy = self.__binning_func(X_copy, 'bmi')\n            X_copy['bmi'].fillna('N\\A', inplace = True)\n        else:\n            X_copy['bmi'].fillna(-1, inplace = True)\n            \n        return X_copy\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processor = DataPreprocessor(\n    binning_age = True,\n    binning_glucose_level = True,\n    binning_bmi = True\n)\nprocessor.fit_transform(df).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Interactions and polinoms","metadata":{}},{"cell_type":"code","source":"class InterPolinomsFeatures(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, degree = 1, interaction_only = False):\n        self.degree = degree\n        self.interaction_only = interaction_only\n        self.encoder_inner = PolynomialFeatures(\n            degree = degree, \n            interaction_only = interaction_only,\n            include_bias = False\n        )\n        self.encoder = make_column_transformer(\n                    (self.encoder_inner,make_column_selector(dtype_exclude='object')),\n                    remainder='passthrough'\n                )\n        \n    @staticmethod\n    def __convert_to_float(X):\n        X_copy = X.copy()\n        for name in X.columns:\n            if 'polynomialfeatures__' in name:\n                X_copy[name] = X_copy[name].astype(float)\n        return X_copy\n    \n    def __columns_name_change(self, name):\n        name = name.replace('polynomialfeatures__', '')\n        for col_name in re.findall(r'\\bx\\d+\\b', name):\n            name = name.replace(col_name, self.object_columns_dict[col_name])\n        return name\n    \n    def fit(self, X, y = None):\n        X_copy = X.copy()\n        self.encoder.fit(X_copy)\n        object_columns = X_copy.select_dtypes(exclude='object').columns\n        self.object_columns_dict = dict()\n        for i in enumerate(object_columns):\n            self.object_columns_dict[f'x{i[0]}'] = i[1]\n        return self\n                 \n    def transform(self, X, y = None):\n        X_copy = X.copy()\n        X_copy = self.encoder.transform(X_copy)\n        X_copy = pd.DataFrame(X_copy, columns = self.encoder.get_feature_names())\n        X_copy = self.__convert_to_float(X_copy)\n        X_copy.columns = [self.__columns_name_change(name) for name in X_copy.columns]\n        return X_copy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"InterPolinomsFeatures(\n    degree = 2, \n    interaction_only = False\n).fit_transform(\n    X_train.fillna(0)\n               ).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical encoding","metadata":{}},{"cell_type":"code","source":"class CategoricalEncoderOneHot():\n    \n    def __init__(self):\n        self.encoder_inner = OneHotEncoder(sparse=False, handle_unknown='ignore')\n        self.encoder = make_column_transformer(\n                    (self.encoder_inner,make_column_selector(dtype_include='object')),\n                    remainder='passthrough'\n                )\n        \n    def __columns_name_change(self, name):\n        if 'onehotencoder__' in name:\n            name = name.replace('onehotencoder__', '')\n            col_name = re.findall(r'^x\\d+', name)[0]\n            name = name.replace(col_name, self.object_columns_dict[col_name])\n        return name\n    \n    def fit(self, X, y = None):\n        X_copy = X.copy()\n        self.encoder.fit(X_copy)\n        object_columns = X_copy.select_dtypes(include='object').columns\n        self.object_columns_dict = dict()\n        for i in enumerate(object_columns):\n            self.object_columns_dict[f'x{i[0]}'] = i[1]\n        return self\n    \n    def transform(self, X, y = None):\n        X_copy = X.copy()\n        X_proc = self.encoder.transform(X_copy)\n        X_proc = pd.DataFrame(X_proc)\n        X_proc.columns = [self.__columns_name_change(name) for name in self.encoder.get_feature_names()]\n        return X_proc\n\n\n\nclass CategoricalEncoderOrdinal():\n    \n    def __init__(self):\n        self.encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)\n        \n    def fit(self, X, y = None):\n        self.object_columns = X.select_dtypes(include='object').columns\n        self.non_object_columns = X.select_dtypes(exclude='object').columns\n        self.encoder.fit(X[self.object_columns])\n        return self\n    \n    def transform(self, X, y = None):\n        X_copy = X.copy()\n        X_copy[self.object_columns] = self.encoder.transform(X_copy[self.object_columns])\n        return X_copy\n\n    \nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    \n    encoders = {\n        'one_hot' : CategoricalEncoderOneHot,\n        'ordinal' : CategoricalEncoderOrdinal\n    }\n        \n    def __init__(self, mode = 'one_hot'):\n        if mode in self.encoders.keys():\n            self.mode = mode\n            self.encoder = self.encoders[mode]()\n        else:\n            raise AttributeError('Wrong mode name')\n    \n    \n    def fit(self, X, y = None):\n        self.encoder.fit(X, y)\n        return self\n    \n    def transform(self, X, y = None):\n        return self.encoder.transform(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = processor.fit_transform(X_train)\na = CategoricalEncoder(mode = 'ordinal')\na.fit(test_df)\ntest_df_cat = a.transform(test_df).head()\ntest_df_cat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = CategoricalEncoder(mode = 'ordinal')\na.fit(X_train)\na.transform(X_train).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CategoricalEncoder(mode = 'ordinal').fit_transform(X_train).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CategoricalEncoder(mode = 'one_hot').fit_transform(X_train).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature selection","metadata":{}},{"cell_type":"code","source":"class Selector(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, percent = 50):\n        self.percent = percent\n        self.selector_inner = SelectPercentile(percentile=percent)\n        \n    def fit(self, X, y):\n        self.selector_inner.fit(X, y)\n        self.columns_names = X.columns[self.selector_inner.get_support()]\n        return self\n        \n    \n    def transform(self, X, y = None):\n        X_copy = X.copy()\n        X_proc = self.selector_inner.transform(X_copy)\n        X_proc = pd.DataFrame(X_proc, columns = self.columns_names)\n        return X_proc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Selector(percent = 30).fit_transform(X_train[['age', 'avg_glucose_level', 'id']], y_train).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Scaling","metadata":{}},{"cell_type":"code","source":"class Scaler(BaseEstimator, TransformerMixin):\n    \n    scalers = {\n        'standart': StandardScaler, \n        'minmax'  : MinMaxScaler\n    }\n    \n    def __init__(self, mode = 'minmax'):\n        if mode in self.scalers.keys():\n            self.mode = mode\n            self.scaler_inner = self.scalers[self.mode]()\n        else:\n            raise AttibuteError('Wrong mode name')\n        \n    def fit(self, X, y = None):\n        self.scaler_inner.fit(X)\n        self.columns_names = X.columns\n        return self\n        \n    def transform(self, X, y = None):\n        X_copy = X.copy()\n        X_proc = self.scaler_inner.transform(X_copy)\n        X_proc = pd.DataFrame(X_proc, columns = self.columns_names)\n        return X_proc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Scaler(mode = 'minmax').fit_transform(df[['age', 'heart_disease']]).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"def plot_learning_curve(estimator, X, y, axes=None, ylim=None, cv=5,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    '''\n    https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py\n    '''\n\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title('Learning curve')\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1)\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n    plt.show()\n\n\n\ndef precision_recall(model, X_test, y_test):\n    precision, recall, thresholds = precision_recall_curve(\n        y_test, \n        model.decision_function(X_test)\n    )\n    close_zero = np.argmin(np.abs(thresholds))\n    plt.plot(precision[close_zero], recall[close_zero], 'o', markersize=10,\n     label=\"threshold 0\", fillstyle=\"none\", c='k', mew=2)\n    plt.plot(precision, recall, label=\"precision recall curve\")\n    plt.xlabel(\"Precision\")\n    plt.ylabel(\"Recall\")\n    plt.legend(loc=\"best\")\n    plt.show()\n\n\ndef eval_result(model, X_test, y_test, X_train, y_train, validation = False):\n    if type(model) == GridSearchCV:\n        model = model.best_estimator_\n    pipeline = False\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        pred = model.predict(X_test)\n        print(classification_report(y_test, pred, target_names = ['No Stroke', 'Stroke']))\n        display(pd.DataFrame(confusion_matrix(y_test, pred), \n                         columns = ['Stroke Not Predicted', 'Stroke Predicted'],\n                         index = ['No Stroke', 'Stroke']))\n        \n        if type(model) == Pipeline:\n            pipeline = True\n            pipe = model[:-1]\n            model = model[-1]\n        \n        if pipeline:\n            X_test = pipe.transform(X_test)\n            X_train = pipe.transform(X_train)\n            \n        if ((hasattr(model, 'feature_importances_') \n        or hasattr(model, 'coef_')) \n            and not validation):\n            try:\n                model_feat_imp = model.feature_importances_\n            except:\n                model_feat_imp = [abs(i) for i in model.coef_[0]]\n            \n                \n            features = pd.DataFrame({\n                'Variable'  :X_test.columns,\n                'Importance':model_feat_imp\n            })\n            features.sort_values('Importance', ascending=False, inplace=True)\n            display(features.head(20))\n        if not validation:\n            try:\n                precision_recall(model, X_test, y_test)\n                plot_learning_curve(model, X_train, y_train, n_jobs=-1)\n            except:\n                pass\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linear model","metadata":{}},{"cell_type":"code","source":"pipe_lm = make_pipeline(\n    DataPreprocessor(),\n    InterPolinomsFeatures(),\n    CategoricalEncoder(),\n    Selector(),\n    Scaler(),\n    LogisticRegression(max_iter = 500, random_state = 1)\n)\n\nparam_grid = {\n    'datapreprocessor__binning_age': [True, False],\n    'datapreprocessor__binning_glucose_level': [True, False],\n    'datapreprocessor__binning_bmi': [True, False],\n    'interpolinomsfeatures__interaction_only': [True, False],\n    'interpolinomsfeatures__degree': [1, 2, 3],\n    'categoricalencoder__mode': ['one_hot', 'ordinal'],\n    'selector__percent': [30, 50, 100],\n    'scaler__mode': ['standart', 'minmax'],\n    'logisticregression__class_weight': [\n        {0:1, 1:5},\n        {0:1, 1:7},\n        {0:1, 1:10},\n        {0:1, 1:15},\n    ],\n    'logisticregression__C': [0.1, 0.5, 0.7, 1, 2]\n}\npipe_lm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe_lm[:-1].fit_transform(X_train, y_train).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid = GridSearchCV(\n        pipe_lm, \n        param_grid=param_grid, \n        cv=5, \n        n_jobs = -1, \n        verbose = 2,\n        scoring = 'f1_macro'\n    )\ngrid.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best and worst parameters combination:","metadata":{}},{"cell_type":"code","source":"lm_results = pd.DataFrame(grid.cv_results_)\nlm_results.sort_values(by='mean_test_score', ascending = False, inplace = True)\n\ndisplay(lm_results.head(4))\ndisplay(lm_results.tail(4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Best cross val score: {grid.best_score_}\")\nprint(f\"\\nBest params:\")\nfor param, val in grid.best_params_.items():\n    print(f'{param}: {val}')\nprint('\\n')\neval_result(grid, X_test, y_test, X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}