{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport csv\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom numpy.random import seed\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.random import set_seed\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, GRU, Reshape\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\nfrom tensorflow.keras.models import load_model\npd.set_option('display.max_rows', None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/corpus-of-russian-news-articles-from-lenta/lenta-ru-news.csv').sample(frac = 0.3, \n                                                                                                 random_state = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA and data preparation ","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'][334]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.title('Missing data')\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['topic'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(df[df['topic'] == 'Библиотека'].index, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['tags'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (12, 8))\ndf['title'].apply(len).hist(bins = 40)\nplt.title('Length of title distribution')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'].apply(type).value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['text'].apply(type) == float]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(df[df['text'].apply(type) == float].index, inplace = True)\ndf['text'].apply(type).value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['len'] = df['text'].apply(len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (12, 8))\ndf['len'].hist(bins = 130)\nplt.title('Length of text distribution')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['len'] > 20000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (12, 8))\ndf['len'][df['len'] < 6000].hist(bins = 130)\nplt.title('Length of text distribution')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['len'] < 300].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(df[df['len'] < 300].index, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_drop = {'\\x03', '\\x04', '\\x08',  '\\r', '\\x0f', '\\x13', '\\x1d', '\\x1e', '\\x1f', '<', '=', '>', '@',  '^',  '`', '{', '|', '}', \n '~', '\\x7f', '\\x97', '\\x98', '\\xa0', '¡', '©', '«', '\\xad', '®', '¯', '°', '±', '´', 'µ', '·', '¸', '»', '¼', '½', '¾', \n '¿', 'Á', 'Æ', 'Ç', 'È', 'É', 'Ë', 'Í', 'Ó', 'Ô', 'Ö', '×', 'Ø', 'Ü', 'Þ', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', \n 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', '÷', 'ø', 'ú', 'û', 'ü', 'ý', 'þ', 'ā', 'ă', \n 'ą', 'ć', 'Č', 'č', 'Đ', 'ē', 'ė', 'ę', 'ě', 'ğ', 'ī', 'İ', 'ı', 'Ł', 'ł', 'ń', 'ņ', 'ō', 'ŏ', 'ř', 'ś', 'Ş', 'ş', 'Š', \n 'š', 'Ū', 'ų', 'ŷ', 'ź', 'ż', 'ž', 'ț', 'ɔ', 'ə', 'ɢ', 'ɪ', 'ʒ', 'ʟ', 'ʼ', '˚', '́', '̆', '̈', 'Δ', 'Θ', 'Λ', 'Ξ', 'Σ', 'έ',\n 'ί', 'α', 'β', 'γ', 'δ', 'ε', 'θ', 'ι', 'κ', 'λ', 'μ', 'ν', 'ο', 'π', 'ρ', 'ς', 'σ', 'τ', 'υ', 'χ', 'ψ', 'ω', 'ό', 'ύ',\n 'ώ',  'Љ',  'љ', 'ў', 'Қ', 'қ', 'ң', 'ү', 'Ұ', 'ӧ', 'א', 'ו', 'י', 'ל', 'ם', 'פ', 'ץ', 'צ', 'ק', 'ר', 'ש', 'ת', '،', '؛', '؟', 'آ', 'ئ', 'ا', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ط', 'ظ', 'ع', 'ف', 'ق', 'ك', 'ل', 'م'\n , 'ن', 'ه', 'و', 'ى', 'ي', 'پ', 'چ', 'ک', 'گ', 'ی', '۴', '۵', '۷', '۹', 'ખ', 'ફ', 'બ', 'ર', 'લ', 'સ', 'ા', 'ો', '્', 'ᴥ', \n 'ᵉ', 'ᵍ', 'ᵏ', 'ᵒ', 'ᵘ', 'ᶅ', 'ᶘ', 'ᶜ', 'ᶠ', 'Ḥ', 'ḫ', 'ả', 'ầ', 'ệ', 'ố', 'ộ', '\\u2002', '\\u2003', '\\u2009', '\\u200a', \n '\\u200b', '\\u200c', '\\u200d', '\\u200e', '\\u200f', '‘', '’', '“', '”', '„', '•', '…', '\\u2028', '\\u202a', '\\u202c', \n '\\u202d', '\\u202f', '‼', '\\u2066', 'ⁱ', '⁽', '⁾', 'ⁿ', '€', '₽', '№', '™', '↓', '−', '∙', '∞', '∩', '≠', '⏳', '①', 'Ⓑ',\n 'Ⓔ', 'Ⓕ', 'Ⓝ', 'Ⓞ', 'Ⓡ', 'Ⓢ', 'Ⓣ', '─', '►', '☀', '☁', '☎', '☑', '☝', '☠', '☺', '♀', '♂', '♥', '♪', '⚔', '⚖', \n '⚡', '⚪', '⚽', '✅', '✈', '✌', '✔', '✨', '✵', '❄', '❗', '❤', '➖', '➡', '➰', '⠀', '⬅', '⬆', '⭐', '⭕', '、',\n '。', '々', '「', '」', '『', '』', '〜', '〰', 'あ', 'い', 'う', 'え', 'お', 'か', 'が', 'き', 'ぎ', 'く', 'ぐ', 'け', 'こ', \n 'ご', 'さ', 'ざ', 'し', 'じ', 'す', 'ず', 'せ', 'そ', 'た', 'だ', 'ち', 'っ', 'て', 'で', 'と', 'ど', 'な', 'に', 'ね', 'の', \n 'は', 'ふ', 'ぽ', 'ま', 'み', 'む', 'め', 'も', 'ゃ', 'や', 'ゅ', 'よ', 'ら', 'り', 'る', 'れ', 'ろ', 'わ', 'を', 'ん', 'ア', \n 'イ', 'ウ', 'ェ', 'エ', 'オ', 'カ', 'ガ', 'キ', 'ギ', 'ク', 'ケ', 'コ', 'ゴ', 'サ', 'シ', 'ジ', 'ス', 'ズ', 'セ', 'ソ', 'ダ', \n 'チ', 'ッ', 'ツ', 'ト', 'ド', 'ナ', 'ニ', 'ネ', 'ハ', 'バ', 'パ', 'ピ', 'ブ', 'プ', 'ヘ', 'ペ', 'ボ', 'ポ', 'マ', 'ム', 'メ',\n 'ャ', 'ュ', 'ョ', 'ラ', 'リ', 'ル', 'レ', 'ロ', 'ン', 'ヴ', '・', 'ー', '一', '三', '上', '下', '乗', '事', '五', '人', '今',\n '从', '仕', '会', '何', '使', '俞', '光', '入', '内', '写', '出', '分', '到', '加', '動', '北', '十', '口', '可', '国', '園', \n '土', '基', '外', '天', '女', '妙', '嬉', '子', '寝', '屋', '山', '市', '平', '幼', '度', '当', '後', '微', '思', '性', '悦', \n '悲', '愛', '感', '成', '担', '持', '敏', '方', '日', '時', '普', '暮', '月', '有', '本', '李', '東', '极', '枚', '柒', '柯', \n '植', '極', '様', '歧', '段', '気', '沙', '治', '洪', '活', '満', '然', '照', '片', '物', '犬', '狭', '猫', '理', '生', '産',\n '用', '番', '痢', '的', '皮', '目', '直', '真', '社', '祭', '立', '素', '組', '編', '練', '置', '考', '肌', '自', '良', '花',\n '茶', '落', '虫', '虾', '見', '视', '調', '負', '買', '賀', '購', '超', '躍', '还', '通', '遊', '達', '部', '集', '雨', '順', \n '顔', '魚', '걸', '계', '근', '글', '기', '다', '달', '대', '동', '라', '로', '르', '리', '림', '만', '메', '모', '목', '무', \n '물', '복', '브', '빌', '쁘', '사', '스', '실', '어', '여', '예', '오', '올', '음', '이', '접', '줄', '지', '진', '창', '코', \n '크', '투', '티', '팅', '평', '픽', '한', '행', '헌', '회', 'ﬁ', '️', '\\ufeff',  '🆘', '🌅', '🌈', '🌊', '🌌', '🌎', '🌞', \n '🌟', '🌠', '🌱', '🌴', '🌷', '🌸', '🌹', '🌻', '🍁', '🍂', '🍃', '🍄', '🍉', '🍋', '🍌', '🍏', '🍐', '🍑', '🍒', \n '🍓', '🍕', '🍡', '🍥', '🍦', '🍩', '🍬', '🍴', '🍷', '🍹', '🍻', '🍼', '🎀', '🎁', '🎄', '🎅', '🎆', '🎇', '🎉', '🎊', \n '🎙', '🎥', '🎭', '🎶', '🏃', '🏋', '🏖', '🏝', '🏡', '🏺', '🏻', '🏼', '🏽', '🏾', '🏿', '🐅', '🐈', '🐍', '🐎', '🐓',\n '🐠', '🐳', '🐴', '🐶', '🐸', '🐺', '🐻', '🐾', '👀', '👇', '👉', '👊', '👋', '👌', '👍', '👏', '👑', '👒', '👖', \n '👗', '👙', '👛', '👜', '👟', '👠', '👣', '👨', '👯', '👰', '👸', '👹', '👽', '👿', '💃', '💅', '💇', '💋', '💌', \n '💍', '💎', '💐', '💓', '💔', '💕', '💖', '💗', '💘', '💙', '💚', '💛', '💜', '💞', '💥', '💦', '💩', '💪', '💫',\n '💭', '💯', '💸', '📍', '📝', '📲', '📷', '📸', '📺', '📻', '🔎', '🔙', '🔥', '🔪', '🔴', '🔸', '🔹', '🕊', '🕺', '🖕',\n '🖤', '🗣', '🗼', '😀', '😁', '😂', '😃', '😄', '😅', '😆', '😇', '😈', '😉', '😊', '😋', '😌', '😍', '😎', '😏', \n '😐', '😒', '😘', '😜', '😝', '😡', '😢', '😥', '😦', '😨', '😩', '😬', '😭', '😯', '😱', '😳', '😴', '😶', '😸', \n '😹', '😻', '😼', '😽', '🙀', '🙃', '🙄', '🙅', '🙇', '🙈', '🙋', '🙌', '🙏', '🚀', '🚂', '🚑', '🚗', '🚢', '🚨',\n '🛎', '🛒', '🤔', '🤖', '🤘', '🤣', '🤤', '🤦', '🤩', '🤪', '🤫', '🤭', '🤵', '🤷', '🤼', '🥇', '🥈', '🥉', '🥊', \n '🥰', '🥳', '🦀', '🦄', '🦅', '🦋', '🧛', '🧜'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_convert = {\n    'Ｈ':'H',\n    'Ｋ':'K',\n    'Ｎ':'N',\n    '𝐀':'A',\n    '𝐃':'D',\n    '𝐊':'K',\n    '𝐍':'N',\n    '𝐎':'O',\n    '𝐓':'T',\n    '𝐕':'V',\n    '𝒆':'e',\n    '𝒊':'i',\n    '𝒍':'l',\n    '𝒎':'m',\n    '𝒐':'o',\n    '𝒔':'s',\n    '𝒕':'t',\n    '𝒗':'v',\n    '𝒚':'y',\n    '𝒛':'z',\n    '🇦':'A',\n    '🇧':'B',\n    '🇨':'C',\n    '🇩':'D',\n    '🇪':'E',\n    '🇬':'G',\n    '🇮':'I',\n    '🇰':'K',\n    '🇲':'M',\n    '🇷':'R',\n    'S':'U'\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_clean(text):\n    for char in to_drop:\n        text = text.replace(char, ' ')\n    text = re.sub(r'\\s+', ' ', text)\n    \n    for to_be_replaced, to_replace in to_convert.items():\n        text = text.replace(to_be_replaced, to_replace)\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_clean('𝐓𝐓𝐓𝐓𝐓𝐓𝐓75-летняя Аника 🐺🐺🐺🐺🐺🐺🐺🐺Д. из румынской деревни ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(to_clean)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text to batches convertion","metadata":{}},{"cell_type":"code","source":"vocab = set()\n\nfor news in df['text']:\n    vocab.update(set(news))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = sorted(vocab)\nprint(vocab)\nlen(vocab)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind_to_char = np.array(vocab)\nchar_to_ind = {u:i for i, u in enumerate(vocab)}\nchar_to_ind","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoding = lambda text: np.array([char_to_ind[c] for c in text])\ndf['encoded text'] = df['text'].apply(encoding)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_batches(text, seq_len, step):\n    if len(text) < seq_len:\n        return []\n    else:\n        sequences = []\n        for i in range(0, len(text), step):\n            batch = text[i:i+seq_len+1]\n            if len(batch) > seq_len:\n                sequences.append(text[i:i+seq_len+1])\n        return sequences","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test ='''Как сообщило \"Эхо Москвы\", Конгресс русских общин (КРО) намерен идти на парламентские выборы в едином блоке\nс движением Юрия Болдырева. Об этом заявил сегодня на ХI съезде КРО его лидер Дмитрий Рогозин. Он подверг резкой\nкритике КПРФ, заявив, что \"вместо создания широкого патриотического фронта всем просто предлагают вступить в эту \nпартию\". Движение \"Отечество\", по словам Рогозина, представляет собой партию \"ельцинистов без Ельцина и повторяет \nпуть, уже пройденный НДР\". В качестве приоритетов КРО Дмитрий Рогозин назвал \"борьбу с террором, коррупцией и \nсепаратизмом.\"  Кроме того, делегаты проголосовали за юридическое оформление выхода КРО из состава коллективных \nчленов движения \"Отечество\".  Комментируя это решение, лидер КРО Дмитрий Рогозин заявил, что в настоящее время из \n\"Отеч'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_len = 120\ngenerate_batches(test, seq_len = seq_len, step = 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch in generate_batches(test, seq_len = seq_len, step = 100):\n    print(len(batch))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences = []\nfor text in df['encoded text']:\n    batches = generate_batches(text, seq_len = seq_len, step = 400)\n    sequences.extend(batches)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Number of sequences: {len(sequences)}')\nprint(f'Example of sequence: {sequences[0]}')\nprint(f'Lenght of last sequence: {len(sequences[-1])}')  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sequences, test_sequences = train_test_split(\n    sequences,\n    test_size=0.3, \n    random_state=42\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sequences_array = []\nfor seq in train_sequences:\n    train_sequences_array.extend(seq)\n    \ntest_sequences_array = []\nfor seq in test_sequences:\n    test_sequences_array.extend(seq)\n    \nprint('for training len:', len(train_sequences_array))\nprint('for test len:', len(test_sequences_array))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sequences_array = np.array(train_sequences_array)\ntest_sequences_array = np.array(test_sequences_array)\nprint(train_sequences_array)\n\ntrain_char_dataset = tf.data.Dataset.from_tensor_slices(train_sequences_array)\ntrain_sequences = train_char_dataset.batch(seq_len+1, drop_remainder=True) \n\ntest_char_dataset = tf.data.Dataset.from_tensor_slices(test_sequences_array)\ntest_sequences = test_char_dataset.batch(seq_len+1, drop_remainder=True) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_seq_targets(seq):\n    input_txt = seq[:-1]\n    target_txt = seq[1:]\n    return input_txt, target_txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_sequences.map(create_seq_targets)\ntest_dataset = test_sequences.map(create_seq_targets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\nbuffer_size = 10000\n\ntrain_dataset = train_dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\ntest_dataset = test_dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(vocab)\nvocab_size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"def sparse_cat_loss(y_true,y_pred):\n    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(vocab_size, batch_size):\n    model = Sequential()\n    model.add(Embedding(vocab_size, 64, batch_input_shape=[batch_size, None]))\n    model.add(LSTM(512,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n    model.add(LSTM(512,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n    model.add(LSTM(512,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n    model.add(Dense(vocab_size))\n    model.compile(optimizer='adam', loss=sparse_cat_loss) \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_text(model, start_seed,num_generate=100,temperature=1.0):\n\n    input_eval = [char_to_ind[s] for s in start_seed]\n    input_eval = tf.expand_dims(input_eval, 0)\n\n    text_generated = []\n    model.reset_states()\n\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        predictions = tf.squeeze(predictions, 0)\n        predictions = predictions / temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n        input_eval = tf.expand_dims([predicted_id], 0)\n        text_generated.append(ind_to_char[predicted_id])\n\n    return (start_seed + ''.join(text_generated))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed(101)\nset_seed(101)\n\nes = EarlyStopping(monitor='val_loss',patience=4)\n\nmodel = create_model(\n    vocab_size = vocab_size,\n    batch_size=batch_size\n)\n\nmodel.compile(optimizer='adam', loss=sparse_cat_loss) \n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3 epochs","metadata":{}},{"cell_type":"code","source":"model.fit(\n    train_dataset,\n    epochs = 3,\n    validation_data = test_dataset,\n    callbacks=[es] \n)\n\nmodel.save('LSTM 3x512 3e.h5')\n\nhisto = pd.DataFrame(model.history.history)\nmetric = ['loss', 'val_loss']\nhisto[metric].plot()\nplt.title(' and '.join(metric))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_pred = create_model(vocab_size, batch_size=1)\nmodel_pred.load_weights('LSTM 3x512 3e.h5')\nmodel_pred.build(tf.TensorShape([1, None]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"А\"\nsize = 10000\ntemp = 1\nprint(generate_text(model_pred,string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"А\"\nsize = 10000\ntemp = 0.5\nprint(generate_text(model_pred,string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"А\"\nsize = 10000\ntemp = 0.2\nprint(generate_text(model_pred,string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"А\"\nsize = 10000\ntemp = 0.1\nprint(generate_text(model_pred,string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 10 epochs","metadata":{}},{"cell_type":"code","source":"model.fit(\n    train_dataset,\n    epochs = 7,\n    validation_data = test_dataset,\n    callbacks=[es] \n)\n\nmodel.save('LSTM 3x512 10e.h5')\n\nhisto = pd.DataFrame(model.history.history)\nmetric = ['loss', 'val_loss']\nhisto[metric].plot()\nplt.title(' and '.join(metric))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_pred10 = create_model(vocab_size, batch_size=1)\nmodel_pred10.load_weights('LSTM 3x512 10e.h5')\nmodel_pred10.build(tf.TensorShape([1, None]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"А\"\nsize = 10000\ntemp = 1\nprint(generate_text(model_pred10, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"А\"\nsize = 10000\ntemp = 0.5\nprint(generate_text(model_pred10, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"А\"\nsize = 10000\ntemp = 0.2\nprint(generate_text(model_pred10, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"А\"\nsize = 10000\ntemp = 0.1\nprint(generate_text(model_pred10, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 20 epochs","metadata":{}},{"cell_type":"code","source":"model.fit(\n    train_dataset,\n    epochs = 10,\n    validation_data = test_dataset,\n    callbacks=[es] \n)\n\nmodel.save('LSTM 3x512 20e.h5')\n\nhisto = pd.DataFrame(model.history.history)\nmetric = ['loss', 'val_loss']\nhisto[metric].plot()\nplt.title(' and '.join(metric))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_pred20 = create_model(vocab_size, batch_size=1)\nmodel_pred20.load_weights('LSTM 3x512 20e.h5')\nmodel_pred20.build(tf.TensorShape([1, None]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"А\"\nsize = 10000\ntemp = 1\nprint(generate_text(model_pred20, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"А\"\nsize = 10000\ntemp = 0.5\nprint(generate_text(model_pred20, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"А\"\nsize = 10000\ntemp = 0.2\nprint(generate_text(model_pred20, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"А\"\nsize = 10000\ntemp = 0.1\nprint(generate_text(model_pred20, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 30 epochs","metadata":{}},{"cell_type":"code","source":"model.fit(\n    train_dataset,\n    epochs = 10,\n    validation_data = test_dataset,\n    callbacks=[es] \n)\n\nmodel.save('LSTM 3x512 30e.h5')\n\nhisto = pd.DataFrame(model.history.history)\nmetric = ['loss', 'val_loss']\nhisto[metric].plot()\nplt.title(' and '.join(metric))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_pred30 = create_model(vocab_size, batch_size=1)\nmodel_pred30.load_weights('LSTM 3x512 30e.h5')\nmodel_pred30.build(tf.TensorShape([1, None]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"А\"\nsize = 10000\ntemp = 1\nprint(generate_text(model_pred30, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"А\"\nsize = 10000\ntemp = 0.5\nprint(generate_text(model_pred30, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"А\"\nsize = 10000\ntemp = 0.2\nprint(generate_text(model_pred30, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"А\"\nsize = 10000\ntemp = 0.1\nprint(generate_text(model_pred30, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}