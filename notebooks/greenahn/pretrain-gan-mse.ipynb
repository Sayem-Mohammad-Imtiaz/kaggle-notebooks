{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Pretraining generator and critic models"},{"metadata":{},"cell_type":"markdown","source":"Training a GAN can be hard.\nThis is mainly because of the initialization problem, in the beginning neither generator nor critic knows  which way to go, what is to optimize.\nThis is the case of 'blind leading the blind'.\nHere we pretrain both models, before putting them together as a GAN.\n(credits to J. Howard, fast.ai)\n\nCheck out how the crappy images were generated: https://www.kaggle.com/greenahn/crappify-imgs\n\nAnother version of this notebook with some supervised features in loss function (Mean Absolute Error + FeatureLoss) is at: https://www.kaggle.com/greenahn/pretrain-gan-feature-loss\n\nGithub repository: https://github.com/nupam/GANs-for-Image-enhancement/"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"../input\"))\n\nfrom tqdm import tqdm_notebook as tqdm\n\nimport fastai\nfrom fastai.vision import *\nfrom fastai.callbacks import *\nfrom fastai.utils.mem import *\nfrom fastai.vision.gan import *\nimport gc\nfrom torchvision.models import vgg16_bn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## These folders contain crappy images in different resolution with differnt crappafication logic (randomly selected)\norig_path = Path('../input/flickrproc/hr/hr')\nfnames_df = pd.read_csv('../input/flickrproc/files.csv')\n\nFOLDERS = {256:Path('../input/flickrproc/crappy_256/crappy/'), 320:Path('../input/flickrproc/crappy_320/crappy/'), }\nFOLDERS","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting images ready"},{"metadata":{},"cell_type":"markdown","source":"### First, let us have a look at the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"src=ImageList.from_df(fnames_df, path = orig_path, cols='name')\nsrc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_one(r, c, ax):\n    figsize = (6,6)\n    name = Path(src.items[r]).name\n    if c == 0:\n        ax.title.set_text('original')\n        open_image(src.items[r]).show(ax, figsize=figsize)\n    else:\n        ax.title.set_text('crappy ' + str(c))\n        open_image(list(FOLDERS.values())[c-1]/name).show(ax, figsize=figsize)\n        \nplot_multi(plot_one, 15, 3, figsize=(25, 75))\ndel src","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data(size, bs, folder=256, split=0.9):\n    folder = FOLDERS[folder]\n    src = ImageImageList.from_df(fnames_df, \n                           path = folder, cols='name')\n    src = src.split_by_idx(np.arange(int(src.items.shape[0]*split), src.items.shape[0]))\n    \n    data = src.label_from_func(lambda x: orig_path/Path(x).name).transform(get_transforms(), size=size, tfm_y=True).databunch(bs=bs).normalize(imagenet_stats, do_y=True)\n    data.c = 3\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_gen = get_data(128,32, 256)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_gen.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generator"},{"metadata":{},"cell_type":"markdown","source":"### Loss Function\nIts is simple Mean squared error between input and generated images.\n\n### Model\nThe model used here is a unet with pretrained resnet34\nweight normalization is used for stabalizing the learning process, as batch-normalization adds noise and in GANs it is not desireable"},{"metadata":{"trusted":true},"cell_type":"code","source":"wd = 1e-3\ny_range = (-3.,3.)\nloss_gen = MSELossFlat()\narch = models.resnet34\n\ndef create_gen_learner():\n    return unet_learner(data_gen, arch, wd=wd, blur=True, norm_type=NormType.Weight,\n                         self_attention=True, y_range=y_range, loss_func=loss_gen, model_dir=\"/kaggle/working/\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen = create_gen_learner()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LR selection"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"learn_gen = learn_gen.to_fp16()\nlearn_gen.lr_find(end_lr=1) ## LR selection\nlearn_gen.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.fit_one_cycle(1,1.2e-3, pct_start=0.7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.recorder.plot_lr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.unfreeze()\nlearn_gen.fit_one_cycle(2, slice(1e-5,6e-4), pct_start=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.show_results()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Increasing Image size\nwe increase the image size to 256 and train again"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.freeze()\ndel learn_gen.data, data_gen\nlearn_gen.data = get_data(bs=32, size=256)\nlearn_gen = learn_gen.to_fp16()\ngc.collect()\n\nlearn_gen.lr_find()\nlearn_gen.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.fit_one_cycle(2, 0.0008, pct_start=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.unfreeze()\nlearn_gen.fit_one_cycle(4, slice(8e-6,6e-4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen = learn_gen.to_fp32()\nlearn_gen.save('gen_pre')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### how does it performs now?"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.show_results(rows=10, figsize=(30, 80))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generated images are blurry, it understands where texts are and tries to remove them, but high level features like eyes, lines, circles, fur etc. are ignored.\nGenerated images seem good enough for pretraing, generator will not be blind anymore."},{"metadata":{},"cell_type":"markdown","source":"## Saving generated images\nWe are going then to use it for pretraining critic"},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf gen_imgs/\n!mkdir gen_imgs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path_gen = Path('gen_imgs/')\n\ndef save_preds(dl):\n    i=0\n    names = dl.dataset.items\n    \n    for b in tqdm(dl):\n        preds = learn_gen.pred_batch(batch=b, reconstruct=True)\n        for o in preds:\n            o.save(path_gen/Path(names[i]).name)\n            i += 1\n        del preds\n    del names\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del learn_gen.data\ngc.collect()\ntorch.cuda.empty_cache()\ngpu_mem_get_free()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.data = get_data(256, 16, 256, 1.0)\nsave_preds(learn_gen.data.fix_dl)\nopen_image(path_gen.ls()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del  learn_gen\ngc.collect()\ntorch.cuda.empty_cache()\ngpu_mem_get_free()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Critic"},{"metadata":{},"cell_type":"markdown","source":"### Data\nWe use the above generated images as of one class and original images of another."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_critic_data(bs, size=256, split=0.9):\n    \n    def labeler(x):\n        ret = 'generated' if Path(x).parent.name == 'gen_imgs' else 'original'\n        return ret\n    \n    df = fnames_df\n    valid_names = list(df['name'].iloc[int(split*len(df)):])\n    \n    src1 = ImageList.from_df(df, path = Path('gen_imgs'), cols='name')\n    src2 = ImageList.from_df(df, path = orig_path, cols='name')\n    src1.add(items=src2)\n    \n    src = src1.split_by_valid_func(lambda x : Path(x).name in valid_names)\n    data = src.label_from_func(labeler)\n    data = data.transform(get_transforms(), size=size).databunch(bs=bs).normalize(imagenet_stats)\n    \n    data.c = 3\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_critic = get_critic_data(24)\ndata_critic.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model\nModel used is fast.ai gran_critic with default parameters, it uses spectral normalization, which keeps loss from vanishing or exploding."},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_critic = AdaptiveLoss(nn.BCEWithLogitsLoss())\n\ndef create_critic_learner(data, metrics):\n    return   Learner(data_critic, gan_critic(), metrics=metrics, loss_func=loss_critic, wd=wd, model_dir=\"/kaggle/working/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_critic = create_critic_learner(data_critic, accuracy_thresh_expand)\nlearn_critic.lr_find()\nlearn_critic.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_critic.fit_one_cycle(1, 1e-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_critic.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_critic.save('critic-pre')\n!rm -rf gen_imgs/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rm tmp.pth","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}