{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model as lm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler, label_binarize\nfrom sklearn.metrics import accuracy_score,confusion_matrix,roc_curve, auc, f1_score, precision_score, recall_score\nfrom sklearn.svm import SVC\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bank Marketing Analysis for Term Deposit"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 1. Background"},{"metadata":{},"cell_type":"markdown","source":"The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) was subscribed or not. Data set has 20 predictor varaibles (features) and around 41K rows. Top 6 rows of the dataset is shown below."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/bank-additional-full.csv\",sep=';')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Customers who received phone calls may not be unique and a same customer might have received multiple calls. The last column in the dataset 'y' is our response. This report tries to get the best model to predict the subscription of term deposit. This report is divided into five sections. Graphical analysis is explored in Section 2, various methodologies to treat imbalance in data is discussed in Section 3, different models are fit in Section 4 and conclusive remarks are drawn in Section 5."},{"metadata":{},"cell_type":"markdown","source":"## 2. Graphical Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = data[data['y'] == 'yes']\ndata2 = data[data['y'] == 'no']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the given dataset, there are 10 categorical variables and 10 continous variables. All categorical variables are visualized using stacked barplots (color coded using OSU colors :) for 'yes' and 'no' outcome of term deposit subscription. Below are the 10 barplots."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2, 2, figsize=(12,10))\n\nb1 = ax[0, 0].bar(data1['day_of_week'].unique(),height = data1['day_of_week'].value_counts(),color='#000000')\nb2 = ax[0, 0].bar(data2['day_of_week'].unique(),height = data2['day_of_week'].value_counts(),bottom = data1['day_of_week'].value_counts(),color = '#DC4405') \nax[0, 0].title.set_text('Day of week')\n#ax[0, 0].legend((b1[0], b2[0]), ('Yes', 'No'))\nax[0, 1].bar(data1['month'].unique(),height = data1['month'].value_counts(),color='#000000')\nax[0, 1].bar(data2['month'].unique(),height = data2['month'].value_counts(),bottom = data1['month'].value_counts(),color = '#DC4405') \nax[0, 1].title.set_text('Month')\nax[1, 0].bar(data1['job'].unique(),height = data1['job'].value_counts(),color='#000000')\nax[1, 0].bar(data1['job'].unique(),height = data2['job'].value_counts()[data1['job'].value_counts().index],bottom = data1['job'].value_counts(),color = '#DC4405') \nax[1, 0].title.set_text('Type of Job')\nax[1, 0].tick_params(axis='x',rotation=90)\nax[1, 1].bar(data1['education'].unique(),height = data1['education'].value_counts(),color='#000000') #row=0, col=1\nax[1, 1].bar(data1['education'].unique(),height = data2['education'].value_counts()[data1['education'].value_counts().index],bottom = data1['education'].value_counts(),color = '#DC4405') \nax[1, 1].title.set_text('Education')\nax[1, 1].tick_params(axis='x',rotation=90)\n#ax[0, 1].xticks(rotation=90)\nplt.figlegend((b1[0], b2[0]), ('Yes', 'No'),loc=\"right\",title = \"Term deposit\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2, 3, figsize=(15,10))\n\nb1 = ax[0, 0].bar(data1['marital'].unique(),height = data1['marital'].value_counts(),color='#000000')\nb2 = ax[0, 0].bar(data1['marital'].unique(),height = data2['marital'].value_counts()[data1['marital'].value_counts().index],bottom = data1['marital'].value_counts(),color = '#DC4405') \nax[0, 0].title.set_text('Marital Status')\n#ax[0, 0].legend((b1[0], b2[0]), ('Yes', 'No'))\nax[0, 1].bar(data1['housing'].unique(),height = data1['housing'].value_counts(),color='#000000')\nax[0, 1].bar(data1['housing'].unique(),height = data2['housing'].value_counts()[data1['housing'].value_counts().index],bottom = data1['housing'].value_counts(),color = '#DC4405') \nax[0, 1].title.set_text('Has housing loan')\nax[0, 2].bar(data1['loan'].unique(),height = data1['loan'].value_counts(),color='#000000')\nax[0, 2].bar(data1['loan'].unique(),height = data2['loan'].value_counts()[data1['loan'].value_counts().index],bottom = data1['loan'].value_counts(),color = '#DC4405') \nax[0, 2].title.set_text('Has personal loan')\nax[1, 0].bar(data1['contact'].unique(),height = data1['contact'].value_counts(),color='#000000')\nax[1, 0].bar(data1['contact'].unique(),height = data2['contact'].value_counts()[data1['contact'].value_counts().index],bottom = data1['contact'].value_counts(),color = '#DC4405') \nax[1, 0].title.set_text('Type of Contact')\nax[1, 1].bar(data1['default'].unique(),height = data1['default'].value_counts(),color='#000000')\nax[1, 1].bar(data1['default'].unique(),height = data2['default'].value_counts()[data1['default'].value_counts().index],bottom = data1['default'].value_counts(),color = '#DC4405') \nax[1, 1].title.set_text('Has credit in default')\nax[1, 2].bar(data1['poutcome'].unique(),height = data1['poutcome'].value_counts(),color='#000000')\nax[1, 2].bar(data1['poutcome'].unique(),height = data2['poutcome'].value_counts()[data1['poutcome'].value_counts().index],bottom = data1['poutcome'].value_counts(),color = '#DC4405') \nax[1, 2].title.set_text('Outcome of the previous marketing campaign')\nplt.figlegend((b1[0], b2[0]), ('Yes', 'No'),loc=\"right\",title = \"Term deposit\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the categories in each categorical variable has disproportinate 'yes' compared to 'no' for term deposit subscription. Only noteworthy mention is that in 'Outcome of the previous marketing campaign' variable, failure category has more 'yes' compared to 'no'. This means, if the previous campaign call failed to get the customers to subscribe, repeated calls got customers to subscribe to term deposit. For 10 continous variables, unstacked histograms are plotted below."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2, 2, figsize=(12,10))\n\nax[0, 0].hist(data2['age'],color = '#DC4405',alpha=0.7,bins=20, edgecolor='white') \nax[0, 0].hist(data1['age'],color='#000000',alpha=0.5,bins=20, edgecolor='white')\nax[0, 0].title.set_text('Age')\nax[0, 1].hist(data2['duration'],color = '#DC4405',alpha=0.7, edgecolor='white') \nax[0, 1].hist(data1['duration'],color='#000000',alpha=0.5, edgecolor='white')\nax[0, 1].title.set_text('Contact duration')\nax[1, 0].hist(data2['campaign'],color = '#DC4405',alpha=0.7, edgecolor='white') \nax[1, 0].hist(data1['campaign'],color='#000000',alpha=0.5, edgecolor='white')\nax[1, 0].title.set_text('Number of contacts performed')\nax[1, 1].hist(data2[data2['pdays'] != 999]['pdays'],color = '#DC4405',alpha=0.7, edgecolor='white') \nax[1, 1].hist(data1[data1['pdays'] != 999]['pdays'],color='#000000',alpha=0.5, edgecolor='white')\nax[1, 1].title.set_text('Previous contact days')\nplt.figlegend((b1[0], b2[0]), ('Yes', 'No'),loc=\"right\",title = \"Term deposit\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2, 3, figsize=(15,10))\nax[0, 0].hist(data2['previous'],color = '#DC4405',alpha=0.7, edgecolor='white') \nax[0, 0].hist(data1['previous'],color='#000000',alpha=0.5, edgecolor='white')\nax[0, 0].title.set_text('Number of contacts performed previously')\nax[0, 1].hist(data2['emp.var.rate'],color = '#DC4405',alpha=0.7, edgecolor='white') \nax[0, 1].hist(data1['emp.var.rate'],color='#000000',alpha=0.5, edgecolor='white')\nax[0, 1].title.set_text('Employment variation rate')\nax[0, 2].hist(data2['cons.price.idx'],color = '#DC4405',alpha=0.7, edgecolor='white') \nax[0, 2].hist(data1['cons.price.idx'],color='#000000',alpha=0.5, edgecolor='white')\nax[0, 2].title.set_text('Consumer price index')\nax[1, 0].hist(data2['cons.conf.idx'],color = '#DC4405',alpha=0.7, edgecolor='white') \nax[1, 0].hist(data1['cons.conf.idx'],color='#000000',alpha=0.5, edgecolor='white')\nax[1, 0].title.set_text('Consumer confidence index')\nax[1, 1].hist(data2['euribor3m'],color = '#DC4405',alpha=0.7, edgecolor='white') \nax[1, 1].hist(data1['euribor3m'],color='#000000',alpha=0.5, edgecolor='white')\nax[1, 1].title.set_text('Euribor 3 month rate')\nax[1, 2].hist(data2['nr.employed'],color = '#DC4405',alpha=0.7, edgecolor='white') \nax[1, 2].hist(data1['nr.employed'],color='#000000',alpha=0.5, edgecolor='white')\nax[1, 2].title.set_text('Number of employees')\nplt.figlegend((b1[0], b2[0]), ('Yes', 'No'),loc=\"right\",title = \"Term deposit\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the histograms show similar behavior (proportion of 'no' is higher than 'yes') except 'Previous contact days'. Proportion of 'yes' is higher compared to 'no' (only if contacted more than once)."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = data.iloc[:,0:20]\npredictors = predictors.drop(['pdays'],axis=1)\ny = data.iloc[:,20]\nX = pd.get_dummies(predictors)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Treating Imbalanced Data"},{"metadata":{},"cell_type":"markdown","source":"Given data set is highly imbalanced, i.e. number of data belonging to 'no' category is way higher than 'yes' category."},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This imbalance has to treated so as to make sure that there is no bias in modeling. Imbalance is generally treated in three ways."},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Random Undersampling"},{"metadata":{},"cell_type":"markdown","source":"In this method, the majority category, in this case 'no' category is randomly sampled to match the size of the minority 'yes' category. Remaining data of majority category is discarded."},{"metadata":{"trusted":true},"cell_type":"code","source":"rus = RandomUnderSampler(random_state=0)\nX_Usampled, y_Usampled = rus.fit_resample(X, y)\npd.Series(y_Usampled).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Random Oversampling"},{"metadata":{},"cell_type":"markdown","source":"In this method, the minority category 'no' is randomly sampled with replacement to match the size of the majority 'no' category. Minority category entries will be repeated many times."},{"metadata":{"trusted":true},"cell_type":"code","source":"ros = RandomOverSampler(random_state=0)\nX_Osampled, y_Osampled = ros.fit_resample(X, y)\npd.Series(y_Osampled).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3 SMOTE - Synthetic Minority Oversampling Technique"},{"metadata":{},"cell_type":"markdown","source":"This is an oversampling technique in which instead of randomly repeating minority 'yes' category, new entires are sythetically created maintaining the convexity of minority entry space. Minority category will again match the majority category samples."},{"metadata":{"trusted":true},"cell_type":"code","source":"sm = SMOTE(random_state=0)\nX_SMOTE, y_SMOTE = sm.fit_resample(X, y)\npd.Series(y_SMOTE).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These different imbalance treatements will be used in different data modeling techniques based on the need."},{"metadata":{},"cell_type":"markdown","source":"## 4. Model Fitting"},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Perceptron"},{"metadata":{},"cell_type":"markdown","source":"Simple linear classifier is fitted on the imbalanced data. Model is highly biased towards 'no' class. Accuracy is high for this model which is expected for imbalanced data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nsc = StandardScaler()\nsc.fit(X_train)\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)\nperp_model = lm.Perceptron().fit(X_train_std,y_train)\ny_pred = perp_model.predict(X_test_std)\nprint(\"Accuracy: \",round(accuracy_score(y_test, y_pred),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion matrix gives a better idea. Categorization of minority 'yes' class is far from perfect which is indicated by poor precision score and recall score."},{"metadata":{"trusted":true},"cell_type":"code","source":"mat = confusion_matrix(y_test,y_pred,labels=['no','yes'])\nprint(mat)\ny_test = label_binarize(y_test,classes=['no','yes'])\ny_pred = label_binarize(y_pred,classes=['no','yes'])\nprint(\"Precision: \",round(precision_score(y_test,y_pred),2),\"Recall: \",round(recall_score(y_test,y_pred),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When percptron model is fit for SMOTE data, bias is shifted towards 'yes' class. This results in better recall score and worse precision score than before."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nsm = SMOTE(random_state=0)\nX_SMOTE, y_SMOTE = sm.fit_resample(X_train, y_train)\nsc = StandardScaler()\nsc.fit(X_SMOTE)\nX_train_std = sc.transform(X_SMOTE)\nX_test_std = sc.transform(X_test)\nperp_model = lm.Perceptron().fit(X_train_std,y_SMOTE)\ny_pred = perp_model.predict(X_test_std)\nprint(\"Accuracy: \",round(accuracy_score(y_test, y_pred),2))\nmat = confusion_matrix(y_test,y_pred)\nprint(\"Confusion Matrix: \\n\",mat)\ny_test = label_binarize(y_test,classes=['no','yes'])\ny_pred = label_binarize(y_pred,classes=['no','yes'])\nprint(\"Precision: \",round(precision_score(y_test,y_pred),2),\"Recall: \",round(recall_score(y_test,y_pred),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Decision Tree"},{"metadata":{},"cell_type":"markdown","source":"Decision tree is another simple yet powerful classification tool. But this model faces the same problem of bad precision score and bad recall score for imbalanced data."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\ntree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=7)\nmodel = tree.fit(X_train,y_train)\ny_pred = model.predict(X_test)\ny_test = label_binarize(y_test,classes=['no','yes'])\ny_pred = label_binarize(y_pred,classes=['no','yes'])\nprint(\"Precision: \",round(precision_score(y_test,y_pred),2),\"Recall: \",round(recall_score(y_test,y_pred),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When decision tree is fit on SMOTE data, recall score improves but precision score deteriorates."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\ntree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=7)\nX_SMOTE, y_SMOTE = sm.fit_resample(X_train, y_train)\nmodel = tree.fit(X_SMOTE,y_SMOTE)\ny_pred = model.predict(X_test)\ny_test = label_binarize(y_test,classes=['no','yes'])\ny_pred = label_binarize(y_pred,classes=['no','yes'])\nprint(\"Precision: \",round(precision_score(y_test,y_pred),2),\"Recall: \",round(recall_score(y_test,y_pred),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 Random Forest"},{"metadata":{},"cell_type":"markdown","source":"Random forest is an ensemble technique which reduces the variance in the classification technique. But this classification worsens the bias that is already in the data. Random forest performs badly in terms of precision and recall when applied on imbalanced data."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nforest = RandomForestClassifier(n_estimators= 1000,criterion=\"gini\", max_depth=5,min_samples_split = 0.4,min_samples_leaf=1, class_weight=\"balanced\")\nmodel = forest.fit(X_train,y_train)\ny_pred = model.predict(X_test)\npd.Series(y_pred).value_counts()\ny_test = label_binarize(y_test,classes=['no','yes'])\ny_pred = label_binarize(y_pred,classes=['no','yes'])\nprint(\"Precision: \",round(precision_score(y_test,y_pred),2),\"Recall: \",round(recall_score(y_test,y_pred),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random forest classification hardly performs better when applied on SMOTE data in terms of precision and recall."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nforest = RandomForestClassifier(n_estimators= 1000,criterion=\"gini\", max_depth=5,min_samples_split = 0.4,min_samples_leaf=1, class_weight=\"balanced\")\nX_SMOTE, y_SMOTE = sm.fit_resample(X_train, y_train)\nmodel = forest.fit(X_SMOTE,y_SMOTE)\ny_pred = model.predict(X_test)\npd.Series(y_pred).value_counts()\ny_test = label_binarize(y_test,classes=['no','yes'])\ny_pred = label_binarize(y_pred,classes=['no','yes'])\nprint(\"Precision: \",round(precision_score(y_test,y_pred),2),\"Recall: \",round(recall_score(y_test,y_pred),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4 Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"Logisitic regression was fit on imbalanced, random undersampled, random oversampled and SMOTE data. Last three models (logistic regression on treated data) performs fairly better than previous models especially in terms of recall, but still there is room for improvement on precision."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nmodel = lm.LogisticRegression(random_state=0, solver='lbfgs',multi_class='auto',max_iter=1000).fit(X_train,y_train)\ny_pred = model.predict_proba(X_test)\ny_pred = y_pred[:,1]\ny_test = label_binarize(y_test,classes=['no','yes'])\nfpr_imb, tpr_imb, _ = roc_curve(y_test, y_pred)\nroc_auc_imb = auc(fpr_imb, tpr_imb)\ny_pred = model.predict(X_test)\ny_pred = label_binarize(y_pred,classes=['no','yes'])\nprint(\"Imbalanced -\")\nprint(\"Precision: \",round(precision_score(y_test,y_pred),2),\"Recall: \",round(recall_score(y_test,y_pred),2))\n# Undersampled\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nrus = RandomUnderSampler(random_state=0)\nX_Usampled, y_Usampled = rus.fit_resample(X_train, y_train)\nmodel = lm.LogisticRegression(random_state=0, solver='lbfgs',multi_class='auto',max_iter=5000).fit(X_Usampled,y_Usampled)\ny_pred = model.predict_proba(X_test)\ny_pred = y_pred[:,1]\ny_test = label_binarize(y_test,classes=['no','yes'])\nfpr_us, tpr_us, _ = roc_curve(y_test, y_pred)\nroc_auc_us = auc(fpr_us, tpr_us)\ny_pred = model.predict(X_test)\ny_pred = label_binarize(y_pred,classes=['no','yes'])\nprint(\"Random undersampled -\")\nprint(\"Precision: \",round(precision_score(y_test,y_pred),2),\"Recall: \",round(recall_score(y_test,y_pred),2))\n# Oversampled\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nros = RandomOverSampler(random_state=0)\nX_Osampled, y_Osampled = ros.fit_resample(X_train, y_train)\nmodel = lm.LogisticRegression(random_state=0, solver='lbfgs',multi_class='auto',max_iter=5000).fit(X_Osampled, y_Osampled)\ny_pred = model.predict_proba(X_test)\ny_pred = y_pred[:,1]\ny_test = label_binarize(y_test,classes=['no','yes'])\nfpr_os, tpr_os, _ = roc_curve(y_test, y_pred)\nroc_auc_os = auc(fpr_os, tpr_os)\ny_pred = model.predict(X_test)\ny_pred = label_binarize(y_pred,classes=['no','yes'])\nprint(\"Random oversampled -\")\nprint(\"Precision: \",round(precision_score(y_test,y_pred),2),\"Recall: \",round(recall_score(y_test,y_pred),2))\n# SMOTE\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nsm = SMOTE(random_state=0)\nX_SMOTE, y_SMOTE = sm.fit_resample(X_train, y_train)\nmodel = lm.LogisticRegression(random_state=0, solver='lbfgs',multi_class='auto',max_iter=5000).fit(X_SMOTE,y_SMOTE)\ny_pred = model.predict_proba(X_test)\ny_pred = y_pred[:,1]\ny_test = label_binarize(y_test,classes=['no','yes'])\nfpr_smote, tpr_smote, _ = roc_curve(y_test, y_pred)\nroc_auc_smote = auc(fpr_smote, tpr_smote)\ny_pred = model.predict(X_test)\ny_pred = label_binarize(y_pred,classes=['no','yes'])\nprint(\"SMOTE -\")\nprint(\"Precision: \",round(precision_score(y_test,y_pred),2),\"Recall: \",round(recall_score(y_test,y_pred),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ROC curve depicts the variation of True Positive Rate to False Positive Rate. Area under ROC curve is slightly better for over sampled data compared to imbalanced and undersampled data."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nlw = 2\nplt.plot(fpr_imb, tpr_imb,\n         label='Imbalanced data ROC curve (area = {0:0.4f})'\n               ''.format(roc_auc_imb),\n         color='deeppink', linestyle=':', linewidth=2)\n\nplt.plot(fpr_us, tpr_us,\n         label='Undersampled data ROC curve (area = {0:0.4f})'\n               ''.format(roc_auc_us),\n         color='blue', linestyle='--', linewidth=2)\n\nplt.plot(fpr_os, tpr_os,\n         label='Random Oversampled data ROC curve (area = {0:0.4f})'\n               ''.format(roc_auc_os),\n         color='darkred', linestyle='--', linewidth=2)\n\nplt.plot(fpr_smote, tpr_smote,\n         label='SMOTE data ROC curve (area = {0:0.4f})'\n               ''.format(roc_auc_smote),\n         color='darkgreen', linestyle='--', linewidth=2)\n\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.00])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.5 SVM"},{"metadata":{},"cell_type":"markdown","source":"Support Vector Machine is employed for SMOTE data using two kernels: linear and guassian. Gaussian kernel performs the best in terms of both precision and recall."},{"metadata":{"trusted":true},"cell_type":"code","source":"sm = SMOTE(random_state=0)\nX_SMOTE, y_SMOTE = sm.fit_resample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X_SMOTE, y_SMOTE, test_size=0.3)\nsvm = SVC(kernel='linear')\nmodel = svm.fit(X_train, y_train)\ny_pred = model.predict(X_test)\ny_test = label_binarize(y_test,classes=['no','yes'])\ny_pred = label_binarize(y_pred,classes=['no','yes'])\nprint(\"Linear kernel- \",\"Precision: \",round(precision_score(y_test,y_pred),2),\"Recall: \",round(recall_score(y_test,y_pred),2))\nfpr_linear, tpr_linear, _ = roc_curve(y_test, y_pred)\nroc_auc_linear = auc(fpr_linear, tpr_linear)\nsm = SMOTE(random_state=0)\nX_SMOTE, y_SMOTE = sm.fit_resample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X_SMOTE, y_SMOTE, test_size=0.3)\nsvm = SVC(kernel='rbf')\nmodel = svm.fit(X_train, y_train)\ny_pred = model.predict(X_test)\ny_test = label_binarize(y_test,classes=['no','yes'])\ny_pred = label_binarize(y_pred,classes=['no','yes'])\nprint(\"Guassian kernel- \",\"Precision: \",round(precision_score(y_test,y_pred),2),\"Recall: \",round(recall_score(y_test,y_pred),2))\nfpr_rbf, tpr_rbf, _ = roc_curve(y_test, y_pred)\nroc_auc_rbf = auc(fpr_rbf, tpr_rbf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ROC curves for two different kernels."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nlw = 2\n\nplt.plot(fpr_linear, tpr_linear,\n         label='Linear Kernel ROC curve (area = {0:0.4f})'\n               ''.format(roc_auc_linear),\n         color='darkred', linestyle='--', linewidth=2)\n\nplt.plot(fpr_rbf, tpr_rbf,\n         label='Gaussian Kernel ROC curve (area = {0:0.4f})'\n               ''.format(roc_auc_rbf),\n         color='darkgreen', linestyle='--', linewidth=2)\n\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.00])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Conclusion"},{"metadata":{},"cell_type":"markdown","source":"For the given data, visualization of data, ways to treat imbalance in the data and best predictive model to determine the term deposit subscription was explored. From visualization, it can be derived that repeated campaign calls to customers within 20 days of previous call increases the subscription. After treating the imbalance in data using SMOTE, SVM with Gaussian kernel performs the best in terms of precision and recall."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}