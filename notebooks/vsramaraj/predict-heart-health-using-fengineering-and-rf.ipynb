{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"    %reset -f\n    # 1.1 Call data manipulation libraries\n    import pandas as pd\n    import numpy as np\n    \n    # 1.2 Feature creation libraries\n    from sklearn.random_projection import SparseRandomProjection as sr  # Projection features\n    from sklearn.cluster import KMeans                    # Cluster features\n    from sklearn.preprocessing import PolynomialFeatures  # Interaction features\n    \n    # 1.3 For feature selection\n    from sklearn.feature_selection import SelectKBest\n    from sklearn.feature_selection import mutual_info_classif  # Selection criteria\n    \n    # 1.4 Data processing\n    # 1.4.1 Scaling data in various manner\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler, scale\n    # 1.4.2 Transform categorical (integer) to dummy\n    from sklearn.preprocessing import OneHotEncoder\n    \n    # 1.5 Splitting data\n    from sklearn.model_selection import train_test_split\n    \n    # 1.6 Decision tree modeling\n    from sklearn.tree import  DecisionTreeClassifier as dt\n    \n    # 1.7 RandomForest modeling\n    from sklearn.ensemble import RandomForestClassifier as rf\n    \n    # 1.8 Plotting libraries to plot feature importance\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    \n    # 1.9 Misc\n    import os, time, gc","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/heart.csv\")\ndata.head()\ndata.shape  # . (303, 14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bd4d7a07b9aa4c75afabe305e6a0daaa7d06cc0"},"cell_type":"code","source":"# Split the CSV in training and test data\nX_train, X_test, y_train, y_test = train_test_split(\n        data.drop('target', 1), \n        data['target'], \n        test_size = 0.3, \n        random_state=10\n        ) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82888764b5169f7c9ed26ac6a37d7575f07db170"},"cell_type":"code","source":"# 2.2 Look at data\nX_train.head(5)\nX_train.shape  # (212, 13)                  \nX_test.shape   # (91,)         \n# 2.3 Data types\nX_train.dtypes.value_counts() \n# int64      12\n# float64     1\n# dtype: int64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"853fd8d470a556a588e85ff9349f9bd707b2ac4f"},"cell_type":"code","source":"# 2.4 Target classes are almost balanc\ndata.target.value_counts()\n\n# check if any row has all zeroes or nulls\nx = np.sum(data, axis = 1)\nx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9e716707e0ac946b816d24640dab0a6092ddcc5"},"cell_type":"code","source":"# 3 Check if there are Missing values? None\ndata.isnull().sum().sum()  # 0\nX_test.isnull().sum().sum()   # 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9b92712696b1ac20d810594ebd73ad3b21abfdb"},"cell_type":"code","source":"############################ BB. Feature Engineering #########################\n\n## i)   Shooting in dark. These features may help or may not help\n## ii)  There is no theory as to which features will help\n## iii) Fastknn is another method not discussed here\n\n############################################################################\n############################ Using Statistical Numbers #####################\n\n\n#  4. Feature 1: Row sums of features 1:93. More successful\n#                when data is binary.\n\nX_train['sum'] = X_train.sum(numeric_only = True, axis=1)  # numeric_only= None is default\nX_test['sum'] = X_test.sum(numeric_only = True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2e293f2aaff1ca86f36ad454b7d92dd4e4a6867"},"cell_type":"code","source":"# 4.1 Assume that value of '0' in a cell implies missing feature\n#     Transform train and test dataframes\n#     replacing '0' with NaN\n#     Use pd.replace()\ntmp_train = X_train.replace(0, np.nan)\ntmp_test = X_test.replace(0,np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f418bf059a100c5f956d3825b620c7f6978511a6"},"cell_type":"code","source":"# 4.2 Check if tmp_train is same as train or is a view\n#     of train? That is check if tmp_train is a deep-copy\n\ntmp_train is X_train                # False\n#tmp_train is train.values.base    # False\ntmp_train._is_view                # False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97ad3f1f29886b5039c09cca4a04ce3130917683"},"cell_type":"code","source":"# 4.3 Check if 0 has been replaced by NaN\ntmp_train.head(1)\ntmp_test.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c32dc8712a9b8c7255a8d302486a9b706c31791"},"cell_type":"code","source":"# 5. Feature 2 : For every row, how many features exist\n#                that is are non-zero/not NaN.\n#                Use pd.notna()\ntmp_train.notna().head(1)\nX_train[\"count_not0\"] = tmp_train.notna().sum(axis = 1)\nX_test['count_not0'] = tmp_test.notna().sum(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e893b88655fe5f0b429b346a2a64ba80c4e206cc"},"cell_type":"code","source":"# 6. Similary create other statistical features\n#    Feature 3\n#    Pandas has a number of statistical functions\n#    Ref: https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#computations-descriptive-stats\n\nfeat = [ \"var\", \"median\", \"mean\", \"std\", \"max\", \"min\"]\nfor i in feat:\n    X_train[i] = tmp_train.aggregate(i,  axis =1)\n    X_test[i]  = tmp_test.aggregate(i,axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05c399b731e7e88d17f65efb384c142f26623c4e"},"cell_type":"code","source":"# 7 Delete not needed variables and release memory\ndel(tmp_train)\ndel(tmp_test)\ngc.collect()\n\n\n# 7.1 So what do we have finally\nX_train.shape                # 61878 X (1+ 93 + 8) ; 93rd Index is target\nX_train.head(1)\nX_test.shape                 # 144367 X (93 + 8)\nX_test.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"facf9a44e69779c183b29f151d8b930c4696a138"},"cell_type":"code","source":"# 8. Before we proceed further, keep target feature separately\ntarget = y_train\ntarget.tail(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da283b73041207a14d7ba0a36be715797f42c00b"},"cell_type":"code","source":"# 9.1 And then drop 'target' column from train\n#      'test' dataset does not have 'target' col\n#X_train.drop(columns = ['target'], inplace = True)\nX_train.shape                # 61878 X 101\n\n\n# 9.2. Store column names of our data somewhere\n#     We will need these later (at the end of this code)\ncolNames = X_train.columns.values\ncolNames","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd95fcdddda8470cd9167d895849373479f07c54"},"cell_type":"code","source":"###########################################################################\n################ Feature creation Using Random Projections ##################\n# 10. Random projection is a fast dimensionality reduction feature\n#     Also used to look at the structure of data\n\n# 11. Generate features using random projections\n#     First stack train and test data, one upon another\ntmp = pd.concat([X_train,X_test],\n                axis = 0,            # Stack one upon another (rbind)\n                ignore_index = True\n                )\n\ntmp.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebef3a0466c0810e30257b2ac941356290842461"},"cell_type":"code","source":"# 12.2 Transform tmp t0 numpy array\n#      Henceforth we will work with array only\ntmp = tmp.values\ntmp.shape       # (303, 21)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef958c2a019499f103ae8ea65a5ca1f019727359"},"cell_type":"code","source":"# 13. Let us create 10 random projections/columns\n#     This decision, at present, is arbitrary\nNUM_OF_COM = 8\n\n# 13.1 Create an instance of class\nrp_instance = sr(n_components = NUM_OF_COM)\n\n# 13.2 fit and transform the (original) dataset\n#      Random Projections with desired number\n#      of components are returned\nrp = rp_instance.fit_transform(tmp[:, :13])\n\n# 13.3 Look at some features\nrp[: 5, :  3]\nnp.shape\n\n# 13.4 Create some column names for these columns\n#      We will use them at the end of this code\nrp_col_names = [\"r\" + str(i) for i in range(8)]\nrp_col_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6203cf5575b42ac17e36cbc8f6788a562357bc8"},"cell_type":"code","source":"rp = rp_instance.fit_transform(tmp[:, :93])\nrp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc73f397a3cf4543edda3c0da9723f29edbf2ef8"},"cell_type":"code","source":"###############################################################################\n############################ Feature creation using kmeans ####################\n######################Can be skipped without loss of continuity################\n\n\n# 14. Before clustering, scale data\n# 15.1 Create a StandardScaler instance\nse = StandardScaler()\n# 15.2 fit() and transform() in one step\ntmp = se.fit_transform(tmp)\n# 15.3\ntmp.shape               # 303 X 21 (an ndarray)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c027e06497918186042ccd547fd178afe5852049"},"cell_type":"code","source":"tmp[: 5, :  3]\n\n# 16. Perform kmeans using 93 features.\n#     No of centroids is no of classes in the 'target'\ncenters = target.nunique()    # 2 unique classes\ncenters               # 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1309eba38bf37ec6dddb54a05cc22d82112eab53"},"cell_type":"code","source":"# 17.1 Begin clustering\nstart = time.time()\n\n# 17.2 First create object to perform clustering\nkmeans = KMeans(n_clusters=centers, # How many\n                n_jobs = 4)         # Parallel jobs for n_init\n\n\n\n# 17.3 Next train the model on the original data only\n#fit the cluster \nkmeans.fit(tmp[:, : 13])\n\nend = time.time()\n(end-start)/60.0      # .39","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"624c8c95633974a85a1654b0b77cf306d0301aa5"},"cell_type":"code","source":"# 18 Get clusterlabel for each row (data-point)\nkmeans.labels_\nkmeans.labels_.size   # 303","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d0dbeb59c97d53ce0967c6a1e1b85c7a7015c81"},"cell_type":"code","source":"# 19. Cluster labels are categorical. So convert them to dummy\n\n# 19.1 Create an instance of OneHotEncoder class\nohe = OneHotEncoder(sparse = False)\n\n# 19.2 Use ohe to learn data\n#      ohe.fit(kmeans.labels_)\nohe.fit(kmeans.labels_.reshape(-1,1))     # reshape(-1,1) recommended by fit()\n                                          # '-1' is a placeholder for actual\n# 19.3 Transform data now\ndummy_clusterlabels = ohe.transform(kmeans.labels_.reshape(-1,1))\ndummy_clusterlabels\ndummy_clusterlabels.shape    # 303, 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aaaad819213258397d7442db9bba4f3eee69470a"},"cell_type":"code","source":"# 19.4 We will use the following as names of new nine columns\n#      We need them at the end of this code\n\nk_means_names = [\"k\" + str(i) for i in range(2)]\nk_means_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34a2a1b17634ce811d9fe5598f58d96cb53bc8a9"},"cell_type":"code","source":"############################ Interaction features #######################\n# 21. Will require lots of memory if we take large number of features\n#     Best strategy is to consider only impt features\n\ndegree = 2\npoly = PolynomialFeatures(degree,                 # Degree 2\n                          interaction_only=True,  # Avoid e.g. square(a)\n                          include_bias = False    # No constant term\n                          )\n\n\n# 21.1 Consider only first 8 features\n#      fit and transform\ndf =  poly.fit_transform(tmp[:, : 8])\n\n\ndf.shape     # 303, 36\n\npoly_names = [ \"poly\" + str(i)  for i in range(36)]\npoly_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0f131959123738425678fc842998edce40362af"},"cell_type":"code","source":"################ concatenate all features now ##############################\n\n# 22 Append now all generated features together\n# 22 Append random projections, kmeans and polynomial features to tmp array\n\ntmp.shape          # 303 X 21","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53a0b22db406c7ec9c68a1c30451956da6d14679"},"cell_type":"code","source":"#  22.1 If variable, 'dummy_clusterlabels', exists, stack kmeans generated\n#       columns also else not. 'vars()'' is an inbuilt function in python.\n#       All python variables are contained in vars().\n\nif ('dummy_clusterlabels' in vars()):               #\n    tmp = np.hstack([tmp,rp,dummy_clusterlabels, df])\nelse:\n    tmp = np.hstack([tmp,rp, df])       # No kmeans      <==\n\n    \ntmp.shape          # 303X 67  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b55d41a07d56839d90cbd4a9902b505341649a36"},"cell_type":"code","source":"###Data modelling done*********************************************\n# 22.1 Separate train and test\nX = tmp\nX.shape                             # 61878 X 135 if no kmeans: (61878, 126)\n\n# 22.2\ny = pd.concat([y_train,y_test],\n                axis = 0,            # Stack one upon another (rbind)\n                ignore_index = True\n                )\ny.shape        # 303,\n\n# 22.3 Delete tmp\ndel tmp\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c514c2d4b0711ce25c23a68feea5383b1a72eae3"},"cell_type":"code","source":"################## Model building #####################\n\n\n# 23. Split train into training and validation dataset\nX_train, X_test, y_train, y_test = train_test_split(\n                                                    X,\n                                                    y,\n                                                    test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d417b341c0d69a1632367044376485e847039ea"},"cell_type":"code","source":"# 23.1\nX_train.shape    # 43314 X 135  if no kmeans: (212, 67)\nX_test.shape     # 18564 X 135; if no kmeans: (91, 67)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef81e21a3baa194b0f17ff75b227afbf47737399"},"cell_type":"code","source":"# 24 Decision tree classification\n# 24.1 Create an instance of class\nclf1_dt = dt(min_samples_split = 5,\n         min_samples_leaf= 3\n        )\n\nstart = time.time()\n# 24.2 Fit/train the object on training data\n#      Build model\nclf1_dt = clf1_dt.fit(X_train, y_train)\nend = time.time()\n(end-start)/60                     \n###0.0001556873321533203","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb347a47d5ccd87c14d2732a1a276235c8d5d117"},"cell_type":"code","source":"# 25. Instantiate RandomForest classifier\nclf1_rf = rf(n_estimators=50)\n\n# 25.1 Fit/train the object on training data\n#      Build model\n\nstart = time.time()\nclf1_rf = clf1_rf.fit(X_train, y_train)\nend = time.time()\n(end-start)/60     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6389b6ef19f239387858f974bcff6ff6eb215851"},"cell_type":"code","source":"# 25.2 Use model to make predictions\nclasses1_rf = clf1_rf.predict(X_test)\n# 25.3 Check accuracy\n(classes1_rf == y_test).sum()/y_test.size ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7aa586cc3678452b1df6ba9faef2e88af88e275"},"cell_type":"code","source":"##****************************************\n## Using feature importance given by model\n##****************************************\n\n# 26. Get feature importance\n# 26. Get feature importance\nclf1_rf.feature_importances_        # Column-wise feature importance\nclf1_rf.feature_importances_.size   # 67","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e634e839470fbc6218164597a7af35bd0c02771"},"cell_type":"code","source":"# 26.1 To our list of column names, append all other col names\n#      generated by random projection, kmeans (onehotencoding)\n#      and polynomial features\n#      But first check if kmeans was used to generate features\n\nif ('dummy_clusterlabels' in vars()):       # If dummy_clusterlabels labels are defined\n    colNames = list(colNames) + rp_col_names+ k_means_names + poly_names\nelse:\n    colNames = colNames = list(colNames) + rp_col_names +  poly_names      # No kmeans      <==\n\n# 26.1.1 So how many columns?\nlen(colNames)           # 67 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7789b58afadef6def0b25343ace98b781d628ef"},"cell_type":"code","source":"# 26.2 Create a dataframe of feature importance and corresponding\n#      column names. Sort dataframe by importance of feature\nfeat_imp = pd.DataFrame({\n                   \"importance\": clf1_rf.feature_importances_ ,\n                   \"featureNames\" : colNames\n                  }\n                 ).sort_values(by = \"importance\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7242e5b217b2219512fbcfde9a6d1d2514f91ca"},"cell_type":"code","source":"feat_imp.shape              \nfeat_imp.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fac44469ca1d514435a34416ff5d2509dbbf6f07"},"cell_type":"code","source":"# 26.3 Plot feature importance for first 20 features\ng = sns.barplot(x = feat_imp.iloc[  : 20 ,  1] , y = feat_imp.iloc[ : 20, 0])\ng.set_xticklabels(g.get_xticklabels(),rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db5eb055b527062ca1257885f7b68801d81444dd"},"cell_type":"code","source":"# 27 Select top 13 columns and get their indexes\n#      Note that in the selected list few kmeans\n#      columns also exist\nnewindex = feat_imp.index.values[:13]\nnewindex","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4f8c909d47a78bce92198f435ebe8bfd4ef6c57"},"cell_type":"code","source":"# 27.1 Use these top 13 columns for classification\n# 28.1  Create DTree classifier object\nclf2_dt = dt(min_samples_split = 5, min_samples_leaf= 3)\n\n# 27.2 Train the object on data\nstart = time.time()\nclf2_dt = clf2_dt.fit(X_train[: , newindex], y_train)\nend = time.time()\n(end-start)/60    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1c0d190eb2eebbfc4dda8608bc9a1860b18d4ff"},"cell_type":"code","source":"# 27.3  Make prediction\nclasses2_dt = clf2_dt.predict(X_test[: , newindex])\n\n# 27.4 Accuracy?\n(classes2_dt == y_test).sum()/y_test.size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4ebe9577e9c99ec63bc29a377513b430679b8be"},"cell_type":"code","source":"# 27x Select top 20 columns and get their indexes\n#      Note that in the selected list few kmeans    columns also exist\nnewindex2 = feat_imp.index.values[:20]\nnewindex2\n\n# 27.1x Use these top 13 columns for classification\n# 28.1x  Create DTree classifier object\nclf2_dt = dt(min_samples_split = 8, min_samples_leaf= 2)\n\n# 27.2 Train the object on data\nstart = time.time()\nclf2_dt = clf2_dt.fit(X_train[: , newindex2], y_train)\nend = time.time()\n(end-start)/60  \n\n# 27.3x  Make prediction\nclasses2_dt = clf2_dt.predict(X_test[: , newindex2])\n\n# 27.4x Accuracy?\n(classes2_dt == y_test).sum()/y_test.size \n##########################################################\n#   *Accuracy ~ 78.02 -- increased from previous of 71%  #\n##########################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c49bd9bfcf30f043994f2eae9347004ed14c284"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a4b81d06a944bf357685938c319c4b93f5ebe43"},"cell_type":"code","source":"# 25x. Instantiate RandomForest classifier\nclf2_rf = rf(n_estimators=100)\n## changed estimators from 50 to 100\n\n# 25.1x Fit/train the object on training data\n#      Build model\n\nstart = time.time()\nclf2_rf = clf2_rf.fit(X_train[:,newindex2], y_train)\n# newindex2 has top 20 features\n\nend = time.time()\n(end-start)/60      \n\n# 25.2x Use model to make predictions\nclasses2_rf = clf2_rf.predict(X_test[: , newindex2])\n# 25.3x Check accuracy\n(classes2_rf == y_test).sum()/y_test.size \n##################################################################################\n# Accuracy remained almost the same around 84 to 87 percent. No major variation  #\n##################################################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bf577d42ca19a6c9aafe37f565a1d7f373456d5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}