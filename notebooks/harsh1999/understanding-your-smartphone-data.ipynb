{"cells":[{"metadata":{},"cell_type":"markdown","source":"# > Understanding your Smartphone Data\n\n* In this kernel, we work with the smartphone dataset. We are given certain activity features about users and also the activity that they are engaged in. The data is from the various sensors that are present in the smartphone that these users own.\n* We are required to feed all these inputs to a network and then predict what activity the user is currently engaged in. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing the libraries:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support as error_metric\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Loading the training and testing set:\n\nprint(os.listdir(\"../input\"))\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting a look at our training data:\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let us also check for null values in the training and test sets:\n\nprint(train.isnull().values.any())\nprint(test.isnull().values.any())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let us remove the columns that are not necessary from both the training as well as the test set:\n\ntrain.drop('subject', axis = 1, inplace = True)\ntest.drop('subject', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us check the datatype we are dealing with in both the training and testing set:\n\nprint(train.dtypes.value_counts())\nprint(test.dtypes.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting a better sense of our training data:\n\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we will try adn find the columns which have data type object because we need to one hot encode them:\n\n\nobject_feature = train.dtypes == np.object\nobject_feature = train.columns[object_feature]\nobject_feature\n\n# Thus we observe that only one column that is 'Activity' is of data type object and thus we will one hot encode it.\ntrain['Activity']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encoding the 'Activity' column:\n\nlabel_encoder = LabelEncoder()\nfor x in [train, test]:\n    x['Activity'] = label_encoder.fit_transform(x.Activity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if our label encoding worked as expected:\n\ntrain.Activity.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now, we try finding the correlations between different features using pandas.corr():\n\nfeature_cols = train.columns[: -1]   #exclude the Activity column\n\n# Calculate the correlation values:\n\ncorrelated_values = train[feature_cols].corr()\n\n# Stack the data and convert to a dataframe:\n\ncorrelated_values = (correlated_values.stack().to_frame().reset_index()\n                    .rename(columns={'level_0': 'Feature_1', 'level_1': 'Feature_2', 0:'Correlations'}))\ncorrelated_values.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating an abs column for correlation column:\n\ncorrelated_values['abs_correlation'] = correlated_values.Correlations.abs()\ncorrelated_values.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we pick the most correlated features:\n\ntrain_fields = correlated_values.sort_values('Correlations', ascending = False).query('abs_correlation>0.8')\ntrain_fields.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now splitting the training and validation sets:\n\n# Getting the split indexes:\n\nsplit_data = StratifiedShuffleSplit(n_splits = 1, test_size = 0.3, random_state = 42)\ntrain_idx, val_idx = next(split_data.split(train[feature_cols], train.Activity))\n\n# Creating the dataframes:\n\nx_train = train.loc[train_idx, feature_cols]\ny_train = train.loc[train_idx, 'Activity']\n\nx_val = train.loc[val_idx, feature_cols]\ny_val = train.loc[val_idx, 'Activity']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_train.value_counts(normalize = True))\nprint(y_val.value_counts(normalize = True))\n\n# Thus, we observe that we have the same ratio of all the classes in both the training and validation or development set.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building a model:\n\nlr_l2 = LogisticRegressionCV(cv=4, penalty='l2', max_iter = 1000, n_jobs = -1)\nlr_l2 = lr_l2.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicitng using the Logistic Regression model:\n\ny_predict = list()\ny_proba = list()\n\nlabels = ['lr_l2']\nmodels = [lr_l2]\n\nfor lab, mod in zip(labels, models):\n    y_predict.append(pd.Series(mod.predict(x_val), name = lab))\n    y_proba.append(pd.Series(mod.predict_proba(x_val).max(axis=1), name = lab))\n    #.max(axis = 1) for a 1 dimensional dataframe\n\ny_predict = pd.concat(y_predict, axis = 1)\ny_proba = pd.concat(y_proba, axis = 1)\n\ny_predict.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the precision, recall and F1 score for our model:\n\nmetrics = list()\nconfusion_m = dict()\n\nfor lab in labels:\n    precision, recall, f_score, _ = error_metric(y_val, y_predict[lab], average = 'weighted')\n    \n    accuracy = accuracy_score(y_val, y_predict[lab])\n    \n    confusion_m[lab] = confusion_matrix(y_val, y_predict[lab])\n    \n    metrics.append(pd.Series({'Precision': precision, 'Recall': recall,\n                            'F_score': f_score, 'Accuracy': accuracy}, name = lab))\n    \nmetrics= pd.concat(metrics, axis =1) \nmetrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Building the second network:\n\nimport sklearn.neural_network as nn\nmlpADAM =  nn.MLPClassifier(hidden_layer_sizes=(900,), max_iter=1000 , alpha=1e-4, solver='adam' , verbose=10, tol=1e-19, random_state=1, learning_rate_init=.001)\nnnModelADAM = mlpADAM.fit(x_train , y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualising the convergence:\n\nX = np.linspace(1, nnModelADAM.n_iter_ , nnModelADAM.n_iter_)\nplt.plot(X , nnModelADAM.loss_curve_, label = 'ADAM Convergence')\nplt.title('Error Convergence ')\nplt.ylabel('Cost function')\nplt.xlabel('Iterations')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating the training and test scores:\n\nprint(\"Training set score for ADAM: %f\" % mlpADAM.score(x_train, y_train))\nprint(\"Validation set score for ADAM: %f\" % mlpADAM.score(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the test data and dividing into test data and label:\n\ntestData  = test.drop('Activity' , axis=1).values\ntestLabel = test.Activity.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label encoding the Activity:\n\nencoder = LabelEncoder()\n\n# Encoding test labels:\n\nencoder.fit(testLabel)\ntestLabelE = encoder.transform(testLabel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the test score our Adam Classifier:\n\nprint(\"Test set score for ADAM: %f\"     % mlpADAM.score(testData , testLabelE ))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}