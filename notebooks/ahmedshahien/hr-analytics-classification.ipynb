{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Content \nThis note contain three main points :\n- EDA\n- Univariate visualization\n- Bivariate visualzation\n- Multivariate visualization\n\nAnd as we will find that, our data is imbalanced so we will try to treat this by \n\n-  imblearn \n\nthen we do our clsaaification "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\n\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import our train and test data \ntrain=pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_train.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#display some rows from our train data \ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#shape of our training  data\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#some information about training data\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the number of nulls in training data\nna_counts=train.isnull().sum()\nna_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting the number of null values in each feature\nplt.figure(figsize=[10,5]);\nsns.barplot(na_counts.index.values,na_counts);\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We note that there is many missing values in company_size and comany_type ,so if i dropped all the nulls in data , i will lose more than half my training dataset ,so i will drop these two columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop company_size , company_type\ntrain.drop(['company_size','company_type'],axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"after that we will find few rows with missing values so we will drop it to start analyze our clean data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping nulls in rest of our training data \ntrain.dropna(axis=0,inplace=True)\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for duplicates\ntrain.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"# Univariate visualization"},{"metadata":{},"cell_type":"markdown","source":"Number of males and femals"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[15,7])\nplt.subplot(1,2,1)\nsorted_counts=train['gender'].value_counts()\nplt.pie(sorted_counts,labels=sorted_counts.index,startangle=90,counterclock=False,wedgeprops={'width':0.4})\nplt.axis('square');\nplt.subplot(1,2,2)\nsns.barplot(sorted_counts.index.values,sorted_counts)\n#plt.yticks(range(np.arange(0,len(train)+1000,2000))\nplt.yticks(range(0,14000,2000));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"number of  relevant_exoeriance in each unique one"},{"metadata":{"trusted":true},"cell_type":"code","source":"#number of relevant experiance in each unique one\nbase_color=sns.color_palette()[2]\nrects1=sns.countplot(data=train,x='relevent_experience',color=base_color);\n\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Count of education level "},{"metadata":{},"cell_type":"markdown","source":"One alternative univariate plot type that you might see for categorical data is the waffle plot, also known as the square pie chart. While the standard pie chart uses a circle to represent the whole, a waffle plot is plotted onto a square divided into a 10x10 grid. Each small square in the grid represents one percent of the data, and a number of squares are colored by category to indicate total proportions. Compared to a pie chart, it is much easier to make precise assessments of relative frequencies."},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating waffle plot for education level\ndef percentage_blocks(df, var):\n    \"\"\"\n    Take as input a dataframe and variable, and return a Pandas series with\n    approximate percentage values for filling out a waffle plot.\n    \"\"\"\n    # compute base quotas\n    percentages = 100 * train[var].value_counts() / df.shape[0]\n    counts = np.floor(percentages).astype(int) # integer part = minimum quota\n    decimal = (percentages - counts).sort_values(ascending = False)\n\n    # add in additional counts to reach 100\n    rem = 100 - counts.sum()\n    for cat in decimal.index[:rem]:\n        counts[cat] += 1\n\n    return counts\n\n\nwaffle_counts = percentage_blocks(train, 'education_level')\n\nprev_count = 0\n# for each category,\nfor cat in range(waffle_counts.shape[0]):\n    # get the block indices\n    blocks = np.arange(prev_count, prev_count + waffle_counts[cat])\n    # and put a block at each index's location\n    x = blocks % 10 # use mod operation to get ones digit\n    y = blocks // 10 # use floor division to get tens digit\n    plt.bar(x = x, height = 0.8, width = 0.8, bottom = y)\n    prev_count += waffle_counts[cat]\n\n    \n# aesthetic wrangling\nplt.legend(waffle_counts.index, bbox_to_anchor = (1, 0.5), loc = 6);\nplt.axis('off');\nplt.axis('square');\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the distribution of training ours for all our candidates "},{"metadata":{"trusted":true},"cell_type":"code","source":"#showing some statistic about training hours\nax = sns.boxplot(data=train['training_hours'], orient=\"v\", palette=\"Set2\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#showing the distribution of training hours\nbin_edges = np.arange(0, train['training_hours'].max()+20, 20)\nsns.distplot(train['training_hours'], bins = bin_edges, kde = False,\n            hist_kws = {'alpha' : 1})\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bivariate visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of each gender in each relevant experiance unique value"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Number of each gender in each relevant experiance unique value\nsns.set(style=\"whitegrid\")\nax = sns.countplot(x=\"gender\", hue=\"relevent_experience\", data=train)\nplt.title('Number of each gender in each relevant experiance unique value');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the number of each gender in each education level\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Number of each gender in each education level\nct_counts = train.groupby(['gender', 'education_level']).size()\n\n# Use Series.reset_index() to convert a series into a dataframe object\nct_counts = ct_counts.reset_index(name='count')\n \n# Use DataFrame.pivot() to rearrange the data, to have gender type on rows\nct_counts = ct_counts.pivot(index = 'gender', columns = 'education_level', values = 'count')\n \n \nsns.heatmap(ct_counts,annot=True,fmt='d');\nplt.title('Number of each gender in each education level');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert the \"gender\" column from a plain object type into an ordered categorical type\ngender_types = list(train['gender'].unique())\nvclasses = pd.api.types.CategoricalDtype(ordered=True, categories=gender_types)\ntrain['gender'] = train['gender'].astype(vclasses);\n\n# Plot the Seaborn's FacetGrid\ng = sns.FacetGrid(data = train, col = 'gender')\ng.map(plt.hist, \"training_hours\");\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#statics for each gender with training hours \nax=sns.boxplot(data=train,x='gender',y='training_hours')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#number of each gender in each target unique\nax = sns.countplot(y=\"gender\", hue=\"target\", data=train)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multivariate visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"#statistic for each gender in every education level & training_hours\nax=sns.boxplot(data=train,x='gender',y='training_hours',hue='education_level')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling our data to make our classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's take alook again on our data set\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['gender']=train['gender'].replace({'Male':0,'Female':1,'Other':2})\ntrain['enrolled_university']=train['enrolled_university'].replace({'no_enrollment':0,'Full time course':1,'Part time course':2})\ntrain['relevent_experience']=train['relevent_experience'].replace({'No relevent experience':0,'Has relevent experience':1})\ntrain['education_level']=train['education_level'].replace({'Graduate':1,'Masters':2,'Phd':3})\n\ntrain['major_discipline']=train['major_discipline'].replace({'STEM':1, 'Humanities':2, 'Arts':3, 'Business Degree':4,'No Major':5,'Other':6})\n\ntrain['experience']=train['experience'].replace({'>20':30,'<1':0})\n\ntrain['last_new_job']=train['last_new_job'].replace({'>4':5,'1':1,'never':0,'4':4, '3':3, '2':2})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['experience']=train['experience'].replace({'>20':30,'<1':0,'15':15,'13':13,'7':7, '5':5, '16':16, '4':4, '11':11, '18':18, '19':19, '12':12,\n       '10':10, '9':9, '2':2, '6':6, '14':14, '3':3, '8':8, '20':20, '17':17, '1':1})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['training_hours']=train['training_hours']/train['training_hours'].max()\ntrain['experience']=train['experience']/train['experience'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's checking again for coulmns type to see the correlation\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation factor between target and each column"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation matrix\nplt.figure(figsize=[10,10])\nsns.heatmap(train.corr(),annot=True,fmt='.3f')\nplt.title('Correlation factor between target and each column');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From correlation matrix we can define which features are strong related to our target and which not."},{"metadata":{"trusted":true},"cell_type":"code","source":"#our features and target\nX=train.drop(['enrollee_id','city','target'],axis=1) #these featured don't benefit us in our clsassification \ny=train['target']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Treating our imbalance data \nAs we see from our visualization , we have an imbalance data in our training data set , since the 0 in target is more higher than 1 "},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\nfrom collections import Counter\n\n\nrand=RandomOverSampler(random_state=42)\nx_ros, y_ros = rand.fit_resample(X, y)\nprint(f\"Imbalanced target class: {Counter(y)} Balanced target class:{Counter(y_ros)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting our data \nX_train, X_test, y_train, y_test = train_test_split(x_ros, y_ros, test_size=0.33, random_state=44, shuffle =True)\n\n#Splitted Data\nprint('X_train shape is ' , X_train.shape)\nprint('X_test shape is ' , X_test.shape)\nprint('y_train shape is ' , y_train.shape)\nprint('y_test shape is ' , y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  GradientBoostingClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import Libraries\nfrom sklearn.ensemble import GradientBoostingClassifier\n \n\nGBCModel = GradientBoostingClassifier(n_estimators=500,max_depth=50,random_state=33) \nGBCModel.fit(X_train, y_train)\n\n#Calculating Details\nprint('GBCModel Train Score is : ' , GBCModel.score(X_train, y_train))\nprint('GBCModel Test Score is : ' , GBCModel.score(X_test, y_test))\n \n#Calculating Prediction\ny_pred = GBCModel.predict(X_test)\ny_pred_prob = GBCModel.predict_proba(X_test)\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see there is overfitting when we use GradientBoostingClassifier, so we will try to use random forest"},{"metadata":{},"cell_type":"markdown","source":"# Random forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# applying  random forest\nRandomForestClassifierModel = RandomForestClassifier(criterion = 'gini',n_estimators=500,max_depth=10,random_state=33) #criterion can be also : entropy \nRandomForestClassifierModel.fit(X_train, y_train)\n\n#Calculating Details\nprint('RandomForestClassifierModel Train Score is : ' , RandomForestClassifierModel.score(X_train, y_train))\nprint('RandomForestClassifierModel Test Score is : ' , RandomForestClassifierModel.score(X_test, y_test))\nprint('RandomForestClassifierModel features importances are : ' , RandomForestClassifierModel.feature_importances_)\nprint('----------------------------------------------------')\n\n#Calculating Prediction\ny_pred = RandomForestClassifierModel.predict(X_test)\ny_pred_prob = RandomForestClassifierModel.predict_proba(X_test)\nprint('Predicted Value for RandomForestClassifierModel is : ' , y_pred[:10])\nprint('Prediction Probabilities Value for RandomForestClassifierModel is : ' , y_pred_prob[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculating f1 score\n\nF1Score = f1_score(y_test, y_pred, average='micro') #it can be : binary,macro,weighted,samples\nprint('F1 Score is : ', F1Score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import Libraries\nfrom sklearn.metrics import classification_report\n#----------------------------------------------------\n\n#----------------------------------------------------\n#Calculating classification Report :  \n#classification_report(y_true, y_pred, labels=None, target_names=None,sample_weight=None, digits=2, output_dict=False)\n\nClassificationReport = classification_report(y_test,y_pred)\nprint('Classification Report is : ', ClassificationReport )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating Confusion Matrix\nCM = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix is : \\n', CM)\n\n# drawing confusion matrix\nsns.heatmap(CM, center = True,annot=True,fmt='.1f')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling our test data to predict it"},{"metadata":{"trusted":true},"cell_type":"code","source":"#imprting our test data \ntest=pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#some rows from our test data\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of nulls \ntest.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We shouldn't drop any rows from our test dataset because they are our finally target ,so we will handel our missing values"},{"metadata":{},"cell_type":"markdown","source":"First we will drop company size and company type as we droped them from our training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping columns which we removed when training our model\ntest.drop(['company_size','company_type','enrollee_id','city'],axis=1,inplace=True)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['gender']=test['gender'].replace({'Male':0,'Female':1,'Other':2})\ntest['enrolled_university']=test['enrolled_university'].replace({'no_enrollment':0,'Full time course':1,'Part time course':2})\ntest['relevent_experience']=test['relevent_experience'].replace({'No relevent experience':0,'Has relevent experience':1})\ntest['education_level']=test['education_level'].replace({'Graduate':1,'High School':4,'Primary School':6,'Masters':2,'Phd':3})\n\ntest['major_discipline']=test['major_discipline'].replace({'STEM':1, 'Humanities':2, 'Arts':3, 'Business Degree':4,'No Major':5,'Other':6})\n\ntest['experience']=test['experience'].replace({'>20':30,'<1':0})\n\ntest['last_new_job']=test['last_new_job'].replace({'>4':5,'1':1,'never':0,'4':4, '3':3, '2':2})\ntest['experience']=test['experience'].replace({'>20':30,'<1':0,'15':15,'13':13,'7':7, '5':5, '16':16, '4':4, '11':11, '18':18, '19':19, '12':12,\n       '10':10, '9':9, '2':2, '6':6, '14':14, '3':3, '8':8, '20':20, '17':17, '1':1})\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will fill nulls with most frequent value in each column"},{"metadata":{"trusted":true},"cell_type":"code","source":"test['gender'].fillna(test['gender'].mode()[0],inplace=True)\ntest['enrolled_university'].fillna(test['enrolled_university'].mode()[0],inplace=True)\ntest['education_level'].fillna(test['education_level'].mode()[0],inplace=True)\ntest['major_discipline'].fillna(test['major_discipline'].mode()[0],inplace=True)\ntest['last_new_job'].fillna(test['last_new_job'].mode()[0],inplace=True)\ntest['experience'].fillna(test['experience'].mean(),inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting our test data "},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred= RandomForestClassifierModel.predict(test)\ntest_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you find this kernal useful for you please upvote , to encourage me to do more"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}