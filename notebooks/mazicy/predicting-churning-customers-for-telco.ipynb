{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing some additional libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# models that I want to try with this dataset to predict the churning customers\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n# some utility libs\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's read in the data and explore."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndisplay(df.head())\ndisplay(df.columns)\ndisplay(df.describe())\ndisplay(df.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like we need to clean the boolean columns\nTotalCharges column needs to be converted to float as well\nLet's take a look at what unique values the columns have before cleaning up"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df.columns:\n    # excluding the numerical columns\n    if col not in ['customerID','MonthlyCharges', 'TotalCharges', 'tenure']:\n        print(col,':', df[col].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'No ineternet service' & 'No phone service' will probably need to be removed since they are indicated in their own columns\n# As a rough check, let's see how churn and non-churn customers are distributed by each categorical columns\n\nfor col in df.columns:\n    if col not in ['customerID','MonthlyCharges', 'TotalCharges', 'tenure']:\n        print(col,':', df[col].unique(),'\\n')\n        sns.countplot(col, data=df, palette='coolwarm', hue='Churn')\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like the gender is not playing much role but we will keep it there for now\n\nThe data seems imbalanced in terms of churn/no-churn customers which we will have to fix by undersampling"},{"metadata":{},"cell_type":"markdown","source":"Below code will process the data-cleanse that we mentioned above."},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a copy to clean the data and store\ndf_edited = df.copy()\n\n# cleaning up the categorical into booleans\ndf_edited = df.replace({'No internet service':0, 'No phone service':0, 'No':0, 'Yes':1})\ndf_edited.replace({'InternetService': {0: 'No'}}, inplace=True)\n\n# Replacing Empty Total Charges with Monthly Charges under the assumption that these are new accounts & converting it to float\ndf_edited['TotalCharges'].loc[df_edited['TotalCharges']==' '] = df_edited[df_edited['TotalCharges']==' ']['MonthlyCharges']\ndf_edited['TotalCharges'] = df_edited.TotalCharges.astype('float64')\n\n# dropping the customer id since it is very unlikeyly that it's useful\ndf_edited.drop('customerID', axis=1, inplace=True)\n\n# getting dummy values for the categorical values\ngender = pd.get_dummies(df_edited.gender, drop_first=True, prefix='gender')\nPaymentMethod = pd.get_dummies(df_edited.PaymentMethod, drop_first=False, prefix='PaymentMethod')\nContract = pd.get_dummies(df_edited.Contract, drop_first=False, prefix='Contract')\nInternetService = pd.get_dummies(df_edited.InternetService, drop_first=False, prefix='InternetService')\n\n# drop the original columns and add dummy columns\ndf_edited.drop(['gender', 'PaymentMethod','Contract','InternetService'], axis=1, inplace=True)\ndf_edited = pd.concat([df_edited, gender, PaymentMethod, Contract, InternetService], axis=1)\n\n# let's check if the columns are clean enough to be used in modeling\nfor col in df_edited.columns:\n    if col not in ['customerID','MonthlyCharges', 'TotalCharges', 'tenure']:\n        print(col,':', df_edited[col].unique(),'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's look at the numerical columns to see if there's anything interesting"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['MonthlyCharges', 'TotalCharges', 'tenure']:\n    sns.distplot(df_edited[col])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see if the expectation of 'high tenure = high TotalCharges' is correct"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.lmplot(x='tenure', y='TotalCharges', data=df_edited, col = 'Churn',\n          scatter_kws = dict(s=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Slopes are as expected but there's a difference between the churn groups.\n\nIt looks like the churning group has higher slope, let's check the slope."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_linear = LinearRegression()\nno_churn = df_edited[df_edited['Churn']==0]\nchurn = df_edited[df_edited['Churn']==1]\nmodel_linear.fit(X=no_churn['tenure'].values.reshape(-1,1),y=no_churn['TotalCharges'].values.reshape(-1,1))\nprint('no churn:',model_linear.coef_[0][0])\nmodel_linear.fit(X=churn['tenure'].values.reshape(-1,1),y=churn['TotalCharges'].values.reshape(-1,1))\nprint('churn:',model_linear.coef_[0][0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This highly likely means that churning customers have higher MonthlyCharges\n\nLet's check the distribution of MonthlyCharges."},{"metadata":{"trusted":true},"cell_type":"code","source":"Churn = df_edited[df_edited.Churn==1]\nNo_Churn = df_edited[df_edited.Churn==0]\n\nsns.distplot(Churn[['MonthlyCharges']], hist=False, rug=True, label='Churn', axlabel='MonthlyCharges')\nsns.distplot(No_Churn[['MonthlyCharges']], hist=False, rug=True, label='No Churn')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can deduce from this that majority of churning customers have higher MonthlyCharges."},{"metadata":{},"cell_type":"markdown","source":"Now, before preparing the train and test data, we need to fix the imbalance.\nThe plan is to match the numbers of the Churn and No-Churn groups."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_edited.Churn.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will use randomized 1869 customers of no_churn group.\ndf_edited = df_edited.sample(frac=1)\ndf_edited_balanced = pd.concat([df_edited[df_edited['Churn']==1],df_edited[df_edited['Churn']==0][:1869]],axis=0)\nsns.countplot(df_edited_balanced['Churn'], palette='coolwarm')\nprint(df_edited_balanced.Churn.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the models, we will try LogisticRegression, RandomForestClassifier, and XGBoostClassifier.\nThe default settings will be used for these and once we select a model, we can dive into improvements."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare the train and test data sets\nX=df_edited_balanced.drop('Churn', axis=1)\ny=df_edited_balanced.Churn\ntest_size = 0.3\nrandom_state = 70\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\nmodels = [LogisticRegression(),DecisionTreeClassifier(), RandomForestClassifier(), XGBClassifier()]\nfor model in models:\n    print ('\\n','-'*100,'\\n',model)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(confusion_matrix(y_test,y_pred))\n    print(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBoost and LogisticRegression are the best contenders.\nJust out of curiosity, let's take a look at the results without the undersampling."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare the train and test data sets\nX=df_edited.drop('Churn', axis=1)\ny=df_edited.Churn\ntest_size = 0.3\nrandom_state = 70\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\nmodels = [LogisticRegression(),DecisionTreeClassifier(), RandomForestClassifier(), XGBClassifier()]\nfor model in models:\n    print ('\\n','-'*100,'\\n',model)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(confusion_matrix(y_test,y_pred))\n    print(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can confirm that the overall accuracy went up but the sensitivity for actual churning customers have dropped significantly. I believe the purpose of these models are to predict which customers are churning and above result shows that fixing the imbalance with undersampling is a necessity for training these models."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}