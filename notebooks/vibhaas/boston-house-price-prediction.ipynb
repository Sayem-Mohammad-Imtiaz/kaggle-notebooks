{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>1. Business Understanding"},{"metadata":{},"cell_type":"markdown","source":"This initial phase focuses on understanding the project objectives and requirements from a business perspective, and then converting this knowledge into a data mining problem definition, and a preliminary plan designed to achieve the objectives.\n\nIn this situation let's pretend we are a real estate agency in Boston MA and we are interested in purchasing some houses. We would like to know which houses are under value to help us narrow down the list and put in an accurate bid on a house.\n\n<b>Objective:</b> Identify what makes a property valuable? What is a fair price for a house?"},{"metadata":{},"cell_type":"markdown","source":"Load Library"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import libraries for data handling\nimport os\nimport pandas as pd\nimport numpy as np\n\n#import for visualization\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#import for Linear regression\nfrom sklearn.linear_model import LinearRegression\n\n#import for Visualization\nimport plotly.express as px\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load Data into Pandas Dataframe"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/input/boston-house-prices/housing.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndata = pd.read_csv('/kaggle/input/boston-house-prices/housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see first 5 rows. Data is loaded Successfully!"},{"metadata":{},"cell_type":"markdown","source":"### Dataset : Boston\n### Goal      : Predict medv column in Test Dataset!\n<ol>\n<li>\t<b>\tcrim\t:\t</b>\tper capita crime rate by town.\n<li>\t<b>\tzn\t:\t</b>\tproportion of residential land zoned for lots over 25,000 sq.ft.\n<li>\t<b>\tindus\t:\t</b>\tproportion of non-retail business acres per town.\n<li>\t<b>\tchas\t:\t</b>\tCharles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n<li>\t<b>\tnox\t:\t</b>\tnitrogen oxides concentration (parts per 10 million).\n<li>\t<b>\trm\t:\t</b>\taverage number of rooms per dwelling.\n<li>\t<b>\tage\t:\t</b>\tproportion of owner-occupied units built prior to 1940.\n<li>\t<b>\tdis\t:\t</b>\tweighted mean of distances to five Boston employment centres.\n<li>\t<b>\trad\t:\t</b>\tindex of accessibility to radial highways.\n<li>\t<b>\ttax\t:\t</b>\tfull-value property-tax rate per &#36;10,000.\n<li>\t<b>\tptratio\t:\t</b>\tpupil-teacher ratio by town.\n<li>\t<b>\tblack\t:\t</b>\t1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n<li>\t<b>\tlstat\t:\t</b>\tlower status of the population (percent).\n<li>\t<b>\tmedv\t:\t</b>\tmedian value of owner-occupied homes in &#36;1000s.\n</ol>"},{"metadata":{},"cell_type":"markdown","source":"## 2. Data Understanding (EDA)"},{"metadata":{},"cell_type":"markdown","source":"### Print a concise summary of a DataFrame.\nThis method prints information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Information on the Dataframe\nprint(\"\\n\\n\", data.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Understand the data**\n\nBefore cleaning data, there are a couple of things we would like to know, for example, the dimension of a dataset, the data type of each variable, the first few rows and the last few rows of the data and name of each variable, etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Dimension of Dataset is \" + str(data.shape))  # check out the dimension of the dataset\nprint(\"\\nlook at the data types for each column \")\nprint(data.dtypes)  # look at the data types for each column\n\n#print(data.head())  # read the first five rows\n#print(data.tail())  # read the last five rows\n\n#print(data.columns.values)  # return an array of column names\nprint(\"\\nlist of column names \")\nprint(data.columns.values.tolist())  # return a list of column names","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check Missing Values**\n\nNext, we would like to check if there are any missing values. To check this, we can use the function dataframe.isnull() in pandas. It will return True for missing components and False for non-missing cells. However, when the dimension of a dataset is large, it could be difficult to figure out the existence of missing values. In general, we may just want to know if there are any missing values first. The function dataframe.isnull().values.any() returns True when there is at least one missing value occurring in the data. The function dataframe.isnull().sum().sum() returns the number of missing values in the data set. "},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print(data.isnull())  # checking missing values\ndata.notnull()  # checking non-missing values\ndata.isnull().values.any()  # only want to know if there are any missing values\ndata.notnull().sum()  # knowling number of non-missing values for each variable\n\ndata.isnull().sum().sum()  # knowing how many missing values in the data\ndata[\"MEDV\"].isnull().values.any()  # only want to know if there are any missing values in MEDV\ndata[\"MEDV\"].isnull().sum()  # return the number of missing values in MEDV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()  # knowing how many missing values in the data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Reference\n* Missing Values : [link](https://miamioh.instructure.com/courses/38817/pages/data-cleaning#:~:text=The%20function%20dataframe.-,isnull().,values%20in%20the%20data%20set.&text=A%20simple%20way%20to%20deal,missing%20values%20in%20the%20dataset.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math \n\n# Struge formula\nprint(1 + 3.322*(math.log10(len(data))))\n\n# round off the bin size\nbin_size = round((1 + 3.322*(math.log10(len(data)))))\nprint(bin_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of bins are calculated as per Sturge’s rule K = 1 + 3. 322 logN\nsns.distplot(data.MEDV, bins = bin_size, hist = True, rug = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<ol>\n<b>Linear Regression Assumptions </b>\n<li>Linear relationship between target and features\n<li>No outliers\n<li>No high-leverage points\n<li>Homoscedasticity of error terms\n<li>Uncorrelated error terms\n<li>Independent features"},{"metadata":{},"cell_type":"markdown","source":"#1 Linear Relationship Between Target & Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the heatmap of correlation between features\ncorr = data.corr()\n\nplt.figure(figsize=(20,10))\nsns.heatmap(corr, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':10})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Highly correlated**\n<li>more access to highway more tax\n<li>Medium Correlated\n<li>more crim less price\n<li>more polution less price\n<li>more tax less price\n<li>more pt ratio less price\n<li>more lower population less price\n<li>more distance less industry\n<li>more industry more pollution\n"},{"metadata":{},"cell_type":"markdown","source":"### 3. Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import for Models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nfrom math import sqrt\nimport statsmodels.api as sm\nfrom sklearn import metrics\n\n#importing plotly and cufflinks in offline mode\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n#import warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Split Data into Test & Train"},{"metadata":{},"cell_type":"markdown","source":"##### Benefit to splitting a dataset into some ratio of training and testing subsets for a learning algorithm\n\n<ul>\n\n<li> <b>Motivation:</b> we need a way to choose between machine learning models and our goal is to estimate likely performance of a model on out-of-sample data.\n<li> <b>Initial idea:</b> we can train and test on the same data. However this will cause overfitting. As the number of features in a dataset increases the problem will increase\n<li><b>Alternative idea:</b> we can use train/test split. We can split the dataset into two pieces so that the model can be trained and tested on different data.\nThen, testing accuracy is a better estimate than training accuracy of out-of-sample performance.\n</ul>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Machine Learning \npredictor = data.drop(['MEDV'], 1)\ntarget = data['MEDV']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Model"},{"metadata":{},"cell_type":"markdown","source":"## lm1 : Raw data only"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split data to 80% training data and 20% of test to check the accuracy of our model\nX_train, X_test, y_train, y_test = train_test_split(predictor, target, test_size=0.20, random_state=0)\nprint(\"Training and testing split by 80/20 was successful\")\n\n#Model object\nlm1 = LinearRegression(fit_intercept=True,normalize=False)\n#Model Training\nlm1.fit(X_train,y_train)\n# Predict\ny_pred = lm1.predict(X_test)\n\n# Evaluate\n# 1. root-mean-square error (RMSE) for the Model\n# 2. R-Sqauared for the Model\n\n#Calculate root-mean-square error (RMSE):\nprint (\"root-mean-square error (RMSE) for the model is : {}\".format(round(sqrt(mean_squared_error(y_test,y_pred)),2)))\n\n#Calculate R-squared for the Model:\nprint (\"R-Squared for the above model : {}\".format(round(r2_score(y_test,y_pred)*100,2)),\"%\")\n\n#OLS model summary\nmodel1 = sm.OLS(y_train,X_train).fit()\nmodel1.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Create a Random Forest Regressor\nreg = RandomForestRegressor()\n\n# Train the model using the training sets \nreg.fit(X_train, y_train)\n\n# Model prediction on train data\ny_pred = reg.predict(X_train)\n\n# Model Evaluation\nprint('R^2:',metrics.r2_score(y_train, y_pred))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBoost Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import XGBoost Regressor\nfrom xgboost import XGBRegressor\n\n#Create a XGBoost Regressor\nreg = XGBRegressor()\n\n# Train the model using the training sets \nreg.fit(X_train, y_train)\n\n# Model prediction on train data\ny_pred = reg.predict(X_train)\n\n# Model Evaluation\nprint('R^2:',metrics.r2_score(y_train, y_pred))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))\n\n# Visualizing the differences between actual prices and predicted values\nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices\")\nplt.title(\"Prices vs Predicted prices\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import XGBoost Regressor\nfrom xgboost import XGBRegressor\n\n#Create a XGBoost Regressor\nreg = XGBRegressor()\n\n# Train the model using the training sets \nreg.fit(X_train, y_train)\n\n# Model prediction on train data\ny_pred = reg.predict(X_test)\n\n# Model Evaluation\nprint('R^2:',metrics.r2_score(y_test, y_pred))\n#print('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_pred))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))\n#print('MAE:',metrics.mean_absolute_error(y_test, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking residuals\nplt.scatter(y_pred,y_train-y_pred)\nplt.title(\"Predicted vs residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SVM Regressor**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import SVM Regressor\nfrom sklearn import svm\n\n# Create a SVM Regressor\nreg = svm.SVR()\n\n# Train the model using the training sets \nreg.fit(X_train, y_train)\n\n# Model prediction on train data\ny_pred = reg.predict(X_train)\n\n# Model Evaluation\nprint('R^2:',metrics.r2_score(y_train, y_pred))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n# Import Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor\n# Import XGBoost Regressor\nfrom xgboost import XGBRegressor\n# Import SVM Regressor\nfrom sklearn import svm\n#from sklearn import cross_validation\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nimport time\n\n# Create Model Array\nmodels = []\nmodels.append((\"LR\",LinearRegression(fit_intercept=True,normalize=False)))\nmodels.append((\"RFM\",RandomForestRegressor()))\nmodels.append((\"XGB\",XGBRegressor()))\nmodels.append((\"SVM\",svm.SVR()))\n\nresult = []\n\n#measure the MSE \nfor name,model in models:\n    \n    start_time = time.time()\n    \n    cv_result = cross_val_score(model,X_train,y_train, cv = 10, scoring = \"neg_mean_squared_error\")\n    print(name, cv_result)\n    print(\"-\"*3,name, \" Mean MSE of cross-validation: \", format(round(cv_result.mean(),2)))\n    execution_time = (time.time() - start_time)\n    \n    result.append((name,format(round(cv_result.mean(),2)), execution_time))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(result)\n    \ndf = pd.DataFrame(result, columns =['Algo', 'MSE', 'Execution_time']) \ndf \n\n# sort df by Count column\npd_df = df.sort_values(['MSE']).reset_index(drop=True)\npd_df\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd_df[['Algo', 'MSE']].groupby(['Algo']).sum().iplot(kind='bar',xTitle='Models', yTitle='MSE/Timing', title = 'Model Comparision')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd_df[['Algo', 'Execution_time']].groupby(['Algo']).sum().iplot(kind='bar',xTitle='Models', yTitle='MSE/Timing', title = 'Model Comparision')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Machine Learning \npredictor = data.drop(['MEDV'], 1)\ntarget = data['MEDV']\n\n# Split data to 80% training data and 20% of test to check the accuracy of our model\nX_train, X_test, y_train, y_test = train_test_split(predictor, target, test_size=0.20, random_state=0)\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn import svm\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nimport time\n\n# Create Model Array\nmodels = []\nmodels.append((\"LR\",LinearRegression(fit_intercept=True,normalize=False)))\nmodels.append((\"RFM\",RandomForestRegressor()))\nmodels.append((\"XGB\",XGBRegressor()))\nmodels.append((\"SVM\",svm.SVR()))\n\nresult = []\n\n#measure the MSE \nfor name,model in models:\n    \n    start_time = time.time()\n    \n    cv_result = cross_val_score(model,X_train,y_train, cv = 10, scoring = \"neg_mean_squared_error\")\n    #print(name, cv_result)\n    print(name, \" Mean MSE of cross-validation: \", format(round(cv_result.mean(),2)))\n    execution_time = (time.time() - start_time)\n    \n    result.append((name,format(round(cv_result.mean(),2)), execution_time))\n\n\npd.DataFrame(result)\n    \ndf = pd.DataFrame(result, columns =['Algo', 'MSE', 'Execution_time']) \ndf \n\n# sort df by Count column\npd_df = df.sort_values(['MSE'])\nprint(pd_df)\n\n\ndf[['Algo', 'MSE']].groupby(['Algo']).sum().iplot(kind='bar',xTitle='Models', yTitle='MSE/Timing', title = 'Model Comparision')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Machine Learning \npredictor = data.drop(['MEDV'], 1)\ntarget = data['MEDV']\n\n# Split data to 80% training data and 20% of test to check the accuracy of our model\nX_train, X_test, y_train, y_test = train_test_split(predictor, target, test_size=0.20, random_state=0)\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn import svm\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nimport time\n\n# Create Model Array\nmodels = []\nmodels.append((\"LR\",LinearRegression(fit_intercept=True,normalize=False)))\nmodels.append((\"RFM\",RandomForestRegressor()))\nmodels.append((\"XGB\",XGBRegressor()))\nmodels.append((\"SVM\",svm.SVR()))\n\nresult = []\n\n#measure the MSE \nfor name,model in models:\n    \n    start_time = time.time()\n    \n    cv_result = cross_val_score(model,X_train,y_train, cv = 10, scoring = \"neg_mean_squared_error\")\n    #print(name, cv_result)\n    print(\"-\"*3,name, \" Mean MSE of cross-validation: \", format(round(cv_result.mean(),2)))\n    execution_time = (time.time() - start_time)\n    \n    result.append((name,format(round(cv_result.mean(),2)), execution_time))\n\n\npd.DataFrame(result)\n    \ndf = pd.DataFrame(result, columns =['Algo', 'MSE', 'Execution_time']) \ndf \n\n# sort df by Count column\npd_df = df.sort_values(['MSE'])\nprint(pd_df)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean square Error differes with change in train/test split ratio\n#40\n* Algo\tMSE\tExecution_time\n* LR\t-23.72\t0.086060\n* RFM\t-12.48\t3.461339\n* XGB\t-9.32\t0.572908\n* SVM\t-69.05\t0.205350\n\n#30\n* \tAlgo\tMSE\tExecution_time\n* 0\tLR\t-22.83\t0.084721\n* 1\tRFM\t-14.75\t3.695503\n* 2\tXGB\t-10.48\t0.812710\n* 3\tSVM\t-67.22\t0.213389\n\n#20\n* \tAlgo\tMSE\tExecution_time\n* 0\tLR\t-21.4\t0.080288\n* 1\tRFM\t-10.57\t3.938169\n* 2\tXGB\t-7.83\t0.668272\n* 3\tSVM\t-64.68\t0.239741"},{"metadata":{},"cell_type":"markdown","source":"XGB is least in all the cases"},{"metadata":{},"cell_type":"markdown","source":"##Tensorflow"},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Processing\nX = data.iloc[:, 0:13].values\ny = data.iloc[:, 13].values\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To train the model, let's import the TensorFlow 2.0 classes. \nfrom tensorflow.keras.layers import Input, Dense, Activation,Dropout\nfrom tensorflow.keras.models import Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_layer = Input(shape=(X.shape[1],))\ndense_layer_1 = Dense(100, activation='relu')(input_layer)\ndense_layer_2 = Dense(50, activation='relu')(dense_layer_1)\ndense_layer_3 = Dense(25, activation='relu')(dense_layer_2)\noutput = Dense(1)(dense_layer_3)\n\nmodel = Model(inputs=input_layer, outputs=output)\nmodel.compile(loss=\"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train, batch_size=2, epochs=10, verbose=1, validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\npred_train = model.predict(X_train)\nprint(np.sqrt(mean_squared_error(y_train,pred_train)))\n\npred = model.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test,pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reference:\n    https://stackabuse.com/tensorflow-2-0-solving-classification-and-regression-problems/\n        "},{"metadata":{},"cell_type":"markdown","source":"https://towardsdatascience.com/explained-deep-learning-in-tensorflow-chapter-1-9ab389fe90a1"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}