{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Replacing zero values with null values since these features should not have zero values"},{"metadata":{"trusted":false},"cell_type":"code","source":"data[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = data[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0, np.nan)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Replacing null values with means of each feature"},{"metadata":{"trusted":false},"cell_type":"code","source":"data['Glucose'].fillna(data['Glucose'].mean(), inplace = True)\ndata['BloodPressure'].fillna(data['BloodPressure'].mean(), inplace = True)\ndata['SkinThickness'].fillna(data['SkinThickness'].mean(), inplace = True)\ndata['Insulin'].fillna(data['Insulin'].mean(), inplace = True)\ndata['BMI'].fillna(data['BMI'].mean(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(data.corr())\nplt.figure(figsize=(20,15))\nsns.heatmap(data.corr(), annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"scatter = pd.plotting.scatter_matrix(data, figsize = (20, 20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"scatter_2 = sns.pairplot(data, hue = 'Outcome')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Assign X and y values"},{"metadata":{"trusted":false},"cell_type":"code","source":"X = data.iloc[:, :-1].values\ny = data.iloc[:, -1].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting dataset into training set and test set"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature scaling"},{"metadata":{"trusted":false},"cell_type":"code","source":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN Classifier"},{"metadata":{},"cell_type":"markdown","source":"## Grid Search to find best hyper parameters for KNN Classifier"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nparameters = {'n_neighbors': list(range(0,51)), 'metric':['minkowski', 'euclidean', 'manhattan', 'chebyshev']}\ngrid_search = GridSearchCV(estimator = KNeighborsClassifier(), param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\ngrid_search.fit(X_train, y_train)\nprint('KNN Classifier Grid Search Best Accuracy = {:.2f}%'.format(grid_search.best_score_ *100))\nprint('KNN Classifier Best Parameters:', grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training KNN Classifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"KNN = KNeighborsClassifier(n_neighbors = 31, metric = 'minkowski')\nKNN.fit(X_train, y_train)\nKNN_y_pred = KNN.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Computing TP, TN, FP, FN and Accuracy of KNN Classifier Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"print('k-Nearest Neighbors Confusion Matrix:', confusion_matrix(y_test, KNN_y_pred))\nprint('k-Nearest Neighbors Model Accuracy = {:.2f}%'.format(accuracy_score(y_test, KNN_y_pred)*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## k-Fold Cross-Validation"},{"metadata":{"trusted":false},"cell_type":"code","source":"KNN_scores = cross_val_score(estimator = KNN, X = X_train, y = y_train, cv = 10)\nprint('k-Nearest Neighbors Accuracies:', KNN_scores)\nprint('k-Nearest Neighbors Mean Accuracy = {:.2f}%'.format(KNN_scores.mean()*100))\nKNN_score = round(KNN_scores.mean(), 4)\nprint('k-Nearest Neighbors Standard Deviation = {:.2f}%'.format(KNN_scores.std()*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"## Grid Search to find best hyper parameters for Logistic Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nC = list(range(1,11))\nparameters = {'C': C, 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\ngrid_search = GridSearchCV(estimator = LogisticRegression(random_state = 0), param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\ngrid_search.fit(X_train, y_train)\nprint('Logistic Regression Grid Search Best Accuracy = {:.2f}%'.format(grid_search.best_score_ *100))\nprint('Logistic Regression Best Parameters:', grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"C = np.arange(0, 1.1, 0.001) #narrowing possible values of C\nparameters = {'C': C, 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\ngrid_search = GridSearchCV(estimator = LogisticRegression(random_state = 0), param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\ngrid_search.fit(X_train, y_train)\nprint('Logistic Regression Grid Search Best Accuracy = {:.5f}%'.format(grid_search.best_score_ *100))\nprint('Logistic Regression Best Parameters:', grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"LR = LogisticRegression(C = 0.005, solver = 'liblinear', random_state = 0)\nLR.fit(X_train, y_train)\nLR_y_pred = LR.predict(X_test)\nprint('Logistic Regression Confusion Matrix:', confusion_matrix(y_test, LR_y_pred))\nprint('Logistic Regression Model Accuracy = {:.2f}%'.format(accuracy_score(y_test, LR_y_pred)*100))\nLR_scores = cross_val_score(estimator = LR, X = X_train, y = y_train, cv = 10)\nprint('Logistic Regression Scores:', LR_scores)\nprint('Logistic Regression Mean Accuracy = {:.2f}%'.format(LR_scores.mean()*100))\nLR_score = round(LR_scores.mean(), 4)\nprint('Logistic Regression Standard Deviation = {:.2f}%'.format(LR_scores.std()*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machine"},{"metadata":{},"cell_type":"markdown","source":"## Grid Search to find best hyper parameters for SVM Classifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.svm import SVC\nC = list(range(1,11))\nparameters = {'C': C}\ngrid_search = GridSearchCV(estimator = SVC(kernel = 'linear', random_state = 0), param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\ngrid_search.fit(X_train, y_train)\nprint('SVM Grid Search Best Accuracy = {:.2f}%'.format(grid_search.best_score_ *100))\nprint('SVM Best Parameters:', grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"C = np.arange(0, 2.1, 0.001) #narrowing possible values of C\nparameters = {'C': C}\ngrid_search = GridSearchCV(estimator = SVC(kernel = 'linear', random_state = 0), param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\ngrid_search.fit(X_train, y_train)\nprint('SVM Grid Search Best Accuracy = {:.2f}%'.format(grid_search.best_score_ *100))\nprint('SVM Best Parameters:', grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training SVM Classifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"svc = SVC(C = 0.007, kernel = 'linear', random_state = 0)\nsvc.fit(X_train, y_train)\nsvc_y_pred = svc.predict(X_test)\nprint('SVM Confusion Matrix:', confusion_matrix(y_test, svc_y_pred))\nprint('Support Vector Machine Model Accuracy = {:.2f}%'.format(accuracy_score(y_test, svc_y_pred)*100))\nsvc_scores = cross_val_score(estimator = svc, X = X_train, y = y_train, cv = 10)\nprint('Support Vector Machine Scores:', svc_scores)\nprint('Support Vector Machine Mean Accuracy = {:.2f}%'.format(svc_scores.mean()*100))\nsvc_score = round(svc_scores.mean(), 4)\nprint('Support Vector Machine Standard Deviation = {:.2f}%'.format(svc_scores.std()*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Kernel Support Vector Machine"},{"metadata":{},"cell_type":"markdown","source":"## Grid Search to find best hyper parameters for Kernel SVM Classifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"gamma = ['scale', 'auto'] + list(range(1,51))\nC = list(range(1,11))\nparameters = {'C': C, 'kernel': ['rbf', 'sigmoid'], 'gamma': gamma}\ngrid_search = GridSearchCV(estimator = SVC(random_state = 0), param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\ngrid_search.fit(X_train, y_train)\nprint('Kernel SVM Grid Search Best Accuracy = {:.2f}%'.format(grid_search.best_score_ *100))\nprint('Kernel SVM Best Parameters:', grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"gamma = ['scale', 'auto'] + list(np.arange(0, 2.1, 0.1))\nC = np.arange(0, 1.1, 0.1)\nparameters = {'C': C, 'kernel': ['rbf', 'sigmoid'], 'gamma': gamma}\ngrid_search = GridSearchCV(estimator = SVC(random_state = 0), param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\ngrid_search.fit(X_train, y_train)\nprint('Kernel SVM Grid Search Best Accuracy = {:.2f}%'.format(grid_search.best_score_ *100))\nprint('Kernel SVM Best Parameters:', grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"gamma = ['scale', 'auto'] + list(np.arange(0, 0.5, 0.01))\nC = np.arange(0, 0.5, 0.01)\nparameters = {'C': C, 'kernel': ['rbf', 'sigmoid'], 'gamma': gamma}\ngrid_search = GridSearchCV(estimator = SVC(random_state = 0), param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\ngrid_search.fit(X_train, y_train)\nprint('Kernel SVM Grid Search Best Accuracy = {:.2f}%'.format(grid_search.best_score_ *100))\nprint('Kernel SVM Best Parameters:', grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training Kernel SVM Classifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"ksvc = SVC(C = 0.09, gamma = 0.07, kernel = 'sigmoid', random_state = 0)\nksvc.fit(X_train, y_train)\nksvc_y_pred = ksvc.predict(X_test)\nprint('Kernel SVM Confusion Matrix:', confusion_matrix(y_test, ksvc_y_pred))\nprint('Kernel Support Vector Machine Model Accuracy = {:.2f}%'.format(accuracy_score(y_test, ksvc_y_pred)*100))\nksvc_scores = cross_val_score(estimator = ksvc, X = X_train, y = y_train, cv = 10)\nprint('Kernel Support Vector Machine Scores:', ksvc_scores)\nprint('Kernel Support Vector Machine Mean Accuracy = {:.2f}%'.format(ksvc_scores.mean()*100))\nksvc_score = round(ksvc_scores.mean(), 4)\nprint('Kernel Support Vector Machine Standard Deviation = {:.2f}%'.format(ksvc_scores.std()*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes"},{"metadata":{},"cell_type":"markdown","source":"## Training Naive Bayes Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\ngnb_y_pred = gnb.predict(X_test)\nprint('Naive Bayes Confusion Matrix:', confusion_matrix(y_test, gnb_y_pred))\nprint('Naive Bayes Model Accuracy = {:.2f}%'.format(accuracy_score(y_test, gnb_y_pred)*100))\ngnb_scores = cross_val_score(estimator = gnb, X = X_train, y = y_train, cv = 10)\nprint('Naive Bayes Scores:', gnb_scores)\nprint('Naive Bayes Mean Accuracy = {:.2f}%'.format(gnb_scores.mean()*100))\ngnb_score = round(gnb_scores.mean(), 4)\nprint('Naive Bayes Standard Deviation = {:.2f}%'.format(gnb_scores.std()*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree Classifier"},{"metadata":{},"cell_type":"markdown","source":"## Grid Search to find best hyper parameters"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nmax_feat = list(range(0,9)) + ['auto', 'sqrt', 'log2']\nparameters = {'criterion':['gini', 'entropy'], 'max_features': max_feat}\ngrid_search = GridSearchCV(estimator = DecisionTreeClassifier(random_state = 0), param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\ngrid_search.fit(X_train, y_train)\nprint('Decision Tree Grid Search Best Accuracy =  {:.2f}%'.format(grid_search.best_score_ *100))\nprint('Decision Tree Best Parameters:', grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training Decision Tree Classifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"dt = DecisionTreeClassifier(criterion = 'entropy', max_features = 5, random_state = 0)\ndt.fit(X_train, y_train)\ndt_y_pred = dt.predict(X_test)\nprint('Decision Tree Confusion Matrix:', confusion_matrix(y_test, dt_y_pred))\nprint('Decision Tree Model Accuracy = {:.2f}%'.format(accuracy_score(y_test, dt_y_pred)*100))\ndt_scores = cross_val_score(estimator = dt, X = X_train, y = y_train, cv = 10)\nprint('Decision Tree Scores:', dt_scores)\nprint('Decision Tree Mean Accuracy = {:.2f}%'.format(dt_scores.mean()*100))\ndt_score = round(dt_scores.mean(), 4)\nprint('Decision Tree Standard Deviation = {:.2f}%'.format(dt_scores.std()*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier"},{"metadata":{},"cell_type":"markdown","source":"## GridSearch to find best hyper parameters"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmax_feat = list(range(1, 9)) + ['auto', 'sqrt', 'log2']\nparameters = {'n_estimators':list(range(1,31)), 'criterion':['gini', 'entropy'], 'max_features': max_feat}\ngrid_search = GridSearchCV(estimator = RandomForestClassifier(random_state = 0), param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\ngrid_search.fit(X_train, y_train)\nprint('Random Forest Grid Search Best Accuracy = {:.2f}%'.format(grid_search.best_score_ *100))\nprint('Random Forest Best Parameters:', grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"max_feat = list(range(1, 9)) + ['auto', 'sqrt', 'log2']\nparameters = {'n_estimators':list(range(1, 51)), 'criterion':['gini', 'entropy'], 'max_features': max_feat}\ngrid_search = GridSearchCV(estimator = RandomForestClassifier(random_state = 0), param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\ngrid_search.fit(X_train, y_train)\nprint('Random Forest Grid Search Best Accuracy = {:.2f}%'.format(grid_search.best_score_ *100))\nprint('Random Forest Best Parameters:', grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training Random Forest Classifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators = 30, criterion = 'gini', max_features = 3, random_state = 0)\nrf.fit(X_train, y_train)\nrf_y_pred = rf.predict(X_test)\nprint('Random Forest Confusion Matrix:', confusion_matrix(y_test, rf_y_pred))\nprint('Random Forest Model Accuracy = {:.2f}%'.format(accuracy_score(y_test, rf_y_pred)*100))\nrf_scores = cross_val_score(estimator = rf, X = X_train, y = y_train, cv = 10)\nprint('Random Forest Scores:', rf_scores)\nprint('Random Forest Mean Accuracy = {:.2f}%'.format(rf_scores.mean()*100))\nrf_score = round(rf_scores.mean(), 4)\nprint('Random Forest Standard Deviation = {:.2f}%'.format(rf_scores.std()*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost Classifier"},{"metadata":{},"cell_type":"markdown","source":"## Grid Search to find best hyper parameters"},{"metadata":{"trusted":false},"cell_type":"code","source":"from xgboost import XGBClassifier\nparameters = {'booster': ['gbtree', 'dart'], 'gamma': list(range(0, 11)), 'max_depth':list(range(1,7))}\ngrid_search = GridSearchCV(estimator = XGBClassifier(random_state = 0), param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\ngrid_search.fit(X_train, y_train)\nprint('XGBoost Grid Search Best Accuracy = {:.2f}%'.format(grid_search.best_score_ *100))\nprint('XGBoost Best Parameters:', grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train XGBoost Classifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"xgb = XGBClassifier(booster = 'gbtree', gamma = 1, max_depth = 3, random_state = 0)\nxgb.fit(X_train, y_train)\nxgb_y_pred = xgb.predict(X_test)\nprint('XGBoost Confusion Matrix:', confusion_matrix(y_test, xgb_y_pred))\nprint('XGBoost Model Accuracy = {:.2f}%'.format(accuracy_score(y_test, xgb_y_pred)*100))\nxgb_scores = cross_val_score(estimator = xgb, X = X_train, y = y_train, cv = 10)\nprint('XGBoost Scores:', xgb_scores)\nprint('XGBoost Mean Accuracy = {:.2f}%'.format(xgb_scores.mean()*100))\nxgb_score = round(xgb_scores.mean(), 4)\nprint('XGBoost Standard Deviation = {:.2f}%'.format(xgb_scores.std()*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CatBoost Classifier"},{"metadata":{},"cell_type":"markdown","source":"## Grid Search to find best hyper parameters"},{"metadata":{"trusted":false},"cell_type":"code","source":"from catboost import CatBoostClassifier\nparameters = {'n_estimators': list(range(1,51)), 'max_depth':list(range(1,7))}\ngrid_search = GridSearchCV(estimator = CatBoostClassifier(random_state = 0), param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\ngrid_search.fit(X_train, y_train)\nprint('CatBoost Grid Search Best Accuracy = {:.2f}%'.format(grid_search.best_score_ *100))\nprint('CatBoost Best Parameters:', grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training CatBoost Classifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"cb = CatBoostClassifier(n_estimators = 18, max_depth = 6, random_state = 0)\ncb.fit(X_train, y_train)\ncb_y_pred = xgb.predict(X_test)\nprint('CatBoost Confusion Matrix:', confusion_matrix(y_test, cb_y_pred))\nprint('CatBoost Model Accuracy = {:.2f}%'.format(accuracy_score(y_test, cb_y_pred)*100))\ncb_scores = cross_val_score(estimator = cb, X = X_train, y = y_train, cv = 10)\nprint('CatBoost Scores:', cb_scores)\nprint('CatBoost Mean Accuracy = {:.2f}%'.format(cb_scores.mean()*100))\ncb_score = round(cb_scores.mean(), 4)\nprint('CatBoost Standard Deviation = {:.2f}%'.format(cb_scores.std()*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding the most accurate model"},{"metadata":{"trusted":false},"cell_type":"code","source":"accuracies = []\naccuracies.append(KNN_score)\nprint(accuracies)\naccuracies.extend((LR_score, svc_score, ksvc_score, gnb_score, dt_score, rf_score, xgb_score, cb_score))\nprint(accuracies)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"models = ['KNN', 'LR', 'SVC', 'KSVC', 'NB', 'DT', 'RF', 'XGB', 'CB']\nscore_dict = dict(zip(models, accuracies))\nprint(score_dict)\nbest_model = max(score_dict, key = score_dict.get)\nprint('Best Model is', best_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The model which achieved the highest mean accuracy of 77.52% is the CatBoost Classifier."},{"metadata":{},"cell_type":"markdown","source":"# Final Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"cb = CatBoostClassifier(n_estimators = 18, max_depth = 6, random_state = 0)\ncb.fit(X_train, y_train)\ncb_y_pred = xgb.predict(X_test)\nprint('CatBoost Confusion Matrix:', confusion_matrix(y_test, cb_y_pred))\nprint('CatBoost Model Accuracy = {:.2f}%'.format(accuracy_score(y_test, cb_y_pred)*100))\ncb_scores = cross_val_score(estimator = cb, X = X_train, y = y_train, cv = 10)\nprint('CatBoost Scores:', cb_scores)\nprint('CatBoost Mean Accuracy = {:.2f}%'.format(cb_scores.mean()*100))\ncb_score = round(cb_scores.mean(), 4)\nprint('CatBoost Standard Deviation = {:.2f}%'.format(cb_scores.std()*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}