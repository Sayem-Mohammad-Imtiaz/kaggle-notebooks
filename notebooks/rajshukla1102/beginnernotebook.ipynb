{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.graph_objects as go\nimport matplotlib.style as style\nimport plotly.express as px\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.decomposition import PCA\nfrom sklearn import linear_model, decomposition\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(r'/kaggle/input/heart-disease-uci/heart.csv')\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include=\"all\").T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. ###### We can see there is no missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('target').size()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Taget variable seems quite balanced."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.plot(subplots=True, sharex=True ,figsize=(20,50))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()['target'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"style.use(\"ggplot\")\nsns.set_style('whitegrid')\nmask = np.zeros_like(df.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.subplots(figsize = (16,9))\n\nsns.heatmap(df.corr(),annot=True,mask=mask)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('target', axis=1).corrwith(df.target).plot(kind='bar', grid=True, figsize=(12, 8), color=['salmon'],\n                                                   title=\"Correlation with target\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation\n**As we can see chol and fbs are very least correlated with target.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sex'].value_counts().plot(kind='barh',color=['r','y'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"style.use(\"ggplot\")\nplt.figure(figsize=(16,9))\npd.crosstab(df.target, df.sex).plot(kind='bar')\nplt.xlabel(\"0-No disease 1-Suffering from Disease\")\nplt.title(\"Male and Female\")\nplt.legend([\"F\",\"M\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Obserbvation\nWe can observe that female population has less suffering than male."},{"metadata":{"trusted":true},"cell_type":"code","source":"dis_var=[]\ncon_var=[]\nfor i in df.columns:\n    if len(df[i].unique())>10:\n        con_var.append(i)\n    else:\n        dis_var.append(i)\nprint(\"Categeorical variable: \",dis_var)\nprint(\"Continuous variable: \",con_var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,16))\nfor i,j in enumerate(dis_var):\n    plt.subplot(3,3,i+1)\n    df[df[\"target\"] == 0][j].hist(bins=35, color='blue', label='Have Heart Disease = NO', alpha=0.8)\n    df[df[\"target\"] == 1][j].hist(bins=35, color='red', label='Have Heart Disease = YES', alpha=0.8)\n    plt.xlabel(j,fontsize=16)\n    plt.legend(fontsize=13)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observation\n#### cp(chest-pain)\n(Value 0: typical angina\nValue 1: atypical angina\nValue 2: non-anginal pain\nValue 3: asymptomatic)\nWe can see paitient with value 1 2 3 are more likely to have heart disease than value 0(typical angina)\n\n#### restecg: (resting electrocardiographic results)\nPeople with value 1 is more likely to have heart disease\n\n#### exang(whether the patient had angina during exercise)\n\nVal 0: no\nVal 1: yes\npeople with a value of 0 (No ==> angina induced by exercise) have more heart disease than people with a value of 1\n\n#### slope(slope of the ST segment during the most demanding part of the exercise)\nPeople with a slope value of 2 (Downslopins: signs of an unhealthy heart) are more likely to have heart disease than people\nwith a slope value of 0(Upsloping: best heart rate with exercise) \nor 1 (Flatsloping: minimal change (typical healthy heart)).\n\n#### thal(Results of the blood flow observed via the radioactive dye.\nPeople with a thal value of 2 (defect corrected: once was a defect but ok now) are more likely to have heart disease.\n\n#### ca(Number of main blood vessels coloured by the radioactive dye)\npeople with value 0 are more likely to suffer from heart disease"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,16))\nfor i,j in enumerate(con_var):\n    plt.subplot(3,3,i+1)\n    df[df[\"target\"] == 0][j].hist(bins=35, color='blue', label='Have Heart Disease = NO', alpha=0.8)\n    df[df[\"target\"] == 1][j].hist(bins=35, color='red', label='Have Heart Disease = YES', alpha=0.8)\n    plt.xlabel(j,fontsize=16)\n    plt.legend(fontsize=13)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observation\n#### age \n    patient age ranging between 40 to 60 are prone to heart disease\n\n#### trestbps(resting blood pressure in millimeters of mercury (mm Hg) when the patient was admitted to the hospital)\n    danger is more between 130mm hg to 140mm Hg\n\n#### chol(cholestrol)\n    above 200 is at higher risk\n\n#### thalach(max heart rate during the stress test)\n    patient with max 140 are at higher risk\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\n\nplt.scatter(df.age[df.target==1],\n            df.thalach[df.target==1],\n            c=\"red\")\n\nplt.scatter(df.age[df.target==0],\n            df.thalach[df.target==0],\n            c=\"lightblue\")\nplt.title(\"Age and Max Heart Rate\")\nplt.xlabel(\"age\")\nplt.ylabel(\"max Heart Rate\")\nplt.legend([\"Suffering:yes\", \"Suffering:no\"]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observation\nwe can see many young patient has higher max heart rate yet they are safe as compared to old people"},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.get_dummies(df, columns=['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'], drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\nscale_var = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\ndataset[scale_var] = sc.fit_transform(dataset[scale_var])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dataset.drop('target', axis=1)\ny = dataset.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Creation"},{"metadata":{},"cell_type":"markdown","source":"*Comparing all algorithm*"},{"metadata":{"trusted":true},"cell_type":"code","source":"allAlgo = [('lr', LogisticRegression()),('knn', KNeighborsClassifier()),('dclf', DecisionTreeClassifier()),\n          ('svm', SVC()),('nb', GaussianNB())]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = []\nnames = []\nfor name, algo in allAlgo:\n    kfold = KFold(n_splits=10, random_state=None)\n    cv_results = cross_val_score(algo, X_train, y_train, cv=kfold, scoring='accuracy')\n    res.append(cv_results)\n    names.append(name)\n    print(name, cv_results.mean(), cv_results.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nplt.boxplot(res)\ntick=range(1,len(names)+1)\nplt.xticks(ticks=tick,labels=names)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### We can identify that Logistic and SVM is giving quite good accuracy."},{"metadata":{},"cell_type":"markdown","source":"##### For our easiness we make a function that calculate everything with train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        pred = clf.predict(X_train)\n        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n        print(\"Train Result:\\n**********************************************\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n        \n    elif train==False:\n        pred = clf.predict(X_test)\n        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n        print(\"Test Result:\\n************************************************\")        \n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(solver='saga',penalty='elasticnet',l1_ratio=0.6,max_iter=1000)\nlr.fit(X_train, y_train)\n\nprint_score(lr, X_train, y_train, X_test, y_test, train=True)\nprint_score(lr, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying PCA on Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=10) \nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)\nexpained_variance = pca.explained_variance_ratio_\nclassifier = LogisticRegression(random_state = 1)\nclassifier.fit(X_train_pca, y_train)\nprint_score(classifier, X_train_pca, y_train, X_test_pca, y_test, train=True)\nprint_score(classifier, X_train_pca, y_train, X_test_pca, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can observe by applying pca we are not getting a good accuracy.**"},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"sv = SVC(kernel='rbf',C=1)\nsv.fit(X_train, y_train)\nprint_score(sv, X_train, y_train, X_test, y_test, train=True)\nprint_score(sv, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNeighboursClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\nprint_score(knn, X_train, y_train, X_test, y_test, train=True)\nprint_score(knn, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GaussianNB"},{"metadata":{"trusted":true},"cell_type":"code","source":"gn = GaussianNB()\ngn.fit(X_train, y_train)\nprint_score(gn, X_train, y_train, X_test, y_test, train=True)\nprint_score(gn, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DecisionTreeClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"dc = DecisionTreeClassifier()\ndc.fit(X_train, y_train)\nprint_score(dc, X_train, y_train, X_test, y_test, train=True)\nprint_score(dc, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cross-validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\ncross_validation = cross_val_score(estimator = lr, X = X_train, y = y_train, cv = 10)\nprint(\"Cross validation of LR model = \",cross_validation)\nprint(\"Cross validation of LR model (in mean) = \",cross_validation.mean())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\ncross_validation = cross_val_score(estimator = sv, X = X_train, y = y_train, cv = 10)\nprint(\"Cross validation of SVC model = \",cross_validation)\nprint(\"Cross validation of SVC model (in mean) = \",cross_validation.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion"},{"metadata":{},"cell_type":"markdown","source":"**KFold Crossvalidation is performed for Logistic regression and SVM\nWe can conclude that logistic regression is performing well in KFold CrossValidation**\n\n*Beginner Code Please correct if i am wrong somewhere*"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}