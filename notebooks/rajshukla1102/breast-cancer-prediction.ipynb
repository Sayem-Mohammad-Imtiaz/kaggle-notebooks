{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\npd.set_option('display.max_columns',None)\npd.set_option('display.max_rows',None)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/breast-cancer-wisconsin-data/data.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"shape \",df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('Unnamed: 32',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include=\"all\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No null values here!!\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.rename({'diagnosis':'target'}, axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 357 benign, 212 malignant"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['target']=[1 if i == \"M\" else 0 for i in df['target']]\n##replacing malignant with 1 and benign with 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.plot(subplots=True, sharex=True ,figsize=(20,50))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observation \nData is looking quite balanced and we can move on to visualization"},{"metadata":{},"cell_type":"markdown","source":"## Data Visulaization"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(x=\"target\", data=df)\nplt.title(\"Diagnosis M=1 , B=0\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('id',axis=1,inplace=True)\ncorr=df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.style as style\nstyle.use(\"ggplot\")\nsns.set_style('whitegrid')\nplt.subplots(figsize = (16,9))\n\nsns.heatmap(corr,annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()['target'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize = (9,13))\nsns.heatmap(df.corr()[['target']].sort_values(by='target', ascending=False), annot=True, cmap='BrBG')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"concave points_worst, perimeter_worst, concave points_mean, radius_worst, perimeter_mean\nThey resemble high correlation with target"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Box_plots(df):\n    plt.figure(figsize=(10, 4))\n    plt.title(\"Box Plot\")\n    sns.boxplot(df)\n    plt.show()\nfor i in df.columns:\n    Box_plots(df[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see there are some outliers in our data we will have to remove this for better result"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop([\"target\"], axis = 1)\ny = df.target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = X.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Outliers "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import LocalOutlierFactor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LocalOutlierFactor()\ny_pred = clf.fit_predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_score = clf.negative_outlier_factor_\noutlier_score = pd.DataFrame()\noutlier_score[\"score\"] = X_score\nthreshold = -2.0\nfiltre = outlier_score[\"score\"] < threshold\noutlier_index = outlier_score[filtre].index.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.drop(outlier_index)\ny = y.drop(outlier_index).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, y,test_size = 0.3,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"allAlgo = [('lr', LogisticRegression()),('knn', KNeighborsClassifier()),('dclf', DecisionTreeClassifier()),\n          ('svm', SVC()),('nb', GaussianNB()),('rf',RandomForestClassifier()),]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = []\nalgoName = []\nfor name, model in allAlgo:\n    kfold = KFold(n_splits=10)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=\"accuracy\")\n    res.append(cv_results)\n    algoName.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see logistic regression and random forest classifier shows good accuracy without scaling data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(res)\nax.set_xticklabels(algoName)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipelines = []\n\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('lr',LogisticRegression())])))\n\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('knn',KNeighborsClassifier())])))\n\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('dclf',DecisionTreeClassifier())])))\n\npipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('nb',GaussianNB())])))\n\npipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('svm', SVC())])))\n\npipelines.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('rf', RandomForestClassifier())])))\n\nres = []\nalgoName = []\nfor name, model in pipelines:\n    kfold = KFold(n_splits=10)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=\"accuracy\")\n    res.append(cv_results)\n    algoName.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nfig.suptitle('Scaled Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(res)\nax.set_xticklabels(algoName)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the difference after scaling data!! LR, SVM, Rf show quite good accuracy"},{"metadata":{},"cell_type":"markdown","source":"## Tuning SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nc_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\nkernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\nparam_grid = dict(C=c_values, kernel=kernel_values)\nmodel = SVC()\nkfold = KFold(n_splits=11)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=\"accuracy\", cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nmodel = SVC(kernel='linear',C=0.3)\nmodel.fit(rescaledX, Y_train)\n# estimate accuracy on validation dataset\nrescaledValidationX = scaler.transform(X_test)\npredictions = model.predict(rescaledValidationX)\nprint(accuracy_score(Y_test, predictions))\nprint(confusion_matrix(Y_test, predictions))\nprint(classification_report(Y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GaussianNB"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nnb_classifier = GaussianNB()\nnb_classifier.fit(rescaledX, Y_train)\n# estimate accuracy on validation dataset\nrescaledValidationX = scaler.transform(X_test)\npredictions = nb_classifier.predict(rescaledValidationX)\nprint(accuracy_score(Y_test, predictions))\nprint(confusion_matrix(Y_test, predictions))\nprint(classification_report(Y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RandomForestClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nrf=RandomForestClassifier(random_state=101)\nrf.fit(rescaledX, Y_train)\n# estimate accuracy on validation dataset\nrescaledValidationX = scaler.transform(X_test)\npredictions = rf.predict(rescaledValidationX)\nprint(accuracy_score(Y_test, predictions))\nprint(confusion_matrix(Y_test, predictions))\nprint(classification_report(Y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nlr=LogisticRegression(solver='saga',penalty='l2',l1_ratio=0.6,random_state=41)\nlr.fit(rescaledX, Y_train)\n# estimate accuracy on validation dataset\nrescaledValidationX = scaler.transform(X_test)\npredictions = lr.predict(rescaledValidationX)\nprint(accuracy_score(Y_test, predictions))\nprint(confusion_matrix(Y_test, predictions))\nprint(classification_report(Y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nxgb_classifier = XGBClassifier()\nxgb_classifier.fit(rescaledX, Y_train)\n# estimate accuracy on validation dataset\nrescaledValidationX = scaler.transform(X_test)\npredictions = xgb_classifier.predict(rescaledValidationX)\nprint(accuracy_score(Y_test, predictions))\nprint(confusion_matrix(Y_test, predictions))\nprint(classification_report(Y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cross-Validation"},{"metadata":{},"cell_type":"markdown","source":"#### RandomforestClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\ncross_validation = cross_val_score(estimator = rf, X = X_train, y =Y_train, cv =10)\nprint(\"Cross validation of SVC model = \",cross_validation)\nprint(\"Cross validation of SVC model (in mean) = \",cross_validation.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_validation = cross_val_score(estimator = lr, X = X_train, y =Y_train, cv =10)\nprint(\"Cross validation of SVC model = \",cross_validation)\nprint(\"Cross validation of SVC model (in mean) = \",cross_validation.mean())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}