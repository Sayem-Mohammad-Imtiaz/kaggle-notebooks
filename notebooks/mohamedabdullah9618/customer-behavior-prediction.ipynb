{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Customer Behaviour Prediction\n\nin this notebook we will prepare and clean the data then apply a predictive model to predict if a new client will pay the bill statement of the credict card or not.\n\n#### Content\n1. <a href=\"#1\">Exploring the Data</a><br>\n2. <a href=\"#2\">Data Cleaning</a><br>\n3. <a href=\"#3\">Data Preprocessing</a><br>\n4. <a href=\"#4\">K-Fold</a>\n5. <a href=\"#5\">Model Training</a>\n6. <a href=\"#6\">Conclusion</a>","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"1\">Exploring the Data</a>","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv')\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a name=\"2\">Data Cleaning</a>","metadata":{}},{"cell_type":"markdown","source":"The main aim of **Data Cleaning** is to identify and remove errors & duplicate data, in order to create a reliable dataset. This improves the quality of the training data for analytics and enables accurate decision-making.\n\nin our dataset, we can apply some modifications on the dataframe in order to remove nans or outliers and improve the quality of the dataset.","metadata":{}},{"cell_type":"markdown","source":"as the columns data has different types so we will deal with each type of them separately.","metadata":{}},{"cell_type":"code","source":"to_be_dropped = ['ID']\ncategorical_cols = ['SEX', 'EDUCATION', 'MARRIAGE']\nnumerical_cols = ['LIMIT_BAL', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2',\n                  'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', 'PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'AGE']\ntarget_col = ['will_pay']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"for `PAY_n values`, They all present an undocumented label -2. If 1,2,3, etc are the months of delay, 0 should be labeled 'pay duly' and every negative value should be seen as a 0","metadata":{}},{"cell_type":"code","source":"for col in ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']:\n    data[col] = np.where(data[col]<=0, 0, data[col]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove NaNs","metadata":{}},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop nans\ndata = data.dropna()\n\n# drop unneeded cols\ndata = data.drop(to_be_dropped, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### update some values","metadata":{}},{"cell_type":"code","source":"# change some columns name to be more understandable\ndata = data.rename(columns={'PAY_0': 'PAY_1', 'default.payment.next.month': 'will_pay'})\n\n# replace unknown values with nans\ndata['EDUCATION'] = data['EDUCATION'].replace('unknown', np.NaN) \ndata['MARRIAGE'] = data['MARRIAGE'].replace('unknown', np.NaN)\n\n# replace 'others' values with different name\ndata['EDUCATION'] = data['EDUCATION'].replace('others', 'other education') \ndata['MARRIAGE'] = data['MARRIAGE'].replace('others', 'other status')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data['SEX'].value_counts(), '\\n')\nprint(data['EDUCATION'].value_counts(), '\\n')\nprint(data['MARRIAGE'].value_counts(), '\\n')\nprint(data['will_pay'].value_counts(), '\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that the data is **unbalanced** as we see from the target values. we will handle this case later!","metadata":{}},{"cell_type":"markdown","source":"Now, we need to apply **one hot encodings** on the categorical columns. and we will do this process in two steps: \n1. convert integer values to its original categorical values\n2. convert categorical values to one hot encoding","metadata":{}},{"cell_type":"markdown","source":"### convert int values to categorical variables in the dataframe","metadata":{}},{"cell_type":"code","source":"# convert int to categorical\ndef int2cat(df, col, dic):\n    \"\"\"\n    Parameters:\n        df : dataframe object\n        col: column name in the dataframe\n        dic: int to categorical dictionary related to this column\n    Return:\n        df : return the dataframe with this column updated \n    \"\"\"\n    df[col] = df[col].apply(lambda x: dic[x])\n    return df\n\n\n\nsex_dic = {1:'male', 2:'female'}\neducation_dic = {1:'graduate school', 2:'university', 3:'high school', 4:'others', 5:'unknown', 6:'unknown', 0:'unknown'}\nmarriage_dic = {1:'married', 2:'single', 3:'others', 0: 'unknown'}\n\ncategorical_dics = [sex_dic, education_dic, marriage_dic]\n\nfor col, dic in list(zip(categorical_cols, categorical_dics)):\n    data = int2cat(data, col, dic)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we have done the first step successfully in order to apply **one hot encoding**. the second step will be in the next section!","metadata":{}},{"cell_type":"markdown","source":"# <a name=\"3\">Data Preprocessing</a>","metadata":{}},{"cell_type":"markdown","source":"### get the features and labels from the dataframe","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx = data.drop('will_pay', axis=1)\ny = data['will_pay']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### create a pipeline for preprocessing and model training","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\n\n# Preprocess the categorical features\ncategorical_processor = Pipeline([\n    ('cat_encoder', OneHotEncoder(handle_unknown='ignore'))\n                                ])\n\nnumerical_processor = Pipeline([\n    ('standard_scaler', StandardScaler())\n                               ])\n\ndata_preprocessor = ColumnTransformer([\n    ('categorical_pre', categorical_processor, (categorical_cols)),\n    ('numerical_pre', numerical_processor, (numerical_cols))\n                                    ]) \n\npipeline = Pipeline([\n    ('data_preprocessing', data_preprocessor),\n    #('dt', RandomForestClassifier(max_depth=2, random_state=0))\n                    ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_transformed = pipeline.fit_transform(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Apply SMOTE technique for oversampling ","metadata":{}},{"cell_type":"markdown","source":"The unbalanced problem can add a bias to the model towards either of the two classes. one of the techniques to overcome this problem is **SMOTE**","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=101)\nsmote_x, smote_y = smote.fit_resample(x_transformed, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(smote_x.shape, smote_y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a name=\"4\">K-Fold</a>","metadata":{}},{"cell_type":"markdown","source":"by applying **K-Fold**, it ensures that every observation from the original dataset has the chance of appearing in training and test set. This is one among the best approach if we have a limited input data.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier, AdaBoostClassifier\nfrom xgboost import XGBClassifier\n\nresults = {\n    'model': [],\n    'score': []\n}\n\nmodels = [\n    ('random_forest', RandomForestClassifier(n_estimators=10)),\n    ('svm', SVC(gamma='auto')),\n    ('decision_tree', DecisionTreeClassifier(max_depth = 3, class_weight = \"balanced\")),\n    ('xgboost', XGBClassifier())\n]\n\ncv = KFold(n_splits=5)\n\nfor (model_name, model) in models:\n    score = np.mean(cross_val_score(model, smote_x, smote_y, scoring='accuracy', cv=cv, n_jobs=1))\n    results['model'].append(model_name)\n    results['score'].append(score)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we see that **xgboost** model gives the highest score among all the models. so we will investigate in this model in the next section.","metadata":{}},{"cell_type":"markdown","source":"# <a name=\"5\"> Model Training</a>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n\nx_train, x_test, y_train, y_test = train_test_split(smote_x, smote_y, test_size=0.2)\n\nxgb = XGBClassifier(n_estimators=100, max_depth=5, booster='gbtree')\n\nxgb.fit(x_train, y_train)\n\ny_pred = xgb.predict(x_train)\nprint('\\n----------------------- TRAIN RESULTS ------------------------')\nprint(classification_report(y_train, y_pred))\n\ny_pred = xgb.predict(x_test)\nprint('\\n----------------------- TEST RESULTS -------------------------')\nprint(classification_report(y_test, y_pred))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a name=\"6\">Conclusion</a>","metadata":{}},{"cell_type":"markdown","source":"by applying the **XGboost** model, we reached to **87% accuracy** on the test set and also high **F1-score** for the two classes. I think this may sound good result especially that we deal with high data imbalance. <br>\n\nFianlly, in the future work we can improve this result by applying the **GridSearch** technique in order to choose the best parameters that will give the model highest score!","metadata":{}},{"cell_type":"markdown","source":"Thank you for spending time reading this notebook. if you find it useful, please give it upvote.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}