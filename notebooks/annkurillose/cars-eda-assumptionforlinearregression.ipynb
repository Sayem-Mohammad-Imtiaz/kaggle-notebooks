{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Load libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsns.set_style('darkgrid')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/vehicle-dataset-from-cardekho/car data.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Car_Name** : This column should be filled with the name of the car.\n\n**Year** : This column should be filled with the year in which the car was bought.\n\n**Selling_Price** : This column should be filled with the price the owner wants to sell the car at.\n\n**Present_Price** : This is the current ex-showroom price of the car.\n\n**Kms_Driven** : This is the distance completed by the car in km.\n\n**Fuel_Type**: Fuel type of the car i.e Diesel,Petrol,CNG\n\n**Seller_Type**: Defines whether the seller is a dealer or an individual.\n\n**Transmission** : Defines whether the car is manual or automatic.\n\n**Owner** : Defines the number of owners the car has previously had.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Checking null values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are no non null values present in the columns","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Visualizing and Analyzing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"a=['Car_Name','Fuel_Type','Seller_Type','Transmission']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing how many unique values are there in categorical columns\nfor i in a:\n    print(i ,len(df[i].unique()))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since there are 98 unique different items in the column Car_Names we will drop that column.\ndf=df.drop('Car_Name',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Percentage of each category in Fuel_Type,Seller_Type,Transmission\nfig, axes=plt.subplots(1,3,figsize=(15,10))\ndf['Fuel_Type'].value_counts().plot(kind='pie',autopct='%.3f%%',ax=axes[0],textprops={'fontsize': 13})\ndf['Seller_Type'].value_counts().plot(kind='pie',autopct='%.3f%%',ax=axes[1],textprops={'fontsize': 13})\ndf['Transmission'].value_counts().plot(kind='pie',autopct='%.3f%%',ax=axes[2],textprops={'fontsize': 13})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Inference:\n    * The petrol cars grabs 79.4% market share which is much more than diesel and CNG cars.\n    * The dealer selling cars (64.8% )are more than the individual selling cars.\n    * Manual Transmission cars(86.7%) are more than automatic transmission cars.\n    ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of previous owners\nsns.countplot(df['Owner'],palette='husl')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Cars with no owner is much more than the cars having previous owner(s).","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.heatmap(df.corr(),annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Selling price have a very high positive correlation with present price which implies that higher the present price higher the selling price.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Distribution of Selling_Price, Year, Present_Price, Kms_Driven\nfig, axes=plt.subplots(2,2,figsize=(15,5))\nsns.distplot(df['Selling_Price'],ax=axes[0,0])\nsns.distplot(df['Year'],ax=axes[0,1])\nsns.distplot(df['Present_Price'],ax=axes[1,0])\nsns.distplot(df['Kms_Driven'],ax=axes[1,1])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Inference:\n  * Majority of the selling price in low range except for few models.\n  * Most of the cars are of the year 2010 - 2018.\n  * The present price follows a similar trend as the selling price.\n  * Most of the cars Kms driven lie below 100000.\n        \n    ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting 'Year', 'Present_Price', 'Kms_Driven', 'Owner' against the traget variable selling price to find their relation\nsns.pairplot(df,x_vars=['Year', 'Present_Price', 'Kms_Driven', 'Owner'],y_vars=['Selling_Price'],height=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Inference:\n\n* The newer models have higher selling price as compared to old models. \n\n* The present price and the selling price have a linear relationship, as the present price increases the selling price also increases.\n\n* Selling price is a bit higher when the Kms driven are low.\n\n* The cars with no owner have much higher selling price as compared with used cars.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# To find further inference \nsns.lmplot(x='Present_Price',y='Selling_Price',data=df, fit_reg=False,col='Transmission',hue='Fuel_Type',height=4,aspect=1.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Inference: \n   *  There is no CNG cars with automatic transmission.\n   *  Cars with automatic transmission and with fuel type diesel have higher selling price.\n   \n   ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Outliers","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Count of outliers in each numerical column\na=['Year','Selling_Price','Present_Price','Kms_Driven','Owner']\n\nfor i in a:\n    q1 = df[i].quantile(0.25)\n    q3 = df[i].quantile(0.75)\n    iqr = q3-q1\n\n    UL = q3 + (1.5 * iqr)\n    LL = q1 - (1.5 * iqr)\n    print(i,df[(df[i]>UL) | (df[i]<LL)].count()[i])\n    #print(cars[(cars[i]>UL) | (cars[i]<LL)][i])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Outliers of Selling price\nq1 = df['Selling_Price'].quantile(0.25)\nq3 = df['Selling_Price'].quantile(0.75)\niqr = q3-q1\nUL = q3 + (1.5 * iqr)\nLL = q1 - (1.5 * iqr)\ndf[(df['Selling_Price']>UL) | (df['Selling_Price']<LL)].sort_index()\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Outliers of Present Price\nq1 = df['Present_Price'].quantile(0.25)\nq3 = df['Present_Price'].quantile(0.75)\niqr = q3-q1\nUL = q3 + (1.5 * iqr)\nLL = q1 - (1.5 * iqr)\ndf[(df['Present_Price']>UL) | (df['Present_Price']<LL)].sort_index()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Note that Selling_Price and Present_Price have almost the same data points as the outliers which means that these outliers represents important information that are rare. Hence we cannot ignore or remove these outliers.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visual representation of outliers\nfig, axes=plt.subplots(2,2,figsize=(15,8))\nsns.boxplot('Selling_Price',data=df,ax=axes[0,0])\nsns.boxplot('Year',data=df,ax=axes[0,1])\nsns.boxplot('Present_Price',data=df,ax=axes[1,0])\nsns.boxplot('Kms_Driven',data=df,ax=axes[1,1])\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Changing the categorical value to numerical","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.get_dummies(df,drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model building ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* The dependent variable will be 'Selling_price' rest all the variables will be cosidered as independent variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df.drop('Selling_Price',axis=1)\ny=df['Selling_Price']\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Linear Regression\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardizing the data by taking mean to 0 and standard deviation to 1.\nfrom sklearn.preprocessing import StandardScaler\nss=StandardScaler()\nX_std = ss.fit_transform(X)\nX_std=pd.DataFrame(X_std, columns=X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nXc=sm.add_constant(X_std)\nols=sm.OLS(y,Xc)\nmodel=ols.fit()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Inference: \n\n* R2= 0.883 implier that 88.3 % of the variation in selling price is explained by the independent variables.\n\n* Probability of F-stats = 0 implies that atleast one of the features plays a  significant role in predicting selling price.\n\n* From the Pvalues we can also say that except Fuel type and owner all the other variables plays a significant role in predicting the model.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=model.predict(Xc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting predicted values vs actual values\nplt.scatter(y_pred,y)\nplt.plot(y_pred,y_pred,'r')\nplt.xlabel('y predicted')\nplt.ylabel('y actual')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking assumptions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 1. Multicollinearity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif=pd.DataFrame()\nvif['VIF']=[variance_inflation_factor(X_std.values,i) for i in range(X_std.shape[1])]\nvif['feature']=X_std.columns\nvif.sort_values('VIF',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is high vif values for few features. To remove multicollinearity we can remove features one by one till all the vif values lie below 10.\n","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"X_vif=X_std.copy()\nwhile vif['VIF'].max()>10:\n    a=vif[vif['VIF']==vif['VIF'].max()].iloc[0,1]\n    X_vif=X_vif.drop(a,axis=1)\n    \n    vif=pd.DataFrame()\n    vif['VIF']=[variance_inflation_factor(X_vif.values,i) for i in range(X_vif.shape[1])]\n    vif['feature']=X_vif.columns\nvif\n\n# We can see that one columns have been removed which brings all the vif values below 10.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.Linearity","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.regplot(y,model.predict(),line_kws={'color':'red'})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It shows a linear trend but to confirm we further check using rainbow test.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.diagnostic import linear_rainbow\nlinear_rainbow(res=model,frac=0.5) \n# Since pvalue > 0.05 we conclude that the data is  linear.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3. Normality","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import norm\nsns.distplot(model.resid,fit=norm)\nnorm.fit(model.resid)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The light blue is the actual distribution of residuals and the dark line, if the residual is perfectly normal.  \n* The graph shows that the residuals does not follow a normal curve. \n* We can do transformations to make it normal.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.stats as stats\nstats.shapiro(model.resid)\n# p value = 0 < 0.05 hence we reject null hypothesis (ie.It is normally distributed) Which means that it is not normally distributed.","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#### QQ plot ( quantile quantile plot)\n\nimport scipy.stats as stats\nstats.probplot(model.resid,plot=plt)\nplt.show()\n\n# here only the extreme values are going from normality.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4. Autocorelation\n  \nFrom the model Durbin-watson = 1.795 its very close to 2. Hence we can say that there is very low/ negligible autocorelation. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.tsa.api as smt\nacf = smt.graphics.plot_acf(model.resid, lags=40 , alpha=0.05)\nacf.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ACF: The correlation between the observation at the current time spot and the observations at previous time spots\n    \nThe blue shade is the threshold, autocorrelation is large for lag 0 and for others it is less.\n    ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 5. Homoscadacity : test of variance","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.residplot(model.predict(Xc),model.resid,lowess =True, line_kws ={'color':'red'} )\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.show()\n# from the graph it is hetro ( since there is high varience in the output )\n# we can further check using ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip\nname=['F-stat','p=value']\ntest=sms.het_goldfeldquandt(y=model.resid,x=Xc)\nlzip(name, test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"H0: Residuals got constant variance\n\nH1: Residuals varience is not constant\n\nSince p-value > 0.05.  We fail to reject the null hypothesis and conclude that variance of residuals is constant. Hence hetroscadastic is not present.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Basic Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_std,y,test_size=0.3,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr=LinearRegression()\nlr.fit(X_train,y_train)\ny_pred=lr.predict(X_test)\n\nfrom sklearn.metrics import r2_score, mean_squared_error\nprint('R^2 on the test data', r2_score(y_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Regularisation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Lasso ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso, Ridge","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso=Lasso(alpha=0.01)\nlasso.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"pd.DataFrame(lasso.coef_,index=X_train.columns,columns=['coefs'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = lasso.predict(X_test)\nfrom sklearn.metrics import r2_score\nr2_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ridge","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge=Ridge(alpha=0.01)\nridge.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(ridge.coef_,index=X_train.columns,columns=['coefs'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = ridge.predict(X_test)\nr2_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### There is no much difference in the score. This is because lasso and ridge needs many columns to make a difference in the prediction. Hence we try using all the columns\ncars = pd.read_csv(\"../input/vehicle-dataset-from-cardekho/car data.csv\")\ncars=pd.get_dummies(cars,drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=cars.drop('Selling_Price',axis=1)\ny=cars['Selling_Price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler()\nX_std = ss.fit_transform(X)\nX_std=pd.DataFrame(X_std, columns=X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_std,y,test_size=0.3,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso, Ridge\nfrom sklearn.metrics import r2_score\n\n\nlasso=Lasso(alpha=0.01)\nlasso.fit(X_train,y_train)\n\npd.DataFrame(lasso.coef_,index=X_train.columns,columns=['coefs'])\n\ny_pred = lasso.predict(X_test)\nr2_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge=Ridge(alpha=0.01)\nridge.fit(X_train,y_train)\n\npd.DataFrame(ridge.coef_,index=X_train.columns,columns=['coefs'])\n\ny_pred = ridge.predict(X_test)\nr2_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In lasso the scores have improved to 0.88 as compared to basic model with score 0.85","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We will use some other model and compare the results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = {'Lasso': Lasso(alpha=0.01),\n          'Ridge':Ridge(alpha=0.01),\n          'RandomForest' : RandomForestRegressor(),\n          'DecisionTree' : DecisionTreeRegressor(),\n          'GradientBoosting' : GradientBoostingRegressor(),\n          'AdaBoost' : AdaBoostRegressor()}\n\n\ndef Different_model_scores(models):\n    model_scores = {}    \n    for name, model in models.items():        \n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        model_scores[name]=r2_score(y_test,y_pred)\n    return model_scores\nmodel_scores = Different_model_scores(models)\nmodel_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It can be observed that the best model is when gradient boosting is used with a R2 score of 0.90","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}