{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\nfake_data = pd.read_csv('../input/fake-and-real-news-dataset/Fake.csv')\ntrue_data = pd.read_csv('../input/fake-and-real-news-dataset/True.csv')\n\n\n\n# combind datafram  \ndf = pd.concat([fake_data, true_data])\n\n\ntexts = df['text'].values.tolist()\n# texts[:5] # preview first 10 records","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-19T15:54:46.000259Z","iopub.execute_input":"2021-09-19T15:54:46.001152Z","iopub.status.idle":"2021-09-19T15:54:47.084551Z","shell.execute_reply.started":"2021-09-19T15:54:46.00111Z","shell.execute_reply":"2021-09-19T15:54:47.083739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re # Regular Expression\n\nimport nltk # Natural Language Toolkit\nnltk.download('stopwords')\nstopwords = nltk.corpus.stopwords.words('english')\nfrom nltk.stem import PorterStemmer,WordNetLemmatizer\nnltk.download('wordnet')\n\ncleaned_text = texts.copy() # cleaned text\npt = PorterStemmer()\nwordnet = WordNetLemmatizer()\nfor index, text in enumerate(texts):\n  # remove handles\n  text = re.sub(r'@[\\w]*','',text)\n\n  # remove urls\n  text = re.sub(r'http[^ ]*','',text)\n  text = re.sub(r'pic.[^ ]*','',text)\n\n  # remove punctuations, numbers, special characters\n  text = re.sub(r'[^A-Za-z#]',' ',text)\n\n  # to lowercase\n  text = text.lower()\n\n  # remove stopwords\n  text = ' '.join(i for i in text.split() if i not in stopwords)\n\n  # stemming\n  text = ' '.join(pt.stem(i) for i in text.split())\n\n  # lemmatizing\n  text = ' '.join(wordnet.lemmatize(i) for i in text.split())\n\n  cleaned_text[index] = text\n\ndf['cleaned'] = cleaned_text\ndf.head() # preview","metadata":{"execution":{"iopub.status.busy":"2021-09-19T15:54:47.08634Z","iopub.execute_input":"2021-09-19T15:54:47.086684Z","iopub.status.idle":"2021-09-19T16:00:58.888033Z","shell.execute_reply.started":"2021-09-19T15:54:47.086648Z","shell.execute_reply":"2021-09-19T16:00:58.887335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nbow = CountVectorizer(min_df=2,max_features=1000)\nbow.fit(df['cleaned']) # transform text to bow\nbow_df = bow.transform(df['cleaned']).toarray() # formatting\n\nprint(bow.get_feature_names()) # show words\nprint(bow_df) # shows feature vector\nprint(bow_df.shape[1]) # number of unique words\n\nbow_train = pd.DataFrame(bow_df)\nprint(bow_train)\nlabel = [0 for i in range(len(fake_data))] + [1 for i in range(len(true_data))]\nbow_train['label'] = label\nbow_train.head() # preview","metadata":{"execution":{"iopub.status.busy":"2021-09-19T16:00:58.889292Z","iopub.execute_input":"2021-09-19T16:00:58.889533Z","iopub.status.idle":"2021-09-19T16:01:20.522515Z","shell.execute_reply.started":"2021-09-19T16:00:58.889502Z","shell.execute_reply":"2021-09-19T16:01:20.521716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\n\nx = bow_train.iloc[:,0:-1]\ny = bow_train['label']\n\nprint(x)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T16:01:20.524616Z","iopub.execute_input":"2021-09-19T16:01:20.524876Z","iopub.status.idle":"2021-09-19T16:01:20.665195Z","shell.execute_reply.started":"2021-09-19T16:01:20.524843Z","shell.execute_reply":"2021-09-19T16:01:20.664448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.neighbors import KNeighborsClassifier\niteration = 1\nbow_acc = []\nfor i in range(iteration):\n# split data into train data, test data\n    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n# classifier\n    k = 7\n    model = KNeighborsClassifier(n_neighbors=k)\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    acc = accuracy_score(y_pred,y_test)\n    bow_acc.append(acc)\n    print(f'[{i+1}] accuracy = {acc}')\n\nprint(f\"AVG Accurancy: {sum(bow_acc)/iteration}\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-19T16:01:20.666558Z","iopub.execute_input":"2021-09-19T16:01:20.666839Z","iopub.status.idle":"2021-09-19T16:12:35.022186Z","shell.execute_reply.started":"2021-09-19T16:01:20.666802Z","shell.execute_reply":"2021-09-19T16:12:35.021406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(min_df=2,max_features=1000)\ntfidf.fit(df['cleaned'])\ntfidf_df = tfidf.transform(df['cleaned']).toarray()\n\nprint(tfidf.get_feature_names()) # show words\nprint(tfidf_df) # shows feature vector\nprint(tfidf_df.shape[1]) # number of unique words\n\ntfidf_train = pd.DataFrame(tfidf_df)\nlabel = [0 for i in range(len(fake_data))] + [1 for i in range(len(true_data))]\ntfidf_train['label'] = label\ntfidf_train.head() # preview\n","metadata":{"execution":{"iopub.status.busy":"2021-09-19T16:12:35.023645Z","iopub.execute_input":"2021-09-19T16:12:35.023915Z","iopub.status.idle":"2021-09-19T16:12:56.173446Z","shell.execute_reply.started":"2021-09-19T16:12:35.02388Z","shell.execute_reply":"2021-09-19T16:12:56.172747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\n\nx_tfidf = tfidf_train.iloc[:,0:-1]\ny_tfidf = tfidf_train['label']\nprint(x_tfidf)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T16:12:56.174877Z","iopub.execute_input":"2021-09-19T16:12:56.17536Z","iopub.status.idle":"2021-09-19T16:12:56.353863Z","shell.execute_reply.started":"2021-09-19T16:12:56.175325Z","shell.execute_reply":"2021-09-19T16:12:56.353097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# classifier\nfrom sklearn.neighbors import KNeighborsClassifier\niteration = 1\n\ntfidf_arr = []\nfor i in range(iteration):\n# split data into train data, test data\n    x_train, x_test, y_train, y_test = train_test_split(x_tfidf,y_tfidf,test_size=0.2)\n\n    k = 7\n    model_tfidf = KNeighborsClassifier(n_neighbors=k)\n    model_tfidf.fit(x_train,y_train)\n    y_pred = model_tfidf.predict(x_test)\n    acc = accuracy_score(y_pred,y_test)\n    tfidf_arr.append(acc)\n    print(f'[{i+1}] accuracy = {acc}')\n    \nprint(f\"AVG Accurancy: {sum(tfidf_arr)/iteration}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-19T16:12:56.355173Z","iopub.execute_input":"2021-09-19T16:12:56.355792Z","iopub.status.idle":"2021-09-19T16:24:20.010314Z","shell.execute_reply.started":"2021-09-19T16:12:56.355754Z","shell.execute_reply":"2021-09-19T16:24:20.008527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv('../input/textdb3/fake_or_real_news.csv')\n\nreal_news = test_data.loc[test_data['label'] == 'REAL']\nfake_news = test_data.loc[test_data['label'] == 'FAKE']\n\n\ntest_news = pd.concat([fake_news, real_news])\ntext_news = test_news['text'].values.tolist()\n","metadata":{"execution":{"iopub.status.busy":"2021-09-19T16:24:20.011566Z","iopub.execute_input":"2021-09-19T16:24:20.011894Z","iopub.status.idle":"2021-09-19T16:24:20.329706Z","shell.execute_reply.started":"2021-09-19T16:24:20.011857Z","shell.execute_reply":"2021-09-19T16:24:20.328984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re # Regular Expression\n\nimport nltk # Natural Language Toolkit\nnltk.download('stopwords')\nstopwords = nltk.corpus.stopwords.words('english')\nfrom nltk.stem import PorterStemmer,WordNetLemmatizer\nnltk.download('wordnet')\n\ncleaned_text = text_news.copy() # cleaned text\n\npt = PorterStemmer()\nwordnet = WordNetLemmatizer()\nfor index, text in enumerate(text_news):\n  # remove handles\n  text = re.sub(r'@[\\w]*','',text)\n\n  # remove urls\n  text = re.sub(r'http[^ ]*','',text)\n  text = re.sub(r'pic.[^ ]*','',text)\n\n  # remove punctuations, numbers, special characters\n  text = re.sub(r'[^A-Za-z#]',' ',text)\n\n  # to lowercase\n  text = text.lower()\n\n  # remove stopwords\n  text = ' '.join(i for i in text.split() if i not in stopwords)\n\n  # stemming\n  text = ' '.join(pt.stem(i) for i in text.split())\n\n  # lemmatizing\n  text = ' '.join(wordnet.lemmatize(i) for i in text.split())\n\n  cleaned_text[index] = text\n\ntest_news['cleaned'] = cleaned_text\ntest_news.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T16:24:20.332183Z","iopub.execute_input":"2021-09-19T16:24:20.332591Z","iopub.status.idle":"2021-09-19T16:25:58.141246Z","shell.execute_reply.started":"2021-09-19T16:24:20.332556Z","shell.execute_reply":"2021-09-19T16:25:58.140549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nbow = CountVectorizer(min_df=2,max_features=1000)\nbow.fit(test_news['cleaned']) # transform text to bow\nbow_df = bow.transform(test_news['cleaned']).toarray() # formatting\n\nprint(bow.get_feature_names()) # show words\nprint(bow_df) # shows feature vector\nprint(bow_df.shape[1]) # number of unique words\n\nbow_train = pd.DataFrame(bow_df)\nlabel = [0 for i in range(len(fake_news))] + [1 for i in range(len(real_news))]\nbow_train['label'] = label\nbow_train.head(10) # preview","metadata":{"execution":{"iopub.status.busy":"2021-09-19T16:25:58.142567Z","iopub.execute_input":"2021-09-19T16:25:58.142814Z","iopub.status.idle":"2021-09-19T16:26:03.456513Z","shell.execute_reply.started":"2021-09-19T16:25:58.142782Z","shell.execute_reply":"2021-09-19T16:26:03.455855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(min_df=2,max_features=1000)\ntfidf.fit(test_news['cleaned'])\ntfidf_df = tfidf.transform(test_news['cleaned']).toarray()\n\nprint(tfidf.get_feature_names()) # show words\nprint(tfidf_df) # shows feature vector\nprint(tfidf_df.shape[1]) # number of unique words\n\ntfidf_train = pd.DataFrame(tfidf_df)\nlabel = [0 for i in range(len(fake_news))] + [1 for i in range(len(real_news))]\ntfidf_train['label'] = label\n","metadata":{"execution":{"iopub.status.busy":"2021-09-19T16:26:03.457818Z","iopub.execute_input":"2021-09-19T16:26:03.458085Z","iopub.status.idle":"2021-09-19T16:26:08.820156Z","shell.execute_reply.started":"2021-09-19T16:26:03.458052Z","shell.execute_reply":"2021-09-19T16:26:08.8194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_bow = bow_train.iloc[:,0:-1]\ny_bow = bow_train['label']\n\n\ny_bow_pred = model.predict(x_bow)\nacc_bow = accuracy_score(y_bow_pred,y_bow)\nprint(f\"bow acc: {acc_bow}\")\n\n\nx_tfidf = tfidf_train.iloc[:,0:-1]\ny_tfidf = tfidf_train['label']\n\n\ny_tfidf_pred = model_tfidf.predict(x_tfidf)\nacc_tfidf = accuracy_score(y_tfidf_pred,y_tfidf)\nprint(f\"tfidf acc: {acc_tfidf}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-19T16:26:08.821286Z","iopub.execute_input":"2021-09-19T16:26:08.821996Z","iopub.status.idle":"2021-09-19T16:42:16.899865Z","shell.execute_reply.started":"2021-09-19T16:26:08.821957Z","shell.execute_reply":"2021-09-19T16:42:16.8981Z"},"trusted":true},"execution_count":null,"outputs":[]}]}