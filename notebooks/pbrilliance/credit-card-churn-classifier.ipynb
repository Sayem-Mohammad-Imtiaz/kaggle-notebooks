{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Credit Card Churn Classifier\n### Below I attempt to predict if a banks' credit card customer will churn or not."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/credit-card-customers/BankChurners.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n## Remove columns that are not needed\n### The first column 'CLIENTNUM' is just an id value and is of no use to this analysis. The final two columns appear to be results from some prior classification analysis that I have no use for. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop 'CLIENTNUM'\ndf.drop(columns='CLIENTNUM', inplace=True)\n\n# Drop last two columns\ndf = df.iloc[:, :-2]\n\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### There are no null values in the dataset. Prior to moving on, I rename the columns to make them less verbose."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.rename(columns={'Customer_Age' : 'age',\n                   'Gender' : 'sex',\n                   'Dependent_count' : 'dependents',\n                   'Education_Level' : 'education',\n                   'Marital_Status' : 'mar_status',\n                   'Income_Category' : 'income',\n                   'Card_Category' : 'card_type',\n                   'Months_on_book' : 'months_customer',\n                   'Total_Relationship_Count' : 'customer_products',\n                   'Months_Inactive_12_mon' : 'ttm_inactive',\n                   'Contacts_Count_12_mon' : 'ttm_contact',\n                   'Credit_Limit' : 'card_limit',\n                   'Total_Revolving_Bal' : 'balance',\n                   'Avg_Open_To_Buy' : 'available_credit',\n                   'Total_Amt_Chng_Q4_Q1' : 'ttm_trans_chng',\n                   'Total_Trans_Amt' : 'ttm_trans',\n                   'Total_Trans_Ct' : 'ttm_trans_cnt',\n                   'Total_Ct_Chng_Q4_Q1' : 'ttm_trans_cnt_chng',\n                   'Avg_Utilization_Ratio' : 'util_ratio'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Next I went through every single column to ensure that all the values made sense. For the sake of brevity, below I only show the ones that I edited in some way.\n\n---\n\n## 'Attrition_Flag'\n### This is the target/label variable. I first check to see the distribution of existing customers to attrited customers then I convert values like so:\n* existing customers : 0\n* attrited customers: 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['Attrition_Flag'].value_counts())\n\nprint(f'Existing Customer: {round(8500/10127, 4) * 100}%')\nprint(f'Attrited Customer: {round(1627/10127, 4) * 100}%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Attrition_Flag'].replace({'Existing Customer' : 0,\n                              'Attrited Customer' : 1}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Next I create a churn feature from 'Attrition_Flag' which is inserted at the end of the dataset and drop 'Attrition_Flag'"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['churn'] = df['Attrition_Flag']\ndf.drop(columns = {'Attrition_Flag'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n## 'Gender' (I renamed this to 'sex')\n\n### I convert this metric like so:\n* M becomes 0\n* F becomes 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sex'].replace({'M' : 0, 'F' : 1}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n## Check correlation between features"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation = np.round(df.corr(),2)\n\ndropSelf = np.zeros_like(correlation)\ndropSelf[np.triu_indices_from(dropSelf)] = True\n\nplt.figure(figsize=(12,12))\nsns.set_style(\"white\")\nsns.heatmap(data=correlation, annot=True, cmap=sns.diverging_palette(240, 10, n=15), mask=dropSelf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From the above correlation matrix it is clear that there is high correlation between:\n* months_customer & age\n* available_credit & card_limit\n* util_ratio & balance\n* ttm_trans_cnt & ttm_trans\n\n### the only one I am going to deal with at this point is the perfectly correlated available_credit and card_limit. The available credit feature is simply the card_limit - balance, therefore it is redundant and I opt to remove it."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns='available_credit', inplace=True)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n## Naive Analysis\n### For a baseline score I will simply take the data as-is and run it through some models."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Feature and Target Variables\nX = pd.get_dummies(df)\nX.drop(columns='churn', inplace=True)\ny = df['churn']\n\n# Function to run and analyze each model\ndef run_model(x, y, name):\n    # Split Data\n    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n   \n    if name == 'logistic':\n        model = LogisticRegression(max_iter=1000,\n                                   solver='liblinear')\n    elif name == 'logistic_bal':\n        model = LogisticRegression(max_iter=1000,\n                                   solver='liblinear',\n                                   class_weight='balanced')     \n    elif name == 'rand_forest':\n        model = RandomForestClassifier()\n    elif name == 'rand_forest_bal':\n        model = RandomForestClassifier(class_weight='balanced_subsample')\n    elif name == 'xgb':\n        model = XGBClassifier()\n    elif name == 'xgb_bal':\n        # Note, the weight value was calculated using the two\n        # churn target variable values:\n        # (total non-churn customer) / (total churn)\n        model = XGBClassifier(scale_pos_weight=5.22)\n    else:\n        print('Error, Incorrect Model')\n\n    # Cross-Validation method 1:  cross_val_predict()\n    # Note: if accuracy score is extremely high, may have overfitting\n    cv_pred = cross_val_predict(model, X_train, y_train, cv=5)\n    print(f'Training Data CV Score Method 1: {np.round(metrics.accuracy_score(y_train, cv_pred),4) * 100}%') \n        \n    # Cross-Validation method 2:  cross_val_score()\n    kfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)\n    cv_result = cross_val_score(model, X_train, y_train, cv=kfold, scoring=\"accuracy\")\n    print(f'Training Data CV Score Method 2: {np.round(cv_result.mean(),4) * 100}%')\n\n    # Fit model\n    model.fit(X_train, y_train)\n\n    # Make predictions\n    y_pred = model.predict(X_test)\n    print(f'Testing Data Accuracy Score: {np.round(metrics.accuracy_score(y_test, y_pred), 4) * 100}%')\n\n    # Classification Report\n    print(f'\\n\\n{name} Classification Report:')\n    print('------------------------------------------------------------')\n    print(metrics.classification_report(y_test, y_pred))\n\n    # Confustion Matrix Heat Map\n    sns.heatmap(metrics.confusion_matrix(y_test,y_pred), annot=True, fmt=\".0f\")\n    plt.title(f'{name} confustion matrix')\n    plt.xlabel('Predicted Values')\n    plt.ylabel('Actual Values')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"run_model(X, y, 'logistic')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The results for the basic logistic regression appear on paper to be decent with an overall 89% accuracy score, however the ability for this model to predict churn correctly is not good at only 51% accuracy. \n\n### The reasoning for this is due to the imbalance between the target variable values. Because only 16% of the samples are churned customers, there is a large disparity between those that have churned and those that have not.\n\n### Thankfully sklearn provides various hyperparameter values to deal with imbalanced data. Below I re-run the logistic regression, however this time in include the hyperparameter: \n* class_weight = 'balanced'\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Balanced logisitc regression \nrun_model(X, y, 'logistic_bal')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Notice that after adding in the balanced hyperparameter, the overall accuracy score decreased, yet the accuracy for predicting churned customer dramatically increased, thus making it a more useful model overall as now both churned and non-churned customers are being predicted correctly roughly 85% of the time. \n\n### One negative side-effect of using the 'balanced' hyperparameter is that now I'm getting a lot of the non_churn customers being classified as churns.\n\n\n### At this point I could try removing features or adjusting other metrics to improve the score, but since I was able to get a baseline that is reasonable, I opt to just try some other models to see if that alone improves the scores.\n\n---\n\n## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unbalanced Random Forest\nrun_model(X, y, 'rand_forest')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Balanced\nrun_model(X, y, 'rand_forest_bal')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Random Forest results are interesting. First of all, the balanced score was less than the unbalanced. Second in both cases the prediction of the churned customers is only around 70% accurate. As things stand, this model is an improvement over unbalanced LR, but balanced LR is still better overall. \n\n---\n\n## XGBoost XBGClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# XGBClassifier Unbalanced \nrun_model(X, y, 'xgb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# XGBClassifier Balanced\nrun_model(X, y, 'xgb_bal')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So the XGBClassifier wins on all levels, however the balanced model works the best. The final results have a 90% accuracy on predicting customers who will churn. With the limited data supplied, minimal model tuning, and no major changes to the data, I am satisfied with these results. \n\n### The scores could potentially be improved through various other means, I tried scaling the data, log transforming right-skewed features, and some other various things, but none of them improved the scores all that much (if at all)."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}