{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ALE explanation using Heart Disease dataset","metadata":{"id":"NL3gJaTZQn02"}},{"cell_type":"markdown","source":"ALE, like PDP (Partial Dependence Plots) is a model agnostic technique useful for the explaining and visualizing the effects of each feature on the results of black box models. However, it is better compared to PDP as it addresses the primary drawbacks of the same. PD Plots ignore the correlation between the features and considers them to be independent which leads to erroneous results when the features of the dataset are highly correlated. ALE, on the other hand, produce good results in spite of there being a correlation between features and are also less computationally expensive. They visualise the effect that each feature, isolated from all other features, has on the predictions of the model. \n\nA second order ALE plot can also show the combined effects of multiple features on the outcome. \n","metadata":{"id":"fBNkV5_QAZxn"}},{"cell_type":"code","source":"!pip install alibi","metadata":{"id":"E0XS0nIQ4z-j","outputId":"ea31afe0-ee88-41e9-875f-6f1c0768a115","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom alibi.explainers.ale import ALE, plot_ale","metadata":{"id":"-IVv7dXk401W","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reading the Heart disease dataset","metadata":{"id":"P-YjlzS9ytM4"}},{"cell_type":"code","source":"data = pd.read_csv('../input/heart-disease-cleveland-uci/heart_cleveland_upload.csv')\n# To display the top 5 rows\ndata.head(5)","metadata":{"id":"bLYsaMIJwhzl","outputId":"dfa1038f-a020-4407-a36a-5e4278cce0b4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heart = data.copy()","metadata":{"id":"Bd5Bfk7hJMG-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = 'condition'\nfeatures_list = list(heart.columns)\nfeatures_list.remove(target)","metadata":{"id":"i07Voc-mJTj2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = heart.pop('condition')","metadata":{"id":"ijHivAigKFkv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting the data into training and testing set","metadata":{"id":"TxlKN2q1zBAy"}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(heart, y, test_size=0.2, random_state=33)","metadata":{"id":"SYkPwV1tKO_R","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Logistic Regression Model","metadata":{"id":"1eoaJewOQgyD"}},{"cell_type":"markdown","source":"Fitting a logistic regression model","metadata":{"id":"01K18G-mzgGW"}},{"cell_type":"code","source":"lr = LogisticRegression(max_iter=1000)\nlr.fit(X_train, y_train)","metadata":{"id":"f5BzoudP5Hq2","outputId":"cd7c6aba-6473-4e4d-838c-ba08065b80e3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluation the model","metadata":{"id":"VBBu7f0tzmD0"}},{"cell_type":"code","source":"accuracy_score(y_test, lr.predict(X_test))","metadata":{"id":"TM_yGu3u5K00","outputId":"b9b3ee89-7325-4fc2-e315-476a019100e6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model gave a quite good accuracy.","metadata":{"id":"uVklYGN_9UVY"}},{"cell_type":"markdown","source":"Now we calculate the Accumulated Local Effects using Probability space :","metadata":{"id":"1pd34wJXzuKw"}},{"cell_type":"code","source":"proba_fun_lr = lr.predict_proba","metadata":{"id":"PU5arS4g5M68","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"proba_ale_lr = ALE(proba_fun_lr, feature_names=features_list, target_names=[0,1])","metadata":{"id":"FOwrLSdO5Oga","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"proba_exp_lr = proba_ale_lr.explain(X_train.values)","metadata":{"id":"t4MtedYx5Pzw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Probability space","metadata":{"id":"Fs-eYdQnK8KH"}},{"cell_type":"markdown","source":"Plotting the ALE plots based on feature effects on the probabilities of each class","metadata":{"id":"xpsgnQ5ELi7c"}},{"cell_type":"code","source":"plot_ale(proba_exp_lr, n_cols=3, fig_kw={'figwidth': 12, 'figheight': 15});","metadata":{"id":"xBuzFbGd6uS_","outputId":"295a4aa0-1872-44fc-8142-224895ae5e27","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The units of Y-axis (ALE) relative probability mass, i.e. given a feature value how much probability the model assigns to each class relative to the mean prediction.\n","metadata":{"id":"AJZJnGpYMq34"}},{"cell_type":"markdown","source":"Let us consider cp for example.","metadata":{"id":"9cQNQPqQD8Xq"}},{"cell_type":"code","source":"plot_ale(proba_exp_lr, features=[2]);","metadata":{"id":"Wwf3cbuvD08y","outputId":"6b860ac0-5b04-4cc2-dcd6-7d80f72d5e87","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this plot,the ALE lines cross the x-axis at approximately 1.6cm, which suggests that for instances of cp around ~1.6cm, the feature effect on the prediction is the same as the average feature effect. Also, going towards the extreme values of the feature, the model assigns a large positive value for 0 and  large negative penalty towards 3 for classification of \"0\" and vice versa for 1. In other words, the feature effect and the probability is more for instances of higher values of cp when the prediction is 1(having heart disease), and similarly when the cp value is lower, there is more probabolity of no heart disease, as expected.","metadata":{"id":"sapFG1LUEM5M"}},{"cell_type":"markdown","source":"Now we plot a histogram for cp","metadata":{"id":"LNKaOc_wPjQd"}},{"cell_type":"code","source":"x_train=X_train.to_numpy()\nfig, ax = plt.subplots()\nfor target in range(2):\n    ax.hist(x_train[y_train==target][:,2],label=target);\n\nax.set_xlabel(features_list[2])\nax.legend();","metadata":{"id":"BvIcWajm6zYV","outputId":"3ea3ab77-39e5-4a21-cc6c-9b94a64a9420","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is clearly seen that as the cp value increases, more no of patients are likely to get the disease.","metadata":{"id":"i_OVExKvPpNV"}},{"cell_type":"markdown","source":"# 2. Using Gradient Boosting Model :","metadata":{"id":"LX5y-15NsH4o"}},{"cell_type":"markdown","source":"Now, we see the effects of ALE on a non linear model using Gradient Boosting.\n","metadata":{"id":"eoijV1vVsHHf"}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier","metadata":{"id":"xHA5togp61VU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gb = GradientBoostingClassifier()\ngb.fit(X_train, y_train)","metadata":{"id":"7e4sNqb365aG","outputId":"c384a5df-ea68-470b-f207-c0721f4d1491","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Testing the accuracy.","metadata":{"id":"LQNuN2MypJeV"}},{"cell_type":"code","source":"accuracy_score(y_test, gb.predict(X_test))","metadata":{"id":"vOKSEARH67bW","outputId":"52ac7643-98a4-4b81-c2ce-ca9b8bb2f321","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The accuracy is good enough.\n","metadata":{"id":"pOYHeBkKpTLm"}},{"cell_type":"code","source":"proba_fun_gb = gb.predict_proba","metadata":{"id":"5V69QLsU69rs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"proba_ale_gb = ALE(proba_fun_gb, feature_names=features_list, target_names=['0','1'])","metadata":{"id":"3acGPof36_18","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"proba_exp_gb = proba_ale_gb.explain(X_train.values[:,:])","metadata":{"id":"XPuR4JN97B99","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gb.feature_importances_","metadata":{"id":"7pR3-SL97GrN","outputId":"3924c759-cc9f-43b9-b8d3-280dd087ce7e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By printing out the feature importances, it can be deduced that ca and thal are the most important features fbs and restecg are the least important.","metadata":{"id":"BKPP26aQq_JA"}},{"cell_type":"markdown","source":"Plotting the graphs for all features towards the probability space","metadata":{"id":"yg-BwkUnrZ-x"}},{"cell_type":"code","source":"plot_ale(proba_exp_gb, n_cols=2, fig_kw={'figwidth': 15, 'figheight': 15})","metadata":{"id":"ZHkqx3Mi7Ick","outputId":"d13e2788-6b56-4c41-e3c3-07d3c148c3c0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since gradient boosting is applied, the plots are no longer linear.\nAs seen in the above graphs, fbs and restecg have almost flat lines which clearly indicates that they are the least important features. In contrast, the plot of ca shows a quite increase in feature effect at higher levels of ca.\n","metadata":{"id":"YyiRD3SlKkwf"}},{"cell_type":"markdown","source":"###Comparing Logistic Regression and Gradient Boosting","metadata":{"id":"zhs_lEMkL6tt"}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(8, 4), sharey='row');\nplot_ale(proba_exp_lr, features=[11,2], targets=[1], ax=ax, line_kw={'label': 'LR'});\nplot_ale(proba_exp_gb, features=[11,2], targets=[1], ax=ax, line_kw={'label': 'GB'});","metadata":{"id":"acUvPNJf7LSd","outputId":"a7434cb6-27e5-4348-8639-1f6e9b2342a2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have considered 2 features namely, cp and ca. The following conclusions can be made from the above graphs:\n\n1.   In both models, the feature weight of ca for predicting class '1' is high. It is increasing in nature an has a high positive influence at 3. From about 0 to 0.5 ca, the feature has a high negative influence on predicting the class '1'.\n2.   Conversely, the second graph shows that the feature weight of cp has a high importance in the LR Model, but doesn't affect the predictions much on GB Model.\n\n","metadata":{"id":"K7aOyfyLtywg"}},{"cell_type":"code","source":"","metadata":{"id":"WfkwWgLeNdf6","trusted":true},"execution_count":null,"outputs":[]}]}