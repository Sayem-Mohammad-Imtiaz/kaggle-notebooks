{"cells":[{"metadata":{},"cell_type":"markdown","source":"### A beginner's guide to model the German credit Risk data"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Necessary imports\nimport pandas as pd\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import fbeta_score\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import cross_val_score\nimport shap\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_credit = pd.read_csv(\"../input/german-credit-data-with-risk/german_credit_data.csv\", index_col=0)\n\n### below renaming is just to make the data consistent with the one on my local\ndf_credit.rename(columns = {'Checking account': 'Credit History', 'Sex': 'Gender'}, inplace=True)\n\ny = df_credit['Risk']\nX = df_credit.drop(columns = ['Risk'])\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.1)\n\ndf_train = pd.concat([X_train, y_train], axis = 1)\ndf_test = pd.concat([X_test, y_test], axis = 1)\n\nprint(df_train.shape, df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train and Test Distribution comparison "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.loc[:, 'Credit History'].hist(alpha=0.5, label='Train', density=True)    \ndf_test.loc[:, 'Credit History'].hist(alpha=0.5, label='Test', density=True)\nplt.xlabel('Credit History')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.loc[:, 'Age'].hist(alpha=0.5, label='Train', density=True)    \ndf_test.loc[:, 'Age'].hist(alpha=0.5, label='Test', density=True)  \nplt.xlabel('Age')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.loc[:, 'Job'].hist(alpha=0.5, label='Train', density=True)    \ndf_test.loc[:, 'Job'].hist(alpha=0.5, label='Test', density=True)\nplt.xlabel('Job')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.loc[:, 'Duration'].hist(alpha=0.5, label='Train', density=True)    \ndf_test.loc[:, 'Duration'].hist(alpha=0.5, label='Test', density=True)\nplt.xlabel('Duration')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.loc[:, 'Credit amount'].hist(alpha=0.5, label='Train', density=True)    \ndf_test.loc[:, 'Credit amount'].hist(alpha=0.5, label='Test', density=True)\nplt.xlabel('Credit amount')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.loc[:, 'Saving accounts'].hist(alpha=0.5, label='Train', density=True)    \ndf_test.loc[:, 'Saving accounts'].hist(alpha=0.5, label='Test', density=True)\nplt.xlabel('Saving accounts')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import ks_2samp\nks_2samp(df_train['Age'], df_test['Age'])\nks_2samp(df_train['Credit amount'], df_test['Credit amount'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preliminary data analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.Risk.value_counts() ### Good = 1 (credit worthy), Bad = 0 (not worthy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.Risk.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_summ = df_train.describe()\ndf_train_summ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_train['Credit amount'])\nplt.title('Credit amount distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### the credit amount is highly skewed distribution, lets analyse the extreme values beyond 3 sigma\ndef extreme_count(sig_factor, feat):\n    sig_cutoff = df_train_summ[feat]['mean'] + sig_factor*df_train_summ[feat]['std'] \n    sig_count = len(df_train[df_train[feat] > sig_cutoff])\n    print(\"instances of {} greater than {} sigma ({} cutoff) are {}\".format(feat, sig_factor, sig_cutoff, sig_count))\n    return\n\nextreme_count(3, feat = 'Credit amount')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_train['Age'])\nplt.title('Age distribution')\nextreme_count(3, feat = 'Age')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_train['Duration'])\nplt.title('Duration distribution')\nextreme_count(3, feat = 'Duration')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Even if there are certain instances where the above features are beyond 3sigma of their mean value, they dont appear to be \noutliers, as its legible to have certain certain loans with high credit value, or loan duration is longer, or older population \nis seeking loan. Hence, not eliminating these rows'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finding Missing values, checking if they are legitimate and applying apt transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### NaN is a valid field here implying no saving account\n\ndf_train['Saving accounts'].value_counts()\ndf_train['Saving accounts'].unique()\n\n### So, replacing NaN with 'no account'\ndf_train.loc[df_train['Saving accounts'].isnull(), 'Saving accounts'] = 'no account'\ndf_train.loc[df_train['Credit History'].isnull(), 'Credit History'] = 'no history'\n\n### Replaced in df\ndf_train['Saving accounts'].value_counts()\ndf_train['Saving accounts'].unique()\n\n### No NaNs anymore\ndf_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking data types and categorical states of features for encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection for label and one hot encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dtypes = pd.DataFrame((df_credit.dtypes == 'object'), columns = ['obj_type'])\nobj_list = df_dtypes[(df_dtypes.obj_type == True)].index\nprint(\"Features for label encoding:\", obj_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Label Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[obj_list].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def le_col(df, col):\n    le = LabelEncoder()\n    le.fit(df[col])\n    df[col] = le.transform(df[col])\n    return df, le\n\ndf_train, le_gender = le_col(df_train, 'Gender')\ndf_train, le_housing = le_col(df_train, 'Housing')\ndf_train, le_sa = le_col(df_train, 'Saving accounts')\ndf_train, le_purpose = le_col(df_train, 'Purpose')\ndf_train, le_ch = le_col(df_train, 'Credit History')\ndf_train, le_risk = le_col(df_train, 'Risk')\n\ndf_train[obj_list].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of Risk variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.Risk.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA 1 : More credit history is equivalent to credit worthiness\n\nConclusion: As the credit history increases, the good risk increases proportionately i..e credit worthiness improves sharply"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist([df_train.loc[df_train['Risk'] == 0, 'Credit History'].values, df_train.loc[df_train['Risk'] == 1, 'Credit History'].values], alpha=0.5, label=['Bad Risk', 'Good Risk'])\nplt.legend(loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train['Risk'] == 0]['Credit History'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train['Risk'] == 1]['Credit History'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA 2 : Are young people more credit worthy?"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist([df_train.loc[df_train['Risk'] == 0, 'Age'].values, df_train.loc[df_train['Risk'] == 1, 'Age'].values], alpha=0.5, label=['Bad Risk', 'Good Risk'])\nplt.legend(loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.Age[df_train.Age <= 30] = 0\ndf_train.Age[(df_train.Age > 30) & (df_train.Age < 45)] = 1\ndf_train.Age[(df_train.Age >= 45)] = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train['Risk'] == 0]['Age'].value_counts()\ndf_train[df_train['Risk'] == 1]['Age'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modelling"},{"metadata":{},"cell_type":"markdown","source":"X_train, y_train prep"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = df_train['Risk']\nX_train = df_train.drop(columns = ['Risk'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test data prep"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.isnull().sum()\n### So, replacing NaN with 'no account' and 'no history'\ndf_test.loc[df_test['Saving accounts'].isnull(), 'Saving accounts'] = 'no account'\ndf_test.loc[df_test['Credit History'].isnull(), 'Credit History'] = 'no history'\ndf_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['Gender'] = le_gender.transform(df_test['Gender'])\ndf_test['Housing'] = le_housing.transform(df_test['Housing'])\ndf_test['Saving accounts'] = le_sa.transform(df_test['Saving accounts'])\ndf_test['Purpose'] = le_purpose.transform(df_test['Purpose'])\ndf_test['Credit History'] = le_ch.transform(df_test['Credit History'])\n\ndf_test['Risk'] = le_risk.transform(df_test['Risk'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = df_test['Risk']\nX_test = df_test.drop(columns = ['Risk'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import fbeta_score, make_scorer\nftwo_scorer = make_scorer(fbeta_score, beta=1/5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Assuming, it is bad to classify a customer as good when they are bad i.e. objective is to reduce FP, we want better precision \n### Hence, applying beta = 1/5 and selecting fbeta_score as evaluation metric\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nfbeta_score(y_test, y_pred, beta=1/5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}