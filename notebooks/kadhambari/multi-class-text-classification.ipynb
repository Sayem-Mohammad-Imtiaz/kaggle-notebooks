{"cells":[{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"6c3c4146-8473-41e2-a662-413efb27665a","_uuid":"637cff8e2103e323ef609bd2c350cb29aa05e099"},"source":"# to be imported\nfrom keras.preprocessing.text import text_to_word_sequence\nimport pandas as pd\nfrom keras.preprocessing.text import Tokenizer\nimport numpy as np\nfrom __future__ import print_function\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.layers import Embedding\nfrom keras.layers import Conv1D, GlobalMaxPooling1D"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"1da17257-6c48-42fe-b761-15148be8a56a","_uuid":"ddb47654b861d29d92251edeed58352f05490e21"},"source":"\n# Read the input dataset \nd = pd.read_csv(\"../input/consumer_complaints.csv\", \n                usecols=('product','consumer_complaint_narrative'),\n                dtype={'consumer_complaint_narrative': object})\n# Only interested in data with consumer complaints\nd=d[d['consumer_complaint_narrative'].notnull()]\n\nd=d[d['product'].notnull()]\nd.reset_index(drop=True,inplace=True)\nx = d.iloc[:, 1].values\ny = d.iloc[:, 0].values\nprint(y)\n\n#there are 11 unique classes for classification\nprint(np.unique(y, return_counts=True))\n\n"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"432b95a1-0703-4279-a5e2-3f4797ba9854","_uuid":"077100d4dd6995a3f78f183057055969e2808927"},"source":" # encode the text with word sequences - Preprocessing step 1\ntk = Tokenizer(num_words= 200, filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True, split=\" \")\ntk.fit_on_texts(x)\nx = tk.texts_to_sequences(x)\nx = sequence.pad_sequences(x, maxlen=200)\n\nprint(x)"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"9316d3ac-cb03-421e-8d81-225238d13978","_uuid":"80b7c94682a711491f8bdf574c382c8f156024dc"},"source":" # Label Encoding categorical data for the classification category\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_Y = LabelEncoder()\ny = labelencoder_Y.fit_transform(y)\nprint(y)\nprint(np.unique(y, return_counts=True))"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"scrolled":true,"_cell_guid":"8a79ea2e-8033-4919-b376-9ace5c02afab","_uuid":"f9c775c1d7793986af517cf922df277e9414a221"},"source":"# Perform one hot encoding \nfrom keras import utils as np_utils\ny = np_utils.to_categorical(y, num_classes= 11)\n\nprint(y)"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"3a817ac7-7adf-404b-becf-5f2a0a9d6d05","_uuid":"af2812f73524c6fe28fa276bc5c67b0db5110ee1"},"source":"# Seeding\nnp.random.seed(200)\nindices = np.arange(len(x))\nnp.random.shuffle(indices)\nx = x[indices]\ny = y[indices]"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true},"source":"index_from=3\nstart_char = 1\nif start_char is not None:\n        x = [[start_char] + [w + index_from for w in x1] for x1 in x]\nelif index_from:\n        x = [[w + index_from for w in x1] for x1 in x]"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{},"source":"\n\nnum_words = None\nif not num_words:\n        num_words = max([max(x1) for x1 in x])\n        \noov_char = 2\nskip_top = 0\n# by convention, use 2 as OOV word\n# reserve 'index_from' (=3 by default) characters:\n# 0 (padding), 1 (start), 2 (OOV)\nif oov_char is not None:\n        x = [[w if (skip_top <= w < num_words) else oov_char for w in x1] for x1 in x]\nelse:\n        x = [[w for w in x1 if (skip_top <= w < num_words)] for x1 in x]\n        \n# split test and train data\ntest_split = 0.2\nidx = int(len(x) * (1 - test_split))\nx_train, y_train = np.array(x[:idx]), np.array(y[:idx])\nx_test, y_test = np.array(x[idx:]), np.array(y[idx:])\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)\nprint(y)"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{},"source":"x_train = sequence.pad_sequences(x_train, maxlen=201)\nx_test = sequence.pad_sequences(x_test, maxlen=201)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{},"source":"\nmax_features = 1000\nmaxlen = 201\nembedding_dims = 50\nfilters = 250\nkernel_size = 3\nhidden_dims = 250\n\n\n# CNN with max pooling imeplementation \nprint('Build model...')\nmodel = Sequential()\n# we start off with an efficient embedding layer which maps\n# our vocab indices into embedding_dims dimensions\nmodel.add(Embedding(max_features,\n                    embedding_dims,\n                    input_length=maxlen))\nmodel.add(Dropout(0.2))\n\n# we add a Convolution1D, which will learn filters\n# word group filters of size filter_length:\nmodel.add(Conv1D(filters,\n                 kernel_size,\n                 padding='valid',\n                 activation='relu',\n                 strides=1))\n# we use max pooling:\nmodel.add(GlobalMaxPooling1D())\n\n# We add a vanilla hidden layer:\nmodel.add(Dense(hidden_dims))\nmodel.add(Dropout(0.2))\nmodel.add(Activation('relu'))\n\n# We project onto a single unit output layer, and squash it with a sigmoid:\nmodel.add(Dense(11))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train,\n          batch_size=32,\n          epochs=50,\n          validation_data=(x_test, y_test))"}],"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"nbconvert_exporter":"python","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","pygments_lexer":"ipython3","version":"3.6.3","name":"python"}}}