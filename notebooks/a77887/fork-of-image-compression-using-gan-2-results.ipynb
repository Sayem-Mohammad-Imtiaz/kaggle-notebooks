{"cells":[{"metadata":{"_cell_guid":"08963816-b283-4302-af42-1357b7e25052","_uuid":"e265b52c-584d-4f80-92ab-f9bfec0f3734","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c4ab2f4f-a173-4553-a22f-f35a2d1db962","_uuid":"04dc0a56-cb90-4c69-b973-0c79214dc59e","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nimport torchvision.utils as vutils\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport copy\nimport time\nimport cv2 as cv\nfrom tqdm import tqdm_notebook as tqdm\nimport matplotlib.image as mpimg\n\nimport torchvision.transforms.functional as TF","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"15441219-cac0-409e-bbc6-a90c647c9665","_uuid":"72f80c88-1643-4205-8688-7916305d26f2","trusted":true},"cell_type":"code","source":"# Code -- https://github.com/alexandru-dinu/cae\n# DataBase -- https://www.kaggle.com/hsankesara/flickr-image-dataset\n\n\n\n\nimg_dir = '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/flickr30k_images/'\nimg_list = os.listdir(img_dir)\nprint(len(img_list))\nvalid_ratio = 0.8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"352ebe3e-f5e0-4fce-b5da-71a230783edd","_uuid":"8cf61d41-c8f2-4a15-89cc-3be91e942e62","trusted":true},"cell_type":"code","source":"class ImageData(Dataset):\n    def __init__(self,is_train=True):\n        self.is_train = is_train\n        self.transform = transforms.Compose([transforms.ToTensor(),])\n        self.train_index = int(valid_ratio * len(img_list))\n        self.crop = transforms.CenterCrop((218,178))\n    def __len__(self):\n        if self.is_train:\n            return self.train_index\n        else:\n            return len(img_list) - self.train_index -1\n    def __getitem__(self, index):\n        if not self.is_train:\n            index = self.train_index + index\n#         print(\"hey  \"*4 + str(index))\n        img = mpimg.imread(img_dir+img_list[index])\n        img = self.crop(TF.to_pil_image(img))\n        img = self.transform(img)\n        img = (img-0.5) /0.5\n#         img = (img - 255.0) / 255.0\n        return img","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9732df15-3bc9-46ec-be0c-304661836ac2","_uuid":"dcdd031a-9938-40c6-a3cc-7417733e858d","trusted":true},"cell_type":"code","source":"batch_size=20\ndataset = ImageData(is_train=False)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\ndevice = 'cuda'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8d126d61-eebd-4959-adc4-d0cbfda2278c","_uuid":"75e6a343-b614-481e-8a12-3eca4ff0c7b0","trusted":true},"cell_type":"code","source":"a = next(iter(dataloader))\nprint(a[0].shape)\nimg = a[15]\nimg = img *0.5 + 0.5\nplt.imshow(img.permute(1,2,0))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8e6a177b-dc74-401d-8ffa-5befd521651e","_uuid":"0511b538-0033-492e-a269-4b1b64d320ef","trusted":true},"cell_type":"code","source":"# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a3e261cf-79be-4cf6-bfd4-208be76cbd12","_uuid":"b5953492-9a59-43bd-9a2e-5f69e5227946","trusted":true},"cell_type":"code","source":"IMG_WIDTH = 178\nIMG_HEIGHT = 218\nlatent_size = 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_images_to_show = 5\n\n\nvalid_dataset = ImageData(is_train=False)\nbatch_size = num_images_to_show\nvalid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\nvalid_batch = next(iter(valid_dataloader)).to(device)\nvalid_batch_1 = next(iter(valid_dataloader)).to(device)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0ef5c761-0d0f-490e-a4f8-b770c3eab3bd","_uuid":"5fd1ec79-1cc6-4b96-b451-f74d29969670","trusted":true},"cell_type":"code","source":"# Encoder Model\nclass Encoder(nn.Module):\n    def __init__(self,num_channels_in_encoder):\n        super(Encoder, self).__init__()\n        \n        # ENCODER\n\n        # 64x64x64\n        self.e_conv_1 = nn.Sequential(\n            nn.ZeroPad2d((1, 2, 1, 2)),\n            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(5, 5), stride=(2, 2)),nn.LeakyReLU()\n        )\n\n        # 128x32x32\n        self.e_conv_2 = nn.Sequential(\n            nn.ZeroPad2d((1, 2, 1, 2)),\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(5, 5), stride=(2, 2)),\n            nn.LeakyReLU()\n        )\n        \n        # 128x32x32\n        self.e_block_1 = nn.Sequential(\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n        )\n\n        # 128x32x32\n        self.e_block_2 = nn.Sequential(\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n        )\n\n        # 128x32x32\n        self.e_block_3 = nn.Sequential(\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n        )\n\n        # 32x32x32\n        self.e_conv_3 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=num_channels_in_encoder, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)),\n            nn.Tanh()\n        )\n    def forward(self, x):\n        ec1 = self.e_conv_1(x)\n        ec2 = self.e_conv_2(ec1)\n        eblock1 = self.e_block_1(ec2) + ec2\n        eblock2 = self.e_block_2(eblock1) + eblock1\n        eblock3 = self.e_block_3(eblock2) + eblock2\n        ec3 = self.e_conv_3(eblock3)  # in [-1, 1] from tanh activation\n        return ec3","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5bff9e09-cee6-428a-8368-95da81e9f10a","_uuid":"839c8341-9f75-4a31-8b7e-818269e482b5","trusted":true},"cell_type":"code","source":"device","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1c175dca-7190-4bb2-a510-1d86de4e902c","_uuid":"4ddb97c8-6514-4cab-817b-8222e972596e","trusted":true},"cell_type":"code","source":"# Generator / Decoder Model\n\nclass Generator(nn.Module):\n    def __init__(self,num_channels_in_encoder):\n        super(Generator, self).__init__()\n        \n        # DECODER\n#         self.latent_fc1 = nn.Sequential(\n#             nn.Linear(latent_size,1000),\n#             nn.Sigmoid(),\n#         )\n#         self.latent_fc2 = nn.Sequential(\n#             nn.Linear(1000,54*44),\n#             nn.Sigmoid(),\n#         )\n        # 128x64x64\n        self.d_up_conv_1 = nn.Sequential(\n        nn.Conv2d(in_channels=num_channels_in_encoder, out_channels=64, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.ConvTranspose2d(in_channels=64, out_channels=128, kernel_size=(2, 2), stride=(2, 2))\n        )\n\n        # 128x64x64\n        self.d_block_1 = nn.Sequential(\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n        )\n\n        # 128x64x64\n        self.d_block_2 = nn.Sequential(\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n        )\n\n        # 128x64x64\n        self.d_block_3 = nn.Sequential(\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n        )\n\n        # 256x128x128\n        self.d_up_conv_2 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=32, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.ConvTranspose2d(in_channels=32, out_channels=256, kernel_size=(2, 2), stride=(2, 2))\n        )\n\n        # 3x128x128\n        self.d_up_conv_3 = nn.Sequential(\n            nn.Conv2d(in_channels=256, out_channels=16, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ReflectionPad2d((3, 3, 3, 3)),\n            nn.Conv2d(in_channels=16, out_channels=3, kernel_size=(3, 3), stride=(1, 1)),\n            nn.Tanh()\n        )\n\n        \n        \n    def forward(self, x):\n        uc1 = self.d_up_conv_1(x)\n        dblock1 = self.d_block_1(uc1) + uc1\n        dblock2 = self.d_block_2(dblock1) + dblock1\n        dblock3 = self.d_block_3(dblock2) + dblock2\n        uc2 = self.d_up_conv_2(dblock3)\n        dec = self.d_up_conv_3(uc2)\n        return dec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_channels_in_encoder = 28\nnetE28 = Encoder(num_channels_in_encoder).to(device)\nnetE28.apply(weights_init)\n\nnum_channels_in_encoder = 28\nnetG28 = Generator(num_channels_in_encoder).to(device)\nnetG28.apply(weights_init)\n\nnetG28.load_state_dict(torch.load(\"/kaggle/input/trained-image-compressionmodels/netG28.model\"))\nnetE28.load_state_dict(torch.load(\"/kaggle/input/trained-image-compressionmodels/netE28.model\"))\n\n\nnetE28.eval()\nnetG28.eval()\n\n\nreconstructed_img_28 = netG28(netE28(valid_batch))\nreconstructed_img_28_1 = netG28(netE28(valid_batch_1))\n\n\n\n\ndel netE28\ndel netG28\ntorch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_channels_in_encoder = 16\nnetG16 = Generator(num_channels_in_encoder).to(device)\nnetG16.apply(weights_init)\n\nnum_channels_in_encoder = 16\nnetE16 = Encoder(num_channels_in_encoder).to(device)\nnetE16.apply(weights_init)\n\nnetG16.load_state_dict(torch.load(\"/kaggle/input/trained-image-compressionmodels/netG16.model\"))\nnetE16.load_state_dict(torch.load(\"/kaggle/input/trained-image-compressionmodels/netE16.model\"))\n\n\nnetE16.eval()\nnetG16.eval()\nreconstructed_img_16 = netG16(netE16(valid_batch))\nreconstructed_img_16_1 = netG16(netE16(valid_batch_1))\n\ndel netE16\ndel netG16\ntorch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_channels_in_encoder = 8\nnetE8 = Encoder(num_channels_in_encoder).to(device)\nnetE8.apply(weights_init)\n\n\nnetG8 = Generator(num_channels_in_encoder).to(device)\nnetG8.apply(weights_init)\n\n\nnetG8.load_state_dict(torch.load(\"/kaggle/input/trained-image-compressionmodels/netG8.model\"))\nnetE8.load_state_dict(torch.load(\"/kaggle/input/trained-image-compressionmodels/netE8.model\"))\n\n\nnetG8.eval()\nnetE8.eval()\n\nreconstructed_img_8 = netG8(netE8(valid_batch))\nreconstructed_img_8_1 = netG8(netE8(valid_batch_1))\n\ndel netE8\ndel netG8\ntorch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8b16da2d-dd02-4cd7-a846-858e10b1d498","_uuid":"455c713c-47dc-49a1-b127-db82978fea5c","trusted":true},"cell_type":"code","source":"\nf, axarr = plt.subplots(num_images_to_show,4)\n\naxarr[0,0].title.set_text('Original \\n Image')\naxarr[0,1].title.set_text('Reconstructed Image with \\n 43% Compression')\naxarr[0,2].title.set_text('Reconstructed Image with \\n 68% Compression')\naxarr[0,3].title.set_text('Reconstructed Image with \\n 84% Compression')\n\nfor i in range(4):\n    axarr[0,i].title.set_fontsize(15)\n\nfor i in range(num_images_to_show):\n    axarr[i,0].imshow((valid_batch[i].cpu().detach().permute(1, 2, 0) * 0.5) + 0.5)\n    axarr[i,1].imshow((reconstructed_img_28[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5)\n    axarr[i,2].imshow((reconstructed_img_16[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5)\n    axarr[i,3].imshow((reconstructed_img_8[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5)\n    f.set_figheight(20)\n    f.set_figwidth(20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.savefig('results.png')\nf.savefig('results.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axarr = plt.subplots(2,2)\n\naxarr[0,0].title.set_text('Original \\n Image')\naxarr[0,1].title.set_text('Reconstructed Image with \\n 43% Compression')\naxarr[1,0].title.set_text('Reconstructed Image with \\n 68% Compression')\naxarr[1,1].title.set_text('Reconstructed Image with \\n 84% Compression')\n\nfor i in range(2):\n    for j in range(2):\n        axarr[i,j].title.set_fontsize(40)\ni = 0\n\n\nreimg = (valid_batch_1[i].cpu().detach().permute(1, 2, 0) * 0.5) + 0.5\nreimg_28 = (reconstructed_img_28_1[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5\nreimg_16 = (reconstructed_img_16_1[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5\nreimg_8 = (reconstructed_img_8_1[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5\n\n\n\naxarr[0,0].imshow(reimg)\naxarr[0,1].imshow(reimg_28)\naxarr[1,0].imshow(reimg_16)\naxarr[1,1].imshow(reimg_8)\nf.set_figheight(50)\nf.set_figwidth(50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.savefig('results1.png')\nf.savefig('results1.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nfrom math import exp\n\ndef gaussian(window_size, sigma):\n    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n    return gauss/gauss.sum()\n\ndef create_window(window_size, channel):\n    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n    return window\n\ndef _ssim(img1, img2, window, window_size, channel, size_average = True):\n    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n\n    mu1_sq = mu1.pow(2)\n    mu2_sq = mu2.pow(2)\n    mu1_mu2 = mu1*mu2\n\n    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n\n    C1 = 0.01**2\n    C2 = 0.03**2\n\n    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n\n    if size_average:\n        return ssim_map.mean()\n    else:\n        return ssim_map.mean(1).mean(1).mean(1)\n\nclass SSIM(torch.nn.Module):\n    def __init__(self, window_size = 11, size_average = True):\n        super(SSIM, self).__init__()\n        self.window_size = window_size\n        self.size_average = size_average\n        self.channel = 1\n        self.window = create_window(window_size, self.channel)\n\n    def forward(self, img1, img2):\n        (_, channel, _, _) = img1.size()\n\n        if channel == self.channel and self.window.data.type() == img1.data.type():\n            window = self.window\n        else:\n            window = create_window(self.window_size, channel)\n            \n            if img1.is_cuda:\n                window = window.cuda(img1.get_device())\n            window = window.type_as(img1)\n            \n            self.window = window\n            self.channel = channel\n\n\n        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n\ndef ssim(img1, img2, window_size = 11, size_average = True):\n    (_, channel, _, _) = img1.size()\n    window = create_window(window_size, channel)\n    \n    if img1.is_cuda:\n        window = window.cuda(img1.get_device())\n    window = window.type_as(img1)\n    \n    return _ssim(img1, img2, window, window_size, channel, size_average)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reimg = reimg.view(1,reimg.shape[0],reimg.shape[1],reimg.shape[2])\nreimg_28 = reimg_28.view(1,reimg_28.shape[0],reimg_28.shape[1],reimg_28.shape[2])\nreimg_16 = reimg_16.view(1,reimg_16.shape[0],reimg_16.shape[1],reimg_16.shape[2])\nreimg_8 = reimg_8.view(1,reimg_8.shape[0],reimg_8.shape[1],reimg_8.shape[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reimg = int(reimg.view(1,reimg.shape[0],reimg.shape[1],reimg.shape[2]) * 256)\n# reimg_28 = int(reimg_28.view(1,reimg_28.shape[0],reimg_28.shape[1],reimg_28.shape[2]) *256)\n# reimg_16 = int(reimg_16.view(1,reimg_16.shape[0],reimg_16.shape[1],reimg_16.shape[2]) *256)\n# reimg_8 = int(reimg_8.view(1,reimg_8.shape[0],reimg_8.shape[1],reimg_8.shape[2]) *256)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import torch\n# from torch.autograd import Variable\n\n# img1 = Variable(torch.rand(1, 1, 256, 256))\n# img2 = Variable(torch.rand(1, 1, 256, 256))\n\n# if torch.cuda.is_available():\n#     img1 = img1.cuda()\n#     img2 = img2.cuda()\n\n\n# reimg_28 = reimg_28.view(1,reimg_28.shape[0],reimg_28.shape[1],reimg_28.shape[2])\n# reimg = reimg.view(1,reimg.shape[0],reimg.shape[1],reimg.shape[2])\n# reimg = reimg.view(1,reimg.shape[0],reimg.shape[1],reimg.shape[2])\nprint(ssim(reimg, reimg_28))\nprint(ssim(reimg, reimg_16))\nprint(ssim(reimg, reimg_8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reimg.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}