{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---\n## Fetal Health Classification Problem\n---\n### Aurthor: Avinash Bagul\n##### MSc Artificial Intelligence (University of Aberdeen)","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/fetal-health-classification/fetal_health.csv')\ndf.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking for number of missing values in each column.....","metadata":{}},{"cell_type":"code","source":"import missingno as msno\nn = msno.bar(df,color=\"gray\")\nprint(n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Description of the Data","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of Target class: Highly imbalanced","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=\"fetal_health\",data = df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking for outliers in the data","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.boxplot(data = df,palette = \"Set1\")\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removing the outliers: by setting upper and lower threshold","metadata":{}},{"cell_type":"code","source":"# Function to set upper and lower bound to 3rd standard deviation and remove outliers\n\ndef removeOutlier(att, df):\n\n    lowerbound = att.mean() - 3 * att.std()\n    upperbound = att.mean() + 3 * att.std()\n\n    print('lowerbound: ',lowerbound,' -------- upperbound: ', upperbound )\n\n    df1 = df[(att > lowerbound) & (att < upperbound)]\n\n    print((df.shape[0] - df1.shape[0]), ' number of outliers from ', df.shape[0] )\n    print(' ******************************************************')\n    \n    df = df1.copy()\n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removing outliers from columns showing outiers in the boxplot visualized above","metadata":{}},{"cell_type":"code","source":"df = removeOutlier(df.histogram_variance, df)\ndf = removeOutlier(df.histogram_median, df)\ndf = removeOutlier(df.histogram_mean, df)\ndf = removeOutlier(df.histogram_mode, df)\ndf = removeOutlier(df.percentage_of_time_with_abnormal_long_term_variability, df)\ndf = removeOutlier(df.mean_value_of_short_term_variability, df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Correlation HeatMap","metadata":{}},{"cell_type":"code","source":"corrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,10))\ng = sns.heatmap(df[top_corr_features].corr(),annot = True,cmap = \"RdYlGn\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Balancing Dataset:","metadata":{}},{"cell_type":"code","source":"df.fetal_health.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import resample\n\n# Separate Target Classes\ndf_1 = df[df.fetal_health==1]\ndf_2 = df[df.fetal_health==2]\ndf_3 = df[df.fetal_health==3]\n \n# Upsample minority class\ndf_2_upsampled = resample(df_2, \n                                 replace=True,     # sample with replacement\n                                 n_samples=1601,    # to match majority class\n                                 random_state=123) # reproducible results\n\ndf_3_upsampled = resample(df_3, \n                                 replace=True,     # sample with replacement\n                                 n_samples=1601,    # to match majority class\n                                 random_state=123) # reproducible results\n\n# Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_1, df_2_upsampled, df_3_upsampled])\n \n# Display new class counts\ndf_upsampled.fetal_health.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Separating Fetures and Target Variable","metadata":{}},{"cell_type":"code","source":"x = df_upsampled.drop('fetal_health', axis = 1)\ny = df_upsampled['fetal_health'] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.25, random_state = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature Scaling: Standardization","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Models I am going to use are: \n# XGBoost\n# AdaBoost\n# CataBoost\n# RandomForest\n# LBGM Classifier\n# Voting Classifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluator Function: \nAccuracy, Precision, Recall, f1-Score, roc_auc_score and Confusion Matrix","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score\nfrom sklearn.preprocessing import LabelBinarizer\nfrom mlxtend.plotting import plot_confusion_matrix\n\ndef evaluator(y_test, y_pred):    \n    \n    # Accuracy:\n    print('Accuracy is: ', accuracy_score(y_test,y_pred))\n    print('')\n    # Classification Report:\n    print('Classification Report: \\n',classification_report(y_test,y_pred))\n\n    # Area Under The Curve Score:\n\n    lb = LabelBinarizer()\n    y_test1 = lb.fit_transform(y_test)\n    y_pred1 =lb.transform(y_pred)\n    print('AUC_ROC Score: ',roc_auc_score(y_test1,y_pred1,average='macro'),'\\n\\n')\n\n    print('Confusion Matrix: \\n\\n')\n    plt.style.use(\"ggplot\")\n    cm = confusion_matrix(y_test,y_pred)\n    plot_confusion_matrix(conf_mat = cm,figsize=(8,6),show_normed=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Building Model:","metadata":{}},{"cell_type":"markdown","source":"# XGBOOST","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxgb_classifier = XGBClassifier()\nxgb_classifier.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_xgb = xgb_classifier.predict(x_test)\n\nevaluator(y_test, pred_xgb)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AdaBoost:","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nada_classifier = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=1),\n    n_estimators=200\n)\n\nada_classifier.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_ada = ada_classifier.predict(x_test)\n\nevaluator(y_test, pred_ada)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CatBoost:","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier\n\ncat_classifier = CatBoostClassifier(iterations=1000, verbose = 0)\n\ncat_classifier.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_cat = cat_classifier.predict(x_test)\n\nevaluator(y_test, pred_cat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LBGM Classifier:","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nlgb_classifier = LGBMClassifier()\nlgb_classifier.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_lgb = lgb_classifier.predict(x_test)\n\nevaluator(y_test,pred_lgb)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest Classifier:","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf_classifier = RandomForestClassifier()\n\nrf_classifier.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_rf = rf_classifier.predict(x_test)\n\nevaluator(y_test, pred_rf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Important Features","metadata":{}},{"cell_type":"code","source":"important_features = pd.DataFrame({'Features': x.columns, \n                                   'Importance': rf_classifier.feature_importances_})\n\n# sort the dataframe in the descending order according to the feature importance\nimportant_features = important_features.sort_values('Importance', ascending = False)\n\n# create a barplot to visualize the features based on their importance\nsns.barplot(x = 'Importance', y = 'Features', data = important_features)\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.title('Feature Importance', fontsize = 15)\nplt.xlabel('Importance', fontsize = 15)\nplt.ylabel('Features', fontsize = 15)\n\n# display the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Voting Classifier:","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\nvc = VotingClassifier(estimators = [(\"xgb_classifier\",xgb_classifier),('ada_classifier', ada_classifier),('cat _classifier', cat_classifier),(\"lgb_classifier\",lgb_classifier),(\"rf_classifier\",rf_classifier)],voting='soft')\nvc.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_vc = vc.predict(x_test)\n\nevaluator(y_test, pred_vc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Result and Conclusion:\nAll models perform good except for adaboost (after balancing).\n\nAccuracy has significantly increased by 4 to 5 percent after balancing out the data. To balance the data resampling was done by up_sampling i.e, duplicating the minority class to meet the value_count of majority class.\n\nRandom Forest Classifier is performing the best based on the evaluation matrices used.","metadata":{}},{"cell_type":"markdown","source":"---\n### **Thank You**\n---\n\nAuthor: Avinash Vinayak Bagul\n(MSc Artificial Intelligence)","metadata":{}}]}