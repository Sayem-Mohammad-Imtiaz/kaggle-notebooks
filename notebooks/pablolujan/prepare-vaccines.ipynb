{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#import libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n#Natural language toolkit\nimport nltk\n#list of punctuation\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer\nimport base64\nfrom IPython.display import HTML\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#read csv\n#nltk.download()\ndf = pd.read_csv('../input/all-covid19-vaccines-tweets/vaccination_all_tweets.csv')\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().mean().sort_values(ascending=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rellenamos vacíos\ndf.fillna(value={'user_location':'none','hashtags':'none','user_description':'none','source':'none'} ,inplace = True)\n#convertimos a minúsculas\ndf = df.apply(lambda x: x.astype(str).str.lower())\n#mapeamos true/false a número\ntrue_map = {'true':1,'false':0}\ndf = df.applymap(lambda q: true_map.get(q) if q in true_map else q)\n#eliminamos columnas\ndf.pop('id')\ndf.pop('user_name')\ndf.pop('user_created')\ndf.pop('is_retweet')\ndf.pop('user_description')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['user_followers', 'user_friends','user_favourites','retweets','favorites']] = df[['user_followers', 'user_friends','user_favourites','retweets','favorites']].apply(pd.to_numeric)\ndf.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_punctuation(text):\n    no_punct = ''.join([c for c in text if c not in string.punctuation])\n    return no_punct\n\ndef remove_stopwords(text):\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\n\nlemmatizer = WordNetLemmatizer()\ndef word_lemmatizer(text):\n    lem_text = [lemmatizer.lemmatize(i) for i in text]\n    return lem_text\n\ntokenizer = RegexpTokenizer(r'\\w+')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove punctuation\ndf['text']=df['text'].apply(lambda x: remove_punctuation(x))\ndf['user_location']=df['user_location'].apply(lambda x: remove_punctuation(x))\ndf['hashtags']=df['hashtags'].apply(lambda x: remove_punctuation(x))\ndf['source']=df['source'].apply(lambda x: remove_punctuation(x))\ndf.head()#","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenize\ndf['text']=df['text'].apply(lambda x: tokenizer.tokenize(x))\ndf['user_location']=df['user_location'].apply(lambda x: tokenizer.tokenize(x))\ndf['hashtags']=df['hashtags'].apply(lambda x: tokenizer.tokenize(x))\ndf['source']=df['source'].apply(lambda x: tokenizer.tokenize(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove stopwords\ndf['text']=df['text'].apply(lambda x: remove_stopwords(x))\ndf['user_location']=df['user_location'].apply(lambda x: remove_stopwords(x))\ndf['hashtags']=df['hashtags'].apply(lambda x: remove_stopwords(x))\ndf['source']=df['source'].apply(lambda x: remove_stopwords(x))\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#word lemmatizer\ndf['text']=df['text'].apply(lambda x: word_lemmatizer(x))\ndf['user_location']=df['user_location'].apply(lambda x: word_lemmatizer(x))\ndf['hashtags']=df['hashtags'].apply(lambda x: word_lemmatizer(x))\ndf['source']=df['source'].apply(lambda x: word_lemmatizer(x))\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a random sample dataframe\n#df = pd.DataFrame(np.random.randn(50, 4), columns=list('ABCD'))\n\n# create a link to download the dataframe\ncreate_download_link(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}