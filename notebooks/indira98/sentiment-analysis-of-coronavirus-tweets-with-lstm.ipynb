{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-21T09:03:30.465397Z","iopub.execute_input":"2021-05-21T09:03:30.465738Z","iopub.status.idle":"2021-05-21T09:03:30.488299Z","shell.execute_reply.started":"2021-05-21T09:03:30.465702Z","shell.execute_reply":"2021-05-21T09:03:30.48734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentiment analysis of Coronavirus tweets with LSTM","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Tensorflow imports\n\n# for building model\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Flatten, GlobalMaxPooling1D, Dense, Dropout\n\n# for Padding\nfrom tensorflow.keras import preprocessing\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# for Tokenization \nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\n# NLTK imports\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# For visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# For regular expressions\nimport re\n\n# For data preprocessing\nfrom string import punctuation, digits\n","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:02:41.204052Z","iopub.execute_input":"2021-05-21T09:02:41.204767Z","iopub.status.idle":"2021-05-21T09:02:48.686487Z","shell.execute_reply.started":"2021-05-21T09:02:41.204632Z","shell.execute_reply":"2021-05-21T09:02:48.685548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use 'Latin 1' encoding to recognize latin characters\n# importing dataset\n\ntrain_df = pd.read_csv(\"/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_train.csv\", encoding=\"Latin-1\")\ntest_df = pd.read_csv(\"/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_test.csv\", encoding=\"Latin-1\")\n\nprint(f\"train dataset shape >> {train_df.shape}\")\nprint(f\"test dataset shape >> {test_df.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:06:12.918174Z","iopub.execute_input":"2021-05-21T09:06:12.918535Z","iopub.status.idle":"2021-05-21T09:06:13.11622Z","shell.execute_reply.started":"2021-05-21T09:06:12.918502Z","shell.execute_reply":"2021-05-21T09:06:13.115234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creat new dataset (which will consist from 2 columns: label, data)\n\ndef data_label_split(dataset):\n    data = dataset['OriginalTweet']\n    label = dataset['Sentiment']\n    return data,label\n\ntrain_data,train_label = data_label_split(train_df)\ntest_data,test_label = data_label_split(test_df)\n\ntrain = pd.DataFrame({\n    'label':train_label,\n    'data':train_data\n})\n\ntest = pd.DataFrame({\n    'label':test_label,\n    'data':test_data\n})\n\n# Define function which will make new labels\n\ndef reassign_label(x):\n    if x == \"Extremely Positive\" or x == \"Positive\":\n        return 1\n    elif x ==\"Extremely Negative\" or x ==\"Negative\":\n        return -1\n    elif x ==\"Neutral\":\n        return 0\n\ntrain.label = train.label.apply(lambda x:reassign_label(x))\ntest.label = test.label.apply(lambda x:reassign_label(x))\n","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:06:36.141744Z","iopub.execute_input":"2021-05-21T09:06:36.142051Z","iopub.status.idle":"2021-05-21T09:06:36.18637Z","shell.execute_reply.started":"2021-05-21T09:06:36.142026Z","shell.execute_reply":"2021-05-21T09:06:36.18519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 Data preprocessing","metadata":{}},{"cell_type":"code","source":"def remove_punctuation(s):\n    list_punctuation = list(punctuation)\n    for i in list_punctuation:\n        s = s.replace(i,'')\n    return s.lower()\n\ndef clean_sentence(sentence):\n    sentence = sentence.lower()\n    sentence = re.sub(r'(\\W)\\1{2,}', r'\\1', sentence) \n    sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n    sentence = re.sub(r'(?P<url>https?://[^\\s]+)', '', sentence) # remove URL adresses\n    sentence = re.sub(r\"\\@(\\w+)\", '', sentence) # remove usernames\n    sentence = re.sub(r\"\\#(\\w+)\", '', sentence) # remove hashtags\n    sentence = re.sub(r\"\\$(\\w+)\", '', sentence) # remove cashtags\n    sentence = sentence.replace(\"-\",' ')\n    tokens = sentence.split()\n    tokens = [remove_punctuation(w) for w in tokens] # remove punctuations\n    stop_words = set(stopwords.words('english')) # remove stopwords\n    tokens = [w for w in tokens if not w in stop_words]\n    remove_digits = str.maketrans('', '', digits)\n    tokens = [w.translate(remove_digits) for w in tokens]\n    tokens = [w.strip() for w in tokens]\n    tokens = [w for w in tokens if w!=\"\"]\n    tokens = ' '.join(tokens)\n    return tokens","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:07:08.204983Z","iopub.execute_input":"2021-05-21T09:07:08.205346Z","iopub.status.idle":"2021-05-21T09:07:08.214091Z","shell.execute_reply.started":"2021-05-21T09:07:08.205318Z","shell.execute_reply":"2021-05-21T09:07:08.213054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean sentences in train and test data\ntrain.data = train.data.apply(lambda sentence:clean_sentence(sentence))\ntest.data = test.data.apply(lambda sentence:clean_sentence(sentence))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:07:19.478762Z","iopub.execute_input":"2021-05-21T09:07:19.479068Z","iopub.status.idle":"2021-05-21T09:07:32.223718Z","shell.execute_reply.started":"2021-05-21T09:07:19.479042Z","shell.execute_reply":"2021-05-21T09:07:32.222604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove letter ' â ' in words\ndef rem_latin_a(sentences):\n    sentences = sentences.replace(\"â\", \"\")\n    return sentences\n\ntrain.data = train.data.apply(lambda sentences:rem_latin_a(sentences))\ntest.data = test.data.apply(lambda sentences:rem_latin_a(sentences))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:07:35.220919Z","iopub.execute_input":"2021-05-21T09:07:35.22124Z","iopub.status.idle":"2021-05-21T09:07:35.247223Z","shell.execute_reply.started":"2021-05-21T09:07:35.221216Z","shell.execute_reply":"2021-05-21T09:07:35.246548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace all empty lines in the 'data' column with np.nan objects \ntrain['data'].replace('', np.nan, inplace=True)\ntest['data'].replace('', np.nan, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:07:45.566759Z","iopub.execute_input":"2021-05-21T09:07:45.567313Z","iopub.status.idle":"2021-05-21T09:07:45.575418Z","shell.execute_reply.started":"2021-05-21T09:07:45.567278Z","shell.execute_reply":"2021-05-21T09:07:45.574726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Discard null values\n\n# Originally there were 41157 rows, now it has become 41106. The training dataset was reduced by 51 lines.\ntrain.dropna(subset=['data'], inplace=True)\n\n# Originally there were 3798 lines, now it has become 3795. The test dataset was reduced by 3 lines.\ntest.dropna(subset=['data'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:07:59.075842Z","iopub.execute_input":"2021-05-21T09:07:59.076397Z","iopub.status.idle":"2021-05-21T09:07:59.115556Z","shell.execute_reply.started":"2021-05-21T09:07:59.076362Z","shell.execute_reply":"2021-05-21T09:07:59.114647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting data to train and test\n\n# Train data\ntrain_data = train.data\ntrain_label = train.label\n\n# Test data\ntest_data = test.data\ntest_label = test.label","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:08:10.885859Z","iopub.execute_input":"2021-05-21T09:08:10.886317Z","iopub.status.idle":"2021-05-21T09:08:10.892215Z","shell.execute_reply.started":"2021-05-21T09:08:10.886288Z","shell.execute_reply":"2021-05-21T09:08:10.891393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.1 Label preparation","metadata":{}},{"cell_type":"code","source":"# Plot a graph of the number of elements in each class in the train data\n\nsns.countplot(x=train_label)\nplt.title('The number of elements in each class in the train data', fontdict={'fontname': 'Comic Sans MS', 'fontsize': 15})\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:08:33.850123Z","iopub.execute_input":"2021-05-21T09:08:33.850601Z","iopub.status.idle":"2021-05-21T09:08:34.07835Z","shell.execute_reply.started":"2021-05-21T09:08:33.850563Z","shell.execute_reply":"2021-05-21T09:08:34.07739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot a graph of the number of elements in each class in the test data\n\nsns.countplot(x=test_label)\nplt.title('The number of elements in each class in the test data', fontdict={'fontname': 'Comic Sans MS', 'fontsize': 15})\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:08:54.907724Z","iopub.execute_input":"2021-05-21T09:08:54.908053Z","iopub.status.idle":"2021-05-21T09:08:55.08243Z","shell.execute_reply.started":"2021-05-21T09:08:54.908026Z","shell.execute_reply":"2021-05-21T09:08:55.081467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert categorical variable (in our case: -1, 0, 1) into dummy/indicator variables. Such as -1 to 1 0 0 \ntrain_label = pd.get_dummies(train_label)\ntest_label = pd.get_dummies(test_label)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:09:09.717913Z","iopub.execute_input":"2021-05-21T09:09:09.718223Z","iopub.status.idle":"2021-05-21T09:09:09.725391Z","shell.execute_reply.started":"2021-05-21T09:09:09.718197Z","shell.execute_reply":"2021-05-21T09:09:09.724552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_label","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:48:59.253494Z","iopub.execute_input":"2021-05-21T09:48:59.253828Z","iopub.status.idle":"2021-05-21T09:48:59.267371Z","shell.execute_reply.started":"2021-05-21T09:48:59.253801Z","shell.execute_reply":"2021-05-21T09:48:59.266591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.3 Lemmatization","metadata":{}},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\n\n# Function to convert nltk tag to wordnet tag\ndef nltk_tag_to_wordnet_tag(nltk_tag):\n    if nltk_tag.startswith('J'):\n        return wordnet.ADJ\n    elif nltk_tag.startswith('V'):\n        return wordnet.VERB\n    elif nltk_tag.startswith('N'):\n        return wordnet.NOUN\n    elif nltk_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return None\n\n# Function below is necessery to find lemma of each word\ndef lemmatize_sentence(de_punct_sent):\n    # Tokenize the sentence and find the POS tag for each token\n    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(de_punct_sent))\n    # Tuple of (token, wordnet_tag)\n    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n    lemmatized_sentence = []\n    for word, tag in wordnet_tagged:\n        if tag is None:\n            # If there is no available tag, append the token as is\n            lemmatized_sentence.append(word)\n        else:\n            # else use the tag to lemmatize the token\n            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n    return \" \".join(lemmatized_sentence)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:09:39.653224Z","iopub.execute_input":"2021-05-21T09:09:39.653553Z","iopub.status.idle":"2021-05-21T09:09:39.661012Z","shell.execute_reply.started":"2021-05-21T09:09:39.65352Z","shell.execute_reply":"2021-05-21T09:09:39.65998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lemmatization of train and test data\ntrain_lem = [lemmatize_sentence(i) for i in train_data]\ntest_lem = [lemmatize_sentence(i) for i in test_data]","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:09:52.062608Z","iopub.execute_input":"2021-05-21T09:09:52.062975Z","iopub.status.idle":"2021-05-21T09:10:48.04973Z","shell.execute_reply.started":"2021-05-21T09:09:52.062944Z","shell.execute_reply":"2021-05-21T09:10:48.048796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.4 Tokenization","metadata":{}},{"cell_type":"code","source":"samples_1 = train_lem\nsamples_2 = test_lem\n\ntokenizer = Tokenizer(num_words=25000) # 25 000 most frequently used words\ntokenizer.fit_on_texts(samples_1) # Creat an index of all words from training data\n\n# Convert strings to integer index lists\n\n# After that, we pass the ready-made (from the line above) indexes for all words from train\ntrain_data = tokenizer.texts_to_sequences(samples_1)\n\n# And for test. So, we should get the same word indexes in the two texts\ntest_data = tokenizer.texts_to_sequences(samples_2)\n\nword_index = tokenizer.word_index # Find out the calculated index of words\nprint('Found %s unique tokens.' % len(word_index))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:11:14.583805Z","iopub.execute_input":"2021-05-21T09:11:14.584113Z","iopub.status.idle":"2021-05-21T09:11:15.987849Z","shell.execute_reply.started":"2021-05-21T09:11:14.584089Z","shell.execute_reply":"2021-05-21T09:11:15.986996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### The preprocessing stage is done. Now we have the training, test data and their respective labels:\n    \n# -train_data\n# -test_data\n\n# -train_label\n# -test_label","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:11:21.622532Z","iopub.execute_input":"2021-05-21T09:11:21.622961Z","iopub.status.idle":"2021-05-21T09:11:21.627773Z","shell.execute_reply.started":"2021-05-21T09:11:21.622914Z","shell.execute_reply":"2021-05-21T09:11:21.626656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we want to find out what is the maximum number of words in a sentence. \n# After that, we will fill in the sentences with the missing number of words.\n\nlens =  [len(s) for s in train_data]\n\nplt.title('The maximum number of words in a sentence', fontdict={'fontname': 'Comic Sans MS', 'fontsize': 17})\nplt.xlabel('The max number of words in a sentence', fontdict={'fontname': 'Comic Sans MS', 'fontsize': 15})\nplt.ylabel('The number of sentences', fontdict={'fontname': 'Comic Sans MS', 'fontsize': 15})\nplt.hist(lens,bins=200)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:11:36.249805Z","iopub.execute_input":"2021-05-21T09:11:36.250135Z","iopub.status.idle":"2021-05-21T09:11:36.881026Z","shell.execute_reply.started":"2021-05-21T09:11:36.250107Z","shell.execute_reply":"2021-05-21T09:11:36.879954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maxlen = 40 # maximal length of sentences\n\n# Convert lists of integers to a two-dimensional tensor \n# with integers and with a shape (samples, max. length)\n\ntrain_data = preprocessing.sequence.pad_sequences(train_data, maxlen=maxlen, padding='post', truncating='post') \ntest_data = preprocessing.sequence.pad_sequences(test_data, maxlen=maxlen, padding='post', truncating='post')\n","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:11:48.394891Z","iopub.execute_input":"2021-05-21T09:11:48.3952Z","iopub.status.idle":"2021-05-21T09:11:48.598141Z","shell.execute_reply.started":"2021-05-21T09:11:48.395173Z","shell.execute_reply":"2021-05-21T09:11:48.597071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define some parameters for the model\nembedding_dim = 32 # Dimension of the dense embedding.\n\nvocab_inp_size = len(word_index) + 1 # Size of the vocabulary\n\nhidden_size = 256","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:12:46.334964Z","iopub.execute_input":"2021-05-21T09:12:46.33538Z","iopub.status.idle":"2021-05-21T09:12:46.339892Z","shell.execute_reply.started":"2021-05-21T09:12:46.335341Z","shell.execute_reply":"2021-05-21T09:12:46.339056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 Building Classification Model","metadata":{}},{"cell_type":"code","source":"model = Sequential() \n\n# The Embedding layer is necessery in order to convert words (in out case from integers) to vectors\nmodel.add(Embedding(input_dim = vocab_inp_size, output_dim = embedding_dim, input_length = maxlen))\n\n# Apply Dropout to use recurrent decimation to combat overfitting \nmodel.add(LSTM(hidden_size, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)) \n\n# Use Flatten() or GlobalMaxPooling1D() to convert 3D output to 2D.\n# It allows you to add one or more Dense layers to your model \nmodel.add(Flatten()) # or model.add(GlobalMaxPooling1D())\n\n#model.add(Dense(64, activation='relu'))#as experiment we tried to add one additional layer,but it is not give as higher accuracy\n#model.add(Dropout(0.4))\n\nmodel.add(Dense(3, activation='softmax'))#activation function for classification\n\nmodel.summary()\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['categorical_accuracy']) \n\nhistory = model.fit(train_data, train_label, epochs=10, batch_size=128, validation_split=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:15:19.948356Z","iopub.execute_input":"2021-05-21T09:15:19.948686Z","iopub.status.idle":"2021-05-21T09:26:13.509913Z","shell.execute_reply.started":"2021-05-21T09:15:19.948658Z","shell.execute_reply":"2021-05-21T09:26:13.509093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.1 Visualization of model training","metadata":{}},{"cell_type":"code","source":"acc = history.history['categorical_accuracy']\nval_acc = history.history['val_categorical_accuracy'] \nloss = history.history['loss'] \nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc') \nplt.title('Training and validation accuracy') \nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss') \nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:27:01.061271Z","iopub.execute_input":"2021-05-21T09:27:01.061614Z","iopub.status.idle":"2021-05-21T09:27:01.33527Z","shell.execute_reply.started":"2021-05-21T09:27:01.06158Z","shell.execute_reply":"2021-05-21T09:27:01.334278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4 Model Evaluation","metadata":{}},{"cell_type":"code","source":"model.evaluate(test_data,test_label)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T09:27:31.602126Z","iopub.execute_input":"2021-05-21T09:27:31.602435Z","iopub.status.idle":"2021-05-21T09:27:35.07479Z","shell.execute_reply.started":"2021-05-21T09:27:31.602407Z","shell.execute_reply":"2021-05-21T09:27:35.073879Z"},"trusted":true},"execution_count":null,"outputs":[]}]}