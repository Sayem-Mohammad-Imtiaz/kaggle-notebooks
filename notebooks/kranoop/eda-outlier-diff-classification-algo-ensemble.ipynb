{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization library  \nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import time\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Handling the Missing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(data.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The last column Unnamed: 32 has all the missing value it have to be removed.\nAlso the id column need to be remove since its just an ID which wont contribute anything for our prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.drop(\"Unnamed: 32\", axis =1)\ndata=data.drop(\"id\", axis =1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"frequency of cancer stages"},{"metadata":{},"cell_type":"markdown","source":"63% Benign cases compared to 37% Malignant cases, potentially indicating higher number of Benign"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data['diagnosis'],label=\"Count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"diagnosis\"].replace(\"M\",0,inplace = True)\ndata[\"diagnosis\"].replace(\"B\",1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"change value of diognose M = 0 and B = 1"},{"metadata":{},"cell_type":"markdown","source":"data[\"diagnosis\"].replace(\"M\",0,inplace = True)\ndata[\"diagnosis\"].replace(\"B\",1,inplace = True)"},{"metadata":{},"cell_type":"markdown","source":"**Discover outliers with Box plot**\n\nOutlier it will plotted as point in boxplot but other population will be grouped together and display as boxes."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.boxplot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As identified there outliers present in the data, now let us check the data for each column for outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.boxplot(column=(['radius_mean','texture_mean','perimeter_mean']),vert=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"in the box plot we see the outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.boxplot(column=(['compactness_mean','concavity_mean','concave points_mean']),vert=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.boxplot(column=(['area_mean']),vert=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Working with Outliers: Correcting, Removing**\n\nOutliers and should be dropped or correct , as they cause issues when you model your data.\n\nWe are going to use Interquartile Range Method for removing the outliers in the data.\nThe IQR is calculated as the difference between the 75th and the 25th percentiles of the data . We can then calculate the cutoff for outliers as 1.5 times the IQR and subtract this cut-off from the 25th percentile and add it to the 75th percentile to give the actual limits on the data.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"q25, q75 = np.percentile(data['radius_mean'], 25), np.percentile(data['radius_mean'], 75)\niqr = q75 - q25\ncut_off = iqr * 1.5\nlower, upper = q25 - cut_off, q75 + cut_off\n\noutliers = [x for x in data['radius_mean'] if x < lower or x > upper]\nlen(outliers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the radius_mean column we have 14 outliers identified which need to be removed."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['radius_mean'] = np.where(data['radius_mean']< lower, np.NaN, data['radius_mean'])\ndata['radius_mean'] = np.where(data['radius_mean']> upper, np.NaN, data['radius_mean'])\ndata['radius_mean'].isna().sum()\ndata['radius_mean'].replace(to_replace =np.NaN, \n                 value =data['radius_mean'].median(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the identified outliers are initially replaced with NaN which is null.\nonce the outliers are replaced with NaN , calculatd the Median of the column and replaced with outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Nomally above statement is coded as below, but if we do so it will identify the medean of data which \n# includes the outliers, which will be wrong hence we need to replace the outliers with 0 or NaN then idenfy the median\n\n# outliers = [x for x in data['radius_mean'] if x < lower or x > upper]\n# data['radius_mean'].replace(to_replace =[x for x in data['radius_mean'] if x > lower and x < upper], \n#                 value =data['radius_mean'].median(),inplace=True)\n\n#Same need to be performed for all the columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in ['texture_mean','perimeter_mean','area_mean','compactness_mean','concavity_mean','concave points_mean','symmetry_mean','fractal_dimension_mean','radius_se','texture_se','perimeter_se','area_se','smoothness_se','compactness_se','concavity_se','concave points_se','symmetry_se','fractal_dimension_se','radius_worst','texture_worst','perimeter_worst','area_worst','smoothness_worst','compactness_worst','concavity_worst','concave points_worst','symmetry_worst','fractal_dimension_worst']:\n    q25, q75 = np.percentile(data[column], 25), np.percentile(data[column], 75)\n    iqr = q75 - q25\n    cut_off = iqr * 1.5\n    lower, upper = q25 - cut_off, q75 + cut_off\n    data[column].replace(to_replace =[x for x in data[column] if x > lower and x < upper], \n                 value =data[column].median(),inplace=True)\nprint( 'Outliers are replaced with Median')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now all the outliers are removed with median"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once the outliers are removed we can look at the correlation between the columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data.corr()\nplt.figure(figsize=(18,18))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 10},\n           cmap= 'coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AS per the above heatmap we see highly correlated values to be removed, hence either of the one should be removed. Now below are the variables which will use for prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_backup=data\ndata=data.drop([\"radius_se\"], axis =1)\ndata.head()\nprediction_var = list(['texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se',  'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se' ,'radius_worst', 'texture_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst'])\n#prediction_var ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us see the correlation metrics once again after removing the columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data.corr()\nplt.figure(figsize=(18,18))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 10},\n           cmap= 'coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the data looks good !"},{"metadata":{},"cell_type":"markdown","source":"Split data into training and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#convert the datset into Test and Train data\nX_train, X_test, Y_train, Y_test = train_test_split(data.drop('diagnosis', axis=1), data['diagnosis'],\\\n                                                    test_size=0.2, random_state=156)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let us execute different models and obtain the results"},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlgr = LogisticRegression(max_iter = 200)\nlgr.fit(X_train,Y_train)\nypred=lgr.predict(X_test)\nypred_Log=ypred\nprint('Accuracy score - ' ,lgr.score(X_test,Y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nRFmodel=RandomForestClassifier()\nresult=RFmodel.fit(X_train, Y_train)\nypred=result.predict(X_test)\nypred_RandomForest=ypred\nprint('Accuracy score - ' ,accuracy_score(Y_test,ypred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gradient Boosting**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier() \nresult=gb.fit(X_train, Y_train)\nypred=result.predict(X_test)\n\nprint('Accuracy score - ' ,accuracy_score(Y_test,ypred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gradient Boosting XGBoost**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nXGB = XGBClassifier() \nresult=XGB.fit(X_train, Y_train)\nypred=result.predict(X_test)\n\nprint('Accuracy score - ' ,accuracy_score(Y_test,ypred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Naive Bayes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nNB = GaussianNB() \nresult=NB.fit(X_train, Y_train)\nypred=result.predict(X_test)\n\nprint('Accuracy score - ' ,accuracy_score(Y_test,ypred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Ensemble**\n\nNow let us do an Ensemble to get the bet model\nAdding back all the predictions got from the diffrent models executed above."},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_diagnosis=lgr.predict(data.drop('diagnosis',axis=1))\nrf_diagnosis=RFmodel.predict(data.drop('diagnosis',axis=1))\ngb_diagnosis=gb.predict(data.drop('diagnosis',axis=1))\nxgb_diagnosis=XGB.predict(data.drop('diagnosis',axis=1))\nnb_diagnosis=NB.predict(data.drop('diagnosis',axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['lr_diagnosis']=lr_diagnosis\ndata['rf_diagnosis']=rf_diagnosis\ndata['gb_diagnosis']=gb_diagnosis\ndata['xgb_diagnosis']=xgb_diagnosis\ndata['nb_diagnosis']=nb_diagnosis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Train2, X_Test2, Y_Train2, Y_Test2 = train_test_split(data.drop(\"diagnosis\",axis=1),data[\"diagnosis\"],test_size=0.25,random_state=123)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let us run  Gradient Boosting with all updated features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier() \nresult=gb.fit(X_Train2, Y_Train2)\nypred2=result.predict(X_Test2)\naccuracy_score(Y_Test2,ypred2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix( Y_Test2 ,gb.predict(X_Test2))\n\nf,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm,annot = True,linewidth = 1,fmt =\".0f\",ax = ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**\n\nWe have executed 5 models on the data and none of the models were giving above 90% accuracy. Later I have added all 5 predictions back to actual data to improve the final predictions as features. After adding new features again(which is called as Ensamble) I have executed the Gradient boosting method on the updated data and which gave the 97% of accuracy."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}