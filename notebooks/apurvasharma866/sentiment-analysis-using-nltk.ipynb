{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Problem Description\n# Features present :\n* Clothing ID: Integer Categorical variable that refers to the specific piece being reviewed.\n* Age: Positive Integer variable of the reviewers age.\n* Title: String variable for the title of the review.\n* Review Text: String variable for the review body.\n* Rating: Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.\n* Positive Feedback Count: Positive Integer documenting the number of other customers who found this review positive.\n* Division Name: Categorical name of the product high level division.\n* Department Name: Categorical name of the product department name.\n* Class Name: Categorical name of the product class name.\n\n# Target :\n\n* Recommended IND: Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Importing Modules","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\nimport random\nimport os\nfrom os import path\nfrom PIL import Image\n\n# Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Set Plot Theme\nsns.set_palette([\n    \"#30a2da\",\n    \"#fc4f30\",\n    \"#e5ae38\",\n    \"#6d904f\",\n    \"#8b8b8b\",\n])\n# Alternate # plt.style.use('fivethirtyeight')\n\n# Pre-Processing\nimport string\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nimport re\nfrom nltk.stem import PorterStemmer\n\n# Modeling\nimport statsmodels.api as sm\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.sentiment.util import *\nfrom nltk.util import ngrams\nfrom collections import Counter\nfrom gensim.models import word2vec\n\n# Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NLTK\n\nNLTK stands for Natural Language Toolkit. This toolkit is one of the most powerful NLP libraries which contains packages to make machines understand human language and reply to it with an appropriate response. Tokenization, Stemming, Lemmatization, Punctuation, Character count.\n\nUsing nltk we can determine the mood of text, NLTK’s N-grams, and gensim.models’s word2vec. It also includes statsmodels.api which offers an array of linear models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://www.guru99.com/images/1/122118_0534_NLPNaturalL3.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Importing the Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read and Peak at Data\ndf = pd.read_csv(\"../input/womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv\",index_col =[0])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the number of unique values for each column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Number of unique values\ndf.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that almost 3000 title data is missing. But since we will not use this feature we won't trim the data further","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T.drop(\"count\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking age distribution and positive feedback","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Continous Distributions\nf, ax = plt.subplots(1,3,figsize=(16,8), sharey=False)\nsns.distplot(df.Age, ax=ax[0])\nax[0].set_title(\"Age Distribution\")\nax[0].set_ylabel(\"Density\")\nsns.distplot(df[\"Positive Feedback Count\"], ax=ax[1])\nax[1].set_title(\"Positive Feedback Count Distribution\")\nsns.distplot(np.log10((df[\"Positive Feedback Count\"][df[\"Positive Feedback Count\"].notnull()]+1)), ax=ax[2])\nax[2].set_title(\"Positive Feedback Count Distribution\\n[Log 10]\")\nax[2].set_xlabel(\"Log Positive Feedback Count\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the people are between 35 - 50 years.Trend suggest that the core market segment for this clothing brand is women between 34 and 50. With its single peak and slight right tail, the distribution of age is more or less normal.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Imputing the Data\n* Dropping Clothing ID and Title\n* Checking Review Text columns\n* Dropping columns which don't have any review","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(labels =['Clothing ID','Title'],axis = 1,inplace = True)#Dropping unwanted columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['Review Text'].isnull()].shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df[~df['Review Text'].isnull()]  #Dropping columns which don't have any review\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualising Data\n* Checking Rating vs Recommended IND column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\n\npx.histogram(data, x = data['Rating'], color = data[\"Recommended IND\"])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualising Class name counts ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(data, x = data['Class Name'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.scatter(data, x=\"Age\", y=\"Positive Feedback Count\", facet_row=\"Recommended IND\", facet_col=\"Rating\",trendline=\"ols\",category_orders={\"Rating\": [1,2,3,4,5],'Recommended IND':[0,1]})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting Divisions and Age with Recommended as hue","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"px.box(data, x=\"Age\", y=\"Division Name\", orientation=\"h\",color = 'Recommended IND')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualising Polarity of Review Texts","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"err1 = data['Review Text'].str.extractall(\"(&amp)\")\nerr2 = data['Review Text'].str.extractall(\"(\\xa0)\")\nprint('with &amp',len(err1[~err1.isna()]))\nprint('with (\\xa0)',len(err2[~err2.isna()]))\n\ndata['Review Text'] = data['Review Text'].str.replace('(&amp)','')\ndata['Review Text'] = data['Review Text'].str.replace('(\\xa0)','')\n\nerr1 = data['Review Text'].str.extractall(\"(&amp)\")\nprint('with &amp',len(err1[~err1.isna()]))\nerr2 = data['Review Text'].str.extractall(\"(\\xa0)\")\nprint('with (\\xa0)',len(err2[~err2.isna()]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install TextBlob\nfrom textblob import *\n\ndata['polarity'] = data['Review Text'].map(lambda text: TextBlob(text).sentiment.polarity)\ndata['polarity']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(data, x = 'polarity',color=\"Rating\", opacity = 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.box(data, y=\"polarity\", x=\"Department Name\", orientation=\"v\",color = 'Recommended IND')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['review_len'] = data['Review Text'].astype(str).apply(len)\npx.histogram(data, x = 'review_len' ,color = \"Recommended IND\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['token_count'] = data['Review Text'].apply(lambda x: len(str(x).split()))\npx.histogram(data, x = 'token_count',color = \"Recommended IND\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reviews with Positive Polarity ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sam = data.loc[data.polarity == 1,['Review Text']].sample(3).values\nfor i in sam:\n    print(i[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reviews with Neutral Polarity ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sam = data.loc[data.polarity == 0.5,['Review Text']].sample(3).values\nfor i in sam:\n    print(i[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reviews with negative Polarity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sam = data.loc[data.polarity < 0,['Review Text']].sample(3).values\nfor i in sam:\n    print(i[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualising the polarity of Reviews using a pie chart","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"negative = (len(data.loc[data.polarity <0,['Review Text']].values)/len(data))*100\npositive = (len(data.loc[data.polarity >0.5,['Review Text']].values)/len(data))*100\nneutral  = len(data.loc[data.polarity >0 ,['Review Text']].values) - len(data.loc[data.polarity >0.5 ,['Review Text']].values)\nneutral = neutral/len(data)*100\n\nfrom matplotlib import pyplot as plt \nplt.figure(figsize =(10, 7)) \nplt.pie([positive,negative,neutral], labels = ['Positive','Negative','Neutral'] , colors = [\"green\" ,\"red\" ,\"mediumslateblue\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating N grams \n\nN-grams of texts are extensively used in *text mining* and *natural language processing* tasks. They are basically a set of co-occuring words within a given window and when computing the n-grams you typically move one word forward (although you can move X words forward in more advanced scenarios). ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ndef top_n_ngram(corpus,n = None,ngram = 1):\n    vec = CountVectorizer(stop_words = 'english',ngram_range=(ngram,ngram)).fit(corpus)\n    bag_of_words = vec.transform(corpus) #Have the count of  all the words for each review\n    sum_words = bag_of_words.sum(axis =0) #Calculates the count of all the word in the whole review\n    words_freq = [(word,sum_words[0,idx]) for word,idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq,key = lambda x:x[1],reverse = True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"   # Visualizing Top 10 Unigrams","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"common_words = top_n_ngram(data['Review Text'], 10,1)\ndf = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\nplt.figure(figsize =(10,5))\ndf.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\nkind='bar', title='Top 10 unigrams in review after removing stop words')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing Top 20 Bigrams","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"common_words = top_n_ngram(data['Review Text'], 20,2)\ndf = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\nplt.figure(figsize =(10,5))\ndf.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\nkind='bar', title='Top 20 bigrams in review after removing stop words')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing Top 20 Part-of-Speech\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"blob= TextBlob(str(data['Review Text']))\npos = pd.DataFrame(blob.tags,columns =['word','pos'])\npos1 = pos.pos.value_counts()[:20]\nplt.figure(figsize = (10,5))\npos1.plot(kind='bar',title ='Top 20 Part-of-speech taggings')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining our data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data['Recommended IND']\nX = data.drop(columns = 'Recommended IND')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualising Heatmaps","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.heatmap(X.corr(),annot =True, cmap = \"icefire\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling Multi - Colinearity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"set1 =set()\ncor = X.corr()\nfor i in cor.columns:\n    for j in cor.columns:\n        if cor[i][j]>0.8 and i!=j:\n            set1.add(i)\nprint(set1)\nX = X.drop(labels = ['token_count'],axis = 1)\nprint(\"correlation: \", X.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Statistical Description of Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class1 =[]\nfor i in X.polarity:\n    if float(i)>=0.0:\n        class1.append(1)\n    elif float(i)<0.0:\n        class1.append(0)\nX['sentiment'] = class1\n\nX.groupby(X['sentiment']).describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of X: \" , X.shape)\nprint(\"Shape of y: \" , y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODEL I","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Creating the model\n\n* Stemming is the process of producing morphological variants of a root/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers using PorterStemmer from NLTK\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport re\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\n\ncorpus =[]\nX.index = np.arange(len(X))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RE -> Tokenizing -> Stemming -> Corpus Creation ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nfor i in tqdm(range(len(X))):\n    review = re.sub('[^a-zA-z]',' ',X['Review Text'][i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review =[ps.stem(i) for i in review if not i in set(stopwords.words('english'))]\n    review =' '.join(review)\n    corpus.append(review)\ncorpus[0:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bag of words\nTo create the bag of words model, we need to create a matrix where the columns correspond to the most frequent words in our dictionary where rows correspond to the document or sentences.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://i.pinimg.com/originals/dd/80/9c/dd809cd103e3f94c252b6073c474bcac.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(corpus)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Count Vector","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer as CV\ncv  = CV(max_features = 3000,ngram_range=(1,1))\nX_cv = cv.fit_transform(corpus).toarray()\ny = y.values\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_cv, y, test_size = 0.20, random_state = 0)\nfrom sklearn.naive_bayes import BernoulliNB\nclassifier = BernoulliNB()\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics\nacc = accuracy_score(y_test, y_pred)\nprint(\"Accuracy of the classifier: \",acc)\nprint(\"Confusion matrix is :\\n\",metrics.confusion_matrix(y_test,y_pred))\nprint(\"Classification report: \\n\" ,metrics.classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Term frequency- Inverse Document Frequency Technique (tf-idf)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer as TV\ntv  = TV(ngram_range =(1,1),max_features = 3000)\nX_tv = tv.fit_transform(corpus).toarray()\nX_train, X_test, y_train, y_test = train_test_split(X_tv, y, test_size = 0.20, random_state = 0)\nfrom sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprint(\"Accuracy of the classifier: \",acc)\nprint(\"Confusion matrix is :\\n\",metrics.confusion_matrix(y_test,y_pred))\nprint(\"Classification report: \\n\" ,metrics.classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating a ANN structure","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\ntokenizer = Tokenizer(num_words = 3000)\ntokenizer.fit_on_texts(corpus)\nsequences = tokenizer.texts_to_sequences(corpus)\npadded = pad_sequences(sequences, padding='post')\nword_index = tokenizer.word_index\ncount = 0\nfor i,j in word_index.items():\n    if count == 11:\n        break\n    print(i,j)\n    count = count+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 64\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(3000, embedding_dim),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(6, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 10\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.fit(padded,y,epochs= num_epochs,validation_split= 0.39)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualising Model history","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = model.history.history\nloss = pd.DataFrame(loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('Basic ANN Performance', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = range(1,11)\nax1.plot(epoch_list, loss['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, loss['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 11, 1))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, loss['loss'], label='Train Loss')\nax2.plot(epoch_list, loss['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 11, 1))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking the model on a random example\n* Check if the review will be recommended or not.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_string = \"I hate it\"\nsample = tokenizer.texts_to_sequences(sample_string)\npadded_sample = pad_sequences(sample, padding='post')\nprint(\"Padded sample\", padded_sample.T)\nprint(\"Probabilty of a person recommending :\",model.predict(padded_sample.T)[0][0]*100,\"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hence, the model predicts that for a comment \"I hate it\" there is only 24.61% chance of recommending it to someone","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_string = \"i love the fabric\"\nsample = tokenizer.texts_to_sequences(sample_string)\npadded_sample = pad_sequences(sample, padding='post')\nprint(\"Padded sample\", padded_sample.T)\nprint(\"Probabilty of a person recommending :\",model.predict(padded_sample.T)[0][0]*100,\"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hence, the model predicts that for a comment \"I love the fabric\" there is 99.98 % chance of recommending it to someone","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Model II","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Creating a model to check Recommended","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Creating X and y for training and test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data['Recommended IND'].tolist()\nX = list(data[\"Review Text\"])\n\n# Separate out the sentences and labels into training and test sets\ntraining_size = int(len(X) * 0.8)\n\ntraining_sentences = X[0:training_size]\ntesting_sentences = X[training_size:]\ntraining_labels = y[0:training_size]\ntesting_labels = y[training_size:]\n\n# Make labels into numpy arrays for use with the network later\ntraining_labels_final = np.array(training_labels)\ntesting_labels_final = np.array(testing_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Corpus","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = 1000\nembedding_dim = 16\nmax_length = 100\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\n\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(training_sentences)\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(training_sentences)\npadded = pad_sequences(sequences,maxlen=max_length, padding=padding_type, \n                       truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences,maxlen=max_length, \n                               padding=padding_type, truncating=trunc_type)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_review(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n\nprint(decode_review(padded[1]))\nprint(training_sentences[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build a basic sentiment network\n# Note the embedding layer is first, \n# and the output is only 1 node as it is either 0 or 1 (negative or positive)\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(6, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 10\nmodel.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = model.history.history\nloss = pd.DataFrame(loss)\nloss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('Basic ANN Performance', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = range(1,11)\nax1.plot(epoch_list, loss['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, loss['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 11, 1))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, loss['loss'], label='Train Loss')\nax2.plot(epoch_list, loss['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 11, 1))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}