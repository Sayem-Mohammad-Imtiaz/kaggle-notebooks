{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# STAT8017 Data mining techniques â€“ Group project\n# Data Analysis of Cardiovascular Disease Dataset\n#------------------------------------------------------------------------------------------\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn\nfrom itertools import product\n\n# data transformation & splitting\nfrom sklearn.preprocessing import RobustScaler, label_binarize\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n# decision tree, logistic\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nimport graphviz\nfrom sklearn.linear_model import LogisticRegressionCV\n\n# clustering analysis\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\nimport scipy.cluster.hierarchy as sch\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import jaccard_score, adjusted_rand_score, silhouette_score, calinski_harabasz_score, roc_curve, auc, accuracy_score, classification_report, confusion_matrix\nfrom sklearn.metrics.cluster import contingency_matrix\n\n# ensemble methods, MLP\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.neural_network import MLPClassifier\n#------------------------------------------------------------------------------------------   \n# Input data files are available in the read-only \"../input/\" directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:04:24.412683Z","iopub.execute_input":"2021-05-30T13:04:24.413227Z","iopub.status.idle":"2021-05-30T13:04:26.162362Z","shell.execute_reply.started":"2021-05-30T13:04:24.413105Z","shell.execute_reply":"2021-05-30T13:04:26.160799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read data\ndf = pd.read_csv('/kaggle/input/cardiovascular-disease-dataset/cardio_train.csv', sep = ';', index_col = 'id')\n\n# preview\npd.options.display.float_format = '{:,.2f}'.format\ndisplay(df.head())","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-05-30T13:04:57.698124Z","iopub.execute_input":"2021-05-30T13:04:57.698779Z","iopub.status.idle":"2021-05-30T13:04:57.876681Z","shell.execute_reply.started":"2021-05-30T13:04:57.698721Z","shell.execute_reply":"2021-05-30T13:04:57.875889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"### Check missing values:","metadata":{"editable":false}},{"cell_type":"code","source":"# check blank rows\ndf.isnull().sum() ","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:04:58.318197Z","iopub.execute_input":"2021-05-30T13:04:58.318572Z","iopub.status.idle":"2021-05-30T13:04:58.330324Z","shell.execute_reply.started":"2021-05-30T13:04:58.318542Z","shell.execute_reply":"2021-05-30T13:04:58.329002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**None of the variables have missing values.**","metadata":{"editable":false}},{"cell_type":"markdown","source":"### Examining the variables:","metadata":{"editable":false}},{"cell_type":"code","source":"# before data cleaning\ndisplay(df.describe())","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-05-30T13:04:58.945864Z","iopub.execute_input":"2021-05-30T13:04:58.946539Z","iopub.status.idle":"2021-05-30T13:04:59.023044Z","shell.execute_reply.started":"2021-05-30T13:04:58.94649Z","shell.execute_reply":"2021-05-30T13:04:59.021618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Age : Converting to years for ease of understanding.**","metadata":{"editable":false}},{"cell_type":"code","source":"# convert age from days to years\ndf['age'] = df['age']/365","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:04:59.39003Z","iopub.execute_input":"2021-05-30T13:04:59.390477Z","iopub.status.idle":"2021-05-30T13:04:59.42537Z","shell.execute_reply.started":"2021-05-30T13:04:59.39044Z","shell.execute_reply":"2021-05-30T13:04:59.42373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Gender: Converting female to 0 and male to 1**","metadata":{"editable":false}},{"cell_type":"code","source":"# convert gender to 0=female and 1=male\ndf['gender'] = df['gender'] - 1","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:04:59.769942Z","iopub.execute_input":"2021-05-30T13:04:59.770365Z","iopub.status.idle":"2021-05-30T13:04:59.777622Z","shell.execute_reply.started":"2021-05-30T13:04:59.770329Z","shell.execute_reply":"2021-05-30T13:04:59.776469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Height and Weight : Using BMI as an indicator to remove records that do not make sense.**","metadata":{}},{"cell_type":"code","source":"# calculate BMI\ndf['BMI'] = df['weight']/(np.power(df['height']/100, 2))\n\n# BMI Distribution\nprint(df['BMI'].describe())\nseaborn.histplot(data = df, x = 'BMI', bins = 100)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:05:00.153778Z","iopub.execute_input":"2021-05-30T13:05:00.154224Z","iopub.status.idle":"2021-05-30T13:05:00.582577Z","shell.execute_reply.started":"2021-05-30T13:05:00.15418Z","shell.execute_reply":"2021-05-30T13:05:00.581452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove BMI > 150\ndrop_criteria_bmi = df[df['BMI'] > 150].index\n\n# number of records to be removed\nprint(drop_criteria_bmi.size)\n\n# remove records\ndf.drop(drop_criteria_bmi, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:05:00.584487Z","iopub.execute_input":"2021-05-30T13:05:00.584825Z","iopub.status.idle":"2021-05-30T13:05:00.60819Z","shell.execute_reply.started":"2021-05-30T13:05:00.584791Z","shell.execute_reply":"2021-05-30T13:05:00.606908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**AP_HI and AP_LO : Blood pressure should always be positive, not exceeding a certain threshold (300). AP_HI > AP_LO checking should be enforced.**","metadata":{}},{"cell_type":"code","source":"# ap_hi is higher than 250 or lower than 60\ndrop_criteria_aphi = df[(df['ap_hi'] > 210) | (df['ap_hi'] < 60)].index\n\n# ap_lo is higher than 200 or lower than 10\ndrop_criteria_aplo = df[(df['ap_lo'] > 140) | (df['ap_lo'] < 30)].index\n\n# ap_lo is higher than 'ap_hi\ndrop_criteria_ap = df[df['ap_lo'] > df['ap_hi']].index \n\n# number of records to be removed\ndrop_criteria = drop_criteria_aphi.union(drop_criteria_aplo)\ndrop_criteria.union(drop_criteria_ap)\nprint(drop_criteria.size)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-05-30T13:05:00.789827Z","iopub.execute_input":"2021-05-30T13:05:00.790257Z","iopub.status.idle":"2021-05-30T13:05:00.807368Z","shell.execute_reply.started":"2021-05-30T13:05:00.79022Z","shell.execute_reply":"2021-05-30T13:05:00.806538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove the records\ndf.drop(drop_criteria, inplace = True)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-05-30T13:05:01.01736Z","iopub.execute_input":"2021-05-30T13:05:01.017729Z","iopub.status.idle":"2021-05-30T13:05:01.032382Z","shell.execute_reply.started":"2021-05-30T13:05:01.0177Z","shell.execute_reply":"2021-05-30T13:05:01.031412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data cleaning result:","metadata":{}},{"cell_type":"code","source":"# after data cleaning\ndisplay(df.describe())\n\n# distribution of response variable\ndisplay(pd.DataFrame(df['cardio'].value_counts()))","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-05-30T13:05:01.461109Z","iopub.execute_input":"2021-05-30T13:05:01.461758Z","iopub.status.idle":"2021-05-30T13:05:01.547613Z","shell.execute_reply.started":"2021-05-30T13:05:01.461722Z","shell.execute_reply":"2021-05-30T13:05:01.546446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The data is balanced. There is a fairly even split between individuals with the disease and without the disease.**","metadata":{}},{"cell_type":"markdown","source":"# Visualizing Variables","metadata":{}},{"cell_type":"code","source":"def pie_chart(df, col, labels):\n    data = df[col].value_counts().to_numpy()\n    def absolute_value(val):\n        a  = np.round(val / 100 * data.sum(), 0)\n        return str('%0.0f' % a) + '\\n(' + ('%0.2f' % val) + '%)'\n    plt.pie(data, labels = labels, autopct=absolute_value)\n    plt.legend(title=col)\n    plt.show() \n\npie_chart(df, 'gender', ['Female', 'Male'])\npie_chart(df, 'cardio', ['No', 'Yes'])\npie_chart(df, 'cholesterol', ['Normal', 'Above normal', 'Well above normal'])\npie_chart(df, 'gluc', ['Normal', 'Above normal', 'Well above normal'])\npie_chart(df, 'smoke', ['No', 'Yes'])\npie_chart(df, 'alco', ['No', 'Yes'])\npie_chart(df, 'active', ['No', 'Yes'])","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:05:01.956567Z","iopub.execute_input":"2021-05-30T13:05:01.956918Z","iopub.status.idle":"2021-05-30T13:05:03.374149Z","shell.execute_reply.started":"2021-05-30T13:05:01.956888Z","shell.execute_reply":"2021-05-30T13:05:03.373047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_subset = df[['age', 'height', 'weight', 'ap_hi', 'ap_lo']]\nflierprops = dict(markerfacecolor='lightblue', marker='o',markeredgecolor='lightblue') \n# <=> rs = {'markerfacecolor'='lightblue', 'marker'='o'}\nboxprops = dict(facecolor='lightblue',color = 'lightblue') # color: box line color; facecolor: fill-in color\nplt.figure(figsize=(10, 5))\nplt.boxplot(df_subset.values,labels=df_subset.columns,\n           flierprops=flierprops,boxprops=boxprops,\n            patch_artist=True)\nplt.show()\ndf_subset.boxplot()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:05:03.376334Z","iopub.execute_input":"2021-05-30T13:05:03.376689Z","iopub.status.idle":"2021-05-30T13:05:04.280205Z","shell.execute_reply.started":"2021-05-30T13:05:03.37665Z","shell.execute_reply":"2021-05-30T13:05:04.279038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import QuantileTransformer\nquantile_transformer = QuantileTransformer(random_state=0)\nX_trans = quantile_transformer.fit_transform(df_subset)\npd.DataFrame(X_trans, columns=df_subset.columns).hist(bins = 5)\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:20:04.281653Z","iopub.execute_input":"2021-05-30T13:20:04.282073Z","iopub.status.idle":"2021-05-30T13:20:05.072365Z","shell.execute_reply.started":"2021-05-30T13:20:04.28204Z","shell.execute_reply":"2021-05-30T13:20:05.070695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flierprops = dict(markerfacecolor='lightblue', marker='o',markeredgecolor='lightblue') \n# <=> rs = {'markerfacecolor'='lightblue', 'marker'='o'}\nboxprops = dict(facecolor='lightblue',color = 'lightblue') # color: box line color; facecolor: fill-in color\nplt.figure(figsize=(10, 5))\nplt.boxplot(df_subset.values,labels=df_subset.columns,\n           flierprops=flierprops,boxprops=boxprops,\n            patch_artist=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:05:06.402266Z","iopub.execute_input":"2021-05-30T13:05:06.402702Z","iopub.status.idle":"2021-05-30T13:05:06.608702Z","shell.execute_reply.started":"2021-05-30T13:05:06.40267Z","shell.execute_reply":"2021-05-30T13:05:06.607807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# pair-plot\nseaborn.pairplot(df, vars = ['age', 'height', 'weight', 'ap_hi', 'ap_lo'], hue = 'cardio')\nplt.show()","metadata":{"editable":false,"execution":{"iopub.status.idle":"2021-05-30T13:06:21.309921Z","shell.execute_reply.started":"2021-05-30T13:05:06.610267Z","shell.execute_reply":"2021-05-30T13:06:21.308958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pairplots by gender","metadata":{}},{"cell_type":"code","source":"# gender pair-plot\nseaborn.pairplot(df[df.gender == 0], vars = ['age', 'height', 'weight', 'ap_hi', 'ap_lo'], hue = 'cardio')\nplt.show()\nseaborn.pairplot(df[df.gender == 1], vars = ['age', 'height', 'weight', 'ap_hi', 'ap_lo'], hue = 'cardio')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation heatmap","metadata":{}},{"cell_type":"code","source":"# correlation heatmap\nplt.figure(figsize=(16, 8))\nseaborn.heatmap(df.corr(), annot=True, fmt='.3f')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-05-30T08:45:55.864626Z","iopub.execute_input":"2021-05-30T08:45:55.865071Z","iopub.status.idle":"2021-05-30T08:45:56.637308Z","shell.execute_reply.started":"2021-05-30T08:45:55.865046Z","shell.execute_reply":"2021-05-30T08:45:56.636221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Transformation & Train-Test Splitting ","metadata":{"editable":false}},{"cell_type":"code","source":"# explanatory variables\nx = df.drop(columns = ['cardio', 'BMI'])\n\n# response variable\ny = df['cardio']","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-05-30T13:06:21.311845Z","iopub.execute_input":"2021-05-30T13:06:21.312459Z","iopub.status.idle":"2021-05-30T13:06:21.32174Z","shell.execute_reply.started":"2021-05-30T13:06:21.312413Z","shell.execute_reply":"2021-05-30T13:06:21.320311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split data\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.4, random_state = 8017)\n\n# RobustScaler ALL variables\nscaler = RobustScaler()\nscaler.fit(x_train)\nx_train = pd.DataFrame(scaler.transform(x_train), index=x_train.index, columns=x_train.columns)\nx_test = pd.DataFrame(scaler.transform(x_test), index=x_test.index, columns=x_test.columns)\n\npd.options.display.float_format = '{:,.4f}'.format\ndisplay(x_train.head(5))\ndisplay(x_train.describe())","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:06:29.433848Z","iopub.execute_input":"2021-05-30T13:06:29.434617Z","iopub.status.idle":"2021-05-30T13:06:29.573166Z","shell.execute_reply.started":"2021-05-30T13:06:29.434564Z","shell.execute_reply":"2021-05-30T13:06:29.571935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree","metadata":{"editable":false}},{"cell_type":"code","source":"%%time\n\n# parameters candidates\nparameters = {'max_depth':range(2,32)}\n\n# fitting\nDecisionTree_GSCV = GridSearchCV(DecisionTreeClassifier(random_state=8017), \n                                 parameters, n_jobs=-1, verbose=3, return_train_score=True)\nDecisionTree_GSCV.fit(x_train, y_train)\nDecisionTree_model = DecisionTree_GSCV.best_estimator_","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-05-30T13:06:31.998965Z","iopub.execute_input":"2021-05-30T13:06:31.9994Z","iopub.status.idle":"2021-05-30T13:06:43.076947Z","shell.execute_reply.started":"2021-05-30T13:06:31.999353Z","shell.execute_reply":"2021-05-30T13:06:43.075278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot training & testing scores\ntrain_dt_scores = DecisionTree_GSCV.cv_results_['mean_train_score']\ntest_dt_scores = DecisionTree_GSCV.cv_results_['mean_test_score']\n\nplt.plot(train_dt_scores, \"g.--\")\nplt.plot(test_dt_scores, \"g.-\")\nplt.ylim(0.4, 1.05)\nplt.xticks(range(30), range(2, 32))\nplt.legend([\"DT training score\", \"DT test score\"])\nplt.axvline(np.argmax(test_dt_scores), linestyle=\"dotted\", color=\"red\")\nplt.annotate(np.max(test_dt_scores).round(4), (np.argmax(test_dt_scores), np.max(test_dt_scores)), xycoords=\"data\",\n                 xytext=(50, 25), textcoords=\"offset pixels\", arrowprops=dict(facecolor=\"black\", shrink=0.1), fontsize=10,\n                 horizontalalignment=\"center\", verticalalignment=\"top\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:06:43.080345Z","iopub.execute_input":"2021-05-30T13:06:43.080914Z","iopub.status.idle":"2021-05-30T13:06:43.462871Z","shell.execute_reply.started":"2021-05-30T13:06:43.080845Z","shell.execute_reply":"2021-05-30T13:06:43.461594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy scores\nprint(DecisionTree_model.get_params())\nprint(f\"Training Score: {round(DecisionTree_model.score(x_train, y_train),4)}\")\nprint(f\"Testing Score: {round(DecisionTree_model.score(x_test, y_test),4)}\")","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-05-30T13:06:43.464669Z","iopub.execute_input":"2021-05-30T13:06:43.465016Z","iopub.status.idle":"2021-05-30T13:06:43.492991Z","shell.execute_reply.started":"2021-05-30T13:06:43.464984Z","shell.execute_reply":"2021-05-30T13:06:43.491835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature importances\nd = {'feature importance':list(DecisionTree_model.feature_importances_)}\ntable = pd.DataFrame(d, index=x_train.columns)\n\ndisplay(  table.sort_values('feature importance', ascending=False)  )","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-05-30T13:06:43.494842Z","iopub.execute_input":"2021-05-30T13:06:43.49536Z","iopub.status.idle":"2021-05-30T13:06:43.510129Z","shell.execute_reply.started":"2021-05-30T13:06:43.495311Z","shell.execute_reply":"2021-05-30T13:06:43.509185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{"editable":false}},{"cell_type":"code","source":"%%time\n\n# fitting\nLogistic_model =  LogisticRegressionCV(Cs = 50, cv = 5, random_state=8017)\nLogistic_model.fit(x_train, y_train)\nprint(Logistic_model.get_params())","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-05-30T13:07:03.134924Z","iopub.execute_input":"2021-05-30T13:07:03.135373Z","iopub.status.idle":"2021-05-30T13:07:14.313892Z","shell.execute_reply.started":"2021-05-30T13:07:03.135336Z","shell.execute_reply":"2021-05-30T13:07:14.312612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# regularaization candidates\nprint('Candidates of Regularization Parameter C:')\nprint(Logistic_model.Cs_, '\\n')\n\n# accuracy scores\nprint(f\"Training Score: {round(Logistic_model.score(x_train, y_train),4)}\")\nprint(f\"Testing Score: {round(Logistic_model.score(x_test, y_test),4)}\")","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-05-30T13:07:20.218855Z","iopub.execute_input":"2021-05-30T13:07:20.219276Z","iopub.status.idle":"2021-05-30T13:07:20.249939Z","shell.execute_reply.started":"2021-05-30T13:07:20.21924Z","shell.execute_reply":"2021-05-30T13:07:20.248857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fitted parameters\nprint(f'Best Regularization Parameter C = {round(Logistic_model.C_[0],4)}')\nprint(f'intercept = {round(Logistic_model.intercept_[0],4)}')\nd = {'estimates' : list(Logistic_model.coef_[0]),\n     'absolute' : np.abs(list(Logistic_model.coef_[0]))\n    }\ntable = pd.DataFrame(d, index=x_train.columns)\n\ndisplay(  table.sort_values('absolute', ascending=False).drop(columns='absolute')  )","metadata":{"editable":false,"execution":{"iopub.status.busy":"2021-05-30T13:07:20.462788Z","iopub.execute_input":"2021-05-30T13:07:20.463123Z","iopub.status.idle":"2021-05-30T13:07:20.481607Z","shell.execute_reply.started":"2021-05-30T13:07:20.463094Z","shell.execute_reply":"2021-05-30T13:07:20.480044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cluster Analysis\n**Using the two most important features: `age` and `ap_hi`**","metadata":{}},{"cell_type":"code","source":"# sample the first 2500 records only due to computation limit\nsX = x_train[['age', 'ap_hi']][0:2500].to_numpy()\nsY = y_train[0:2500].to_numpy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### K-means","metadata":{}},{"cell_type":"code","source":"# function to plot decision boundary\ndef plot_decision_boundary(x, y, model, title):\n    \n    h = 0.02\n    x_min, x_max = x[:, 0].min()-0.1, x[:, 0].max() +0.1\n    y_min, y_max = x[:, 1].min()-0.1, x[:, 1].max() +0.1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    # Obtain labels for each point in mesh. Use last trained model.\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.clf()\n    plt.imshow(Z, interpolation='nearest',\n               extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n               cmap=plt.cm.Paired,\n               aspect='auto', origin='lower')\n\n    plt.scatter(x[:, 0:1], x[:, 1:2], c=y, edgecolors='k')\n    plt.title(title, fontsize = 20)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# K-means random\nkmean1 = KMeans(n_clusters=2, init='random', random_state=0)\nkmean1.fit(sX)\nplot_decision_boundary(sX, sY, kmean1, \"Prediction Boundary of K-Means\")\nplt.plot(kmean1.cluster_centers_[:, 0], kmean1.cluster_centers_[:, 1], '*', markersize=20, color=\"red\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# K-means++\nkmean2 = KMeans(n_clusters=2, init='k-means++', random_state=0)\nkmean2.fit(sX)\nplot_decision_boundary(sX, sY, kmean2, \"Prediction Boundary of K-Means ++\")\nplt.plot(kmean2.cluster_centers_[:, 0], kmean2.cluster_centers_[:,1], '*', markersize=20, color=\"red\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training accuracy scores\nkmeans1_pred = kmean1.predict(sX) # K-means random\nkmeans2_pred = kmean2.predict(sX) # K-means++\nprint('K-means(random) training accuracy: ', accuracy_score(sY, kmeans1_pred))\nprint('K-means++ training accuracy: ', accuracy_score(sY, kmeans2_pred), '\\n')\n\n# testing accuracy scores\nkmeans1_pred_test = kmean1.predict(x_test[['age','ap_hi']]) # K-means random\nkmeans2_pred_test = kmean2.predict(x_test[['age','ap_hi']]) # K-means++\nprint('K-means(random) testing accuracy: ', accuracy_score(y_test, kmeans1_pred_test))\nprint('K-means++ testing accuracy: ', accuracy_score(y_test, kmeans2_pred_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Agglomerative Clustering","metadata":{}},{"cell_type":"code","source":"# function to plot dengrogram\ndef plot_dendrogram(model, **kwargs): # provided by Mathew Kallada. \n\n    # Children of hierarchical clustering\n    children = model.children_\n\n    # Distances between each pair of children\n    # Since we don't have this information, we can use a uniform one for plotting\n    distance = np.arange(children.shape[0])\n\n    # The number of observations contained in each cluster level\n    no_of_observations = np.arange(2, children.shape[0] + 2)\n\n    # Create linkage matrix and then plot the dendrogram\n    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n    \n    sch.dendrogram(linkage_matrix, **kwargs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ward's linkage and complete linkage\nH_C_ward = AgglomerativeClustering(n_clusters=2) # default linkage is ward. \nH_C_complete = AgglomerativeClustering(n_clusters=2, linkage='complete')\n\n# dendrogram (on 250 records only)\nhc_ward_pred = H_C_ward.fit_predict(sX[0:250])\nhc_complete_pred = H_C_complete.fit_predict(sX[0:250])\n\nfig = plt.figure(figsize=(25, 10))\nax = fig.add_subplot(1, 2, 1)\nplot_dendrogram(H_C_ward)\nax.set_title('Linkage method is ward')\n\nax = fig.add_subplot(1, 2, 2)\nZ2 = plot_dendrogram(H_C_complete)\nax.set_title('Linkage method is complete')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy score (on 2500 records)\nhc_ward_pred = H_C_ward.fit_predict(sX)\nhc_complete_pred = H_C_complete.fit_predict(sX)\n\nprint(\"ward's linkage training accuracy: \", accuracy_score(sY, hc_ward_pred))\nprint('complete linkage training accuracy: ', accuracy_score(sY, hc_complete_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DBSCAN","metadata":{}},{"cell_type":"code","source":"# DBSCAN\ndbscan = DBSCAN(eps=0.26, min_samples=20)\ndbscan_pred = dbscan.fit_predict(sX)\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(dbscan_pred)) - (1 if -1 in dbscan_pred else 0)\nn_noise_ = list(dbscan_pred).count(-1)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\nprint('Estimated number of noise points: %d' % n_noise_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot\nplt.scatter(x=sX[:,0], y=sX[:,1], c=dbscan_pred, edgecolors='k')\nplt.show()\n\n# accuracy score\nprint(\"DBSCAN training accuracy: \", accuracy_score(sY, dbscan_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gaussian Mixture","metadata":{}},{"cell_type":"code","source":"# Gaussian Mixture\ngmm = GaussianMixture(n_components=2, covariance_type='full', max_iter=20, random_state=8017) \ngmm.fit(sX)\nplot_decision_boundary(sX, sY, gmm, \"Gaussian Mixture\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training accuracy scores\ngmm_pred = gmm.predict(sX)\nprint('Gaussian Mixture Model training accuracy: ', accuracy_score(sY, gmm_pred), '\\n')\n\n# testing accuracy scores\ngmm_pred_test = gmm.predict(x_test[['age','ap_hi']])\nprint('Gaussian Mixture Model testing accuracy: ', accuracy_score(y_test, gmm_pred_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Clustering Performance","metadata":{}},{"cell_type":"code","source":"# function to calculate entropy score\ndef get_entropy(y, pred, n_class):\n    p = np.zeros((n_class, n_class))\n    tb = contingency_matrix(y, pred)\n    for i in range(n_class):\n        for j in range(n_class):\n            p[i, j] = tb[i, j]/np.sum(tb[i, :])\n            \n    E = np.zeros((n_class, 1))\n    for i in range(n_class):\n        for j in range(n_class):\n            if (p[i, j] != 0):\n                E[i] = E[i] - p[i, j] * np.log(p[i, j])\n    Entropy = np.dot(np.sum(tb, 1) / np.sum(tb), E)\n    return Entropy","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy scores of all clustering\nresult = pd.DataFrame({'Model':['K-means (Random)','K-means (K-means++)','Dendrogram (Ward)','Dendrogram (Complete)','DBSCAN','Gaussian Mixture Model'],\n                       'Training Accuracy': [accuracy_score(sY, kmeans1_pred),\n                                             accuracy_score(sY, kmeans2_pred),\n                                             accuracy_score(sY, hc_ward_pred),\n                                             accuracy_score(sY, hc_complete_pred),\n                                             accuracy_score(sY, dbscan_pred),\n                                             accuracy_score(sY, gmm_pred)],\n                       \n                       # external measurement\n                       'Entropy': [get_entropy(sY, kmeans1_pred, 2)[0], \n                                  get_entropy(sY, kmeans2_pred, 2)[0], \n                                  get_entropy(sY, hc_ward_pred, 2)[0], \n                                  get_entropy(sY, hc_complete_pred, 2)[0],\n                                  get_entropy(sY, dbscan_pred, 2)[0], \n                                  get_entropy(sY, gmm_pred, 2)[0]],\n                       'Adjusted Rand Index': [adjusted_rand_score(sY, kmeans1_pred), \n                                              adjusted_rand_score(sY, kmeans2_pred),\n                                              adjusted_rand_score(sY, hc_ward_pred), \n                                              adjusted_rand_score(sY, hc_complete_pred), \n                                              adjusted_rand_score(sY, dbscan_pred), \n                                              adjusted_rand_score(sY, gmm_pred)],\n                       \n                       # internal measurement\n                       'Silhouette Coefficient': [silhouette_score(sX, kmeans1_pred),\n                                                 silhouette_score(sX, kmeans2_pred), \n                                                 silhouette_score(sX, hc_ward_pred),\n                                                 silhouette_score(sX, hc_complete_pred),\n                                                 silhouette_score(sX, dbscan_pred),\n                                                 silhouette_score(sX, gmm_pred)],\n                       'Calinski Harabasz Score': [calinski_harabasz_score(sX, kmeans1_pred),  #Ratio of between-cluster dispersion to within-cluster dispersion\n                                                   calinski_harabasz_score(sX, kmeans2_pred), \n                                                   calinski_harabasz_score(sX, hc_ward_pred),\n                                                   calinski_harabasz_score(sX, hc_complete_pred),\n                                                   calinski_harabasz_score(sX, dbscan_pred),\n                                                   calinski_harabasz_score(sX, gmm_pred)]\n                      })\nresult","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**K-means has the highest training accuracy.**\n\n**Dendrogram (Complete) works the best in entropy, while K-means (Random) works the best on adjusted rand index.**\n\n**It can be observed that Dendrogram (Complete) works the best in silhouette score, while K-means (K-means++) works the best regarding calinski harabasz score.**","metadata":{}},{"cell_type":"markdown","source":"# Ensemble Methods","metadata":{}},{"cell_type":"markdown","source":"### Bagging Classifier","metadata":{}},{"cell_type":"code","source":"%%time\n\n# parameters candidates\nparameters = {'base_estimator__max_depth': [4,6,8,12,24],\n              'n_estimators': [20, 50, 100, 200]}\n\n# fitting\nBagging_GSCV = GridSearchCV(BaggingClassifier(DecisionTreeClassifier(), random_state=8017), \n                            parameters, n_jobs=-1, verbose=3, return_train_score=True)\nBagging_GSCV.fit(x_train, y_train)\nBagging_model = Bagging_GSCV.best_estimator_\nBagging_model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_bagging_scores = Bagging_GSCV.cv_results_['mean_train_score']\ntest_bagging_scores = Bagging_GSCV.cv_results_['mean_test_score']\n\n#plt.plot(test_dt_scores, 'go-')\nplt.plot(train_bagging_scores, 'ro--')\nplt.plot(test_bagging_scores, 'ro-')\nplt.ylim(0.4, 1.05)\nplt.xticks(range(20), range(2, 22))\nplt.legend([\"Bagging training score\", \"Bagging test score\"])\nplt.axvline(np.argmax(test_bagging_scores), linestyle=\"dotted\", color=\"red\")\nplt.annotate(np.max(test_bagging_scores).round(4), (np.argmax(test_bagging_scores), np.max(test_bagging_scores)), xycoords=\"data\",\n                 xytext=(-40, 30), textcoords=\"offset pixels\", arrowprops=dict(facecolor=\"black\", shrink=0.1), fontsize=10,\n                 horizontalalignment=\"center\", verticalalignment=\"top\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy scores\nprint(Bagging_model.get_params())\nprint(f\"Training Score: {round(Bagging_model.score(x_train, y_train),4)}\")\nprint(f\"Testing Score: {round(Bagging_model.score(x_test, y_test),4)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"%%time\n\n# parameters candidates\nparameters = {'n_estimators': [20, 50, 100, 200],\n              'max_depth':[6,8,12,24,48]}\n\n# fitting\nRandomForest_GSCV = GridSearchCV(RandomForestClassifier(random_state=8017), \n                                 parameters, n_jobs=-1, verbose=3)\nRandomForest_GSCV.fit(x_train, y_train)\nRandomForest_model = RandomForest_GSCV.best_estimator_\nRandomForest_model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy scores\nprint(RandomForest_model.get_params())\nprint(f\"Training Score: {round(RandomForest_model.score(x_train, y_train),4)}\")\nprint(f\"Testing Score: {round(RandomForest_model.score(x_test, y_test),4)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Adaboost","metadata":{}},{"cell_type":"code","source":"%%time\n\n# parameters candidates\nparameters = {'base_estimator__max_depth': [2,3,4,6,8,12],\n              'n_estimators': [20, 50, 100, 200]}\n\n# fitting\nAdaboost_GSCV = GridSearchCV(AdaBoostClassifier(DecisionTreeClassifier(), random_state=8017), \n                             parameters, n_jobs=-1, verbose=3)\nAdaboost_GSCV.fit(x_train, y_train)\nAdaboost_model = Adaboost_GSCV.best_estimator_\nAdaboost_model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy scores\nprint(Adaboost_model.get_params())\nprint(f\"Training Score: {round(Adaboost_model.score(x_train, y_train),4)}\")\nprint(f\"Testing Score: {round(Adaboost_model.score(x_test, y_test),4)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gradient Boosting","metadata":{}},{"cell_type":"code","source":"%%time\n\n# parameters candidates\nparameters = {'max_depth': [2,4,6,8,10,12],\n              'n_estimators': [200],\n              'learning_rate': [0.01]}\n\n# fitting\nGradientBoost_GSCV = GridSearchCV(GradientBoostingClassifier(random_state=8017), \n                             parameters, n_jobs=-1, verbose=3)\nGradientBoost_GSCV.fit(x_train, y_train)\nGradientBoost_model = GradientBoost_GSCV.best_estimator_\nGradientBoost_model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy scores\nprint(GradientBoost_model.get_params())\nprint(f\"Training Score: {round(GradientBoost_model.score(x_train, y_train),4)}\")\nprint(f\"Testing Score: {round(GradientBoost_model.score(x_test, y_test),4)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Support Vector Machine","metadata":{"editable":false}},{"cell_type":"markdown","source":"### Linear SVC","metadata":{"editable":false}},{"cell_type":"code","source":"%%time\n\n# parameters candidates\nparameters = {'C': np.logspace(-4, 4, 50)}\n\n# fitting\nLinearSVC_GSCV = GridSearchCV(LinearSVC(dual=False, random_state=8017), \n                              parameters, n_jobs=-1, verbose=3)\nLinearSVC_GSCV.fit(x_train, y_train)\nLinearSVC_model = LinearSVC_GSCV.best_estimator_\nLinearSVC_model\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy scores\nprint(LinearSVC_model.get_params())\nprint(f\"Training Score: {round(LinearSVC_model.score(x_train, y_train),4)}\")\nprint(f\"Testing Score: {round(LinearSVC_model.score(x_test, y_test),4)}\")","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Non-Linear SVC","metadata":{"editable":false}},{"cell_type":"code","source":"%%time\n\n# fitting\nSVC_model = SVC(kernel='rbf', random_state=8017)\nSVC_model.fit(x_train, y_train)\nSVC_model","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# accuracy scores\nprint(SVC_model.get_params())\nprint(f\"Training Score: {round(SVC_model.score(x_train, y_train),4)}\")\nprint(f\"Testing Score: {round(SVC_model.score(x_test, y_test),4)}\")","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MLP","metadata":{"editable":false}},{"cell_type":"code","source":"# layer sizes candidates\nls = [x for x in [4,8,16,32]] + [x for x in product([2,4,8], [4,8])] + [x for x in product([2,4,8], [8,16], [4,8])]\nls","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# parameters candidates\nparameters = {'hidden_layer_sizes': ls}\n\n# fitting\nMLP_GSCV = GridSearchCV(MLPClassifier(random_state=8017), \n                        parameters, n_jobs=-1, verbose=3)\nMLP_GSCV.fit(x_train, y_train)\nMLP_model = MLP_GSCV.best_estimator_\nMLP_model","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# top 10 cv scores for the MLP candidates\npd.DataFrame(MLP_GSCV.cv_results_).sort_values('rank_test_score').head(10)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scores\nprint(MLP_model)\nprint(f\"Training Score: {round(MLP_model.score(x_train, y_train),4)}\")\nprint(f\"Testing Score: {round(MLP_model.score(x_test, y_test),4)}\")","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compare All Models","metadata":{"editable":false}},{"cell_type":"code","source":"# accuracy scores of all models\nresult = pd.DataFrame({'Model':['Decision Tree','Logistic',\n                                'Bagging','Random Forest','Adaboost','Gradient Boost',\n                                'Linear SVC','Non-linear SVC','MLP'],\n                       'Prediction Accuracy': [DecisionTree_model.score(x_test, y_test),\n                                               Logistic_model.score(x_test, y_test),\n                                               Bagging_model.score(x_test, y_test),\n                                               RandomForest_model.score(x_test, y_test),\n                                               Adaboost_model.score(x_test, y_test),\n                                               GradientBoost_model.score(x_test, y_test),\n                                               LinearSVC_model.score(x_test, y_test),\n                                               SVC_model.score(x_test, y_test),\n                                               MLP_model.score(x_test, y_test)]}\n                     )\nresult.sort_values('Prediction Accuracy', ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification Report & Confusion Matrix","metadata":{}},{"cell_type":"code","source":"def plot_confusion_matrix(classifier, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    pred_train = classifier.predict_proba(x_train)\n    pred_test = classifier.predict_proba(x_test)\n    acc_train = accuracy_score(y_train, np.argmax(pred_train, 1))\n    acc_test = accuracy_score(y_test, np.argmax(pred_test, 1))\n\n    print(\"Training ACC:\", round(acc_train, 4), \"Testing ACC:\", round(acc_test, 4))\n    cm = confusion_matrix(y_test, np.argmax(pred_test, 1))\n    print(\"Confusion matrix: \\n\", cm)\n    print(\"Testing:\\n\",classification_report(y_test, np.argmax(pred_test, 1), target_names=classes))\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:18:41.080499Z","iopub.execute_input":"2021-05-30T10:18:41.080823Z","iopub.status.idle":"2021-05-30T10:18:41.090565Z","shell.execute_reply.started":"2021-05-30T10:18:41.080794Z","shell.execute_reply":"2021-05-30T10:18:41.08999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(RandomForest_model, classes=['No','Yes'],\n                      title='Confusion matrix of Random Forest')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:18:41.545911Z","iopub.execute_input":"2021-05-30T10:18:41.546207Z","iopub.status.idle":"2021-05-30T10:18:43.594032Z","shell.execute_reply.started":"2021-05-30T10:18:41.546183Z","shell.execute_reply":"2021-05-30T10:18:43.592972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(DecisionTree_model, classes=['No','Yes'],\n                      title='Confusion matrix of Decision Tree')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:19:21.917685Z","iopub.execute_input":"2021-05-30T10:19:21.918042Z","iopub.status.idle":"2021-05-30T10:19:22.197957Z","shell.execute_reply.started":"2021-05-30T10:19:21.918011Z","shell.execute_reply":"2021-05-30T10:19:22.19667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(Logistic_model, classes=['No','Yes'],\n                      title='Confusion matrix of Logistic Regression')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:18:46.697858Z","iopub.execute_input":"2021-05-30T10:18:46.698344Z","iopub.status.idle":"2021-05-30T10:18:47.436699Z","shell.execute_reply.started":"2021-05-30T10:18:46.698314Z","shell.execute_reply":"2021-05-30T10:18:47.435671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ROC Curve","metadata":{}},{"cell_type":"code","source":"yy_test = label_binarize(y_test, classes=[0, 1])\nplt.figure(figsize=(20, 20))\ndef plot_roc_curve(classifier, label):\n    # Compute ROC curve and ROC area for each class\n    fpr = []\n    tpr = []\n    roc_auc = []\n    pred_test = classifier.predict_proba(x_test)\n    fpr, tpr, _ = roc_curve(yy_test, pred_test[:, 1])\n    roc_auc = auc(fpr, tpr)\n    \n    plt.plot(fpr, tpr, label=label+(' (area = %0.4f)' % roc_auc))\ndef show_roc_curve():\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:22:52.886592Z","iopub.execute_input":"2021-05-30T10:22:52.887084Z","iopub.status.idle":"2021-05-30T10:22:52.902199Z","shell.execute_reply.started":"2021-05-30T10:22:52.887046Z","shell.execute_reply":"2021-05-30T10:22:52.901388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc_curve(RandomForest_model, label='Random Forest')\nplot_roc_curve(DecisionTree_model, label='Decision Tree')\nplot_roc_curve(Logistic_model, label='Logistic Regression')\nshow_roc_curve()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T10:22:53.078373Z","iopub.execute_input":"2021-05-30T10:22:53.078702Z","iopub.status.idle":"2021-05-30T10:22:54.183479Z","shell.execute_reply.started":"2021-05-30T10:22:53.078654Z","shell.execute_reply":"2021-05-30T10:22:54.18236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}