{"cells":[{"metadata":{},"cell_type":"markdown","source":"Each record in the database describes a Boston suburb or town. The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970. The attributes are deﬁned as follows (taken from the UCI Machine Learning Repository1): \n\n* CRIM: per capita crime rate by town\n* ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n* INDUS: proportion of non-retail business acres per town\n* CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n* NOX: nitric oxides concentration (parts per 10 million)\n* RM: average number of rooms per dwelling\n* AGE: proportion of owner-occupied units built prior to 1940\n* DIS: weighted distances to ﬁve Boston employment centers\n* RAD: index of accessibility to radial highways\n* TAX: full-value property-tax rate per 10k dollar\n* PTRATIO: pupil-teacher ratio by town \n* B: 1000(Bk−0.63)^2 where Bk is the proportion of blacks by town  \n* LSTAT: % lower status of the population\n* MEDV: Median value of owner-occupied homes in thousands dollars\n\n\n**Here \"MEDV\" is the predictor** \n\nCHAS is a categorical variable and rest are numerical variable","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Imporing Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Importing Dataset\n\ncolumn_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndf = pd.read_csv('/kaggle/input/boston-house-prices/housing.csv', header=None, delimiter=r\"\\s+\", names= column_names)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()    # Checking Data types and missing values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there is no object and no missing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Another way (Most used) of checking missing data in dataset\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see the statistical description of the whole dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(\"CHAS\", axis = 1).describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation Matrix\nplt.figure(figsize= (12, 8))\nsns.heatmap(df.drop(\"CHAS\", axis = 1).corr(), annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. \"DIS\" feature is highly correlated with \"INDUS\", \"NOX\" and \"AGE\"\n\n2. \"TAX\" feature is highly correlated with \"RAD\" ,\"INDUS\" and \"NOX\"\n\n3. \"MEDV\" has high positive correlation with \"RM\" which is the no. of rooms \n\n4. \"MEDV\" has high negative correlation with \"LSTATE\" \n\n5. We must take steps to high correlated features when using Linear Regression not to account multicolinearity \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation with predictors \nplt.figure(figsize= (10, 6))\ncorrelation = df.drop(\"CHAS\", axis = 1).corr().iloc[0:12,-1]\ncorrelation.plot(kind = \"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Univariate Analysis of MEDV\nplt.figure(figsize= (8, 6))\nsns.distplot(df[\"MEDV\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from the histogram that the predictor is rightly skewed ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize= (8, 6))\nsns.scatterplot(x = df[\"LSTAT\"], y= df[\"MEDV\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize= (8, 6))\nsns.scatterplot(x = df[\"RM\"], y= df[\"MEDV\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize= (8, 6))\nsns.regplot(x = df[\"TAX\"], y= df[\"RAD\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize= (8, 6))\nsns.regplot(x = df[\"INDUS\"], y= df[\"NOX\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"CHAS\"].value_counts().plot(kind = \"bar\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This indicates that the feature is imbalanced","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(\"MEDV\", axis = 1)\ny = df[\"MEDV\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.3, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that y_train and y_test has similar distributions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Scaling The dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multiple Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_lr = lr.predict(X_test_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = mean_squared_error(y_test, y_pred_lr)**(1/2)\nrmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.score(X_test_scaled, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize= (10, 6))\nsns.regplot(y_test, y_pred_lr)\nplt.xlim([0, 60])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets try Random Forest Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor(random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_rf = rf.predict(X_test_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = mean_squared_error(y_test, y_pred_rf)**0.5\nrmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_score(y_test, y_pred_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize= (10, 6))\nsns.regplot(y_test, y_pred_rf)\nplt.xlim([0, 60])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, The score has improved a lot. Though we have not done hyperparameter tuning. Lets see the accuracy of the model with hyperparameter tuning using Randomized Search","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n\n# Fit the random search model\nrf_random.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract best hyperparameters from 'rf_random'\n\nbest_hyperparams = rf_random.best_params_\nprint('Best hyerparameters:\\n', best_hyperparams)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract best model from 'rf_random'\nbest_model = rf_random.best_estimator_\n\n# Predict the test set labels\ny_pred_rf = best_model.predict(X_test_scaled)\n\n# Evaluate the test set RMSE\nrmse_test = mean_squared_error(y_test, y_pred_rf)**(1/2)\n\n# Print the test set RMSE\nprint('Test set RMSE of gb: {:.2f}'.format(rmse_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_score(y_test, y_pred_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}