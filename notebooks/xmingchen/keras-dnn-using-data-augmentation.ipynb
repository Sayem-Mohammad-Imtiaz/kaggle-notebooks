{"cells":[{"metadata":{"_cell_guid":"1860bdf6-6d52-4492-9314-a3896b61adc2","_execution_state":"idle","_uuid":"33c93b2473a265544d30bc86bcf74acf64d7f126"},"cell_type":"markdown","source":"In this notebook, it is shown that classification accuracy can be improved by data augmentation by artifically generating data for the most important feature, inspired by https://www.kaggle.com/wpncrh/tensorflow-nn. "},{"metadata":{"_cell_guid":"c90793a5-7cf9-4b84-aa14-720339918caa","_execution_state":"idle","_uuid":"013af5d625615fa86b8765f8383be8a0215417a3","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nfrom keras import models\nfrom keras import layers\nfrom keras import regularizers\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras import optimizers\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":102,"outputs":[]},{"metadata":{"_cell_guid":"c3d89af1-335d-40ce-b30a-7ffc6fe60325","_execution_state":"idle","_uuid":"c1fa56e1c4b7b15c2c203e8e76c05c6dd66353b7"},"cell_type":"markdown","source":"Get basic information about dataset. No data cleaning seems to be necessary."},{"metadata":{"_cell_guid":"c20bb743-40e5-4a75-8d6b-9699e822132a","_execution_state":"idle","_uuid":"1955b51c2130f2e3fc82180f0d3b5379c8a981f4","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('../input/voice.csv')\nprint('head entries:\\n', df.head())\nprint('dataset info:\\n', df.info())\nprint('#null:\\n', df.isnull().sum())\nprint('col. name: ', df.columns)","execution_count":103,"outputs":[]},{"metadata":{"_cell_guid":"4d1e99bf-208e-429e-bcea-a8a0d2f84b5c","_execution_state":"idle","_uuid":"b35b9d07d696dc224c49f4f3c5dedb51efa3c6cd"},"cell_type":"markdown","source":"Investigate **feautre correlations**: Some features seem to be redundant - \"kurt\", \"centriod\", \"maxdom\"."},{"metadata":{"_cell_guid":"b5978049-e272-434f-b21a-9b70b06d3036","_execution_state":"idle","_uuid":"255fbd511ae578a0571f5c81b61ffd3a5f39196b","trusted":true},"cell_type":"code","source":"import seaborn as sns\nax = plt.axes()\nsns.heatmap(df.corr(), vmax=0.8, ax=ax, linewidths=0.25, square=True, linecolor='black')\nax.set_title('Orignal feature correlations')\n\n# feature separation\ng = sns.PairGrid(df, hue='label', vars=[\"meanfun\", \"sfm\", 'IQR'])\ng = g.map(plt.scatter, s=4)\nplt.show()\n\n#grid = sns.FacetGrid(df, row=\"label\", col=\"meanfun\", margin_titles=True)\n#grid.map(plt.hist, \"sfm\");\n","execution_count":104,"outputs":[]},{"metadata":{"_cell_guid":"4b91dde7-c68b-4c0f-a87d-cdcbca9f334b","_execution_state":"idle","_uuid":"ee7c978e182a21388db0392666dae308f4558500"},"cell_type":"markdown","source":"Investigate **feautre importances** and identified *meanfun* (mean fundamental frequency) as the most important feature for gender classification."},{"metadata":{"_cell_guid":"77c1908c-9805-4796-9e44-de2f6ce8893e","_execution_state":"idle","_uuid":"5032f80c51e441e6186fdf7097a5fd54135139e8","trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_classif\n\ndef select_kbest_clf(df, target, k=5):\n    \"\"\"\n    Selecting K-Best features for classification\n    - df: A pandas dataFrame with the training data\n    - target: target variable name in DataFrame\n    - k: desired number of features from the data\n    \n    returns feature_scores: scores for each feature \n    \"\"\"\n    feat_selector = SelectKBest(f_classif, k=k)\n    _= feat_selector.fit(df.drop(target, axis=1), df[target])\n    \n    feat_scores = pd.DataFrame()\n    feat_scores[\"F Score\"] = feat_selector.scores_\n    feat_scores[\"P Value\"] = feat_selector.pvalues_\n    feat_scores[\"Support\"] = feat_selector.get_support()\n    feat_scores[\"Attribute\"] = df.drop(target, axis=1).columns\n    \n    return feat_scores\n    \nk=select_kbest_clf(df, 'label', k=5).sort_values(['F Score'],ascending=False)\nplt.figure()\nk1=sns.barplot(x=k['F Score'],y=k['Attribute'])\nk1.set_title('Feature Importance')","execution_count":105,"outputs":[]},{"metadata":{"_cell_guid":"e011c46b-cca9-4bc0-8dbd-8e35d92db40d","_execution_state":"idle","_uuid":"2677dc614bba39393f4ca376e8d493017b9b14a3"},"cell_type":"markdown","source":"Data augmentation imposing small variations to *meanfun* feature."},{"metadata":{"_cell_guid":"ee5e79e5-6d2c-4327-a95b-6b41f00d24e0","_execution_state":"idle","_uuid":"415160782f353928d44f8d849479c078e4fac7c6","trusted":true},"cell_type":"code","source":"# increase dataset size by perturbing the most important feature\nfor i in range(3):\n    copy = df\n    \n    copy['meanfun'] = copy['meanfun'] + np.random.uniform(0, 1e-2)\n    \n    df = df.append(copy, ignore_index=True)\n    # print(\"shape of df after {0}th intertion of this loop is {1}\".format(i, df.shape))\n\ndf.apply(np.random.permutation)","execution_count":106,"outputs":[]},{"metadata":{"_cell_guid":"1bb466a6-0e2b-40b4-9f93-fd05d94ab66a","_execution_state":"idle","_uuid":"c98aadce6351e3c01b10f222a42aefd2bbfdafa0"},"cell_type":"markdown","source":" - **Data preprocessing**: label encoding and feature scaling; \n - **Dimension reduction** by deleting redundant features (it seems not to provide any improvement);\n - Dataset split to training and test sets."},{"metadata":{"_cell_guid":"1b58f739-ff8f-4352-8f8c-a7771526bdee","_execution_state":"idle","_uuid":"cb2f50d212cd1c4e357e3880b2e59e01ff2b5613","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA\n\n# features vs. label\nX = df.iloc[:, :-1].values\ny = df.iloc[:, -1]\n\n# Encode label category: male -> 1, female -> 0\ngender_encoder = LabelEncoder()\ny = gender_encoder.fit_transform(y)\n\n# redundant features are identified by investigating feature correlation\nX2 = np.delete(np.array(X), [7, 11, 17] , 1)  \nX = X2\n\nprint(X.shape)\n\n# feature normalization\nscaler = StandardScaler()\n\n#X = scaler.fit_transform(X)\n#scaler = MinMaxScaler()\nfor i in range(X.shape[1]):\n    vec = scaler.fit_transform(X2[:, i].reshape([-1, 1]))\n    #print(vec.shape)\n    X[:, i] = vec.reshape(-1)\nplt.hist(X[:, 2])\nplt.show()\n\n# dimension reduction via PCA\n#pca = PCA(n_components=17)\n#pca.fit(X2)\n#X2 = pca.transform(X2) \n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)","execution_count":107,"outputs":[]},{"metadata":{"_cell_guid":"11467948-ad92-4476-8b3b-a85d5373dc24","_execution_state":"idle","_uuid":"cc22530e1c9748781bb94a63e2a9fa2b5dee09d0"},"cell_type":"markdown","source":" - Using DNN under Keras framework \n - Training: fine-tuning parameters\n - Validation and error analysis by investigating estimated class probabilities at error positions."},{"metadata":{"_cell_guid":"f76a9811-1a2c-4e38-8d4b-99598fead095","_execution_state":"idle","_uuid":"f8ebfb70baa1988e97d39576e20118fee89deb9f","trusted":true},"cell_type":"code","source":"nDim = X_train.shape[1]    # number of features used for classification\n\n# DNN architecture \nnUnitL1 = 128              \nnUnitL2 = 64\nnUnitL3 = 32\nbatch_size = 32\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(nUnitL1, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), input_shape=(nDim,)))\n#model.add(layers.Dropout(0.25))\nmodel.add(layers.Dense(nUnitL2, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001)))\n#model.add(layers.Dropout(0.25))\nmodel.add(layers.Dense(nUnitL3, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\n#model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              \nmodel.compile(optimizer=optimizers.RMSprop(lr=0.0002),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nreduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5,\n                              patience=5, min_lr=0.00001)\nhist = model.fit(X_train, y_train, epochs=50,\n          batch_size=batch_size, \n          verbose=1,callbacks=[reduce_lr], \n          validation_data = [X_test, y_test])\nresults = model.evaluate(X_test, y_test)\n\nloss_values = hist.history['loss']\nval_loss_values = hist.history['val_loss']\n\nepochs = range(1, 51)\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()\nprint('final test resutls: ', results)\n\n# result evaluation\ny_pred = model.predict(X_test)\nypred = np.zeros((y_pred.shape)).astype(int)\nypred[np.where(y_pred > 0.5)] = 1\nprint(y_pred.shape, y_test.shape)\n#print(ypred[:10], y_test[:10])\nfalsePos = np.array(np.where(~np.equal(ypred.reshape(-1), y_test))).reshape([-1,1])\n#print(falsePos[:10])\nprint(len(falsePos))\n\nmaxPos = np.array(np.where(y_pred==np.max(y_pred))).reshape(-1)\nprint(maxPos[0])\nprint(X_test[maxPos[0], :])\nfor pos in falsePos:\n    print(y_pred[pos])\n    #print(X_test[pos, :])\n#plt.figure()\n#plt.plot(y_pred, 'b.')\n#plt.plot(y_test, 'r*')","execution_count":109,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}