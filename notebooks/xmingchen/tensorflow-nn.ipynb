{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"e0bbd2eb-2f6c-0bf5-1171-ead8b0960fdd"},"source":"The following is an attempt to use Google's Tensorflow ML library to run a NN classifier. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f785ca3c-fe42-c8f7-e76a-33642418311b"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom sklearn.cross_validation import train_test_split\nimport random\n\nvoice =pd.read_csv('../input/voice.csv')\nvoice = pd.DataFrame(voice)\nprint(voice.head())\n# Any results you write to the current directory are saved as output."},{"cell_type":"markdown","metadata":{"_cell_guid":"897c3fd3-fd0c-fd08-6308-215085ac2b4f"},"source":"Increasing dataset size via adding small distortions may help training, but not necessarily. The problem here is that all original data is used for training. Permutation is necessary for a correct validation. It is verified that adding noise does not help in this case."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"17450ab1-1082-d2f6-c4f8-94335f42edbc"},"outputs":[],"source":"#for i in range(6):\n#\tcopy = voice\n#\tcopy['meanfreq']=copy['meanfreq']+random.gauss(.0001,.001) # add noice to mean freq var\n#\tvoice=voice.append(copy,ignore_index=True) # make voice df 2x as big\n#\tprint(\"shape of df after {0}th intertion of this loop is {1}\".format(i,voice.shape))\n\n#voice.apply(np.random.permutation)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"555f2d7c-9cfc-c973-4f32-dd7a24a82475"},"outputs":[],"source":"label = voice.pop(\"label\")\n\n# converts from dataframe to np array\nvoice=voice.values\n\n# --------------- Pre-processing: feature selection ---------------------\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# some features are redundant as shown by correlation, therefore, they are dropped.\nX2 = np.delete(voice, [7, 11, 17] , 1)  \n\n# data standarization (zero-mean, unit variance) ~ truncation to [-1, 1]\nscaler = StandardScaler()\nscaler.fit(X2)\nX2 = scaler.transform(X2)\n\n# dimension reduction using PCA\npca = PCA(n_components=16)\npca.fit(X2)\nX2 = pca.transform(X2) \nnFeature = X2.shape[1]\n\n# convert train labels to one hots: tf.one_hot()\ntrain_labels = pd.get_dummies(label).values\n\nTestPortion = 0.2\n\n# It is important to take same test_size and random_state for comparison\nx_train,x_test,y_train,y_test = train_test_split(X2, train_labels, test_size=TestPortion, random_state=1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"5b7c9b6f-3b76-0697-8584-a17d7a5aef23"},"source":"Now some preparation for tf session: Placeholders for data to be feed in. \nThen we build our NN with one hidden layer."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"362c8c22-09c2-ef76-b68e-8f5377a927a9"},"outputs":[],"source":"# place holder for inputs. feed in later\nx = tf.placeholder(\"float\", [None, nFeature])\n\n# take 'nFeautre' features to 'N2' nodes in hidden layer\nN2 = 100\nw1 = tf.Variable(tf.truncated_normal([nFeature, N2], stddev=0.05, name = 'w1')) # tf.Variable(tf.random_normal([nFeature, N2], stddev=.05, name='w1'))\nb1 = tf.Variable(tf.constant(0., shape=[N2])) #b1 = tf.Variable(tf.zeros([N2]))\n\n# calculate activations \n#hidden_output = tf.nn.softmax(tf.matmul(x, w1) + b1)\nhidden_output = tf.nn.relu(tf.matmul(x, w1) + b1)\n\n# bring from 'N2' nodes to 2 for output: init weights with truncated normal distribution\nw2 = tf.Variable(tf.truncated_normal([N2, 2], stddev=0.025, name = 'w2')) \nb2 = tf.Variable(tf.constant(0., shape=[2])) \n\n# placeholder for labels\ny_ = tf.placeholder(\"float\", [None,2])\n\n# #implement model. these are predicted ys\n#y = tf.nn.softmax(tf.matmul(hidden_output, w2) + b2)\ny = tf.nn.sigmoid(tf.matmul(hidden_output, w2) + b2)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"abbc1e68-5c61-1b90-7719-02b98c53627c"},"outputs":[],"source":"loss = tf.reduce_mean(tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(y, y_, name='xentropy')))"},{"cell_type":"markdown","metadata":{"_cell_guid":"be2b9b43-f1a4-fb62-bfe9-5361ed3e71fc"},"source":"Create an optimizer op, and a train step which minimizes the weight and bias vars. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"00e9adab-e1bf-63b0-8cb7-26554220f08e"},"outputs":[],"source":"opt = tf.train.AdamOptimizer(learning_rate=1e-4)\ntrain_step = opt.minimize(loss, var_list=[w1,b1,w2,b2])"},{"cell_type":"markdown","metadata":{"_cell_guid":"7d8c879e-6f8f-72f4-2c2c-10e2247d63fc"},"source":"Create a way to assess the accuracy of our prediction:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b7c59dc-1073-e236-c7bf-046cad795cf9"},"outputs":[],"source":"tf_correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\ntf_accuracy = tf.reduce_mean(tf.cast(tf_correct_prediction, \"float\"))"},{"cell_type":"markdown","metadata":{"_cell_guid":"8077db30-5f4c-d94f-4bc6-79463aac8f91"},"source":"Function to get batches for training. Start tf session and initialize variables. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"730cb9a9-da5f-5e6e-59fd-3e2db56440e6"},"outputs":[],"source":"def get_mini_batch(x,y):\n\trows=np.random.choice(x.shape[0], 100)\n\treturn x[rows], y[rows]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"037d6bc1-f632-b8d7-efb7-4e9b6b1366e5"},"outputs":[],"source":"# Start an interactive session\nsess = tf.InteractiveSession()\nsummary_writer = tf.train.SummaryWriter('voices')\n# summary_writer = tf.train.SummaryWriter('voices', sess.graph)\n\n# # init all vars\ninit = tf.global_variables_initializer()\nsess.run(init)"},{"cell_type":"markdown","metadata":{"_cell_guid":"bdd28995-d6cf-848f-23ba-3c99839e6472"},"source":"Now, it is time to train. Run a loop for ntrials iterations. Feed the training data into the train_step. Then print our accuracy on the test set and the corresponding value of the loss function."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f3801aa5-73d2-6974-c8de-99f5791d7579"},"outputs":[],"source":"ntrials = 10000\nfor i in range(ntrials):\n    # get mini batch\n    a,b=get_mini_batch(x_train,y_train)\n    # run train step, feeding arrays of 100 rows each time\n    _, cost =sess.run([train_step,loss], feed_dict={x: a, y_: b})\n    if i%100 ==0:\n        trainAccuracy = tf_accuracy.eval(feed_dict={x: a, y_: b})  \n        print(\"epoch is {0} and cost is {1} with train accuracy {2}\".format(i, cost, trainAccuracy))    \t"},{"cell_type":"markdown","metadata":{"_cell_guid":"84479181-b723-2fc7-3908-91ad4ca9bbf5"},"source":"Check accuracy on test set and close the session"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"56faa7fd-f7cc-c7fa-208d-5c0d59a55971"},"outputs":[],"source":"result = tf_accuracy.eval(feed_dict={x: x_test, y_: y_test})\n\nprint(\"Test accuracy: {}\".format(result))\nsess.close()"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}