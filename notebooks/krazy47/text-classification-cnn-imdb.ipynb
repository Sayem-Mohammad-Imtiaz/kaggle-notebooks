{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 参考 https://github.com/AnthonyK97/Text-Classification-on-IMDB\n# https://github.com/AnthonyK97/Text-Classification-on-IMDB/blob/main/2%20CNN%2BGlove.ipynb\n\nimport os\nimport sys\nimport random\nimport time\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom collections import OrderedDict\nimport re, string\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\ndef seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    # some cudnn methods can be random even after fixing the seed\n    # unless you tell it to be deterministic\n    torch.backends.cudnn.deterministic = True\n    \n!mkdir ./model_bakup/\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nclass CFG:\n    batch_size = 20\n    lr = 0.02\n    eval_step_num = 50\n    mid_eval = False\n    best_eval_acc = 0.0\n    model_output_dir = './model_bakup/'\n    seed = 2033\n    use_ema = False\n    use_adversial_training = False\n    \nDEBUG_RUN = True\n\nglobal_start_t = time.time()\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:35:07.212405Z","iopub.execute_input":"2021-05-31T11:35:07.212891Z","iopub.status.idle":"2021-05-31T11:35:09.622635Z","shell.execute_reply.started":"2021-05-31T11:35:07.21276Z","shell.execute_reply":"2021-05-31T11:35:09.621473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(seed=42)\n\nimdb_data = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\nimdb_data['sentiment'] = imdb_data['sentiment'].map({'positive': 1, 'negative': 0})\nprint('before drop_duplicates, imdb_data.shape: ', imdb_data.shape)\nimdb_data = imdb_data.drop_duplicates()\nprint('after drop_duplicates, imdb_data.shape: ', imdb_data.shape)\nimdb_data = imdb_data.sample(30000)\nprint('after sample, imdb_data.shape: ', imdb_data.shape)\nimdb_data = imdb_data.sample(len(imdb_data)).reset_index(drop=True)  # shuffle\n\nimdb_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:35:09.624968Z","iopub.execute_input":"2021-05-31T11:35:09.625415Z","iopub.status.idle":"2021-05-31T11:35:11.212042Z","shell.execute_reply.started":"2021-05-31T11:35:09.625354Z","shell.execute_reply":"2021-05-31T11:35:11.210897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_WORDS = 10000   # 仅考虑最高频的10000个词\nMAX_LEN = 200\nword_count_dict = {}\n\ndef clean_text(text):\n    lowercase = text.lower().replace('\\n', ' ')\n    stripped_html = re.sub('<br />', ' ', lowercase)\n    cleaned_punctuation = re.sub('[%s]'%re.escape(string.punctuation), '', stripped_html)\n    return cleaned_punctuation\n\nfor review in imdb_data['review'].values:\n    cleaned_text = clean_text(review)\n    for word in cleaned_text.split(' '):\n        word_count_dict[word] = word_count_dict.get(word, 0) + 1\n            \ndf_word_dict = pd.DataFrame(pd.Series(word_count_dict, name='count'))\ndf_word_dict = df_word_dict.sort_values(by='count', ascending=False)\n\ndf_word_dict = df_word_dict[:MAX_WORDS-2]     # 总共取前max_words-2个词\ndf_word_dict['word_id'] = range(2, MAX_WORDS)\n\nword_id_dict = df_word_dict['word_id'].to_dict()\nword_id_dict['<unknown>'] = 0\nword_id_dict['<padding>'] = 1\n\ndf_word_dict.head(15)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:35:11.215819Z","iopub.execute_input":"2021-05-31T11:35:11.216143Z","iopub.status.idle":"2021-05-31T11:35:16.180096Z","shell.execute_reply.started":"2021-05-31T11:35:11.216113Z","shell.execute_reply":"2021-05-31T11:35:16.178601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pad(data_list, pad_length):\n    padded_list = data_list.copy()\n    \n    if len(data_list) > pad_length:\n        padded_list = data_list[-pad_length:]\n        \n    if len(data_list) < pad_length:\n        padded_list = [1] * (pad_length-len(data_list)) + data_list\n        \n    return padded_list\n\ndef text_to_token(text):\n    cleaned_text = clean_text(text)\n    word_token_list = [word_id_dict.get(word, 0) for word in cleaned_text.split(' ')]\n    pad_list = pad(word_token_list, MAX_LEN)\n    token = ' '.join([str(x) for x in pad_list])\n    return token\n            \nprocess_start_t = time.time()\nprint('start processing...')\nimdb_data['review_tokens'] = imdb_data['review'].map(text_to_token)\nprint('ok, cost time: ', time.time()-process_start_t)\nimdb_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:35:16.18257Z","iopub.execute_input":"2021-05-31T11:35:16.183254Z","iopub.status.idle":"2021-05-31T11:35:21.345252Z","shell.execute_reply.started":"2021-05-31T11:35:16.183186Z","shell.execute_reply":"2021-05-31T11:35:21.344203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(imdb_data['review'].values[0])","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:35:21.346906Z","iopub.execute_input":"2021-05-31T11:35:21.347371Z","iopub.status.idle":"2021-05-31T11:35:21.35509Z","shell.execute_reply.started":"2021-05-31T11:35:21.347325Z","shell.execute_reply":"2021-05-31T11:35:21.352405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_NUM = 15000\nimdb_data_test = imdb_data.iloc[:5000]\nimdb_data_valid = imdb_data.iloc[5000:10000]\nimdb_data_train = imdb_data.iloc[10000:TRAIN_NUM+10000]\n\nif DEBUG_RUN:\n    SAMPLE_NUM = 3000\n    imdb_data_test = imdb_data_test.sample(SAMPLE_NUM)\n    imdb_data_valid = imdb_data_valid.sample(SAMPLE_NUM)\n    imdb_data_train = imdb_data_train.sample(2*SAMPLE_NUM)\n\nprint(f'imdb_data_train.shape: {imdb_data_train.shape}, imdb_data_valid.shape: {imdb_data_valid.shape}, '\n      f'imdb_data_test.shape: {imdb_data_test.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:35:21.356778Z","iopub.execute_input":"2021-05-31T11:35:21.357298Z","iopub.status.idle":"2021-05-31T11:35:21.380005Z","shell.execute_reply.started":"2021-05-31T11:35:21.357246Z","shell.execute_reply":"2021-05-31T11:35:21.378646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg = CFG()\nseed_everything(seed=cfg.seed)\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:35:21.381549Z","iopub.execute_input":"2021-05-31T11:35:21.382027Z","iopub.status.idle":"2021-05-31T11:35:21.394242Z","shell.execute_reply.started":"2021-05-31T11:35:21.381979Z","shell.execute_reply":"2021-05-31T11:35:21.39282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class imdbDataset(Dataset):\n    def __init__(self, data_df):\n        self.data_df = data_df\n        \n    def __len__(self):\n        return len(self.data_df)\n    \n    def __getitem__(self, index):\n        label = self.data_df.iloc[index]['sentiment']\n        label = torch.tensor([float(label)], dtype=torch.float, device=device)\n        \n        tokens = self.data_df.iloc[index]['review_tokens']\n        feature = torch.tensor([int(x) for x in tokens.split(' ')], dtype=torch.long, device=device)\n            \n        return feature, label\n    \ndef generate_data_iter(cfg):\n    global imdb_data_train, imdb_data_valid, imdb_data_test\n    ds_train = imdbDataset(imdb_data_train)\n    ds_valid = imdbDataset(imdb_data_valid)\n    ds_test = imdbDataset(imdb_data_test)\n    print('len of ds_train: ', len(ds_train), 'len of ds_valid: ', len(ds_valid),\n          'len of ds_test: ', len(ds_test))\n\n    dl_train = DataLoader(ds_train, batch_size=cfg.batch_size, shuffle=True, num_workers=0)\n    dl_valid = DataLoader(ds_valid, batch_size=cfg.batch_size, shuffle=False, num_workers=0)\n    dl_test = DataLoader(ds_test, batch_size=cfg.batch_size, shuffle=False, num_workers=0)\n    return dl_train, dl_valid, dl_test\n\ndl_train, dl_valid, dl_test = generate_data_iter(cfg)\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:35:21.398981Z","iopub.execute_input":"2021-05-31T11:35:21.399439Z","iopub.status.idle":"2021-05-31T11:35:21.414496Z","shell.execute_reply.started":"2021-05-31T11:35:21.399399Z","shell.execute_reply":"2021-05-31T11:35:21.412867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class model_EMA:\n    '''\n    # https://zhuanlan.zhihu.com/p/68748778\n    Example\n    # 初始化\n    ema = EMA(model, 0.999)\n\n    # 训练阶段，更新完参数后\n\n    '''\n    def __init__(self, model, decay=0.99):\n        self.model = model\n        self.decay = decay\n        self.registered = False\n        self.shadow = {}\n        self.backup = {}\n\n    def is_registered(self):\n        return self.registered\n\n    def register(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = param.data.clone()\n        self.registered = True\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                assert name in self.shadow\n                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n                self.shadow[name] = new_average.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                assert name in self.shadow\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                assert name in self.backup\n                param.data = self.backup[name]\n        self.backup = {}\n        \nclass FGM():\n    '''\n    Example\n    # 初始化\n    fgm = FGM(model,epsilon=1,emb_name='word_embeddings.')\n    for batch_input, batch_label in data:\n        # 正常训练\n        loss = model(batch_input, batch_label)\n        loss.backward() # 反向传播，得到正常的grad\n        # 对抗训练\n        fgm.attack() # 在embedding上添加对抗扰动\n        #model.zero_grad()  # 如果需要两次回传梯度不累加, 只使用后面添加扰动之后的得到的梯度，则去掉该行的注释！\n        loss_adv = model(batch_input, batch_label)\n        loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度\n        fgm.restore() # 恢复embedding参数\n        # 梯度下降，更新参数\n        optimizer.step()\n        model.zero_grad()\n    '''\n    def __init__(self, model, emb_name, epsilon=1.0, adv_random=False):\n        # emb_name这个参数要换成你模型中embedding的参数名\n        self.model = model\n        self.epsilon = epsilon\n        self.emb_name = emb_name\n        self.adv_random = adv_random\n        self.backup = {}\n\n    def attack(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and self.emb_name in name:\n                #print('found an param: ', name)\n                self.backup[name] = param.data.clone()\n                norm = torch.norm(param.grad)\n                #print('in attack() norm is ', norm)\n                #print('param.data: ', param.data, 'param.grad: ', param.grad)\n                #print('in attack() norm.shape is ', norm.shape, 'param.data.shape: ', param.data.shape, 'param.grad.shape: ', param.grad.shape)\n                if norm!=0 and not torch.isnan(norm):\n                    epsilon = self.epsilon\n                    if self.adv_random:\n                        epsilon *= random.uniform(0.5, 1.5)\n                    r_at = epsilon * param.grad / norm\n                    #r_at = 0.1 * random.uniform(0.5, 1.5) * param.grad\n                    #r_at = 0.1 * param.grad\n                    param.data.add_(r_at)\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and self.emb_name in name:\n                assert name in self.backup\n                param.data = self.backup[name]\n        self.backup = {}\n        \nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:35:21.417229Z","iopub.execute_input":"2021-05-31T11:35:21.419081Z","iopub.status.idle":"2021-05-31T11:35:21.441812Z","shell.execute_reply.started":"2021-05-31T11:35:21.41905Z","shell.execute_reply":"2021-05-31T11:35:21.440237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EMBEDDING_DIM = 100\n\nclass CNN_Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.embedding = nn.Embedding(num_embeddings=MAX_WORDS, embedding_dim=EMBEDDING_DIM, padding_idx=1)\n        \n        self.conv = nn.Sequential()\n        self.conv.add_module('conv_1', nn.Conv1d(in_channels=EMBEDDING_DIM, out_channels=16, kernel_size=5))\n        self.conv.add_module('pool_1', nn.MaxPool1d(kernel_size=2))\n        self.conv.add_module('relu_1', nn.ReLU())\n        self.conv.add_module('conv_2', nn.Conv1d(in_channels=16, out_channels=128, kernel_size=2))\n        self.conv.add_module('pool_2', nn.MaxPool1d(kernel_size=2))\n        self.conv.add_module('relu_2', nn.ReLU())\n        \n        self.dense = nn.Sequential()\n        self.dense.add_module('flatten', nn.Flatten())\n        self.dense.add_module('linear', nn.Linear(6144, 1))\n        self.dense.add_module('sigmoid', nn.Sigmoid())\n        \n    def forward(self, x):\n        x = self.embedding(x).transpose(1, 2)\n        x = self.conv(x)\n        y = self.dense(x)\n        return y\n    \nmodel = CNN_Net()\nprint(model)\nmodel.to(device)     \n\nmodel_param_num = sum(p.numel() for p in model.parameters())\nmodel_trainable_param_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint('model_param_num: ', model_param_num, 'model_trainable_param_num: ', \n      model_trainable_param_num)\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:35:21.44402Z","iopub.execute_input":"2021-05-31T11:35:21.445014Z","iopub.status.idle":"2021-05-31T11:35:26.590213Z","shell.execute_reply.started":"2021-05-31T11:35:21.44492Z","shell.execute_reply":"2021-05-31T11:35:26.588128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy(y_pred, y_true):\n    if type(y_pred)==list:\n        y_pred = np.array(y_pred)\n    y_pred = (y_pred > 0.5)\n    if type(y_true)==list:\n        y_true = np.array(y_true)\n    acc = (y_pred==y_true).mean()\n    return acc\n\ndef evaluate(model, dl_test, device):\n    global cfg\n    model.eval()\n    \n    y_true_lst, y_pred_lst = [], []\n    with torch.no_grad():\n        for step, batch in enumerate(dl_test):\n            feature, label = batch\n            feature, label = feature.to(device), label.to(device)\n            y_pred = model(feature)\n            y_pred_lst += list(y_pred.detach().cpu().numpy())\n            y_true_lst += list(label.detach().cpu().numpy())\n            \n    model.train() # 恢复模型为训练状态\n    acc = accuracy(y_pred_lst, y_true_lst)\n\n    return acc\n    \ndef train(model, dl_train, optimizer, loss_func, device):\n    global cfg, global_step_num, global_best_valid_acc, dl_valid,  model_ema, fgm\n    model.train()  # 将模型置为训练状态\n    \n    y_true_lst, y_pred_lst = [], []\n    for step, batch in enumerate(dl_train):\n        global_step_num += 1\n        feature, label = batch\n        feature, label = feature.to(device), label.to(device)\n        y_pred = model(feature)\n        train_loss = loss_func(y_pred, label)\n        y_pred_lst += list(y_pred.detach().cpu().numpy())\n        y_true_lst += list(label.detach().cpu().numpy())\n        train_loss.backward()\n        optimizer.step()\n        model.zero_grad()\n        \n        if cfg.mid_eval and (global_step_num % cfg.eval_step_num == 0):\n            valid_acc = evaluate(model, dl_valid, device)\n            print(f'step_num: {global_step_num}, valid_acc: {valid_acc:.5f}')\n            if valid_acc > global_best_valid_acc:\n                global_best_valid_acc = valid_acc\n                print(f'step_num: {global_step_num}, get new best val_acc: {valid_acc:.5f}, save the model now!')                \n                torch.save(model.state_dict(), os.path.join(cfg.model_output_dir, 'best_step_model.pth'))\n        \n    acc = accuracy(y_pred_lst, y_true_lst)\n    return acc\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:35:26.591883Z","iopub.execute_input":"2021-05-31T11:35:26.592338Z","iopub.status.idle":"2021-05-31T11:35:26.610016Z","shell.execute_reply.started":"2021-05-31T11:35:26.592282Z","shell.execute_reply":"2021-05-31T11:35:26.608572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"global_best_train_acc, global_best_valid_acc = 0.0, 0.0\nglobal_train_acc = 0.0\nglobal_step_num = 0\n\nepochs = 10\n# optimizer=torch.optim.Adagrad(model.parameters(), lr=0.06)\n# optimizer=torch.optim.Adadelta(model.parameters(), lr=10.0)\noptimizer=torch.optim.Adam(model.parameters(), lr=0.007, weight_decay=1e-5)\n# optimizer=torch.optim.AdamW(model.parameters(), lr=0.007, weight_decay=0.01)\nloss_func = nn.BCELoss()\n\nmodel_ema = None\nfgm = None\nif cfg.use_ema:\n    model_ema = model_EMA(model, decay=0.999)\n    model_ema.register()\n    \nif cfg.use_adversial_training:\n    fgm = FGM(model, 'embedding', epsilon=1.0, adv_random=True)\n    \nfor epoch in range(epochs):\n    train_acc = train(model, dl_train, optimizer, loss_func, device)\n    valid_acc = evaluate(model, dl_valid, device)\n    test_acc = evaluate(model, dl_test, device)\n    print(f'in epoch: {epoch}, train_acc: {train_acc:.5f}, valid_acc: {valid_acc:.5f}, test_acc: {test_acc:.5f}')\n    if train_acc > global_best_train_acc:\n        global_best_train_acc = train_acc\n    if valid_acc > global_best_valid_acc:\n        global_best_valid_acc = valid_acc\n        global_train_acc = train_acc\n        print(f'at the end of epoch, global_step_num: {global_step_num} get new best_valid_acc: {valid_acc:.5f}, save the model now!')\n        torch.save(model.state_dict(), os.path.join(cfg.model_output_dir, 'best_step_model.pth'))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-31T11:35:26.612296Z","iopub.execute_input":"2021-05-31T11:35:26.613251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CNN_Net()\nmodel.to(device)\n\nmodel.load_state_dict(torch.load(os.path.join(cfg.model_output_dir, 'best_step_model.pth')))\ntest_acc = evaluate(model, dl_test, device)\nprint(f'final test_acc: {test_acc:.5f}, best_val_acc: {global_best_valid_acc:.5f}, '\n      f'train_acc: {global_train_acc:.5f}, best_train_acc: {global_best_train_acc:.5f}')\n\nprint('total finished, cost time: ', time.time() - global_start_t)\n\n# no mid_eval  batch_size=20   lr=0.007  Adam  weight_decay=0.0   seed=2033\n\n##################################################################################################\n\n# no mid_eval  batch_size=20   lr=0.007  Adam  weight_decay=0.0   seed=2032\n# final test_acc: 0.81667, best_val_acc: 0.82033, train_acc: 0.99050, best_train_acc: 0.99267\n# total finished, cost time:  96.26502442359924\n\n# no mid_eval  batch_size=20   lr=0.007  Adam  weight_decay=0.01  seed=2032  不收敛！\n# final test_acc: 0.49800, best_val_acc: 0.50400, train_acc: 0.49083, best_train_acc: 0.50783\n# total finished, cost time:  95.44412612915039\n\n# no mid_eval  batch_size=20   lr=0.007  Adam  weight_decay=1e-3  seed=2032\n# final test_acc: 0.84100, best_val_acc: 0.85033, train_acc: 0.89633, best_train_acc: 0.91183\n# total finished, cost time:  96.47475743293762\n\n# no mid_eval  batch_size=20   lr=0.007  Adam  weight_decay=1e-4  seed=2032\n# final test_acc: 0.82833, best_val_acc: 0.82433, train_acc: 0.91117, best_train_acc: 0.95617\n# total finished, cost time:  135.2124011516571\n    \n# no mid_eval  batch_size=20   lr=0.007  Adam  weight_decay=1e-5  seed=2032\n# final test_acc: 0.82567, best_val_acc: 0.83100, train_acc: 0.88600, best_train_acc: 0.98067\n# total finished, cost time:  136.57839918136597","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with mid_eval  batch_size=4  lr=0.007\n# final test_acc: 0.49967, best_val_acc: 0.50733, train_acc: 0.50367, best_train_acc: 0.51283\n# total finished, cost time:  188.66995453834534\n    \n# with mid_eval  batch_size=4  lr=0.006\n# final test_acc: 0.51067, best_val_acc: 0.51967, train_acc: 0.49783, best_train_acc: 0.50883\n# total finished, cost time:  264.76468110084534\n\n# with mid_eval  batch_size=4  lr=0.001\n# final test_acc: 0.81800, best_val_acc: 0.82767, train_acc: 0.99233, best_train_acc: 0.99467\n# total finished, cost time:  265.68268299102783\n    \n# with mid_eval  batch_size=8  lr=0.006\n# final test_acc: 0.54067, best_val_acc: 0.53467, train_acc: 0.00000, best_train_acc: 0.52800\n# total finished, cost time:  433.7452108860016\n\n# with mid_eval  batch_size=8  lr=0.005\n# final test_acc: 0.52200, best_val_acc: 0.52833, train_acc: 0.00000, best_train_acc: 0.63483\n# total finished, cost time:  431.8099343776703\n\n# with mid_eval  batch_size=8  lr=0.004\n# final test_acc: 0.82100, best_val_acc: 0.82467, train_acc: 0.00000, best_train_acc: 0.99100\n# total finished, cost time:  440.1590278148651\n    \n# with mid_eval  batch_size=8  lr=0.003\n# final test_acc: 0.82333, best_val_acc: 0.83333, train_acc: 0.00000, best_train_acc: 0.99450\n# total finished, cost time:  537.3426532745361\n    \n# with mid_eval  batch_size=8  lr=0.002\n# final test_acc: 0.82400, best_val_acc: 0.83167, train_acc: 0.00000, best_train_acc: 0.99333\n# total finished, cost time:  433.7970836162567\n\n# with mid_eval  batch_size=8  lr=0.001\n# final test_acc: 0.81400, best_val_acc: 0.82200, train_acc: 0.00000, best_train_acc: 1.00000\n# total finished, cost time:  434.2089605331421\n    \n# with mid_eval  batch_size=20   lr=0.005\n# final test_acc: 0.82900, best_val_acc: 0.82900, train_acc: 0.00000, best_train_acc: 1.00000\n# total finished, cost time:  194.9231276512146\n\n# with mid_eval  batch_size=20   lr=0.007  AdamW  weight_decay=0.01\n# final test_acc: 0.82500, best_val_acc: 0.82467, train_acc: 0.97900, best_train_acc: 0.98717\n# total finished, cost time:  97.36473631858826\n\n\n    \n# with mid_eval  batch_size=32   lr=0.005\n# final test_acc: 0.81900, best_val_acc: 0.84133, train_acc: 0.57733, best_train_acc: 0.99217\n# total finished, cost time:  143.55632901191711","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adadelta  lr=1.0  seed=2036  no mid_eval\n# final test_acc: 0.72440, best_val_acc: 0.72220, train_acc: 0.89160, best_train_acc: 0.90800\n# total finished, cost time:  192.1149570941925\n\n# Adadelta  lr=1.8  seed=2036  no mid_eval\n# final test_acc: 0.77940, best_val_acc: 0.78580, train_acc: 0.83680, best_train_acc: 0.89567\n# total finished, cost time:  191.68706226348877\n    \n# Adadelta  lr=2.0  seed=2036  no mid_eval\n# final test_acc: 0.79520, best_val_acc: 0.79220, train_acc: 0.88280, best_train_acc: 0.89840\n# total finished, cost time:  265.7658565044403\n\n# Adadelta  lr=2.5  seed=2036  no mid_eval\n# final test_acc: 0.77540, best_val_acc: 0.78520, train_acc: 0.82920, best_train_acc: 0.87640\n# total finished, cost time:  191.7624773979187\n    \n# Adadelta  lr=3.0  seed=2036  no mid_eval\n# final test_acc: 0.78020, best_val_acc: 0.78220, train_acc: 0.84933, best_train_acc: 0.86280\n# total finished, cost time:  192.8262016773224\n\n# Adadelta  lr=4.0  seed=2036  no mid_eval\n# final test_acc: 0.74100, best_val_acc: 0.75020, train_acc: 0.80387, best_train_acc: 0.80387\n# total finished, cost time:  192.1702754497528\n\n# Adadelta  lr=5.0  seed=2036  no mid_eval\n# final test_acc: 0.70080, best_val_acc: 0.70720, train_acc: 0.69053, best_train_acc: 0.69053\n# total finished, cost time:  192.71857452392578\n\n# Adadelta  lr=10.0  seed=2036  no mid_eval\n# final test_acc: 0.50840, best_val_acc: 0.50420, train_acc: 0.50393, best_train_acc: 0.50700\n# total finished, cost time:  249.54866433143616","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adagrad  lr=0.03  seed=2036  no mid_eval\n# final test_acc: 0.80940, best_val_acc: 0.81160, train_acc: 0.92600, best_train_acc: 0.93807\n# total finished, cost time:  276.79689049720764\n\n# Adagrad  lr=0.04  seed=2036  no mid_eval\n# final test_acc: 0.81240, best_val_acc: 0.82040, train_acc: 0.90707, best_train_acc: 0.95013\n# total finished, cost time:  261.7913763523102\n\n# Adagrad  lr=0.05  seed=2036  no mid_eval\n# final test_acc: 0.82920, best_val_acc: 0.83240, train_acc: 0.92020, best_train_acc: 0.96613\n# total finished, cost time:  185.5982859134674\n\n# in epoch: 0, train_acc: 0.53927, valid_acc: 0.64200, test_acc: 0.64580\n# get new best_valid_acc: 0.64200, save the model now!\n# in epoch: 1, train_acc: 0.73927, valid_acc: 0.78620, test_acc: 0.78740\n# get new best_valid_acc: 0.78620, save the model now!\n# in epoch: 2, train_acc: 0.82847, valid_acc: 0.81080, test_acc: 0.80860\n# get new best_valid_acc: 0.81080, save the model now!\n# in epoch: 3, train_acc: 0.86187, valid_acc: 0.82840, test_acc: 0.82320\n# get new best_valid_acc: 0.82840, save the model now!\n# in epoch: 4, train_acc: 0.88567, valid_acc: 0.81700, test_acc: 0.81460\n# in epoch: 5, train_acc: 0.90407, valid_acc: 0.82420, test_acc: 0.82340\n# in epoch: 6, train_acc: 0.92020, valid_acc: 0.83240, test_acc: 0.82920\n# get new best_valid_acc: 0.83240, save the model now!\n# in epoch: 7, train_acc: 0.93720, valid_acc: 0.83040, test_acc: 0.82900\n# in epoch: 8, train_acc: 0.95140, valid_acc: 0.83140, test_acc: 0.82880\n# in epoch: 9, train_acc: 0.96613, valid_acc: 0.82900, test_acc: 0.82960\n\n# Adagrad  lr=0.06  seed=2036  no mid_eval\n# final test_acc: 0.50240, best_val_acc: 0.50160, train_acc: 0.50373, best_train_acc: 0.50387\n# total finished, cost time:  262.0888602733612","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adam  lr=0.005  seed=2036  no mid_eval\n# final test_acc: 0.84520, best_val_acc: 0.85200, train_acc: 0.93880, best_train_acc: 0.99280\n# total finished, cost time:  189.37609314918518\n\n# AdamW  lr=0.005  seed=2036  no mid_eval\n# final test_acc: 0.84800, best_val_acc: 0.85580, train_acc: 0.92647, best_train_acc: 0.99747\n# total finished, cost time:  193.3394296169281","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adam  lr=0.005  seed=2035  no mid_eval\n# final test_acc: 0.84640, best_val_acc: 0.85240, train_acc: 0.92160, best_train_acc: 0.99400\n# total finished, cost time:  199.5630133152008\n\n# AdamW  lr=0.005  seed=2035  mid_eval\n# final test_acc: 0.85720, best_val_acc: 0.85860, train_acc: 0.90340, best_train_acc: 0.99813\n# total finished, cost time:  270.2926092147827","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adagrad lr=0.02  seed=2034\n# final test_acc: 0.77740, best_val_acc: 0.76220, train_acc: 0.93720, best_train_acc: 0.94513\n# total finished, cost time:  282.7268285751343\n\n# AdamW  lr=0.005  seed=2034\n# final test_acc: 0.85700, best_val_acc: 0.85380, train_acc: 0.90427, best_train_acc: 1.00000\n# total finished, cost time:  292.4551508426666\n\n# Adam  lr=0.001  seed=2034\n# final test_acc: 0.80540, best_val_acc: 0.79580, train_acc: 0.96187, best_train_acc: 0.99933\n# total finished, cost time:  289.06110072135925\n\n# Adam  lr=0.003  seed=2034\n# final test_acc: 0.83780, best_val_acc: 0.84600, train_acc: 0.93907, best_train_acc: 1.00000\n# total finished, cost time:  285.3573806285858\n\n# Adam  lr=0.005  seed=2034\n# final test_acc: 0.85160, best_val_acc: 0.85500, train_acc: 0.89727, best_train_acc: 0.99993\n# total finished, cost time:  284.3437497615814\n\n# Adam  lr=0.007  seed=2034\n# final test_acc: 0.83020, best_val_acc: 0.83620, train_acc: 0.87953, best_train_acc: 0.98880\n# total finished, cost time:  283.920378446579\n\n# Adam  lr=0.01  seed=2034\n# final test_acc: 0.50240, best_val_acc: 0.50160, train_acc: 0.50433, best_train_acc: 0.50433\n# total finished, cost time:  267.2974200248718","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}