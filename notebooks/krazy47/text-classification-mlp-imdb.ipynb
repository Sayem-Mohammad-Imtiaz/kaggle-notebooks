{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 参考 https://github.com/AnthonyK97/Text-Classification-on-IMDB\n# https://github.com/AnthonyK97/Text-Classification-on-IMDB/blob/main/2%20CNN%2BGlove.ipynb\n\nimport os\nimport sys\nimport random\nimport time\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom collections import OrderedDict\nimport re, string\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\ndef seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    # some cudnn methods can be random even after fixing the seed\n    # unless you tell it to be deterministic\n    torch.backends.cudnn.deterministic = True\n    \n!mkdir ./model_bakup/\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nclass CFG:\n    batch_size = 20\n    lr = 0.02\n    eval_step_num = 50\n    mid_eval = False\n    best_eval_acc = 0.0\n    model_output_dir = './model_bakup/'\n    seed = 2032\n    use_ema = False\n    use_adversial_training = False\n    \nDEBUG_RUN = True\n\nglobal_start_t = time.time()\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:22:55.235105Z","iopub.execute_input":"2021-06-02T10:22:55.235511Z","iopub.status.idle":"2021-06-02T10:22:57.071165Z","shell.execute_reply.started":"2021-06-02T10:22:55.235413Z","shell.execute_reply":"2021-06-02T10:22:57.070237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(seed=42)\n\nimdb_data = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\nimdb_data['sentiment'] = imdb_data['sentiment'].map({'positive': 1, 'negative': 0})\nprint('before drop_duplicates, imdb_data.shape: ', imdb_data.shape)\nimdb_data = imdb_data.drop_duplicates()\nprint('after drop_duplicates, imdb_data.shape: ', imdb_data.shape)\nimdb_data = imdb_data.sample(30000)\nprint('after sample, imdb_data.shape: ', imdb_data.shape)\nimdb_data = imdb_data.sample(len(imdb_data)).reset_index(drop=True)  # shuffle\n\nimdb_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:22:57.073009Z","iopub.execute_input":"2021-06-02T10:22:57.073371Z","iopub.status.idle":"2021-06-02T10:22:58.603608Z","shell.execute_reply.started":"2021-06-02T10:22:57.07333Z","shell.execute_reply":"2021-06-02T10:22:58.602305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_WORDS = 10000   # 仅考虑最高频的10000个词\nMAX_LEN = 200\nword_count_dict = {}\n\ndef clean_text(text):\n    lowercase = text.lower().replace('\\n', ' ')\n    stripped_html = re.sub('<br />', ' ', lowercase)\n    cleaned_punctuation = re.sub('[%s]'%re.escape(string.punctuation), '', stripped_html)\n    return cleaned_punctuation\n\nfor review in imdb_data['review'].values:\n    cleaned_text = clean_text(review)\n    for word in cleaned_text.split(' '):\n        word_count_dict[word] = word_count_dict.get(word, 0) + 1\n            \ndf_word_dict = pd.DataFrame(pd.Series(word_count_dict, name='count'))\ndf_word_dict = df_word_dict.sort_values(by='count', ascending=False)\n\ndf_word_dict = df_word_dict[:MAX_WORDS-2]     # 总共取前max_words-2个词\ndf_word_dict['word_id'] = range(2, MAX_WORDS)\n\nword_id_dict = df_word_dict['word_id'].to_dict()\nword_id_dict['<unknown>'] = 0\nword_id_dict['<padding>'] = 1\n\ndf_word_dict.head(15)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:22:58.605403Z","iopub.execute_input":"2021-06-02T10:22:58.605763Z","iopub.status.idle":"2021-06-02T10:23:02.677465Z","shell.execute_reply.started":"2021-06-02T10:22:58.605728Z","shell.execute_reply":"2021-06-02T10:23:02.676537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pad(data_list, pad_length):\n    padded_list = data_list.copy()\n    \n    if len(data_list) > pad_length:\n        padded_list = data_list[-pad_length:]\n        \n    if len(data_list) < pad_length:\n        padded_list = [1] * (pad_length-len(data_list)) + data_list\n        \n    return padded_list\n\ndef text_to_token(text):\n    cleaned_text = clean_text(text)\n    word_token_list = [word_id_dict.get(word, 0) for word in cleaned_text.split(' ')]\n    pad_list = pad(word_token_list, MAX_LEN)\n    token = ' '.join([str(x) for x in pad_list])\n    return token\n            \nprocess_start_t = time.time()\nprint('start processing...')\nimdb_data['review_tokens'] = imdb_data['review'].map(text_to_token)\nprint('ok, cost time: ', time.time()-process_start_t)\nimdb_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:23:02.679194Z","iopub.execute_input":"2021-06-02T10:23:02.679557Z","iopub.status.idle":"2021-06-02T10:23:06.366413Z","shell.execute_reply.started":"2021-06-02T10:23:02.679512Z","shell.execute_reply":"2021-06-02T10:23:06.36547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_NUM = 15000\nimdb_data_test = imdb_data.iloc[:5000]\nimdb_data_valid = imdb_data.iloc[5000:10000]\nimdb_data_train = imdb_data.iloc[10000:TRAIN_NUM+10000]\n\nif DEBUG_RUN:\n    SAMPLE_NUM = 3000\n    imdb_data_test = imdb_data_test.sample(SAMPLE_NUM)\n    imdb_data_valid = imdb_data_valid.sample(SAMPLE_NUM)\n    imdb_data_train = imdb_data_train.sample(2*SAMPLE_NUM)\n\nprint(f'imdb_data_train.shape: {imdb_data_train.shape}, imdb_data_valid.shape: {imdb_data_valid.shape}, '\n      f'imdb_data_test.shape: {imdb_data_test.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:23:06.367754Z","iopub.execute_input":"2021-06-02T10:23:06.3681Z","iopub.status.idle":"2021-06-02T10:23:06.381859Z","shell.execute_reply.started":"2021-06-02T10:23:06.368063Z","shell.execute_reply":"2021-06-02T10:23:06.380833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg = CFG()\nseed_everything(seed=cfg.seed)\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:23:06.383214Z","iopub.execute_input":"2021-06-02T10:23:06.383667Z","iopub.status.idle":"2021-06-02T10:23:06.391399Z","shell.execute_reply.started":"2021-06-02T10:23:06.383633Z","shell.execute_reply":"2021-06-02T10:23:06.390285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class imdbDataset(Dataset):\n    def __init__(self, data_df):\n        self.data_df = data_df\n        \n    def __len__(self):\n        return len(self.data_df)\n    \n    def __getitem__(self, index):\n        label = self.data_df.iloc[index]['sentiment']\n        label = torch.tensor([float(label)], dtype=torch.float, device=device)\n        \n        tokens = self.data_df.iloc[index]['review_tokens']\n        feature = torch.tensor([int(x) for x in tokens.split(' ')], dtype=torch.long, device=device)\n            \n        return feature, label\n    \ndef generate_data_iter(cfg):\n    global imdb_data_train, imdb_data_valid, imdb_data_test\n    ds_train = imdbDataset(imdb_data_train)\n    ds_valid = imdbDataset(imdb_data_valid)\n    ds_test = imdbDataset(imdb_data_test)\n    print('len of ds_train: ', len(ds_train), 'len of ds_valid: ', len(ds_valid),\n          'len of ds_test: ', len(ds_test))\n\n    dl_train = DataLoader(ds_train, batch_size=cfg.batch_size, shuffle=True, num_workers=0)\n    dl_valid = DataLoader(ds_valid, batch_size=cfg.batch_size, shuffle=False, num_workers=0)\n    dl_test = DataLoader(ds_test, batch_size=cfg.batch_size, shuffle=False, num_workers=0)\n    return dl_train, dl_valid, dl_test\n\ndl_train, dl_valid, dl_test = generate_data_iter(cfg)\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:23:06.39324Z","iopub.execute_input":"2021-06-02T10:23:06.393613Z","iopub.status.idle":"2021-06-02T10:23:06.406216Z","shell.execute_reply.started":"2021-06-02T10:23:06.393578Z","shell.execute_reply":"2021-06-02T10:23:06.405511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class model_EMA:\n    '''\n    # https://zhuanlan.zhihu.com/p/68748778\n    Example\n    # 初始化\n    ema = EMA(model, 0.999)\n\n    # 训练阶段，更新完参数后\n\n    '''\n    def __init__(self, model, decay=0.99):\n        self.model = model\n        self.decay = decay\n        self.registered = False\n        self.shadow = {}\n        self.backup = {}\n\n    def is_registered(self):\n        return self.registered\n\n    def register(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = param.data.clone()\n        self.registered = True\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                assert name in self.shadow\n                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n                self.shadow[name] = new_average.clone()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                assert name in self.shadow\n                self.backup[name] = param.data\n                param.data = self.shadow[name]\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                assert name in self.backup\n                param.data = self.backup[name]\n        self.backup = {}\n        \nclass FGM():\n    '''\n    Example\n    # 初始化\n    fgm = FGM(model,epsilon=1,emb_name='word_embeddings.')\n    for batch_input, batch_label in data:\n        # 正常训练\n        loss = model(batch_input, batch_label)\n        loss.backward() # 反向传播，得到正常的grad\n        # 对抗训练\n        fgm.attack() # 在embedding上添加对抗扰动\n        #model.zero_grad()  # 如果需要两次回传梯度不累加, 只使用后面添加扰动之后的得到的梯度，则去掉该行的注释！\n        loss_adv = model(batch_input, batch_label)\n        loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度\n        fgm.restore() # 恢复embedding参数\n        # 梯度下降，更新参数\n        optimizer.step()\n        model.zero_grad()\n    '''\n    def __init__(self, model, emb_name, epsilon=1.0, adv_random=False):\n        # emb_name这个参数要换成你模型中embedding的参数名\n        self.model = model\n        self.epsilon = epsilon\n        self.emb_name = emb_name\n        self.adv_random = adv_random\n        self.backup = {}\n\n    def attack(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and self.emb_name in name:\n                #print('found an param: ', name)\n                self.backup[name] = param.data.clone()\n                norm = torch.norm(param.grad)\n                #print('in attack() norm is ', norm)\n                #print('param.data: ', param.data, 'param.grad: ', param.grad)\n                #print('in attack() norm.shape is ', norm.shape, 'param.data.shape: ', param.data.shape, 'param.grad.shape: ', param.grad.shape)\n                if norm!=0 and not torch.isnan(norm):\n                    epsilon = self.epsilon\n                    if self.adv_random:\n                        epsilon *= random.uniform(0.5, 1.5)\n                    #r_at = epsilon * param.grad / norm\n                    r_at = 0.1 * random.uniform(0.5, 1.5) * param.grad\n                    #r_at = 0.1 * param.grad\n                    param.data.add_(r_at)\n\n    def restore(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and self.emb_name in name:\n                assert name in self.backup\n                param.data = self.backup[name]\n        self.backup = {}\n        \nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:23:06.408858Z","iopub.execute_input":"2021-06-02T10:23:06.409271Z","iopub.status.idle":"2021-06-02T10:23:06.425437Z","shell.execute_reply.started":"2021-06-02T10:23:06.409233Z","shell.execute_reply":"2021-06-02T10:23:06.424617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EMBEDDING_DIM = 50\n\nclass MLP_Net(nn.Module):\n    def __init__(self, hidden_size=1000):\n        super().__init__()\n        \n        self.embedding = nn.Embedding(num_embeddings=MAX_WORDS, embedding_dim=EMBEDDING_DIM, padding_idx=1)\n        \n#         self.fc = nn.Sequential()\n#         self.fc.add_module('fc_1', nn.Linear(EMBEDDING_DIM*MAX_LEN, 200))\n#         self.fc.add_module('relu_1', nn.ReLU())\n#         self.fc.add_module('fc_2', nn.Linear(200, 500))\n#         self.fc.add_module('relu_2', nn.ReLU())\n#         self.fc.add_module('fc_3', nn.Linear(500, 250))\n#         self.fc.add_module('relu_3', nn.ReLU())\n#         self.fc.add_module('fc_4', nn.Linear(250, 100))\n#         self.fc.add_module('relu_4', nn.ReLU())\n#         self.fc.add_module('fc_5', nn.Linear(100, 1))\n#         self.fc.add_module('sigmoid_1', nn.Sigmoid())\n        \n        self.fc = nn.Sequential()\n        self.fc.add_module('fc_1', nn.Linear(EMBEDDING_DIM*MAX_LEN, hidden_size))\n        self.fc.add_module('relu_1', nn.ReLU())\n        self.fc.add_module('dropout_1', nn.Dropout(p=0.5))\n#         self.fc.add_module('relu_1', nn.LeakyReLU())\n        self.fc.add_module('fc_2', nn.Linear(hidden_size, 1))\n        self.fc.add_module('sigmoid_final', nn.Sigmoid())\n        \n    def forward(self, x):\n        x = self.embedding(x).view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\nmodel = MLP_Net()\nprint(model)\nmodel.to(device)     \nmodel_param_num = sum(p.numel() for p in model.parameters())\nmodel_trainable_param_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint('model_param_num: ', model_param_num, 'model_trainable_param_num: ', \n      model_trainable_param_num)\n\nprint('ok')\n# model_param_num:  10502001 model_trainable_param_num:  10502001","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:23:06.427013Z","iopub.execute_input":"2021-06-02T10:23:06.427559Z","iopub.status.idle":"2021-06-02T10:23:10.797036Z","shell.execute_reply.started":"2021-06-02T10:23:06.427509Z","shell.execute_reply":"2021-06-02T10:23:10.795184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy(y_pred, y_true):\n    if type(y_pred)==list:\n        y_pred = np.array(y_pred)\n    y_pred = (y_pred > 0.5)\n    if type(y_true)==list:\n        y_true = np.array(y_true)\n    acc = (y_pred==y_true).mean()\n    return acc\n\ndef evaluate(model, dl_test, device):\n    global cfg\n    model.eval()\n    \n    y_true_lst, y_pred_lst = [], []\n    with torch.no_grad():\n        for step, batch in enumerate(dl_test):\n            feature, label = batch\n            feature, label = feature.to(device), label.to(device)\n            y_pred = model(feature)\n            y_pred_lst += list(y_pred.detach().cpu().numpy())\n            y_true_lst += list(label.detach().cpu().numpy())\n            \n    model.train() # 恢复模型为训练状态\n    acc = accuracy(y_pred_lst, y_true_lst)\n\n    return acc\n    \ndef train(model, dl_train, optimizer, loss_func, device):\n    global cfg, global_step_num, global_best_valid_acc, dl_valid,  model_ema, fgm\n    model.train()  # 将模型置为训练状态\n    \n    loss_func1 = nn.BCELoss()\n    loss_func2 = nn.MSELoss()\n\n    y_true_lst, y_pred_lst = [], []\n    for step, batch in enumerate(dl_train):\n        global_step_num += 1\n        feature, label = batch\n        feature, label = feature.to(device), label.to(device)\n        y_pred = model(feature)\n        #train_loss = loss_func(y_pred, label)\n        train_loss = 0.5*loss_func1(y_pred, label) + 0.5*loss_func2(y_pred, label)\n        y_pred_lst += list(y_pred.detach().cpu().numpy())\n        y_true_lst += list(label.detach().cpu().numpy())\n        train_loss.backward()\n        if cfg.use_adversial_training:\n            fgm.attack() # 在embedding上添加对抗扰动\n            y_pred = model(feature)\n            #loss_adv = loss_func(y_pred, label)\n            loss_adv = 0.5*loss_func1(y_pred, label) + 0.5*loss_func2(y_pred, label)\n            loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度\n            fgm.restore() # 恢复embedding参数\n            \n        optimizer.step()\n        model.zero_grad()\n        if cfg.use_ema:\n            model_ema.update()\n        \n        if cfg.mid_eval and (global_step_num % cfg.eval_step_num == 0):\n            if cfg.use_ema:\n                model_ema.apply_shadow()\n        \n            valid_acc = evaluate(model, dl_valid, device)\n            print(f'step_num: {global_step_num}, valid_acc: {valid_acc:.5f}')\n            if valid_acc > global_best_valid_acc:\n                global_best_valid_acc = valid_acc\n                print(f'step_num: {global_step_num}, get new best val_acc: {valid_acc:.5f}, save the model now!')                \n                torch.save(model.state_dict(), os.path.join(cfg.model_output_dir, 'best_step_model.pth'))\n                \n            if cfg.use_ema:\n                model_ema.restore()\n        \n    acc = accuracy(y_pred_lst, y_true_lst)\n    return acc\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:23:10.798424Z","iopub.execute_input":"2021-06-02T10:23:10.798775Z","iopub.status.idle":"2021-06-02T10:23:10.816152Z","shell.execute_reply.started":"2021-06-02T10:23:10.798739Z","shell.execute_reply":"2021-06-02T10:23:10.815237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"global_best_train_acc, global_best_valid_acc = 0.0, 0.0\nglobal_train_acc = 0.0\nglobal_step_num = 0\n\nepochs = 150\n#epochs = 3\n# optimizer=torch.optim.Adagrad(model.parameters(), lr=0.06)\n# optimizer=torch.optim.Adadelta(model.parameters(), lr=10.0)\noptimizer=torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-3)\n# optimizer=torch.optim.AdamW(model.parameters(), lr=0.006, weight_decay=0.01)\n# loss_func = nn.BCELoss()\nloss_func = nn.MSELoss()\n\nmodel_ema = None\nfgm = None\nif cfg.use_ema:\n    model_ema = model_EMA(model, decay=0.95)\n    model_ema.register()\n    \nif cfg.use_adversial_training:\n    fgm = FGM(model, 'embedding', epsilon=1.0, adv_random=True)\n    \nfor epoch in range(epochs):\n    #lr_val = random.uniform(0.001, 0.006)\n    #optimizer=torch.optim.Adam(model.parameters(), lr=lr_val, weight_decay=1e-5)\n    train_acc = train(model, dl_train, optimizer, loss_func, device)\n    if cfg.use_ema:\n        model_ema.apply_shadow()\n        \n    valid_acc = evaluate(model, dl_valid, device)\n    test_acc = evaluate(model, dl_test, device)\n    print(f'in epoch: {epoch}, train_acc: {train_acc:.5f}, valid_acc: {valid_acc:.5f}, test_acc: {test_acc:.5f}')\n    if train_acc > global_best_train_acc:\n        global_best_train_acc = train_acc\n    if valid_acc > global_best_valid_acc:\n        global_best_valid_acc = valid_acc\n        global_train_acc = train_acc\n        print(f'at the end of epoch, global_step_num: {global_step_num} get new best_valid_acc: {valid_acc:.5f}, save the model now!')\n        torch.save(model.state_dict(), os.path.join(cfg.model_output_dir, 'best_step_model_1.pth'))\n        \n    if cfg.use_ema:\n        model_ema.restore()\n        \nmodel = MLP_Net()\nmodel.to(device)\n\nmodel.load_state_dict(torch.load(os.path.join(cfg.model_output_dir, 'best_step_model_1.pth')))\ntest_acc = evaluate(model, dl_test, device)\nprint(f'final test_acc: {test_acc:.5f}, best_val_acc: {global_best_valid_acc:.5f}, '\n      f'train_acc: {global_train_acc:.5f}, best_train_acc: {global_best_train_acc:.5f}')\n\n#assert False\n\n# final test_acc: 0.80500, best_val_acc: 0.81967, train_acc: 0.96117, best_train_acc: 0.98283\n# in epoch: 0, train_acc: 0.52233, valid_acc: 0.54767, test_acc: 0.54633\n# at the end of epoch, global_step_num: 300 get new best_valid_acc: 0.54767, save the model now!\n# in epoch: 1, train_acc: 0.68633, valid_acc: 0.58667, test_acc: 0.59533\n# at the end of epoch, global_step_num: 600 get new best_valid_acc: 0.58667, save the model now!\n# in epoch: 2, train_acc: 0.77650, valid_acc: 0.66967, test_acc: 0.68333\n# at the end of epoch, global_step_num: 900 get new best_valid_acc: 0.66967, save the model now!\n# in epoch: 3, train_acc: 0.83233, valid_acc: 0.74067, test_acc: 0.75100\n# at the end of epoch, global_step_num: 1200 get new best_valid_acc: 0.74067, save the model now!\n# in epoch: 4, train_acc: 0.86467, valid_acc: 0.75800, test_acc: 0.77267\n# at the end of epoch, global_step_num: 1500 get new best_valid_acc: 0.75800, save the model now!\n# in epoch: 5, train_acc: 0.89417, valid_acc: 0.76833, test_acc: 0.77333\n# at the end of epoch, global_step_num: 1800 get new best_valid_acc: 0.76833, save the model now!\n# in epoch: 6, train_acc: 0.90850, valid_acc: 0.78033, test_acc: 0.78700\n# at the end of epoch, global_step_num: 2100 get new best_valid_acc: 0.78033, save the model now!\n# in epoch: 7, train_acc: 0.92850, valid_acc: 0.77200, test_acc: 0.77567\n# in epoch: 8, train_acc: 0.93567, valid_acc: 0.76900, test_acc: 0.78400\n# in epoch: 9, train_acc: 0.93367, valid_acc: 0.76267, test_acc: 0.78600\n# in epoch: 10, train_acc: 0.94700, valid_acc: 0.78233, test_acc: 0.79033\n# at the end of epoch, global_step_num: 3300 get new best_valid_acc: 0.78233, save the model now!\n# in epoch: 11, train_acc: 0.95367, valid_acc: 0.77200, test_acc: 0.78000\n# in epoch: 12, train_acc: 0.93933, valid_acc: 0.78367, test_acc: 0.79067\n# at the end of epoch, global_step_num: 3900 get new best_valid_acc: 0.78367, save the model now!\n# in epoch: 13, train_acc: 0.95050, valid_acc: 0.77233, test_acc: 0.79167\n# in epoch: 14, train_acc: 0.96167, valid_acc: 0.76600, test_acc: 0.78133\n# in epoch: 15, train_acc: 0.94517, valid_acc: 0.77500, test_acc: 0.79333\n# in epoch: 16, train_acc: 0.95367, valid_acc: 0.78467, test_acc: 0.79500\n# at the end of epoch, global_step_num: 5100 get new best_valid_acc: 0.78467, save the model now!\n# in epoch: 17, train_acc: 0.96367, valid_acc: 0.79133, test_acc: 0.79167\n# at the end of epoch, global_step_num: 5400 get new best_valid_acc: 0.79133, save the model now!\n# in epoch: 18, train_acc: 0.96150, valid_acc: 0.78433, test_acc: 0.79233\n# in epoch: 19, train_acc: 0.94867, valid_acc: 0.78700, test_acc: 0.79867\n# in epoch: 20, train_acc: 0.96750, valid_acc: 0.79567, test_acc: 0.79967\n# at the end of epoch, global_step_num: 6300 get new best_valid_acc: 0.79567, save the model now!\n# in epoch: 21, train_acc: 0.97100, valid_acc: 0.78300, test_acc: 0.79500\n# in epoch: 22, train_acc: 0.95767, valid_acc: 0.78800, test_acc: 0.79200\n# in epoch: 23, train_acc: 0.94933, valid_acc: 0.78533, test_acc: 0.78667\n# in epoch: 24, train_acc: 0.96950, valid_acc: 0.79867, test_acc: 0.79700\n# at the end of epoch, global_step_num: 7500 get new best_valid_acc: 0.79867, save the model now!\n# in epoch: 25, train_acc: 0.96833, valid_acc: 0.79167, test_acc: 0.79367\n# in epoch: 26, train_acc: 0.95533, valid_acc: 0.78167, test_acc: 0.79333\n# in epoch: 27, train_acc: 0.95950, valid_acc: 0.79000, test_acc: 0.79000\n# in epoch: 28, train_acc: 0.97000, valid_acc: 0.79300, test_acc: 0.79467\n# in epoch: 29, train_acc: 0.96283, valid_acc: 0.79167, test_acc: 0.80133\n# in epoch: 30, train_acc: 0.96533, valid_acc: 0.79233, test_acc: 0.79300\n# in epoch: 31, train_acc: 0.95783, valid_acc: 0.79667, test_acc: 0.79267\n# in epoch: 32, train_acc: 0.96017, valid_acc: 0.79600, test_acc: 0.79633\n# in epoch: 33, train_acc: 0.96950, valid_acc: 0.80633, test_acc: 0.80167\n# at the end of epoch, global_step_num: 10200 get new best_valid_acc: 0.80633, save the model now!\n# in epoch: 34, train_acc: 0.97033, valid_acc: 0.78900, test_acc: 0.80233\n# in epoch: 35, train_acc: 0.96250, valid_acc: 0.77100, test_acc: 0.77833\n# in epoch: 36, train_acc: 0.95467, valid_acc: 0.79533, test_acc: 0.79600\n# in epoch: 37, train_acc: 0.97167, valid_acc: 0.81133, test_acc: 0.80100\n# at the end of epoch, global_step_num: 11400 get new best_valid_acc: 0.81133, save the model now!\n# in epoch: 38, train_acc: 0.97683, valid_acc: 0.79267, test_acc: 0.80333\n# in epoch: 39, train_acc: 0.96617, valid_acc: 0.77900, test_acc: 0.78767\n# in epoch: 40, train_acc: 0.95450, valid_acc: 0.79767, test_acc: 0.79867\n# in epoch: 41, train_acc: 0.95683, valid_acc: 0.80800, test_acc: 0.80800\n# in epoch: 42, train_acc: 0.97583, valid_acc: 0.80333, test_acc: 0.80433\n# in epoch: 43, train_acc: 0.97183, valid_acc: 0.80567, test_acc: 0.79467\n# in epoch: 44, train_acc: 0.96233, valid_acc: 0.79500, test_acc: 0.78967\n# in epoch: 45, train_acc: 0.96400, valid_acc: 0.79300, test_acc: 0.78367\n# in epoch: 46, train_acc: 0.96317, valid_acc: 0.78833, test_acc: 0.79267\n# in epoch: 47, train_acc: 0.96633, valid_acc: 0.79333, test_acc: 0.80267\n# in epoch: 48, train_acc: 0.97200, valid_acc: 0.79667, test_acc: 0.80533\n# in epoch: 49, train_acc: 0.97050, valid_acc: 0.79800, test_acc: 0.79533\n# in epoch: 50, train_acc: 0.96283, valid_acc: 0.80233, test_acc: 0.79267\n# in epoch: 51, train_acc: 0.95967, valid_acc: 0.79100, test_acc: 0.79067\n# in epoch: 52, train_acc: 0.97033, valid_acc: 0.80200, test_acc: 0.80233\n# in epoch: 53, train_acc: 0.97283, valid_acc: 0.79800, test_acc: 0.80567\n# in epoch: 54, train_acc: 0.96933, valid_acc: 0.79300, test_acc: 0.80133\n# in epoch: 55, train_acc: 0.95533, valid_acc: 0.79100, test_acc: 0.79833\n# in epoch: 56, train_acc: 0.96367, valid_acc: 0.79867, test_acc: 0.80567\n# in epoch: 57, train_acc: 0.97617, valid_acc: 0.80167, test_acc: 0.80267\n# in epoch: 58, train_acc: 0.97467, valid_acc: 0.79433, test_acc: 0.80900\n# in epoch: 59, train_acc: 0.95967, valid_acc: 0.79567, test_acc: 0.80167\n# in epoch: 60, train_acc: 0.95883, valid_acc: 0.78600, test_acc: 0.80100\n# in epoch: 61, train_acc: 0.97900, valid_acc: 0.79567, test_acc: 0.79633\n# in epoch: 62, train_acc: 0.97400, valid_acc: 0.78733, test_acc: 0.79333\n# in epoch: 63, train_acc: 0.96150, valid_acc: 0.78233, test_acc: 0.78467\n# in epoch: 64, train_acc: 0.96300, valid_acc: 0.80100, test_acc: 0.80567\n# in epoch: 65, train_acc: 0.96900, valid_acc: 0.80333, test_acc: 0.80367\n# in epoch: 66, train_acc: 0.97550, valid_acc: 0.79667, test_acc: 0.79967\n# in epoch: 67, train_acc: 0.97033, valid_acc: 0.78367, test_acc: 0.79600\n# in epoch: 68, train_acc: 0.96500, valid_acc: 0.79967, test_acc: 0.81200\n# in epoch: 69, train_acc: 0.95667, valid_acc: 0.80733, test_acc: 0.79933\n# in epoch: 70, train_acc: 0.97033, valid_acc: 0.79733, test_acc: 0.80067\n# in epoch: 71, train_acc: 0.97600, valid_acc: 0.79967, test_acc: 0.80033\n# in epoch: 72, train_acc: 0.97033, valid_acc: 0.78533, test_acc: 0.78200\n# in epoch: 73, train_acc: 0.95633, valid_acc: 0.79500, test_acc: 0.79533\n# in epoch: 74, train_acc: 0.96117, valid_acc: 0.81967, test_acc: 0.80500\n# at the end of epoch, global_step_num: 22500 get new best_valid_acc: 0.81967, save the model now!\n# in epoch: 75, train_acc: 0.97667, valid_acc: 0.79333, test_acc: 0.80233\n# in epoch: 76, train_acc: 0.98267, valid_acc: 0.79300, test_acc: 0.80133\n# in epoch: 77, train_acc: 0.97367, valid_acc: 0.78700, test_acc: 0.78233\n# in epoch: 78, train_acc: 0.94567, valid_acc: 0.79233, test_acc: 0.79300\n# in epoch: 79, train_acc: 0.96300, valid_acc: 0.79567, test_acc: 0.79833\n# in epoch: 80, train_acc: 0.97550, valid_acc: 0.80433, test_acc: 0.80767\n# in epoch: 81, train_acc: 0.97667, valid_acc: 0.79033, test_acc: 0.80033\n# in epoch: 82, train_acc: 0.96150, valid_acc: 0.79333, test_acc: 0.78667\n# in epoch: 83, train_acc: 0.95717, valid_acc: 0.79900, test_acc: 0.79667\n# in epoch: 84, train_acc: 0.97517, valid_acc: 0.80300, test_acc: 0.79900\n# in epoch: 85, train_acc: 0.97483, valid_acc: 0.79500, test_acc: 0.79800\n# in epoch: 86, train_acc: 0.96833, valid_acc: 0.79600, test_acc: 0.80600\n# in epoch: 87, train_acc: 0.95450, valid_acc: 0.79233, test_acc: 0.80300\n# in epoch: 88, train_acc: 0.97267, valid_acc: 0.80867, test_acc: 0.81200\n# in epoch: 89, train_acc: 0.97767, valid_acc: 0.80333, test_acc: 0.80400\n# in epoch: 90, train_acc: 0.97450, valid_acc: 0.79733, test_acc: 0.79433\n# in epoch: 91, train_acc: 0.94333, valid_acc: 0.80167, test_acc: 0.79067\n# in epoch: 92, train_acc: 0.96883, valid_acc: 0.80400, test_acc: 0.79933\n# in epoch: 93, train_acc: 0.97850, valid_acc: 0.79900, test_acc: 0.81000\n# in epoch: 94, train_acc: 0.97633, valid_acc: 0.79667, test_acc: 0.79933\n# in epoch: 95, train_acc: 0.96167, valid_acc: 0.79733, test_acc: 0.79600\n# in epoch: 96, train_acc: 0.95900, valid_acc: 0.80100, test_acc: 0.79933\n# in epoch: 97, train_acc: 0.97633, valid_acc: 0.80833, test_acc: 0.81667\n# in epoch: 98, train_acc: 0.97467, valid_acc: 0.80033, test_acc: 0.80700\n# in epoch: 99, train_acc: 0.96400, valid_acc: 0.79600, test_acc: 0.79533\n# in epoch: 100, train_acc: 0.96417, valid_acc: 0.79300, test_acc: 0.81000\n# in epoch: 101, train_acc: 0.97033, valid_acc: 0.80133, test_acc: 0.80567\n# in epoch: 102, train_acc: 0.96767, valid_acc: 0.80233, test_acc: 0.80267\n# in epoch: 103, train_acc: 0.97017, valid_acc: 0.79467, test_acc: 0.79867\n# in epoch: 104, train_acc: 0.97567, valid_acc: 0.80933, test_acc: 0.80200\n# in epoch: 105, train_acc: 0.96400, valid_acc: 0.79633, test_acc: 0.79500\n# in epoch: 106, train_acc: 0.95267, valid_acc: 0.80500, test_acc: 0.79900\n# in epoch: 107, train_acc: 0.97167, valid_acc: 0.81033, test_acc: 0.80300\n# in epoch: 108, train_acc: 0.98033, valid_acc: 0.80833, test_acc: 0.80067\n# in epoch: 109, train_acc: 0.96667, valid_acc: 0.79467, test_acc: 0.78867\n# in epoch: 110, train_acc: 0.95833, valid_acc: 0.79400, test_acc: 0.79900\n# in epoch: 111, train_acc: 0.96767, valid_acc: 0.79233, test_acc: 0.79800\n# in epoch: 112, train_acc: 0.97533, valid_acc: 0.79233, test_acc: 0.79867\n# in epoch: 113, train_acc: 0.96900, valid_acc: 0.79433, test_acc: 0.79700\n# in epoch: 114, train_acc: 0.96567, valid_acc: 0.78733, test_acc: 0.79867\n# in epoch: 115, train_acc: 0.96483, valid_acc: 0.80967, test_acc: 0.79333\n# in epoch: 116, train_acc: 0.97750, valid_acc: 0.79833, test_acc: 0.79300\n# in epoch: 117, train_acc: 0.97350, valid_acc: 0.80467, test_acc: 0.79567\n# in epoch: 118, train_acc: 0.96967, valid_acc: 0.80733, test_acc: 0.80067\n# in epoch: 119, train_acc: 0.97283, valid_acc: 0.80400, test_acc: 0.80133\n# in epoch: 120, train_acc: 0.96950, valid_acc: 0.80200, test_acc: 0.80300\n# in epoch: 121, train_acc: 0.95900, valid_acc: 0.80400, test_acc: 0.79267\n# in epoch: 122, train_acc: 0.96383, valid_acc: 0.81467, test_acc: 0.79867\n# in epoch: 123, train_acc: 0.97967, valid_acc: 0.81300, test_acc: 0.79933\n# in epoch: 124, train_acc: 0.97967, valid_acc: 0.79567, test_acc: 0.79867\n# in epoch: 125, train_acc: 0.96200, valid_acc: 0.79100, test_acc: 0.78767\n# in epoch: 126, train_acc: 0.94850, valid_acc: 0.80167, test_acc: 0.79567\n# in epoch: 127, train_acc: 0.98283, valid_acc: 0.80167, test_acc: 0.81133\n# in epoch: 128, train_acc: 0.98000, valid_acc: 0.80300, test_acc: 0.80100\n# in epoch: 129, train_acc: 0.96617, valid_acc: 0.80600, test_acc: 0.80100\n# in epoch: 130, train_acc: 0.96183, valid_acc: 0.80433, test_acc: 0.79967\n# in epoch: 131, train_acc: 0.96833, valid_acc: 0.77967, test_acc: 0.78733\n# in epoch: 132, train_acc: 0.96833, valid_acc: 0.79567, test_acc: 0.80233\n# in epoch: 133, train_acc: 0.97100, valid_acc: 0.80400, test_acc: 0.79400\n# in epoch: 134, train_acc: 0.96717, valid_acc: 0.80267, test_acc: 0.79367\n# in epoch: 135, train_acc: 0.96750, valid_acc: 0.79567, test_acc: 0.79633\n# in epoch: 136, train_acc: 0.97683, valid_acc: 0.80867, test_acc: 0.80633\n# in epoch: 137, train_acc: 0.95883, valid_acc: 0.78600, test_acc: 0.79700\n# in epoch: 138, train_acc: 0.96133, valid_acc: 0.80033, test_acc: 0.80700\n# in epoch: 139, train_acc: 0.97833, valid_acc: 0.80333, test_acc: 0.80400\n# in epoch: 140, train_acc: 0.97717, valid_acc: 0.79800, test_acc: 0.81900\n# in epoch: 141, train_acc: 0.96967, valid_acc: 0.79233, test_acc: 0.79933\n# in epoch: 142, train_acc: 0.95300, valid_acc: 0.79267, test_acc: 0.80067\n# in epoch: 143, train_acc: 0.96467, valid_acc: 0.80467, test_acc: 0.79633\n# in epoch: 144, train_acc: 0.97667, valid_acc: 0.79900, test_acc: 0.80600\n# in epoch: 145, train_acc: 0.98017, valid_acc: 0.80633, test_acc: 0.79600\n# in epoch: 146, train_acc: 0.96400, valid_acc: 0.80033, test_acc: 0.78767\n# in epoch: 147, train_acc: 0.95433, valid_acc: 0.80033, test_acc: 0.79700\n# in epoch: 148, train_acc: 0.96983, valid_acc: 0.80267, test_acc: 0.80933\n# in epoch: 149, train_acc: 0.98167, valid_acc: 0.78400, test_acc: 0.78900\n\n# seed = 2032\n# final test_acc: 0.80500, best_val_acc: 0.81967, train_acc: 0.96117, best_train_acc: 0.98283\n\n# seed = 2033\n# final test_acc: 0.81633, best_val_acc: 0.81933, train_acc: 0.97733, best_train_acc: 0.98467\n\n# seed = 2034\n# final test_acc: 0.81700, best_val_acc: 0.81733, train_acc: 0.98350, best_train_acc: 0.98350","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-02T10:23:10.817412Z","iopub.execute_input":"2021-06-02T10:23:10.817758Z","iopub.status.idle":"2021-06-02T10:23:35.095896Z","shell.execute_reply.started":"2021-06-02T10:23:10.817725Z","shell.execute_reply":"2021-06-02T10:23:35.095011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimizer=torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-4)  epoch=150\n# final test_acc: 0.78567, best_val_acc: 0.78700, train_acc: 1.00000, best_train_acc: 1.00000\n# total finished, cost time:  1460.5737087726593\n\n# optimizer=torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-3)  epoch=150\n# final test_acc: 0.81000, best_val_acc: 0.81200, train_acc: 0.99933, best_train_acc: 0.99983\n# total finished, cost time:  1039.3225367069244\n\n# optimizer=torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-3)  epoch=150   LeakyRELU\n# final test_acc: 0.80100, best_val_acc: 0.80633, train_acc: 0.99850, best_train_acc: 0.99983\n# total finished, cost time:  1046.8245861530304\n\n# optimizer=torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-2)  不收敛！！！\n\n# optimizer=torch.optim.Adam(model.parameters(), lr=0.004, weight_decay=1e-3)  epoch=150\n# final test_acc: 0.80067, best_val_acc: 0.80667, train_acc: 0.99033, best_train_acc: 0.99733\n# total finished, cost time:  1028.5406827926636\n\n# optimizer=torch.optim.AdamW(model.parameters(), lr=0.005, weight_decay=0.01)\n# final test_acc: 0.77700, best_val_acc: 0.78867, train_acc: 0.97367, best_train_acc: 0.99933\n# total finished, cost time:  1047.6061577796936\n\n# optimizer=torch.optim.AdamW(model.parameters(), lr=0.005, weight_decay=0.001)\n# final test_acc: 0.78433, best_val_acc: 0.77900, train_acc: 0.96367, best_train_acc: 0.99383\n# total finished, cost time:  1044.7504827976227\n\n# optimizer=torch.optim.AdamW(model.parameters(), lr=0.006, weight_decay=0.001)\n# final test_acc: 0.77867, best_val_acc: 0.77267, train_acc: 0.95467, best_train_acc: 0.98800\n# total finished, cost time:  1052.4147984981537","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:23:35.09719Z","iopub.execute_input":"2021-06-02T10:23:35.097546Z","iopub.status.idle":"2021-06-02T10:23:35.103254Z","shell.execute_reply.started":"2021-06-02T10:23:35.097517Z","shell.execute_reply":"2021-06-02T10:23:35.101356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(seed=cfg.seed+2)\nmodel = MLP_Net()\nmodel.to(device)     \nmodel_param_num = sum(p.numel() for p in model.parameters())\nmodel_trainable_param_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nglobal_best_train_acc, global_best_valid_acc = 0.0, 0.0\nglobal_train_acc = 0.0\nglobal_step_num = 0\n\noptimizer=torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-3)\n# optimizer=torch.optim.AdamW(model.parameters(), lr=0.006, weight_decay=0.01)\n# loss_func = nn.BCELoss()\nloss_func = nn.MSELoss()\n\nmodel_ema = None\nfgm = None\nif cfg.use_ema:\n    model_ema = model_EMA(model, decay=0.95)\n    model_ema.register()\n    \nif cfg.use_adversial_training:\n    fgm = FGM(model, 'embedding', epsilon=1.0, adv_random=True)\n    \nfor epoch in range(epochs):\n    train_acc = train(model, dl_train, optimizer, loss_func, device)\n    if cfg.use_ema:\n        model_ema.apply_shadow()\n        \n    valid_acc = evaluate(model, dl_valid, device)\n    test_acc = evaluate(model, dl_test, device)\n    print(f'in epoch: {epoch}, train_acc: {train_acc:.5f}, valid_acc: {valid_acc:.5f}, test_acc: {test_acc:.5f}')\n    if train_acc > global_best_train_acc:\n        global_best_train_acc = train_acc\n    if valid_acc > global_best_valid_acc:\n        global_best_valid_acc = valid_acc\n        global_train_acc = train_acc\n        print(f'at the end of epoch, global_step_num: {global_step_num} get new best_valid_acc: {valid_acc:.5f}, save the model now!')\n        torch.save(model.state_dict(), os.path.join(cfg.model_output_dir, 'best_step_model_2.pth'))\n        \n    if cfg.use_ema:\n        model_ema.restore()\n        \nmodel = MLP_Net()\nmodel.to(device)\n\nmodel.load_state_dict(torch.load(os.path.join(cfg.model_output_dir, 'best_step_model_2.pth')))\ntest_acc = evaluate(model, dl_test, device)\nprint(f'final test_acc: {test_acc:.5f}, best_val_acc: {global_best_valid_acc:.5f}, '\n      f'train_acc: {global_train_acc:.5f}, best_train_acc: {global_best_train_acc:.5f}')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:23:35.104643Z","iopub.execute_input":"2021-06-02T10:23:35.105019Z","iopub.status.idle":"2021-06-02T10:23:58.565752Z","shell.execute_reply.started":"2021-06-02T10:23:35.104974Z","shell.execute_reply":"2021-06-02T10:23:58.564927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(seed=cfg.seed+3)\nmodel = MLP_Net()\nmodel.to(device)     \nmodel_param_num = sum(p.numel() for p in model.parameters())\nmodel_trainable_param_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nglobal_best_train_acc, global_best_valid_acc = 0.0, 0.0\nglobal_train_acc = 0.0\nglobal_step_num = 0\n\noptimizer=torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-3)\nloss_func = nn.MSELoss()\n\nmodel_ema = None\nfgm = None\nif cfg.use_ema:\n    model_ema = model_EMA(model, decay=0.95)\n    model_ema.register()\n    \nif cfg.use_adversial_training:\n    fgm = FGM(model, 'embedding', epsilon=1.0, adv_random=True)\n    \nfor epoch in range(epochs):\n    train_acc = train(model, dl_train, optimizer, loss_func, device)\n    if cfg.use_ema:\n        model_ema.apply_shadow()\n        \n    valid_acc = evaluate(model, dl_valid, device)\n    test_acc = evaluate(model, dl_test, device)\n    print(f'in epoch: {epoch}, train_acc: {train_acc:.5f}, valid_acc: {valid_acc:.5f}, test_acc: {test_acc:.5f}')\n    if train_acc > global_best_train_acc:\n        global_best_train_acc = train_acc\n    if valid_acc > global_best_valid_acc:\n        global_best_valid_acc = valid_acc\n        global_train_acc = train_acc\n        print(f'at the end of epoch, global_step_num: {global_step_num} get new best_valid_acc: {valid_acc:.5f}, save the model now!')\n        torch.save(model.state_dict(), os.path.join(cfg.model_output_dir, 'best_step_model_3.pth'))\n        \n    if cfg.use_ema:\n        model_ema.restore()\n        \nmodel = MLP_Net()\nmodel.to(device)\nmodel.load_state_dict(torch.load(os.path.join(cfg.model_output_dir, 'best_step_model_3.pth')))\ntest_acc = evaluate(model, dl_test, device)\nprint(f'final test_acc: {test_acc:.5f}, best_val_acc: {global_best_valid_acc:.5f}, '\n      f'train_acc: {global_train_acc:.5f}, best_train_acc: {global_best_train_acc:.5f}')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:23:58.569351Z","iopub.execute_input":"2021-06-02T10:23:58.571294Z","iopub.status.idle":"2021-06-02T10:24:21.563195Z","shell.execute_reply.started":"2021-06-02T10:23:58.571256Z","shell.execute_reply":"2021-06-02T10:24:21.562214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_names_lst = ['best_step_model_1.pth', 'best_step_model_2.pth', 'best_step_model_3.pth']\n\ndef evaluate_ensemble(model_names_lst, dl_test, device):\n    y_pred_avg = None\n    for model_name in model_names_lst:\n        model = MLP_Net()\n        model.to(device)\n        model.load_state_dict(torch.load(os.path.join(cfg.model_output_dir, model_name)))\n        model.eval()\n\n        y_true_lst, y_pred_lst = [], []\n        with torch.no_grad():\n            for step, batch in enumerate(dl_test):\n                feature, label = batch\n                feature, label = feature.to(device), label.to(device)\n                y_pred = model(feature)\n                y_pred_lst += list(y_pred.detach().cpu().numpy())\n                y_true_lst += list(label.detach().cpu().numpy())\n                \n        if y_pred_avg is None:\n            y_pred_avg = y_pred_lst\n        else:\n            y_pred_avg = [v1+v2 for v1, v2 in zip(y_pred_avg, y_pred_lst)]\n        del model\n            \n    y_pred_avg = [v/3.0 for v in y_pred_avg]\n    acc = accuracy(y_pred_avg, y_true_lst)\n\n    return acc\n\nvalid_acc_ensemble = evaluate_ensemble(model_names_lst, dl_valid, device)\ntest_acc_ensemble = evaluate_ensemble(model_names_lst, dl_test, device)\nprint(f'final test_acc_ensemble: {test_acc_ensemble:.5f}, valid_acc_ensemble: {valid_acc_ensemble:.5f}')\n\nprint('ok, finished, total cost time: ', time.time() - global_start_t)\n\n# final test_acc: 0.79067, best_val_acc: 0.78567, train_acc: 0.93000, best_train_acc: 0.93100  # model_1  epoch_10\n# final test_acc: 0.79767, best_val_acc: 0.77733, train_acc: 0.93717, best_train_acc: 0.93717  # model_2  epoch_10\n# final test_acc: 0.78867, best_val_acc: 0.78067, train_acc: 0.93250, best_train_acc: 0.93250  # model_3  epoch_10\n# final test_acc_ensemble: 0.82233, valid_acc_ensemble: 0.80767","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:24:21.564451Z","iopub.execute_input":"2021-06-02T10:24:21.564954Z","iopub.status.idle":"2021-06-02T10:24:31.55359Z","shell.execute_reply.started":"2021-06-02T10:24:21.564915Z","shell.execute_reply":"2021-06-02T10:24:31.552678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_names_lst = ['best_step_model_1.pth', 'best_step_model_2.pth', 'best_step_model_3.pth']\n\ndef zero_model_params(model):\n    for name, param in model.named_parameters():\n        param.data.fill_(0.0)\n        \ndef add_two_model(model, model_to_add):\n    state_1 = model.state_dict()\n    state_2 = model_to_add.state_dict()\n    for layer in state_1:\n        state_1[layer] = (state_1[layer] + state_2[layer])\n    model.load_state_dict(state_1)\n    return model\n\ndef evaluate_ensemble_sum(model_names_lst, dl_test, device):\n    model = MLP_Net()\n    model.to(device)\n    zero_model_params(model)\n    for model_name in model_names_lst:\n        model_to_add = MLP_Net()\n        model_to_add.to(device)\n        model_to_add.load_state_dict(torch.load(os.path.join(cfg.model_output_dir, model_name)))\n        model = add_two_model(model, model_to_add)\n    for name, param in model.named_parameters():\n        param.data.div_(3.0)\n    model.eval()\n\n    y_true_lst, y_pred_lst = [], []\n    with torch.no_grad():\n        for step, batch in enumerate(dl_test):\n            feature, label = batch\n            feature, label = feature.to(device), label.to(device)\n            y_pred = model(feature)\n            y_pred_lst += list(y_pred.detach().cpu().numpy())\n            y_true_lst += list(label.detach().cpu().numpy())\n            \n    acc = accuracy(y_pred_lst, y_true_lst)\n    return acc\n\nvalid_acc_ensemble = evaluate_ensemble_sum(model_names_lst, dl_valid, device)\ntest_acc_ensemble = evaluate_ensemble_sum(model_names_lst, dl_test, device)\nprint(f'final test_acc_ensemble: {test_acc_ensemble:.5f}, valid_acc_ensemble: {valid_acc_ensemble:.5f}')\n\nprint('ok, finished, total cost time: ', time.time() - global_start_t)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:31:50.295952Z","iopub.execute_input":"2021-06-02T10:31:50.29628Z","iopub.status.idle":"2021-06-02T10:31:54.069084Z","shell.execute_reply.started":"2021-06-02T10:31:50.296254Z","shell.execute_reply":"2021-06-02T10:31:54.06824Z"},"trusted":true},"execution_count":null,"outputs":[]}]}