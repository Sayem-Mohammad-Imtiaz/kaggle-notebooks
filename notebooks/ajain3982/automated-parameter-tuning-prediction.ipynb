{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## imports\n\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom hyperopt import Trials, fmin, tpe, hp, STATUS_OK,STATUS_FAIL\nfrom scipy.stats import shapiro\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv')\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1 = data.copy()\ndata1['sex'] = data1['sex'].apply(lambda x : \"Male\" if x==0 else \"Female\")\ndata1['exng'] = data1['exng'].apply(lambda x : \"Exercise induced\" if x==0 else \"Not Exercise induced\")\ndata1['fbs'] = data1['fbs'].apply(lambda x : \"High sugar\" if x==1 else \"Low sugar\")\n\ndata1['restecg'] = data1['restecg'].replace({0:'normal',\n                                            1:'ST-T wave abnormality',\n                                            2:'left ventricular hypertrophy'})\n\ndata1['output'] = data1['output'].apply(lambda x : \"Higher chance of heart attack\" if x==1 else \"Low chance of heart attack\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## continuous columns \ncont_cols = [['age','trtbps'],\n             ['chol','thalachh']]\nx_labels = [['Age','Resting Blood Pressure'],\n            ['Cholestrol levels','Maximum heart rate']]\n\nfig,ax = plt.subplots(nrows=2,ncols=2,figsize=(25,15))\nfor i in range(2):\n    for j in range(2):\n        ax_sub = sns.histplot(x=cont_cols[i][j],hue='output',data=data1,ax=ax[i][j],multiple='stack',alpha=0.5)\n        ax_sub.set(xlabel = x_labels[i][j],title='Distribution of ' + x_labels[i][j])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations \n* It seems like age, cholestrol and maximum heart rate levels are normal.\n* We will further down the line check the p-value and confirm it.","metadata":{}},{"cell_type":"code","source":"###cat plots\n\ncat_cols = [['sex','exng','cp'],\n            ['caa','fbs','restecg']]\n\nx_labels = [['Sex of the patient','Angina','Chest Pain Type'],\n            ['Number of major vessels','Fasting Blood Sugar','Resting ECG']]\n\n\nfig,ax = plt.subplots(nrows=2,ncols=3,figsize=(25,15))\n\nfor i in range(0,2):\n    for j in range(0,3):\n        ax_sub = sns.countplot(x=cat_cols[i][j],hue='output',data=data1,ax=ax[i][j])\n        ax_sub.set(xlabel=x_labels[i][j],title = 'Count by ' + x_labels[i][j] )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above plot displays count of all the categorical values grouped by output. From the plots - \n* Exercise induced angina leads to a higher chance of a heart attack.\n* There is a higher risk of a heart attack when you have non-anginal pain.\n\nOfcourse, these observations are limited to the sample size we have at hand.","metadata":{}},{"cell_type":"code","source":"## continuous columns \ncont_cols = [['age','trtbps'],\n             ['chol','thalachh']]\nx_labels = [['Age','Resting Blood Pressure'],\n            ['Cholestrol levels','Maximum heart rate']]\n\nfig,ax = plt.subplots(nrows=2,ncols=2,figsize=(25,15))\nfor i in range(2):\n    for j in range(2):\n        ax_sub = sns.boxplot(x='output',y=cont_cols[i][j],data=data1,ax=ax[i][j])\n        ax_sub.set(xlabel = x_labels[i][j],title='Boxplot of ' + x_labels[i][j])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations - \n* We can clearly see some outliers in blood pressure and heart attack columns.\n\n\n### Factors leading to higher chance of heart attack - \n* 40 to 58 aged people.\n\n* Exercise induced and Non anginal chest pain\n\n* B.P between 120 and 140\n\n* Cholesterol level b/w 200 to ~250\n\n* Having ST-T wave normality\n\n* If maximum heart rate is above 150\n\n* 0 major vessels\n","metadata":{}},{"cell_type":"code","source":"ax = plt.figure(figsize=(15,10))\nsns.heatmap(data=data.corr(),annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations - \n* CP column, i.e. chest pain has the highest correlation with the output column.\n* Same is with maximum heart rate achieved.\n\nThese observations are obvious because symptoms are physical. It would have been more fun if we could look at some of the inferred or symptoms which are not physical. ","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### Removing outliers - \n* Removal of outliers is done by calculating IQR\n* I am using shapiro-wilk test to calculate the P-value. ","metadata":{}},{"cell_type":"code","source":"## outlier removal\nfor col in [item for sublist in cont_cols for item in sublist]:\n    q1 = data[col].quantile(0.25)\n    q3 = data[col].quantile(0.75)\n    \n    p_v = shapiro(data[col]).pvalue\n    print(\"P-Value before removal of outlier from \" + col + \": \" + str(p_v))\n\n    iqr = q3-q1\n    iqr_left = q1 - iqr*1.5\n    iqr_right = q3 + iqr*1.5\n    print(iqr_left, iqr_right)\n    \n    data = data[(data[col]>=iqr_left) & (data[col]<=iqr_right)]\n    p_v = shapiro(data[col]).pvalue\n    print(\"P-Value after removal of outlier from \" + col + \": \" + str(p_v))\n    \n    print(\"\\n\\n\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations - \n* The null hypothesis in SW test is that the distribution is normally distributed. If p < 0.05, we say that null hypothesis is rejected.\n* Clearly, Cholestrol column is normal as the the p-value after removal of outliers is 0.178.\n* I am surprised that rest of all the continuous value columns are not normal. From the above histogram plots, they seemed normal.\n\n\nOkay, let's move ahead to machine learning model.","metadata":{}},{"cell_type":"code","source":"target = data['output']\ndata.drop('output',axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Scaling","metadata":{}},{"cell_type":"code","source":"cont_cols_list = [item for sublist in cont_cols for item in sublist]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(data,target,train_size=0.8,random_state=42)\nsc = StandardScaler()\nX_train[cont_cols_list] = sc.fit_transform(X_train[cont_cols_list])\nX_test[cont_cols_list] = sc.transform(X_test[cont_cols_list])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Modelling\nI am using something called hyperopt to pick up all the parameters and even the model type for the classification problem. Hyperopt uses something called Tree Parzen estimators to come up with the next set of parameters to evaluate. There are three things required - \n* Objective function - Trains the model and return 1 - **cross_val_score**\n * The Fmin in hyperopt minimizes the objective function.\n \n\n* Search Space - [This](https://github.com/hyperopt/hyperopt/wiki/FMin) tutorial lists down all different kinds of methods you can pass for search spaces.\n\n* Trials object - The object used to store all the results.\n\n","metadata":{}},{"cell_type":"code","source":"def objective_function(params):\n    try:\n        if params['type'] == 'dtree':\n            clf = DecisionTreeClassifier(criterion = params['criterion'], max_depth = params['max_depth'],\n                                        min_samples_split = int(params['min_samples_split']))\n        elif params['type'] == 'svm':\n            if params['kernel']['ktype'] == 'linear':\n                clf = SVC(C = params['C'],kernel = 'linear')\n            else:\n                clf  = SVC(C = params['C'],kernel = 'rbf',degree = params['kernel']['degree'])\n\n        elif params['type'] == 'random_forest':\n            clf = RandomForestClassifier(criterion = params['criterion'], max_depth = params['max_depth'],\n                                        min_samples_split = int(params['min_samples_split']))\n\n        loss = 1 - cross_val_score(clf,X_train,y_train,cv=5).mean()\n        return {'loss': loss, 'status':STATUS_OK,'clf':clf}\n    except:\n        return {'loss':1,'status':STATUS_FAIL,'clf':None}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A search space consists of nested function expressions, including stochastic expressions. The stochastic expressions are the hyperparameters. Sampling from this nested stochastic program defines the random search algorithm. The hyperparameter optimization algorithms work by replacing normal \"sampling\" logic with adaptive exploration strategies, which make no attempt to actually sample from the distributions specified in the search space.","metadata":{}},{"cell_type":"code","source":"search_space = hp.choice('classifier_type', [\n    {\n        'type': 'random_forest',\n        'criterion': hp.choice('rftree_criterion', ['gini', 'entropy']),\n        'max_depth': hp.choice('rftree_max_depth',[3,4,5,6,7]),\n        'min_samples_split': hp.uniform('rftree_min_samples_split', 3, 20),\n        'n_estimators': hp.uniform('rftree_n_estimators', 25, 100),\n        \n    },\n    {\n        'type': 'svm',\n        'C': hp.choice('svm_C', [1,2,3,4,5,6,7,8,9]),\n        'kernel': hp.choice('svm_kernel', [\n            {'ktype': 'linear'},\n            {'ktype': 'RBF','degree':hp.choice('rbf_degree',[2,3,4,5,6,7])},\n            ]),\n    },\n    {\n        'type': 'dtree',\n        'criterion': hp.choice('dtree_criterion', ['gini', 'entropy']),\n        'max_depth': hp.choice('dtree_max_depth',[3,4,5,6,7]),\n        'min_samples_split': hp.uniform('dtree_min_samples_split', 3, 20),\n        'n_estimators': hp.uniform('dtree_n_estimators', 25, 100),\n    },\n    ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrials = Trials()\nbest = fmin(objective_function,search_space,trials=trials,algo=tpe.suggest,max_evals=150)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_clf = trials.best_trial['result']['clf']\nbest_clf.fit(X_train,y_train)\npreds = best_clf.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting trial results","metadata":{}},{"cell_type":"code","source":"x = []\ny = []\nfor i,j in enumerate(trials.results):\n    x.append(i)\n    y.append(j['loss'])\n\nplt.figure(figsize=(20,10))\nax = sns.lineplot(x=x,y=y)\nax.set(xlabel = 'Iteration Number', ylabel = '1 - cross_val_score',title='Score vs iteration')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see at about 60th iteration we reached the minimum. ","metadata":{}},{"cell_type":"markdown","source":"#### Accuracy report","metadata":{}},{"cell_type":"code","source":"print(str(classification_report(y_test,preds)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(pd.crosstab(y_test,preds,rownames=['Actual'],colnames=['Predicted']),annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n* The purpose of this notebook is not to bring out the best model. The notebook documents my thought process during the full investigative journey of this dataset. Ofcourse, adding more models, more hyperparameters in hyperopt search space and increase in the number of trials also can lead to more performance.\n* Since, the dataset size is very low small, change in the confusion matrix numbers can lead to higher accuracy numbers.\n\n**If you liked the notebook, please don't forget to upvote.**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}