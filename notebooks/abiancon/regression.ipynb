{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La variabile dipendente da prevedere è la temperatura percepita in base alla temperatura reale, tipo di precipitazione, velocità del vento, umidità ecc..."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/szeged-weather/weatherHistory.csv')\ny = dataset['Apparent Temperature (C)']\ndataset = dataset.drop(columns = ['Formatted Date', 'Loud Cover', 'Daily Summary','Apparent Temperature (C)'])\ndataset.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Con LabelEncoder vengono codificati i valori stringa in campi numerici"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ndataset['Summary'] = encoder.fit_transform(dataset['Summary'])\ncolumn = dataset['Precip Type'].apply(str)\ndataset['Precip Type'] = encoder.fit_transform(column)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tramite la matrice di correlazione è possibile studiare quanto le features siano correlate tra loro. I dati non sono eccessivamente correlati tra loro, infatti, come si può vedere dai risultati, la regressione lineare ottiene un buon risultato. Vengono comunque utilizzati i metodi di regolarizzazione che mirano a ridurre la dimensione dei coefficienti stimando i pesi aggiungendo una penalità alfa.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\ncorrmat = dataset.corr()\nplt.figure(figsize=(10,10))\ng = sns.heatmap(corrmat,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Il dataset è stato diviso in set di dati di allenamento e di test. In questo caso è stato considerato il 30% del set di dati come dati di test e il restante 70% come dati di addestramento."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(dataset, y, test_size = 0.3, random_state = 123)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I regolarizzatori Ridge e Lasso dei modelli lineari presuppongono che tutte le feature siano centrate attorno a 0 e abbiano stesso ordine di varianza. Se una feature presenta una varianza di ordine di grandezza più grandi di altre, potrebbe dominare la funzione obiettivo e rendere lo stimatore incapace di apprendere correttamente da altre feature come ci si aspetta. Per questo motivo trasformiamo i dati secondo la tecnica di standardizzazione secondo cui s(x) = (x - u)/sd dove u è la media dei campioni di training e sd è la deviazione standard dei campioni di training."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\ns = StandardScaler(copy=True, with_mean=True, with_std=True)\nX_train = s.fit_transform(X_train)\nX_test = s.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Come primo metodo di regressione è stata utilizzata una rete neurale per la regressione, poi regressione lineare, ridge regression, lasso regression, elastic net regression.\nLe metriche utilizzate per valutare i modelli sono:\n\n- r2_score: restituisce il coefficiente di determinazione R^2 della previsione. Il coefficiente R^2 è definito come (1 - u / v), dove u è la somma residua dei quadrati ((y_true - y_pred) ** 2) .sum () e v è la somma totale dei quadrati ((y_true - y_true.mean ()) ** 2) .sum (). Il miglior punteggio possibile è 1.0.\n\n- mean_squared_error: è calcolato elevando al quadrato la differenza tra il valore reale y e quello predetto. Il quadrato determina una particolarità dell’errore quadratico medio, cioè che i grandi errori sono fortemente penalizzati."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nmlp = MLPRegressor(hidden_layer_sizes = 5, activation = 'tanh', alpha = 0.0001)\nmlp.fit(X_train, y_train)\ny_predtest = mlp.predict(X_test)\nprint('TEST SET:')\nprint('MSE: ', mean_squared_error(y_test, y_predtest))\nprint('R2: ', r2_score(y_test, y_predtest))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Regressione Lineare:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge, Lasso, ElasticNet, RidgeCV, LinearRegression, LassoCV, ElasticNetCV\nlregr = LinearRegression()\nlregr.fit(X_train, y_train)\ny_predtest = lregr.predict(X_test)\nprint('TEST SET:')\nprint('MSE: ', mean_squared_error(y_test, y_predtest))\nprint('R2: ', r2_score(y_test, y_predtest))\nprint('Coefficienti:')\nlregr.coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Con regressione ridge si intende un termine usato per riferirsi a un modello di regressione lineare i cui coefficienti non sono stimati dal metodo dei minimi quadrati, ma da un altro stimatore, chiamato stimatore ridge, che possiede bias ma ha una varianza inferiore rispetto ai minimi quadrati.\nLo stimatore ridge riduce i coefficienti di regressione, in modo che le variabili, con un contributo minore al risultato, abbiano i loro coefficienti vicini allo zero."},{"metadata":{"trusted":true},"cell_type":"code","source":"#è fondamentale calcolare il valore della penalità, dato dal coefficiente alfa in sklearn.\n#Per farlo sarà calcolato alfa tra questi otto valori, tramite RidgeCV, che permette di adattare\n#un modello di regressione ridge con cross-validation.\n\nalpha = [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 10, 100, 1000]\nmodel_cv = RidgeCV(alphas = alpha)\nmodel_cv.fit(X_train, y_train)\nprint('ALPHA: ', model_cv.alpha_)\n\nridge_model = Ridge(alpha = 1.0)\nridge_model.fit(X_train, y_train)\ny_predtest = ridge_model.predict(X_test)\nprint('TEST SET:')\nprint('MSE: ', mean_squared_error(y_test, y_predtest))\nprint('R2: ', r2_score(y_test, y_predtest))\nprint('Coefficienti:')\nridge_model.coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lasso riduce i coefficienti di regressione verso lo zero penalizzando il modello di regressione con una penalità (alpha) che ha l’effetto di forzare alcune delle stime dei coefficienti, con un contributo minore al modello, a essere esattamente uguale a zero.\nCiò significa che il lasso può anche essere visto come un’alternativa ai metodi di feature selection per eseguire la selezione delle variabili al fine di ridurre la complessità del modello."},{"metadata":{"trusted":true},"cell_type":"code","source":"#calcolo di alpha tramite LassoCV\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nalpha = [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 10, 100, 1000]\nmodel_cv = LassoCV(alphas = alpha)\nmodel_cv.fit(X_train, y_train)\nprint('ALPHA: ', model_cv.alpha_)\n\nlasso_model = Lasso(alpha = 0.0001)\nlasso_model.fit(X_train, y_train)\ny_predtest = lasso_model.predict(X_test)\nprint('TEST SET:')\nprint('MSE: ', mean_squared_error(y_test, y_predtest))\nprint('R2: ', r2_score(y_test, y_predtest))\nprint('Coefficienti:')\nlasso_model.coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Elastic Net combina le proprietà della regressione di Ridge e Lasso."},{"metadata":{"trusted":true},"cell_type":"code","source":"#calcolo di alpha tramite ElasticNetCV\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nalpha = [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 10, 100, 1000]\nmodel_cv = ElasticNetCV(alphas = alpha)\nmodel_cv.fit(X_train, y_train)\nprint('ALPHA: ', model_cv.alpha_)\n\nelastic_model = ElasticNet(alpha = 0.0001)\nelastic_model.fit(X_train, y_train)\ny_predtest = elastic_model.predict(X_test)\nprint('TEST SET:')\nprint('MSE: ', mean_squared_error(y_test, y_predtest))\nprint('R2: ', r2_score(y_test, y_predtest))\nprint('Coefficienti:')\nelastic_model.coef_","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}