{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Preprocessing Text to be used in models"},{"metadata":{},"cell_type":"markdown","source":"# Importing the required libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport re\nfrom nltk.stem.snowball import SnowballStemmer\nimport tqdm\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read the csv and extract the comments and the labels"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dir = \"../input/sarcastic-comments-on-reddit/train-balanced-sarcasm.csv\"\ndata = pd.read_csv(dir)\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comments = data['comment'].values\nlabels = data['label'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning the text and removing links/ punctuation and so on.."},{"metadata":{"trusted":true},"cell_type":"code","source":"text_cleaning = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\nstemmer = SnowballStemmer('english', ignore_stopwords=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_data(text):\n    text = re.sub(text_cleaning, ' ', str(text).lower()).strip()\n    text = stemmer.stem(str(text))\n    return text\n\nX = []\nfor i in tqdm.tqdm(range(len(comments))):\n    X.append(preprocess_data(comments[i]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenize the cleaned text and pad them accordingly"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(oov_token='<OOV>')\ntokenizer.fit_on_texts(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequences = tokenizer.texts_to_sequences(X)\npadded = pad_sequences(sequences, padding='post', maxlen=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split the data into train and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain, xtest, ytrain, ytest = train_test_split(np.array(padded), np.array(labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now the data is ready to be used as inputs in a neural network"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Actual Sentence: {comments[860]}\\nStemmed Sentence: {X[860]}\\nTokenized: {sequences[860]}\\nPadded: {padded[860]}\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}