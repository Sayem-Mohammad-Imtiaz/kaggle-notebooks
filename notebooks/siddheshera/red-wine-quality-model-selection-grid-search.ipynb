{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Importing Libraries**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Importing Dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking if there are any NULL values"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quality > 6.5 = 'good'\n\nQuality <6.5 = 'bad'"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['quality'] = [1 if i > 6.5 else 0 for i in dataset['quality']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['quality'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Splitting dataset into Train and Test set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state= 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Feature Scaling**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Importing different models**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Models of Selection**"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(['Logistic Regression 1', LogisticRegression(C = 0.1)])\nmodels.append(['Logistic Regression 2', LogisticRegression(C = 0.5)])\nmodels.append(['Logistic Regression 3', LogisticRegression(C = 1.0)])\nmodels.append(['KNeighbours 1', KNeighborsClassifier(n_neighbors = 3, metric = 'minkowski', p = 2)])\nmodels.append(['KNeighbours 2', KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)])\nmodels.append(['SVM 1', SVC(kernel= 'linear')])\nmodels.append(['SVM 2', SVC(kernel= 'rbf')])\nmodels.append(['Naive Bayes', GaussianNB()])\nmodels.append(['Decision Tree 1', DecisionTreeClassifier(criterion= 'gini')])\nmodels.append(['Decision Tree 2', DecisionTreeClassifier(criterion= 'entropy')])\nmodels.append(['Random Forest 1', RandomForestClassifier(n_estimators= 50, criterion= 'gini')])\nmodels.append(['Random Forest 2', RandomForestClassifier(n_estimators= 100, criterion= 'gini')])\nmodels.append(['Random Forest 3', RandomForestClassifier(n_estimators= 200, criterion= 'gini')])\nmodels.append(['Random Forest 4', RandomForestClassifier(n_estimators= 50, criterion= 'entropy')])\nmodels.append(['Random Forest 5', RandomForestClassifier(n_estimators= 100, criterion= 'entropy')])\nmodels.append(['Random Forest 6', RandomForestClassifier(n_estimators= 200, criterion= 'entropy')])\n\nfor m in range(len(models)):\n  model = models[m][1]\n  model.fit(x_train, y_train)\n  y_pred = model.predict(x_test)\n  cm = confusion_matrix(y_test, y_pred)\n  accuracies = cross_val_score(estimator = model, X = x_train, y = y_train, cv = 10)\n  print(models[m][0])\n  print(cm)\n  print('Accuracy Score',accuracy_score(y_test, y_pred))\n  print(\"Mean Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n  print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n  print('-----------------------------------')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As RandomForest has better accuracies than other models, so now GridSearch is applied on RandomForest for a better hyperparameters tuning."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = [{'n_estimators': [50, 100, 200, 300, 400, 500, 1000], 'criterion': ['gini']},\n              {'n_estimators': [50, 100, 200, 300, 400, 500, 1000], 'criterion': ['entropy']}]\ngrid_search = GridSearchCV(estimator = model,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(x_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nbest_std = grid_search.cv_results_['std_test_score'][grid_search.best_index_]\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint('Best Standard Deviation: {:.2f} %'.format(best_std*100))\nprint(\"Best Parameters:\", best_parameters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above shows the best hyperparameters for RandomForest which can make the model more efficient."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}