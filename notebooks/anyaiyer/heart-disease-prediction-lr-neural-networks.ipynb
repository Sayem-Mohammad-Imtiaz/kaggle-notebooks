{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Heart Disease Model","metadata":{}},{"cell_type":"markdown","source":"According to CDC, heart disease is the leading cause of death in the United States. Wouldn't it be great if we tried to diagnose heart disease before it becomes\nsevere? My model predicts whether a patient has heart disease or not based on the patient's medical reports.\n\n## Dataset Specifics\nIn the data, you are given several attributes: \n\n 1. age\n \n 2. sex\n \n 3. chest pain type (4 values)\n \n 4. resting blood pressure\n \n 5. serum cholesterol in mg/dl\n \n 6.  fasting blood sugar > 120 mg/dl\n \n 7. resting electrocardiographic results (values 0, 1, 2)\n \n 8. maximum heart rate achieved\n \n 9. exercise induced angina\n \n 10. oldpeak = ST depression induced by exercise relative to rest \n \n 11. the slope of the peak exercise ST segment\n \n 12.  number of major vessels (0-3) colored by flourosopy\n \n 13.   thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n\n## Algorithm \nThis is a classification problem (binary classification) and the results can be interpreted as 0 and 1 (0 = without heart disease, 1 = with heart disease). I used two methods: a [neural network]( https://en.wikipedia.org/wiki/Artificial_neural_network) using **Keras**, and [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression#:~:text=Logistic%20regression%20is%20a%20statistical,a%20form%20of%20binary%20regression). My neural network involves the use of [Early Stopping](https://en.wikipedia.org/wiki/Early_stopping) and [Dropout Layers](https://keras.io/api/layers/regularization_layers/dropout/) to prevent overfitting of the data. Logistic Regression is used when dealing with categorical data (in this case, patients with and without heart disease).\n\n\n| Type | Accuracy |  Precision| Recall|F1-Score|\n|--|--|--|--|--|\n| Logistic Regression | 85% | 0 = 88%, 1 = 82% | 0 = 80%, 1 = 89% |0 = 83%, 1 = 86%   |\n| Neural Network|  87%| 0 = 90%, 1 = 83%| 0 = 80%, 1 = 91%| 0 = 84%, 1 = 87%\n\n**[My Github](https://github.com/anyaiyer/heart-disease-predictor) for this project**\n\n\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heart = pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heart.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heart.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heart.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"sns.set_theme()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data visualization is a useful tool in comparing these features of patients to find the most correlated attributes with the presence of heart disease.\nVarious plot types such as heatmaps, countplots, barplots, and histplots help find common patterns between patients with and without heart disease. My code\nincludes a few of these plots to compare and contrast patients. ","metadata":{}},{"cell_type":"markdown","source":"More people have heart disease.\nMore females have heart disease than males; more females are included in this dataset","metadata":{}},{"cell_type":"code","source":"sns.countplot(x='target',data=heart,hue='sex') ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most patients are ages 50-60.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nheart['age'].plot(kind='hist',bins=40)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heart.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Attribute info: \n- age\n- sex\n-  pain type (4 values)\n- resting blood pressure\n- serum cholestoral in mg/dl\n- fasting blood sugar > 120 mg/dl\n- resting electrocardiographic results (values 0,1,2)\n- maximum heart rate achieved\n- exercise induced angina\n- oldpeak = ST depression induced by exercise relative to rest\n- the slope of the peak exercise ST segment\n- number of major vessels (0-3) colored by flourosopy\n- thal: 3 = normal; 6 = fixed defect; 7 = reversable defect","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\nsns.heatmap(heart.corr(),cmap='viridis',annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using a heatmap, we can get the most correlated features with target.","metadata":{}},{"cell_type":"markdown","source":"Most correlated features:\n- slope (slope of peak exercise ST segment) -> 35% correlated\n\n- thalach (max heart rate achieved) -> 42% correlated\n\n- restecg (resting electrocadiographic results) -> 14% correlated\n\n- cp (chest pain type) -> 43% correlated (most correlated feature with target) ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.barplot(x='cp',y='target',data=heart)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Chest pain of 1 is the most common.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.countplot(x='restecg',data=heart,hue='target')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most people who had heart disease have a restcg of 1.","metadata":{}},{"cell_type":"code","source":"heart.corr()['target'][:-1].sort_values().plot(kind='bar')\nplt.tight_layout","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Visual representation (bar chart) showing most correlated features with target column.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nheart['thalach'].plot(kind='hist',bins=40)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most people have a thalach between 140 and 170.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.countplot(x='slope',data=heart,hue='target')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most affected people have a slope of 2","metadata":{}},{"cell_type":"markdown","source":"## Data PreProcessing ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nheart.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No null values","metadata":{}},{"cell_type":"code","source":"heart.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data is already cleaned -> no need to fill in missing data or convert data to numerical data.","metadata":{}},{"cell_type":"markdown","source":"## Train Test Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heart.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = heart.drop('target',axis=1).values\ny = heart['target'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(heart)) # data size is small","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Features scaling](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#:~:text=Transform%20features%20by%20scaling%20each,e.g.%20between%20zero%20and%20one) (also known as Standardization) helps normalise the data within a specific range. This ensures\nmore accurate results as the model does not have to process large ranges of data. MinMaxScaler transforms the data\nsuch that it is all within a given range.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = MinMaxScaler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = scaler.fit_transform(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Dense(40,activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(20,activation='relu'))\nmodel.add(Dropout(0.2))\n\n# BINARY CLASSIFICATION so use sigmoid for the last layer\nmodel.add(Dense(1,activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use of [EarlyStopping](https://en.wikipedia.org/wiki/Early_stopping) and [Dropout layers](https://keras.io/api/layers/regularization_layers/dropout/) prevents overfitting of the data.\n","metadata":{}},{"cell_type":"code","source":"early_stop = EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to fit the model, we pass in X_train, y_train, the number of epochs (number of times the model will \nwork through the entire dataset), validation data (testing data), batch size (number of samples to work through \nbefore updating the model parameters), and early stopping.","metadata":{}},{"cell_type":"code","source":"model.fit(x=X_train,y=y_train,epochs=200,validation_data=(X_test,y_test),batch_size=30,callbacks=[early_stop])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"code","source":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Eventually, the validation loss goes below the loss. This is ideal as the loss is reaching a minimum point, and overfitting is not occuring.","metadata":{}},{"cell_type":"code","source":"predictions = model.predict_classes(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,predictions))\nprint(confusion_matrix(y_test,predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='target',data=heart) # fairly balanced ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Recall is most important because we need to detect all the true positives of heart disease. It is the most important that recall is high for all positive cases. Accuracy is ok because the data set is fairly balanced. Precision is less important than recall in this case.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import load_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neural_net_model = model.save('heart-disease-predictor.h5') # save model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_loss # loss vs. val loss ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logmodel = LogisticRegression()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logmodel.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = logmodel.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,predictions))\nconfusion_matrix(y_test,predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = logmodel.score(X_test, y_test)*100\n\nprint(\"Test Accuracy {:.2f}%\".format(acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = \"heart-disease-LR.pkl\"  # save model with pickle\n\nwith open(filename, 'wb') as file:  \n    pickle.dump(logmodel, file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}