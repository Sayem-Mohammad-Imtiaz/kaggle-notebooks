{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Elemental Libraries \nimport pandas as pd \nimport numpy as np\n\n# Basic Visualization\nimport seaborn as sns \nimport matplotlib.pyplot as plt \n%matplotlib inline \nsns.set_style(style=\"whitegrid\")\nimport cufflinks as cf\ncf.go_offline()\n\n#Other Visualiation\nimport joypy as jp\nfrom matplotlib import cm\nfrom IPython.display import display\nfrom PIL import Image\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Methodology \n ### In analzying data, you might find easy to lose the focus, that is why I am following OSEMN methodology \n which consist on \n * Obtain Data\n * Scrub Data\n * Explore Data\n * Model Data\n * Interpret Result \n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from PIL import Image\ndisplay(Image.open('../input/imagenes/osemn.jpeg'))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Obtain Data"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/iris-flower-dataset/IRIS.csv')\n\nprint(\"----Techinical Information----\")\nprint('Data Set Shape = {}'.format(df.shape))\nprint('Data Set Memory Usage = {:.2f} MB'.format(df.memory_usage().sum()/1024**2))\nprint('\\n')\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scrub Data\n\n## 1. General View"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('df_columns are ={}'.format(df.columns.to_list()))\nprint('-------------')\nprint('Species in data are = {}'.format(df['species'].value_counts()))\nprint('-------------')\nprint('Target Column is = {}'.format(df.columns[-1]))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# 2. Finding Missing Values \ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# 3. Tranforming Data\nfrom sklearn.preprocessing import OrdinalEncoder\n\nOrdi_Var = df.iloc[:,4:]    # From Text to Numbers using Ordinal Encoder\nOE = OrdinalEncoder().fit_transform(Ordi_Var)\ndf['target'] = OE\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore Data\n\n## Visualization"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.pairplot(data=df, hue='species', palette='Set2')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"label = 'species'\n\nf,axes = plt.subplots(2,2, figsize = (10,10) , dpi=100)\nsns.violinplot(x = label , y = 'sepal_length', data = df , ax= axes[0,0])\nsns.violinplot(x = label , y = 'sepal_width', data = df , ax= axes[0,1])\nsns.violinplot(x = label , y = 'petal_length', data = df , ax= axes[1,0])\nsns.violinplot(x = label  , y = 'petal_width', data = df , ax= axes[1,1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Well... and what does it mean ?  \n\n***if the violin plot could speak*** , it would say\n- *** Iris has two parts 1. Sepal , 2. Petal\n- hey bro look this ***\"Iris_virgnica\"*** has the  biggest sepal in terms of length from 4cm to 8cm but most of them has a 6.5cm lenght average*** No idea what is this... well if the flower were a man , \"he would be the fattest man\" , no idea yet, so check the image \n- *** Iris Setosa *** is long , think of it as snake it would be the longest snake among those 3 \n- *** Iris virginica petal*** is the biggest one... look \" from 4 cm to 7 cm , +-3.4 avergae , and also in tems of width \n\nBasically bro Im trying to say that ***Iris Virginica is the Biggest dude in the Iris neighborhood ***\n\n\nSo when you put some data in the algorithm I will compare all of them and tell you I know wha it that...\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"!pip install joypy\nimport joypy\n\njoypy.joyplot(data=df.iloc[:,0:], by = 'species'  ,overlap=7, figsize=(10,10), legend = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. look this image, the red one means \"petal width\" Iris-Setosa has the maximun width ,the maximun \"petal length\" and the maximun \"sepal legth\"\n2.  the orange one means \"sepal width\" in this case Iris Virginica won"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"display(Image.open('../input/imagenes/dont understan.jfif'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check this out"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"display(Image.open('../input/imagenes/i_image.jpg'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Anomalies and Outliers\n\nNow We have to go through the data set to make sure there are not ***ouliers*** (in this case it means there are not Iris from chernobyl with really long petal or with no petal or any other wierd part) Then we are going to check the relationship between petal and length using heatmap (really usefull when we have many columns \"Variable\")"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Analizying Target Variable \ndf['species'].value_counts()\n\n# We want to predict or classify Iris (flowers) , therefore we should have a balanced data set\n# 3 flowers to predict , with n-rows for each one","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Dropping columns\ndf.drop('species',axis = 1, inplace=True)\n\ndef corr_heat (frame):   #<---Heat Map\n    correlation = frame.corr()\n    f,ax = plt.subplots(figsize=(15,10))\n    mask = np.triu(correlation)\n    sns.heatmap(correlation, annot=True, mask=mask,ax=ax,cmap='viridis')\n    bottom,top = ax.get_ylim()\n    ax.set_ylim(bottom+ 0.5, top - 0.5)\n    \n    \ncorr_heat(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### And this what?\n\nHere we can notice that Sepal-length is strong realted with the Petal width-length in other word it means that these dimmension explain each other , ***think of it like the relation between your arms and legs ,you may guess how long are someone arms by looking his/her legs and vice-versa***\n\nhowever, the Sepalwidth has not positive relation with petal length/width, *** think of how difficult would be guess how long are someone arms by taking a look of their ears only ***\n\nWhen We have this cases Usually I wil drop the column but instead I will run two escenario having/no having the Columns just to make sure how would it affect the model "},{"metadata":{},"cell_type":"markdown","source":"# Model Data "},{"metadata":{},"cell_type":"markdown","source":"### Scenario 1 . Keeping \"Sepal_Width_cm\"\n\n#### Normalizing - Dropping columns, outliers, etc.\n\nIn this case We are going to check the all columns to find any outliers"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.iloc[:,:4].iplot(kind= 'box' , boxpoints = 'outliers')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can notice that ***Sepal_Width_cm has some outliers*** , generally speaking outlier are those values that vary significatly comparing to the others, When we have outlier we have to evaluate if they need to be drop. There are many techniques avalable to Detect and Drop outliers but now I will use the ***Boxplot*** Visualization to find them - all values greater than 4 and lesser than 2.2"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Extracting Outliers \n\n#Finding by index\nOutliers = df[(df['sepal_width']>4) | (df['sepal_width']<2.2)].index\n\n#Visualization\ndf[(df['sepal_width']>4) | (df['sepal_width']<2.2)]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Dropping \ndf.drop(Outliers, axis = 0 , inplace=True)\n\ndf.iloc[:,:4].iplot(kind= 'box' , boxpoints = 'outliers')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Scenario 1 \nSC_X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n\n# Scenario 2 \nSC_1X = df[['sepal_length', 'petal_length', 'petal_width']]\n\n#Variable to predict \ny = df.iloc[:,-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning \n\n1. Logistic Regression\n2. Decision Tree Classification\n3. Random Forest Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score,train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scenario 1 "},{"metadata":{"trusted":true},"cell_type":"code","source":"#---> Splitting Data\nX_train, X_test, y_train, y_test = train_test_split(SC_X, y, test_size=0.2, random_state=104)\n\n\n# 2. Second Bring the model \nLGR = LogisticRegression(random_state=0) #Logistic Regression\nDT = DecisionTreeClassifier() #Decision Tree\nRDF = RandomForestClassifier () #Randome Forest \n\n\n# --------------> 3.Fit models\n#-->Scenario 1 \nLGR = LGR.fit(X_train,y_train)\nDT = DT.fit(X_train,y_train)\nRDF = RDF.fit(X_train,y_train)\n\n#4. Prediction\ny_pred_LGR = LGR.predict(X_test)\ny_pred_DT = DT.predict(X_test)\ny_pred_RDF = RDF.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scenario 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"#---> Splitting Data\nX1_train, X1_test, y_train, y_test = train_test_split(SC_1X, y, test_size=0.2, random_state=104)\n\n\n# 2. Second Bring the model \nLGR1 = LogisticRegression(random_state=0) #Logistic Regression\nDT1 = DecisionTreeClassifier() #Decision Tree\nRDF1 = RandomForestClassifier () #Randome Forest \n\n\n# --------------> 3.Fit models\n#-->Scenario 1 \nLGR1 = LGR1.fit(X1_train,y_train)\nDT1 = DT1.fit(X1_train,y_train)\nRDF1 = RDF1.fit(X1_train,y_train)\n\n#4. Prediction\ny_pred_LGR1 = LGR1.predict(X1_test)\ny_pred_DT1 = DT1.predict(X1_test)\ny_pred_RDF1 = RDF1.predict(X1_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Interpret result "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"\n#Confusion Matriz\nprint(\"-------------------------Scenario 1-----------------------\")\nprint(\"\\n\")\nprint(\"The acurracy score of Logistical regression is {}\".format(accuracy_score(y_test,y_pred_LGR)),\"\\n\",confusion_matrix(y_test,y_pred_LGR))\nprint(\"\\n\")\nprint(\"The acurracy score of Decision Tree is {}\".format(accuracy_score(y_test,y_pred_DT)),\"\\n\",confusion_matrix(y_test,y_pred_DT))\nprint(\"\\n\")\nprint(\"The acurracy score of Randome Forest is {}\".format(accuracy_score(y_test,y_pred_RDF)),\"\\n\",confusion_matrix(y_test,y_pred_RDF))\nprint(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"\n#Confusion Matriz\nprint(\"-------------------------Scenario 2-----------------------\")\nprint(\"\\n\")\nprint(\"The acurracy score of Logistical regression-2nd Scenario is {}\".format(accuracy_score(y_test,y_pred_LGR1)),\"\\n\",confusion_matrix(y_test,y_pred_LGR1))\nprint(\"\\n\")\nprint(\"The acurracy score of Decision Tree-2nd Scenario is {}\".format(accuracy_score(y_test,y_pred_DT1)),\"\\n\",confusion_matrix(y_test,y_pred_DT1))\nprint(\"\\n\")\nprint(\"The acurracy score of Randome Forest-2nd Scenario is {}\".format(accuracy_score(y_test,y_pred_RDF1)),\"\\n\",confusion_matrix(y_test,y_pred_RDF1))\nprint(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finally... I can conclude\n\n1. All moddel preformance excellent \n2. SepalWidthCm did not affect significatly the performance , in other word having or not does not make any significant improvement \n3. to evaluate the model we can use cross_validation_Score \n4. if we have 30 test, 70 train the model tends to be less accurate "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}