{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <div align=\"center\"> <font color= 'Red'> PROJE 4\n#  <div align=\"center\"> <font color= 'Red'> Customer Segmentation Research"},{"metadata":{},"cell_type":"markdown","source":"## Contents\n1.  The Aim of Analysis\n2.  General Information of the Data\n3.  Data Exploration\n4.  Overview about Outliers\n5.  Checking for NULL Values \n6.  Filling of the Row Data \n7.  Clustering Methods\n     * 7.1 Kmeans\n       * 7.1.a   Determining Number of Clusters with Elbow Method\n       * 7.1.b   Determining Number of Clusters with Silhouette Scores Method\n       * 7.1.c   Kmeans Clustering\n     * 7.2 Hierarchical Clustering\n     * 7.3 DBSCAN\n     * 7.4 Spectral Clustering\n     * 7.5 GMM\n8.  Compering Results for the Optimal Number of Cluster Model     \n9.  DIMENSION REDUCTION\n     * 9.1 PCA\n     * 9.2 T-SNE\n     * 9.3 UMAP\n\n10. Understanding Clusters by Customer Segmentation in 3 Different Kmeans Models\n    * 10.1 Kmeans Model with 8 Clusters\n    * 10.2 Kmeans Model with 4 Clusters\n11. Conclusion \n"},{"metadata":{},"cell_type":"markdown","source":"## <div align=\"center\"> <font color= 'blue'> 1.The Aim of Analysis\n                              \nThis case requires to develop a customer segmentation to define marketing strategy. The sample Dataset summarizes the usage behavior of about 9000 active credit card holders during the last 6 months. The file is at a customer level with 18 behavioral variables.\nThis study also aims to see different Machine Learning Methods on the same data set. "},{"metadata":{},"cell_type":"markdown","source":"## <div align=\"center\"> <font color= 'blue'> 2.  General Information of the Data"},{"metadata":{},"cell_type":"markdown","source":"**CUST_ID :** Identification of Credit Card holder (Categorical)\n\n**BALANCE :** Balance amount left in their account to make purchases (\n\n**BALANCE_FREQUENCY :** How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n\n**PURCHASES :** Amount of purchases made from account\n\n**ONEOFF_PURCHASES :** Maximum purchase amount done in one-go\n\n**INSTALLMENTS_PURCHASES :** Amount of purchase done in installment\n\n**CASH_ADVANCE :** Cash in advance given by the user\n\n**PURCHASES_FREQUENCY :** How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n\n**ONEOFFPURCHASESFREQUENCY :** How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n\n**PURCHASESINSTALLMENTSFREQUENCY :** How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n\n**CASHADVANCEFREQUENCY :** How frequently the cash in advance being paid\n\n**CASHADVANCETRX :** Number of Transactions made with \"Cash in Advanced\"\n\n**PURCHASES_TRX :** Numbe of purchase transactions made\n\n**CREDIT_LIMIT :** Limit of Credit Card for user\n\n**PAYMENTS :** Amount of Payment done by user\n\n**MINIMUM_PAYMENTS :** Minimum amount of payments made by user\n\n**PRCFULLPAYMENT :** Percent of full payment paid by user\n\n**TENURE :** Tenure of credit card service for user\n\n**The credit card data has 17 attributes for each customer which include the balance (credit owed by the customer), cash advance (when a customer withdraws cash using the credit card), the customerâ€™s credit limit, minimum payment, percentage of full payments and tenure.**\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"#installment packages\n\nimport pandas as pd\nimport numpy as np \n\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\n\n\nfrom sklearn.decomposition import PCA \nfrom sklearn.manifold import TSNE\nimport umap\n\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn import metrics\nfrom sklearn.metrics import silhouette_score\n\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.mixture import GaussianMixture\n\nfrom sklearn import cluster \nfrom sklearn.cluster import SpectralClustering\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <div align=\"center\"> <font color= 'blue'> 3.  Data Exploration "},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.read_csv(\"../input/ccdata/CC GENERAL.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.drop(\"CUST_ID\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"df[['BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES','ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE','PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY',\n    'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY','CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS',\n    'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT', 'TENURE']].nunique()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"df[['CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'TENURE']].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#A quick check on vriables\n\nfrom scipy.stats import norm \n\ngraph_by_variables = ['BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES',\n       'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE',\n       'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY',\n       'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY',\n       'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS',\n       'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT', 'TENURE']\nplt.figure(figsize=(15,18))\n\nfor i in range(0,17):\n    plt.subplot(6,3,i+1)\n    sns.distplot(df[graph_by_variables[i]].dropna(),fit=norm)\n    plt.title(graph_by_variables[i])\n\nplt.tight_layout()\n    ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"#Frequency variables in charts\n\ndf[['BALANCE_FREQUENCY',\n 'PURCHASES_FREQUENCY',\n 'ONEOFF_PURCHASES_FREQUENCY',\n 'PURCHASES_INSTALLMENTS_FREQUENCY',\n 'CASH_ADVANCE_FREQUENCY',\n'PRC_FULL_PAYMENT']].hist(figsize=(10,8))\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#int. values in histogram\n\ndf[['TENURE','CASH_ADVANCE_TRX', 'PURCHASES_TRX']].hist(figsize=(10,8))\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df[['BALANCE', 'PURCHASES', 'ONEOFF_PURCHASES','INSTALLMENTS_PURCHASES', 'CASH_ADVANCE',\n    'CREDIT_LIMIT', 'PAYMENTS', 'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT']].hist(figsize=(10,10))\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"plt.scatter(df['BALANCE_FREQUENCY'], df['BALANCE'])\nplt.xlabel('BALANCE_FREQUENCY')\nplt.ylabel('BALANCE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df[['TENURE']].plot.kde(figsize=(5,5),title='Tenure')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a clear view of tenure on customer level."},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(9,7))\nsns.heatmap(df.corr(),cmap='coolwarm')\n\nplt.title('Correlation Matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Balance has a higher level of correlation with Cash Advance, Cash Advance Frequency and Credit Limit. \nPayments variable has a high correletion with Purchases and one off Purchases.\nTenure has a negative correlation with Cash Advance and Cash Advance Frequency variables. "},{"metadata":{"trusted":false},"cell_type":"code","source":"ax = df[['BALANCE_FREQUENCY', 'PURCHASES_FREQUENCY',\n         'ONEOFF_PURCHASES_FREQUENCY', 'PURCHASES_INSTALLMENTS_FREQUENCY',\n         'CASH_ADVANCE_FREQUENCY','PRC_FULL_PAYMENT']].plot.kde(figsize=(12,9), bw_method=3) #,ind=[0, 2, 3,4]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <div align='center'> <font color='blue'> 4. Overview about Outliers"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,35))\n\nfor i in range(0,17):\n    plt.subplot(6, 3, i+1)\n    plt.boxplot(df[graph_by_variables[i]].dropna())\n    plt.title(graph_by_variables[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df[['BALANCE_FREQUENCY',\n 'PURCHASES_FREQUENCY',\n 'ONEOFF_PURCHASES_FREQUENCY',\n 'PURCHASES_INSTALLMENTS_FREQUENCY',\n 'CASH_ADVANCE_FREQUENCY',\n'PRC_FULL_PAYMENT']].plot.box(figsize=(18,10),title='Frequency',legend=True);\nplt.tight_layout()\n#tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Outliers will stay as it is. My aim is to define clusters within data rthym itself. "},{"metadata":{},"cell_type":"markdown","source":"## <div align='center'> <font color='blue'> 5.  Checking for NULL Values"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"df.isnull().sum().sort_values(ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <div align='center'> <font color='blue'> 6.  Filling of the Row Data"},{"metadata":{"trusted":false},"cell_type":"code","source":"df.MINIMUM_PAYMENTS  = df.MINIMUM_PAYMENTS.fillna(df.MINIMUM_PAYMENTS.mean()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.CREDIT_LIMIT      = df.CREDIT_LIMIT.fillna(df.CREDIT_LIMIT.mean()) ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"df.isnull().sum().sort_values(ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### New column names have been amended to the data set for the further steps. "},{"metadata":{"trusted":false},"cell_type":"code","source":"df.columns= ['BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES',\n       'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE',\n       'PURCHASES_FREQUENCY', 'ONEOFF_PURCH_FREQ',\n       'PURCH_INST_FREQ', 'CASH_ADVANCE_FREQ',\n       'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS',\n       'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT', 'TENURE']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <div align='center'> <span style=\"color:blue\"> 7.Clustering Methods"},{"metadata":{},"cell_type":"markdown","source":"Rewriting each histogram as a vector by using Euclidean distance."},{"metadata":{"trusted":false},"cell_type":"code","source":"scaler = StandardScaler()\ndf_std = scaler.fit_transform(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Custumers per cluster\n\nn_clusters = 10\n\nclustering = KMeans(n_clusters=n_clusters,\n                    random_state=0\n                   )\n\ncluster_labels = clustering.fit_predict(df_std)\n\n# plot cluster sizes\n\nplt.hist(cluster_labels, bins=range(n_clusters+1))\nplt.title ('Customers per Cluster')\nplt.xlabel('Cluster')\nplt.ylabel('Customers')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <div align='center'> <span style=\"color:blue\"> 7.1  Kmeans"},{"metadata":{},"cell_type":"markdown","source":"# <div align='center'> <span style=\"color:blue\"> 7.1.a Determining Number of Clusters with Elbow Method"},{"metadata":{"trusted":false},"cell_type":"code","source":"wcss = []\ncluster_list = range(1, 11)\nfor i in cluster_list :\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 40)\n    kmeans.fit(df_std)\n    wcss.append(kmeans.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(cluster_list, wcss)\nplt.title('Elbow Method')\nplt.xlabel('Clusters')\nplt.ylabel('WCSS')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <div align='center'> <span style=\"color:blue\"> 7.1.b Determining Number of Clusters with Silhouette Scores Method"},{"metadata":{"trusted":false},"cell_type":"code","source":"silhouette_scores = [] \n\nfor n_cluster in range(2, 11):\n    silhouette_scores.append( \n        silhouette_score(df_std, KMeans(n_clusters = n_cluster).fit_predict(df_std))) \n    \n# Plotting a bar graph to compare the results \nk = [2, 3, 4, 5, 6,7,8,9,10] \nplt.bar(k, silhouette_scores) \nplt.xlabel('Number of clusters', fontsize = 10) \nplt.ylabel('Silhouette Score', fontsize = 10) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to make a decision I checked Silhouette Scores. Even though, 3rd cluster model gives the highest score, I considered somewhere in 6th,7th and 8th clusters."},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:Blue\"> <div align='center'> 7.1.c Kmeans Clustering\n### Kmeans with Different Number of Clusters from 2 to 20"},{"metadata":{"trusted":false},"cell_type":"code","source":"kmeans_values=[]\n\nfor cluster in range(2,20):\n    kmeans = KMeans(n_clusters=cluster, random_state=40).fit_predict(df_std)\n    sil_score = metrics.silhouette_score(df_std,kmeans, metric='euclidean')\n    print(\"Silhouette score for {} cluster k-means: {:.3f}\".format(cluster,\n                                                               metrics.silhouette_score(df_std, kmeans, metric='euclidean')))\n    kmeans_values.append((cluster,sil_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Deciding, between 4th cluster and the 3rd and 8th cluster models will be completed at the end of this study by charts."},{"metadata":{"trusted":false},"cell_type":"code","source":"kmeans8 = df_std.copy() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"kmeans8_  = KMeans(n_clusters=8, random_state=40).fit(kmeans8) #without 'predict' we do not labels yet.","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"#If I would like to contiune with this model, I would add the clusters to the dataframe by following command.\n#df['Kmeans_cluster']=kmeans8_.labels_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:Blue\"> <div align='center'> 7.2 Hierarchical Clustering"},{"metadata":{"trusted":false},"cell_type":"code","source":"siliuette_list_hierarchical = []\n\nfor cluster in range(2,20,2):\n    for linkage_method in ['ward', 'average', 'complete']:\n        agglomerative = AgglomerativeClustering(linkage=linkage_method, affinity='euclidean',n_clusters=cluster).fit_predict(df_std)\n        sil_score = metrics.silhouette_score(df_std, agglomerative, metric='euclidean')\n        siliuette_list_hierarchical.append((cluster, sil_score, linkage_method, len(set(agglomerative)) ) )\n        \ndf_hierarchical = pd.DataFrame(siliuette_list_hierarchical, columns=['cluster', 'sil_score','linkage_method', 'number_of_clusters'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_hierarchical.sort_values('sil_score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I also would like to check further values in this method as below. "},{"metadata":{},"cell_type":"markdown","source":"I could catch similar value of %70 in 8 cluster model.\nDue to the results, I choose linkage_method :average , number of cluster:8\nThe silhouette score of the Agglomerative Clustering solution: 0.7095"},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:Blue\"> <div align='center'> 7.3 DBSCAN"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\nneigh = NearestNeighbors(n_neighbors=2)\nnbrs = neigh.fit(df_std)\ndistances, indices = nbrs.kneighbors(df_std)\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplt.plot(distances)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Until around 8000th variable eps is under 2. density is too much in the general data set Here we can also see "},{"metadata":{"trusted":false},"cell_type":"code","source":"siliuette_list_dbscan = []\n\nfor eps in np.arange(0.1,2,0.2):\n    for min_sample in range(1,10):\n        dbscan = DBSCAN(eps=eps, min_samples= min_sample)\n        dbscan.fit(df_std)\n        sil_score = metrics.silhouette_score(df_std, dbscan.labels_, metric='euclidean')\n        siliuette_list_dbscan.append((eps, min_sample, sil_score, len(set(dbscan.labels_))) )\n        \ndf_dbscan = pd.DataFrame(siliuette_list_dbscan, columns=['eps', 'min_samples', 'sil_score', 'number_of_clusters'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_dbscan.sort_values('sil_score', ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"siliuette_list_dbscan = []\n\nfor eps in np.arange(0.1,3,0.2):\n    for min_sample in range(1,20,4):\n        dbscan = DBSCAN(eps=eps, min_samples=min_sample)\n        dbscan.fit(df_std)\n        sil_score = metrics.silhouette_score(df_std, dbscan.labels_, metric='euclidean')\n        siliuette_list_dbscan.append((eps, min_sample, sil_score, len(set(dbscan.labels_))) )\ndf_dbscan = pd.DataFrame(siliuette_list_dbscan, columns=['eps', 'min_samples', 'sil_score', 'number_of_clusters'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_dbscan.sort_values('sil_score', ascending=False).tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even though, checking the silhouette score to choose the best DBSCAN metric does not give the best model for our data set. \nThis method seems not a suitable one for our desired model even with a high silhouette score. The reason behind this is that we have a high number of variables in the same cluster itself but distribution of variables does not distributes homogenous in all clusters.\n\nHowever,In order to see differences in clusters, I chose the one with 1 minimum sample and 10 clusters with eps:9.5 which gives DBSCAN silhouette_score of 0.751. "},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:Blue\"> <div align='center'> 7.4 Spectral Clustering"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Normalizing the Data \ndf_nor = normalize(df_std) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(9,6))\nplt.hist(df_std)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"plt.hist(df_nor)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking Spectral Values in Different 'n_neighbours' and 'Clusters' "},{"metadata":{"trusted":false},"cell_type":"code","source":"silhouette_list_spectral= []\n\nfor cluster in range(2,10):\n    for neighbours in np.arange (3,10,2):\n        spectral = SpectralClustering(n_clusters=cluster, affinity=\"nearest_neighbors\",n_neighbors=neighbours, assign_labels='discretize',\n                                      random_state=40).fit_predict(df_std)\n        sil_score = metrics.silhouette_score(df_std,spectral, metric='euclidean')\n        silhouette_list_spectral.append((cluster,sil_score, neighbours))\n\n    \ndf_spectral= pd.DataFrame(silhouette_list_spectral, columns=['cluster', 'sil_score', 'neighbours'] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_spectral.sort_values('sil_score', ascending= False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3, 5, 7, and 9 n_neighbours models give the highest Silhouette scores in 7 number of clusters.  I chose the 3 n_neighbour model."},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:Blue\"> <div align='center'> 7.5 GMM "},{"metadata":{},"cell_type":"markdown","source":"K-means does not account for variance. This works fine for when your data is circular. In contrast, Gaussian mixture models can handle even very oblong clusters. \nTherefore, I also would like to check this method to see the results of an unsupervised machine learning purpose. "},{"metadata":{"trusted":false},"cell_type":"code","source":"siliuette_list_GMM = []\n\nfor cluster in range(2,21,2):\n    for covariance_type in ['full', 'tied', 'diag', 'spherical']:\n        gmm  = GaussianMixture(n_components = cluster,covariance_type = covariance_type, random_state = 40).fit_predict(df_std)\n        sil_score = metrics.silhouette_score(df_std, gmm, metric='euclidean')\n        siliuette_list_GMM.append((cluster, sil_score, covariance_type, len(set(gmm)) ) )\n        \ndf_gmm = pd.DataFrame(siliuette_list_GMM, columns=['cluster', 'sil_score','covariance_type', 'number_of_clusters'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_gmm.sort_values('sil_score', ascending=False).tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Silhouette scores are very low. \nSpherical covariance type worked better in the model in 10 clusters. "},{"metadata":{},"cell_type":"markdown","source":"# <div align = \"center\"> <span style=\"color:Blue\">  8. Compering Results for the Optimal Number of Cluster Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"kmeans_      = KMeans(n_clusters=8, random_state=40).fit_predict(df_std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"gmm_         = GaussianMixture(n_components=10, covariance_type='spherical', random_state=40).fit_predict(df_std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"hierarchical_= AgglomerativeClustering(linkage='average', affinity='euclidean', n_clusters=7).fit_predict(df_std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"spectral_    = SpectralClustering(n_clusters=7, affinity=\"rbf\", n_neighbors=5, assign_labels='discretize',\n                                  random_state=40).fit_predict(df_nor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dbscan       = DBSCAN(eps=9.5, min_samples=1).fit_predict(df_std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"kmeansSilhouette_Score        = metrics.silhouette_score(df_std, kmeans_, metric='euclidean')\nGMM_Silhouette_Score          = metrics.silhouette_score(df_std, gmm_, metric='euclidean')\nDBSCAN_Silhouette_Score       = metrics.silhouette_score(df_std, dbscan, metric='euclidean')\nHierarchical_Silhouette_Score = metrics.silhouette_score(df_std, hierarchical_, metric='euclidean')\nSpectral_Silhouette_Score     = metrics.silhouette_score(df_std, spectral_, metric='euclidean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Clustering_Silhouette_Scores  = [ ['KMeans',kmeansSilhouette_Score ], ['GMM', GMM_Silhouette_Score],\n                                ['Hierarchical',Hierarchical_Silhouette_Score ], ['Spectral', Spectral_Silhouette_Score],\n                                ['DBSCAN', DBSCAN_Silhouette_Score]]\n\nClustering_Silhouette_Scores  = pd.DataFrame(Clustering_Silhouette_Scores, columns=['Clustering Method', 'Silhouette Score']) \nClustering_Silhouette_Scores.sort_values(by='Silhouette Score', ascending= False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Although the Hierarchical method seems more suitable with a high silhouette score, after checking clusters and the number of variables in each cluster, this method seems that it is not a suitable one to understand this data set.**"},{"metadata":{},"cell_type":"markdown","source":"## Hierarchical Method Chart "},{"metadata":{"trusted":false},"cell_type":"code","source":"df['cluster'] = hierarchical_ ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['cluster'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(15,10)) \nplt.title(\"Dendrogram with Linkage Method: Average\") \nplt.xlabel('Sample Index or Cluster Size') \nplt.ylabel('Distance') \n          \n\ndendrogram(linkage(df_std, method='average')) \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This insufficient interpretation of the model on data set can be seen by the results above.** \n\n\n**DBSCAN, as a second option was also used to see distribution of variables in clusters.**"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Adding clusters from the selected dbscan model to the data set.\n\ndf['cluster'] = dbscan ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['cluster'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Clearly, density is very much in the first cluster and lower number of variables in other clusters.**\n\n**However, checking cluster numbers and distribution of variables in each cluster shows that this method is not convenient for this data set.** \n\n**I continued with Kmeans. In the following sections, we can see the difference in two choices of kmeans models.**"},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:blue\"> <div align='center'> 9. DIMENSION REDUCTION"},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:Blue\"> <div align='center'> 9.1 PCA "},{"metadata":{},"cell_type":"markdown","source":"**Here, we will see the results after dimension reduction methods.**\n\n**Considering our choice of Kmeans model, I will continue with Kmeans with 8 clusters. (At the end of this study we also see the customer segmentation difference in 4 clusters Kmeans Model)**\n\n\n**PCA, T-SNE, and UMAP will be used in the following sections to see the differences and how it affects our Kmeans models.** "},{"metadata":{"trusted":false},"cell_type":"code","source":"#Following up with this model, I added the clusters to the dataframe by following command.\n\nkmeans_       = KMeans(n_clusters=8, random_state=40).fit(df_std)\n\ndf['cluster'] = kmeans_.labels_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Normalizing the Data \ndf_nor = normalize(df_std) \n\n\n# View the new feature data's shape \ndf_nor.shape ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### PCA in 2 Dimensions"},{"metadata":{"trusted":false},"cell_type":"code","source":"# PCA 2 dimensions\n\npca = PCA(n_components=2).fit(df_nor)\n\ndf_pca2 = pca.fit_transform(df_nor)\nprint(\"original shape:   \", df_nor.shape)\nprint(\"transformed shape:\", df_pca2.shape)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"#Result in a DataFrame\n\ndf_pca = pd.DataFrame(df_pca2) \ndf_pca.columns = ['A1', 'A2']\ndf_pca.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.scatter(df_pca['A1'], df_pca['A2'],\n            c = KMeans(n_clusters=8, random_state=40).fit_predict(df_pca2), cmap =None) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(pca.explained_variance_)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"plt.figure(figsize = (10,5))\nplt.plot(pca.explained_variance_ratio_)\nplt.title('Total variance explained: {}'.format(pca.explained_variance_ratio_.sum()))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# PCA 3 dimension\n\npca = PCA(n_components=3).fit(df_nor)\n\ndf_pca3 = pca.fit_transform(df_nor)\nprint(\"original shape:   \", df_nor.shape)\nprint(\"transformed shape:\", df_pca2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_pca3 = pd.DataFrame(df_pca3) \ndf_pca3.columns = ['A1', 'A2', 'A3']\ndf_pca3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_pca3.columns = ['A1', 'A2','A3'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig = px.scatter_3d(df_pca3, x=df_pca3['A1'], y= df_pca3['A2'], z=df_pca3['A3'], color=df['cluster']) \n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"print(pca.explained_variance_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we would like to use PCA dimension reduction, PCA in 3 dimensions gives a better score of explained variance."},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize = (10,5))\nplt.plot(pca.explained_variance_ratio_)\nplt.title('Total variance explained: {}'.format(pca.explained_variance_ratio_.sum()))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:Blue\"> <div align='center'> 9.2 T-SNE"},{"metadata":{},"cell_type":"markdown","source":"## Setting T-SNE 2 Dimensions"},{"metadata":{"trusted":false},"cell_type":"code","source":"tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\ndf_tsne2 = tsne.fit_transform(df_nor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#df_tsne2.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Store results of T-SNE in a data frame\nresult =  pd.DataFrame(df_tsne2, columns=['TSNE%i' % i for i in range(2)])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"plt.scatter(result['TSNE0'], result['TSNE1'],\n            c  = KMeans(n_clusters=8, random_state=40).fit_predict(df_tsne2), cmap =None) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:Blue\"> <div align='center'> 9.3 UMAP"},{"metadata":{},"cell_type":"markdown","source":"**UMAP is a nonlinear dimensionality reduction algorithm in the same family as t-SNE. In the first phase of UMAP, a weighted k nearest neighbor graph is computed, in the second a low dimensionality layout of this is then calculated. Then the embedded data points can be visualized in a new space and compared with other variables of interest.**"},{"metadata":{"trusted":false},"cell_type":"code","source":"import umap","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fit= umap.UMAP(n_neighbors=5, min_dist=0.3, metric='correlation')\n#umap_results=fit.fit_transform(df_nor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"umap_results=fit.fit_transform(df_nor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"umap_results.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.scatter(df_pca['A1'], df_pca['A2'],\n            c= KMeans(n_clusters=8, random_state=40).fit_predict(umap_results), cmap =None) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.scatter(umap_results[:, 0], umap_results[:, 1], cmap='Spectral',\n            c = KMeans(n_clusters=8, random_state=40).fit_predict(umap_results), s=8) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**After checking all dimension reduction Methods with their graphs, clearly we do not need any of those methods.** "},{"metadata":{},"cell_type":"markdown","source":"# <div align = \"center\"> <span style=\"color:Blue\"> 10.  Understanding Clusters by Customer Segmentation"},{"metadata":{},"cell_type":"markdown","source":"## 10.1 Kmeans Model with 8 Clusters"},{"metadata":{},"cell_type":"markdown","source":"kmeans8_  = KMeans(n_clusters=8, random_state=40).fit(df_std)\ndf['cluster'] = kmeans8_.labels_"},{"metadata":{},"cell_type":"markdown","source":"|CL     |BALANCE|PURCH|ONEOFF_PURC|INST_PURC|CASH_ADV|CASH_ADV_TRX|PURCH_TRX|CREDIT_LMT|PAYMENTS|MIN_PAYMENTS|PRC_FULL_PAYMENT|\n|:-----:|:-----:|:---:|:---------:|:-------:|:------:|:----------:|:-------:|:--------:|:------:|:----------:|:--------------:|\n|CL0    | LOW   | LOW |   LOW     | LOW     |  LOW   |   MED      |  LOW    |  MED     |  MED   |    LOW     |    MED         | \n|CL1    | MED   | LOW |   LOW     | MED     |  LOW   |   LOW      |  MED    |  MED     |  LOW   |   HIGH     |    MED         |\n|CL2    | MED   | LOW |   LOW     | MED     | HIGH   |   MED      |  LOW    |  LOW     |  LOW   |    LOW     |    MED         |\n|CL3    | HIGH  | MED |   LOW     | MED     | HIGH   |   HIGH     |  MED    |  MED     |  HIGH  |   HIGH     |    LOW         |\n|CL4    | HIGH  | HIGH|  HIGH     | HIGH    | HIGH   |   HIGH     |  HIGH   |  HIGH    |  HIGH  |   HIGH     |    HIGH        |\n|CL5    | MED   | LOW |   LOW     | LOW     | MED    |   MED      |  LOW    |  LOW     |  MED   |    MED     |    LOW         |\n|CL6    | MED   | MED |   MED     | MED     |: MED   |   MED      |  MED    |  MED     |  MED   |    LOW     |    MED         |"},{"metadata":{},"cell_type":"markdown","source":"|  CL   |BALANCE_FRE|PURCH_FREQ|ONEOFF_PURCH_FREQ|PURCH_INST_FREQ|CASH_ADV_FREQ |TENURE|\n|:-----:|:---------:|:--------:|:---------------:|:-------------:|:------------:|:----:|\n|CL0    |   LOW     |   LOW    |     LOW         |     LOW       |     LOW      |:----:|\n|CL1    |  HIGH     |   HIGH   |     LOW         |     HIGH      |     LOW      |:----:|\n|CL2    |   LOW     |   LOW    |     MED         |     LOW       |     MED      |:----:|\n|CL3    |  HIGH     |   LOW    |     MED         |     LOW       |     MED      |:----:|\n|CL4    |  HIGH     |   HIGH   |     HIGH        |     HIGH      |     LOW      |:----:|\n|CL5    |  HIGH     |   MED    |     MED         |     LOW       |     LOW      |:----:|\n|CL6    |  HIGH     |   HIGH   |     HIGH        |     MED       |     LOW      |:----:|\n"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"col_list= ['BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES',\n       'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE',\n       'PURCHASES_FREQUENCY', 'ONEOFF_PURCH_FREQ',\n       'PURCH_INST_FREQ', 'CASH_ADVANCE_FREQ',\n       'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS',\n       'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT', 'TENURE']\n\n\nfor column in col_list:\n    plt.figure(figsize=(15,3))\n    for i in range(0,7):\n        plt.subplot(1,7,i+1)\n        cluster = df[df['cluster']==i]\n        cluster[column].hist()\n        plt.title('{} \\n{}'.format(column, i))\n        \n    plt.tight_layout()\n    plt.show()\n    \n\n    ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"df.TENURE.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(6,6))\nsns.scatterplot(x='CREDIT_LIMIT', y='PAYMENTS', hue='cluster',data=df, s = df.TENURE*10);\nplt.xlabel('CREDIT_LIMIT',size=15)\n#plt.xlim([0,40000])\nplt.ylabel('PAYMENTS', size =10)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"best_columns = [\"BALANCE\", \"PURCHASES\", \"CASH_ADVANCE\",\"CREDIT_LIMIT\", \"PAYMENTS\", \"MINIMUM_PAYMENTS\", \"TENURE\"]\n\nbest_columns.append(\"cluster\")\nplt.figure(figsize=(25,25))\nsns.pairplot( df[best_columns], hue=\"cluster\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig, axes = plt.subplots(3,2,figsize=(20,12))\ntitle_font = {'family': 'arial', 'color': 'darkred','weight': 'bold','size': 14 }\n\nfor i in range(0,6):\n    \n    plt.subplot(3, 2, i+1)\n    plt.scatter(df['PAYMENTS'], df[best_columns[i]], c= df['cluster'], s=60)\n    legend = plt.legend(loc=\"upper left\", title=\"clusters\")\n    plt.title('Customers in '+ str(best_columns[i]), fontdict=title_font, fontsize=13)\n    #plt.xlabel('customer_behaviours',size=15)\n    plt.ylabel(str(best_columns[i]),size=15)\n    \n\nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This study aims to understand the customer behaviours based on credit card users.**\n\n  * *Large Payments* are done by a small group with expensive purchases and a credit limit that is between average and high.\n  * *Small group of people have a higher amount of *Cash Advance* especially after  payments of 30.000. Large group of people have a lower cash limit contrats large payments.\n  * *Credit Limit is very low on a large group of customers with little purchases. \n   \n"},{"metadata":{},"cell_type":"markdown","source":"## 10.2 Kmeans Model with 4 Clusters"},{"metadata":{"trusted":false},"cell_type":"code","source":"kmeans4_  = KMeans(n_clusters=4, random_state=40).fit(df_std)\ndf['cluster'] = kmeans4_.labels_\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"col_list= ['BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES',\n       'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE',\n       'PURCHASES_FREQUENCY', 'ONEOFF_PURCH_FREQ',\n       'PURCH_INST_FREQ', 'CASH_ADVANCE_FREQ',\n       'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS',\n       'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT', 'TENURE']\n\n\nfor column in col_list:\n    plt.figure(figsize=(15,3))\n    for i in range(0,4):\n        plt.subplot(1,4,i+1)\n        cluster = df[df['cluster']==i]\n        cluster[column].hist()\n        plt.title('{} \\n{}'.format(column, i))\n        \n    plt.tight_layout()\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"|CL     |BALANCE|PURCH|ONEOFF_PURC|INST_PURC|CASH_ADV|CASH_ADV_TRX|PURCH_TRX|CREDIT_LMT|PAYMENTS|MIN_PAYMENTS|PRC_FULL_PAYMENT|\n|:-----:|:-----:|:---:|:---------:|:-------:|:------:|:----------:|:-------:|:--------:|:------:|:----------:|:--------------:|\n|CL0    | LOW   | LOW |   LOW     | LOW     |  LOW   |   MED      |  LOW    |  LOW     |  LOW   |   LOW      |    HIGH        | \n|CL1    | MED   | LOW |   LOW     | MED     |  LOW   |   LOW      |  MED    |  MED     |  MED   |   MED      |    MED         |\n|CL2    | MED   | LOW |   LOW     | MED     | HIGH   |   MED      |  LOW    |  HIGH    |  HIGH  |   HIGH     |    HIGH        |\n|CL3    | HIGH  | MED |   LOW     | MED     | HIGH   |   HIGH     |  MED    |  LOW     |  LOW   |   LOW      |    LOW         |"},{"metadata":{},"cell_type":"markdown","source":"|  CL   |BALANCE_FRE|PURCH_FREQ|ONEOFF_PURCH_FREQ|PURCH_INST_FREQ|CASH_ADV_FREQ |TENURE|\n|:-----:|:---------:|:--------:|:---------------:|:-------------:|:------------:|:----:|\n|CL0    |  HIGH     |   HIGH   |     MED         |     HIGH      |     LOW      |:----:|\n|CL1    |  HIGH     |   LOW    |     LOW         |     LOW       |     LOW      |:----:|\n|CL2    |   MED     |   MED    |     LOW         |     MED       |     HIGH     |:----:|\n|CL3    |  HIGH     |   HIGH   |     HIGH        |     HIGH      |     LOW      |:----:|"},{"metadata":{"trusted":false},"cell_type":"code","source":"best_columns = [\"BALANCE\", \"PURCHASES\", \"CASH_ADVANCE\",\"CREDIT_LIMIT\", \"PAYMENTS\", \"MINIMUM_PAYMENTS\", \"TENURE\"]\n\nbest_columns.append(\"cluster\")\nplt.figure(figsize=(25,25))\nsns.pairplot( df[best_columns], hue=\"cluster\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <div align = \"center\"> <span style=\"color:Blue\">  11. Conclusion"},{"metadata":{},"cell_type":"markdown","source":"After executing different methods, I chose the Kmeans Model as data does not include different small groups but very similar in each other. Therefore, I did not prefer to use density-based algorithms.  I can use this kind of algorithms to research for extreme customers in a credit card fraud. This study aims customer segmentation by using customer behaviors.\n\n\nCompering 2 different Kmeans Models showed that we have a better understanding of customer segmentation by using the 8 clusters model.\n**Some of the outstanding results:**\n\n* **Cluster 0:** This customer group indicates a small group of customers who are small spenders with the lowest minimum payment. \n\n* **Cluster 1:** These customers purchase frequently with the highest installment purchase frequency percentage contrast of a lower cash advance percentage. This group is using their credit cards for a small number of purchases. \n\n* **Cluster 2:** This segment points out new customers with a lower credit limit and average balance level.\n\n* **Cluster 3:** This cluster targets a group of customers who have a high balance and cash advances. This group also has a low purchase frequency.  We can assume that this customer segment uses their credit cards as a loan.\n\n* **Cluster 4:** A small group of customers with the highest credit limit and the highest minimum payments. We can assume that these customers tend to increase credit limits to follow up on their spending habits. \n\n* **Cluster 5:** This segment has the lowest INSTALLMENTS_PURCHASES and PRC_FULL_PAYMENT percentages.  \n\n* **Cluster 6:** It is a similar customer segment with cluster 4 but with a lower Minimum Payment percentage. \n"},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}