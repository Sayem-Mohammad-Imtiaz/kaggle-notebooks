{"cells":[{"metadata":{"_uuid":"f5b08921464bc1c9b3f8a77b6ffdca310d43d311"},"cell_type":"markdown","source":"# Diamond Price Modelling\n\nThis is my first ever machine learning project. I will be applying what I've learnt so far (since 2 weeks) on this data set.\n\nIn this notebook, we will explore the factors that affect the price of a diamond with a goal of finding a model to help predict the price of diamonds.\n\n### Notice\n- The problem is requires **Supervised Learning**. The instances come with an expected output (*i.e* **the diamond's price**).\n- Predicting the price of a diamond from dataset is a **Regression Task**. More specifically, a **Multivariate Regression Task**.\n- We will be using the **Batch Learning** technique since the data is not live-fed from a source.\n- We will also be the **Root Mean Square Error (RMSE)** for our performance measure (typical for Regression tasks).\n\n**If you like this notebook, please up-vote! It keeps me motivated. Thank you! :)**"},{"metadata":{"trusted":true,"_uuid":"180c544ff0cc8846a61727131110c16931531b66"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\n\n# Load the diamond's dataset\ndiamonds = pd.read_csv(\"../input/diamonds.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bacd95ae85187616786bcffab1ba86d18cca80f2"},"cell_type":"markdown","source":"## Take a quick look at the Data Structure\n\nLet's take a quick look at our diamonds dataset."},{"metadata":{"trusted":true,"_uuid":"fdc508bba54af545a29398b1ebdfa0abc07b280a"},"cell_type":"code","source":"# Preview the top (five) rows of the dataset\ndiamonds.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92c4db009c52f110efdf17d9c3f4ba7211c26b71"},"cell_type":"markdown","source":"A little more information about our dataset."},{"metadata":{"trusted":true,"_uuid":"5ca1ebae61f1fd0037a1cdade95be2766c081be3"},"cell_type":"code","source":"# Preview the little information about dataset\ndiamonds.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9b541fe3a696f62d7a480a7690d18042068615f"},"cell_type":"markdown","source":"Noticed an unnecessary column `Unnamed: 0`. It just acts as index, it is not needed, thus that needs to be dropped."},{"metadata":{"trusted":true,"_uuid":"8d6406a2fd677b96ddd4f638a49bfe7cf261795a"},"cell_type":"code","source":"# Drop the \"Unnamed: 0\" column\ndiamonds = diamonds.drop(\"Unnamed: 0\", axis = 1)\n\n# Price is int64, best if all numeric attributes have the same datatype, especially as float64\ndiamonds[\"price\"] = diamonds[\"price\"].astype(float)\n\n# Preview dataset again\ndiamonds.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28560cb24e26e8cadd83899c3bbe730499505efa"},"cell_type":"markdown","source":"There are **53490** instances in the dataset. There are also no missing instances in the dataset. *That looks clean!*\n\nIt's easier to work a dataset when all its attributes are numerical. The **cut**, **color** and **clarity** attributes are non-numeric (They are *objects*). We still have to convert them to be numerical.\n\nLet's find out what categories exist for each of them."},{"metadata":{"trusted":true,"_uuid":"18c2df0783aac6512cd1c4985c03b15bd3b4779d"},"cell_type":"code","source":"# The diamond cut categories\ndiamonds[\"cut\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"937d53114ca7312ab391d80d533115a9e3ead78f"},"cell_type":"code","source":"# The diamond color categories\ndiamonds[\"color\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33266c36ddfc6f454ff1591fa47d8275f605155b"},"cell_type":"code","source":"# The diamond clarity categories\ndiamonds[\"clarity\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df9b04fe7978d0bc962ed9104086a7092e623ff6"},"cell_type":"markdown","source":"They each don't have so many categories, but still, we will be having a much longer table (*more columns*). That aside for now.\n\nLet's take a preview of the summary of the numerical attributes and then an histogram on the dataset."},{"metadata":{"trusted":true,"_uuid":"02e455ae818ba388e9d8857420d1cc012e1c32b7"},"cell_type":"code","source":"# Summary of each numerical attribute\ndiamonds.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c8fe62b6ee8c4f430ba826f5b40ba47f278ea8a"},"cell_type":"code","source":"diamonds.hist(bins = 50, figsize = (20, 15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22fff7e34396e9e175fb8166622a5cb6666587ec"},"cell_type":"markdown","source":"Okay good! But what about the categorical attributes? What happens to them now? We are still going to make out a solution for them. But there's also something important we have to do and that is **\"Creating a Test Set\"**.\n\n## Create a Test Set\n\nI've learnt it is a good practice to separate your **Train Set** and **Test Set** (80% and 20% from your dataset respectively). The Test set will make us see our model's performance on the new instances.\n\nThat's clear! **But**, even though we want to do this, it doesn't feel right taking purely random samples of the dataset, else we could introduce a significant **Sampling Bias**. A good solution is by performing **Stratified Sampling**. The dataset will be divided into homogeneous subgroups called strata, and the right number of instances is sampled from each stratum to guarantee that the test set is representative of the overall dataset. (*Hopefully, that's clear enough*)\n\nTo use this, we will need a very important attribute of the dataset to predict the of the diamond. How can we try select the attribute even though the team of those who gave you data didn't give you a clue? This is where we need to know the attributes that are most correlated to the price of the diamond. We use the **Standard Correlation Coefficient** (Pearson's r) to determine."},{"metadata":{"trusted":true,"_uuid":"66d1b01967ff318e41998140347f0e64c3699f60"},"cell_type":"code","source":"# Create a correlation matrix between every pair of attributes\ncorr_matrix = diamonds.corr()\n\n# Plot the correlation with seaborn\nplt.subplots(figsize = (10, 8))\nsns.heatmap(corr_matrix, annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"064348bdcba07f7c3254590dff0716f5c0b8bd50"},"cell_type":"markdown","source":"From the plot above, we could deduce that:\n- `x`, `y`, `z` have strong correlations with `price`\n- `carat` has the strongest correlation with `price` (0.92)\n- `table` and `depth` have the weakest correlations\n\nIt is amazing so see that `carat` correlates best with price. Its score is pretty high! Now we use this for our *Stratified Sampling*.\n\nLet's take a closer look at the `carat`'s histogram."},{"metadata":{"trusted":true,"_uuid":"0c3f65261fe0ad33e7e6de34b797a6a83357016a"},"cell_type":"code","source":"diamonds[\"carat\"].hist(bins = 50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"122aa7684963b30e097fb69a1fb01d953407c7fe"},"cell_type":"markdown","source":"Most diamonds are roughly between 0.3 and 1.5 Carats. Let's divide them into 5 categories, with those more than the 5th category merging into the 5th category."},{"metadata":{"trusted":true,"_uuid":"953fac985d3903433dcdbe8c3461a53bdd251405"},"cell_type":"code","source":"# Divide the diamond carats by 0.4 to limit the number of carat categories\n# Round up to have discrete categories\ndiamonds[\"carat_cat\"] = np.ceil(diamonds[\"carat\"] / 0.35)\n\n# Merge categories > 5 in 5\ndiamonds[\"carat_cat\"].where(diamonds[\"carat_cat\"] < 5, 5.0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"427c6871fc50f77f4e52802a9c50d262f91f2c00"},"cell_type":"markdown","source":"Now let's see how much the diamonds are distributed in relation to the carat categories."},{"metadata":{"trusted":true,"_uuid":"006117c7c255a67bf04e2cddbf3c331cb5694112"},"cell_type":"code","source":"# Check the distribution of the diamonds in the categories\ndiamonds[\"carat_cat\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd3a2e755bc153a4f1bef07743bff3bbf8a2c88d"},"cell_type":"code","source":"diamonds[\"carat_cat\"].hist()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c567a10b175c9e852152977ce62fbc391c16565a"},"cell_type":"markdown","source":"The distribution looks nice enough.\n\n**PS:** I adjusted the divisor of `carat` until the distribution of the diamonds looked nice. *You could comment if you have any other ideas to doing this* :)\n\nAnd yup! We can now perform a Stratified Sampling based on the carat categories :) I will use Scikit-Learn's `StratifiedShuffleSplit` class."},{"metadata":{"trusted":true,"_uuid":"d0ce7cc61315fa40c13c16b608c031e34a3bd805"},"cell_type":"code","source":"# Import the sklearn module\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# Run the split. Creates on split and shares 20% of the dataset for the test set\nsplit = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n\n# Separate the stratified train set and the test set\nfor train_index, test_index in split.split(diamonds, diamonds[\"carat_cat\"]):\n    strat_train_set = diamonds.loc[train_index]\n    strat_test_set = diamonds.loc[test_index]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99990d3d13318c1e7668fd8f5fd6a079a69e926a"},"cell_type":"markdown","source":"We now have our Train set and Test set, both stratified. From here, we don't need the `carat_cat` anylonger, hence we can drop it."},{"metadata":{"trusted":true,"_uuid":"0824412c2142a18b74cb03f08ed939daeca7d024"},"cell_type":"code","source":"for set in (strat_train_set, strat_test_set):\n    set.drop([\"carat_cat\"], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7bedf553477678764e1ff9b6f7ecae64365a49f"},"cell_type":"markdown","source":"Our new `diamonds` dataset will now be the *Stratified Train set*."},{"metadata":{"trusted":true,"_uuid":"ab127b165b0a45d9123cb061ef4945df440733b7"},"cell_type":"code","source":"# Redefined diamonds dataset\ndiamonds = strat_train_set.copy()\ndiamonds.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ebae8453d98ad9afe9d6669a7a713e9aa7c2c74"},"cell_type":"markdown","source":"## Data Visualization\n\nLet's play around with some visualization of our dataset and make some observations out of it."},{"metadata":{"trusted":true,"_uuid":"4916b2ebc18f57a26907c189198bcb7f6a30aae8"},"cell_type":"code","source":"sns.pairplot(diamonds[[\"price\", \"carat\", \"cut\"]], hue = \"cut\", height = 5)\nplt.show()\nsns.barplot(x = \"carat\", y = \"cut\", data = diamonds)\nplt.show()\nsns.barplot(x = \"price\", y = \"cut\", data = diamonds)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74b451953c77a976ebd96c3abdbdd0b1bcfdfc1f"},"cell_type":"markdown","source":"`Fair` cuts are most weighed, but they aren't the most expensive diamonds. `Premium` cuts weigh less than the fair and then cost more. `Ideal` cuts weigh way less and they are least expensive. The cut therefore is relatively considered while determining the price of the diamond."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"2f816c19ae22e7873b391ba76c6448d5b57b1b0f"},"cell_type":"code","source":"sns.pairplot(diamonds[[\"price\", \"carat\", \"color\"]], hue = \"color\", height = 5)\nplt.show()\nsns.barplot(x = \"carat\", y = \"color\", data = diamonds)\nplt.show()\nsns.barplot(x = \"price\", y = \"color\", data = diamonds)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7aa1e3012a301fd74d2c396b48b2632eb110bc7"},"cell_type":"markdown","source":"Here, we could see that the color `J` which is the most weighed is also the most priced. The last 2 plots are very similar. We could see here that the color of the diamond is also very dependent on its price."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"ddb904fae99fd7e990ce9072074afa7b9efe81df"},"cell_type":"code","source":"sns.pairplot(diamonds[[\"price\", \"carat\", \"clarity\"]], hue = \"clarity\", height = 5)\nplt.show()\nsns.barplot(x = \"carat\", y = \"clarity\", data = diamonds)\nplt.show()\nsns.barplot(x = \"price\", y = \"clarity\", data = diamonds)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7abdddc6dc5a878259b57fbdd086e0eed6ecb31c"},"cell_type":"markdown","source":"Here, we could see that `I1` doesn't hold the highest clarity, even though it is the most priced. But there's something else: Apart from `I1`, if the rest stays, the price of a diamond could **fairly** be relative to its clarity, to some extent."},{"metadata":{"trusted":true,"_uuid":"e67a08b347ec26b30a4df53983ed11581a9c1dc0"},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\n\nattributes = [\"x\", \"y\", \"z\", \"table\", \"depth\", \"price\"]\nscatter_matrix(diamonds[attributes], figsize=(25, 20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"badef2cfd2b351c95af73010ad9a11bb40f17a7f"},"cell_type":"markdown","source":"These visualizations have been met by our theories during correlation. And it's very obvious here that depth and table have very weak correlation with price.\n\n## Feature Scaling\n\nWith few exceptions, Machine Learning algrorithms don't perform well when the input numerical attributes have very different scales. We sure want our models to work well, so how can we go about it?\n\nFeature scaling can be done in 2 ways: **Min-max scaling** and **Standardization**. I would preferably use Standardization, because it is much less affected by outliers. Scikit-Learn provides a transformer called `StandardScaler` for this transformation.\n\n**PS:** You don't stratify your label, which is in our case `price`."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"ece416a8725491e86ec1e00cb3cd515fcf0f9e15"},"cell_type":"code","source":"# Do not stratify the label\ndiamonds = strat_train_set.drop(\"price\", axis = 1)\n\n# Set a new dataset label variable\ndiamond_labels = strat_train_set[\"price\"].copy()\n\n# Drop all the category, so we could have only numeric\ndiamonds_num = diamonds.drop([\"cut\", \"color\", \"clarity\"], axis = 1)\ndiamonds_num.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3dd7e82c4188d721dabb731aa16b19ad6d61ed37"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Perform the feature scaling on the numeric attributes of the dataset\nnum_scaler = StandardScaler()\ndiamonds_num_scaled = num_scaler.fit_transform(diamonds_num)\n\n# Preview \npd.DataFrame(diamonds_num_scaled).head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a21a62d81926067f07c205bee510a62f2f30659f"},"cell_type":"markdown","source":"That is what our data *will* look like during its processing. That's for the Machine Learning algorithm.\n\n## Handling Categorical Attributes\n\nFrom above, we split the columns of our dataset, putting aside the category attributes. Remember we still wanted to convert these attributes to numericaal attributes. How to we go about this?\n\nWe create one binary attribute per category: one attribute will be one while the rest will be 0. This is called **One-Hot Encoding**. Scikit-Learn provides a `OneHotEncoder` encoder to convert our category attributes to One-Hot vectors."},{"metadata":{"trusted":true,"_uuid":"14d4ba33d56062fd2086c713e03b39561675e6b5"},"cell_type":"code","source":"# We need only the category attributes to work with here\ndiamonds_cat = diamonds[[\"cut\", \"color\", \"clarity\"]]\ndiamonds_cat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"528d7257979f3d7fb4f785f4afce8ac64aca18f6"},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n# Perform the one-hot encoding on the category attributes of the dataset\ncat_encoder = OneHotEncoder()\ndiamonds_cat_encoded = cat_encoder.fit_transform(diamonds_cat)\n\n# Convert the encoded categories to arrays and Preview\npd.DataFrame(diamonds_cat_encoded.toarray()).head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ab065cef20b8f500eb9d1f1f6bbbeae2570787a"},"cell_type":"markdown","source":"Now that's what I meant by a longer table. What do we do next?\n\n## Transformation Pipeline\nWe have our tables reformed, what we do now to to merge the numeric feature scaled attributes and the encoded category attributes. An easy way to do this without writing so much like above is to Scikit-Learn's `ColumnTransformer` class. This merging provides a single pipeline for the whole dataset."},{"metadata":{"trusted":true,"_uuid":"7de87ca8ba09353c9860291435633fc5c1e11f09"},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\nnum_attribs = list(diamonds_num)\ncat_attribs = [\"cut\", \"color\", \"clarity\"]\n\n# Pipeline to transform our dataset\npipeline = ColumnTransformer([\n    (\"num\", StandardScaler(), num_attribs), # Perform feaured scaling on numeric attributes\n    (\"cat\", OneHotEncoder(), cat_attribs) # Perform One-Hot encoding on the category attributes\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2443855264fe92b3d33654396cf89cfdec94cba"},"cell_type":"code","source":"# Transformed dataset to feed the ML Algorithm\ndiamonds_ready = pipeline.fit_transform(diamonds)\n\n# Preview\npd.DataFrame(diamonds_ready).head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3826371ab3c449e4e08cc745306d1969ade0f730"},"cell_type":"markdown","source":"We now have our newly transformed dataset that can easily be fed to our Machine Learning Algorithms.\n\n## Select and Train Model\n\nWe will create one function that will run through each algorithm. We'll also have variables that hold results of the algorithms for future comparisons. What does our fat function do?\n\n- Fits the dataset into the model and create series of predictions to compare with labels. We check its performance with the **RMSE**.\n\n- Performs some evaluation using Cross validation, which splits the training set into a number of CVs and train the model on the smaller sets. We then compare its mean with our intial RMSE.\n\n- We view how well the model performs on our test set and compare its RMSE with that of the train set. Hopefully we don't have much difference.\n\n- Some accuracy test of the model on the dataset is being evaluated\n\n- We could also have a preview of what's going on. Some parts of the test set will be selected and then the model will run on them and we can then compare its result with the expected prices.\n\n- We also get a preview this model's plot results and see how well it fits"},{"metadata":{"trusted":true,"_uuid":"d6882945f964ff249494667b647de6974a8cf3f6"},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom random import randint\n\n# Our test set\n\n# Remove label from test set\nX_test = strat_test_set.drop(\"price\", axis = 1)\n# Have label stand alone\ny_test = strat_test_set[\"price\"].copy()\n\n# Our models performance holder\nmodels_rmse = [] # Holds Models original RMSE\ncvs_rmse_mean = [] # Holds the Cross Validation RMSE Mean\ntests_rmse = [] # Holds the tests RMSE\ntests_accuracy = [] # Holds the tests accuracy\nmodels = [] # Holds the models name\n\ndef display_model_performance(model_name, model, diamonds = diamonds_ready, labels = diamond_labels,\n                              models_rmse = models_rmse, cvs_rmse_mean = cvs_rmse_mean, tests_rmse = tests_rmse,\n                              tests_accuracy = tests_accuracy, pipeline = pipeline, X_test = X_test,\n                              y_test = y_test, cv = True):\n    # Fit dataset in model\n    model.fit(diamonds, labels)\n    \n    # Setup predictions\n    predictions = model.predict(diamonds)\n    \n    # Get models performance\n    model_mse = mean_squared_error(labels, predictions)\n    model_rmse = np.sqrt(model_mse)\n    \n    # Cross validation\n    cv_score = cross_val_score(model, diamonds, labels, scoring = \"neg_mean_squared_error\", cv = 10)\n    cv_rmse = np.sqrt(-cv_score)\n    cv_rmse_mean = cv_rmse.mean()\n    \n    print(\"RMSE: %.4f\" %model_rmse)\n    models_rmse.append(model_rmse)\n    \n    print(\"CV-RMSE: %.4f\" %cv_rmse_mean)\n    cvs_rmse_mean.append(cv_rmse_mean)\n    \n    print(\"--- Test Performance ---\")\n    \n    X_test_prepared = pipeline.transform(X_test)\n    \n    # Fit test dataset in model\n    model.fit(X_test_prepared, y_test)\n    \n    # Setup test predictions\n    test_predictions = model.predict(X_test_prepared)\n    \n    # Get models performance on test\n    test_model_mse = mean_squared_error(y_test, test_predictions)\n    test_model_rmse = np.sqrt(test_model_mse)\n    print(\"RMSE: %.4f\" %test_model_rmse)\n    tests_rmse.append(test_model_rmse)\n    \n    # Tests accuracy\n    test_accuracy = round(model.score(X_test_prepared, y_test) * 100, 2)\n    print(\"Accuracy:\", str(test_accuracy)+\"%\")\n    tests_accuracy.append(test_accuracy)\n    \n    # Check how well model works on Test set by comparing prices\n    start = randint(1, len(y_test))\n    some_data = X_test.iloc[start:start + 7]\n    some_labels = y_test.iloc[start:start + 7]\n    some_data_prepared = pipeline.transform(some_data)\n    print(\"Predictions:\\t\", model.predict(some_data_prepared))\n    print(\"Labels:\\t\\t\", list(some_labels))\n    \n    models.append(model_name)\n    \n    # Preview plot\n    plt.scatter(diamond_labels, model.predict(diamonds_ready))\n    plt.xlabel(\"Actual\")\n    plt.ylabel(\"Predicted\")\n    x_lim = plt.xlim()\n    y_lim = plt.ylim()\n    plt.plot(x_lim, y_lim, \"k--\")\n    plt.show()\n    \n    print(\"------- Test -------\")\n    plt.scatter(y_test, model.predict(X_test_prepared))\n    plt.xlabel(\"Actual\")\n    plt.ylabel(\"Predicted\")\n    plt.plot(x_lim, y_lim, \"k--\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6f32db1c13280f1c54c954b897120ce18682d84"},"cell_type":"markdown","source":"We can now start fitting models and get their performance error. Remember we are using **Root Mean Squared Error** for our performance measure.\n\nLet's start with the easiest model - `Linear Regression`"},{"metadata":{"_uuid":"bac23b587246aaf6775095ae1f6324fab64024b8"},"cell_type":"markdown","source":"### Linear Regression"},{"metadata":{"trusted":true,"_uuid":"0c8c9171ceb6868c8d1c5dcb2319b2f86aa6462a"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression(normalize = True)\ndisplay_model_performance(\"Linear Regression\", lin_reg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"938e0510394f768bb6dfdbdffc899e87debbdcb4"},"cell_type":"markdown","source":"### Decision Tree Regression"},{"metadata":{"trusted":true,"_uuid":"eeaddc06d8c2db4ad0c6ed71c8060931f0eaa83a"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor(random_state = 42)\ndisplay_model_performance(\"Decision Tree Regression\", tree_reg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"769bd7f2b126b42af35c29a0c516e500bb4c5c43"},"cell_type":"markdown","source":"### Random Forest Regression"},{"metadata":{"trusted":true,"_uuid":"2bd6e31f57bf170093f7c53ff9fe74e4569376ee"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators = 10, random_state = 42)\ndisplay_model_performance(\"Random Forest Regression\", forest_reg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b633325879c964bec93a3562d790f9679cd95b5"},"cell_type":"markdown","source":"### Ridge Regression"},{"metadata":{"trusted":true,"_uuid":"2ddd77db28061fbdcfe488382cefb4466131d2a8"},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\nridge_reg = Ridge(normalize = True)\ndisplay_model_performance(\"Ridge Regression\", ridge_reg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd96dc244455260d76df206219a446fac019b2c9"},"cell_type":"markdown","source":"### Lasso Regression"},{"metadata":{"trusted":true,"_uuid":"0e4996ab2102ab9b466e1739dd86ba73bf8f9d77"},"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\nlasso_reg = Lasso(normalize = True)\ndisplay_model_performance(\"Lasso Regression\", lasso_reg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab4e6eef934986706904e75c3b8de8fcaf9048e4"},"cell_type":"markdown","source":"### Elastic Net Regression"},{"metadata":{"trusted":true,"_uuid":"590c2f08a9a20881f314f70edf8357b3e2ab87e1"},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\n\nnet_reg = ElasticNet()\ndisplay_model_performance(\"Elastic Net Regression\", net_reg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26ade22b38261f84bf1dba9af99a69e3b5adf416"},"cell_type":"markdown","source":"### AdaBoost Regression"},{"metadata":{"trusted":true,"_uuid":"def894407aa834e7faf3df2db8574674a88309cb"},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\n\nada_reg = AdaBoostRegressor(n_estimators = 100)\ndisplay_model_performance(\"AdaBoost Regression\", ada_reg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb48f648e2b612e6c05d63222acc79675bfb6075"},"cell_type":"markdown","source":"### GradientBoosting Regression"},{"metadata":{"trusted":true,"_uuid":"9eec5d163eba4e5344739f07df114687893cb698"},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\ngrad_reg = GradientBoostingRegressor(n_estimators = 100, learning_rate = 0.1,\n                                     max_depth = 1, random_state = 42, loss = 'ls')\ndisplay_model_performance(\"GradientBoosting Regression\", grad_reg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"797e4a0019d413398e7d962844167fb9d5052350"},"cell_type":"markdown","source":"## Compare Models Performance"},{"metadata":{"trusted":true,"_uuid":"1eec5efd735edaed648f07a1b7bbaf1fd8710d38"},"cell_type":"code","source":"compare_models = pd.DataFrame({ \"Algorithms\": models, \"Models RMSE\": models_rmse, \"CV RMSE Mean\": cvs_rmse_mean,\n                              \"Tests RMSE\": tests_rmse, \"Tests Accuracy\": tests_accuracy })\ncompare_models.sort_values(by = \"Tests Accuracy\", ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab9ecea7bda47eb25adc95d6e5d138e9547db598"},"cell_type":"markdown","source":"Okay. I was really surprised seeing some **100% accuracy** from the `Decision Tree Regression `Model. That is just too perfect. Noticed that some dataset from the test set were selected and compared and that was just right!\n\nAnother model that we could depend on is the `Random Forest Regression`. It works relatively fine in my opinion."},{"metadata":{"trusted":true,"_uuid":"463d2b08a2bd8e585e658c5ddc3101d702e7b9b2"},"cell_type":"code","source":"sns.barplot(x = \"Tests Accuracy\", y = \"Algorithms\", data = compare_models)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a30b446781087a6ad229911016c768b215a56db6"},"cell_type":"markdown","source":"## Save model"},{"metadata":{"trusted":true,"_uuid":"3609a8dab0ff7ae8128da67b35e760d2ea0c3db6"},"cell_type":"code","source":"import pickle\n\nwith open('final_model.pkl', 'wb') as f:\n    pickle.dump(tree_reg, f)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a80f8f9cbd2390b91b442b6dd30c2617ac792e08"},"cell_type":"markdown","source":"## Conclusion\n\nThe **Decision Tree Algorithm** wins it all here!\n\nWhat do you think about the deduced model? Could it really be? Please comment your opinions.\n\nThat's all for now. I'm still learning, so I would love some feedback!\n\n**If you find this notebook useful, Please upvote this notebook it keeps me motivated.** Also I do look forward to suggestions, so please comment if any. Thank you!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}