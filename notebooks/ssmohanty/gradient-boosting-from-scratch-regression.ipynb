{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as pt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\npd.set_option('display.max_columns', None)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-29T06:25:03.31357Z","iopub.execute_input":"2021-07-29T06:25:03.31388Z","iopub.status.idle":"2021-07-29T06:25:04.695745Z","shell.execute_reply.started":"2021-07-29T06:25:03.313854Z","shell.execute_reply":"2021-07-29T06:25:04.694254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Data from the link :\nLink - https://www.kaggle.com/schirmerchad/bostonhoustingmlnd","metadata":{}},{"cell_type":"code","source":"input_ads = pd.read_csv('../input/bostonhoustingmlnd/housing.csv')\n\n#-----------------------------------------------------------------\nprint(input_ads.shape)\ninput_ads.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:25:04.942859Z","iopub.execute_input":"2021-07-29T06:25:04.943163Z","iopub.status.idle":"2021-07-29T06:25:04.986931Z","shell.execute_reply.started":"2021-07-29T06:25:04.94314Z","shell.execute_reply":"2021-07-29T06:25:04.984779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Null Check","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(input_ads.isnull().sum()).T","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:25:27.991209Z","iopub.execute_input":"2021-07-29T06:25:27.991573Z","iopub.status.idle":"2021-07-29T06:25:28.002728Z","shell.execute_reply.started":"2021-07-29T06:25:27.99154Z","shell.execute_reply":"2021-07-29T06:25:28.001432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Description of target variable","metadata":{}},{"cell_type":"code","source":"input_ads['MEDV'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:25:30.180387Z","iopub.execute_input":"2021-07-29T06:25:30.180738Z","iopub.status.idle":"2021-07-29T06:25:30.197184Z","shell.execute_reply.started":"2021-07-29T06:25:30.180709Z","shell.execute_reply":"2021-07-29T06:25:30.195999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Manipulation of dataset into train and test ","metadata":{}},{"cell_type":"code","source":"X = input_ads[[cols for cols in list(input_ads.columns) if 'MEDV' not in cols]]\ny = input_ads['MEDV']\n\nX, X_test, y, y_test = train_test_split(X, y, test_size=0.30, random_state=100)\n\n#--------------------------------------------------------------------------------\n#Scaling the datasets\nscaler = StandardScaler()\n\nX_arr = scaler.fit_transform(X)\nX_test_arr = scaler.fit_transform(X_test)\n\ny_arr = np.array(y).reshape(X_arr.shape[0],1)\ny_test_arr = np.array(y_test).reshape(X_test_arr.shape[0],1)\n\n#--------------------------------------------------------------------------------\nprint(X_arr.shape)\nprint(X_test_arr.shape)\nprint(y_arr.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:25:34.860922Z","iopub.execute_input":"2021-07-29T06:25:34.861269Z","iopub.status.idle":"2021-07-29T06:25:34.881102Z","shell.execute_reply.started":"2021-07-29T06:25:34.861244Z","shell.execute_reply":"2021-07-29T06:25:34.879944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Manual Gradient Boosting Code","metadata":{}},{"cell_type":"markdown","source":"## Defining some important UDF's for the training of gradient boosting","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\n#---------------------------------------------------------------------------------------------------------\n#Loss func\ndef loss_calc(y_true,y_pred):\n    \n    loss = (1/len(y_true)) * 0.5*np.sum(np.square(y_true-y_pred))\n        \n    return loss\n\n#---------------------------------------------------------------------------------------------------------\n#Gradient Calc\ndef gradient_calc(y_true,y_pred):\n    \n    grad = -(y_true-y_pred)\n    \n    return grad\n\n#---------------------------------------------------------------------------------------------------------\n#The base estimator\ndef tree_creator(r_state,X,y):\n    \n    d_tree = DecisionTreeRegressor(random_state=r_state,criterion='mse',\n                                    max_depth=2,min_samples_split=5,\n                                    min_samples_leaf=5,max_features=3)\n    d_tree.fit(X,y)\n    \n    return d_tree\n\n#---------------------------------------------------------------------------------------------------------\n#Predicting through gradient boosting regression\ndef predict_grad_boost(models_tray,alpha,test_x=X_test_arr,train_y=y_arr):\n    \n    initial_pred = np.array([np.mean(train_y)] * len(test_x))\n        \n    final_pred = initial_pred.reshape(len(initial_pred),1)\n    #print(final_pred.shape)\n    \n    for i in range(len(models_tray)):\n        \n        model = models_tray[i]\n        temp_pred = (model.predict(test_x)).reshape(len(test_x),1)\n        #print(temp_pred.shape)\n        final_pred -= alpha * temp_pred\n    \n    return final_pred\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:32:44.354414Z","iopub.execute_input":"2021-07-29T06:32:44.354703Z","iopub.status.idle":"2021-07-29T06:32:44.362108Z","shell.execute_reply.started":"2021-07-29T06:32:44.354678Z","shell.execute_reply":"2021-07-29T06:32:44.361227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UDF for Gradient Boosting training","metadata":{}},{"cell_type":"code","source":"def grad_boost_train(train_x,train_y,alpha=0.01,r_state=100,n_iters=101):\n\n    model_tray = [] #Tray to collect the trained boosted stage estimators\n    loss_counter = [] #Tray for loss capture\n\n    \n    initial_pred = np.array([np.mean(train_y)] * len(train_y))\n\n    print('Initial val :',initial_pred.shape)\n    model_pred = initial_pred.reshape(len(initial_pred),1)\n\n    for epoch in range(n_iters): #Unit iteration\n\n        if epoch%100==0:\n            print('#---------- Epoch number :',epoch,' -----------#')\n        \n        #Calculating loss\n        loss = loss_calc(y_true=train_y,\n                         y_pred=model_pred)\n\n        loss_counter.append(loss)\n        \n        #Calculating the gradient (residuals)\n        grads = gradient_calc(y_true=train_y,\n                              y_pred=model_pred)\n        #print(grads.shape)\n        #Building the regression tree on the gradient (residuals)\n        tree_grad = tree_creator(r_state=r_state,\n                                 X=train_x,\n                                 y=grads)\n        #print(train_x.shape)\n        #print(tree_grad.predict(train_x).shape)\n        \n        #Predicting the residuals according to the tree fit above\n        pred_m = (tree_grad.predict(train_x)).reshape(len(train_x),1)\n        \n        #Updating model through learning rate\n        model_pred -= alpha * pred_m\n        \n        #Appending the model into tray\n        model_tray.append(tree_grad)\n        \n    return model_tray,loss_counter,initial_pred","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:33:26.193713Z","iopub.execute_input":"2021-07-29T06:33:26.193995Z","iopub.status.idle":"2021-07-29T06:33:26.201416Z","shell.execute_reply.started":"2021-07-29T06:33:26.193972Z","shell.execute_reply":"2021-07-29T06:33:26.200273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Invoking the gradient boosting training UDF","metadata":{}},{"cell_type":"code","source":"#-----------------------------------------------------------------------------------------------------------------------\n#Defining some hyper-params\nn_estimators = 1001 #No of boosting steps\nalpha =0.01 #Learning rate\n\n#Training gradient boosting regression\nmodels_list,loss_counter,initial_pred = grad_boost_train(train_x=X_arr,\n                                                         train_y=y_arr,\n                                                         alpha=alpha,\n                                                         r_state=100,\n                                                         n_iters=n_estimators)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:33:52.574465Z","iopub.execute_input":"2021-07-29T06:33:52.574823Z","iopub.status.idle":"2021-07-29T06:33:53.231908Z","shell.execute_reply.started":"2021-07-29T06:33:52.574794Z","shell.execute_reply":"2021-07-29T06:33:53.230616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting the Loss curve (There should be a decrease in training loss over boosting rounds)","metadata":{}},{"cell_type":"code","source":"sns.set_style('darkgrid')\nax = sns.lineplot(x=range(n_estimators),y=loss_counter)\nax.set(xlabel='Number of boosting rounds',ylabel='Loss',title='Loss vs Boosting rounds plot')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:34:01.983707Z","iopub.execute_input":"2021-07-29T06:34:01.984046Z","iopub.status.idle":"2021-07-29T06:34:02.484475Z","shell.execute_reply.started":"2021-07-29T06:34:01.984006Z","shell.execute_reply":"2021-07-29T06:34:02.483509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insights - We see that the loss decreases sharply after ~150 boosting rounds then flattens for a long period of time indicating that the model is approaching the global minima in terms of the loss function","metadata":{}},{"cell_type":"markdown","source":"## Predicting on the test dataset using the manual training above (Only the trained residual models are passed) ","metadata":{}},{"cell_type":"code","source":"manual_gbm_pred = predict_grad_boost(models_tray=models_list, #Passing the fitted estimators into the predict function\n                                     alpha=alpha, #The alpha val used during training\n                                     test_x=X_test_arr) #Test dataset\nmanual_gbm_pred.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:34:41.741151Z","iopub.execute_input":"2021-07-29T06:34:41.741474Z","iopub.status.idle":"2021-07-29T06:34:41.816197Z","shell.execute_reply.started":"2021-07-29T06:34:41.741442Z","shell.execute_reply":"2021-07-29T06:34:41.814977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating the predictions from the manual gradient boosting model","metadata":{}},{"cell_type":"code","source":"print('MSE of test set :',mean_squared_error(y_test_arr,manual_gbm_pred))\nprint('RMSE of test set :',np.sqrt(mean_squared_error(y_test_arr,manual_gbm_pred)))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:34:45.491902Z","iopub.execute_input":"2021-07-29T06:34:45.492233Z","iopub.status.idle":"2021-07-29T06:34:45.497713Z","shell.execute_reply.started":"2021-07-29T06:34:45.492199Z","shell.execute_reply":"2021-07-29T06:34:45.496826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Benchmarking against sklearn implementation of gradient boosting","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\nskl_gbm = GradientBoostingRegressor(random_state=100,n_estimators=1001,criterion='mse',\n                                    max_depth=2,min_samples_split=5,\n                                    min_samples_leaf=5,max_features=3)\n\n#-------------------------------------------------------------------------------\nskl_gbm.fit(X_arr,y_arr)\nskl_pred = skl_gbm.predict(X_test_arr)\n\n#-------------------------------------------------------------------------------\nprint('MSE of test set :',mean_squared_error(y_test_arr,skl_pred))\nprint('RMSE of test set :',np.sqrt(mean_squared_error(y_test_arr,skl_pred)))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:34:49.17179Z","iopub.execute_input":"2021-07-29T06:34:49.172119Z","iopub.status.idle":"2021-07-29T06:34:49.540365Z","shell.execute_reply.started":"2021-07-29T06:34:49.172092Z","shell.execute_reply":"2021-07-29T06:34:49.539077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Comments :\n1. The manual implementation of the gradient boosting algorithm is providing similar results to its sklearn counterpart (Even better in terms of MSE and RMSE metric) \n2. This notebook can be easily scaled to other datasets as well for robust benchmarking","metadata":{}},{"cell_type":"markdown","source":"# END","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}