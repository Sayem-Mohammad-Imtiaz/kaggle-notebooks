{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as pt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\npd.set_option('display.max_columns', None)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-31T17:49:56.858066Z","iopub.execute_input":"2021-07-31T17:49:56.858552Z","iopub.status.idle":"2021-07-31T17:49:58.263884Z","shell.execute_reply.started":"2021-07-31T17:49:56.858454Z","shell.execute_reply":"2021-07-31T17:49:58.262602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing data","metadata":{}},{"cell_type":"code","source":"input_ads = pd.read_csv('../input/titanic/train_data.csv')\ninput_ads.drop(columns=['Unnamed: 0','Title_1','Title_2','Title_3','Title_4'],inplace=True) #Dropping un-necessary columns\n#-----------------------------------------------------------------\nprint(input_ads.shape)\ninput_ads.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:49:58.265883Z","iopub.execute_input":"2021-07-31T17:49:58.266291Z","iopub.status.idle":"2021-07-31T17:49:58.339674Z","shell.execute_reply.started":"2021-07-31T17:49:58.266253Z","shell.execute_reply":"2021-07-31T17:49:58.338393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Null Check","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(input_ads.isnull().sum()).T","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:49:58.341882Z","iopub.execute_input":"2021-07-31T17:49:58.342239Z","iopub.status.idle":"2021-07-31T17:49:58.360182Z","shell.execute_reply.started":"2021-07-31T17:49:58.342199Z","shell.execute_reply":"2021-07-31T17:49:58.35894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Description of the data","metadata":{}},{"cell_type":"code","source":"input_ads.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:49:58.361962Z","iopub.execute_input":"2021-07-31T17:49:58.362317Z","iopub.status.idle":"2021-07-31T17:49:58.426215Z","shell.execute_reply.started":"2021-07-31T17:49:58.362282Z","shell.execute_reply":"2021-07-31T17:49:58.425133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Description of target variable","metadata":{}},{"cell_type":"code","source":"#Total survived vs not-survived split in the training data\ninput_ads['Survived'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:49:58.427686Z","iopub.execute_input":"2021-07-31T17:49:58.428292Z","iopub.status.idle":"2021-07-31T17:49:58.439788Z","shell.execute_reply.started":"2021-07-31T17:49:58.428245Z","shell.execute_reply":"2021-07-31T17:49:58.438429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Manipulation of data into train-test","metadata":{}},{"cell_type":"code","source":"target = 'Survived' #To predict\n\n#--------------------------------------------------------------------------------\n#Splitting into X & Y datasets (supervised training)\nX = input_ads[[cols for cols in list(input_ads.columns) if target not in cols]]\ny = input_ads[target]\n\n#--------------------------------------------------------------------------------\n#Since test data is already placed in the input folder separately, we will just import it\ntest_ads = pd.read_csv('../input/titanic/test_data.csv')\ntest_ads.drop(columns=['Unnamed: 0','Title_1','Title_2','Title_3','Title_4'],inplace=True) #Dropping un-necessary columns\n\n#Splitting into X & Y datasets (supervised training)\nX_test = test_ads[[cols for cols in list(test_ads.columns) if target not in cols]]\ny_test = test_ads[target]\n\nprint('Train % of total data:',100 * X.shape[0]/(X.shape[0] + X_test.shape[0]))\n#--------------------------------------------------------------------------------\n#Manipulation of datasets for convenience and consistency\nX_arr = np.array(X)\nX_test_arr = np.array(X_test)\n\ny_arr = np.array(y).reshape(X_arr.shape[0],1)\ny_test_arr = np.array(y_test).reshape(X_test_arr.shape[0],1)\n\n#--------------------------------------------------------------------------------\n#Basic Summary\nprint(X_arr.shape)\nprint(X_test_arr.shape)\nprint(y_arr.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:49:58.441198Z","iopub.execute_input":"2021-07-31T17:49:58.44171Z","iopub.status.idle":"2021-07-31T17:49:58.471465Z","shell.execute_reply.started":"2021-07-31T17:49:58.441673Z","shell.execute_reply":"2021-07-31T17:49:58.470431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Standard scaling the x-data","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n#----------------------------------------------------------\nscaler = StandardScaler()\nX_arr = scaler.fit_transform(X_arr)\nX_test_arr = scaler.transform(X_test_arr)\n\n#----------------------------------------------------------\nX_arr[0:3]","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:49:58.472862Z","iopub.execute_input":"2021-07-31T17:49:58.473177Z","iopub.status.idle":"2021-07-31T17:49:58.482156Z","shell.execute_reply.started":"2021-07-31T17:49:58.473124Z","shell.execute_reply":"2021-07-31T17:49:58.481214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Artificial Neural Network (ANN) from Scratch","metadata":{}},{"cell_type":"markdown","source":"## UDFs for activation, initialization, layer_propagation","metadata":{}},{"cell_type":"code","source":"#All popular activation functions\ndef activation_fn(z,type_):\n    \n    #print('Activation : ',type_)\n    \n    if type_=='linear':\n        activated_arr = z\n    \n    elif type_=='sigmoid':\n        activated_arr = 1/(1+np.exp(-z))\n    \n    elif type_=='relu': \n        activated_arr = np.maximum(np.zeros(z.shape),z)\n    \n    elif type_=='tanh':\n        activated_arr = (np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n    \n    elif type_=='leaky_relu':\n        activated_arr = np.maximum(0.01*z,z)\n    \n    elif type_=='softmax':\n        exp_ = np.exp(z)\n        exp_sum = np.sum(exp_)\n        activated_arr = exp_/exp_sum\n        \n    return activated_arr\n\n#----------------------------------------------------------------------------------------------------------------------------\n#Initialization of params\ndef generate_param_grid(a_prev,n_hidden,hidden_size_list):\n    \n    parameters = {}\n    features = a_prev.shape[0] #Total features\n    n_examples = a_prev.shape[1]\n    \n    for n_hidden_idx in range(1,n_hidden+1):\n        \n        n_hidden_nodes = hidden_size_list[n_hidden_idx] #Should start from 0\n        \n        #print('#------------ Layer :',n_hidden_idx,'---- Size :',n_hidden_nodes,'---- Prev features :',features,'------#')\n\n        parameters['w' + str(n_hidden_idx)] = np.random.rand(n_hidden_nodes,features) * 0.1 #Xavier Initialization\n        parameters['b' + str(n_hidden_idx)] = np.zeros((n_hidden_nodes,1)) * 0.1\n        \n        features = n_hidden_nodes\n    \n    return parameters#Return randomly initiated params\n    \n#---------------------------------------------------------------------------------------------------------------------------\n#Propagation between z and activation\ndef layer_propagation(a_prev,w,b,activation):\n    \n    #print(a_prev.shape)\n    #print(w.shape)\n    #print(b.shape)\n    \n    z_ = np.dot(w,a_prev) + b\n    \n    a = activation_fn(z=z_,\n                      type_=activation)\n    \n    return z_,a","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:49:58.484958Z","iopub.execute_input":"2021-07-31T17:49:58.485307Z","iopub.status.idle":"2021-07-31T17:49:58.497706Z","shell.execute_reply.started":"2021-07-31T17:49:58.485272Z","shell.execute_reply":"2021-07-31T17:49:58.496726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UDF for forward propagation","metadata":{}},{"cell_type":"code","source":"def forward_propagation(params_dict,data_x,data_y,n_hidden,hidden_size_list,activation_list):\n    \n    cache = {'a0' : data_x.T}\n    a = data_x.T.copy()\n    \n    for layer_idx in range(1,n_hidden+1):\n        \n        #print('#---------- Layer :',layer_idx,'-- No of Nodes :',hidden_size_list[layer_idx])\n        #nodes = hidden_size_list[layer_idx]\n        activation_ = activation_list[layer_idx]\n        w_ = params_dict['w'+str(layer_idx)]\n        b_ = params_dict['b'+str(layer_idx)]\n        \n        z,a = layer_propagation(a_prev=a,\n                                 w=w_,\n                                 b=b_,\n                                 activation=activation_)\n        \n        cache['z'+str(layer_idx)] = z\n        cache['a'+str(layer_idx)] = a\n    \n    return cache,a","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:49:58.499538Z","iopub.execute_input":"2021-07-31T17:49:58.500051Z","iopub.status.idle":"2021-07-31T17:49:58.516199Z","shell.execute_reply.started":"2021-07-31T17:49:58.500003Z","shell.execute_reply":"2021-07-31T17:49:58.514975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UDF for cost calculation, gradient calculation & back-propagation ","metadata":{}},{"cell_type":"code","source":"#Calculation of the total cost incurred by the model\ndef cost_calculation(activation_list,y_true,y_pred):\n    \n    if activation_list[-1]=='sigmoid':\n        #print('sig')\n        m = y_true.shape[1]\n        cost = (-1/m) * np.sum((y_true * np.log(y_pred)) + ((1-y_true) * np.log(1 - y_pred)))\n        \n    elif activation_list[-1]=='linear':\n        \n        m = y_true.shape[1]\n        cost = (1/m) * np.sum(np.square(y_true-y_pred))\n        \n     ##-------------------->> Softmax to be added <<----------------------\n    \n    return cost\n\n#Gradient of the activation functions wrt corresponding z\n#--------------------------------------------------------------------------------------------\n#Gradient for each activation type\ndef grad_fn_dz(activation,a):\n    \n    if activation=='linear':\n        grad = 1\n     \n    elif activation=='sigmoid':\n        grad = a*(1-a)\n        \n    elif activation=='tanh':\n        grad = np.square(1-a)\n        \n    elif activation=='relu':\n        grad = np.where(a>=0,1,0)\n    \n    elif activation=='leaky_relu':\n        grad = np.where(a>=0,1,0.01)\n    \n    ##-------------------->> Softmax to be added <<----------------------\n    \n    return grad\n        \n#--------------------------------------------------------------------------------------------\n#UDF for gradient of loss function wrt last layer\ndef dL_last_layer(activation_list,y_true,y_pred):\n    \n    if activation_list[-1]=='sigmoid':\n        \n        #print('Last Layer y true shape :',y_true.shape)\n        #print('Last Layer y pred shape :',y_pred.shape)\n        \n        grad_final_layer = -((y_true/y_pred) - ((1-y_true)/(1-y_pred)))\n        #print('Last Layer gradient shape :',grad_final_layer.shape)\n        \n    elif activation_list[-1]=='linear':\n        \n        grad_final_layer = - 2 * (y_true-y_pred) #Check the sign\n        \n    return grad_final_layer\n\n#--------------------------------------------------------------------------------------------\n#Back=Propagation         \ndef back_propagation(cache,params_dict,data_x,data_y,n_hidden,hidden_size_list,activation_list,y_pred):\n    \n    grads_cache = {}\n    #db_cache = {}\n    \n    da = dL_last_layer(activation_list=activation_list,\n                             y_true=data_y.T,\n                             y_pred=y_pred)\n    #print('Final da shape :',da.shape)\n    \n    m = data_y.shape[0] #Data in the batches\n    \n    #print('dm in backprop :',m)\n    for layer_idx in list(reversed(range(1,n_hidden+1))):\n        \n        #print('# -------- Layer :',layer_idx,'-------- Size :',hidden_size_list[layer_idx],'--------#')\n        \n        activation_ = activation_list[layer_idx]\n        a = cache['a'+str(layer_idx)]\n        a_prev = cache['a'+str(layer_idx-1)]\n        w = params_dict['w'+str(layer_idx)]\n        \n#         print('Shape of a:',a.shape)\n#         print('Shape of a_prev:',a_prev.shape)\n#         print('SHape of w:',w.shape)\n        \n        #z = \n        \n        dz =  da * (grad_fn_dz(activation=activation_,a=a))\n        \n        #print('dz shape :',dz.shape)\n                     \n        dw = (1/m) * np.dot(dz, a_prev.T)\n        #print('dw shape :',dw.shape)\n        grads_cache['dw'+str(layer_idx)] = dw\n                     \n        db = (1/m) * np.sum(dz, axis=1,keepdims=True)\n        #print('db shape :',db.shape)\n        grads_cache['db'+str(layer_idx)] = db\n        \n        da = np.dot(w.T,dz)\n        #print('da shape :',da.shape)\n\n    return grads_cache","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:49:58.518129Z","iopub.execute_input":"2021-07-31T17:49:58.518813Z","iopub.status.idle":"2021-07-31T17:49:58.539929Z","shell.execute_reply.started":"2021-07-31T17:49:58.518756Z","shell.execute_reply":"2021-07-31T17:49:58.53856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UDF for updating weights through gradient descent","metadata":{}},{"cell_type":"code","source":"def update_weights(params,grads_cache,alpha,n_hidden):\n    \n    for layer_idx in list(reversed(range(1,n_hidden+1))):\n        \n        #print('#---- layer :',layer_idx,'----#')\n        \n        dw = grads_cache['dw'+str(layer_idx)]\n        db = grads_cache['db'+str(layer_idx)]\n        \n#         print('dw shape :',dw.shape)\n#         print('db shape :',db.shape)\n#         print('w shape :',params['w'+str(layer_idx)].shape)\n        \n        params['w'+str(layer_idx)] -= alpha * dw\n        params['b'+str(layer_idx)] -= alpha * db\n\n    \n    return params","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:49:58.541647Z","iopub.execute_input":"2021-07-31T17:49:58.542108Z","iopub.status.idle":"2021-07-31T17:49:58.555864Z","shell.execute_reply.started":"2021-07-31T17:49:58.54206Z","shell.execute_reply":"2021-07-31T17:49:58.554704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UDF for predictions","metadata":{}},{"cell_type":"code","source":"def prediction(params,test_x,n_hidden,hidden_size_list,activation_list,threshold):\n    \n    #-----------------------------------------------------------------\n    #Forward Propagation on trained weights\n    cache,y_pred = forward_propagation(params_dict=params,\n                                  data_x=test_x,\n                                  data_y=None,\n                                  n_hidden=n_hidden,\n                                  hidden_size_list=hidden_size_list,\n                                  activation_list=activation_list)\n    #print(cache)\n    preds = np.where(y_pred>threshold,1,0).astype(float)\n    return cache,np.round(y_pred,4),preds","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:49:58.557589Z","iopub.execute_input":"2021-07-31T17:49:58.55804Z","iopub.status.idle":"2021-07-31T17:49:58.56832Z","shell.execute_reply.started":"2021-07-31T17:49:58.55799Z","shell.execute_reply":"2021-07-31T17:49:58.567159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stochastic Gradient Descent (SGD) for training of the ANN ","metadata":{}},{"cell_type":"code","source":"def ANN_train_sgd(data_x_overall,data_y_overall,batch_size,alpha,n_iters,n_hidden,hidden_size_list,activation_list):\n    \n    print('Total training rows :',data_x_overall.shape[0])\n    \n    #----------------------------------------------------------------------------------------\n    #Creating x-y batches according to the provided batch_size\n    \n    n_batches = data_x_overall.shape[0]//batch_size\n    print('Total Batches to create in each epoch/iter :',n_batches)\n    \n    batches_x = np.array_split(data_x_overall,n_batches)\n    print('Total Batches of X:',len(batches_x))\n\n    batches_y = np.array_split(data_y_overall,n_batches)\n    print('Total Batches of y:',len(batches_y))\n    #-------------------------------------------------------------------------------------------\n    cost_history = [] #Record of cost through epochs\n\n    #-------------------------------------------------------------------------------------------\n    #Initialization of params\n    params_dict = generate_param_grid(a_prev=data_x_overall.T,\n                             n_hidden=n_hidden,\n                             hidden_size_list=hidden_size_list)\n    print('#----------------- Initial params ------------------#')\n    print(params_dict)\n    initial_params_abcd = params_dict.copy()\n    \n    #-------------------------------------------------------------------------------------------\n    cache_tray = []\n\n    for epoch in range(n_iters):\n\n        if (epoch>0) & (epoch%100==0):\n            print('#----------------------------------- Epoch :',epoch,'--------------------------------------#')\n            print('cost :',cost)\n            \n        for j in range(len(batches_x)): #For each batch created for each epoch/iter\n            \n            #-------------------------------------------------------------------------\n            #For each batch of data\n            data_x = batches_x[j]\n            data_y = batches_y[j]\n\n            #-------------------------------------------------------------------------\n            #Forward Propagation\n            cache,y_pred = forward_propagation(params_dict=params_dict,\n                                          data_x=data_x,\n                                          data_y=data_y,\n                                          n_hidden=n_hidden,\n                                          hidden_size_list=hidden_size_list,\n                                          activation_list=activation_list)\n            #print(np.max(y_pred))\n            #cache_tray.append(cache)\n            #-------------------------------------------------------------------------\n            #Cost calculation\n            cost = cost_calculation(activation_list=activation_list,\n                             y_true=data_y.T,\n                             y_pred=y_pred)\n\n            #cost_history.append(cost)\n            #print('cost :',cost)\n\n            #-------------------------------------------------------------------------\n            #Back Propagation\n            grads_cache_ = back_propagation(cache=cache,\n                                           params_dict=params_dict,\n                                           data_x=data_x,\n                                           data_y=data_y,\n                                           n_hidden=n_hidden,\n                                           hidden_size_list=hidden_size_list,\n                                           activation_list=activation_list,\n                                           y_pred=y_pred)\n\n            #------------------------------------------------------------------------\n            #Updating weights\n            params_dict = update_weights(params=params_dict,\n                                         grads_cache=grads_cache_,\n                                         alpha=alpha,\n                                         n_hidden=n_hidden)\n            \n        cost_history.append(cost) #Appending cost after each epoch\n\n\n    return initial_params_abcd,params_dict,grads_cache_,cost_history,y_pred,cache_tray\n","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:54:40.54151Z","iopub.execute_input":"2021-07-31T17:54:40.54212Z","iopub.status.idle":"2021-07-31T17:54:40.557936Z","shell.execute_reply.started":"2021-07-31T17:54:40.542084Z","shell.execute_reply":"2021-07-31T17:54:40.556461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the model by invoking the above UDF","metadata":{}},{"cell_type":"code","source":"#Defining hyper-parameters for ANN\n#--------------------------------------------------------------------------------------------------------------------------\nn_hidden = 2       #No of hidden layers\nalpha = 0.003      #Learning_rate\nn_iters = 501      #Total epochs\nhidden_size_list = [0,3,1]               #first element will be 0 and not counted in hidden layers\nactivation_list = [0,'relu','sigmoid']   #first element will be 0 and not counted in hidden layers\nbatch_size = 25    #Batch wise gradient descent\n\n#--------------------------------------------------------------------------------------------------------------------------\ninitial_params_train,params_dict_train,grads,cost_history_train,y_pred_train,cache_tray = ANN_train_sgd(data_x_overall=X_arr,\n                                                                                                       data_y_overall=y_arr,\n                                                                                                       batch_size=batch_size,\n                                                                                                       alpha=alpha,\n                                                                                                       n_iters=n_iters,\n                                                                                                       n_hidden=n_hidden,\n                                                                                                       hidden_size_list=hidden_size_list,\n                                                                                                       activation_list=activation_list)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:54:43.093593Z","iopub.execute_input":"2021-07-31T17:54:43.094048Z","iopub.status.idle":"2021-07-31T17:54:45.627452Z","shell.execute_reply.started":"2021-07-31T17:54:43.094006Z","shell.execute_reply":"2021-07-31T17:54:45.625995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cost-Epoch plot for the manual ANN training ","metadata":{}},{"cell_type":"code","source":"#Cost plot over epochs (1 value at end of each epoch) - over the last batch\nax = sns.lineplot(x=list(range(n_iters)),y=cost_history_train)\nax.set(xlabel='epochs',ylabel='cost',title='Cost vs epoch plot for Manual ANN')","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:54:50.883652Z","iopub.execute_input":"2021-07-31T17:54:50.88416Z","iopub.status.idle":"2021-07-31T17:54:51.113722Z","shell.execute_reply.started":"2021-07-31T17:54:50.884102Z","shell.execute_reply":"2021-07-31T17:54:51.112477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict on the test data","metadata":{}},{"cell_type":"code","source":"cache,preds_proba,manual_preds = prediction(params=params_dict_train,\n                                            test_x=X_test_arr,\n                                            n_hidden=n_hidden,\n                                            hidden_size_list=hidden_size_list,\n                                            activation_list=activation_list,\n                                            threshold=0.5)\n\n#-------------------------------------------------------------------------------------------\nprint('Shape of prediction array :',preds_proba.shape)\nprint('Unique predictions :',np.unique(manual_preds))\nprint('Unique of predict proba :',np.unique(preds_proba),'\\n')\n\nprint('#--------------------- Evaluation ----------------------#')\n#Evaluation of the predictions\nprint('ROC AUC of test set :',roc_auc_score(y_test_arr.ravel(),manual_preds.ravel()))\nprint('Accuracy of test set :',accuracy_score(y_test_arr.ravel(),manual_preds.ravel()))","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:54:56.738605Z","iopub.execute_input":"2021-07-31T17:54:56.739089Z","iopub.status.idle":"2021-07-31T17:54:56.756951Z","shell.execute_reply.started":"2021-07-31T17:54:56.739041Z","shell.execute_reply":"2021-07-31T17:54:56.755823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Benchmarking with Keras functional API","metadata":{}},{"cell_type":"markdown","source":"## Importing necessary libraries","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport keras\nimport tensorflow.keras.models\nimport tensorflow.keras.layers as tfl\nfrom tensorflow.keras import Input\nfrom tensorflow.keras import Model\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.layers import BatchNormalization","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:52:00.110812Z","iopub.execute_input":"2021-07-31T17:52:00.111277Z","iopub.status.idle":"2021-07-31T17:52:00.117505Z","shell.execute_reply.started":"2021-07-31T17:52:00.111236Z","shell.execute_reply":"2021-07-31T17:52:00.11626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining the model with same specifications as manual ","metadata":{}},{"cell_type":"code","source":"def ANN_keras(x):\n    \n    input_ = tfl.Input(shape=(x.shape[1],))\n    \n    x = tfl.Dense(3,activation='relu', name = 'Dense_3')(input_) #Layer 1\n    \n    preds = tfl.Dense(1, activation=\"sigmoid\", name=\"pred\")(x) #Output layer\n    \n    model = Model(input_, preds, name=\"ANN_keras\")\n    model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.SGD(learning_rate=alpha)) #Stochastic Gradient Descent with specified alpha\n    \n    return model\n    \nmodel = ANN_keras(X_arr)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:52:04.390507Z","iopub.execute_input":"2021-07-31T17:52:04.390955Z","iopub.status.idle":"2021-07-31T17:52:04.43777Z","shell.execute_reply.started":"2021-07-31T17:52:04.390917Z","shell.execute_reply":"2021-07-31T17:52:04.436514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the model","metadata":{}},{"cell_type":"code","source":"history = model.fit(X_arr,y_arr, epochs=n_iters, batch_size=batch_size,\n                    validation_data = (X_test_arr,y_test_arr),verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T18:01:01.19327Z","iopub.execute_input":"2021-07-31T18:01:01.193668Z","iopub.status.idle":"2021-07-31T18:02:12.269951Z","shell.execute_reply.started":"2021-07-31T18:01:01.193634Z","shell.execute_reply":"2021-07-31T18:02:12.268653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predicting through keras model","metadata":{}},{"cell_type":"code","source":"keras_pred = model.predict(X_test_arr)\nkeras_pred = np.where(keras_pred>0.5,1,0)\n\n#print(np.unique(keras_pred))\nprint('#--------------------- Evaluation ----------------------#')\n#Evaluation of the predictions\nprint('ROC AUC of test set :',roc_auc_score(y_test_arr.ravel(),keras_pred.ravel()))\nprint('Accuracy of test set :',accuracy_score(y_test_arr.ravel(),keras_pred.ravel()))","metadata":{"execution":{"iopub.status.busy":"2021-07-31T18:02:19.949345Z","iopub.execute_input":"2021-07-31T18:02:19.949729Z","iopub.status.idle":"2021-07-31T18:02:20.015733Z","shell.execute_reply.started":"2021-07-31T18:02:19.949694Z","shell.execute_reply":"2021-07-31T18:02:20.014343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Insights : The manual implementation of ANN is giving very similar predictions as that to the Keras counterparts, indicating the implementation is correct and comparable","metadata":{}},{"cell_type":"markdown","source":"# END","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}