{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as pt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\npd.set_option('display.max_columns', None)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-01T06:51:23.213623Z","iopub.execute_input":"2021-08-01T06:51:23.213967Z","iopub.status.idle":"2021-08-01T06:51:23.224479Z","shell.execute_reply.started":"2021-08-01T06:51:23.21394Z","shell.execute_reply":"2021-08-01T06:51:23.223409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing data","metadata":{}},{"cell_type":"code","source":"input_ads = pd.read_csv('../input/titanic/train_data.csv')\ninput_ads.drop(columns=['Unnamed: 0','Title_1','Title_2','Title_3','Title_4'],inplace=True) #Dropping un-necessary columns\n#-----------------------------------------------------------------\nprint(input_ads.shape)\ninput_ads.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T06:51:23.226291Z","iopub.execute_input":"2021-08-01T06:51:23.226866Z","iopub.status.idle":"2021-08-01T06:51:23.25793Z","shell.execute_reply.started":"2021-08-01T06:51:23.226824Z","shell.execute_reply":"2021-08-01T06:51:23.257061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Null Check","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(input_ads.isnull().sum()).T","metadata":{"execution":{"iopub.status.busy":"2021-08-01T06:51:23.259883Z","iopub.execute_input":"2021-08-01T06:51:23.260366Z","iopub.status.idle":"2021-08-01T06:51:23.276561Z","shell.execute_reply.started":"2021-08-01T06:51:23.260324Z","shell.execute_reply":"2021-08-01T06:51:23.275594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Description of the data","metadata":{}},{"cell_type":"code","source":"input_ads.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T06:51:23.27801Z","iopub.execute_input":"2021-08-01T06:51:23.278479Z","iopub.status.idle":"2021-08-01T06:51:23.329Z","shell.execute_reply.started":"2021-08-01T06:51:23.278437Z","shell.execute_reply":"2021-08-01T06:51:23.328322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Description of target variable","metadata":{}},{"cell_type":"code","source":"#Total survived vs not-survived split in the training data\ninput_ads['Survived'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T06:51:23.330007Z","iopub.execute_input":"2021-08-01T06:51:23.330446Z","iopub.status.idle":"2021-08-01T06:51:23.336945Z","shell.execute_reply.started":"2021-08-01T06:51:23.330408Z","shell.execute_reply":"2021-08-01T06:51:23.336304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Manipulation of data into train-test","metadata":{}},{"cell_type":"code","source":"target = 'Survived' #To predict\n\n#--------------------------------------------------------------------------------\n#Splitting into X & Y datasets (supervised training)\nX = input_ads[[cols for cols in list(input_ads.columns) if target not in cols]]\ny = input_ads[target]\n\n#--------------------------------------------------------------------------------\n#Since test data is already placed in the input folder separately, we will just import it\ntest_ads = pd.read_csv('../input/titanic/test_data.csv')\ntest_ads.drop(columns=['Unnamed: 0','Title_1','Title_2','Title_3','Title_4'],inplace=True) #Dropping un-necessary columns\n\n#Splitting into X & Y datasets (supervised training)\nX_test = test_ads[[cols for cols in list(test_ads.columns) if target not in cols]]\ny_test = test_ads[target]\n\nprint('Train % of total data:',100 * X.shape[0]/(X.shape[0] + X_test.shape[0]))\n#--------------------------------------------------------------------------------\n#Manipulation of datasets for convenience and consistency\nX_arr = np.array(X)\nX_test_arr = np.array(X_test)\n\ny_arr = np.array(y).reshape(X_arr.shape[0],1)\ny_test_arr = np.array(y_test).reshape(X_test_arr.shape[0],1)\n\n#--------------------------------------------------------------------------------\n#Basic Summary\nprint(X_arr.shape)\nprint(X_test_arr.shape)\nprint(y_arr.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T06:51:23.338045Z","iopub.execute_input":"2021-08-01T06:51:23.338491Z","iopub.status.idle":"2021-08-01T06:51:23.358006Z","shell.execute_reply.started":"2021-08-01T06:51:23.338456Z","shell.execute_reply":"2021-08-01T06:51:23.356868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Standard scaling the x-data","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n#----------------------------------------------------------\nscaler = StandardScaler()\nX_arr = scaler.fit_transform(X_arr)\nX_test_arr = scaler.transform(X_test_arr)\n\n#----------------------------------------------------------\nX_arr[0:3]","metadata":{"execution":{"iopub.status.busy":"2021-08-01T06:51:23.360084Z","iopub.execute_input":"2021-08-01T06:51:23.360411Z","iopub.status.idle":"2021-08-01T06:51:23.369688Z","shell.execute_reply.started":"2021-08-01T06:51:23.360382Z","shell.execute_reply":"2021-08-01T06:51:23.368912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Artificial Neural Network (ANN) from Scratch","metadata":{}},{"cell_type":"markdown","source":"## UDFs for activation, initialization, layer_propagation","metadata":{}},{"cell_type":"code","source":"#All popular activation functions\ndef activation_fn(z,type_):\n    \n    #print('Activation : ',type_)\n    \n    if type_=='linear':\n        activated_arr = z\n    \n    elif type_=='sigmoid':\n        activated_arr = 1/(1+np.exp(-z))\n    \n    elif type_=='relu': \n        activated_arr = np.maximum(np.zeros(z.shape),z)\n    \n    elif type_=='tanh':\n        activated_arr = (np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n    \n    elif type_=='leaky_relu':\n        activated_arr = np.maximum(0.01*z,z)\n    \n    elif type_=='softmax':\n        exp_ = np.exp(z)\n        exp_sum = np.sum(exp_)\n        activated_arr = exp_/exp_sum\n        \n    return activated_arr\n\n#----------------------------------------------------------------------------------------------------------------------------\n#Initialization of params\ndef generate_param_grid(a_prev,n_hidden,hidden_size_list):\n    \n    parameters = {}\n    features = a_prev.shape[0] #Total features\n    n_examples = a_prev.shape[1]\n    \n    for n_hidden_idx in range(1,n_hidden+1):\n        \n        n_hidden_nodes = hidden_size_list[n_hidden_idx] #Should start from 0\n        \n        #print('#------------ Layer :',n_hidden_idx,'---- Size :',n_hidden_nodes,'---- Prev features :',features,'------#')\n\n        parameters['w' + str(n_hidden_idx)] = np.random.rand(n_hidden_nodes,features) * 0.1 #Xavier Initialization\n        parameters['b' + str(n_hidden_idx)] = np.zeros((n_hidden_nodes,1)) * 0.1\n        \n        features = n_hidden_nodes\n    \n    return parameters#Return randomly initiated params\n    \n#---------------------------------------------------------------------------------------------------------------------------\n#Propagation between z and activation\ndef layer_propagation(a_prev,w,b,activation):\n    \n    #print(a_prev.shape)\n    #print(w.shape)\n    #print(b.shape)\n    \n    z_ = np.dot(w,a_prev) + b\n    \n    a = activation_fn(z=z_,\n                      type_=activation)\n    \n    return z_,a","metadata":{"execution":{"iopub.status.busy":"2021-08-01T06:51:23.371182Z","iopub.execute_input":"2021-08-01T06:51:23.371838Z","iopub.status.idle":"2021-08-01T06:51:23.384556Z","shell.execute_reply.started":"2021-08-01T06:51:23.371795Z","shell.execute_reply":"2021-08-01T06:51:23.383608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UDF for forward propagation","metadata":{}},{"cell_type":"code","source":"def forward_propagation(params_dict,data_x,data_y,n_hidden,hidden_size_list,activation_list):\n    \n    cache = {'a0' : data_x.T}\n    a = data_x.T.copy()\n    \n    for layer_idx in range(1,n_hidden+1):\n        \n        #print('#---------- Layer :',layer_idx,'-- No of Nodes :',hidden_size_list[layer_idx])\n        #nodes = hidden_size_list[layer_idx]\n        activation_ = activation_list[layer_idx]\n        w_ = params_dict['w'+str(layer_idx)]\n        b_ = params_dict['b'+str(layer_idx)]\n        \n        z,a = layer_propagation(a_prev=a,\n                                 w=w_,\n                                 b=b_,\n                                 activation=activation_)\n        \n        cache['z'+str(layer_idx)] = z\n        cache['a'+str(layer_idx)] = a\n    \n    return cache,a","metadata":{"execution":{"iopub.status.busy":"2021-08-01T06:51:23.385889Z","iopub.execute_input":"2021-08-01T06:51:23.386296Z","iopub.status.idle":"2021-08-01T06:51:23.399252Z","shell.execute_reply.started":"2021-08-01T06:51:23.386256Z","shell.execute_reply":"2021-08-01T06:51:23.398258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UDF for cost calculation, gradient calculation & back-propagation ","metadata":{}},{"cell_type":"code","source":"#Calculation of the total cost incurred by the model\ndef cost_calculation(activation_list,y_true,y_pred,regularization,reg_lambda,params_dict,n_hidden):\n\n    #----------------------------------------------------------------\n    w_ = params_dict['w'+str(n_hidden)]\n    \n    #Applying regularizations\n    if regularization=='L1':\n        reg = np.sum(abs(w_))\n    elif regularization=='L2':\n        reg = 0.5 * np.sum(np.square(w_))\n    elif regularization==None:\n        reg = 0\n    \n    #----------------------------------------------------------------\n    if activation_list[-1]=='sigmoid':\n        #print('sig')\n        m = y_true.shape[1]\n        cost = (-1/m) * (np.sum((y_true * np.log(y_pred)) + ((1-y_true) * np.log(1 - y_pred))) + (reg_lambda*reg))\n        \n    elif activation_list[-1]=='linear':\n        \n        m = y_true.shape[1]\n        cost = (1/m) * np.sum(np.square(y_true-y_pred)) + (reg_lambda*reg)\n        \n     ##-------------------->> Softmax to be added <<----------------------\n    \n    return cost\n\n#Gradient of the activation functions wrt corresponding z\n#--------------------------------------------------------------------------------------------\n#Gradient for each activation type\ndef grad_fn_dz(activation,a):\n    \n    if activation=='linear':\n        grad = 1\n     \n    elif activation=='sigmoid':\n        grad = a*(1-a)\n        \n    elif activation=='tanh':\n        grad = np.square(1-a)\n        \n    elif activation=='relu':\n        grad = np.where(a>=0,1,0)\n    \n    elif activation=='leaky_relu':\n        grad = np.where(a>=0,1,0.01)\n    \n    ##-------------------->> Softmax to be added <<----------------------\n    \n    return grad\n        \n#--------------------------------------------------------------------------------------------\n#UDF for gradient of loss function wrt last layer\ndef dL_last_layer(activation_list,y_true,y_pred,regularization,reg_lambda,params_dict,n_hidden):\n    \n#     #---------------------------------------------------------------------------------------\n#     w_ = params_dict['w'+str(n_hidden)]\n    \n#     #Applying regularization\n#     if regularization=='L1':\n#         reg_derivative = np.divide(w_, abs(w_), out=np.zeros_like(w_), where=abs(w_)!=0)\n#         reg_derivative = np.where(reg_derivative==np.inf,0,reg_derivative)\n#         print('Regularization L1 :',reg_derivative.shape)\n                \n#     elif regularization=='L2':            \n#         reg_derivative = w_\n#         print('Regularization L2 :',reg_derivative.shape)\n    \n#     elif regularization==None:\n#         reg_derivative = 0\n    \n    #----------------------------------------------------------------------------------------\n    if activation_list[-1]=='sigmoid':\n        \n        #print('Last Layer y true shape :',y_true.shape)\n        #print('Last Layer y pred shape :',y_pred.shape)\n        \n        #grad_final_layer = -((y_true/y_pred) - ((1-y_true)/(1-y_pred))) + (reg_lambda * reg_derivative)\n        grad_final_layer = -((y_true/y_pred) - ((1-y_true)/(1-y_pred)))\n        #print('Last Layer gradient shape :',grad_final_layer.shape)\n        \n    elif activation_list[-1]=='linear':\n        \n        #grad_final_layer = - 2 * (y_true-y_pred) + (reg_lambda * reg_derivative) #Check the sign\n        grad_final_layer = - 2 * (y_true-y_pred)\n        \n    return grad_final_layer\n\n#--------------------------------------------------------------------------------------------\n#Back=Propagation         \ndef back_propagation(cache,params_dict,data_x,data_y,n_hidden,hidden_size_list,activation_list,y_pred,regularization,reg_lambda):\n    \n    grads_cache = {}\n    #db_cache = {}\n    \n    da = dL_last_layer(activation_list=activation_list,\n                             y_true=data_y.T,\n                             y_pred=y_pred,\n                             regularization=regularization,\n                             reg_lambda=reg_lambda,\n                             params_dict=params_dict,\n                             n_hidden=n_hidden)\n    #print('Final da shape :',da.shape)\n    \n    m = data_y.shape[0] #Data in the batches\n    \n    #print('dm in backprop :',m)\n    for layer_idx in list(reversed(range(1,n_hidden+1))):\n        \n        #print('# -------- Layer :',layer_idx,'-------- Size :',hidden_size_list[layer_idx],'--------#')\n        \n        activation_ = activation_list[layer_idx]\n        a = cache['a'+str(layer_idx)]\n        a_prev = cache['a'+str(layer_idx-1)]\n        w = params_dict['w'+str(layer_idx)]\n        \n        #-------------------------------------------------------------------------------------------\n        #Applying regularization\n        if regularization=='L1':\n            reg_derivative = np.divide(w, abs(w), out=np.zeros_like(w), where=abs(w)!=0)\n            reg_derivative = np.where(reg_derivative==np.inf,0,reg_derivative)\n            #print('Regularization L1 :',reg_derivative.shape)\n\n        elif regularization=='L2':            \n            reg_derivative = w\n            #print('Regularization L2 :',reg_derivative.shape)\n\n        elif regularization==None:\n            reg_derivative = 0\n        #_-------------------------------------------------------------------------------------------\n        \n        \n#         print('Shape of a:',a.shape)\n#         print('Shape of a_prev:',a_prev.shape)\n#         print('SHape of w:',w.shape)\n        \n        #z = \n        \n        dz =  da * (grad_fn_dz(activation=activation_,a=a))\n        \n        #print('dz shape :',dz.shape)\n                     \n        dw = (1/m) * (np.dot(dz, a_prev.T) + (reg_lambda * reg_derivative))\n        #print('dw shape :',dw.shape)\n        grads_cache['dw'+str(layer_idx)] = dw\n                     \n        db = (1/m) * np.sum(dz, axis=1,keepdims=True)\n        #print('db shape :',db.shape)\n        grads_cache['db'+str(layer_idx)] = db\n        \n        da = np.dot(w.T,dz)\n        #print('da shape :',da.shape)\n\n    return grads_cache","metadata":{"execution":{"iopub.status.busy":"2021-08-01T08:01:33.104515Z","iopub.execute_input":"2021-08-01T08:01:33.104928Z","iopub.status.idle":"2021-08-01T08:01:33.129805Z","shell.execute_reply.started":"2021-08-01T08:01:33.104899Z","shell.execute_reply":"2021-08-01T08:01:33.128812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UDF for updating weights through gradient descent","metadata":{}},{"cell_type":"code","source":"def update_weights(params,grads_cache,alpha,n_hidden):\n    \n    for layer_idx in list(reversed(range(1,n_hidden+1))):\n        \n        #print('#---- layer :',layer_idx,'----#')\n        \n        dw = grads_cache['dw'+str(layer_idx)]\n        db = grads_cache['db'+str(layer_idx)]\n        \n#         print('dw shape :',dw.shape)\n#         print('db shape :',db.shape)\n#         print('w shape :',params['w'+str(layer_idx)].shape)\n        \n        params['w'+str(layer_idx)] -= alpha * dw\n        params['b'+str(layer_idx)] -= alpha * db\n\n    \n    return params","metadata":{"execution":{"iopub.status.busy":"2021-08-01T08:01:39.682687Z","iopub.execute_input":"2021-08-01T08:01:39.683056Z","iopub.status.idle":"2021-08-01T08:01:39.688931Z","shell.execute_reply.started":"2021-08-01T08:01:39.683025Z","shell.execute_reply":"2021-08-01T08:01:39.687915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UDF for predictions","metadata":{}},{"cell_type":"code","source":"def prediction(params,test_x,n_hidden,hidden_size_list,activation_list,threshold):\n    \n    #-----------------------------------------------------------------\n    #Forward Propagation on trained weights\n    cache,y_pred = forward_propagation(params_dict=params,\n                                  data_x=test_x,\n                                  data_y=None,\n                                  n_hidden=n_hidden,\n                                  hidden_size_list=hidden_size_list,\n                                  activation_list=activation_list)\n    #print(cache)\n    preds = np.where(y_pred>threshold,1,0).astype(float)\n    return cache,np.round(y_pred,4),preds","metadata":{"execution":{"iopub.status.busy":"2021-08-01T07:10:12.183566Z","iopub.execute_input":"2021-08-01T07:10:12.184037Z","iopub.status.idle":"2021-08-01T07:10:12.189245Z","shell.execute_reply.started":"2021-08-01T07:10:12.184009Z","shell.execute_reply":"2021-08-01T07:10:12.188493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stochastic Gradient Descent (SGD) for training of the ANN ","metadata":{}},{"cell_type":"code","source":"def ANN_train_sgd(data_x_overall,data_y_overall,batch_size,alpha,n_iters,n_hidden,hidden_size_list,activation_list,regularization,reg_lambda):\n    \n    print('Total training rows :',data_x_overall.shape[0])\n    \n    #----------------------------------------------------------------------------------------\n    #Creating x-y batches according to the provided batch_size\n    \n    n_batches = data_x_overall.shape[0]//batch_size\n    print('Total Batches to create in each epoch/iter :',n_batches)\n    \n    batches_x = np.array_split(data_x_overall,n_batches)\n    print('Total Batches of X:',len(batches_x))\n\n    batches_y = np.array_split(data_y_overall,n_batches)\n    print('Total Batches of y:',len(batches_y))\n    #-------------------------------------------------------------------------------------------\n    cost_history = [] #Record of cost through epochs\n\n    #-------------------------------------------------------------------------------------------\n    #Initialization of params\n    params_dict = generate_param_grid(a_prev=data_x_overall.T,\n                             n_hidden=n_hidden,\n                             hidden_size_list=hidden_size_list)\n    print('#----------------- Initial params ------------------#')\n    print(params_dict)\n    initial_params_abcd = params_dict.copy()\n    \n    #-------------------------------------------------------------------------------------------\n    cache_tray = []\n\n    for epoch in range(n_iters):\n\n        if (epoch>0) & (epoch%100==0):\n            print('#----------------------------------- Epoch :',epoch,'--------------------------------------#')\n            print('cost :',cost)\n            \n        for j in range(len(batches_x)): #For each batch created for each epoch/iter\n            \n            #-------------------------------------------------------------------------\n            #For each batch of data\n            data_x = batches_x[j]\n            data_y = batches_y[j]\n\n            #-------------------------------------------------------------------------\n            #Forward Propagation\n            cache,y_pred = forward_propagation(params_dict=params_dict,\n                                          data_x=data_x,\n                                          data_y=data_y,\n                                          n_hidden=n_hidden,\n                                          hidden_size_list=hidden_size_list,\n                                          activation_list=activation_list)\n            #print(np.max(y_pred))\n            #cache_tray.append(cache)\n            #-------------------------------------------------------------------------\n            #Cost calculation\n            cost = cost_calculation(activation_list=activation_list,\n                             y_true=data_y.T,\n                             y_pred=y_pred,\n                             regularization=regularization,\n                             reg_lambda=reg_lambda,\n                             params_dict=params_dict,\n                             n_hidden=n_hidden)\n\n            #cost_history.append(cost)\n            #print('cost :',cost)\n\n            #-------------------------------------------------------------------------\n            #Back Propagation\n            grads_cache_ = back_propagation(cache=cache,\n                                           params_dict=params_dict,\n                                           data_x=data_x,\n                                           data_y=data_y,\n                                           n_hidden=n_hidden,\n                                           hidden_size_list=hidden_size_list,\n                                           activation_list=activation_list,\n                                           y_pred=y_pred,\n                                           regularization=regularization,\n                                           reg_lambda=reg_lambda)\n\n            #------------------------------------------------------------------------\n            #Updating weights\n            params_dict = update_weights(params=params_dict,\n                                         grads_cache=grads_cache_,\n                                         alpha=alpha,\n                                         n_hidden=n_hidden)\n            \n        cost_history.append(cost) #Appending cost after each epoch\n\n\n    return initial_params_abcd,params_dict,grads_cache_,cost_history,y_pred,cache_tray\n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T07:21:52.30952Z","iopub.execute_input":"2021-08-01T07:21:52.309864Z","iopub.status.idle":"2021-08-01T07:21:52.322251Z","shell.execute_reply.started":"2021-08-01T07:21:52.309836Z","shell.execute_reply":"2021-08-01T07:21:52.321429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the model by invoking the above UDF","metadata":{}},{"cell_type":"code","source":"#Defining hyper-parameters for ANN\n#--------------------------------------------------------------------------------------------------------------------------\nn_hidden = 2       #No of hidden layers\nalpha = 0.003      #Learning_rate\nn_iters = 501      #Total epochs\nhidden_size_list = [0,3,1]               #first element will be 0 and not counted in hidden layers\nactivation_list = [0,'relu','sigmoid']   #first element will be 0 and not counted in hidden layers\nbatch_size = 25    #Batch wise gradient descent\nregularization = 'L1'\nreg_lambda = 0.09\n\n#--------------------------------------------------------------------------------------------------------------------------\ninitial_params_train,params_dict_train,grads,cost_history_train,y_pred_train,cache_tray = ANN_train_sgd(data_x_overall=X_arr,\n                                                                                                       data_y_overall=y_arr,\n                                                                                                       batch_size=batch_size,\n                                                                                                       alpha=alpha,\n                                                                                                       n_iters=n_iters,\n                                                                                                       n_hidden=n_hidden,\n                                                                                                       hidden_size_list=hidden_size_list,\n                                                                                                       activation_list=activation_list,\n                                                                                                       regularization=regularization,\n                                                                                                       reg_lambda=reg_lambda)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T08:01:56.828338Z","iopub.execute_input":"2021-08-01T08:01:56.828912Z","iopub.status.idle":"2021-08-01T08:02:00.467865Z","shell.execute_reply.started":"2021-08-01T08:01:56.82887Z","shell.execute_reply":"2021-08-01T08:02:00.466923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cost-Epoch plot for the manual ANN training ","metadata":{}},{"cell_type":"code","source":"#Cost plot over epochs (1 value at end of each epoch) - over the last batch\nax = sns.lineplot(x=list(range(n_iters)),y=cost_history_train)\nax.set(xlabel='epochs',ylabel='cost',title='Cost vs epoch plot for Manual ANN')","metadata":{"execution":{"iopub.status.busy":"2021-08-01T08:02:04.042372Z","iopub.execute_input":"2021-08-01T08:02:04.042711Z","iopub.status.idle":"2021-08-01T08:02:04.261014Z","shell.execute_reply.started":"2021-08-01T08:02:04.042683Z","shell.execute_reply":"2021-08-01T08:02:04.260152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict on the test data","metadata":{}},{"cell_type":"code","source":"cache,preds_proba,manual_preds = prediction(params=params_dict_train,\n                                            test_x=X_test_arr,\n                                            n_hidden=n_hidden,\n                                            hidden_size_list=hidden_size_list,\n                                            activation_list=activation_list,\n                                            threshold=0.5)\n\n#-------------------------------------------------------------------------------------------\nprint('Shape of prediction array :',preds_proba.shape)\nprint('Unique predictions :',np.unique(manual_preds))\nprint('Unique of predict proba :',np.unique(preds_proba),'\\n')\n\nprint('#--------------------- Evaluation ----------------------#')\n#Evaluation of the predictions\nprint('ROC AUC of test set :',roc_auc_score(y_test_arr.ravel(),manual_preds.ravel()))\nprint('Accuracy of test set :',accuracy_score(y_test_arr.ravel(),manual_preds.ravel()))","metadata":{"execution":{"iopub.status.busy":"2021-08-01T08:02:07.412294Z","iopub.execute_input":"2021-08-01T08:02:07.412628Z","iopub.status.idle":"2021-08-01T08:02:07.424779Z","shell.execute_reply.started":"2021-08-01T08:02:07.412601Z","shell.execute_reply":"2021-08-01T08:02:07.423953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Benchmarking with Keras functional API","metadata":{}},{"cell_type":"markdown","source":"## Importing necessary libraries","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport keras\nimport tensorflow.keras.models\nimport tensorflow.keras.layers as tfl\nfrom tensorflow.keras import Input\nfrom tensorflow.keras import Model\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.layers import BatchNormalization","metadata":{"execution":{"iopub.status.busy":"2021-08-01T07:45:16.667627Z","iopub.execute_input":"2021-08-01T07:45:16.668019Z","iopub.status.idle":"2021-08-01T07:45:16.673402Z","shell.execute_reply.started":"2021-08-01T07:45:16.667986Z","shell.execute_reply":"2021-08-01T07:45:16.67236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining the model with same specifications as manual ","metadata":{}},{"cell_type":"code","source":"#Defining regularization application to the keras functional API\nif regularization=='L1':\n    print('Regularization applied :',regularization,'-:',reg_lambda)\n    reg_keras = tf.keras.regularizers.l1(reg_lambda)\nelif regularization=='L2':\n    print('Regularization applied :',regularization)\n    reg_keras = tf.keras.regularizers.l2(reg_lambda)\nelif regularization==None:\n    print('Regularization applied :',regularization)\n    reg_keras = None\n\ndef ANN_keras(x):\n    \n    input_ = tfl.Input(shape=(x.shape[1],))\n    \n    x = tfl.Dense(3,activation='relu', kernel_regularizer=reg_keras,name = 'Dense_3')(input_) #Layer 1\n    \n    preds = tfl.Dense(1, activation=\"sigmoid\", name=\"pred\")(x) #Output layer\n    \n    model = Model(input_, preds, name=\"ANN_keras\")\n    model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.SGD(learning_rate=alpha)) #Stochastic Gradient Descent with specified alpha\n    \n    return model\n    \nmodel = ANN_keras(X_arr)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T07:45:44.449559Z","iopub.execute_input":"2021-08-01T07:45:44.449881Z","iopub.status.idle":"2021-08-01T07:45:44.487849Z","shell.execute_reply.started":"2021-08-01T07:45:44.449853Z","shell.execute_reply":"2021-08-01T07:45:44.48693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the model","metadata":{}},{"cell_type":"code","source":"history = model.fit(X_arr,y_arr, epochs=n_iters, batch_size=batch_size,\n                    validation_data = (X_test_arr,y_test_arr),verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T07:45:47.967652Z","iopub.execute_input":"2021-08-01T07:45:47.968023Z","iopub.status.idle":"2021-08-01T07:46:28.642825Z","shell.execute_reply.started":"2021-08-01T07:45:47.967991Z","shell.execute_reply":"2021-08-01T07:46:28.641944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predicting through keras model","metadata":{}},{"cell_type":"code","source":"keras_pred = model.predict(X_test_arr)\nkeras_pred = np.where(keras_pred>0.5,1,0)\n\n#print(np.unique(keras_pred))\nprint('#--------------------- Evaluation ----------------------#')\n#Evaluation of the predictions\nprint('ROC AUC of test set :',roc_auc_score(y_test_arr.ravel(),keras_pred.ravel()))\nprint('Accuracy of test set :',accuracy_score(y_test_arr.ravel(),keras_pred.ravel()))","metadata":{"execution":{"iopub.status.busy":"2021-08-01T07:54:17.683841Z","iopub.execute_input":"2021-08-01T07:54:17.684208Z","iopub.status.idle":"2021-08-01T07:54:17.76519Z","shell.execute_reply.started":"2021-08-01T07:54:17.684176Z","shell.execute_reply":"2021-08-01T07:54:17.76421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Insights : The manual implementation of ANN is giving very similar predictions (better by ~7% !!) as that to the Keras counterparts, indicating the implementation is correct and comparable","metadata":{}},{"cell_type":"markdown","source":"# END","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}