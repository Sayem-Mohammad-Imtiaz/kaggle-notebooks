{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as pt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\npd.set_option('display.max_columns', None)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-26T04:13:58.854544Z","iopub.execute_input":"2021-07-26T04:13:58.854956Z","iopub.status.idle":"2021-07-26T04:13:58.866579Z","shell.execute_reply.started":"2021-07-26T04:13:58.85492Z","shell.execute_reply":"2021-07-26T04:13:58.865473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Input of training data from input folder \nLinks - https://www.kaggle.com/azeembootwala/titanic","metadata":{}},{"cell_type":"code","source":"input_ads = pd.read_csv('../input/titanic/train_data.csv')\ninput_ads.drop(columns=['Unnamed: 0','Title_1','Title_2','Title_3','Title_4'],inplace=True) #Dropping un-necessary columns\n#-----------------------------------------------------------------\nprint(input_ads.shape)\ninput_ads.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:13:58.871348Z","iopub.execute_input":"2021-07-26T04:13:58.871918Z","iopub.status.idle":"2021-07-26T04:13:58.899338Z","shell.execute_reply.started":"2021-07-26T04:13:58.871883Z","shell.execute_reply":"2021-07-26T04:13:58.898642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Null Check","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(input_ads.isnull().sum()).T","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:13:58.900535Z","iopub.execute_input":"2021-07-26T04:13:58.900925Z","iopub.status.idle":"2021-07-26T04:13:58.915827Z","shell.execute_reply.started":"2021-07-26T04:13:58.900895Z","shell.execute_reply":"2021-07-26T04:13:58.914946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Describe of the whole data","metadata":{}},{"cell_type":"code","source":"input_ads.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:13:58.917272Z","iopub.execute_input":"2021-07-26T04:13:58.917564Z","iopub.status.idle":"2021-07-26T04:13:58.974898Z","shell.execute_reply.started":"2021-07-26T04:13:58.917537Z","shell.execute_reply":"2021-07-26T04:13:58.974173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Note - The data is already standardized since it is imported from a pre-processed public dataset","metadata":{}},{"cell_type":"code","source":"#Total survived vs not-survived split in the training data\ninput_ads['Survived'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:14:03.934314Z","iopub.execute_input":"2021-07-26T04:14:03.934891Z","iopub.status.idle":"2021-07-26T04:14:03.944496Z","shell.execute_reply.started":"2021-07-26T04:14:03.93484Z","shell.execute_reply":"2021-07-26T04:14:03.943401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Splitting & Pre-Processing ","metadata":{}},{"cell_type":"code","source":"target = 'Survived' #To predict\n\n#--------------------------------------------------------------------------------\n#Splitting into X & Y datasets (supervised training)\nX = input_ads[[cols for cols in list(input_ads.columns) if target not in cols]]\ny = input_ads[target]\n\n#--------------------------------------------------------------------------------\n#Since test data is already placed in the input folder separately, we will just import it\ntest_ads = pd.read_csv('../input/titanic/test_data.csv')\ntest_ads.drop(columns=['Unnamed: 0','Title_1','Title_2','Title_3','Title_4'],inplace=True) #Dropping un-necessary columns\n\n#Splitting into X & Y datasets (supervised training)\nX_test = test_ads[[cols for cols in list(test_ads.columns) if target not in cols]]\ny_test = test_ads[target]\n\nprint('Train % of total data:',100 * X.shape[0]/(X.shape[0] + X_test.shape[0]))\n#--------------------------------------------------------------------------------\n#Manipulation of datasets for convenience and consistency\nX_arr = np.array(X)\nX_test_arr = np.array(X_test)\n\ny_arr = np.array(y).reshape(X_arr.shape[0],1)\ny_test_arr = np.array(y_test).reshape(X_test_arr.shape[0],1)\n\n#--------------------------------------------------------------------------------\n#Basic Summary\nprint(X_arr.shape)\nprint(X_test_arr.shape)\nprint(y_arr.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:14:12.100006Z","iopub.execute_input":"2021-07-26T04:14:12.100383Z","iopub.status.idle":"2021-07-26T04:14:12.126777Z","shell.execute_reply.started":"2021-07-26T04:14:12.100347Z","shell.execute_reply":"2021-07-26T04:14:12.125773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression from scratch","metadata":{}},{"cell_type":"markdown","source":"### Defining fwd prop UDF, Cost function UDF & initiating weights and intercepts","metadata":{}},{"cell_type":"code","source":"#Sigmoid function for the forward propagation as well as backward propagation\ndef sigmoid(arr):\n    \n    sig = 1/(1 + np.exp(-arr))\n    \n    return sig\n\n#Fn for forward propagation of the model (to caculate the predictions)\n#--------------------------------------------------------------------------\ndef fwd_prop(X_arr,w,b):\n    \n    a = np.dot(X_arr,w) + b\n    sig_a = sigmoid(a)\n    #print('Shape of a:',a.shape)\n    \n    return sig_a\n\n#Fn to calculate cost for logistic regression\n#--------------------------------------------------------------------------------------------------\ndef cost_fn(y_true,y_pred,n_examples,reg_alpha,reg_type,w_):\n    \n    #Applying regularizations\n    if reg_type=='L1':\n        reg = np.sum(abs(w_))\n    elif reg_type=='L2':\n        reg = 0.5 * np.sum(np.square(w_))\n    \n    cost = (-1/n_examples) * np.sum((y_true * np.log(y_pred)) + ((1-y_true) * np.log(1 - y_pred))) + (reg_alpha*reg)\n    \n    return cost    \n\n#Fn to convert probabilities into class 0 or 1 based on threshold\n#--------------------------------------------------------------------------\ndef prob_to_class(arr,threshold):\n    \n    mask = arr>threshold\n    #print(mask)\n    arr_class = mask.astype(int)\n    \n    return arr_class\n\n\n#Initiating the weight and intercept vectors with appropriate dimensions\n#--------------------------------------------------------------------------\nnp.random.seed(100) #Setting seed for consistency in case of random number generation \n\n#Weights\n#----------------------------------\nw = np.zeros((X.shape[1],1))\nprint(X_arr.shape[1])\nprint(w.shape)\nprint(w)\n\n#Intercept\n#----------------------------------\nb = np.zeros(1)\nb\n\n#Testing the forward propagation function\n#----------------------------------\na = fwd_prop(X_arr,w,b)\na.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:14:45.080232Z","iopub.execute_input":"2021-07-26T04:14:45.080634Z","iopub.status.idle":"2021-07-26T04:14:45.098722Z","shell.execute_reply.started":"2021-07-26T04:14:45.080603Z","shell.execute_reply":"2021-07-26T04:14:45.097371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### UDF for batch_gradient_descent\n#### 1. If batch_size=1, it becomes stochastic gradient descent","metadata":{}},{"cell_type":"code","source":"def batch_gradient_descent(y_arr_overall,X_arr_overall,w_,b_,n_iters=10,lr=0.01,batch_size=1,reg_alpha=1,reg_type='L2'):\n    \n    print('Total training rows :',X_arr_overall.shape[0])\n    \n    #----------------------------------------------------------------------------------------\n    #Creating x-y batches according to the provided batch_size\n    \n    n_batches = X.shape[0]//batch_size\n    print('Total Batches to create in each epoch/iter :',n_batches)\n    \n    batches_x = np.array_split(X_arr_overall,n_batches)\n    print('Total Batches of X:',len(batches_x))\n\n    batches_y = np.array_split(y_arr,n_batches)\n    print('Total Batches of y:',len(batches_y))\n    \n    cost_history = [] #Cache for cost function o/p at necessary intervals for plotting later\n\n    #----------------------------------------------------------------------------------------\n    for i in range(n_iters): #Total iterations/epochs to train on\n        \n        if i%1000==0:\n            print('#-------------------- Epoch number :',i,'--------------------#')\n        \n        for j in range(len(batches_x)): #For each batch created for each epoch/iter\n            \n            #print('Batch No :',j)\n            \n            X_arr_ = batches_x[j]\n            y_arr_ = batches_y[j]\n\n            n_examples = X_arr_.shape[0]\n            #print(n_examples)\n            #----------------------------------------------------------------------------------------\n            #Forward propagation of the model - calculation of the model prediction\n            a_temp = fwd_prop(X_arr_,w_,b_)\n\n            cost = cost_fn(y_arr_,a_temp,n_examples,reg_alpha,reg_type,w_)\n            \n            if cost == np.inf:\n                print('---- Inf encountered due to exploding gradients ----')\n                return w_,b_,cost_history\n\n            #----------------------------------------------------------------------------------------\n            \n            if reg_type=='L1':\n                \n                reg_derivative = np.divide(w_, abs(w_), out=np.zeros_like(w_), where=abs(w_)!=0)\n                reg_derivative = np.where(reg_derivative==np.inf,0,reg_derivative)\n                \n            elif reg_type=='L2':\n                \n                reg_derivative = w_\n            \n            #Calculating the gradients for the current batch\n            dz = (1/n_examples) * ((a_temp-y_arr_))  #Derivative of Loss fn 'L'(binary crossentropy) wrt z = wx+b\n            \n            dw = np.dot(X_arr_.T,dz) + ((1/n_examples)*(reg_alpha * reg_derivative)) #Derivative of w (weights) wrt 'L' (Applying chain rule of differentiation)\n            db = (1/n_examples) * np.sum(dz) #Derivative of b (intercept) wrt 'L' (Applying chain rule of differentiation)\n            \n            #Updating the weight and the intercept\n            w_ = w_ - (lr * dw)\n            b_ = b_ - (lr * db)\n        \n        #Updating cost into the cache\n        cost_history = cost_history + [cost]\n        \n        #-------------------------------------------------\n        #Progress at regular intervals\n        if (i%5000==0):\n            print(i,': Cost ------->',cost)\n            \n            f_train_a = fwd_prop(X_arr_overall,w_,b_) #Results on whole training data after every 5k epochs\n            #print(f_train_a.shape)\n            \n            f_train_a = prob_to_class(arr=f_train_a,threshold=0.5)\n            print(f_train_a.shape)\n            #print(y_arr_overall.shape)\n            \n            print('ROC AUC of training set :',roc_auc_score(y_arr_overall,f_train_a))\n            print('Accuracy of training set :',accuracy_score(y_arr_overall,f_train_a))\n        \n    return w_,b_,cost_history","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:15:51.55187Z","iopub.execute_input":"2021-07-26T04:15:51.552396Z","iopub.status.idle":"2021-07-26T04:15:51.566805Z","shell.execute_reply.started":"2021-07-26T04:15:51.552361Z","shell.execute_reply":"2021-07-26T04:15:51.565948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training the logistic regression model","metadata":{}},{"cell_type":"code","source":"#Training the model on the training data with the below training specs\n#-----------------------------------------------------------------------------------------------------------------------\nepochs = 20001\nlearning_rate=0.00006\nbatch_size_=50\n\n#-----------------------------------------------------------------------------------------------------------------------\nw_final,b_final,cost_history = batch_gradient_descent(y_arr_overall=y_arr,       #Train y array \n                                                      X_arr_overall=X_arr,       #Train X array\n                                                      w_=w,                      #Passing zero initiated weight vector\n                                                      b_=b,                      #Passing zero initiaed intercept vector\n                                                      n_iters=epochs,            #Total epochs/iters for Gradient Descent\n                                                      lr=learning_rate,          #Learning rate for Gradient Descent\n                                                      batch_size=batch_size_,    #Batch size for Gradient Descent (1 for SGD)\n                                                      reg_alpha=0.05,            #Regularization factor\n                                                      reg_type='L1')             #Regularization Type\n                                                      ","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:16:03.380316Z","iopub.execute_input":"2021-07-26T04:16:03.380789Z","iopub.status.idle":"2021-07-26T04:16:35.51211Z","shell.execute_reply.started":"2021-07-26T04:16:03.38076Z","shell.execute_reply":"2021-07-26T04:16:35.511163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting cost over epochs (Should have a sharp decrease)","metadata":{}},{"cell_type":"code","source":"#Cost plot over epochs (1 value at end of each epoch) - over the last batch\nsns.set_style('darkgrid')\nax = sns.lineplot(x=list(range(0,epochs)),y=cost_history)\nax.set(xlabel='No of epochs',ylabel='Cost',title='Cost vs Epochs - Logistic Regression')","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:18:09.231748Z","iopub.execute_input":"2021-07-26T04:18:09.232082Z","iopub.status.idle":"2021-07-26T04:18:10.540649Z","shell.execute_reply.started":"2021-07-26T04:18:09.232054Z","shell.execute_reply":"2021-07-26T04:18:10.539853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### UDF for predicting","metadata":{}},{"cell_type":"code","source":"def predict(w_,b_,test_x,test_y):\n    \n    print(\"Testing on :\",test_x.shape[0],'rows')\n    \n    a_temp = fwd_prop(test_x,w_,b_) #Using the weights(w_) and bias(b_) vectors derived from training \n    a_temp = prob_to_class(arr=a_temp,threshold=0.5)\n    \n    print('Shape of prediction :',a_temp.shape)\n    \n    print('ROC AUC of test set :',roc_auc_score(test_y,a_temp))\n    print('Accuracy of test set :',accuracy_score(test_y,a_temp))\n    \n    print(a_temp[0:3])\n    \n    return a_temp\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:19:05.533131Z","iopub.execute_input":"2021-07-26T04:19:05.533523Z","iopub.status.idle":"2021-07-26T04:19:05.539506Z","shell.execute_reply.started":"2021-07-26T04:19:05.533491Z","shell.execute_reply":"2021-07-26T04:19:05.538477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions from the manual created linear regression model","metadata":{}},{"cell_type":"code","source":"predictions_ = predict(w_final,b_final,X_test_arr,y_test_arr)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:19:10.001863Z","iopub.execute_input":"2021-07-26T04:19:10.002203Z","iopub.status.idle":"2021-07-26T04:19:10.012011Z","shell.execute_reply.started":"2021-07-26T04:19:10.002172Z","shell.execute_reply":"2021-07-26T04:19:10.011022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linear Regression from sklearn as benchmark","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n#---------------------------------------------------------------------------------------\nlog_reg = LogisticRegression(penalty='l2',random_state=100,solver='sag',max_iter=20001,tol=1e-4,C=20)\nlog_reg.fit(X_arr,y_arr)\n\nprediction_sklearn = log_reg.predict(X_test_arr)\n\n#---------------------------------------------------------------------------------------\nprint('ROC AUC of test set :',roc_auc_score(y_test_arr,prediction_sklearn))\nprint('Accuracy of test set :',accuracy_score(y_test_arr,prediction_sklearn))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:19:15.431452Z","iopub.execute_input":"2021-07-26T04:19:15.431844Z","iopub.status.idle":"2021-07-26T04:19:16.228697Z","shell.execute_reply.started":"2021-07-26T04:19:15.431798Z","shell.execute_reply":"2021-07-26T04:19:16.227749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SGD Classifier\n- (Logistic regression with normal stochastic gradient descent) - Directly comparable with manual implementation","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\n#---------------------------------------------------------------------------------------\nlog_reg_sgd = SGDClassifier(loss='log',penalty='l2',alpha=0.05,random_state=100,epsilon=learning_rate,max_iter=epochs,tol=1e-10)\nlog_reg_sgd.fit(X_arr,y_arr)\n\nprediction_sklearn_sgd = log_reg_sgd.predict(X_test_arr)\n\n#---------------------------------------------------------------------------------------\nprint('ROC AUC of test set :',roc_auc_score(y_test_arr,prediction_sklearn_sgd))\nprint('Accuracy of test set :',accuracy_score(y_test_arr,prediction_sklearn_sgd))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:19:27.442421Z","iopub.execute_input":"2021-07-26T04:19:27.44277Z","iopub.status.idle":"2021-07-26T04:19:27.463691Z","shell.execute_reply.started":"2021-07-26T04:19:27.442741Z","shell.execute_reply":"2021-07-26T04:19:27.462721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparison to SGD Classifier (Because, sklearn logistic regression has other optimization techniques, its not directly comparable) ","metadata":{}},{"cell_type":"markdown","source":"### Percent deviation in the weights (with respect to manual logistic regression weights)","metadata":{}},{"cell_type":"code","source":"100 * (w_final-log_reg_sgd.coef_.ravel().reshape(11,1))/w_final","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:24:39.45156Z","iopub.execute_input":"2021-07-26T04:24:39.451904Z","iopub.status.idle":"2021-07-26T04:24:39.458848Z","shell.execute_reply.started":"2021-07-26T04:24:39.451875Z","shell.execute_reply":"2021-07-26T04:24:39.457817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Insights : \n1. Though the weights are very different as we see above, the models are predicting similarly.\n2. This indicates that there are multiple solutions possible for the data in hand","metadata":{}},{"cell_type":"markdown","source":"# END","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}