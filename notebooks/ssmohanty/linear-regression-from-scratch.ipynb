{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as pt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-26T03:16:47.732267Z","iopub.execute_input":"2021-07-26T03:16:47.732829Z","iopub.status.idle":"2021-07-26T03:16:48.946517Z","shell.execute_reply.started":"2021-07-26T03:16:47.732655Z","shell.execute_reply":"2021-07-26T03:16:48.945253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing data (Boston Housing Data to predict house prices)\n1. Link - https://www.kaggle.com/schirmerchad/bostonhoustingmlnd","metadata":{}},{"cell_type":"code","source":"input_ads = pd.read_csv('../input/bostonhoustingmlnd/housing.csv')\n\n#-----------------------------------------------------------------\n#Summary\nprint(input_ads.shape)\ninput_ads.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:20:24.726116Z","iopub.execute_input":"2021-07-26T03:20:24.726494Z","iopub.status.idle":"2021-07-26T03:20:24.772137Z","shell.execute_reply.started":"2021-07-26T03:20:24.726463Z","shell.execute_reply":"2021-07-26T03:20:24.771295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Null Check","metadata":{}},{"cell_type":"code","source":"input_ads.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T21:04:08.383645Z","iopub.execute_input":"2021-07-23T21:04:08.384038Z","iopub.status.idle":"2021-07-23T21:04:08.396236Z","shell.execute_reply.started":"2021-07-23T21:04:08.383991Z","shell.execute_reply":"2021-07-23T21:04:08.394927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Description of the target variable","metadata":{}},{"cell_type":"code","source":"input_ads['MEDV'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T21:04:08.398105Z","iopub.execute_input":"2021-07-23T21:04:08.398459Z","iopub.status.idle":"2021-07-23T21:04:08.430772Z","shell.execute_reply.started":"2021-07-23T21:04:08.398424Z","shell.execute_reply":"2021-07-23T21:04:08.429648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Splitting & Pre-Processing ","metadata":{}},{"cell_type":"code","source":"#Splitting of the ADS into X and Y components\nX = input_ads[[cols for cols in list(input_ads.columns) if 'MEDV' not in cols]]\ny = input_ads['MEDV']\n\n#Train-test split creation\nX, X_test, y, y_test = train_test_split(X, y, test_size=0.30, random_state=100)\n\n#--------------------------------------------------------------------------------\n#Scaling the datasets\nscaler = StandardScaler()\n\nX_arr = scaler.fit_transform(X)\nX_test_arr = scaler.fit_transform(X_test)\n\ny_arr = np.array(y).reshape(X_arr.shape[0],1)\ny_test_arr = np.array(y_test).reshape(X_test_arr.shape[0],1)\n\n#--------------------------------------------------------------------------------\n#Summary\nprint('Training x rows :',X_arr.shape)\nprint('Testing x rows :',X_test_arr.shape)\nprint('Training y rows :',y_arr.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:22:31.867376Z","iopub.execute_input":"2021-07-26T03:22:31.867877Z","iopub.status.idle":"2021-07-26T03:22:31.891529Z","shell.execute_reply.started":"2021-07-26T03:22:31.867705Z","shell.execute_reply":"2021-07-26T03:22:31.890017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linear Regression from scratch","metadata":{}},{"cell_type":"markdown","source":"### Defining fwd prop UDF, Cost function UDF & initiating weights and intercepts","metadata":{}},{"cell_type":"code","source":"#For forward propagation in the model\ndef fwd_prop(X_arr,w,b):\n    \n    a = np.dot(X_arr,w) + b\n    #print('Shape of a:',a.shape)\n    \n    return a\n\n#Cost function as per regularization\ndef cost_fn(y_true,y_pred,n_examples,reg_alpha,reg_type,w_):\n    \n    if reg_type=='L1':\n        reg = np.sum(abs(w_))\n    elif reg_type=='L2':\n        reg = 0.5 * np.sum(np.square(w_))\n    \n    cost = (1/(2*n_examples)) * np.sum(np.square(y_pred-y_true)) + (reg_alpha*reg)\n    #print('Cost :',cost)\n    return cost    \n\n#Setting seed\nnp.random.seed(100)\n\n#Initializing weights(w) and bias(b) vectors \n#-------------------------------------------\nw = np.random.rand(X.shape[1],1)\nprint(w)\n#-------------------------------------------\nb = np.zeros(1)\nb","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:24:51.916983Z","iopub.execute_input":"2021-07-26T03:24:51.917346Z","iopub.status.idle":"2021-07-26T03:24:51.930494Z","shell.execute_reply.started":"2021-07-26T03:24:51.917316Z","shell.execute_reply":"2021-07-26T03:24:51.929383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UDF for batch_gradient_descent\n#### 1. If batch_size=1, it becomes stochastic gradient descent","metadata":{}},{"cell_type":"code","source":"def batch_gradient_descent(y_arr_overall,n_examples,X_arr_overall,w_,b_,n_iters=10,lr=0.01,batch_size=1,reg_alpha=1,reg_type='L1'):\n    \n    print('Total training rows :',X_arr_overall.shape[0])\n    \n    #----------------------------------------------------------------------------------------\n    #Creating x-y batches according to the provided batch_size\n    \n    n_batches = X.shape[0]//batch_size\n    print('Total Batches to create in each epoch/iter :',n_batches)\n    \n    batches_x = np.array_split(X_arr_overall,n_batches)\n    print('Total Batches of X:',len(batches_x))\n\n    batches_y = np.array_split(y_arr,n_batches)\n    print('Total Batches of y:',len(batches_y))\n    \n    cost_history = [] #Cache for cost function o/p at necessary intervals for plotting later\n\n    #----------------------------------------------------------------------------------------\n    for i in range(n_iters): #Total iterations/epochs to train on\n        \n        if i%1000==0:\n            print('#-------------------- Epoch number :',i,'--------------------#')\n        \n        for j in range(len(batches_x)): #For each batch created for each epoch/iter\n            \n            #print('Batch No :',j)\n            \n            X_arr_ = batches_x[j]\n            y_arr_ = batches_y[j]\n\n            #----------------------------------------------------------------------------------------\n            #Forward propagation of the model - calculation of the model prediction\n            a_temp = fwd_prop(X_arr_,w_,b_)\n\n            cost = cost_fn(y_arr_,a_temp,n_examples,reg_alpha,reg_type,w_)\n            \n            if cost == np.inf: #If any inf is encountered due to exploding gradients\n                print('---- Inf encountered due to exploding gradients ----')\n                return w_,b_,cost_history\n\n            #----------------------------------------------------------------------------------------\n            error = a_temp-y_arr_ #The residual calculation\n            \n            #Applying regularization\n            if reg_type=='L1':\n                \n                reg_derivative = np.divide(w_, abs(w_), out=np.zeros_like(w_), where=abs(w_)!=0)\n                reg_derivative = np.where(reg_derivative==np.inf,0,reg_derivative)\n                \n            elif reg_type=='L2':\n                \n                reg_derivative = w_         \n            \n            #Calculating the gradients for the current batch\n            dw = 1/n_examples * (np.dot(X_arr_.T,error) + (reg_alpha*reg_derivative)) #Customized for regularization\n            db = 1/n_examples * np.sum(error)\n            \n            #Updating the weight and the intercept\n            w_ = w_ - (lr * dw)\n            b_ = b_ - (lr * db)\n        \n        #Updating cost into the cache\n        cost_history = cost_history + [cost]\n        #-------------------------------------------------\n        #Progress at regular intervals\n        if (i%5000==0):\n            print(i,': Cost ------->',cost)\n            \n            f_train_a = fwd_prop(X_arr_overall,w_,b_) #Results on whole training data after every 5k epochs\n            print(f_train_a.shape)\n        \n            print('MSE of training set :',mean_squared_error(y_arr_overall,f_train_a))\n            print('RMSE of training set :',np.sqrt(mean_squared_error(y_arr_overall,f_train_a)))\n        \n    return w_,b_,cost_history\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:31:04.286807Z","iopub.execute_input":"2021-07-26T03:31:04.287233Z","iopub.status.idle":"2021-07-26T03:31:04.303602Z","shell.execute_reply.started":"2021-07-26T03:31:04.287197Z","shell.execute_reply":"2021-07-26T03:31:04.30218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training the linear regression model","metadata":{}},{"cell_type":"code","source":"w_final,b_final,cost_history = batch_gradient_descent(y_arr_overall=y_arr,\n                                                      n_examples=X_arr.shape[0],\n                                                      X_arr_overall=X_arr,\n                                                      w_=w,\n                                                      b_=b,\n                                                      n_iters=20001,\n                                                      lr=0.001,\n                                                      batch_size=20,\n                                                      reg_alpha=0.05,\n                                                      reg_type='L1')","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:25:40.045923Z","iopub.execute_input":"2021-07-26T03:25:40.046354Z","iopub.status.idle":"2021-07-26T03:26:06.579391Z","shell.execute_reply.started":"2021-07-26T03:25:40.046321Z","shell.execute_reply":"2021-07-26T03:26:06.578378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting cost over epochs (Should have a sharp decrease)","metadata":{}},{"cell_type":"code","source":"#Cost plot over epochs (1 value at end of each epoch) - over the last batch\nsns.set_style('darkgrid')\nax = sns.lineplot(x=list(range(0,20001)),y=cost_history)\nax.set(xlabel='No of epochs',ylabel='Cost',title='Cost vs Epochs-Linear/Lasso/Ridge Regression')","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:01:03.735602Z","iopub.execute_input":"2021-07-26T04:01:03.73605Z","iopub.status.idle":"2021-07-26T04:01:05.235973Z","shell.execute_reply.started":"2021-07-26T04:01:03.736007Z","shell.execute_reply":"2021-07-26T04:01:05.234977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### UDF for predicting","metadata":{}},{"cell_type":"code","source":"def predict(w_,b_,test_x,test_y):\n    \n    print(\"Testing on :\",test_x.shape[0],'rows')\n    \n    a_temp = fwd_prop(test_x,w_,b_) #Applying the trained weights(w_) and bias(b_)\n    print('Shape of prediction :',a_temp.shape)\n    \n    print('MSE of test set :',mean_squared_error(test_y,a_temp))\n    print('RMSE of test set :',np.sqrt(mean_squared_error(test_y,a_temp)))\n    \n    print(a_temp[0:3])\n    \n    return a_temp\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:33:52.312282Z","iopub.execute_input":"2021-07-26T03:33:52.31275Z","iopub.status.idle":"2021-07-26T03:33:52.320277Z","shell.execute_reply.started":"2021-07-26T03:33:52.312697Z","shell.execute_reply":"2021-07-26T03:33:52.318818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions from the manual created linear regression model","metadata":{}},{"cell_type":"code","source":"predictions_ = predict(w_final,b_final,X_test_arr,y_test_arr)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:33:59.077031Z","iopub.execute_input":"2021-07-26T03:33:59.077459Z","iopub.status.idle":"2021-07-26T03:33:59.086102Z","shell.execute_reply.started":"2021-07-26T03:33:59.077421Z","shell.execute_reply":"2021-07-26T03:33:59.08464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linear Regression from sklearn as benchmark","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\n#---------------------------------------------------------------------------------------\nlin_reg = LinearRegression()\nlin_reg.fit(X_arr,y_arr)\n\nprediction_sklearn = lin_reg.predict(X_test_arr)\n\n#---------------------------------------------------------------------------------------\nprint('MSE of test set :',mean_squared_error(y_test_arr,prediction_sklearn))\nprint('RMSE of test set :',np.sqrt(mean_squared_error(y_test_arr,prediction_sklearn)))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:34:06.61172Z","iopub.execute_input":"2021-07-26T03:34:06.612149Z","iopub.status.idle":"2021-07-26T03:34:06.880476Z","shell.execute_reply.started":"2021-07-26T03:34:06.612116Z","shell.execute_reply":"2021-07-26T03:34:06.879329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comparing the delta between manual predictions and sklearn predictions","metadata":{}},{"cell_type":"code","source":"delta = prediction_sklearn-predictions_\n\n#---------------------------------------------------------\nprint('25th Quantile of delta :',np.quantile(delta,0.25))\nprint('Median of delta :',np.quantile(delta,0.5))\nprint('75th Quantile of delta :',np.quantile(delta,0.75))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:34:13.671632Z","iopub.execute_input":"2021-07-26T03:34:13.672669Z","iopub.status.idle":"2021-07-26T03:34:13.690753Z","shell.execute_reply.started":"2021-07-26T03:34:13.672607Z","shell.execute_reply":"2021-07-26T03:34:13.689391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking the assumptions of linear regression","metadata":{}},{"cell_type":"markdown","source":"## Calculating the residuals","metadata":{}},{"cell_type":"code","source":"residuals = predictions_ - y_test_arr\nresiduals[0:3]","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:37:40.041866Z","iopub.execute_input":"2021-07-26T03:37:40.042302Z","iopub.status.idle":"2021-07-26T03:37:40.04976Z","shell.execute_reply.started":"2021-07-26T03:37:40.042269Z","shell.execute_reply":"2021-07-26T03:37:40.048535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Assumption 1 : Linear relation between dependent & independent variable","metadata":{}},{"cell_type":"code","source":"print('-- Pair-plot for all the features on the whole data--')\nax = sns.pairplot(input_ads,x_vars = ['RM','LSTAT','PTRATIO'],y_vars = ['MEDV'],size=7,aspect=0.7)\nax.set(title='Pair-plot for all the features on the whole data')","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:38:08.471379Z","iopub.execute_input":"2021-07-26T03:38:08.47176Z","iopub.status.idle":"2021-07-26T03:38:09.519881Z","shell.execute_reply.started":"2021-07-26T03:38:08.471714Z","shell.execute_reply":"2021-07-26T03:38:09.518849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insights : \n1. We observe that RM and LSTAT have linear behaviour to some degree against the taget variable which aligns with the assumption of lineariy for linear regression","metadata":{}},{"cell_type":"code","source":"#Residual vs fitted plot for test data\nsns.set_style(\"darkgrid\")\nax = sns.scatterplot(x=np.divide(predictions_,1000).ravel(),y=np.divide(residuals,1000).ravel(),marker='o')\nax.set(xlabel='Predicted-y in 1k units', ylabel='Residuals in 1k units',title='Residuls vs Fitted Plot for Test Data')","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:37:43.426258Z","iopub.execute_input":"2021-07-26T03:37:43.42669Z","iopub.status.idle":"2021-07-26T03:37:43.700683Z","shell.execute_reply.started":"2021-07-26T03:37:43.426656Z","shell.execute_reply":"2021-07-26T03:37:43.699504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insights - \n1. The above residual vs fitted plot is sowing very loose signs of a pattern which should be the case as the residuals shouldn't have any pattern among themselves as per the assumptions of linear regression  ","metadata":{}},{"cell_type":"markdown","source":"## Assumption 2 : Mean of residulals should be close to 0","metadata":{}},{"cell_type":"code","source":"print('Mean of residuals (Should be 0):',np.mean(residuals))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T21:04:36.950862Z","iopub.execute_input":"2021-07-23T21:04:36.951534Z","iopub.status.idle":"2021-07-23T21:04:36.957798Z","shell.execute_reply.started":"2021-07-23T21:04:36.951483Z","shell.execute_reply":"2021-07-23T21:04:36.956702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insights : \n1. Not abiding by the assumption","metadata":{}},{"cell_type":"markdown","source":"## Assumption 3 : Absence of Multi-Collinearity ","metadata":{}},{"cell_type":"code","source":"# Import library for VIF\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n#------------------------------------------------------------------------------------\ndef calc_vif(X):\n\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n    return(vif)\n\n#------------------------------------------------------------------------------------\nX_VIF = calc_vif(X)\nX_VIF = X_VIF.sort_values(['VIF'],ascending=False) #Sorting by descending order\n#X_VIF[X_VIF['VIF']>4] #Filtering for above 4 #Usually VIF above 4 or 5 is problematic\nX_VIF","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:44:42.511254Z","iopub.execute_input":"2021-07-26T03:44:42.511646Z","iopub.status.idle":"2021-07-26T03:44:42.664346Z","shell.execute_reply.started":"2021-07-26T03:44:42.511614Z","shell.execute_reply":"2021-07-26T03:44:42.663244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insights : \n1. From above, we can see PTRATIO and RM have very high multi-collinearity, We'll have to validate this with correlation analysis","metadata":{}},{"cell_type":"markdown","source":"### Validating the above findings of VIF through pearson's correlation","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"darkgrid\")\nsns.heatmap(input_ads.corr(method='spearman'),annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:46:44.471368Z","iopub.execute_input":"2021-07-26T03:46:44.471843Z","iopub.status.idle":"2021-07-26T03:46:44.870499Z","shell.execute_reply.started":"2021-07-26T03:46:44.4718Z","shell.execute_reply":"2021-07-26T03:46:44.869356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Assumption 4 : Homoskedascity check\n### Definition : There residuals of the model should not be in any pattern","metadata":{}},{"cell_type":"code","source":"#Residual vs fitted plot for test data\n\nsns.set_style(\"darkgrid\")\nax = sns.scatterplot(x=np.divide(predictions_,1000).ravel(),y=np.divide(residuals,1000).ravel(),marker='o')\nsns.lineplot([0,800],[0,0],color='red')\nax.set(xlabel='Predicted-y in 1k units', ylabel='Residuals in 1k units',title='Residuls vs Fitted Plot for Test Data')","metadata":{"execution":{"iopub.status.busy":"2021-07-23T21:04:37.49953Z","iopub.execute_input":"2021-07-23T21:04:37.499908Z","iopub.status.idle":"2021-07-23T21:04:37.798299Z","shell.execute_reply.started":"2021-07-23T21:04:37.499873Z","shell.execute_reply":"2021-07-23T21:04:37.797147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hypothesis Test for Homoskedascity","metadata":{}},{"cell_type":"code","source":"import statsmodels.stats.api as sms\nimport statsmodels.api as sm\nimport pylab as py\nfrom statsmodels.compat import lzip\n\n#-------------------------------------------------------\nname = ['F statistic', 'p-value']\ntest = sms.het_goldfeldquandt(residuals, X_test)\nlzip(name, test)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T21:04:37.799796Z","iopub.execute_input":"2021-07-23T21:04:37.800233Z","iopub.status.idle":"2021-07-23T21:04:38.657383Z","shell.execute_reply.started":"2021-07-23T21:04:37.800175Z","shell.execute_reply":"2021-07-23T21:04:38.65631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insights : \n1. From the plot, we can see theres no definite pattern suggesting no Heteroskedascity\n2. From the hypothesis test above, we can see that p-value (~0.976) > alpha (0.05). Hence, we cannot reject the null hypothesis that there is homoskedascity present","metadata":{}},{"cell_type":"markdown","source":"## Assumption 5 : Normality of residuals","metadata":{}},{"cell_type":"code","source":"sm.qqplot(residuals.ravel(), line ='45', loc=np.mean(residuals.ravel()), scale=np.std(residuals.ravel()))\npy.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T21:04:38.659061Z","iopub.execute_input":"2021-07-23T21:04:38.659519Z","iopub.status.idle":"2021-07-23T21:04:38.874926Z","shell.execute_reply.started":"2021-07-23T21:04:38.659468Z","shell.execute_reply":"2021-07-23T21:04:38.873785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insights : \n1. The residuals are fairly aligned with normal distribution with limited deviation at the initial quantiles","metadata":{}},{"cell_type":"markdown","source":"## Hypothesis testing to check for Gaussian spread of residuals","metadata":{}},{"cell_type":"code","source":"import scipy.stats as stats\n\n#-----------------------------------------------------------\nshap_stat,shap_p = stats.shapiro(residuals.ravel())\nprint('Stat :',shap_stat)\nprint('p-value from SHAPIRO_WILKS test :',shap_p)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T21:04:38.878403Z","iopub.execute_input":"2021-07-23T21:04:38.878767Z","iopub.status.idle":"2021-07-23T21:04:38.885611Z","shell.execute_reply.started":"2021-07-23T21:04:38.878728Z","shell.execute_reply":"2021-07-23T21:04:38.884448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(residuals.ravel(),color='g')","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:54:04.749506Z","iopub.execute_input":"2021-07-26T03:54:04.749936Z","iopub.status.idle":"2021-07-26T03:54:05.020801Z","shell.execute_reply.started":"2021-07-26T03:54:04.749898Z","shell.execute_reply":"2021-07-26T03:54:05.019719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insights : \n1. From the Q-Q plot, we could see that there was some minor deviation from normal distribution at the initial quantiles\n2. From Shapiro-Wilks test we see that the p-value (~0.003) < alpha (0.05). Hence, we can reject the null hypothesis that the residual is normally distributed\n3. From the kde-plot above, we can see that the distribution seems slightly left skewed (matching with initial quantiles of Q-Q plot)\n4. All in all, the residuals are not normally distributed and hence the assumption is violated","metadata":{}},{"cell_type":"markdown","source":"# END","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}