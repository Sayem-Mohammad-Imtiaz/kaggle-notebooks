{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-29T06:36:39.645184Z","iopub.execute_input":"2021-07-29T06:36:39.645602Z","iopub.status.idle":"2021-07-29T06:36:40.711059Z","shell.execute_reply.started":"2021-07-29T06:36:39.64551Z","shell.execute_reply":"2021-07-29T06:36:40.709492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing titanic data from the below link:\nhttps://www.kaggle.com/azeembootwala/titanic","metadata":{}},{"cell_type":"code","source":"input_ads_pre = pd.read_csv('../input/titanic/train_data.csv')\ninput_ads_pre.drop(columns=['Unnamed: 0','Title_1','Title_2','Title_3','Title_4'],inplace=True) #Dropping un-necessary columns\n#-----------------------------------------------------------------\nprint(input_ads_pre.shape)\ninput_ads_pre.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:36:40.712321Z","iopub.execute_input":"2021-07-29T06:36:40.712584Z","iopub.status.idle":"2021-07-29T06:36:40.763729Z","shell.execute_reply.started":"2021-07-29T06:36:40.712558Z","shell.execute_reply":"2021-07-29T06:36:40.762836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Null Check","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(input_ads_pre.isnull().sum()).T","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:36:40.766904Z","iopub.execute_input":"2021-07-29T06:36:40.767227Z","iopub.status.idle":"2021-07-29T06:36:40.782883Z","shell.execute_reply.started":"2021-07-29T06:36:40.767195Z","shell.execute_reply":"2021-07-29T06:36:40.782086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Describing the dataset","metadata":{}},{"cell_type":"code","source":"input_ads_pre.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:36:40.78445Z","iopub.execute_input":"2021-07-29T06:36:40.784932Z","iopub.status.idle":"2021-07-29T06:36:40.838916Z","shell.execute_reply.started":"2021-07-29T06:36:40.784899Z","shell.execute_reply":"2021-07-29T06:36:40.838188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Describing the target variable","metadata":{}},{"cell_type":"code","source":"#Total survived vs not-survived split in the training data\ninput_ads_pre['Survived'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:36:40.839889Z","iopub.execute_input":"2021-07-29T06:36:40.840302Z","iopub.status.idle":"2021-07-29T06:36:40.848154Z","shell.execute_reply.started":"2021-07-29T06:36:40.840273Z","shell.execute_reply":"2021-07-29T06:36:40.847354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Shuffling the data","metadata":{}},{"cell_type":"code","source":"from sklearn.utils import shuffle\n#np.random.seed(100)\n\n#----------------------------------------------------\ninput_ads = shuffle(input_ads_pre,random_state=100)\nprint(input_ads.shape)\ninput_ads = input_ads.reset_index(drop=True)\ninput_ads.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:36:40.849266Z","iopub.execute_input":"2021-07-29T06:36:40.849767Z","iopub.status.idle":"2021-07-29T06:36:40.873209Z","shell.execute_reply.started":"2021-07-29T06:36:40.849735Z","shell.execute_reply":"2021-07-29T06:36:40.872224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Manipulation of Train and Test","metadata":{}},{"cell_type":"code","source":"target = 'Survived' #To predict\n\n#--------------------------------------------------------------------------------\n#Splitting into X & Y datasets (supervised training)\nX = input_ads[[cols for cols in list(input_ads.columns) if target not in cols]]\ny = input_ads[target]\n\nprint(X.columns)\n\n#--------------------------------------------------------------------------------\n#Since test data is already placed in the input folder separately, we will just import it\ntest_ads_pre = pd.read_csv('../input/titanic/test_data.csv')\ntest_ads_pre.drop(columns=['Unnamed: 0','Title_1','Title_2','Title_3','Title_4'],inplace=True) #Dropping un-necessary columns\ntest_ads = shuffle(test_ads_pre,random_state=100)\ntest_ads = test_ads.reset_index(drop=True)\n\n#Splitting into X & Y datasets (supervised training)\nX_test = test_ads[[cols for cols in list(test_ads.columns) if target not in cols]]\ny_test = test_ads[target]\n\nprint('Train % of total data:',100 * X.shape[0]/(X.shape[0] + X_test.shape[0]))\n#--------------------------------------------------------------------------------\n#Manipulation of datasets for convenience and consistency\nX_arr = np.array(X)\nX_test_arr = np.array(X_test)\n\ny_arr = np.array(y).reshape(X_arr.shape[0],1)\ny_test_arr = np.array(y_test).reshape(X_test_arr.shape[0],1)\n\n#--------------------------------------------------------------------------------\n#Basic Summary\nprint(X_arr.shape)\nprint(X_test_arr.shape)\nprint(y_arr.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:36:40.874841Z","iopub.execute_input":"2021-07-29T06:36:40.875486Z","iopub.status.idle":"2021-07-29T06:36:40.902808Z","shell.execute_reply.started":"2021-07-29T06:36:40.875442Z","shell.execute_reply":"2021-07-29T06:36:40.901819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree Classifier from scratch","metadata":{}},{"cell_type":"markdown","source":"## UDF to calculate gini index of each node","metadata":{}},{"cell_type":"code","source":"def gini_node(arr_,k):\n\n    class_elem_total = 0\n    for class_ in k: #Iterating for each class in the node\n        class_elem = (np.sum((arr_==class_).astype(int)))/len(arr_)\n        class_elem = class_elem**2\n        class_elem_total = class_elem_total + class_elem \n        \n    gini_node = 1 - class_elem_total\n    return gini_node","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:36:40.905865Z","iopub.execute_input":"2021-07-29T06:36:40.906234Z","iopub.status.idle":"2021-07-29T06:36:40.911527Z","shell.execute_reply.started":"2021-07-29T06:36:40.906202Z","shell.execute_reply":"2021-07-29T06:36:40.910198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UDF for Gini index of a split (Weighted by the leafs)","metadata":{}},{"cell_type":"code","source":"def gini_split(left_arr,right_arr,k):\n\n    #Total obs in each node\n    m_left = len(left_arr)\n    m_right = len(right_arr)\n    m_total_node = m_left + m_right  \n\n    #Calculating gini index for each \n    gini_left = gini_node(left_arr,k)\n    #print(gini_left)\n    gini_right = gini_node(right_arr,k)\n    #print(gini_right)\n    \n    #Calculation of gini for the split\n    if m_left==0:\n        gini_split = ((m_right/m_total_node) * gini_right)\n        \n    elif m_right==0:\n        gini_split = ((m_left/m_total_node) * gini_left)\n    \n    elif (m_left>0) & (m_right>0):\n        gini_split = ((m_left/m_total_node) * gini_left) + ((m_right/m_total_node) * gini_right)\n\n    return gini_split","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:36:40.913596Z","iopub.execute_input":"2021-07-29T06:36:40.914226Z","iopub.status.idle":"2021-07-29T06:36:40.924213Z","shell.execute_reply.started":"2021-07-29T06:36:40.914183Z","shell.execute_reply":"2021-07-29T06:36:40.923378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UDF for inding best split for a feature (Greedy Exact Search)","metadata":{}},{"cell_type":"code","source":"def feature_split_algo(data,col_idx,min_samples_split=2,min_samples_leaf=2):\n    \n    assert len(data[:,col_idx])>=min_samples_split, \"Data Insufficient - Include more data or reduce min_samples_split hyper-param\"\n    \n    k = np.unique(data[:,-1]) #Unique of all classes going into to function\n    \n    #------------------------------------------------------------------------------------------------------\n    #To be used for thresholds\n    unique_vals = np.unique(data[:,col_idx])\n    \n    #print('Total unique vals in the column :',len(unique_vals))\n    \n    if len(unique_vals)>1:\n\n        gini_node_ = gini_node(data[:,-1],k)\n\n        splits_gini_dict = {}\n        thresholds_discarded = []\n\n        #------------------------------------------------------------------------------------------------------\n        for threshold in unique_vals: #For each threshold possible\n\n            #print('-------- Threshold : -------',threshold)\n\n            left_split = data[data[:,col_idx]<=threshold] #Left extension of tree\n            left_split_target = left_split[:,-1]\n            #print('Len left :',len(left_split_target))\n\n            right_split = data[data[:,col_idx]>threshold] #Right extension of tree\n            right_split_target = right_split[:,-1]\n            #print('Len right :',len(right_split_target))\n\n            #--------------------------------------------------------------------------------------------------\n            if (len(left_split_target)>=min_samples_leaf) & (len(right_split_target)>=min_samples_leaf): #Condition on mininum samples for a split to be eligible\n                \n                #Calculating the gini index of the split\n                gini_split_ = gini_split(left_arr=left_split_target,\n                                         right_arr=right_split_target,\n                                         k=k)\n                #print(gini_split_)\n                    \n                splits_gini_dict.update({threshold : gini_split_})\n\n            else:\n                #Discarding the threshold if condition not met\n                thresholds_discarded.append(threshold)\n\n        #print('Thresholds discarded :',thresholds_discarded)\n        #print(splits_gini_list)\n        #------------------------------------------------------------------------------------------------------\n        # Finding min value keys in dictionary\n\n        #assert len(splits_gini_dict)>0, \"No thresholds evaluated\"\n        \n        \n        #Condition to avoid empty dictionary (if no split is feasible)\n        if len(splits_gini_dict)>0:\n\n            min_gini = min(splits_gini_dict.values())\n            split_val = [key for key in splits_gini_dict if splits_gini_dict[key] == min_gini]\n            #print('Min Score :',min_gini)\n            #print('Min Score Split Value :',split_val[0])\n            #------------------------------------------------------------------------------------------------------\n            split_col_map_dict = {col_idx : split_val[0]}\n            best_score_col_map_dict = {col_idx : min_gini}\n\n            return split_col_map_dict,best_score_col_map_dict\n        \n        else:\n            #print('1. No Split')\n            split_col_map_dict = {col_idx : np.nan}\n            best_score_col_map_dict = {col_idx : np.nan}\n        \n            return split_col_map_dict,best_score_col_map_dict\n            \n    \n    else:\n        #print('2. No Split')\n        split_col_map_dict = {col_idx : np.nan}\n        best_score_col_map_dict = {col_idx : np.nan}\n        \n        #Returning dict of the best split and their best score with their col idx as key\n        return split_col_map_dict,best_score_col_map_dict\n","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:36:40.925695Z","iopub.execute_input":"2021-07-29T06:36:40.926344Z","iopub.status.idle":"2021-07-29T06:36:40.941474Z","shell.execute_reply.started":"2021-07-29T06:36:40.926303Z","shell.execute_reply":"2021-07-29T06:36:40.940726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UDF for overall best split finding algorithm (Greedy Exact Search)","metadata":{}},{"cell_type":"code","source":"def overall_split(data,col_idx_eligible):\n    \n    scores_dict = {}\n    split_val_dict = {}\n    #col_idx_eligible = list(range(data.shape[1])) \n\n    #--------------------------------------------------------------------------------------------------\n    #For a subset of columns required\n    for col_idx in col_idx_eligible:\n\n        #print('\\n#--------- Index of columns :',col_idx,' ---------#\\n')\n        #Finding the best split for the feature\n        split_val_dict_temp,scores_dict_temp = feature_split_algo(data=data,\n                                                                   col_idx=col_idx)\n        print(scores_dict_temp)\n\n        scores_dict.update(scores_dict_temp)\n        split_val_dict.update(split_val_dict_temp)\n\n    #-------------------------------------------------------------------------------------------------\n    best_score_overall = min(scores_dict.values()) #Extracting the min score across all columns\n    best_score_col_idx = [key for key in list(scores_dict.keys()) if scores_dict[key]==best_score_overall] #Extracting the col idx with the best score\n    #print('best_score_col_idx:',best_score_col_idx)\n    best_col_idx_split_val = split_val_dict[best_score_col_idx[0]]\n    \n    #Returning dict of col idx and lsit of best split score and corresponding threshold\n    return {best_score_col_idx[0] : [best_score_overall,best_col_idx_split_val]}\n","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:36:40.942708Z","iopub.execute_input":"2021-07-29T06:36:40.943385Z","iopub.status.idle":"2021-07-29T06:36:40.960664Z","shell.execute_reply.started":"2021-07-29T06:36:40.943342Z","shell.execute_reply":"2021-07-29T06:36:40.959498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UDF for splitting the current dataframe by the best split found out through the above algorithm","metadata":{}},{"cell_type":"code","source":"def split_array(arr,arr_y,col_idx,split_val):\n    \n    #print('Col idx :',col_idx,'-------> Split value :',split_val)\n    \n    #Split for X of data\n    x_left = arr[arr[:,col_idx]<=split_val]\n    x_right = arr[arr[:,col_idx]>split_val]\n    \n    #Split for Y of data\n    y_left = arr_y[arr[:,col_idx]<=split_val]\n    y_right = arr_y[arr[:,col_idx]>split_val]\n    \n    return x_left,x_right,col_idx,split_val,y_left,y_right","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:36:40.962193Z","iopub.execute_input":"2021-07-29T06:36:40.962605Z","iopub.status.idle":"2021-07-29T06:36:40.975958Z","shell.execute_reply.started":"2021-07-29T06:36:40.962563Z","shell.execute_reply":"2021-07-29T06:36:40.975022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Class for the Classification Tree","metadata":{}},{"cell_type":"code","source":"class cart:\n    \n    def __init__(self):\n            print(\"in init\")\n    \n    #UDF for CART\n    def grow_tree(self,x_data,y_data,node_dict,max_depth,feat_idx_list=[0,1,4,5,7,9],min_samples_split=5,min_samples_leaf=2,depth=0,input_ads_=input_ads):\n        \n        input_ads_ = input_ads_.drop(columns=target) #Dropping target column\n        \n        #To restrict going above max depth\n        if depth<=max_depth:\n            \n            print('#---------------------------------- DEPTH :',depth,' -----------------------------------#')\n            \n            #Calculating best split overall\n            split_dict = overall_split(data=np.append(x_data,y_data,axis=-1),\n                                       col_idx_eligible=feat_idx_list)\n\n            split_col_idx = list(split_dict.keys())[0]\n            gini_ = list(split_dict.values())[0][0]\n            split_col_val = list(split_dict.values())[0][1]\n            \n            #--------------------------------------------------------------------------------------------------------------\n            print('1. -------> Entering root_node of depth :',depth)\n            #Splitting on the best split point found\n            x_left,x_right,col_idx,split_val,y_left,y_right = split_array(arr=x_data,arr_y=y_data,\n                                                                          col_idx=split_col_idx,\n                                                                          split_val=split_col_val)\n            \n            #Defining dictionary for the node of tree \n            node_dict = {'col': input_ads_.columns[split_col_idx], 'col_idx':split_col_idx,\n                         'threshold':split_col_val,'val': np.mean(y_data),'n_class_0': len(y_data[y_data==0]),\n                         'n_class_1': len(y_data[y_data==1]),'n_vals':len(y_data)}  # save the information \n            \n            #-----------------------------------------------------------------------\n            print('2. ------->First :\\n',node_dict)\n\n            # generate tree for the left hand side data\n            print('3. -------> Entering left of depth:',depth)\n            node_dict['left'] = self.grow_tree(x_data=x_left,\n                                               y_data=y_left,\n                                               feat_idx_list=feat_idx_list,\n                                               node_dict={},\n                                               max_depth=max_depth,\n                                               min_samples_split=min_samples_split,\n                                               min_samples_leaf=min_samples_leaf,\n                                               depth=depth+1)   \n            #-----------------------------------------------------------------------\n            if node_dict['left']==None:\n                print('4. -------> None:\\n')\n            \n            # right hand side trees\n            print('5. -------> Entering right of depth:',depth)\n            node_dict['right'] = self.grow_tree(x_data=x_right,\n                                               y_data=y_right,\n                                               feat_idx_list=feat_idx_list,\n                                               node_dict={},\n                                               max_depth=max_depth,\n                                               min_samples_split=min_samples_split,\n                                               min_samples_leaf=min_samples_leaf,\n                                               depth=depth+1)\n            if node_dict['right']==None:\n                print('6. --------> None:\\n')\n            \n            #print('After :\\n',node_dict)\n            #Error Handling\n            try:\n                self.depth += 1   # increase the depth since we call fit once\n            except:\n                print('7. -------> Entering except---')\n                return node_dict\n            \n        elif depth>max_depth:\n            return None\n        \n        elif (len(y_data)<min_samples_split) | (len(y_data)<min_samples_leaf):\n            return None\n        \n        elif node_dict is None:\n            return None\n\n        \n        #Returns the fully expanded tree\n        return node_dict\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:36:40.977448Z","iopub.execute_input":"2021-07-29T06:36:40.977871Z","iopub.status.idle":"2021-07-29T06:36:40.994687Z","shell.execute_reply.started":"2021-07-29T06:36:40.977825Z","shell.execute_reply":"2021-07-29T06:36:40.993393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Invoking the UDF for the whole classification tree building ","metadata":{}},{"cell_type":"code","source":"cart_ = cart()\ntree_dict_ = cart_.grow_tree(x_data=X_arr,\n                             y_data=y_arr,\n                             node_dict={},\n                             max_depth=2,\n                             input_ads_=input_ads)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:36:40.99628Z","iopub.execute_input":"2021-07-29T06:36:40.996703Z","iopub.status.idle":"2021-07-29T06:36:41.284927Z","shell.execute_reply.started":"2021-07-29T06:36:40.99666Z","shell.execute_reply":"2021-07-29T06:36:41.283988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UDF for prediction of a single row in the test data","metadata":{}},{"cell_type":"code","source":"def single_row_pred(test_x_,max_depth,temp_tree_dict): #Takes in the tree dict from training\n\n    for i in range(max_depth): #For all depth\n        \n        #print('------ depth :',i)\n        threshold = temp_tree_dict['threshold']\n        split_col_idx = temp_tree_dict['col_idx']\n\n        tree_dict_left = temp_tree_dict['left']\n        tree_dict_right = temp_tree_dict['right']\n        \n        #Traversing into left side\n        if (test_x_[split_col_idx]<=threshold) & (tree_dict_left!=None) & (tree_dict_right!=None):\n\n            temp_tree_dict = tree_dict_left\n\n            if (temp_tree_dict['left']==None) & (temp_tree_dict['right']==None):\n                prediction = temp_tree_dict['val']\n                #pred_list.append(prediction)\n\n        #Traversing into right side\n        elif (test_x_[split_col_idx]>threshold) & (tree_dict_left!=None) & (tree_dict_right!=None):\n\n            temp_tree_dict = tree_dict_right\n\n            if (temp_tree_dict['left']==None) & (temp_tree_dict['right']==None):\n                prediction = temp_tree_dict['val']\n                #pred_list.append(prediction)\n\n        #If end of the tree is reached, generate predictions \n        elif (tree_dict_left==None) & (tree_dict_right==None):\n\n            prediction = temp_tree_dict['val']\n            \n            \n    return prediction","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:36:41.286241Z","iopub.execute_input":"2021-07-29T06:36:41.286605Z","iopub.status.idle":"2021-07-29T06:36:41.294383Z","shell.execute_reply.started":"2021-07-29T06:36:41.286574Z","shell.execute_reply":"2021-07-29T06:36:41.293205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UDF for prediction overall ","metadata":{}},{"cell_type":"code","source":"def predict_tree(test_data,max_depth,tree_object=tree_dict_,threshold=0.5): #Takes in tree dictionary and threshold of proabability\n    \n    pred_list = []\n    \n    #For each row in test data\n    for idx in range(len(test_data)):\n        \n        #Sngle row prediction calculation\n        pred = np.round(single_row_pred(test_x_=test_data[idx],\n                                        max_depth=max_depth,\n                                        temp_tree_dict=tree_object),3)\n        pred_list.append(pred)\n        \n    print('Length of preds :',len(pred_list))\n    \n    #Converting into array\n    pred_proba = np.array(pred_list)\n    \n    #Converting into class predictions (binary) based on threshold\n    pred_list = (np.array(pred_proba)>threshold).astype(int)\n        \n    return pred_list","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:36:41.296179Z","iopub.execute_input":"2021-07-29T06:36:41.296585Z","iopub.status.idle":"2021-07-29T06:36:41.306773Z","shell.execute_reply.started":"2021-07-29T06:36:41.296545Z","shell.execute_reply":"2021-07-29T06:36:41.305686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Invoking prediction UDF (Generating predictions from the manual decision tree classification model on the test data)","metadata":{}},{"cell_type":"code","source":"preds_manual = predict_tree(test_data=X_test_arr\n                            ,max_depth=2,\n                            tree_object=tree_dict_,\n                            threshold=0.5)\n\n#----------------------------------------------------------------------------------------------------\nprint('Total predictions :', len(preds_manual))\nprint('Unique of predictions :',np.unique(preds_manual))\npreds_manual[0:10]\n\n#----------------------------------------------------------------------------------------------------\n#Evaluating the model\nscore = roc_auc_score(y_test_arr, preds_manual)\nprint('1. ROC AUC: %.3f' % score)\nprint('2. Accuracy :',accuracy_score(y_test_arr, preds_manual))\nprint('3. Classification Report -\\n',classification_report(y_test_arr, preds_manual))\nprint('4. Confusion Matrix - \\n',confusion_matrix(y_test_arr, preds_manual))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:36:41.308102Z","iopub.execute_input":"2021-07-29T06:36:41.30841Z","iopub.status.idle":"2021-07-29T06:36:41.335616Z","shell.execute_reply.started":"2021-07-29T06:36:41.308382Z","shell.execute_reply":"2021-07-29T06:36:41.334704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Insights : We got an accuracy of 81% with ~0.78 ROC AUC, very good for a manually implemented model!!!","metadata":{}},{"cell_type":"markdown","source":"# Sklearn Benchmarking","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndt_clf = DecisionTreeClassifier(random_state=100,max_depth=2,min_samples_split=5,min_samples_leaf=2)\ndt_clf.fit(X_arr[:,[0,1,4,5,7,9]],y_arr)\n\nsklearn_preds = dt_clf.predict(X_test_arr[:,[0,1,4,5,7,9]])\n\n#------------------------------------------------------------------------------------------------------\n#Evaluating the model\nscore = roc_auc_score(y_test_arr, sklearn_preds)\nprint('1. ROC AUC: %.3f' % score)\nprint('2. Accuracy :',accuracy_score(y_test_arr, sklearn_preds))\nprint('3. Classification Report -\\n',classification_report(y_test_arr, sklearn_preds))\nprint('4. Confusion Matrix - \\n',confusion_matrix(y_test_arr, sklearn_preds))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:37:45.265952Z","iopub.execute_input":"2021-07-29T06:37:45.266331Z","iopub.status.idle":"2021-07-29T06:37:45.535585Z","shell.execute_reply.started":"2021-07-29T06:37:45.2663Z","shell.execute_reply":"2021-07-29T06:37:45.534462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Insights : The manual implementation performance is exactly the same as the skelarn version with same hyper-params, indicating correct implementation!! ","metadata":{}},{"cell_type":"markdown","source":"# END","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}