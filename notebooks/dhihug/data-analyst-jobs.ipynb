{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import the necessary libraries for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/data-analyst-jobs/DataAnalyst.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get a first look at the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#shape of the dataset,\ndf.shape\n\n#we have 2253 rows and 16 colums","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#find the datatypes in this data set\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for duplicates. It looks like that we don't have any duplicates\ndf.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we can see that there are a few columns with vlaues such as -1, -1.0, '-1'. We'll replace these with as nan for now\n\ndf=df.replace(-1,np.nan)\ndf=df.replace(-1.0,np.nan)\ndf=df.replace('-1',np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#find missing values (nan values)\ndf.isnull().sum()\n\n#we can see that most of the missing data are in the columns 'Comepetitors' and 'Easy Apply'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for easier referencing, we'll rename our columns\ndf.rename(columns={'Unnamed: 0':'index','Job Title': 'job_title','Salary Estimate':'salary_estimate','Job Description':'job_description',\n                  'Rating':'rating','Company Name':'company_name', 'Location':'location', 'Headquarters':'headquarters','Size':'size',\n                  'Founded':'founded', 'Type of ownership':'type_of_ownership', 'Industry':'industry', 'Sector':'sector','Revenue':'revenue', 'Competitors':'competitors', 'Easy Apply':'easy_apply'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill in the missing values\ndf.fillna('0', inplace=True)\ndf['size'].replace(0, 'Unknown')\n\n#check to to if missing values were replaced by 'Unknown' in this case\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clean the company_name column which contain company name and rating and create a new column called company with just the company name\ndf['company'] = df['company_name'].str.replace('\\n.*','')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#drop company_name and index columns\ndf.drop(['company_name', 'index'], axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the salary estimate column to grab the min salary\nsalary = df['salary_estimate'].str.split(\"-\",expand=True,)\n\n\nmin_salary = salary[0]\nmin_salary = min_salary.str.replace('K',' ')\nmin_salary = min_salary.str.replace('$', ' ').fillna(0).astype('int')\n\ndf['min_salary'] = min_salary\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the salary estimate column to grab the max salary\n\nmax_salary = salary[1]\nmax_salary = max_salary.str.replace('K',' ')\nmax_salary = max_salary.str.replace('(Glassdoor est.)',' ')\nmax_salary = max_salary.str.replace('$', ' ')\nmax_salary = max_salary.str.replace('(', ' ')\nmax_salary = max_salary.str.replace(')', ' ').fillna(0).astype('int')\n\ndf['max_salary'] = max_salary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop the salary estimate column as we don't need it anymore\ndf.drop('salary_estimate', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#find the where min_salary is 0 and drop the row because min salary cannot be zero\ndf.loc[df['min_salary']==0]\ndf.drop(index=2149,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check if it drop worked- we can see in the description that the min salary is now at 24K USD/year\ndf.min_salary.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clean the revenue column to get min and max revenue\ndf['revenue'] = df['revenue'].str.replace('$', '')\ndf['revenue'] = df['revenue'].str.replace(' ', '')\ndf['revenue'] = df['revenue'].str.replace('(USD)', '')\ndf['revenue'] = df['revenue'].str.replace('(', '')\ndf['revenue'] = df['revenue'].str.replace(')', '')\ndf.revenue.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['revenue']= df['revenue'].replace('0','Unknown/Non-Applicable' )\ndf['revenue'] = df['revenue'].replace('Unknown/Non-Applicable', None)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['revenue'] = df['revenue'].str.replace('2to5billion', '2billionto5billion')\ndf['revenue'] = df['revenue'].str.replace('5to10billion', '5billionto10billion')\ndf['revenue'] = df['revenue'].str.replace('1to2billion', '1billionto2billion')\ndf['revenue'] = df['revenue'].str.replace('Lessthan1million', '0millionto1million')\ndf['revenue'] = df['revenue'].str.replace('500millionto1billion', '500millionto1billion')\n\ndf['revenue'] = df['revenue'].replace('10+billion', '10billionto11billion')\n\n\ndf.revenue.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['revenue'] = df['revenue'].str.replace('million', '')\ndf['revenue'] = df['revenue'].str.replace('billion', '000')\n\n\ndf.revenue.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split the revenue column to get a min and max revenue\nnew_revenue = df['revenue'].str.split(\"to\",expand=True,)\nmin_revenue = (new_revenue[0]).astype('float64')\ndf['min_revenue']= min_revenue\n\nmax_revenue = (new_revenue[1]).astype('float64')\ndf['max_revenue']= max_revenue\ndf.max_revenue.fillna(0)\ndf.min_revenue.fillna(0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replace the zeroes by 'Unknown' for the type_of_ownership column\ndf['type_of_ownership']= df['type_of_ownership'].replace('0','Unknown')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clean the job_title column\njob_desc =df['job_title'].str.split(',', expand=True)\ndf['job_descrip']= job_desc[0]\ndf['job_descrip']=df['job_descrip'].str.replace('Sr. Data Analyst','Senior Data Analyst')\ndf['job_descrip']=df['job_descrip'].str.replace('Data Analyst Junior','Junior Data Analyst')\ndata_jobs = df.job_descrip.value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot a bar graph to visualize this information#find out which companies are hiring the most\ntop_company=df.company.value_counts().head(10)\n\nplt.figure(figsize=(10,8))\n\nsns.barplot(x=list(top_company.index), y=list(top_company.values), color= 'blue')\nplt.title('Top 10 companies that had the highest number of hirings')\nplt.xlabel('Company Name')\nplt.ylabel('Number of hires')\nplt.xticks(rotation = 90)\nplt.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#salary distrubution visulization using histogram and KDE\n\n#set matplotlib figure\nf, axes = plt.subplots(1, 2, figsize=(7, 7), sharex=True)\nsns.despine(left=True)\n\n\nsns.distplot(df['max_salary'],  color= 'g', ax=axes[0], label = 'Maximum salary')\nsns.distplot(df['min_salary'],   ax=axes[1], label = 'Minimum Salary')\n\n\nplt.title('Salary disturbution for all data analyst jobs')\nplt.xlabel('')\nplt.ylabel('')\nplt.legend()\nplt.xticks(rotation = 90)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# KDE plots for min and max salaries\nsns.kdeplot(data=df['max_salary'], label='Maximum salary', shade=True)\nsns.kdeplot(data=df['min_salary'], label='Minimum salary', shade=True)\n\n# Add title\nplt.title(\"Salary Distribution\", size =16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#find the min average salary by location\nav_salary_loc_min = (df.groupby('location')['min_salary'].mean().sort_values()).head(10).astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#find the max average salary by location\nav_salary_loc_max = (df.groupby('location')['max_salary'].mean().sort_values()).tail(10).astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot bar graph to show highest average salary by location\nplt.figure(figsize =(8,6))\n\nsns.barplot(x=list(av_salary_loc_max.index), y=list(av_salary_loc_max.values) )\n\nplt.title('Highest average salary by location')\nplt.xlabel('Name of city')\nplt.ylabel('Average annual salary in USD')\n\nplt.xticks(rotation =90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot bar graph to show lowest average salary by location\nplt.figure(figsize =(8,5))\n\nsns.barplot(x=list(av_salary_loc_min.index),y=list(av_salary_loc_min.values))\n\n\nplt.title('Lowest average salary by location')\nplt.xlabel('Name of city')\nplt.ylabel('Average annual salary in USD')\n\nplt.xticks(rotation =90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can conclude that that the best paid data analyst jobs are are located in California, with Newark paying upto 190K USD average per year. The worst paid data analyst jobs are located in Utah with Draper paying an average salary of 26K USD per year. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate average revenue\naverage_revenue = (df.min_revenue+df.max_revenue)/2\ndf['average_revenue']= average_revenue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#which sectors make most profit? plot a bargraph to see\ntop_sector= df.groupby('sector')['average_revenue'].mean().sort_values()\ntop_sector=top_sector.tail(20).astype('int')\n\nsns.barplot(x= list(top_sector.index), y= list(top_sector.values) , palette='spring')\n\nplt.title('Top 20 sectors by annual average revenue', size =16)\nplt.xlabel('Sector')\nplt.ylabel('Average revenue in million USD')\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No surprise that insurance and retail make the highest revenue. "},{"metadata":{"trusted":true},"cell_type":"code","source":"top_industry= df.groupby('industry')['average_revenue'].mean()\ntop_industry=top_industry.sort_values(ascending=False).head(20).astype('int')\n\nsns.barplot(x= list(top_industry.index), y= list(top_industry.values) , palette='autumn')\n\nplt.title('Top 20 industries with highest average revenue', size =16)\nplt.xlabel('Industry')\nplt.ylabel('Average revenue in million USD')\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for the values 0 in the company size column, we replace them by 'Unknown'\ndf['size']=df['size'].replace('0', 'Unknown')\nhire_comp_size= df.groupby('size')['job_title'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#find the number of hires depending on the size of the company\nhire_comp_size= df.groupby('size')['job_title'].count()\n\nsns.barplot(x= hire_comp_size.index, y= hire_comp_size.values , palette='autumn')\nplt.title('Number of hires by size of company', size =16)\nplt.xlabel('Size if company')\nplt.ylabel('Number of hires')\nplt.xticks(rotation=90)\n\n#pie chart showing the same results\nexplode =(0,0.1,0,0,0,0,0,0)\nfig1, ax1 = plt.subplots()\nax1.pie(hire_comp_size, explode=explode, labels=hire_comp_size.index, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"18.7% of the total hires occurred in relatively small size (51-200 employees) companies"},{"metadata":{"trusted":true},"cell_type":"code","source":"#find number of hires by type of ownership\nowner_type = (df.groupby('type_of_ownership')['job_title'].count()).sort_values().tail(5)\n#pie chart showing the results\nexplode =(0,0.1,0,0,0,)\n\nfig1, ax1 = plt.subplots(figsize=(100,10))\nax1.pie(owner_type, explode=explode, autopct='%1.1f%%', startangle=90 )\nplt.legend(labels=owner_type.index, loc='upper right', fontsize=8)\nplt.title('Percentage of hires by type of ownership of company', size=16)\nplt.axis(\"off\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"60% of the total hires occurred in private companies."},{"metadata":{"trusted":true},"cell_type":"code","source":"#number of jobs by job title\ndata_jobs = df.job_descrip.value_counts().head(10)\n\n#plot bar graph to show highest average salary by location\nplt.figure(figsize =(8,6))\n\nsns.barplot(x=list(data_jobs.index), y=list(data_jobs.values) , palette='spring')\n\nplt.title('Number of jobs by job title', size=16, fontweight='bold')\nplt.xlabel('Job title', fontweight='bold')\nplt.ylabel('Number of jobs',fontweight='bold')\n\nplt.xticks(rotation =90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Word Cloud of job_titles\nfrom wordcloud import WordCloud, ImageColorGenerator\nfrom PIL import Image\n\nplt.subplots(figsize=(14,14))\n\nwc = WordCloud(background_color = 'lightblue')\ntxt = df['job_title']\nwc.generate(str(' '.join(txt)))\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}