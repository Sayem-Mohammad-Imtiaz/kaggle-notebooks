{"cells":[{"metadata":{"id":"YnH6Xmzfej3_","outputId":"9072d546-a12e-42f4-ae7f-d8aa8dc0d616","trusted":false},"cell_type":"code","source":"'''\nUseful links:\nData preprocessing: https://www.kaggle.com/parthsharma5795/comprehensive-twitter-airline-sentiment-analysis\nTrain ULMFit in IMDB: https://course.fast.ai/videos/?lesson=8\n'''","execution_count":null,"outputs":[]},{"metadata":{"id":"giFFtcEeej35","outputId":"04774db5-6830-476a-cdf7-51fb39cb6c28","trusted":false},"cell_type":"code","source":"# If you are running the notebook in COLAB run the following lines of code\n!pip install torch==1.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n!pip install fastai==2.1.4","execution_count":null,"outputs":[]},{"metadata":{"id":"C-hIT1Rjej4H","trusted":false},"cell_type":"code","source":"from fastai.text.all import *\nimport torch\nimport re\n# import os\n# from os import listdir\n# from pathlib import Path\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython.display import display,HTML","execution_count":null,"outputs":[]},{"metadata":{"id":"sx1-uyuP59OS","outputId":"d418d131-1b65-4d17-c69d-54037994b78f","trusted":false},"cell_type":"code","source":"train_on_gpu = torch.cuda.is_available()\nif(train_on_gpu):\n    print('Training on GPU!')\nelse: \n    print('No GPU available, training on CPU; consider making n_epochs very small.')","execution_count":null,"outputs":[]},{"metadata":{"id":"dGXky7atwedi","trusted":false},"cell_type":"code","source":"url = 'https://raw.githubusercontent.com/arnaujc91/ULMFit/master/data/Tweets.csv'\ntweets = pd.read_csv(url)","execution_count":null,"outputs":[]},{"metadata":{"id":"vKL8XVE7wT5x"},"cell_type":"markdown","source":"## Analyse the data"},{"metadata":{"scrolled":false,"id":"z6KI1VBGej4L","outputId":"3d41cad9-f550-48b0-a856-3a429c632042","trusted":false},"cell_type":"code","source":"tweets.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"PdAzzgp0ej4O","outputId":"76c01f38-f781-4893-e858-4a116ca6cc1f","trusted":false},"cell_type":"code","source":"display(HTML(tweets.to_html(columns=['text'], index=False,header=None, max_rows=20)))","execution_count":null,"outputs":[]},{"metadata":{"id":"4omE8Yx_ej4S","outputId":"c952fd31-7eb0-4725-d28a-5f4f184bf1e7","trusted":false},"cell_type":"code","source":"tweets.airline_sentiment_confidence.hist(bins=40)","execution_count":null,"outputs":[]},{"metadata":{"id":"0bBR9oMzej4V","outputId":"2f8b7e13-5a66-45fb-de8c-2fab2eb75346","trusted":false},"cell_type":"code","source":"tweets.airline_sentiment.value_counts()/len(tweets)","execution_count":null,"outputs":[]},{"metadata":{"id":"0NM3RTTgEBai"},"cell_type":"markdown","source":"Later on we can also use this same model to predict the reason of the complaint:"},{"metadata":{"id":"b-fsLEaDej4Z","outputId":"39d8d177-1875-4cc6-a791-15ae46a87058","trusted":false},"cell_type":"code","source":"tweets.negativereason.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"FgQ6INI2ej4i","outputId":"61b5315f-08c1-4f95-a085-e89328bbc134","trusted":false},"cell_type":"code","source":"'''If you downloaded the file and have it in a \"data\" directory,\n        uncomment and run the lines below'''\n# data_directory = Path(os.getcwd())/'data'\n# assert data_directory.is_dir(), 'Data directory not found'\n# data_files = listdir(data_directory)\n# print(data_files)\n\n# csv_datafile = data_directory/'Tweets.csv'\n# print('Data file:', csv_datafile)\n# tweets = pd.read_csv(csv_datafile)\n# tweets","execution_count":null,"outputs":[]},{"metadata":{"id":"hYwGAUesej4n"},"cell_type":"markdown","source":"Every tweet starts with a [Twitter handle](https://sproutsocial.com/glossary/twitter-handle/) which refers to the Airline, to which the twitter message is adressed to. E.g:"},{"metadata":{"id":"H5VNyeqJej4n","outputId":"7287420b-ede6-4f60-97c2-99ec6adbabb7","trusted":false},"cell_type":"code","source":"list(tweets.text[:5]), list(tweets.airline[:5])","execution_count":null,"outputs":[]},{"metadata":{"id":"TcMyNhAKej4q"},"cell_type":"markdown","source":"We want to remove this information. In order to do so we add a new rule to the [default rules](https://github.com/fastai/fastai/blob/a8ed5a64f93df9be02eef907ddbc355f3ad130d1/fastai/text/core.py#L96) for preprocessing text:"},{"metadata":{"id":"6XC69JTJej4r","trusted":false},"cell_type":"code","source":"def rm_first_handle(t):\n    return re.sub(r'^@\\w* ', '', t)\n\nrules = defaults.text_proc_rules\nrules.insert(0, rm_first_handle)","execution_count":null,"outputs":[]},{"metadata":{"id":"Hi9tGTup8lHZ"},"cell_type":"markdown","source":"**IMPORTANT**: If you run the previous line of code the rules will be modified for the whole Notebook. So later on this will also affect the DataLoaders!"},{"metadata":{"id":"4IAVY33Gej4t"},"cell_type":"markdown","source":"Now let's tokenize the tweets:"},{"metadata":{"scrolled":true,"id":"NpRuGSaYej4u","outputId":"6ecd5e88-8125-41d7-d4aa-4e49b1f475a0","trusted":false},"cell_type":"code","source":"tokenized_df, vocab_count = tokenize_df(tweets,  text_cols='text', tok=SpacyTokenizer())\nvocab = list(vocab_count.keys())\nprint(len(vocab))\nprint(vocab[:5])","execution_count":null,"outputs":[]},{"metadata":{"id":"NrOQVMTh7Vrx","outputId":"8a9b904e-e3e9-48c2-b097-b3d6afff4f38","trusted":false},"cell_type":"code","source":"tokenized_df.text[:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"cDvouEWUej4w"},"cell_type":"markdown","source":"*tokenize_df* returns the tokenized dataframe and also the counting of the tokenized words in the dataset. For example if we want to get all the words that appear at least 3 times, we can write the following code:"},{"metadata":{"scrolled":true,"id":"2TGHLDckej4x","outputId":"30043b36-262f-4544-9994-f61267d22418","trusted":false},"cell_type":"code","source":"len([key for key in vocab_count if vocab_count[key]>2])","execution_count":null,"outputs":[]},{"metadata":{"id":"-VV8UoEZej4z"},"cell_type":"markdown","source":"So 4683 words appear **at least** 3 times in the entire dataset."},{"metadata":{"id":"r2BRmizOej40"},"cell_type":"markdown","source":"We can now compare the tokenized VS the original tweets:\n\n"},{"metadata":{"scrolled":false,"id":"CA-2LCb6ej40","outputId":"5aec9363-4190-4e00-a79e-67afb1c02cc3","trusted":false},"cell_type":"code","source":"i = 0\nfor a,b in zip(list(tokenized_df.text), list(tweets.text)):\n    print('before: ', b)\n    print('after: ', ' '.join(a))\n    print('\\n')\n    if i == 5:\n        break\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{"id":"Mfzk0mFUej43"},"cell_type":"markdown","source":"## Create the DataLoaders"},{"metadata":{"id":"gqABDa6bWpcR","trusted":false},"cell_type":"code","source":"# TODO: Random splitting is not a good idea, we have to see what are the distributions of labels of the tweets.\n# For example if we have 80% negative tweets in training and 20% positive and then the other way around in validation\n# This might not be optimal. In particular we should have a homogenous distribution of tweets in the training set\n# in order to not have a bias towards a particular classifciation. 80% neg => Bias towards neg tweet classification.","execution_count":null,"outputs":[]},{"metadata":{"id":"C_YRkNKzOpaQ","trusted":false},"cell_type":"code","source":"# For an explanation of this function go back to \"Analyse the data\" section.\ndef rm_first_handle(t):\n    return re.sub(r'^@\\w* ', '', t)\n\nrules = defaults.text_proc_rules\nrules.insert(0, rm_first_handle)","execution_count":null,"outputs":[]},{"metadata":{"id":"OYmIMQefej43"},"cell_type":"markdown","source":"We will now create a [TextDataLoader](https://docs.fast.ai/text.data#TextDataLoaders) which is a Wrapper around the [DataLoader](https://docs.fast.ai/data.core.html#DataLoaders) class. The DataLoader splits our dataset between training and validation. The TextDataLoader adds more functionality specific to NLP problems, like the vocabulary of the data.\n\nNow a few things:\n- Remember that the TextDataLoaders just consider a word as part of the vocab if it appears **more than 3 times** in the entire dataset by default if you use `TextDataLoaders.from_df`.\n- We also need `is_lm=True` because we will first train a **Language Model**."},{"metadata":{"scrolled":false,"id":"xy_gntFeej46","outputId":"05ea4e9e-2dd8-4485-e0ee-8d4acb58a879","trusted":false},"cell_type":"code","source":"lm_dls = TextDataLoaders.from_df(tweets, text_col='text',  is_lm=True)\nlm_dls.show_batch(max_n=5)","execution_count":null,"outputs":[]},{"metadata":{"id":"QwpZAYuDej49"},"cell_type":"markdown","source":"Let's see a little bit the structure of this dataloader:"},{"metadata":{"scrolled":false,"id":"LsorDk6mej49","outputId":"ab3f1a91-6aaf-4b4a-d492-306b367b2bb0","trusted":false},"cell_type":"code","source":"lm_dls._docs","execution_count":null,"outputs":[]},{"metadata":{"id":"CqvqB28oej5B"},"cell_type":"markdown","source":"The previous DataLoader was for training or fine tunning the language model. The following one will be the DataLoader used for **classification**."},{"metadata":{"id":"_XjEx7AXej5C","outputId":"1936c8f7-f5a4-485b-9245-8b6c69700cea","trusted":false},"cell_type":"code","source":"tc_dls = TextDataLoaders.from_df(tweets, text_col='text', label_col='airline_sentiment')\ntc_dls.show_batch(max_n=8)","execution_count":null,"outputs":[]},{"metadata":{"id":"-bbkHiyPej5E"},"cell_type":"markdown","source":"From the code of `TextDataLoaders.from_df` we can see that the vocab is created with words that appear **at least 3 times** in the entire dataset. Any words that appear with a lower frequency will be automatically tokenized as `xxunk`, which stands for *unknown*."},{"metadata":{"id":"Nihen0qRej5E"},"cell_type":"markdown","source":"So far the words in the vocab are words that appear at least 3 times in the entire dataset, if we want to change that, we can not do it directly from the high level API that fastai offers, but instead we need a couple of more lines of code. Anyway the following lines are just what `TextDataLoaders.from_df` does but changing the parameter **min_freq** to one."},{"metadata":{"scrolled":true,"id":"pJOj79rvej5F","outputId":"1a330884-4e49-40e6-c9e6-0ed35142ae2d","trusted":false},"cell_type":"code","source":"'''\nIf you wanna have a TextDataLoader with words that appear less than 3 times in the dataset:\n'''\n\n# we set min_freq to ONE to allow any words that appear at least once.\nmin_freq=1\n\ndblock = DataBlock(blocks=[TextBlock.from_df(text_cols='text', is_lm=True, min_freq=min_freq) ],\n                           get_x=ColReader(\"text\"),\n                           splitter = RandomSplitter(valid_pct=0.2))\n\ntweets_f1 = TextDataLoaders.from_dblock(dblock, tweets)","execution_count":null,"outputs":[]},{"metadata":{"id":"p8iRqH26ej5H"},"cell_type":"markdown","source":"Another issue is that eventhough `TextDataLoaders.from_df` call `tokenize_df`, the first will add some extra special tokens that are not provided by `tokenize_df`. Let's see what this means:"},{"metadata":{"scrolled":true,"id":"LL2geuW8ej5I","outputId":"5279e868-cb3b-44bd-824d-e7d40855246e","trusted":false},"cell_type":"code","source":"print('Length of vocabulary obtained from: ')\nprint(f'\\n   - tokenize_df: {len(set(vocab))}')\nprint(f'\\n   - TextDataLoaders.from_df (min_freq=3): {len(lm_dls.vocab)}')\nprint(f'\\n   - TextDataLoaders.from_dblock (min_freq=1): {len(tweets_f1.vocab)}')\n","execution_count":null,"outputs":[]},{"metadata":{"id":"HT46U_Jnej5L"},"cell_type":"markdown","source":"A priori you would expect that the vocab from `tokenize_df`and `TextDataLoaders.from_dblock` to be the same size as both obtain the vocab from any word that appears in the dataset. Despite of this one has 7 more items than the other. Which are those items?"},{"metadata":{"id":"25TDq-1Fej5M","trusted":false},"cell_type":"code","source":"def vocab_diff(vocab1, vocab2):\n    if len(vocab1)>len(vocab2):\n        b_vocab = set(vocab1)\n        s_vocab = set(vocab2)\n    else:\n        b_vocab = set(vocab2)\n        s_vocab = set(vocab1)\n        \n    return list(b_vocab-s_vocab.intersection(b_vocab))\n        ","execution_count":null,"outputs":[]},{"metadata":{"id":"DW5IZPQsej5S"},"cell_type":"markdown","source":"The added special tokens from `TextDataLoaders.from_df` are:"},{"metadata":{"id":"41D8bqyjej5T","outputId":"da52204b-371b-4c64-f265-993ea40b97cd","trusted":false},"cell_type":"code","source":"vocab_diff(tweets_f1.vocab, vocab)","execution_count":null,"outputs":[]},{"metadata":{"id":"i34un_Qeej5Z"},"cell_type":"markdown","source":"Besides, for some reason the special token `xxfake` appears twice in the vocab from `tweets_f1` (probably a bug):"},{"metadata":{"scrolled":true,"id":"OKGwlPDyej5Z","outputId":"50035ca6-3f42-4c6a-913c-a76cffdef46a","trusted":false},"cell_type":"code","source":"import collections\nprint({item:count for item, count in collections.Counter(tweets_f1.vocab).items() if count > 1})","execution_count":null,"outputs":[]},{"metadata":{"id":"1Ayyad-7ej5b","outputId":"9d668145-639b-438f-99e9-830337b4567c","trusted":false},"cell_type":"code","source":"vocab_diff([key for key in vocab_count if vocab_count[key]>2], lm_dls.vocab)","execution_count":null,"outputs":[]},{"metadata":{"id":"sHEjMUrtej5t"},"cell_type":"markdown","source":"## Some tools for debugging the data"},{"metadata":{"id":"3DNxpDYYej5t"},"cell_type":"markdown","source":"A useful tool for debugging can be to find some word in the original texts, for example:"},{"metadata":{"scrolled":false,"id":"8tTMaN9Iej5u","outputId":"40fde52d-8532-43cf-e058-7d1873575d5a","trusted":false},"cell_type":"code","source":"tweets.text[tweets.text.str.contains('think the US site allows that ', regex=False)]","execution_count":null,"outputs":[]},{"metadata":{"id":"XqV9UZ3Cej5w"},"cell_type":"markdown","source":"The word \"*explaining*\" appears twice in the dataset, once in row 2279 and once in row 14225"},{"metadata":{"id":"NuvQgeX0ej5w"},"cell_type":"markdown","source":"Another useful tool is to decode the numericalized datasets:"},{"metadata":{"id":"PcRN4cIeej5x","trusted":false},"cell_type":"code","source":"decoded = lm_dls.train_ds.decode(lm_dls.train_ds)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"id":"-uGqwvwkej51","outputId":"096a02fe-4d2c-4d50-cf7f-07d8c036bf3e","trusted":false},"cell_type":"code","source":"lm_dls.train_ds[0], decoded[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"ojKij_Eiej54"},"cell_type":"markdown","source":"The dictionary between tokenized words and integers is found inside the [Numericalize](https://docs.fast.ai/text.data#Numericalize) class:"},{"metadata":{"scrolled":true,"id":"eVFDmWITej6M","outputId":"21c170d6-14f7-44d6-acdd-d031fbfadb90","trusted":false},"cell_type":"code","source":"lm_dls.train_ds.numericalize.o2i","execution_count":null,"outputs":[]},{"metadata":{"id":"pYeQLsq4ej6O","trusted":false},"cell_type":"code","source":"# [lm_dls.train_ds.numericalize.o2i[word] for word in decoded[0][0].split()]","execution_count":null,"outputs":[]},{"metadata":{"id":"0r3biWlrej6Q"},"cell_type":"markdown","source":"Also useful:"},{"metadata":{"scrolled":false,"id":"vP0UbTqfej6Q","trusted":false},"cell_type":"code","source":"for text in decoded:\n        if 'xxeos' in text[0]:\n            print(text[0])","execution_count":null,"outputs":[]},{"metadata":{"id":"m7fzk-mAej6S"},"cell_type":"markdown","source":"## Training"},{"metadata":{"id":"Vo7ZAUCqHGQ4"},"cell_type":"markdown","source":"Make sure that the vocabs of the classifier and the Language Model are the same (as a crosscheck):"},{"metadata":{"id":"xVu-uN7b2-GT","trusted":false},"cell_type":"code","source":"assert set(lm_dls.vocab) == set(tc_dls.vocab[0]), 'Vocabs are not equal!'","execution_count":null,"outputs":[]},{"metadata":{"id":"vf_EdKqrej6T"},"cell_type":"markdown","source":"Now let's define the [Callbacks](https://docs.fast.ai/callback.tracker) we are going to use:\n- [ActivationStats](https://docs.fast.ai/callback.hook#ActivationStats): Callback that record the mean and std of activations.\n- [ShowGraphCallback](https://docs.fast.ai/callback.progress#ShowGraphCallback): Update a graph of training and validation loss\n- [ParamScheduler](https://docs.fast.ai/callback.schedule#ParamScheduler): We are not going to use it in this guide, but it is definitely interesting to play with it. It allows to change the learning rate at different stages of the training and also to have a different learning rate scheduler for every parameter group. "},{"metadata":{"id":"4Jwm-NT8ej6T","trusted":false},"cell_type":"code","source":"cbs = [\n       ShowGraphCallback,\n       ActivationStats(with_hist=True),\n       SaveModelCallback\n#        ParamScheduler(sched)\n      ]","execution_count":null,"outputs":[]},{"metadata":{"id":"qYNqsuAlej6V"},"cell_type":"markdown","source":"Now let's create the learner. The [Learner](https://docs.fast.ai/learner) class is the class that contains everything necessary for training. It contains:\n- DataLoaders\n- Model\n- Loss function\n- Optimizer \n- Splitter to split the model in several parameter groups\n- Callbacks for the training.\n- etc.\n\nIn the following line we will pass as arguments to the function [language_model_learner](https://docs.fast.ai/text.learner#language_model_learner): \n- The DataLoader\n- The name of the model: [AWS_LSTM](https://docs.fast.ai/text.models.awdlstm)\n- The Callbacks\n- The path were we wanna save the trained model or part of the model, e.g. just the encoder"},{"metadata":{"id":"KeE_PEQJej6W","outputId":"44c13085-4a8d-42d6-e84b-ef1c43b66194","trusted":false},"cell_type":"code","source":"learner = language_model_learner(lm_dls, AWD_LSTM, cbs=cbs, metrics=[accuracy])","execution_count":null,"outputs":[]},{"metadata":{"id":"K7wOFq-4ej6i"},"cell_type":"markdown","source":"We can see that there are already certain Callbacks which are set up by default:"},{"metadata":{"id":"TpqDL7iTej6i","outputId":"2b16e55e-0c82-4cc9-e055-77244e2ebf6c","trusted":false},"cell_type":"code","source":"list(learner.cbs)","execution_count":null,"outputs":[]},{"metadata":{"id":"6zxuJx340FQy"},"cell_type":"markdown","source":"The following code shows the layers of the language model:"},{"metadata":{"id":"8LYXMbgQmE6a","outputId":"c4c31129-90ad-4d69-cf65-63324e200f02","trusted":false},"cell_type":"code","source":"modules = [m for m in flatten_model(learner.model) if has_params(m)]; modules","execution_count":null,"outputs":[]},{"metadata":{"id":"cKhQlRugej6l"},"cell_type":"markdown","source":"And we will do the same for the Learner of the Text Classifier:"},{"metadata":{"id":"xHaICN6Sej6m","trusted":false},"cell_type":"code","source":"learn = text_classifier_learner(tc_dls, AWD_LSTM, drop_mult=0.5, cbs=cbs, metrics=[accuracy, F1Score(average='micro')]).to_fp16()","execution_count":null,"outputs":[]},{"metadata":{"id":"K_ASGSibFyhp","outputId":"eda05add-7ad9-401f-df4a-b7dc8d3cf074","trusted":false},"cell_type":"code","source":"list(learn.cbs)","execution_count":null,"outputs":[]},{"metadata":{"id":"xbecWlLjzu-a"},"cell_type":"markdown","source":"By default when we load a pretrained model with the `language_model_learner`, not all the layers are trainable, to see which layers are potentially trainable we use the following code:"},{"metadata":{"id":"OYnVWHah8iZE","trusted":false},"cell_type":"code","source":"def requires_grad_bool(m:nn.Module)->Optional[bool]:\n    ps = list(m.parameters())\n    return ps[0].requires_grad\n\ndef trainable_layers(learn):\n  modules = [m for m in flatten_model(learn.model) if has_params(m)]\n  for it in modules:\n    print(f\"{requires_grad_bool(it)}  -- \",it)","execution_count":null,"outputs":[]},{"metadata":{"id":"hu5TRoii1_oy","outputId":"60cb498e-0b75-443d-d914-e1d1b6afe6e3","trusted":false},"cell_type":"code","source":"trainable_layers(learner)","execution_count":null,"outputs":[]},{"metadata":{"id":"tyG0R1JSjB2Q"},"cell_type":"markdown","source":"Now we are going to try to find the best learning rate for our models, in order to do so I recommend to check the follwing question asked in stackoverflow --> [choosing-the-learning-rate-using-fastais-learn-lr-find](https://stackoverflow.com/questions/61172627/choosing-the-learning-rate-using-fastais-learn-lr-find)\n"},{"metadata":{"id":"k4c4PgrNej6p","outputId":"cc7f1b91-3214-42b0-f770-3152d258bab9","trusted":false},"cell_type":"code","source":"learner.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"id":"Ioubr-M1ej62","outputId":"0edbfb05-7aa7-40d1-8491-cd3fec89ff40","trusted":false},"cell_type":"code","source":"learn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"id":"BmealYs1ej64"},"cell_type":"markdown","source":"Now let's follow the receipt from Jeremy Howard in his [paper](https://arxiv.org/abs/1801.06146). It is important to understand whats the difference between `fit`and `fit_one_cycle` (for that take a look [here](https://iconof.com/1cycle-learning-rate-policy/)). Also as you will see we will progressively unfreeze the layers during training, this is seen to perform better than just `fit`."},{"metadata":{"id":"YLZcZQQlej64","trusted":false},"cell_type":"code","source":"??learner.fit_one_cycle","execution_count":null,"outputs":[]},{"metadata":{"id":"IM4JvcfEej67","outputId":"afad9505-4180-4b10-9980-e1310103d2b8","trusted":false},"cell_type":"code","source":"learner.fit_one_cycle(10, 2e-2)\ntrainable_layers(learner)\nlearner.save('language_model')\nlearner.save_encoder('finetuned')","execution_count":null,"outputs":[]},{"metadata":{"id":"DSwpJ0TAej7g","outputId":"1a07abed-c1d1-455a-c582-e520d7d9ec54","trusted":false},"cell_type":"code","source":"learn = learn.load_encoder('finetuned')\nlearn.fit_one_cycle(12, 2e-3)\nprint('\\n')\ntrainable_layers(learn)","execution_count":null,"outputs":[]},{"metadata":{"id":"gbgE2ssanc9p","outputId":"86869cec-8694-4921-ae1f-7483b04f798c","trusted":false},"cell_type":"code","source":"learn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"id":"_7WWKB3uej7k","outputId":"dbeb50cc-d739-492f-dab7-38feb596171c","trusted":false},"cell_type":"code","source":"# REFINING 1\nlearn.load('model')\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(5, slice(1e-2/(2.6**4),1e-2))\nprint('\\n')\ntrainable_layers(learn)","execution_count":null,"outputs":[]},{"metadata":{"id":"b6SMyRtbej7p","outputId":"420256aa-92a8-4359-8603-d62f0ca22d70","trusted":false},"cell_type":"code","source":"# REFINING 2\nlearn.load('model')\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(3, slice(5e-3/(2.6**4),5e-3))\nprint('\\n')\ntrainable_layers(learn)","execution_count":null,"outputs":[]},{"metadata":{"id":"Qmv2NDB0ej7r","outputId":"4e705c45-dc8c-4597-ec50-b475e35f6155","trusted":false},"cell_type":"code","source":"# REFINING 3\nlearn.load('model')\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))\nprint('\\n')\ntrainable_layers(learn)","execution_count":null,"outputs":[]},{"metadata":{"id":"JTJP1wEso81R"},"cell_type":"markdown","source":"We reach an accuracy of 83% fo tweet classification, not bad! "},{"metadata":{"id":"d4b38WNtej79"},"cell_type":"markdown","source":"You can now test the language model; you can use it to create inventend sentences and see if they make sense. The more the invented sentences look like made by a person the better trained the language model."},{"metadata":{"id":"PPPwYVoHej8A","trusted":false},"cell_type":"code","source":"learner = learner.load('language_model')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"id":"ADNaK17hej8G","outputId":"c535432b-bbbc-440e-eac1-232fd89ff4fd","trusted":false},"cell_type":"code","source":"learner.predict(\"Never\", 15, temperature=0.75) ","execution_count":null,"outputs":[]},{"metadata":{"id":"14PtCjptfil5"},"cell_type":"markdown","source":"## Statistics of the training"},{"metadata":{"id":"0cBgyw3rromi","outputId":"61369b9a-0c92-4ff8-be74-a32a098b2c07","trusted":false},"cell_type":"code","source":"list(learner.activation_stats.stats)","execution_count":null,"outputs":[]},{"metadata":{"id":"B6PUztGarxXJ","outputId":"0fe4a9a7-4600-4af8-c733-182c0acd5e8e","trusted":false},"cell_type":"code","source":"learner.activation_stats.plot_layer_stats(8)","execution_count":null,"outputs":[]},{"metadata":{"id":"qmQPvtIPv8nT","outputId":"51a49485-d096-4efe-83b8-d79a6f869e70","trusted":false},"cell_type":"code","source":"learner.activation_stats.color_dim(8)","execution_count":null,"outputs":[]},{"metadata":{"id":"QQHEFDLRej8V"},"cell_type":"markdown","source":"## Asses model performance"},{"metadata":{"id":"nrHIgwqTLOe3","outputId":"0207db40-96d4-410e-e9f3-e924ee765e70","trusted":false},"cell_type":"code","source":"n= 1392\nlearn.predict(tweets.text[n]), tweets.airline_sentiment[n]","execution_count":null,"outputs":[]},{"metadata":{"id":"mlxthbjVMibS","outputId":"de07af53-1c20-4583-f28f-b5be17ca11f3","trusted":false},"cell_type":"code","source":"tweets.text[n]","execution_count":null,"outputs":[]},{"metadata":{"id":"ExyZeJocQR-h","outputId":"0b78c1ee-5c33-4a95-a220-07fa070cff3c","trusted":false},"cell_type":"code","source":"clas_int = ClassificationInterpretation.from_learner(learn)","execution_count":null,"outputs":[]},{"metadata":{"id":"eqOG8osZJ0US"},"cell_type":"markdown","source":"Here you can see that there is a Bias wtr to negative classification. This is because in the dataset most of the tweets are negative!"},{"metadata":{"id":"NZLVD4jLQaao","outputId":"af044f03-6715-40ab-e78e-e53910daf532","trusted":false},"cell_type":"code","source":"clas_int.plot_confusion_matrix()","execution_count":null,"outputs":[]},{"metadata":{"id":"pwQuFCcCb_HK","outputId":"a78a5a56-8364-4022-ab24-eb63545ae742","trusted":false},"cell_type":"code","source":"clas_int.print_classification_report()","execution_count":null,"outputs":[]},{"metadata":{"id":"WqU8K6FHQqFj","outputId":"3f6a0a67-622a-4719-b1a6-3cd2459bafe9","trusted":false},"cell_type":"code","source":"clas_int.top_losses()","execution_count":null,"outputs":[]},{"metadata":{"id":"4C11DBpufbzp","outputId":"e7fedbe3-efcb-4ec5-bf07-48291ac151ab","trusted":false},"cell_type":"code","source":"tc_dls.vocab[1]","execution_count":null,"outputs":[]},{"metadata":{"id":"uHAyYhyVe4PL","outputId":"6975bbfb-2f72-48ac-ec09-13e85bfbaffe","trusted":false},"cell_type":"code","source":"preds = learn.get_preds(dl=tc_dls[1], with_input=True, with_loss=True, with_decoded=True, act=None)","execution_count":null,"outputs":[]},{"metadata":{"id":"bODLVjCAe_Jf"},"cell_type":"markdown","source":"The variable predicts contains the following information: \n\n0.   inputs\n1.   predictions\n2.   targets\n3. decoded\n4. losses"},{"metadata":{"id":"ETB9AiRYglLW","outputId":"661b3046-c405-4d75-ee9e-67ab1091540d","trusted":false},"cell_type":"code","source":"torch.sum(preds[3] == preds[2]).item()/len(preds[3])","execution_count":null,"outputs":[]},{"metadata":{"id":"db3VsEpDhQZd","trusted":false},"cell_type":"code","source":"matches = preds[3] == preds[2]","execution_count":null,"outputs":[]},{"metadata":{"id":"Ic3CUCHSfEIC","trusted":false},"cell_type":"code","source":"decoded_valid = L(zip(lm_dls.valid_ds.decode(preds[0]),\n                      list(map(lambda x: tc_dls.vocab[1][x], preds[2].tolist())),\n                      list(map(lambda x: tc_dls.vocab[1][x], preds[3].tolist()))))","execution_count":null,"outputs":[]},{"metadata":{"id":"lbK7zZOeN--X","trusted":false},"cell_type":"code","source":"df = pd.DataFrame(list(decoded_valid[~matches]), columns =['tweet', 'Truth', 'Computed']) ","execution_count":null,"outputs":[]},{"metadata":{"id":"6FL1dceJOg9w","outputId":"80dc7a09-9d13-4155-941d-9356778b836f","trusted":false},"cell_type":"code","source":"display(HTML(df.to_html(index=False)))","execution_count":null,"outputs":[]},{"metadata":{"id":"aYMbZsRoej8X"},"cell_type":"markdown","source":"In order to know if we are getting a good performance, we can compare our model training to a benchmark. We can use the IMDB dataset from fastai and see if we get a similar performance or not. "},{"metadata":{"id":"XGRIJ2VSej8Y","outputId":"1f59af21-841a-414b-a52e-1c41fa617a37","trusted":false},"cell_type":"code","source":"path = untar_data(URLs.IMDB_SAMPLE)\nimdb = pd.read_csv(path/'texts.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"Ad9-8DJGej8c","outputId":"19bb67b7-55ac-443d-a796-670658b8dae7","trusted":false},"cell_type":"code","source":"imdb.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"TeMwmtO-ej8e","outputId":"e2bb5a20-6e26-4dcc-f2f5-02c05bacbc22","trusted":false},"cell_type":"code","source":"imdb_cls  = TextDataLoaders.from_df(imdb, text_col='text', label_col='label')\nimdb_lm = TextDataLoaders.from_df(imdb, text_col='text', is_lm=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"5JoDJQ3Jej8x"},"cell_type":"markdown","source":"We can compare how similar are the features of both datasets, for example we can check how many words does any review contain compared to how many words do the tweets contain."},{"metadata":{"scrolled":true,"id":"ZudMWr7-ej8z","outputId":"eabbc71c-d345-4b3f-c472-6742c75f00a8","trusted":false},"cell_type":"code","source":"pd.concat([tweets.text[:1000].apply(lambda s: len(s.split())),\n           imdb.text.apply(lambda s: len(s.split()))],\n           axis=1,\n           keys=['tweets', 'imdb']).plot.hist(alpha=0.4, bins = 500, xlim=(0,400)) ","execution_count":null,"outputs":[]},{"metadata":{"id":"Ap6XXZbIdUGD"},"cell_type":"markdown","source":"So we can check the words per tweet or the words per review:"},{"metadata":{"id":"xXSMRSC9Y6WW","outputId":"0951d136-f369-4278-b727-c8d8746cdda9","trusted":false},"cell_type":"code","source":"tweets.text.apply(lambda s: len(s.split())).mean(), imdb.text.apply(lambda s: len(s.split())).mean()","execution_count":null,"outputs":[]},{"metadata":{"id":"k7iwkQQpdbiM"},"cell_type":"markdown","source":"Eventhough the words per tweet are much less, the total amount of words is almost teh same:"},{"metadata":{"id":"cjAPNwA7Zl04","outputId":"116e2e4b-e582-4cef-c53d-1f6470ed820b","trusted":false},"cell_type":"code","source":"tweets.text.apply(lambda s: len(s.split())).sum(), imdb.text.apply(lambda s: len(s.split())).sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"4BW_kcO8Zxc-","outputId":"96de00b1-6319-4cbb-fd28-5b141cfe64d6","trusted":false},"cell_type":"code","source":"258446/247797","execution_count":null,"outputs":[]},{"metadata":{"id":"hG5PrZ--ej81","outputId":"9197a461-c06f-4499-af26-dfebf76b5155","trusted":false},"cell_type":"code","source":"tweets.text.apply(lambda s: len(s.split())).hist(bins=60)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"id":"GiTqUpe6ej83","outputId":"72a1134a-a4d9-445c-a2a6-5e685b026935","trusted":false},"cell_type":"code","source":"imdb.text.apply(lambda s: len(s.split())).hist(bins=60)","execution_count":null,"outputs":[]},{"metadata":{"id":"DPTvDez3ej85"},"cell_type":"markdown","source":"As we can see the reviews are in general much bigger than the tweets, something that is already expected but can influence the performance of the training."},{"metadata":{"id":"fiT3AlVWej9H"},"cell_type":"markdown","source":"Another thing to analyze is the vocabs, is the vocab from `imdb` much bigger than from the `tweets`?"},{"metadata":{"id":"J-w_Rt7iej9H","outputId":"ef788386-59b1-430e-c86b-fda4d3e54744","trusted":false},"cell_type":"code","source":"len(lm_dls.vocab), len(imdb_lm.vocab)","execution_count":null,"outputs":[]},{"metadata":{"id":"BJv9-rCPej9N"},"cell_type":"markdown","source":"As we can see the vocab of `imdb` is almost the double as the vocab for the `tweets`, therefore it could be that the language model for the `imdb` performs better."},{"metadata":{"id":"eFWMWas2ej9N"},"cell_type":"markdown","source":"In order to simplify things, I will just define a function that does everything we have done so far:"},{"metadata":{"id":"Iwoyoaz-ej9N","trusted":false},"cell_type":"code","source":"def complete_training(lm_dls, cl_dls, cbs=None):\n\n    if cbs==None:\n        cbs = [\n              ShowGraphCallback,\n              ActivationStats,\n              SaveModelCallback\n            ]\n\n    learner = language_model_learner(lm_dls, AWD_LSTM, cbs = cbs,  metrics=[accuracy])\n    # TRY CHANGING drop_mult, to see if there is an effect in training small datasets\n    learn = text_classifier_learner(cl_dls, AWD_LSTM, drop_mult=0.5, cbs=cbs, metrics=accuracy).to_fp16()\n\n    # ----  TRAIN THE LANGUAGE MODEL  ----\n    learner.fit_one_cycle(10, 2e-2)\n    # learner.save('language_model')\n    learner.save_encoder('finetuned')\n\n    # ----  TRAIN THE CLASSIFIER  ----\n    learn = learn.load_encoder('finetuned')\n    learn.fit_one_cycle(12,  2e-3)\n\n    # REFINING 1\n    learn.freeze_to(-2)\n    learn.fit_one_cycle(5, slice(1e-2/(2.6**4),1e-2))\n\n    # REFINING 2\n    learn.freeze_to(-3)\n    learn.fit_one_cycle(3, slice(5e-3/(2.6**4),5e-3))\n\n    # REFINING 3\n    learn.unfreeze()\n    learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))\n\n    learner.remove_cb(ShowGraphCallback)\n    learn.remove_cb(ShowGraphCallback)\n\n    return learner, learn","execution_count":null,"outputs":[]},{"metadata":{"id":"W1K5jCcDej9P","trusted":false},"cell_type":"code","source":"learner_imdb, learn_imdb = complete_training(imdb_lm, imdb_cls)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}