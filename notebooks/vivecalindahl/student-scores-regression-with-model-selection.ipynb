{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About\nData analysis and prediction of student test scores based on the Kaggle [Predict test scores of students ](https://www.kaggle.com/kwadwoofosu/predict-test-scores-of-students) dataset.\n\nThe main task is to accurately and efficiently predict the post-test scores using info about the students.","metadata":{}},{"cell_type":"markdown","source":"# Imports and setup\nBasic libraries and config.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport sklearn","metadata":{"execution":{"iopub.status.busy":"2021-08-19T11:09:09.846253Z","iopub.execute_input":"2021-08-19T11:09:09.846532Z","iopub.status.idle":"2021-08-19T11:09:10.931734Z","shell.execute_reply.started":"2021-08-19T11:09:09.846508Z","shell.execute_reply":"2021-08-19T11:09:10.930635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_theme(style=\"whitegrid\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data exploration","metadata":{}},{"cell_type":"code","source":"data_path = \"/kaggle/input/predict-test-scores-of-students/test_scores.csv\"\ndf = pd.read_csv(data_path)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-08-19T11:09:13.527401Z","iopub.execute_input":"2021-08-19T11:09:13.527677Z","iopub.status.idle":"2021-08-19T11:09:13.589413Z","shell.execute_reply.started":"2021-08-19T11:09:13.527653Z","shell.execute_reply":"2021-08-19T11:09:13.588402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 2133 students taking the test, each with 11 features, including the prediction target variable \"posttest\". That's few enough that we can manually have a look at them one by one:","metadata":{}},{"cell_type":"code","source":"# Print some info about the features\nfor c in df:\n    print(\"=\"*80)\n    print(\"*\"*3, c, \"*\"*3)\n    print(\"dtype\", df[c].dtype)\n    print(\"Number of unique values:\", df[c].nunique())\n    print(\"Frequencies of unique values:\")\n    print(df[c].value_counts())\n    print(\"NaNs:\", df[c].isna().sum())","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most features  seem to have some predictive value based on common sense. The student id however, is not relevant and shouldn't be used for predctions. ","metadata":{}},{"cell_type":"code","source":"df.drop(columns=['student_id'], inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To better understand what classroom \"type\" that *classroom* represent I check whether *n_students* can be reproduced simply by counting:","metadata":{}},{"cell_type":"code","source":"# Count the number of students that have the same classroom label\nclassroom_count_students = df.groupby('classroom')['n_student'].count()\nn_student_xy = df.merge(classroom_count_students, on='classroom')[['n_student_x', 'n_student_y']]\n# The number of students obtained this way by counting and the n_student variable are the same. \nassert (n_student_xy['n_student_y'] == n_student_xy['n_student_x']).all()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Indeed, *classroom* simply correspond to the group of students that are in the same class:","metadata":{}},{"cell_type":"markdown","source":"One would expect that the pre-test score *pretest* is very informative, perhaps the most informative, about the post-test score *posttest*. My understanding of the pretest score is that before the real test there was a trial test to help the students assess how much effort they need to put into studying to get their desired score. Indeed we see that there is a very strong correlation between the two and the relationship looks fairly linear.","metadata":{}},{"cell_type":"code","source":"sns.relplot(data=df, x='pretest', y='posttest');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From a distributional plot it becomes more clear that the *posttest* values are on average higher than the *pretest* values.","metadata":{}},{"cell_type":"code","source":"# Put pretest and posttest values in one column (\"value\"), labels in column \"variable\"\ndata = df.melt(value_vars=['pretest', 'posttest'])\nsns.displot(data=data, x='value', hue='variable', kind='kde');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The only other numerical feature is the number of students in the class *n_student* and there seems to be a correlation between the number of students and *posttest*:  score results decrease with larger classes. ","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(data=df, x='n_student', y='posttest')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also quickly create bar plots for all the non-numerical features to get an overview of the data:","metadata":{}},{"cell_type":"code","source":"# Bar plots for all non-numerical features\nfor c in df:\n    if df[c].dtype == \"object\":\n        plt.figure()\n        # Group by c, take the mean over posttest, sort and output c values\n        order = df.groupby(c)['posttest'].mean().reset_index().sort_values(by='posttest')[c]\n        sns.barplot(data=df, x=c, y='posttest', order=order)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In general the variables, look like they have predictive power: different categories have different *posttest* mean values. Gender does not show obvious differences, but it could be that there are correlations that aren't visible in a bar chart and it's only one categorical variable, so might as well keep it in there.","metadata":{}},{"cell_type":"markdown","source":"Clearly both the schools and the class rooms show performance differences, but it's unclear to me from the plots above how schools and classes relate to each other. E.g. are the difference in class performance mainly due to being at a particular school? To visualize this better I plot *posttest* class average as function the class rooms, but now grouped by the school and sorted by the school's performance:","metadata":{}},{"cell_type":"code","source":"import itertools\n\ndef plot_classroom_sorted_by_scores(df):\n    \"\"\"Bar plot of the class rooms sorted posttest scores.\n    \n    Sorts first by school score, then by classroom score.\n    \"\"\"\n    # Get the right plot order of classrooms.\n    # Mean posttest per classroom and the school that the uniquely classroom belongs to. \n    df1 = df.groupby(['classroom']).agg(school=('school', lambda x: x.unique()),\n                                         classroom_mean_posttest=('posttest', 'mean')).reset_index()\n    # Mean posttet per school, to use for sorting the classrooms\n    df2 = df.groupby(['school']).agg(school_mean_posttest=('posttest', 'mean')).reset_index()\n    df12 = df1.merge(df2, on='school')\n    # Sort the classrooms by the school mean, then by classroom mean\n    order = df12.sort_values(['school_mean_posttest', 'classroom_mean_posttest'])['classroom']\n\n    # Display the school name instead of the classroom \n    xtick_labels = df12.sort_values(['school_mean_posttest', 'classroom_mean_posttest'])['school'].reset_index(drop=True)\n    indices_to_zero = []\n    for i, l in enumerate(xtick_labels):\n        indices = xtick_labels[xtick_labels == l].index.to_list()\n        middle_index = indices[len(indices)//2]\n        not_middle_index = (i != middle_index)\n        indices_to_zero.append(not_middle_index)\n    # Erase labels\n    xtick_labels[indices_to_zero] = ''\n    # abbreviate labels to fit better\n    #xtick_labels = xtick_labels.apply(lambda x: x[:3] if len(x) > 0 else x)\n\n    # Color by school\n    palette = itertools.cycle(sns.color_palette())\n    # Give colors in the same order as they'll be plotted\n    schools_ordered_by_value = df.groupby('school').mean().sort_values('posttest').index.values\n    school_colors = {school:next(palette) for i, school in  enumerate(schools_ordered_by_value)}\n    # Map classroom to the school\n    classroom_to_school = df12[['classroom', 'school']].set_index('classroom').to_dict()['school']\n    classroom_colors_by_school = {classroom: school_colors[classroom_to_school[classroom]]\\\n                                  for i, classroom in  enumerate(set(df['classroom']))}\n\n    # Plot finally \n    fig, ax = plt.subplots(figsize=(24, 4))\n    ax = sns.barplot(data=df, x='classroom', y='posttest',\n                     # esthetics\n                     order=order, palette=classroom_colors_by_school, ax=ax)\n    ax.set_xticklabels(xtick_labels);\n    ax.set_xlabel(\"classroom (grouped and labeled by school)\");\n    return df12","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df12 = plot_classroom_sorted_by_scores(df)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we see more clearly that also within a school, there are significant differences between different *class_room* (maybe they represent different specializations within a school?). For instance the best class rooms of \"QOQTS\" is as good as the best of \"OJOBU\", even though \"OJOBU\" ranks higher on average. So it makes sense to use *class_room* as a feature for prediction.","metadata":{}},{"cell_type":"markdown","source":"# Data transformations","metadata":{}},{"cell_type":"code","source":"target = [\"posttest\"]\nfeatures = [c for c in df.columns if c not in target]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One-hot encoding of categorical variables","metadata":{}},{"cell_type":"code","source":"# Extract features, encode categorical\ndf_features = pd.get_dummies(df[features], drop_first=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_features.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split into train/validation and test set\nNote: if we were not interested in getting an accurate final estimate, we could skip the testing and use all data for training.","metadata":{"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_features.to_numpy()\ny = df[target].to_numpy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normalize numerical","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale features to [0,1]\nx_scaler = MinMaxScaler().fit(X_train)\n\nX_train = x_scaler.transform(X_train)\nX_test = x_scaler.transform(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Skip target scaling for simplicity, to interpret output more directly\n#y_scaler = MinMaxScaler().fit(y_train)\n\n#y_train = y_scaler.transform(y_train)\n#y_test = y_scaler.transform(y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metrics","metadata":{"tags":[]}},{"cell_type":"code","source":"def mean_absolute_error(y, ypred):\n    return np.mean(np.abs(y.ravel() - ypred.ravel()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross-validation config\nI use k-fold cross-validation to optimize model hyperparameters (if any) and obtain better estimates for the model performance.","metadata":{"tags":[]}},{"cell_type":"code","source":"# Only if need more metrics\n# from sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_splits = 10\ncv = KFold(n_splits=n_splits, shuffle=True, random_state=0)\nscoring = 'neg_mean_absolute_error'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction baseline, 1D linear regression\nAs a (strong) baseline I'll use a linear model, with only one feature, the *pretest* variable.","metadata":{"tags":[]}},{"cell_type":"code","source":"from sklearn import linear_model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regr = linear_model.LinearRegression()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_pretest = df_features.columns.to_list().index('pretest')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X1_train = X_train[:,col_pretest].reshape(-1,1)\nX1_test = X_test[:, col_pretest].reshape(-1,1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Estimate model performance\nscores = cross_val_score(regr, X1_train, y_train, cv=cv, scoring=scoring)\n# Score = neg MAE\nmae, mae_std = np.mean(-scores), np.std(-scores)\nprint(\"Cross-validation MAE:\", mae, mae_std)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit and print train test MAE\n# Fit on train set\nregr.fit(X1_train, y_train)\n\n# Train error\nypred = regr.predict(X1_train)\nprint(\"Train MAE:\", mean_absolute_error(y_train, ypred))\n\n# Test error \nypred = regr.predict(X1_test)\nprint(\"Test MAE:\", mean_absolute_error(y_test, ypred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot model and data\nplt.plot(X1_test.ravel(), y_test, 'o', label='data')\nplt.plot(X1_test.ravel(), ypred, '-', label='model')\nplt.legend()\nplt.xlabel('pretest')\nplt.ylabel('posttest');","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linear regression using all features\nSimply using all features as-is yields a more complex model but with significantly lowered MAE.","metadata":{"tags":[]}},{"cell_type":"code","source":"regr = sklearn.linear_model.LinearRegression()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Estimate model performance\nscores = cross_val_score(regr, X_train, y_train, cv=cv, scoring=scoring)\n# Score = neg MAE\nmae, mae_std = np.mean(-scores), np.std(-scores)\nprint(\"Cross-validation MAE:\", mae, mae_std)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit and print train test MAE\n# Fit on train set\nregr.fit(X_train, y_train)\n\n# Train error\nypred = regr.predict(X_train)\nprint(\"Train MAE:\", mean_absolute_error(y_train, ypred))\n\n# Test error \nypred = regr.predict(X_test)\nprint(\"Test MAE:\", mean_absolute_error(y_test, ypred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot model and data\nplt.plot(X_test[:,col_pretest], y_test, 'o', label='data')\nplt.plot(X_test[:,col_pretest], ypred, '-', label='model')\nplt.legend()\nplt.xlabel('pretest')\nplt.ylabel('posttest');","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also note that the gap between training and test error increased, indicating more of a tendency to overfitting.","metadata":{}},{"cell_type":"markdown","source":"# Feature engineering","metadata":{"tags":[]}},{"cell_type":"markdown","source":"## Reduction, replace classroom and shool with statistics \nThe *school* and especially *classroom* variables lead to fairly high dimensional feature space (>100) due to the many different categories. In order to obtain a more compact representation I experiment with deriving a numerical feature using these variables. Similarly to how *n_student* counts the number of students in a class. I can try adding the classroom and school statistics:\n* the *pretest* average/standard deviation per *class*\n* the *pretest* average/standard deviation per *classroom*  \n\nThen e.g. students from different schools but with similar school averages will be recognized as similar in this sense.","metadata":{"tags":[]}},{"cell_type":"code","source":"df['pretest_school'] = df.groupby('school')['pretest'].transform('mean')\ndf['pretest_classroom'] = df.groupby('classroom')['pretest'].transform('mean')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Could consider the standard deviation too\n#df['pretest_school_std'] = df.groupby('school')['pretest'].transform('std')\n#df['pretest_classroom_std'] = df.groupby('classroom')['pretest'].transform('std')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_red = [c for c in df.columns if c not in target and c not in ['classroom', 'school']]\n# One-hot encode\ndf_features_red = pd.get_dummies(df[features_red])\ndf_features_red","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test linear regression","metadata":{}},{"cell_type":"code","source":"X = df_features_red.to_numpy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale features to [0,1]\nx_scaler = MinMaxScaler().fit(X_train)\n\nX_train = x_scaler.transform(X_train)\nX_test = x_scaler.transform(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regr = linear_model.LinearRegression()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Estimate model performance\nscores = cross_val_score(regr, X_train, y_train, cv=cv, scoring=scoring)\n# Score = neg MAE\nmae, mae_std = np.mean(-scores), np.std(-scores)\nprint(\"Cross-validation MAE:\", mae, mae_std)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit and print train test MAE\n# Fit on train set\nregr.fit(X_train, y_train)\n\n# Train error\nypred = regr.predict(X_train)\nprint(\"Train MAE:\", mean_absolute_error(y_train, ypred))\n\n# Test error \nypred = regr.predict(X_test)\nprint(\"Test MAE:\", mean_absolute_error(y_test, ypred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The MAE is slightly lowered, albeit within the error bars. Since the feature space is smaller, easier to work with and also can be generalized to include new unseen schools and classrooms, at first sight I'd say this is a better representation for continuied modeling.","metadata":{}},{"cell_type":"markdown","source":"## Try adding mix terms\nTo make the model slightly more complex, we can add polynomial mix terms, i.e. for each feature pair $x_1$, $x_2$ add a feature $x_{1,2} = x_1 \\cdot x_2$.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poly = PolynomialFeatures(2, include_bias=False, interaction_only=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poly.fit(X_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_ = poly.transform(X_train)\nX_test_ = poly.transform(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Estimate model performance\nscores = cross_val_score(regr, X_train_, y_train, cv=cv, scoring=scoring)\n# Score = neg MAE\nmae, mae_std = np.mean(-scores), np.std(-scores)\nprint(\"Cross-validation MAE:\", mae, mae_std)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since this has no advantage in performance and is a more complex model, I don't pursue this further here.","metadata":{}},{"cell_type":"markdown","source":"# Model selection, grid search","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, ElasticNet, Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neural_network import MLPRegressor\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to compare models I perform a grid search over a few candidate models as well as over hyperparameters of those models.","metadata":{"tags":[]}},{"cell_type":"code","source":"models = {\n    'linear': LinearRegression(),\n    # Linear model with L2 penalty term\n    'ridge': Ridge(random_state=0),\n    # Linear model with L1 penalty term\n    'lasso': Lasso(random_state=0),\n    # Linear model with L1, L2 penalty terms\n    'elastic': ElasticNet(random_state=0),\n    # Decision tree\n    'decision-tree': DecisionTreeRegressor(random_state=0),\n    # Neural network, fix some of the optimization parameters\n    'neural-network': MLPRegressor(random_state=0,\n                                   max_iter=1000,\n                                   solver='sgd',\n                                   learning_rate='constant',\n                                   momentum=0,\n                                   nesterovs_momentum=False)\n}\nalphas = (0.1, 0.1, 1, 10, 100)\nparams = {\n    'linear':{},\n    'ridge': {'alpha': alphas},\n    'lasso': {'alpha': alphas},\n    'elastic': {'alpha': alphas, 'l1_ratio': (0.25, 0.5, 0.75)},\n    'decision-tree': {'max_depth': (2, 3, 5, 7, 9, 12)},\n    'neural-network': {'alpha': [0.1, 1, 10], \n                       'hidden_layer_sizes': [(10,), (100,), (10, 10), (100,)],\n                       'learning_rate_init': [0.001, 0.01, 0.1]}\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xs = {\n    # Pretest only\n    '1-dim': df_features.iloc[:,col_pretest].to_numpy().reshape(-1,1),\n    # Reduce school, classroom to pretest averages\n    '15-dim': df_features_red.to_numpy(),\n    # All features\n    '126-dim': df_features.to_numpy()\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"search_results = {}\nfor x_name, X in Xs.items():\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n    x_scaler = MinMaxScaler().fit(X_train)\n    X_train = x_scaler.transform(X_train)\n    X_test = x_scaler.transform(X_test)\n    for model_name, regr in models.items():\n        print(f\"{model_name}, {x_name}\")\n        search = GridSearchCV(regr, param_grid=params[model_name], scoring=scoring, cv=cv, verbose=1)\n        # Ignore warnings from combinations that don't converge\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n            search_results[model_name+'_'+x_name] = search.fit(X_train, y_train.ravel())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Collect best results\nbest_results = dict()\nfor name, search in search_results.items():\n    means, stds, params = search.cv_results_['mean_test_score'], search.cv_results_['std_test_score'], search.cv_results_['params']\n    means_stds_params_sorted = sorted(zip(means, stds, params), key=(lambda t: t[0]), reverse=True)\n    first = means_stds_params_sorted[0]\n    mean, std, params = first\n    mean = -mean # Negate scoring\n    best_results[name] = mean, std, params","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_results = dict(sorted(best_results.items(), key=lambda x: x[1][0]))\nprint(*best_results.items(), sep='\\n')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pick the simplest model as baseline\nbaseline = 'linear_1-dim'\nbaseline_val = best_results[baseline][0]\nplt.axhline(baseline_val, linestyle='--', color='gray', linewidth=2, label=baseline)\nplt.legend()\nlabels=[]\nfor i, label in enumerate(best_results):\n    mae, std = best_results[label][:2]\n    plt.errorbar(i, mae, yerr=std, color='C0', marker='o', capsize=3)\n    labels.append(label)\nplt.xticks(np.arange(len(labels)), labels, rotation=90);\nplt.ylabel('Mean absolute error');\nplt.title(\"Model performance\");","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The winners in this case are the relatively simple models with the 15-dimensional feature space, Ridge or unregularized linear perform essentially as well. Neural network also performs on par with them, however since it's generally a more complex model there is no clear benefit for this particular dataset.","metadata":{}},{"cell_type":"markdown","source":"# Final evaluation\nLinear and ridge regression performed the best. Linear was already evaluated on the test data above (MAE $\\approx 2.3$). Below, I evaluate Ridge and expect essentially the same test set performance.","metadata":{}},{"cell_type":"code","source":"first = next(iter(best_results.keys()))\nbest_params = best_results[first][-1]\nprint(first, best_params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regr = Ridge(random_state=0, **best_params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = Xs['15-dim']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\nx_scaler = MinMaxScaler().fit(X_train)\nX_train = x_scaler.transform(X_train)\nX_test = x_scaler.transform(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit and print train test MAE\n# Fit on train set\nregr.fit(X_train, y_train)\n\n# Train error\nypred = regr.predict(X_train)\nprint(\"Train MAE:\", mean_absolute_error(y_train, ypred))\n\n# Test error \nypred = regr.predict(X_test)\nprint(\"Test MAE:\", mean_absolute_error(y_test, ypred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summary\n- A grid search over linear, tree and neural network models combined with different dimensionality of feature space were performed to find a model to predict *posttest*.\n- Result: linear models, Ridge or unregularized, with a reduced feature space (15-dim) performed the best (test MAE $\\approx 2.3$) and are in addition simple and fast to evaluate. ","metadata":{"tags":[]}}]}