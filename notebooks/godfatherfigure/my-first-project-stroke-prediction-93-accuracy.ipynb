{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predicting Stroke in Patients Using Machine Learning\nThis notebook is a stroke prediction classification machine learning project with an imbalanced class. \n\nAccording to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.\n\nThis dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relevant information about the patient.\n\n## What we'll end up with\nSince we already have a dataset, we'll approach the problem with the following machine learning modelling framework.\n\nTo work through these topics, we'll use pandas, Matplotlib and NumPy for data anaylsis, as well as, Scikit-Learn for machine learning and modelling tasks.\n\nWe'll work through each step and by the end of the notebook, we'll have a handful of models, all which can predict whether or not a person has stroke based on a number of different parameters at a considerable accuracy.\n\n\n## 1. Problem Definition\nIn this case, the problem we will be exploring is binary classification.\n\nThis is because we're going to be using a number of different features about a person to predict stroke probability.\n\nIn a statement,\n\n    Given clinical parameters about a patient, can we predict whether or not a patient is likely to get stroke?\n    \n\n## 2. Data\n\nThe original data came in a formatted way from kaggle [stroke prediction dataset](https://www.kaggle.com/fedesoriano/stroke-prediction-dataset) \n\nThe original database contains 5110 observations with 12 attributes. **Attributes** (also called **features**) are the variables that we'll use to predict our **target variable**.\n\n\n## 3. Evaluation\nThe evaluation metric is something to define at the start of a project.\n\nSince machine learning is very experimental,\n\n    If we can reach 95% accuracy at predicting whether or not a patient is likely to get stroke during the proof of concept, we'll pursue this project.\n\nThe reason this is helpful is it provides a rough goal for a machine learning engineer or data scientist to work towards.\n\n## 4. Features\nFeatures are different parts of the data. We're going to visualize the relationships between the different features of the data and how it can lead to stroke.\n\nOne of the most common ways to understand the features is to look at the **data dictionary**.\n\n## Stroke Data Dictionary\nA data dictionary describes the data you're dealing with, not all datasets come with them. \n\nThe following are the features we'll use to predict our likely target variable (stroke or no stroke).\n\n* 1 id: unique identifier\n* 2 gender: \"Male\", \"Female\" or \"Other\"\n* 3 age: age of the patient\n* 4 hypertension: \n    * 0 if the patient doesn't have hypertension \n    * 1 if the patient has hypertension\n* 5 heart_disease: \n    * 0 if the patient doesn't have any heart diseases \n    * 1 if the patient has a heart disease\n* 6 ever_married: \"No\" or \"Yes\"\n* 7 work_type: \n    * \"children\"\n    * \"Govt_jov\" \n    * \"Never_worked\" \n    * \"Private\" or \"Self-employed\"\n* 8 Residence_type: \"Rural\" or \"Urban\"\n* 9 avg_glucose_level: average glucose level in blood\n* 10 bmi: body mass index\n* 11 smoking_status: \n    * \"formerly smoked\" \n    * \"never smoked\" \n    * \"smokes\" or \"Unknown\"*\n* 12 stroke: \n    * 1 if the patient had a stroke \n    * 0 if not\n\n**Note**: \"Unknown\" in smoking_status means that the information is unavailable for this patient","metadata":{}},{"cell_type":"code","source":"## Regular EDA and plotting libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nfrom matplotlib import style\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n# We want our plots to appear in the notebook\n%matplotlib inline \n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n## Model evaluators\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df =  pd.read_csv('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploratory Data Analysis\nSince EDA has no real set methodolgy, the following is a short check list to to walk through:\n\n1. From the dataframe features, can smoking induce stroke?\n2. Can a person with a heart disease be more prone to having stroke?\n3. Can a person with hypertension be more prone to having stroke?\n4. Does gender play a role in a person being prone to stroke?\n5. Does the work type, residence type, average glucose level, bmi, marital status, age play a role in a person having stroke? \n6. Whatâ€™s missing from the data and how do you deal with it?","metadata":{}},{"cell_type":"code","source":"# Check the first 10 rows of the dataframe\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see how many positive (1) and negative (0) samples we have in our dataframe plus the proportion\nprint('Length of entire data:', len(df))\nprint('Length of non stroke patient:', len(df[df['stroke']==0]))\nprint('Length of stroke patient:', len(df[df['stroke']==1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since these two values are not close, our target column can be considered **imbalanced**. An **imbalanced** target column, meaning some classes have far more samples, can be harder to model than a balanced set. From our data dictionary, if the patient has a stroke, it is denoted with 1, if the patient does not have stroke, it is denoted as 0.","metadata":{}},{"cell_type":"code","source":"# Normalized value counts\ndf.stroke.value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the pandas dataframe above,\n* id, hypertension, heart_disease and stroke are int datatype.\n* gender, ever_married, work_type, Residence_type and smoking_status are object datatype.\n* age, avg_glucose_level and bmi are float datatype.","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for missing values\ndf.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for which numeric columns have null values\nfor label, content in df.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill numeric rows with the median\nfor label, content in df.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            \n            # Add a binary column which tells if the data was missing or not\n            df[label+\"_is_missing\"] = pd.isnull(content)\n            \n            # Fill missing numeric values with median since it's more robust than the mean\n            df[label] = content.fillna(content.median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if there's any null values\nfor label, content in df.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Replacing 0 and 1 with \"No\" and \"Yes\" in Hypertension and Heart Disease columns.","metadata":{}},{"cell_type":"code","source":"df['hypertension'].replace([0, 1], ['No', 'Yes'], inplace=True)\ndf['heart_disease'].replace([0, 1], ['No', 'Yes'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Categorising BMI into \"Underweight\", \"Normal Weight\", \"Overweight\" and \"Obese\".","metadata":{}},{"cell_type":"code","source":"results=[]\n\nfor i in df['bmi']:\n    if (i < 19.5):\n        results.append('Underweight')\n   \n    elif (i >= 19.5) & (i < 25.5):\n        results.append('Normal Weight')\n        \n    elif (i >= 25.5) & (i < 30.0):\n        results.append('Overweight')\n        \n    elif (i > 30.0):\n        results.append('Obese')\n        \nresults2 = pd.DataFrame(results, columns=['bmi_category'])\n\ndf['bmi_category']=results2\n\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the stroke value counts with a bar graph\ndf.stroke.value_counts().plot(kind='bar', title='Stroke Class', color=['salmon', 'lightblue'])\nplt.xlabel('0=No Stroke, 1=Stroke')\nplt.ylabel('Count')\nplt.xticks(rotation=0);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stroke Frequency per Gender","metadata":{}},{"cell_type":"code","source":"df.gender.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove the 'other' gender\ndf.drop(df[df['gender'] == 'Other'].index, inplace = True)\ndf['gender'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.gender.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compare stroke column with gender column\npd.crosstab(df.stroke, df.gender)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation**:\n\nSince there are 2994 women, 2853 women do not have stroke and 141 of them have a stroke, we might infer, based on this one variable if the participant is a woman, there's a 4.7% chance she will have a stroke.\n\nAs for males, there are 2115 males, 2007 men do not have stroke and 108 of them have a stroke. So we might predict, if the participant is male, 5.1% chance he will have stroke.\n\nAveraging these two values, we can assume, based on no other parameters, if there's a person, there's a 4.9% chance they have stroke.","metadata":{}},{"cell_type":"markdown","source":"### Stroke Frequency for Gender","metadata":{}},{"cell_type":"code","source":"# Create a plot\npd.crosstab(df.gender, df.stroke).plot(kind='bar',\n                                   figsize=(10, 6),\n                                   color= ['lightblue', 'salmon']);\n\n# Add some attributes to it\nplt.title(\"Stroke Frequency for Gender\")\nplt.xlabel('Gender')\nplt.ylabel('Number of Gender')\nplt.xticks(rotation=0); # keep the labels on the x-axis vertical","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\nFrom the bar chart above, The likelihood of having stroke for both genders is almost equal but females are slightly more prone to having stroke.","metadata":{}},{"cell_type":"markdown","source":"### Age vs Average Glucose Level for Stroke\nLet's try combining a couple of independent variables, such as, age and avg_glucose_level and then comparing them to our stroke target column.\n\nBecause there are so many different values for age and average glucose level, we'll use a scatter plot.","metadata":{}},{"cell_type":"code","source":"# Create another figure\nplt.figure(figsize=(10, 6))\n\n# Start with positive examples\nplt.scatter(df.age[df.stroke==1][:300],\n            df.avg_glucose_level[df.stroke==1][:300],\n            c='salmon')\n\n# For negative example, we want them on the same plot, so we call the plt function again\nplt.scatter(df.age[df.stroke==0][:300],\n            df.avg_glucose_level[df.stroke==0][:300],\n            c='lightblue') \n\n# Add some helpful information\nplt.title('Stroke in Function of Age and Average Glucose Level')\nplt.xlabel('Age')\nplt.ylabel('Average Glucose Level')\nplt.legend(['Stroke', 'No Stroke']);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation**:\n\nIt seems the older a person is, the higher their average glucose level (red dots are higher on the right of the graph). Also, a large proportion of the scatter plot points on the y-axis are concentrated between (50-150), and from my observation, it looks like there are outliers upwards of 150 which indicates people with a high glucose level which might be a factor in causing stroke for older people.\n\nLet's check the age **distribution** with a histogram.","metadata":{}},{"cell_type":"markdown","source":"### Stroke frequency per Age","metadata":{}},{"cell_type":"code","source":"df_age = df.groupby('age', as_index=False)['stroke'].sum()\nfig = px.histogram(df_age, \n                   x = \"age\",\n                   y = \"stroke\", \n                   barmode = \"group\", \n                   nbins = 10, \n                   opacity = 0.75, \n                   range_x = [0,85],\n                   color_discrete_sequence=px.colors.qualitative.Light24)\n\nfig.update_layout(height = 500, \n                  width = 700, \n                  title_text ='Stroke Frequency per Age',\n                  title_font_size= 20,\n                  title_y = 0.97,\n                  title_x = 0.48,\n                  yaxis_title = 'Stroke Frequency')\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation**:\n\nFrom the histogram above, the older a person is, the higher their chances of getting stroke.","metadata":{}},{"cell_type":"markdown","source":"### Stroke Frequency per Age and Gender combined","metadata":{}},{"cell_type":"code","source":"df_age_gender = df.groupby(['gender', 'age'], as_index=False)['stroke'].sum()\nfig = px.histogram(df_age_gender,\n                   x = 'age',\n                   y = 'stroke',\n                   color = 'gender',\n                   barmode = 'group',\n                   nbins = 10,\n                   opacity = 0.75,\n                   color_discrete_sequence = px.colors.qualitative.Prism)\n\nfig.update_layout(height = 500,\n                  width = 700,\n                  title_text = 'Stroke frequency per Age and Gender',\n                  title_font_size = 20,\n                  title_y = 0.95,\n                  title_x = 0.48,\n                  yaxis_title = 'Stroke Frequency')\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\nThe histogram above indicates that the older a person gets, the more likely they are to have stroke and if the gender is female, the older a female gets, the more likely she is to get stroke compared to a male except between the ages of 55-64 where men are more likely to have stroke in comparison to females.","metadata":{}},{"cell_type":"markdown","source":"### Stroke Frequency per Hypertension","metadata":{}},{"cell_type":"code","source":"# Compare stroke column with hypertension column\npd.crosstab(df.stroke, df.hypertension)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a new crosstab and base plot\npd.crosstab(df.hypertension, df.stroke).plot(kind=\"bar\", \n                                   figsize=(10,6), \n                                   color=[\"lightblue\", \"salmon\"])\n\n# Add attributes to the plot to make it more readable\nplt.title(\"Stroke Frequency Per Hypertension\")\nplt.xlabel(\"Hypertension\")\nplt.ylabel(\"Frequency\")\nplt.legend([\"Stroke\", \"No Stroke\"])\nplt.xticks(rotation = 0);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\nFrom the bar plot above, we can see that patients that do not have hypertension slightly have a higher chance of having stroke while there is a chance that those that have hypertension can get stroke also but the likelihood of having stroke is far lesser for people with hypertension than those without hypertension. ","metadata":{}},{"cell_type":"markdown","source":"### Stroke frequency per Smoking Status","metadata":{}},{"cell_type":"markdown","source":"We're going to compare the stroke class with the smoking status. We're trying to deduce if smoking or not smoking plays a role in a person having stroke.","metadata":{}},{"cell_type":"code","source":"# Compare the stroke column with the smoking status\npd.crosstab(df.smoking_status, df.stroke).plot(kind='bar',\n                                               figsize=(10, 6),\n                                               color=['lightblue', 'salmon'])\nplt.title('Stroke Frequency per Smoking Status')\nplt.xlabel('Smoking Status')\nplt.ylabel('Frequency')\nplt.xticks(rotation=0)\nplt.legend(['Stroke', 'No Stroke']);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\nFrom the bar chart above, People that have never smoked tend to be more susceptible to having stroke than those that are still smoking and those that formerly smoked.","metadata":{}},{"cell_type":"markdown","source":"### Stroke Frequency per Heart Disease","metadata":{}},{"cell_type":"code","source":"# Compare stroke column with heart disease column\npd.crosstab(df.stroke, df.heart_disease)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a new crosstab and base plot\npd.crosstab(df.heart_disease, df.stroke).plot(kind=\"bar\", \n                                   figsize=(10,6), \n                                   color=[\"lightblue\", \"salmon\"])\n\n# Add attributes to the plot to make it more readable\nplt.title(\"Stroke Frequency Per Heart Disease\")\nplt.xlabel(\"Heart Disease\")\nplt.ylabel(\"Frequency\")\nplt.legend([\"Stroke\", \"No Stroke\"])\nplt.xticks(rotation = 0);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\nAs the case above with hypertension, from the barchart above, people that do not have a heart disease tend to be more susceptible to having stroke than those with heart disease.","metadata":{}},{"cell_type":"markdown","source":"### Stroke Frequency per Body Mass Index","metadata":{}},{"cell_type":"code","source":"df_bmi = df.groupby('bmi_category', as_index=False)['stroke'].sum()\n\nfig = px.bar(df_bmi,\n             x = 'bmi_category',\n             y = 'stroke',\n             color = 'bmi_category',\n             opacity = 1,\n             color_discrete_sequence = px.colors.qualitative.D3)\n\nfig.update_layout(height = 700,\n                  width = 750,\n                  title_text = 'Stroke Frequency per BMI',\n                  title_font_size= 20,\n                  title_y = 0.97,\n                  title_x = 0.48,\n                  yaxis_title = 'Stroke Frequency')\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\nFrom the bar chart above, the likelihood of a person having stroke is high if the person is overweight or obese.","metadata":{}},{"cell_type":"markdown","source":"### Stroke Frequency per Work Type","metadata":{}},{"cell_type":"code","source":"# Compare the stroke column with the work type\npd.crosstab(df.work_type, df.stroke).plot(kind='bar',\n                                               figsize=(10, 6),\n                                               color=['lightblue', 'salmon'])\nplt.title('Stroke Frequency per Work Type')\nplt.xlabel('Work Type')\nplt.ylabel('Frequency')\nplt.xticks(rotation=0)\nplt.legend(['Stroke', 'No Stroke']);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\nFrom the bar chart above, the likelihood of a person having stroke is higher if the person works for a private company.","metadata":{}},{"cell_type":"markdown","source":"### Stroke Frequency per Residence Type","metadata":{}},{"cell_type":"code","source":"# Compare the stroke column with the residence type\npd.crosstab(df.Residence_type, df.stroke).plot(kind='bar',\n                                               figsize=(10, 6),\n                                               color=['lightblue', 'salmon'])\nplt.title('Stroke Frequency per Residence Type')\nplt.xlabel('Residence Type')\nplt.ylabel('Frequency')\nplt.xticks(rotation=0)\nplt.legend(['Stroke', 'No Stroke']);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\nFrom the bar chart above, the likelihood of a person having stroke is slightly higher if the person lives in an urban area. But the likelihood of having stroke is almost similar for people living in rural and urban areas.","metadata":{}},{"cell_type":"markdown","source":"### Stroke Frequency per Married or Not","metadata":{}},{"cell_type":"code","source":"# Compare the stroke column with the Marital Status\npd.crosstab(df.ever_married, df.stroke).plot(kind='bar',\n                                               figsize=(10, 6),\n                                               color=['lightblue', 'salmon'])\nplt.title('Stroke Frequency per Married or Not')\nplt.xlabel('Ever Married')\nplt.ylabel('Frequency')\nplt.xticks(rotation=0)\nplt.legend(['Stroke', 'No Stroke']);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\nThe probability of having stroke is higher for people that have gotten married at some point or still married than people that have never married. One factor could be that people that get married on average tend to be older people and the propensity to have stroke tend to be higher for older people.","metadata":{}},{"cell_type":"markdown","source":"### Check for Outliers","metadata":{}},{"cell_type":"code","source":"bmi=list(df['bmi'].values)\nhist_data=[bmi]\n\ngroup_lables=['bmi']\ncolour=['Red']\n\nfig=ff.create_distplot(hist_data,group_lables,show_hist=True,colors=colour)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation**:\n\nFrom the histogram above, the distribution of the histogram is a unimodal distribution, outliers are present on the right side of the histogram and due to the outliers, the histogram plot is right skewed, we can either remove the outliers or the distribution curve can be made less-skewed by mapping the values with a log but both cases will lead to loss of the number of datapoints with Stroke = 1","metadata":{}},{"cell_type":"code","source":"bmi=list(df['bmi'].values)\nhist_data=[bmi]\n\ngroup_lables=['bmi']\ncolour=['Red']\n\nfig=ff.create_distplot(hist_data,group_lables,show_hist=True,colors=colour)\nfig.show()\n\ndf.drop(df[df['bmi'] > 47].index, inplace = True)\nprint(\"The shape after removing the BMI outliers : \",df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation**:\n\nAfter removing the outliers, the range is now between 15 and 45 which is where most of the bmi samples are distributed. ","metadata":{}},{"cell_type":"markdown","source":"### Modelling\n\nWe've explored the data, now we'll try to use machine learning to predict our target variable based on the independent variables.\n\nRemember our problem?\n\n    Given clinical parameters about a patient, can we predict whether or not a patient is likely to get stroke?\n\nThat's what we'll be trying to answer.","metadata":{}},{"cell_type":"markdown","source":"### Categorical  Encoding using LabelEncoder","metadata":{}},{"cell_type":"code","source":"# Check the first 5 rows\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the datatype\ndf.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace the no and yes with int 0, 1\ndf['hypertension'].replace(['No', 'Yes'], [0, 1], inplace=True)\ndf['heart_disease'].replace(['No', 'Yes'], [0, 1], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the first 5 rows\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Turning the categorical variables into numbers with Label Encoder\nfrom sklearn.preprocessing import LabelEncoder\n\n# List the categorical features\ncategorical_features = [\"gender\",\"ever_married\",\"work_type\",\"Residence_type\",\"smoking_status\"]\n\n# Instantiate the LabelEncoder\nlabel_encoder = LabelEncoder()\n\nfor col in categorical_features:\n    label_encoder.fit(df[col])\n    df[col] = label_encoder.transform(df[col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the first 5 rows\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After converting the categorical columns into numerical variables, drop the bmi_is_missing and bmi_category columns.","metadata":{}},{"cell_type":"code","source":"# Drop bmi_is_missing and bmi category column\ndf.drop(['bmi_is_missing', 'bmi_category'], axis=1, inplace=True)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Rows containing 0 =', len(df[df['stroke']==0]))\nprint('Rows containing 1 =', len(df[df['stroke']==1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the stroke value counts with a bar graph\nplt.figure(figsize=(6, 6))\nsns.countplot('stroke', data=df)\nplt.title('Imbalanced Stroke Class')\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What is Data Imbalance?\nData imbalance usually reflects an unequal distribution of classes within a dataset. As with the data set we're working with, The proportion of people with stroke and people without stroke is **19.52 : 1**. If we train our binary classification model without fixing this problem, the model will be completely biased towards the no stroke predition class. Since all of our data is numeric and there are no missing values and we have a highly **imbalanced** class, we'll attempt to balance the dataset by **OverSampling** the majority class.","metadata":{}},{"cell_type":"code","source":"# Split into X and y\nX = df.drop('stroke', axis=1)\ny = df['stroke']\n\n# Split into training and test dataset\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_scale=scaler.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size=0.3, stratify=y, shuffle=True, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape, y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### OverSampling\n\nWe're trying to predict our stroke variable using all of the other variables and to be able to do this, we will have to oversample the target column by increasing the minority class to match the majority class.","metadata":{}},{"cell_type":"code","source":"# Oversample and plot imbalanced dataset with SMOTE\nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE()\nX_oversampled, y_oversampled = sm.fit_resample(X, y)\n\n# Plot the dataframe after oversampling\nplt.figure(figsize=(6, 6))\nsns.countplot(x = y_oversampled, data = df)\nplt.title('Balanced Classes')\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train again with the new data\nX_train, X_test, y_train, y_test = train_test_split(X_oversampled, y_oversampled, test_size = 0.2, random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We're going to be using 3 models to evaluate the oversampled dataset:\n\n1. KNeighborsClassifier\n2. Logistic Regression\n3. RandomForestClassifier\n\nAll of the algorithms in the Scikit-Learn library use the same functions, for training a model, model.fit(X_train, y_train) and for scoring a model model.score(X_test, y_test). score() returns the ratio of correct predictions (1.0 = 100% correct).\n\nSince the algorithms we've chosen implement the same methods for fitting them to the data as well as evaluating them, let's put them in a dictionary and create a function which fits and scores them.","metadata":{}},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"code","source":"# Logistic Regression\nnp.random.seed(42)\n\n# Instantiate the model\nlog_reg=LogisticRegression()\n\n# Fit the model\nlog_reg.fit(X_train,y_train)\n\n# Make predictions on the model\ny_pred_log_reg=log_reg.predict(X_test)\n\nprint(classification_report(y_test,y_pred_log_reg));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusion Matrix of LogisticRegression Model","metadata":{}},{"cell_type":"code","source":"# Import Seaborn\nimport seaborn as sns\nsns.set(font_scale=1.5)\n\ndef plot_conf_mat(y_test, y_pred_log_reg):\n    \"\"\"\n    Plots a confusion matrix using Seaborn's heatmap().\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_pred_log_reg),\n                     annot=True, \n                     cbar=False)\n    plt.xlabel(\"true label\")\n    plt.ylabel(\"predicted label\")\n    \nplot_conf_mat(y_test, y_pred_log_reg)\nprint(confusion_matrix(y_test,y_pred_log_reg))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see the model gets confused (predicts the wrong label) relatively the same across both classes. In essence, there are 271 occasaions where the model predicted 0 when it should've been 1 (false negative) and 147 occasions where the model predicted 1 instead of 0 (false positive).","metadata":{}},{"cell_type":"markdown","source":"### ROC Curve and AUC Scores for the Logistic Regression model","metadata":{}},{"cell_type":"code","source":"# Import ROC curve function from metrics module\nfrom sklearn.metrics import plot_roc_curve\n\n# Plot ROC curve and calculate AUC metric\nplot_roc_curve(log_reg, X_test, y_test);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is great, the model does far better than guessing which would be a line going from the bottom left corner to the top right corner, AUC = 0.86. But a perfect model would achieve an AUC score of 1.0.","metadata":{}},{"cell_type":"markdown","source":"### KNeighborsClassifier","metadata":{}},{"cell_type":"code","source":"np.random.seed(42)\n\n# Instantiate the model\nknn = KNeighborsClassifier()\n\n# Fit the model\nknn.fit(X_train, y_train)\n\n# Make predictions on the model\nknn_pred = knn.predict(X_test)\n\nprint(classification_report(knn_pred, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusion Matrix of KNeighborsClassifier Model","metadata":{}},{"cell_type":"code","source":"# Import Seaborn\nimport seaborn as sns\nsns.set(font_scale=1.5)\n\ndef plot_conf_mat(y_test, knn_pred):\n    \"\"\"\n    Plots a confusion matrix using Seaborn's heatmap().\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, knn_pred),\n                     annot=True, \n                     cbar=False)\n    plt.xlabel(\"true label\")\n    plt.ylabel(\"predicted label\")\n    \nplot_conf_mat(y_test, knn_pred)\nprint(confusion_matrix(y_test, knn_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 272 occasaions where the model predicted 0 when it should've been 1 (false negative) and 142 occasions where the model predicted 1 instead of 0 (false positive).","metadata":{}},{"cell_type":"markdown","source":"### ROC Curve and AUC Scores for the KNeighborsClassifier Model","metadata":{}},{"cell_type":"code","source":"# Import ROC curve function from metrics module\nfrom sklearn.metrics import plot_roc_curve\n\n# Plot ROC curve and calculate AUC metric\nplot_roc_curve(knn, X_test, y_test);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is great, the model does far better than guessing which would be a line going from the bottom left corner to the top right corner, AUC = 0.86. But a perfect model would achieve an AUC score of 1.0.","metadata":{}},{"cell_type":"markdown","source":"### Random Forest","metadata":{}},{"cell_type":"code","source":"np.random.seed(42)\n\n# Instantiate the model\nrfc = RandomForestClassifier()\n\n# Fit the model\nrfc.fit(X_train, y_train)\n\n# Make predictions on the model\nrfc_pred = rfc.predict(X_test)\n\nprint(classification_report(rfc_pred, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusion Matrix of RandomForest Model","metadata":{}},{"cell_type":"code","source":"# Import Seaborn\nimport seaborn as sns\nsns.set(font_scale=1.5)\n\ndef plot_conf_mat(y_test, rfc_pred):\n    \"\"\"\n    Plots a confusion matrix using Seaborn's heatmap().\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, rfc_pred),\n                     annot=True, \n                     cbar=False)\n    plt.xlabel(\"true label\")\n    plt.ylabel(\"predicted label\")\n    \nplot_conf_mat(y_test, rfc_pred)\nprint(confusion_matrix(y_test, rfc_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 102 occasaions where the model predicted 0 when it should've been 1 (false negative) and 37 occasions where the model predicted 1 instead of 0 (false positive).","metadata":{}},{"cell_type":"markdown","source":"### ROC Curve and AUC Scores for the RandomForestClassifier Model","metadata":{}},{"cell_type":"code","source":"# Import ROC curve function from metrics module\nfrom sklearn.metrics import plot_roc_curve\n\n# Plot ROC curve and calculate AUC metric\nplot_roc_curve(rfc, X_test, y_test);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is great, the model does far better than guessing which would be a line going from the bottom left corner to the top right corner, AUC = 0.98. But a perfect model would achieve an AUC score of 1.0.","metadata":{}},{"cell_type":"markdown","source":"### Correlation between independent variables","metadata":{}},{"cell_type":"code","source":"# Find the correlation between our independent variables\ncorr_matrix = df.corr()\ncorr_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's make it look a little prettier\ncorr_matrix = df.corr()\nplt.figure(figsize=(15, 10))\nsns.heatmap(corr_matrix, \n            annot=True, \n            linewidths=0.5, \n            fmt= \".2f\", \n            cmap=\"YlGnBu\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n    \nIt looks like there is no correlation between the independent variables. The only correlation is between **age** and **ever_married** and that is because as people get older, they tend to get married and settle down. ","metadata":{}},{"cell_type":"markdown","source":"After instantiating the 3 models and plotting their AUC curve, Random Forest has the highest **accuracy 0.93 , f1 score 0.93 and AUC of 0.98**. ","metadata":{}},{"cell_type":"code","source":"import pickle\n\n# Save an existing model to a file\npickle.dump(rfc, open('gs_random_forest_model.pkl', 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}