{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Escola Piloto Virtual - PEQ/COPPE/UFRJ\n\n## Data Science e Machine Learning na Prática - Introdução e Aplicações na Indústria de Processos\n\nEste notebook é referente à Aula 4 do curso, que trata do problema de [clusterização](https://en.wikipedia.org/wiki/Cluster_analysis) utilizando o modelo [$k$-means](https://en.wikipedia.org/wiki/K-means_clustering). Serão dois estudos de caso:\n\n* [segmentação de mercado](https://pt.wikipedia.org/wiki/Segmenta%C3%A7%C3%A3o_de_mercado);\n* identificação automática de [modos de operação em um reator químico](http://www.learncheme.com/simulations/kinetics-reactor-design/multiple-steady-states-in-cstr-with-heat-exchange)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# comandos mágicos que não se comunicam com a linguagem Python e sim diretamente com o kernel do Jupyter\n# começam com %\n\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# importando os principais módulos que usaremos ao longo da aula\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno\nimport yellowbrick.cluster\n\nimport sklearn.cluster\nimport sklearn.datasets\nimport sklearn.metrics\nimport sklearn.preprocessing\n\nimport scipy.stats\nimport scipy.optimize\nimport scipy.integrate\n\nimport copy\nimport datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.__version__)\nprint(pd.__version__)\nprint(matplotlib.__version__)\nprint(sns.__version__)\nprint(yellowbrick.__version__)\nprint(sklearn.__version__)\nprint(scipy.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Videoaula\n\nEste notebook é explicado em detalhes ao longo da seguinte videoaula:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import YouTubeVideo\nYouTubeVideo(\"1XS8Sw8jeuI\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para a playlist do curso completo, clique [aqui](https://www.youtube.com/playlist?list=PLvr45Arc0UpzsRhzq3q4_KmZcm0utwvvB)."},{"metadata":{},"cell_type":"markdown","source":"# Clusterização\n\n* Na clusterização, o objetivo é agrupar observações similares em grupos chamados de *clusters*.\n\n* É um procedimento análogo à classificação, porém não-supervisionado. Não há um conjunto de dados `y` com os rótulos de cada cluster: os algoritmos devem agrupar as observações apenas utilizando a estrutura da matriz de dados `X`.\n\nVamos utilizar a função [make_blobs](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) do `scikit-learn` de modo a gerar um conjunto de dados bidimensional simples para ver a metodologia em ação:"},{"metadata":{"trusted":true},"cell_type":"code","source":"blob_centers = np.array([[ 0.2,  2.3],[-1.5 ,  2.3],\n                         [-2.8,  1.8],[-2.8,  2.8],[-2.8,  1.3]])\n\nblob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n\nX, y = sklearn.datasets.make_blobs(n_samples=2000, centers=blob_centers,\n                                   cluster_std=blob_std, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Foram fornecidos como parâmetros à função `make_blobs` o número de observações e os centros e desvios-padrão de cada cluster. O número de clusters gerado foi 5, de acordo com os formatos das arrays `blob_centers` e `blob_std`.\n\n* Apesar de a função retornar os rótulos verdadeiros `y`, eles não serão usados no treinamento dos modelos.\n\nVisualizando os dados `X`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(X[:,0],X[:,1], s=4, c='k');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Os seis grupos são facilmente discriminados visualmente. \n\nPara discriminá-los computacionalmente, utilizaremos o modelo [$k$-means](https://en.wikipedia.org/wiki/K-means_clustering), cujo funcionamento será detalhado mais adiante."},{"metadata":{"trusted":true},"cell_type":"code","source":"m = sklearn.cluster.KMeans(n_clusters = 5)\ny_pred = m.fit_predict(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Perceba que foi necessário fornecer a priori o número de clusters. Essa é uma característica do modelo $k$-means.\n\nPara verificar o desempenho do modelo, não é apropriado utilizar a função `accuracy_score`, como no problema de classificação, já que o rótulo de uma classe verdadeira no vetor `y` não necessariamente é o mesmo rótulo do cluster correspondente em `y_pred`.\n\nTrês métricas de clusterização muito utilizadas são:\n\n- **homogeneidade**: maior quanto mais cada cluster contém apenas amostras de uma única classe verdadeira.\n- **completude**: maior quanto mais amostras de cada classe verdadeira são atribuídas a um único cluster.\n- **medida V**: a média harmônica entre homogeneidade e completude.\n\nAs três métricas variam entre 0 e 1 e podem ser calculadas de uma só vez com a função [homogeneity_completeness_v_measure](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_completeness_v_measure.html):"},{"metadata":{"trusted":true},"cell_type":"code","source":"sklearn.metrics.homogeneity_completeness_v_measure(y, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Essas métricas são interessantes, mas na maioria dos problemas práticos não podemos aplicá-las, já que os rótulos verdadeiros `y` não estão disponíveis. Nos casos em que estão disponíveis, o mais apropriado em geral é utilizar modelos de classificação."},{"metadata":{},"cell_type":"markdown","source":"# O modelo $k$-means\n\nPara começarmos a entender o modelo, é interessante visualizar seu resultado em gráfico. A função `plot_decision_boundaries`, definida abaixo, aceita um modelo e um conjunto de dados e exibe as fronteiras de decisão de cada cluster:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# retiradas de github.com/ageron/handson-ml2/blob/master/09_unsupervised_learning.ipynb\n\ndef plot_data(X):\n    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=4)\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights > weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=30, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=50, linewidths=50,\n                color=cross_color, zorder=11, alpha=1)\n\ndef plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n                             show_xlabels=False, show_ylabels=False):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n                cmap=\"Pastel2\")\n    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n                linewidths=1, colors='k')\n    plot_data(X)\n    if show_centroids:\n        plot_centroids(clusterer.cluster_centers_)\n\n    if show_xlabels:\n        plt.xlabel(\"$x_1$\", fontsize=14)\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n    else:\n        plt.tick_params(labelleft=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_decision_boundaries(m, X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* A cada cluster $j$ é atribuído um ponto especial $\\mu_j$ chamado de *centróide*, correspondente à média dos pontos contidos no cluster. Na figura, os centróides estão marcados com `x`.\n\n* Gráficos em que o espaço é particionado em formatos geométricos disjuntos (sem sobreposições) são chamados de [tesselações](https://en.wikipedia.org/wiki/Tessellation). Em particular, o gráfico acima, em que cada partição $j$ é definida como o lugar geométrico dos pontos mais próximos a um certo ponto é chamado de [tesselação de Voronoi](https://en.wikipedia.org/wiki/Voronoi_diagram).\n\n* No modelo $k$-means, as regiões que separam os $k$ clusters são definidas como as partições de Voronoi dos respectivos centróides $\\mu_j$. \n\n* As fronteiras de decisão são lineares. \n\n* A maior fonte de erros parece estar na atribuição de observações de um cluster mais espalhado a um cluster mais compacto. De fato, como veremos, um dos pontos fracos do modelo $k$-means é não lidar bem com clusters de variâncias diferentes.\n\nPodemos expressar os dados `X` em um novo espaço, em que as coordenadas de cada observação são as distâncias a cada centróide. Essa transformacão, conhecida como [quantização vetorial](https://en.wikipedia.org/wiki/Vector_quantization), é efetuada com o método `transform` do modelo:"},{"metadata":{"trusted":true},"cell_type":"code","source":"T = m.transform(X)\nT.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No caso acima, ocorreu aumento de dimensionalidade (já que há mais centróides do que dimensão original do problema, 2). Mas no caso de conjuntos com muitas variáveis, a transformação poderia ser usada para fins de redução de dimensionalidade.\n\nAgora vamos apresentar o modelo formalmente e entender como funciona o cálculo dos centróides."},{"metadata":{},"cell_type":"markdown","source":"## $k$-means - Formalização\n\nEnunciaremos as hipóteses relativas ao modelo:\n\n* **Hipótese 1 (número de clusters)**: o número $k$ de clusters é conhecido a priori.\n\nSeja um conjunto de dados $\\mathbf{X} \\in \\mathbb{R}^{n\\times m}$, em que cada linha corresponde a uma observação e cada coluna corresponde a uma variável. Sendo válida a Hipótese 1, o modelo pode definir os $k$ clusters dividindo o espaço $\\mathbb{R}^{m}$ em $k$ regiões disjuntas associadas a cada um dos clusters. \n\n* **Hipótese 2 (caracterização dos clusters)**: um cluster $j$ pode ser caracterizado por meio de um parâmetro de centralidade.\n\nEm particular, o parâmetro utilizado no $k$-means é o *centróide*, a média $\\mu_j$ das observações contidas no cluster.\n\n* **Hipótese 3 (composição dos clusters)**: cada cluster é composto pelas observações que se encontram mais próximas de seu centróide do que dos demais centróides.\n\nDo ponto de vista geométrico, a hipótese 3 implica que os clusters devem ser [convexos](https://pt.wikipedia.org/wiki/Convexo) e [isotrópicos](https://en.wikipedia.org/wiki/Isotropy). Além do mais, como tudo o que importa para a atribuição de uma observação a um cluster é a distância para o centróide, o modelo pode não funcionar bem quando os clusters têm diferentes variâncias. Para uma demonstração prática do mal funcionamento do modelo no caso de violação das hipóteses, recomendo [esta página](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py).\n\nVálidas as hipóteses, é possível definir uma função objetivo para minimização, denominada *inércia*, a soma dos quadrados das distâncias de cada observação ao centróide de seu cluster:\n\n$$\n\\sum_{j=1}^k \\sum_{i=1}^{n_j}||x_i-\\mu_j||^2,\n$$\n\nem que $j = 1,...,k$ são os vários clusters e $i = 1, ..., {n_j}$ são as observações em um cluster $j$. As variáveis de decisão do espaço de otimização são os centróides $\\mu_j$. \n\nPodemos acessar a inércia de um modelo do `scikit-learn` por meio do atributo `inertia_`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"m.inertia_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Um algoritmo muito usado para efetuar essa minimização de maneira eficiente é o [algoritmo de Lloyd](https://en.wikipedia.org/wiki/Lloyd%27s_algorithm):\n\n1. **ESPECIFIQUE** um número de clusters $k$.\n2. **INICIALIZE** os centróides $\\mu_j$ utilizando $k$ observações aleatórias.\n3. **ATRIBUA** a cada observação o cluster de centróide mais próximo.\n4. **RECALCULE** os centróides $\\mu_j$ de acordo com os clusters recém-atribuídos.\n5. **VOLTE** ao passo 3 e **REPITA** até convergência.\n\nO algoritmo acima tem convergência garantida, apesar de facilmente resultar em mínimos locais que podem corresponder a soluções espúrias. Na prática, executa-se o algoritmo várias vezes com diferentes inicializações e escolhe-se a que resulta em menor inércia. No caso do `scikit-learn`, por default, o algoritmo é executado 10 vezes.\n\nPode-se mostrar que, geometricamente, a etapa 3 do algoritmo resulta nas partições de Voronoi anteriormente apresentadas em gráfico.\n\nA maneira como se efetua a inicialização aleatória dos centróides, no passo 2, é um importante fator para convergência. [ARTHUR e VASSILVITSKII (2007)](https://theory.stanford.edu/~sergei/papers/kMeansPP-soda.pdf), por exemplo, propuseram o [$k$-Means++](https://en.wikipedia.org/wiki/K-means%2B%2B), em que força-se os centróides iniciais a resultarem afastados uns dos outros, aumentando a probabilidade de convergência para a solução ótima. Essa é a inicialização utilizada por default no `scikit-learn`.\n\nHá diversas propostas para melhoria do algoritmo básico acima apresentado. [ELKAN (2003)](https://www.aaai.org/Papers/ICML/2003/ICML03-022.pdf) sugeriu uma metodologia em que muitos cálculos de distância são evitados, utilizando o conceito de [desigualdade triangular](https://pt.wikipedia.org/wiki/Desigualdade_triangular) e por meio do rastreamento de limites superiores e inferiores para distâncias entre observações e centróides. É o algoritmo utilizado por default no `scikit-learn`.\n\n[HUANG (1997)](https://link.springer.com/article/10.1023/A:1009769707641) propôs o $k$-modes, designado para lidar com variáveis categóricas, por meio da definição dos clusters com base no número de categorias coincidentes entre as observações. [HUANG (1997)](https://grid.cs.gsu.edu/~wkim/index_files/papers/kprototype.pdf), em outro paper do mesmo ano, propôs ainda o $k$-prototypes, em que se combina o tratamento de variáveis contínuas e categóricas. Ambas as implementações não estão disponíveis no `scikit-learn`, mas podem ser aplicadas por meio da biblioteca [kmodes](https://pypi.org/project/kmodes/).\n\nOutro avanço importante foi a proposta de [SCULLEY (2010)](https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf) para treino do $k$-means utilizando minilotes, de maneira parecida com o que é feito em redes neurais, o que acelera o algoritmo e possibilita o seu uso em conjuntos de dados que não caibam na memória. No `scikit-learn`, essa metodologia está disponível na classe [MiniBatchKMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html).\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Selecionando o número de clusters\n\nDuas técnicas serão apresentadas para decidir o número de clusters: os métodos [do cotovelo](https://en.wikipedia.org/wiki/Elbow_method_(clustering)) e da [silhueta](https://en.wikipedia.org/wiki/Silhouette_(clustering)).\n\n### Método do cotovelo\n\nNo método do cotovelo, a inércia é plotada em função do número de clusters, como abaixo:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_inercia_per_k (X, k_list, ax = None):\n\n    ax = ax or plt.gca()\n    \n    kmeans_per_k = [sklearn.cluster.KMeans(n_clusters=k, random_state=0).fit(X)\n                    for k in k_list]\n\n    inercias = [m.inertia_ for m in kmeans_per_k]\n\n    ax.plot(k_list, inercias,ls='-',marker='*')\n\n    ax.set_xlabel('$k$')\n    ax.set_ylabel('inércia')\n    \n    return kmeans_per_k","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans_per_k = plot_inercia_per_k(X, range(1,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nota-se que há um ponto de inflexão em $k=4$, a partir do qual não há uma diminuição substancial da inércia conforme se aumenta $k$. Portanto, o valor de $k$ sugerido pelo método seria $4$. Não é a melhor solução (sabemos que o ótimo é $k=5$), apesar de levar a um resultado razoável: "},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_decision_boundaries(kmeans_per_k[3], X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O que ocorre é a interpretação dos dois clusters do canto inferior direito, que são bem próximos, como um cluster só.\n\nObs: o método do cotovelo também é usado em outros casos, como para selecionar o número de componentes principais no PCA, por exemplo.\n\n### Método da silhueta\n\nO coeficiente de silhueta de uma observação $x$ é definido como:\n\n$$\ns = \\frac{b-a}{\\max(a,b)},\n$$\n\nsendo:\n\n* $a$ a distância média intra-cluster, ou seja, a média das distâncias de $x$ às observações do mesmo cluster;\n* $b$ a distância média para o cluster mais próximo, ou seja, a média das distâncias de $x$ às observações do cluster mais próximo.\n\n$s$ varia entre $-1$ e $1$:\n\n* Um valor perto de $1$ sugere que $x$ está afastado dos demais clusters e contido em um cluster de amostras similares.\n* Um valor perto de $0$ sugere que a amostra está próxima de uma fronteira entre clusters.\n* Um valor perto de $-1$ sugere que a amostra está no cluster errado.\n\nA função [silhouette_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) retorna uma métrica definida como a média das silhuetas de todas as observações. Abaixo, visualizamos um gráfico dessa métrica em função do número de clusters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_silhueta_per_k(X, k_list, ax = None):\n\n    ax = ax or plt.gca()\n    \n    kmeans_per_k = [sklearn.cluster.KMeans(n_clusters=k, random_state=0).fit(X)\n                    for k in k_list]\n\n    silhuetas = [sklearn.metrics.silhouette_score(X, m.labels_) for m in kmeans_per_k]\n\n    ax.plot(k_list, silhuetas,ls='-',marker='*')\n\n    ax.set_xlabel('$k$')\n    ax.set_ylabel('silhueta média')\n    \n    return kmeans_per_k","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_silhueta_per_k(X,range(2,10));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O gráfico acima sugere, como o método do cotovelo, a escolha $k=4$. Apesar disso, fica claro que o valor $k=5$ também é bom e consideravelmente melhor do que os subsequentes.\n\nUm gráfico muito mais informativo é o *diagrama de silhueta*. Para plotá-lo, utilizaremos a biblioteca [Yellowbrick](https://www.scikit-yb.org/), que contém rotinas para visualização de resultados de aprendizado de máquina. Em particular, utilizaremos o método [silhouette_visualizer](https://www.scikit-yb.org/en/latest/api/cluster/silhouette.html#yellowbrick.cluster.silhouette.silhouette_visualizer) do módulo [yellowbrick.cluster](https://www.scikit-yb.org/en/latest/api/cluster/), que aceita um modelo e uma matriz de dados e plota o diagrama de silhueta:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_silhueta(X, k_list):\n        \n    n_lines = int(np.ceil(len(k_list)/2))    \n    \n    fig, ax = plt.subplots(n_lines, 2, figsize = (16,4*n_lines))\n\n    for i in range(len(k_list)):\n\n        m = sklearn.cluster.KMeans(k_list[i])\n        yellowbrick.cluster.silhouette_visualizer(m, X, show=False, ax=ax.ravel()[i]);\n\n    fig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_silhueta(X, [2,3,4,5,6,7])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Nos gráficos acima, cada figura em forma de faca corresponde a um cluster e contém os coeficientes de silhueta de suas observações.\n\n* A altura de cada figura indica o tamanho (quantidade de observações) de cada cluster.\n\n* Quanto mais comprida for uma figura, mais os coeficientes de silhueta do cluster correspondente aproximam-se de $1$, o valor ótimo.\n\n* A análise dos gráficos indicaria que $k=4$ ou $5$ seriam escolhas razoáveis, já que em todos os clusters as figuras estendem-se além da linha vertical que representa a silhueta média. \n\n* Na prática, a escolha por $k=5$ poderia ser feita de modo a gerar clusters de tamanhos semelhantes.\n\nAgora vamos analisar uma aplicação prática de clusterização, a segmentação de mercado."},{"metadata":{},"cell_type":"markdown","source":"# Segmentação de mercado\n\n[Segmentação de mercado](https://pt.wikipedia.org/wiki/Segmenta%C3%A7%C3%A3o_de_mercado) é o processo de identificar, em um conjunto heterogêneo de consumidores, grupos com características e comportamentos semelhantes. É uma importante ferramenta para entender o mercado e direcionar ações específicas para cada tipo de cliente.\n\nExistem várias metodologias para efetuar a segmentação. Uma muito conhecida é a [RFM](https://en.wikipedia.org/wiki/RFM_(market_research)), que visa dividir grupos de acordo com os seguintes critérios:\n\n* **Recency**: quando a última compra foi efetuada;\n\n* **Frequency**: com que frequência compras são efetuadas;\n\n* **Monetary Value**: quanto o cliente costuma gastar.\n\nO [conjunto de dados](https://www.kaggle.com/carrie1/ecommerce-data) utilizado refere-se a transações efetuadas entre 01/12/2010 e 09/12/2011 por uma loja online de varejo do setor de presentes baseada no Reino Unido. \n\nA análise aqui efetuada foi baseada [neste curso](https://www.datacamp.com/courses/customer-segmentation-in-python), em particular [nestes](https://s3.amazonaws.com/assets.datacamp.com/production/course_10628/slides/chapter2.pdf), [nestes](https://s3.amazonaws.com/assets.datacamp.com/production/course_10628/slides/chapter3.pdf) e [nestes](https://s3.amazonaws.com/assets.datacamp.com/production/course_10628/slides/chapter4.pdf) slides. Para um estudo mais aprofundado aplicado ao mesmo conjunto de dados, recomendo [este paper](https://link.springer.com/article/10.1057/dbm.2012.17) ou [este notebook](https://www.kaggle.com/fabiendaniel/customer-segmentation).\n\n## Importando e analisando dados\n\nUtilizando a função `read_csv` para ler os dados:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw = pd.read_csv('/kaggle/input/ecommerce-data/data.csv',\n                     encoding=\"ISO-8859-1\", low_memory=False, \n                     parse_dates=[\"InvoiceDate\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizando o DataFrame:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Descrições das colunas, de acordo com o [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Online+Retail):\n\n* **InvoiceNo**: Número da fatura. Nominal, um número integral de 6 dígitos atribuído exclusivamente a cada transação. Se este código começar com a letra 'c', isso indica um cancelamento.\n* **StockCode**: código do produto (item). Nominal, um número integral de 5 dígitos atribuído exclusivamente a cada produto distinto.\n* **Description**: Nome do produto (item). Nominal.\n* **Quantity**: as quantidades de cada produto (item) por transação. Numérico.\n* **InvoiceDate**: Data e hora do inventário. Numérico, o dia e a hora em que cada transação foi gerada.\n* **UnitPrice**: preço unitário. Numérico, preço do produto por unidade em libras esterlinas.\n* **CustomerID**: Número do cliente. Nominal, um número integral de 5 dígitos atribuído exclusivamente a cada cliente.\n* **Contry**: nome do país. Nominal, o nome do país onde cada cliente reside.\n\nAnalisando os tipos das variáveis no DataFrame:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analisando o padrão de dados faltantes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"missingno.bar(df_raw)\nmissingno.matrix(df_raw);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Percebe-se que há muitas compras em que não se identificam os consumidores.\n\n## Pré-processamento dos dados\n\nPodemos retirar as observações em que os consumidores não são identificados:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmp = df_raw.dropna(axis = 0, subset = ['CustomerID'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"É útil calcular a soma total gasta em cada compra:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmp[\"TotalSum\"] = df_tmp[\"Quantity\"] * df_tmp[\"UnitPrice\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Na célula abaixo, os dados são transformados para o espaço RFM:  "},{"metadata":{"trusted":true},"cell_type":"code","source":"snapshot_date = max(df_tmp.InvoiceDate) + datetime.timedelta(days=1)\n\ndf_rfm_raw = df_tmp.groupby(['CustomerID']).agg({\n      'InvoiceDate': lambda x: (snapshot_date - x.max()).days,\n      'InvoiceNo': 'count',\n      'TotalSum': 'sum'})\n\ndf_rfm_raw.rename(columns = {'InvoiceDate': 'Recency',\n                            'InvoiceNo': 'Frequency',\n                            'TotalSum': 'MonetaryValue'}, inplace=True)\n\ndf_rfm_raw","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Poderíamos aplicar a clusterização neste momento, mas as distribuições das variáveis são muito [assimétricas](https://pt.wikipedia.org/wiki/Assimetria_(estat%C3%ADstica)), como mostrado nos gráficos abaixo:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_rfm_raw.skew())\n\nfig, ax = plt.subplots(1, 3, figsize=(15,3))\n\nfor i in range(3):\n    sns.distplot(df_rfm_raw.iloc[:,i], ax=ax[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Acima da figura estão impressos os valores de [skew](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.skew.html) de cada variável, uma medida de assimetria; quanto mais distantes de zero, mais assimétricas são as distribuições.\n\nPodemos aplicar [transformações de potência](https://en.wikipedia.org/wiki/Power_transform) para aumentar a simetria das distribuições, tornando-as mais \"gaussianas\". Para isso, utilizaremos a função [power_transform](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.power_transform.html) do módulo [sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing):"},{"metadata":{"trusted":true},"cell_type":"code","source":"arr_rfm = sklearn.preprocessing.power_transform(df_rfm_raw)\n\ndf_rfm = pd.DataFrame(arr_rfm,\n                      index = df_rfm_raw.index,\n                      columns = df_rfm_raw.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizando as distribuições resultantes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_rfm.skew())\n\nfig, ax = plt.subplots(1, 3, figsize=(15,3))\n\nfor i in range(3):\n    sns.distplot(df_rfm.iloc[:,i], ax=ax[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As duas primeiras distribuições parecem razoáveis, mas a última ainda tem um valor de skew alto. Podemos tentar outra transformação, aplicando a raiz cúbica:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rfm['MonetaryValue'] = np.cbrt(df_rfm_raw['MonetaryValue'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checando:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(5,3))\n\nsns.distplot(df_rfm.iloc[:,i],ax=ax)\nplt.title(f'skew: {df_rfm.skew()[-1]:.3}');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Parece bem melhor!\n\nComo última etapa de pré-processamento, normalizemos as variáveis para que todas tenham média $0$ e desvio-padrão $1$:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rfm = (df_rfm-df_rfm.mean())/df_rfm.std()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Essa normalização faz com que as variáveis se distribuam em torno do mesmo ponto e com o mesmo grau de espalhamento, o que pode ajudar bastante a aplicação do algoritmo.\n\n## Modelagem\n\nO primeiro passo é calcular as curvas de inércia e silhueta em função de $k$ de modo a tentar encontrar uma pista para um bom número de clusters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2,1, figsize = (8,6), sharex=True)\n\nkmeans_per_k = plot_inercia_per_k(df_rfm, range(1,11), ax=ax[0])\nplot_silhueta_per_k(df_rfm, range(2,11), ax=ax[1]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As curvas não são muito informativas. Em particular, na curva de inércia, fora o ponto $k=2$, as demais inflexões são muito sutis. Isso dificulta a escolha visual em que se baseia o método do cotovelo.\n\nChecando os diagramas de silhueta:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_silhueta(df_rfm, [2,3,4,5,6,7])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Os diagramas de silhueta são bem mais informativos. Todas as configurações resultam em clusters com boa quantidade de silhuetas acima da média (que não é muito alta, entretanto). A escolha de 4 clusters parece ser particularmente adequada por resultar em conjuntos de tamanhos semelhantes.\n\nÉ possível visualizar o espaço RFM em gráficos 3d. Abaixo são comparadas duas escolhas, $k=3$ e $k=4$:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, subplot_kw={'projection':'3d'}, figsize=(16,6))\n\nk = [2,3]\n\nfor i in range(len(k)):\n\n    ax[i].scatter(df_rfm.iloc[:,0], df_rfm.iloc[:,1], df_rfm.iloc[:,2],\n                  c=kmeans_per_k[k[i]].labels_, cmap='viridis')\n\n    ax[i].set_title(f'$k={k[i]+1}$')\n    \n    ax[i].set_xlabel('R')\n    ax[i].set_ylabel('F')\n    ax[i].set_zlabel('M')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fica claro dos gráficos anteriores que os clusters obtidos não refletem uma estrutura de agrupamento intrínseca dos dados. Isso não é necessariamente um problema: se o modelo conseguir dividir o espaço em clusters de clientes com características específicas, o resultado pode ser satisfatório.\n\nPara entender melhor as diferenças entre os clusters, é útil traçarmos um [snake plot](https://www.marketingprofs.com/tutorials/snakeplot.asp), em que as médias de recência, frequência e valor monetário de cada um dos clusters são interligados por segmentos de reta:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize=(15,5))\n\nk = [2,3]\n\nfor i in range(len(k)):\n    \n    df_tmp = copy.deepcopy(df_rfm)\n    df_tmp['ID'] = df_rfm.index\n    df_tmp['Cluster'] = kmeans_per_k[k[i]].labels_\n    df_tmp_melt = pd.melt(df_tmp.reset_index(),\n                          id_vars=['ID', 'Cluster'],\n                          value_vars=['Recency','Frequency','MonetaryValue'],\n                          var_name='Attribute',\n                          value_name='Value')\n\n    sns.lineplot('Attribute', 'Value', hue='Cluster', data=df_tmp_melt, \n                 ax=ax[i], palette = sns.color_palette(\"husl\", k[i]+1))\n    \n    ax[i].set_title(f'$k={k[i]+1}$')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Os sombreados definem intervalos de confiança de 95%.\n\nDo primeiro gráfico acima (em que $k=3$), percebe-se que os clusters representam três tipos distintos de clientes:\n\n* $0$: aparecem com média frequência, não gastam muito e compraram há não muito tempo.\n* $1$: aparecem com baixa frequência, gastam pouco e compraram há muito tempo. Em tese, é o cluster de clientes que agregam menos valor ao negócio.\n* $2$: aparecem com bastante frequência, gastam muito e compraram há pouco tempo. Em tese, é o cluster de clientes mais valiosos.\n\nA introdução de mais um cluster no modelo divide o cluster $0$ do gráfico da esquerda em dois, os clusters $0$ e $3$ no gráfico da direita. A escolha por efetuar ou não essa divisão adicional pode ser uma decisão estratégica da companhia, baseada nas ações que serão tomadas a partir da informação fornecida pelo modelo.\n\nAgora vamos iniciar nosso segundo estudo de caso, a identificação de modos de operação em um reator químico."},{"metadata":{},"cell_type":"markdown","source":"# CSTR\n\nO [reator tanque agitado contínuo](https://en.wikipedia.org/wiki/Continuous_stirred-tank_reactor) (CSTR, da sigla em inglês), esquematizado a seguir, é um dos modelos de reator mais utilizados na engenharia química.\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/be/Agitated_vessel.svg\" width=\"300\" height=\"300\"/>\n\nNa modelagem de reatores CSTR, uma hipótese muito utilizada é a da [mistura perfeita](https://en.wikipedia.org/wiki/Perfect_mixing), em que considera-se que cada elemento de fluido da entrada se mistura instantânea e perfeitamente ao conteúdo uniforme do reator. Nesse caso, o modelo é chamado de *CSTR ideal*. \n\nA partir da relação básica de balanço:\n\n$$\\verb! Acumulo = Entrada - Saida + Geracao!,$$\n\nas seguintes equações diferenciais ordinárias podem ser obtidas para um CSTR ideal em que ocorra a reação irreversível $A \\rightarrow B$:\n\n* **Balanço de massa global**:\n\n$$\\frac{dV}{dt} = F_i - F$$\n\n* **Balanço de massa por componente**:\n\n$$ \\frac{dVC_A}{dt} = F_i C_{Ai} - FC_A - VrC_A $$\n\n* **Balanço de energia**:\n\n$$ \\frac{dVT}{dt} = F_i T_i - FT - \\frac{\\Delta H}{\\rho c_p} V_r C_A - \\frac{UA}{\\rho c_p} (T-T_j) $$\n\n* **Balanço de energia na jaqueta de resfriamento**:\n\n$$ \\frac{dT_j}{dt} = \\frac{F_j (T_{ji}-T_j)}{V_j} + \\frac{UA}{\\rho_j c_{pj} V_j} (T-T_j) $$\n\nO modelo fica completo com a adição da [taxa de reação](https://en.wikipedia.org/wiki/Reaction_rate):\n\n* **Taxa de reação ([equação de Arrhenius](https://en.wikipedia.org/wiki/Arrhenius_equation))**:\n\n$$ r = r_0 e^{-E/RT}$$\n\nAlém do mais, podem ser adicionadas equações de [controle](https://en.wikipedia.org/wiki/Control_system) (no caso, do tipo [proporcional](https://en.wikipedia.org/wiki/Proportional_control)):\n\n* **Equações de controle**:\n\n$$ F = F_{set} +P_V (V_{set}-V)$$\n\n$$ F_j = F_{j, set} +P_T (T_{set}-T) $$\n\nSeguem as descrições das variáveis:\n\n* **Variáveis de estado**: \n    * $V$: volume de reação;\n    * $C_A$: concentração do reagente $A$ no reator;\n    * $T$: temperatura do reator;\n    * $T_j$: temperatura da jaqueta.\n* **Variáveis de entrada**: \n    * $F_i$: vazão de entrada do reator;\n    * $C_{Ai}$: concentração de $A$ na entrada do reator;\n    * $T_i$: temperatura da entrada do reator;\n    * $T_{ji}$: temperatura de entrada da jaqueta.\n* **Variáveis manipuladas**: \n    * $F$: vazão de saída do reator;\n    * $F_{j}$: vazão de água de resfriamento da jaqueta.\n* **Variáveis controladas**: $V$ e $T$.\n* **Parâmetros**: \n    * $r_0$: fator pré-exponencial da equação de Arrhenius;\n    * $E$: energia de ativação da equação de Arrhenius;\n    * $\\Delta H$: entalpia de reação;\n    * $\\rho$: densidade da mistura; \n    * $c_p$: capacidade calorífica da mistura; \n    * $U$: coeficiente global de transferência de calor;\n    * $A$: área de troca térmica;\n    * $V_j$: volume da jaqueta; \n    * $\\rho_j$: densidade do fluido de resfriamento;\n    * $c_{pj}$ : capacidade calorífica do fluido de resfriamento;\n    * $P_V$: ganho do controlador de vazão de saída;\n    * $P_T$: ganho do controlador de vazão de fluido de resfriamento.\n    \nO modelo acima é baseado no estudo de caso proposto por [FEITAL *et al.* (2013)](https://aiche.onlinelibrary.wiley.com/doi/abs/10.1002/aic.13953).\n   \n## Implementação\n    \nComo os resultados do modelo são totalmente determinísticos, é útil adicionar ruído de medição para tornar os dados mais realistas. Isso será feito pela função `create_df_with_noise`, definida abaixo, que aceita uma matriz de dados no argumento `array`, a fração da variabilidade total que vai corresponder à variabilidade do ruído no argumento `noise_frac` e o índice máximo para o cálculo dessa fração no argumento `max_index_for_noise`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_df_with_noise (array, noise_frac,\n                          max_index_for_noise, \n                          columns = None,\n                          start = '2020-01-01 00:00:00', \n                          freq = '2T'):\n    \n    df = pd.DataFrame(array, columns = columns)\n\n    sigma_noise = noise_frac*(np.amax(array[:max_index_for_noise,:], \n                                      axis=0)-\n                              np.amin(array[:max_index_for_noise,:], \n                                      axis=0))\n\n    for i in range(df.shape[1]):\n        df.iloc[:,i] = (df.iloc[:,i] + \n                        sigma_noise[i]*np.random.randn(df.shape[0]))\n        \n        \n    df.index = pd.date_range(start = start, \n                             periods = df.shape[0],\n                             freq=freq)\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O modelo CSTR está implementado na classe `CSTR`, definida a seguir."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class CSTR():\n    \n    def __init__ (self, Vj = 3.85, rho = 50.0, rhoj = 62.3, k0 = 7.08e10,\n                   E = 30000.0, R = 1.99, DH = -30000.0,\n                   Cp = 0.75, U = 150.0, A = 250.0, Cpj = 1.0,\n                   nk =  2880, t_sampling = 2, Pv =  -10.0, Pt = -4.,\n                   mu_u = {'Fi': 40,'Cai': 0.5, 'Ti': 530, 'Tji': 530}, \n                   sigma_u = {'Fi': 1.5, 'Cai': 2e-2, 'Ti': 1, 'Tji': 1},\n                   ws_u = 100, sp = {'Vs': 48, 'Fs': 40, 'Fjs': 49.9}, \n                   T_guesses = [500,600,700], ee0 = 2):\n        '''\n        Construtor: inicializa o objeto, deixando-o pronto para a execução \n                    com os métodos `run` ou `simulate`.\n                    \n        Para as unidades dos argumentos, consulte doi.org/10.1002/aic.13953\n        '''\n        \n        # parâmetros do modelo:\n        \n        # volumétricos\n        self.Vj, self.rho, self.rhoj = Vj, rho, rhoj \n\n        # de reação\n        self.k0, self.E, self.R, self.DH = k0, E, R, DH\n\n        # de energia\n        self.Cp, self.U, self.A, self.Cpj  = Cp, U, A, Cpj\n                        \n        # cálculo da constante de reação\n        self.k_fun = lambda k0, T: k0*np.exp(-self.E/(self.R*T))\n        \n        # parâmetros da simulação:\n        \n        # número de observações\n        self.nk = nk\n\n        # instantes de amostragem\n        self.t = np.arange(0,t_sampling*(nk)/60,t_sampling/60)\n        \n        # ganhos do controlador\n        self.Pv, self.Pt = Pv, Pt\n        \n        # variáveis:\n        \n        # variáveis de estado (x)\n        self.x_labels = ['V','Ca','T','Tj']\n        self.x_units  = ['ft$^3$','mol/ft$^3$','R','R']\n        \n        # variáveis de entrada (u)\n        self.u_labels = ['Fi','Cai','Ti','Tji']\n        self.u_units  = ['ft$^3$/h','mol/ft$^3$','R','R']\n        \n        # variáveis manipuladas (m)\n        self.m_labels = ['F','Fj']\n        self.m_units  = ['ft$^3$/h','ft$^3$/h']\n\n        # definindo variáveis u, x e m\n            \n        u_labels = ['Fi', 'Cai', 'Ti', 'Tji']\n        x_labels = ['V', 'Ca', 'T', 'Tj']\n        m_labels = ['F', 'Fj']\n\n        # gerando variáveis de entrada com perturbações de processo\n        \n        mu_u = [mu_u.get(k) for k in u_labels if k in mu_u]\n        sigma_u = [sigma_u.get(k) for k in u_labels if k in sigma_u]\n\n        self.set_input(mu_u, sigma_u, ws_u)\n\n        # definindo set-points\n        \n        [setattr(self, name, value) for name, value in sp.items()]\n        \n        # calculando estados estacionários\n        \n        self.calc_EE(T_guesses)\n        \n        # definindo set-point da temperatura\n\n        self.Ts = self.EE[ee0,1]\n        \n        # definindo condições iniciais\n        \n        V0   = self.Vs\n        Ca0  = self.EE[ee0,0]\n        T0   = self.EE[ee0,1]\n        Tj0  = self.EE[ee0,2]\n\n        F0  = self.Fs\n        Fj0 = self.Fjs\n\n        self.set_initial_conditions (V0, Ca0, T0, Tj0, F0, Fj0)\n        \n    #########################\n\n    def set_input (self, u_ref, sigma, WS):\n        '''\n        Gera variáveis de entrada com variabilidade que simula \n        perturbações de processo.\n        '''        \n        \n        from sklearn.preprocessing import scale\n        \n        self.u = np.zeros((self.nk, len(u_ref)))\n        self.mu_u = np.zeros((self.nk, len(u_ref)))\n                 \n        for i in range(len(u_ref)):\n\n            u = np.random.randn(self.nk+WS-1)\n            u = pd.Series(u).rolling(window=WS).mean().iloc[WS-1:].values\n            self.mu_u[:,i] = u_ref[i]\n            self.u[:,i] = self.mu_u[:,i]+sigma[i]*scale(u)\n            \n    #########################\n    \n    def set_set_points (self,Vs,Ts,Fs,Fjs):\n        '''\n        Atribuição dos set-points.\n        '''  \n        \n        self.Vs, self.Ts, = Vs, Ts\n        self.Fs, self.Fjs = Fs, Fjs\n\n    #########################\n    \n    def calc_EE (self, T_guesses): \n        '''\n        Cálculo dos estados estacionários.\n        '''  \n        \n        Vo = self.Vs\n        Fo = self.Fs\n        Fjo = self.Fjs\n        \n        Cao = self.mu_u[0,1]\n        To  = self.mu_u[0,2]\n        Tjo = self.mu_u[0,3]\n        \n        def F1(T):\n            k = self.k_fun(self.k0,T)\n            return -self.DH*k*Fo*Cao/(Fo/Vo + k)\n\n        def F2(T):\n            Tj  = (Fjo*Tjo/self.Vj + \n                   self.U*self.A*T/(self.rhoj*self.Vj*self.Cpj))/(Fjo/self.Vj+\\\n                                    self.U*self.A/(self.rhoj*self.Vj*self.Cpj))\n            return Fo*self.rho*self.Cp*(T - To) + self.U*self.A*(T - Tj)\n        \n        def F12(T):\n            k = self.k_fun(self.k0,T)\n            Tj  = (Fjo*Tjo/self.Vj + \n                   self.U*self.A*T/(self.rhoj*self.Vj*self.Cpj))/(Fjo/self.Vj+\\\n                                    self.U*self.A/(self.rhoj*self.Vj*self.Cpj))\n            return Fo*self.rho*self.Cp*(T - To) + self.U*self.A*(T - Tj) + \\\n                   self.DH*k*Fo*Cao/(Fo/Vo + k)\n        \n        def Ca(T):\n            k = self.k_fun(self.k0,T)\n            return Cao/(1 + Vo*k/Fo)\n\n        def Tjac(T):\n            tjac=(Fjo*Tjo/self.Vj + \n                  self.U*self.A*T/(self.rhoj*self.Vj*self.Cpj))/(Fjo/self.Vj+\\\n                                   self.U*self.A/(self.rhoj*self.Vj*self.Cpj))\n\n            return tjac\n\n        self.t_ee = []; self.f_ee = []; \n        self.g_ee = []; self.ca_ee = []; \n        self.tj_ee = []\n\n        for T in np.arange(min(T_guesses),max(T_guesses)):\n            self.t_ee.append(T)\n            self.f_ee.append(F1(T))\n            self.g_ee.append(F2(T))\n            self.ca_ee.append(Ca(T))\n            self.tj_ee.append(Tjac(T))\n\n        T1 = scipy.optimize.fsolve(F12,T_guesses[0])\n        Ca1 = Ca(T1);\n        Tj1 = Tjac(T1)\n        \n        T2 = scipy.optimize.fsolve(F12,T_guesses[1])\n        Ca2 = Ca(T2)\n        Tj2 = Tjac(T2)\n        \n        T3 = scipy.optimize.fsolve(F12,T_guesses[2])\n        Ca3 = Ca(T3)\n        Tj3 = Tjac(T3)\n\n        self.EE  = np.array([[Ca1[0], T1[0], Tj1[0]], \n                             [Ca2[0], T2[0], Tj2[0]], \n                             [Ca3[0], T3[0], Tj3[0]]])\n                \n    #########################\n     \n    def model (self,t,x,u,m):\n        '''\n        Sistema de equações diferenciais que constitui\n        o modelo dinâmico propriamente dito do processo.\n        '''  \n        \n        # variáveis de estado\n        V, Ca, T, Tj = x\n\n        # variáveis de entrada  \n        Fi, Cai, Ti, Tji  = u\n\n        # variáveis manipuladas\n        F, Fj  = m\n\n        k = self.k_fun(self.k0,T)\n\n        dx    = np.zeros(4)\n        dx[0] = Fi - F\n        dVC   = Fi*Cai - F*Ca - V*k*Ca\n        dx[1] = (dVC - Ca*dx[0])/V\n        dVT   = Fi*Ti - F*T + \\\n                V*k*Ca*(-self.DH)/(self.rho*self.Cp) - \\\n                self.U*self.A*(T-Tj)/(self.rho*self.Cp)\n        dx[2] = (dVT - T*dx[0])/V\n        dx[3] = (Tji-Tj)*Fj/self.Vj + \\\n                self.U*self.A*(T-Tj)/(self.Vj*self.rhoj*self.Cpj)\n\n        return dx  \n\n    #########################  \n    \n    def control (self,V,T):\n        '''\n        Atuação de controle proporcional.\n        '''  \n\n        # estruturas de controle\n        F  = self.Fs  + self.Pv*(self.Vs - V)\n        Fj = self.Fjs + self.Pt*(self.Ts - T) \n\n        # restrições\n        if Fj > 100: Fj = 100\n        if Fj <   0: Fj =   0\n        if F  > 100: F  = 100\n        if F  <   0: F  =   0\n\n        return np.array([F, Fj])\n    \n    #########################\n    \n    def set_initial_conditions (self, V0, Ca0, T0, Tj0, F0, Fj0):\n        '''\n        Atribuição de condições iniciais.\n        '''  \n        \n        self.x0 = np.array([V0, Ca0, T0, Tj0])\n        self.m0 = np.array([F0, Fj0])\n        \n    #########################\n    \n    def simulate (self, control = True, sp = None, devs = None):\n        '''\n        Integração das equações diferenciais que compõem o modelo.\n        ''' \n        \n        # matriz que armazenará as variáveis de estado\n        self.x = np.zeros((self.nk,self.x0.shape[0]))\n        \n        # matriz que armazenará as variáveis manipuladas\n        self.m = np.zeros((self.nk,self.m0.shape[0]))\n\n        # definindo estado inicial\n        self.x[0,:] = self.x0\n        self.m[0,:] = self.m0\n        \n        # matriz que armazenará os valores nominais de todas as variáveis\n        self.nom = np.zeros((self.nk, \n                             self.x.shape[1]+self.u.shape[1]+self.m.shape[1]))\n        self.nom[0,:] = np.hstack((self.x0, self.mu_u[0,:], self.m0))\n\n        # armazenando set-points                \n        if sp is not None:\n            self.sp = sp\n        else:\n            self.sp = np.zeros(self.nk)+2\n\n        # armazenando valores normais das variáveis que terão desvio        \n        if devs is not None:\n            normais = {}\n            for key, value in devs.items():\n                normais[key] = getattr(self, key)\n                \n        # loop de integração\n                    \n        for j in range(1,self.nk):\n            \n            # introduzindo desvios nas variáveis pertinentes            \n            if devs is not None:\n                for key, value in devs.items():\n                    setattr(self, key, normais[key]*(1+value[j]))\n\n            # integrando!            \n            res = scipy.integrate.solve_ivp(lambda t,x:self.model(t,\n                                                                  x,\n                                                                  self.u[j,:],\n                                                                  self.m[j-1,:]\n                                                                  ),\n                                            [self.t[j-1],self.t[j]],\n                                            self.x[j-1,:],\n                                            rtol=1e-12,\n                                            atol=1e-12) \n            self.x[j,:] = res.y[:,-1]\n            \n            if control:\n            \n                # controlando!\n                self.m[j,:] = self.control(*self.x[j,[0,2]])\n                \n                # setando o set-point de Ts para definir o EE\n                self.Ts = self.EE[int(self.sp[j]),1] \n                self.nom[j,:] =  np.array([self.x0[0]] +\n                                          self.EE[int(self.sp[j]),:].tolist() +\n                                          self.mu_u[j,:].tolist() + \n                                          [self.Fs]+[self.Fjs])\n                    \n            else:\n                \n                self.m[j,:] = self.m[j-1,:]\n\n    #########################\n    \n    def run (self, ee_sp = None, \n             meas_noise_frac = 0.1, max_index_noise_frac = -1, \n             faulty = False, faulty_variable = 'k0',\n             devs_fault = None, index_fault = None):\n        '''\n        Integração do modelo, chamando o método `simulate` e retornando\n        um DataFrame com os dados de processo.\n        ''' \n        \n        if faulty:\n            if devs_fault is None:\n                devs_fault = np.zeros(self.nk)\n                j_f0 = int(self.nk/2)\n                self.index_fault = j_f0\n                j_fn = self.nk\n                for j in range(j_f0, j_fn):\n                    if j >= j_f0 and j<=j_fn:\n                        devs_fault[j] = (j_f0-j)/j_fn\n                    else:\n                        devs_fault[j] = (j_f0-j_fn)/j_fn \n            else:\n                self.index_fault = index_fault\n            self.simulate(sp= ee_sp, devs = {faulty_variable:devs_fault})\n        else:\n            self.simulate(sp = ee_sp)\n\n        array = np.hstack((self.x, self.u, self.m))\n\n        df = create_df_with_noise(array, meas_noise_frac, \n                                  max_index_noise_frac,\n                                  columns = (self.x_labels + \n                                             self.u_labels + \n                                             self.m_labels))\n\n        return df\n        \n    #########################        \n              \n    def plot (self, kind = 'x'):\n        '''\n        Plot das variáveis $x$, $u$ ou $m$.\n        ''' \n        \n        if kind == 'x':\n            y = self.x\n            y_labels = self.x_labels\n            y_units  = self.x_units\n            nom = self.nom[2:,:4]\n            title='State'\n        elif kind == 'u':\n            y = self.u\n            y_labels = self.u_labels\n            y_units  = self.u_units\n            title='Input'\n        elif kind == 'm':\n            y = self.m\n            y_labels = self.m_labels\n            y_units  = self.m_units\n            nom = self.nom[2:,8:]\n            title='Manipulated'\n            \n        fig, ax = plt.subplots(1, y.shape[1], figsize=(15,4))\n            \n        for i in range(y.shape[1]):\n            ax.ravel()[i].plot(self.t, y[:,i])\n            ax.ravel()[i].set_title(y_labels[i]+' ('+y_units[i]+')')\n            if kind == 'x' or kind == 'm':\n                ax.ravel()[i].plot(self.t[2:],\n                                   nom[:,i],':k')\n                \n        fig.suptitle(title+' Variables x time (h) ');\n        \n        fig.tight_layout()\n        \n    #########################    \n    \n    def plot_van_heerden (self, ax = None):\n        '''\n        Plot do diagrama de van Heerden.\n        ''' \n        \n        ax = ax or plt.gca()\n        ax.plot(self.t_ee,self.f_ee,label='Generated heat')\n        ax.plot(self.t_ee,self.g_ee,label='Removed heat')\n        ax.legend()\n        ax.set_xlabel('T (R)')\n        ax.set_ylabel('Q (BTU/h)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Integração do modelo\n\nInicializando um objeto `CSTR`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"c = CSTR()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizando o [diagrama de van Heerden](https://pubs.acs.org/doi/abs/10.1021/ie50522a030):"},{"metadata":{"trusted":true},"cell_type":"code","source":"c.plot_van_heerden()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A figura acima ilustra que há três estados estacionários no processo, já que são três os pontos em que o calor gerado é igual ao calor removido.\n\nNa classe `CSTR`, os set-points disponíveis para a malha de controle são os três estados estacionários (EE) apresentados acima (0, 1 ou 2). Devemos especificá-los por meio de uma array que contém os set-points de cada um dos instantes de amostragem, como definido abaixo:"},{"metadata":{"trusted":true},"cell_type":"code","source":"ee_sp = np.zeros(c.nk)+2\nee_sp[c.nk//3:2*c.nk//3] = 1\nee_sp[2*c.nk//3:] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O primeiro terço dos instantes foi atribuído ao EE 2, o segundo terço ao EE 1 e o último terço ao EE 0.\n\nRodando a simulação:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%time cstr_df = c.run(ee_sp = ee_sp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizando o `DataFrame` resultante:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cstr_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizando a evolução temporal das variáveis de estado, de entrada e manipuladas, respectivamente:"},{"metadata":{"trusted":true},"cell_type":"code","source":"c.plot('x')\nc.plot('u')\nc.plot('m')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A separação entre os estados estacionários fica evidente nos gráficos de $C_A$, $T$ e $T_j$.\n\n## Aplicação do $k$-means\n\nComo no exemplo anterior, começemos gerando os gráficos de inércia e silhueta:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2,1, figsize = (8,6), sharex=True)\n\nplot_inercia_per_k(cstr_df, range(1,11), ax=ax[0])\nplot_silhueta_per_k(cstr_df, range(2,11), ax=ax[1]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_silhueta(cstr_df, [2,3,4,5,6,7])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Em todos os gráficos, é evidente a maior adequação da escolha $k=3$.\n\nVisualizando o espaço $C_A,T,T_j$, tanto com os rótulos verdadeiros quanto com os rótulos atribuídos pelo $k$-means:"},{"metadata":{"trusted":true},"cell_type":"code","source":"m = sklearn.cluster.KMeans(n_clusters=3)\nm.fit(cstr_df)\n\nfig, ax = plt.subplots(1, 2, subplot_kw={'projection':'3d'}, figsize=(16,6))\n\ncolors = [ee_sp, m.labels_]\ntitles = ['Labels verdadeiros', 'Labels $k$-means']\n\nfor i in range(len(colors)):\n\n    ax[i].scatter(cstr_df.loc[:,'Ca'], cstr_df.loc[:,'T'], cstr_df.loc[:,'Tj'],\n                  c=colors[i], cmap='viridis')\n\n    ax[i].set_title(titles[i])\n    \n    ax[i].set_xlabel('Ca')\n    ax[i].set_ylabel('T')\n    ax[i].set_zlabel('Tj')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alguns pontos são enganosos e classificados incorretamente pelo $k$-means. Provavelmente correspondem ao transiente da troca de estados estacionários.\n\nComo temos os labels verdadeiros para esse caso - que são os set-points especificados na simulação - é possível calcular as métricas homogeneidade, completude e medida V:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sklearn.metrics.homogeneity_completeness_v_measure(m.labels_, ee_sp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As métricas confirmam o bom desempenho verificado com as visualizações anteriores.\n\nA metodologia apresentada pode ser utilizada, por exemplo, para gerar um cluster de dados para cada modo de operação no contexto do treinamento de modelos de detecção de falhas. O PCA é um caso de modelo que não conseguiria capturar a estrutura multimodal de dados contendo vários modos de operação. A estratégia poderia ser, portanto, gerar um modelo PCA para cada cluster e utilizar o $k$-means para determinar automaticamente, durante a operação, o estado estacionário em que o processo se encontra no momento. Você pode inclusive tentar implementar essa estratégia utilizando o modelo PCA da Aula 2 (para rodar uma simulação `CSTR` com desvios em variáveis de processo, especifique o argumento `faulty` do método `run` como `True`)."},{"metadata":{},"cell_type":"markdown","source":"**Mão na massa!**\n\n* Adicione o conjunto de dados [Mall Customer Segmentation Data](https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python) ao notebook. \n\n* Efetue uma exploração inicial do conjunto (uma boa biblioteca para isso é a [pandas-profiling](https://github.com/pandas-profiling/pandas-profiling), vale a pena usá-la).\n\n* Aplique o modelo $k$-means para obter clusters de clientes de diferentes características. Identifique que tipos de clientes os clusters conseguem discriminar. \n\nDica: nem todas as variáveis precisam ser utilizadas para que se atinja uma separação adequada e interpretável."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusão\n\nNesta aula aprendemos como resolver problemas de [clusterização](https://en.wikipedia.org/wiki/Cluster_analysis) utilizando o modelo [$k$-means](https://en.wikipedia.org/wiki/K-means_clustering), aplicando aos casos de [segmentação de mercado](https://pt.wikipedia.org/wiki/Segmenta%C3%A7%C3%A3o_de_mercado) e identificação de [modos de operação em um reator químico](http://www.learncheme.com/simulations/kinetics-reactor-design/multiple-steady-states-in-cstr-with-heat-exchange).\n\nÉ importante ressaltar que, apesar de a maior parte das aplicações de aprendizado de máquina ser de algoritmos supervisionados, a maioria dos dados disponíveis *não* possui rótulo. Daí a importância de dominar também metodologias não-supervisionadas, já que elas possibilitam a captura de padrões a partir da estrutura dos dados $X$ sem a necessidade da utilização de rótulos (conjuntos de saída $y$) que guiem o aprendizado.\n\nÉ isso. Até a próxima!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}