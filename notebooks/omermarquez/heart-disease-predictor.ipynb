{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id=\"top\"></a>\n\n# Prediction of Heart Disease in Patients using Machine Learning\n\nThe following notebooks is a collection of code, methodologies and several approaches in a attempt to develope a prediction algorithm, also refered through the notebook as *estimator* or *model*, using Machine Learning.\n\nThis is a personal project, created with educational purposes, doesn't not have any valid medical approval, and should only be of interest by it's coding/developemnt of algorithm.\n\n\nThe baseline development of the project will be divided in the following steps:\n\n1. [Problem Definition](#problem-definition)\n2. [Data](#data)\n3. [Evaluation of the Model](#evaluation)\n4. [Features](#features)\n4. [Tools (Libraries) Setup](#tools)\n4. [Exploratory Data Analysis](#eda) <br>\n    6.1 [Loading External Dataset](#data-loading)<br>\n    6.2 [Statistically Numerical & Graphical Exploration](#data-analysis)<br>\n5. [Modeling (Development of algorithm)](#modeling)<br>\n    7.1 [Baseline Models](#baseline-models)<br>\n    7.2 [Model Improvement](#model-improvement)<br>\n    7.3 [Model's Performance Measurement](#model-performance)<br>\n6. Experimentation\n\nThis first couple of sectons are mostly theoricals, since most of the Machine Learning development is done in the experimentation phase, we will first make an exploratory analysis, and futher enhancement of the baseline model will be creted.\n\nBy last, we the model fulfills the requeriments established in part 3. Evaluation of the Model, a one step model will be created, to encapsulate the experimentation done, and the model will be serialized (saved) for future use."},{"metadata":{},"cell_type":"markdown","source":"---\n<a id=\"problem-definition\"></a>\n## 1. Problem Definition\n\nQuote with the technical description\n\nTo know exactly what we will be doing, we first define the problem that we are tring to solve, how we will attempt to do so, and with what tools.\n\nDefinition:\n\n>  Based on clinical related parameters for a particular patient, like cholesterol level or insuline in the blood, to say a few, how aim to predict if the given patient is likely to have a heart disease, or not.\n\nMaking medical exmans and analysing them, doctors easily know if a given patient have some kind of heart disease, the patterns are obviuos, and most of the heart diseases have roughly the same impact in those medical examans.\n\nThe goal of this project is to create a model, that only needs the result of these examns, we summit them into a computer, and give back whether a patient have, or not, a heart disease. \n\nIn Statistical terms, this is a Binary Classification problem, since we want to predict if the patient is part of one category, or another (2 categories, expressed in 1, or 0, thus, the name *binary* in the classification)."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"data\"></a>\n## 2. Data\n\nFor the development of this project, as any Machine Learning project, we need Data, the more the better, because analysing and finding patterns inside that data is the basis of any prediction algorithm.\n\nFor our purposes, we will take a heart disease dataset, taken out of the [Cleveland UCI Machine Learning Repository](archive.ics.uci.edu/ml/datasets/heart+Disease), where we can find a raw version of the one used here. The .csv version that will be used is taken out of [this Kaggle Page](kaggle.com/tonift/heart-disease-uci).\n\nAn in deep explanation of the dataset (what all this data is about, and from where it comes from) can be found in those links (*hint*: starts with Kaggle).\n\n**Disclaimer:** The data it's opensource, finded in public repository where everybody can access for free, more information and projects about the same subject can be found online."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"evaluation\"></a>\n## 3. Evaluation\n\nBecuase of the nature of this project, we must ensure the project looks promising on it's proof of concept (starting point, early stage of the project where we check if it's whorth investing more time and money on it) stage, hence:\n\n> If we can get a 95% of accuracy at predicting whether a patient has or not a heart disease, we can label the project as a success and pursue a more accurate version."},{"metadata":{},"cell_type":"markdown","source":"## Features <a id=\"features\"></a>\n\n**Feature Dictionary**\n\nHere you can quickly find what the features are about, and what theit classification means.\n\n<span style=\"font-size: small\">Featues is the Machine Learning way to say, a *particular data value* that represents some *attribute* of the patient recorded, generalized to every patient on the dataset.</span>\n\n\n* Sex\n    * 1 = Male, 0 = Female\n\n\n* Fasting Blood Sugar > 120 dg/dl\n    * 1 = True, 0 = False\n\n\n* Exercise Induced Angina\n    * 1 = True, 0 = False\n    \n    \n* Chest Pain Type:\n   - Value 1: typical angina\n   - Value 2: atypical angina\n   - Value 3: non-anginal pain\n   - Value 4: asymptomatic\n<br>\n\n\n* Resting Blood Preasure\n    - (in mm Hg on admission to the hospital)\n<br>\n\n\n* ST depression\n    - ST depression induced by exercise relative to rest\n<br>\n\n\n* Slope\n    - The slope of the peak exercise ST segment\n    - Value 1: upsloping\n    - Value 2: flat\n    - Value 3: downsloping\n<br>\n\n\n* Number of Major Vessels\n    - (0-3) colored by flourosopy\n<br>\n\n\n* THAL\n    * 3 -> normal\n    - 6 -> fixed defect\n    - 7 -> reversable defect\n    \n*The cells belove in a dictionary of what the columns means (fullname), the text above is extra information about certain labels*"},{"metadata":{},"cell_type":"markdown","source":"## Tools Setup <a id=\"tools\"></a>\n\nPython will be used as the programming language where **all** the code will be written. Hence, we will use the following libraries:\n\nFor the Exploratory Data Analysis section of the project (EDA):\n\n* Numpy\n* Pandas\n\n\nAs the main plotting libraries:\n\n* Matplotlib\n* Seaborn\n\n\nWhere the whole machine learning functions and models are:\n* ScikitLearn\n\n\nDue to the Classification nature of the problem, and the official guide of Scikit Learn for choosing an estimator, only 3 types of classifier estimator where imported.\n\nFuthermore, we imported the basic metrics for evaluating the performance of a classification type estimator."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Configuration\n\n# EDA / Plotting\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n# Print the plots in the notebook\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Evaluation of models' performance function\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report, plot_roc_curve\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nplt.style.use(\"seaborn-talk\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis <a id=\"eda\"></a>\n\nThe main goal of this section is to become Subject Matter Expert of the dataset, i.e., become ourselves familiar with the dataset, what the featues means, how they affect the samples, the shape, form, data type and distribution of our data set, as the subgroups, tendencies, etc. This is a really important aspect of the project, since this data is the foundation of our model, if the data is poorly preprocessed, our model will fail.\n\nWe will primary rely on visual methods to explaing the data set, applying statistical concept of descriptive statistic, we will make conclusions about the given dataset.\n\n\nFuthermore, we will try to answer:\n\n* What are we trying to solve?\n* What kind of data do we have? and how the differ from one another? \n* What's missing in the dataset? and how can be deal with that?\n* Where, and how are the outliers? and how we will treat them?\n* What feature can we add, create or remove, to improve our dataset?\n\n### Loading the dataset <a id='data-loading'></a>"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"heart_disease_raw = pd.read_csv(\"../input/heart-disease-uci/heart.csv\")\nheart_disease_raw  # Show the first 5 and last 5 samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Our dataset contains {heart_disease_raw.shape[0]} samples.\\n\")\nprint(f\"Each sample contains {heart_disease_raw.shape[1]} features.\")\nprint('This also can be seen in the shape of the Matrix above.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# And there's no missing values! That's nice\nheart_disease_raw.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Human Friendly Version\n\nTo better understand the dataset, the labels and values will be changed, so that our plots are more meaningful and thus, can be undestood by anyone."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_labels = {   # Change the encoded version of the feature names\n    \"age\": \"Age\",\n    \"sex\": \"Sex\",\n    \"cp\": \"Chest Pain Type\", # Check Extra Info Section\n    \"trestbps\": \"Resting Blood Pressure (mm Hg)\",\n    \"chol\": \"Serum Cholestoral (mg/dl)\",\n    \"fbs\": \"FBS > 120 dg/dl\",\n    'restecg': \"Resting Electrocardiographic Results\", \n    'thalach': \"Maximum Heart Rate Achieved\",\n    'exang': \"Exercise Induced Angina\", \n    'oldpeak': \"ST Depression\", \n    'slope': \"Slope\", \n    'ca': \"Number of Major Vessels\", \n    'thal': \"THAL\", \n    'target': \"Target\"    \n}\n\nsex_class = {\n    1: \"Male\",\n    0: \"Female\"\n}\n\nexercise_angina = {\n    1: \"Yes\",\n    0: \"No\"\n}\n\nfbs_values = {\n    1: \"Yes\",\n    0: \"No\"\n}\n\ntarget_label = {\n    1: \"Positive\",\n    0: \"Negative\"\n}\n\n#Replacements\nheart_disease = heart_disease_raw.rename(columns = new_labels) # Return the copy\nheart_disease[\"Sex\"].replace(sex_class, inplace=True)\nheart_disease[\"Exercise Induced Angina\"].replace(exercise_angina, inplace=True)\nheart_disease[\"FBS > 120 dg/dl\"].replace(fbs_values, inplace=True)\nheart_disease[\"Target\"].replace(target_label, inplace=True)\nheart_disease.head()\n\n#heart_disease.to_csv('data/heart-disease-hf.csv')  # Save for future use\n# hf stands fot human friendly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_disease.info()    # Same Memory Space, new data type added","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Statistically Numerical Exploration <a id='data-analysis'></a>\n\nIn this section, a statistical analysis will be performed on the data set, to evaluate the numerical aspect of the data set, and get more insight about patterns, tendencies, and what to plot in the Graphical Section, to further evaluate the data.\n\nThe statistical analysis will be performed by Python, since the data is inside a DataFrame, a Panas object, Pandas allow us to perform mathematical ops using Numpy, both libraries imported in the configuration section.\n\n\nAs can be seen, most of the features are classified, either Yes or No, or in some range, like Chest Pain Type, that goes from 1 to 4. This ranged values can be analysed using Central Tendency Measurements.\n\nFuthermore, we can also peform this analysis on 3 features, Age, Resting Blood Pressure, and Serum Cholestoral, this values do are continous, just as the clasified one.\n\nEach feature will be analysed on its own, them, in the graphical section, we might see the patters among multiple features.\n\n---\n\n## Central Tendency Values\n\nTo keep this notebook DRY, we will create a function to plot the very basic statistical metrics that are always used to describe the distribution of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def basic_metrics(label, print_label):\n    \n    \"\"\"\n    Basic Function to print central tendency metrics from any label passed.\n    The heart_disease name for the data is assumed.\n    \n        @params\n            label: Name of the column to evaluate\n            print_label: Name of the column, formated to print.\n    \"\"\"\n    print(f\"Average {print_label} of the Patients: {heart_disease[label].mean():.2f}\")\n    print(f\"Most Frequently {print_label} of Patients: {heart_disease[label].median():.2f}\")\n    print(f\"Variance of the samples: {heart_disease[label].var():.2f}\")\n    print(f\"Standar Deviation of Samples from the Mean: {heart_disease[label].std():.2f}\")\n    print()\n    print(f\"Minimun {print_label}: {heart_disease[label].min()}\")\n    print(f\"Max {print_label}: {heart_disease[label].max()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"basic_metrics(\"Age\", \"Age\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The metrics above show us a really small data distribution, with only approx. 9 year of difference between samples, the sample presents a relatively normal distribution.\n\nFuthermore, the most common age is 55, which is the age when heart problems start to be a serious problem. Futher investigation can lead to patterns in the age and target feature.\n\n----"},{"metadata":{},"cell_type":"markdown","source":"### **Resting Blood Pressure** (measured in mm Hg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"basic_metrics(\"Resting Blood Pressure (mm Hg)\", \"Blood Pressure\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In contrast, the Resting Blood Pressure presents a more elevated Variance and Standar Variation, although the average and median are pretty close together. Extreme values (min/max) are far away from each others.\n\n---"},{"metadata":{},"cell_type":"markdown","source":"### **Serum Cholestoral** (measured in mg/dl)"},{"metadata":{"trusted":true},"cell_type":"code","source":"basic_metrics(\"Serum Cholestoral (mg/dl)\", \"Serum Cholestoral\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_disease['Target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before the real analysis, just by checking the distribution of the 'Target' variable. we can find that the dataset is balanced, and thus, the statistical analysis ahead must take this into consideration."},{"metadata":{},"cell_type":"markdown","source":"### Graphical Description\n\nThe graphs above are means to explain the distribution of our data inside the dataset, thus, every feature is plotted individually."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 6), sharey=True)\n\nfig.suptitle(\"Distribution of Data\", fontsize=18)\n\nheart_disease[\"Age\"].hist(ax=ax[0]);\nax[0].set_xlabel(\"Age\")\nax[0].set_ylabel(\"Frequency\")\n\nheart_disease[\"Resting Blood Pressure (mm Hg)\"].hist(ax=ax[1])\nax[1].set_xlabel(\"Blood Pressure (mm Hg)\")\n\nheart_disease[\"Serum Cholestoral (mg/dl)\"].hist(ax=ax[2]);\nax[2].set_xlabel(\"Serum Cholestoral (mg/dl)\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graphs above show us the distribution of the continous, i.e, the non-categorical features of the dataset, the categorical features are going to be ploted agains the target variable to get more insight.\n\nAge shows a normal distibution, most of the samples have an age of 54, with +-9 years of difference among patients. The most important insigth, is that this dataset won't be usefull for too young or too old patients, the near the patient age is 54, the better.\n\nThe Blood Preassure and Surm Cholesterol aren't a normal distribution, but the mayority of the samples are mostly concentrated in the mean, not the media.\n\nThus, the median do is a good metric for the average patient in the dataset.\n\nFuther analysis of this variablesa again others are needed to truly find a pattern."},{"metadata":{},"cell_type":"markdown","source":"## Classification Distribution <a id=\"distribution\"></a>\n\n\nThe distribution of the Features that are classified, will be displayed, using Bar Plots."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n\nfig.text(0.07, 0.5, \"Frequency\", rotation=\"vertical\", fontsize=16)\nfig.text(0, 0.9, \"Distribution of the Features in the Dataset\", fontsize=18)\nfig.subplots_adjust(hspace=0.5)\n\npd.value_counts(heart_disease[\"Sex\"]).plot(ax=ax[0,0], kind=\"pie\", autopct='%.2f', \n                                          fontsize='x-large');\nax[0, 0].set_xlabel(\"Sex\")\nax[0, 0].set_ylabel(\"\")\n\npd.value_counts(heart_disease[\"Chest Pain Type\"]).plot(ax=ax[0,1], kind=\"bar\", rot=0);\nax[0, 1].set_xlabel(\"Chest Pain Type\")\n\npd.value_counts(heart_disease[\"FBS > 120 dg/dl\"]).plot(ax=ax[0,2], kind=\"pie\",\n                                                      autopct='%.2f', fontsize='x-large');\nax[0, 2].set_xlabel(\"FBS > 120 dg/dl\")\nax[0, 2].set_ylabel(\"\")\n\npd.value_counts(heart_disease[\"Exercise Induced Angina\"]).plot(ax=ax[1,0], kind=\"bar\", rot=0);\nax[1, 0].set_xlabel(\"Exercise Induced Angina\")\n\npd.value_counts(heart_disease[\"THAL\"]).plot(ax=ax[1,1], kind=\"bar\", rot=0);\nax[1, 1].set_xlabel(\"THAL\")\n\npd.value_counts(heart_disease[\"Number of Major Vessels\"]).plot(ax=ax[1,2], kind=\"bar\", rot=0)\nax[1, 2].set_xlabel(\"Number of Major Vessels\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the conclusions are done considering that the max. samples are 303.\n\n* Males represents 70% of the total amount of patients, thus, for any given patient, there's a probability of 70% of been a male. While only 30% of the patients are Female. \n\n* 82% of the patients have a FBS over 120 mg/dl, from the [Mayo Clinic](https://www.mayoclinic.org/diseases-conditions/diabetes/diagnosis-treatment/drc-20371451#:~:text=Fasting%20blood%20sugar%20test.&text=A%20fasting%20blood%20sugar%20level,separate%20tests%2C%20you%20have%20diabetes.), 82% of the patients have some kind of diabetes. Since this is a balanced dataset, futher investigation of the correlation of this variable must be analysed.\n\n* As a general, we can seed that all the categorical variables have some kind of dominand category, nevertheless, the rest of categories still represents a important % and can containt some kind of pattern among them.\n\n\nBecause of this, futher investigation of the correlation among the variables must be done before considering any feature undesirable, or jumping to any conclusion."},{"metadata":{},"cell_type":"markdown","source":"### Relationship Among Featurest\n\nPanda's Crosstabs and Scatter Plots are the heart of this section.\n\nThe plots are aimed to show whether there's some kind of relationship among the features or not, and see if the model will be able to make a prediction out of it.\n\nFirst, we will plot each of the main categorical features vs the Target."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"target_column = heart_disease['Target']\n\nfig, ax = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n\nfig.text(0.07, 0.5, \"Frequency\", rotation=\"vertical\", fontsize='xx-large')\n\npd.crosstab(heart_disease[\"Sex\"], target_column).plot(ax=ax[0,0], kind=\"bar\", rot=0, legend=None);\nax[0, 0].set_xlabel(\"Sex\")\n\npd.crosstab(heart_disease[\"Chest Pain Type\"], target_column).plot(ax=ax[0,1], kind=\"bar\", rot=0, legend=None);\nax[0, 1].set_xlabel(\"Chest Pain Type\")\n\npd.crosstab(heart_disease[\"FBS > 120 dg/dl\"], target_column).plot(ax=ax[0,2], kind=\"bar\", rot=0, legend=None);\nax[0, 2].set_xlabel(\"FBS > 120 dg/dl\")\n\n\npd.crosstab(heart_disease[\"Exercise Induced Angina\"], target_column).plot(ax=ax[1,0], kind=\"bar\", rot=0, legend=None);\nax[1, 0].set_xlabel(\"Exercise Induced Angina\")\n\npd.crosstab(heart_disease[\"THAL\"], target_column).plot(ax=ax[1,1], kind=\"bar\", rot=0, legend=None);\nax[1, 1].set_xlabel(\"THAL\")\n\npd.crosstab(heart_disease[\"Number of Major Vessels\"], target_column).plot(ax=ax[1,2], kind=\"bar\", rot=0, legend=None)\nax[1, 2].set_xlabel(\"Number of Major Vessels\");\n\nhandles, label = ax[1, 2].get_legend_handles_labels()\nfig.legend(handles, label, loc='lower left', fontsize='xx-large');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*The cells belove are used to represent numerically the conclusions done from the graphs.*\n\n* A significant amount of Females have a heart disease, although they represent 30% of the data, more then 50% of them have a heart disease.\n\n\n* A significant amount of patients with a Chest Pain of class 0 (asymptomatic) don't have a heart disease, beside been a strong pattern, is a logic assumption.\n\n\n* Since most of the patients in the dataset have a FBS over 120, the bars are higher, nevertheless, the distribution of the Target is more or less evenly, taken the lead the positive cases.\n\n\n* Those who don't present a Exercise Induced Angina have a heart disease, 140 cases from 303, roughly 50% of the patients falls in this category, less frequenty but obvious, those how do have a Exercise Induced Angina, are negative to heart disease, the distintion is big among this 2 groups, and cleary represents a pattern.\n\n\n* Same case for those who have THAL = 2 and 0 class in the Mayor Vessel Number feature, almost 50% of the positive cases falls in this category, strong pattern here."},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_disease['Target'].value_counts()\npd.crosstab(heart_disease[\"Sex\"], target_column)\npd.crosstab(heart_disease[\"FBS > 120 dg/dl\"], target_column)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation Matrix\n\nThe Correlation Matrix will show the cofficient of correlation, how related and in what amount are each variable to one another, **including the Target feature.**\n\nThis matrix will futher support the graph above."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 8))\n\ncorrelation_map = heart_disease_raw.corr()\n\nmask = np.zeros_like(correlation_map)\nmask[np.triu_indices_from(mask)] = True\nmask[np.diag_indices_from(mask)] = False # Mask to hide to upper triangle\n\nax = sns.heatmap(correlation_map, annot=True, fmt='.2f', cmap='YlGnBu', mask=mask); ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scatter Plot of 2 vars + target classification\n\nthalach + exang\nthalach + chest pain\nsex + age"},{"metadata":{},"cell_type":"markdown","source":"*The Correlation matrix shows the correlation coefficient among all the variables.*\n\nThe last row of the plot show the correlation among each feature and the target. The most correlated ones are:\n\n* Chest Pain\n* Maximum Heart Rate Achieved\n\nThis means, logically, the greather the Chest Pain and Heart Rate, the greather the chance of having a heart disease.\n\nNegative Correlated are:\n\n* Exercise Induced Angina by exercise\n* ST depression induced by exercise\n\nThe lower, or the abscen of this feature, the more probability of having a heart disease.\n\nThis, and futher exploration of this correlation are shown below, and the relationship among feature vs target are shown in the graph bars above."},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = heart_disease['Target'] == 'Yes'\n\nfig, ax = plt.subplots(nrows=2, figsize=(12, 15))\nax[0].scatter(x=heart_disease_raw['thalach'], y=heart_disease_raw['age'],\n              c=heart_disease_raw['target'], cmap='Spectral', alpha=0.8, s=40)\n\n\nax[1].scatter(x=heart_disease_raw['trestbps'], y=heart_disease_raw['age'],\n             c=heart_disease_raw['target'], cmap='Spectral');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To better undestand the relationship (correlation) among all the features, the machine learning model is the easiest way.\n\n## Modeling<a id=\"modeling\"></a>\n\n#### Preparing Data for the Machine Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = heart_disease_raw.drop('target', axis=1), heart_disease_raw['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nX_train.head()\ny_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Following the Scikit Learn Cheatsheet, 3 models will be used, here, listed from simple to complex:\n\n1. Logistic Regression\n2. K-Nearest Neighbors\n3. RandomForestClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_n_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Train (fit) and Evaluate a set of machine learning models, passed inside a dict.\n    The Key of the Dict will be the name of the given machine learning model, and thus\n    referenced with that name in the output of this function.\n    \n    The model will be scored with the build in score function that comes inside the model.\n    i.e:\n    `model.score()`\n    \n    return:\n        A dictionary, with the score of each model passed. Model referenced using the name\n        passed in the dictionary.\n    \"\"\"\n    \n    np.random.seed(42)\n    \n    model_store = {}\n    \n    for name, model in models.items():\n        model.fit(X_train, y_train)  # Train each model\n        \n        model_store[name] = model.score(X_test, y_test)\n    \n    return model_store # Return the dictionary with the score of the models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = {'Logistic Regression': LogisticRegression(max_iter=1000), # Avoid Converge Warning\n          'KNN': KNeighborsClassifier(),\n          'Random Forest': RandomForestClassifier()}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Comparison"},{"metadata":{"trusted":true},"cell_type":"code","source":"models_score = pd.DataFrame(train_n_score(models, X_train, X_test, y_train, y_test),\n                           index=['Accuracy'])\n\nax = models_score.T.plot.bar(rot=0, xlabel='Models', ylabel='Accuracy (Max 1)',\n                           colormap='Dark2', yticks=[0, 0.2, 0.4, 0.6, 0.8, 1],\n                           title='Accuracy of the Models', legend=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The baseline models (without hyperparameter adjustment) Random forest and Logistic Regression peformed almost perfectly, in comparison with the K Nearest Neighbors, hence, **Logistic Regression and Random Forest Classifier** will pass to the second phase.\n\n## Model Improvement <a id='model-improvement'></a>\n\nRandomizeSearchCV will helps to tune the hyperparameters and guide our search of the best combination, as a plus, will use Cross Validation, so that we can check that the models are truly performing > 90%, and not just got a lucky split of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Baseline Hyperparameters\n\nlog_model_hyperparam = {\n    'C': np.logspace(-4, 4, 20),\n    'solver': ['liblinear']\n}\n\nrandom_forest_hyperparam = {\n    'n_estimators': np.arange(10, 1000, 50),\n    'max_depth': [None, 3, 5, 10],\n    'min_samples_split': np.arange(2, 20, 2),\n    'min_samples_leaf': np.arange(1, 20, 2)\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tunning LogisticRegression model\n\nnp.random.seed(42)\n\nrs_logistic_model = RandomizedSearchCV(LogisticRegression(), log_model_hyperparam,\n                                      n_iter=20,# Iterate 20 times over the combinations\n                                      verbose=True)\n\nrs_logistic_model.fit(X_train, y_train) \n# Train the model, with cross validation and hyperparam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the best combination of hyperparams\nprint('Best combination of params to the model:', rs_logistic_model.best_params_)\nprint(f'With the given params, the model scored: {rs_logistic_model.best_score_:.3f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tunnig the RandomForest model\n\nnp.random.seed(42)\n\nrs_randomforest_model = RandomizedSearchCV(RandomForestClassifier(), \n                                           random_forest_hyperparam,\n                                      n_iter=20,# Iterate 20 times over the combinations\n                                      verbose=True)\n\nrs_randomforest_model.fit(X_train, y_train) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the best combination of hyperparams\nprint('Best combination of params to the model:', rs_randomforest_model.best_params_)\nprint()\nprint(f'With the given params, the model scored: {rs_randomforest_model.best_score_:.3f}')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"models_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Logistic Regression Model still is the best, despite the small improvement, it's still better than the Random Forest Classifier, hence, we will keep improving our **Logistic Regressor**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exhaustive combination of parameters\n\n# Hyperparametrs v2\nlog_model_hyperparamv2 = {\n    'C': np.logspace(-4, 4, 30),\n    'solver': ['liblinear']\n}\n\ngrid_logistic_model = GridSearchCV(LogisticRegression(), \n                                  param_grid=log_model_hyperparamv2,\n                                  verbose=True)\n\ngrid_logistic_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the best combination of hyperparams\nprint('Best combination of params to the model:', grid_logistic_model.best_params_)\nprint(f'With the given params, the model scored: {grid_logistic_model.best_score_:.3f}')\n\n# Didn't got any improvement :(","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our **Mean Accuracy** didn't get any better, so we will check how is our model doing on other classification related metrics, to check it's overall performance, and thus more insight on how to improve it.\n\nMore specifically, we will check:\n\n1. ROC and AUC score\n2. Confusion Matrix\n3. Precision\n3. Recall\n3. F1-Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First make predictions to have what to compare\n\ny_predicts = grid_logistic_model.predict(X_test)\ny_predicts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(ax, y_test, y_preds):\n    \"\"\"\n    Plot a Confusion Matrix using Seaborn's heatmap and Scikit Learn's confusion_matrix\n    \"\"\"\n    sns.heatmap(confusion_matrix(y_test, y_preds), annot=True, cbar=False, ax=ax,\n               cmap='bone')\n    ax.set_title('Confusion Matrix')\n    ax.set_xlabel('True Values')\n    ax.set_ylabel('Predicted Values')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=2, figsize=(12, 5))\n\nfig.subplots_adjust(wspace=0.4)\n\nax[0].set_title('ROC Curve')\nplot_roc_curve(grid_logistic_model, X_test, y_test, ax=ax[0])\nplot_confusion_matrix(ax[1], y_test, y_predicts);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that our model performed pretty well in the ROC Curve metric, been 1 a perfect score, 0.88 is really good. This performance can also be seen in the Confusion Matrix, where 36 out of 39 where predicted correctly.\n\nIs crearly that the problem lies in the negative side, our model tends to predict that someone doesn't have a heart disease, when they actually are, and this is a bad scenario, 7 of 22 patients where wrongly predicted. \n\nHence, we need to aim for an improvement to correct this behaviour.\n\n### Classification Report and Accuracy using Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = pd.DataFrame(cross_val_score(grid_logistic_model, X, y, \n                                        verbose=False, scoring='accuracy')).T\naccuracy['Mean'] = np.mean(accuracy.T)\naccuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall = pd.DataFrame(cross_val_score(grid_logistic_model, X, y,\n                                      verbose=False, scoring='recall')).T\nrecall['Mean'] = np.mean(recall.T)\nrecall","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision = pd.DataFrame(cross_val_score(grid_logistic_model, X, y, \n                                        verbose=False, scoring='precision')).T\nprecision['Mean'] = np.mean(precision.T)\nprecision","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1 = pd.DataFrame(cross_val_score(grid_logistic_model, X, y, \n                                        verbose=False, scoring='f1')).T\nf1['Mean'] = np.mean(f1.T)\nf1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'Accuracy': accuracy['Mean'], 'F1': f1['Mean'], 'Recall': recall['Mean'],\n         'Precision': precision['Mean']}, index=[0]).T.plot.bar(rot=0, legend=False,\n                                                               title='Cross Validated Metrics',\n                                                               figsize=(12,7));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance\n\nWhich Features of our dataset contributed the most with the algorithm?. Knowing this is one of the most important things for compleatly analyse our dataset, and futher improve our model.\n\nEach model have some way of showing how each feature affects the final prediction, in the case of logistic regression, the model contains `coeff_` that show numerically in what amount each variable contribute.\n\nThis coefficients are highly related with the Correlation Matrix.\n\nThe fact that a feature have way too much weight in the model prediction can be a sign of why the model doesn't get the > 95% accuracy goal, a thus is a sign that we should change the dataset of improve the tunnig."},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_logistic_model.best_estimator_.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_coefficients = dict(zip(heart_disease_raw.columns, \n                                list(grid_logistic_model.best_estimator_.coef_[0]))) # The number of features\nfeature_coefficients","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(feature_coefficients, index=[0]).T.plot.bar(title='Feature Importance',\n                                                         legend=False);","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}