{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# TABLE OF CONTENTS\n\n* [1. Pre Chapter: Introduction of Assignments](#section-one)\n* [2. Setup](#section-two)\n    - [2.1 Load Libraries](#subsection-two-one)\n    - [2.2 Define Color Scheme](#subsection-two-two)\n    - [2.3 Import Data](#subsection-two-three)\n    - [2.4 Wrangle Data](#subsection-two-four)\n* [3. Pandemic Investigation](#section-three)\n    - [3.1 Singapore Pandemic Situation and Comparison to Similar Countries](#subsection-three-one)\n        - [3.1.1 Singapore General Situation](#subsection-three-one-one)\n        - [3.1.2 Situation in Country Similar to Singapore](#subsection-three-one-two)\n        - [3.1.3 Wheree Singapore is Different and Why](#subsection-three-three-three)\n    - [3.2 Are the Number Reported by Singapore Government Reliable](#subsection-three-two)\n        - [3.2.1 World Average Logic Apply to Singapore](#subsection-three-two-one)\n        - [3.2.2 Why Singapore is Different](#subsection-three-two-two)\n    - [3.3 Economy Impact to Singapore](#subsection-three-three)\n        - [3.3.1 STI Index Performance & Helicopter Money](#subsection-three-three-one)\n        - [3.3.2 SEA Group Stock Performance & FOMO Effect](#subsection-three-three-two)\n        - [3.3.3 Singapore Airline Stock Performance & Optimism of Recovery](#subsection-three-three-three)\n    - [3.4 Other Investigations](#subsection-three-four)\n        - [3.4.1 Matrix on Case vs All Assets and Stocks we Pick](#subsection-three-four-one)\n* [4. Conclusion](#section-four)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# 1. Pre Chapter: Introduction of Assignments\n\nIn this assignment, we will discuss:\n1. How does Singapore's pandemic curve look? How does it compare to a country similar to yours? What may explain the similarity or difference?\n2. Are the reported confirmed cases in Singapore and deaths reliable? Why?\n3. What are the economic and financial impacts of Covid19 on Singapore? What are the plausible reasons for the observed impacts?\n4. What do you want to find out more? What do you find?\n\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n# 2. Setup\n1. Settings -> Environment -> \"Always use latest environment\"\n2. Settings -> Internet -> \"On\"\n3. Track Changes Adding for Section 1 and 2 from Previous Problem Set -> Ctrl+F -> \"### PBS_\" -> + X:Change/Add: Why"},{"metadata":{},"cell_type":"markdown","source":"![](http://)<a id=\"subsection-two-one\"></a>\n## 2.1 Load Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# datetime operations\nfrom datetime import timedelta\n\n# for numerical analyiss\nimport numpy as np\n\n# math operations ### PBS_2: ADDING: add the package\nfrom numpy import inf\n\n# to store and process data in dataframe\nimport pandas as pd\n\n# basic visualization package\nimport matplotlib.pyplot as plt\n\n# advanced ploting\nimport seaborn as sns; sns.set()\n\n# interactive visualization\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\n# for offline ploting, must turn this plotly.offline on\nfrom plotly.offline import plot, iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# to interface with operating system\nimport os\n\n# for trendlines\nimport statsmodels\n\n# data manipulation\nfrom datetime import datetime as dt\nfrom scipy.stats.mstats import winsorize ### PBS_2: ADDING: add the package\n\nimport warnings ### PBS_2: ADDING: add the package\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-two-two\"></a>\n## 2.2 Define Color Scheme"},{"metadata":{"trusted":true},"cell_type":"code","source":"# color pallette\n# Hexademical code RRGGBB (True Black #000000, True White #ffffff)\ncnf, dth, rec, act = '#393e46', '#ff2e63', '#21bf73', '#fe9801' ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-two-three\"></a>\n## 2.3 Import Data\n\nCovid 19 data from https://www.kaggle.com/imdevskp/corona-virus-report  \n\nWorldometer snapshot data from https://www.kaggle.com/selfishgene/covid19-worldometer-snapshots-since-april-18\n\nNBER recession indicators: https://www.kaggle.com/stlouisfed/nber-based-recession-indicators-united-states\n\nEcofin File from computer (local zip source, located at Hanye's computer at C:\\Users\\user\\Desktop\\NUS\\NUS MFin Course\\Sem 1\\BMF 5324 Statistic and Analytics\\Lecture Notes\\L2), do not need to change anything here as the file is already uploaded\n\nSTI index File from computer (local csv source, located at Hanye's computer at C:\\Users\\user\\Desktop\\NUS\\NUS MFin Course\\Sem 1\\BMF 5324 Statistic and Analytics\\Lecture Notes\\L2)\n\nT Bill File from computer, not one year as data nota avaiable, so use 10 yrs (local csv source, located at Hanye's computer at C:\\Users\\user\\Desktop\\NUS\\NUS MFin Course\\Sem 1\\BMF 5324 Statistic and Analytics\\Lecture Notes\\L2)\n\nRobinhood Data from Data-Search-Robinhood-Download\n\nSIA and SEA Price Data (local zip source, located at Hanye's computer at C:\\Users\\user\\Desktop\\NUS\\NUS MFin Course\\Sem 1\\BMF 5324 Statistic and Analytics\\Lecture Notes\\L5), do not need to change anything here as the file is already uploaded"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1ST FILE IMPUT (for COVID 19 Situation)\n# list files, this has nothing to do with later codes but just display you the file inside the data\n# In total 6 files: country_wise_latest.csv, day_wise.csv, usa_country_wise, covid19_clean_complete, full_group and woldometer\n# First 5 files from John Hopkins Uni, and becasue we do SG only, we use file except usa_country_wise and woldometer file\n!ls ../input/corona-virus-report\n\n##############################################################################################################################################################\n\n# SECOND FILE INPUT: Ecofin file (for macroeconomy test)\n# Create an empty list\nfiles = []\n\n# Fill the list with the file names of the CSV files in the Kaggle folder\nfor dirname, _, filenames in os.walk('../input/econfin'): ### PBS_2: CHANGING: change file name from ecofinpbs1 to ecofin, easier to coordinate\n    for filename in filenames:\n        files.append(os.path.join(dirname, filename))\n\n# Sort the file names\nfiles = sorted(files)\n\n# Read the CSV files through list comprehension, which can be broken into three parts\n# 1. OUTPUT EXPRESSION [pd.read_csv(f, na_values=['.'])] --- Note: this turns character '.' values into missing value\n# 2. INPUT SEQUENCE [for f] \n# 3. CONDITION (OPTIONAL) [in files] \nseries = [pd.read_csv(f, na_values=['.']) for f in files] # fill in . as na\n\n# Define series name, which becomes the dictionary key\nseries_name = ['btc','cpi','gold','snp','high_yield_bond','inv_grade_bond','moderna',\\\n               'employment','tesla_robinhood','trea_20y_bond','trea_10y_yield','tesla','fed_bs','wti']\n\n# series name = dictionary key, series = dictionary value\nseries_dict = dict(zip(series_name, series))\n\n\n##############################################################################################################################################################\n\n# THIRD FILE INPUT: Ecofin file (for us recession test)\n# Read the dataset CSV to an object\nnber_recession_indicator_month = pd.read_csv('../input/nber-based-recession-indicators-united-states/USRECM.csv')\nnber_recession_indicator_day = pd.read_csv('../input/nber-based-recession-indicators-united-states/USRECD.csv')\n\n\n##############################################################################################################################################################\n\n# FORTH FILE INPUT: STIINDEX (for SG stock return)\n# Read the dataset CSV to an object\nSTI = pd.read_csv('../input/stiidx/STI.csv')\n\n##############################################################################################################################################################\n\n# FIFTH FILE INPUT: STIINDEX (for SG stock return)\n# Read the dataset CSV to an object\ntbill = pd.read_csv('../input/tbill10yr/TBill10Yr.csv')\n\n##############################################################################################################################################################\n### PBS_2:ADDING: import sea and sia data\n# SIXTH FILE INPUT: SIA FILE\nsgairline = pd.read_csv('../input/sgairline/SIA.csv')\n\n# SEVENTH FILE INPUT: SEA LIMITED FILE\nsealimited = pd.read_csv('../input/sgsealimited/SE.csv')\n\n# EIGTH FILE INPUT: ROBINHOOD FOR SEA LIMITED\nsealimited_volume = pd.read_csv('../input/robinhood-stock-popularity-history/tmp/popularity_export/SE.csv')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-two-four\"></a>\n## 2.4 Wrangle Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1st file: Country wise\n# This CSV provides you with general look over the country cases (death etc),not daily, for pie and region general chart\ncountry_wise = pd.read_csv('../input/corona-virus-report/country_wise_latest.csv')\n\n# Replace missing values '' with NAN and then 0\ncountry_wise = country_wise.replace('', np.nan).fillna(0)\ncountry_wise_sg_only = country_wise.loc[country_wise[\"Country/Region\"].isin(['Singapore'])]\n\n\n# Already check only one SG is there, to check simply just un# the following code\n# list_of_country_name = []\n# for i in range (0, 187):\n#     list_of_country_name.append(country_wise.iloc[i, 0])\n# print(list_of_country_name)\n\n# if wanted to see the datastructure of this country wise (control + /, can do all # or un#)\n# country_wise.info()\n\n\n### PBS_2: ADDING: This is not latest but clean complete file for PBS2 usage\nfull_table = pd.read_csv('../input/corona-virus-report/covid_19_clean_complete.csv')\n# Use the following to check full table info\n# full_table.info()\n# type(full_table)\n# full_table.shape\n# full_table.columns\n# full_table.dtypes\n# full_table.head(20)\n\n\n##########################################################################################################################\n# 2nd file: full_group\n# This file contains all information as accumulated, each country, everyday what is the case number and regions\n# Grouped by day, country\nfull_grouped = pd.read_csv('../input/corona-virus-report/full_grouped.csv')\n\n# if want to see datastructure of the full group file (control + /, can do all # or un#)\n# full_grouped.info()\n# full_grouped.head(10)\n\n# Checked, SG, Singpore etc wrong spelling does not appear in this file\n# The way to check it is bascially the same as above, quary a list and find if wrong name such as SG, Singpore etc exist\n# And double check in S start region (slice the list and check if no SG missing)\n\n# Convert Date from Dtype \"Object\" (or String) to Dtype \"Datetime\"\nfull_grouped['Date'] = pd.to_datetime(full_grouped['Date'])\n# full_grouped.info(): to check the info of the changed dataset\n\n# After check it, breakdown to SG only, create another dataframe called \"full_grouped_SG_Only\":\nfull_grouped_SG_Only = full_grouped.loc[full_grouped[\"Country/Region\"].isin(['Singapore'])]\n# Another way to do it: sg_covid = full_grouped[full_grouped['Country/Region']==\"Singapore\"]\n# full_grouped_SG_Only.head(10): to check the info of the changed dataset\n\n##########################################################################################################################\n# 3rd file: day_wise\n# This file only have information on day wise case growth not the country specific\nday_wise = pd.read_csv('../input/corona-virus-report/day_wise.csv')\nday_wise['Date'] = pd.to_datetime(day_wise['Date'])\n\n# if want to see datastructure of the day_wise file (control + /, can do all # or un#)\n# day_wise.info()\n# day_wise.head(10)\n\n##########################################################################################################################\n# 4th file: worldometer\n# This file only have information on day wise case growth not the country specific\nworldometer = pd.read_csv('../input/corona-virus-report/worldometer_data.csv')\n\n# Add some more statistics to worldmeter file\nworldometer['InfectionRate'] = worldometer['TotalCases']/worldometer['Population']\nworldometer['DeathRate'] = worldometer['TotalDeaths']/worldometer['TotalCases']\nworldometer['SeriousRate'] = worldometer['Serious,Critical']/worldometer['TotalCases']\nworldometer['TestRate'] = worldometer['TotalTests']/worldometer['Population']\n\n# if want to see datastructure of the day_wise file (control + /, can do all # or un#)\n# worldometer.info()\n# worldometer.head(10)\n\n\n### PBS_2:ADDING: worldometer data cleaning, upper part still useful as it helps to produce some chart below\n# Worldometer data\n# ================\n\nworldometer_data = worldometer\n\n# Replace missing values '' with NAN and then 0\n# What are the alternatives? Drop or impute. Do they make sense in this context?\nworldometer_data = worldometer_data.replace('', np.nan).fillna(0)\nworldometer_data['Case Positivity'] = round(worldometer_data['TotalCases']/worldometer_data['TotalTests'],2)\nworldometer_data['Case Fatality'] = round(worldometer_data['TotalDeaths']/worldometer_data['TotalCases'],2)\n\n# Case Positivity is infinity when there is zero TotalTests due to division by zero\nworldometer_data[worldometer_data[\"Case Positivity\"] == inf] = 0\n\n# Qcut is quantile cut. Here we specify three equally sized bins and label them low, medium, and high, respectively.\nworldometer_data ['Case Positivity Bin']= pd.qcut(worldometer_data['Case Positivity'], q=3, labels=[\"low\", \"medium\", \"high\"])\n\n# Population Structure\nworldometer_pop_struc = pd.read_csv('../input/covid19-worldometer-snapshots-since-april-18/population_structure_by_age_per_contry.csv')\n\n# Replace missing values with zeros\nworldometer_pop_struc = worldometer_pop_struc.fillna(0)\n#worldometer_pop_struc.info()\n\n# Merge worldometer_data with worldometer_pop_struc\n# Inner means keep only common key values in both datasets\nworldometer_data = worldometer_data.merge(worldometer_pop_struc,how='inner',left_on='Country/Region', right_on='Country')\n\n# Keep observations where column \"Country/Region\" is not 0\nworldometer_data = worldometer_data[worldometer_data[\"Country/Region\"] != 0]\n\n# Inspect worldometer_data's metadata\n# worldometer_data.info()\n# worldometer_data.tail(20)\n# worldometer_data[\"Case Positivity\"].describe()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5th file ecofin File\n\n# 1. S&P \nsnp = series_dict['snp']\nsnp['Date'] = pd.to_datetime(snp['Date'])\nsnp.rename(columns={'Adj Close':'snp'}, inplace=True)\nsnp['snp_return'] = snp['snp'].pct_change()\nsnp['snp_volatility_1m'] = (snp['snp_return'].rolling(20).std())*(20)**(1/2) # Annualize daily standard deviation\nsnp['snp_volatility_1y'] = (snp['snp_return'].rolling(252).std())*(252)**(1/2) # 252 trading days per year\nsnp = snp[['Date','snp','snp_return','snp_volatility_1m','snp_volatility_1y']]\n# Calculate 1-month forward cumulative returns\nsnp['one_month_forward_snp_return'] = snp['snp_return'][::-1].rolling(window=20, min_periods=1).sum()[::-1]\n\n\n# 2. Bitcoin\nbtc = series_dict['btc']\nbtc['Date'] = pd.to_datetime(btc['Date'])\nbtc.rename(columns={'Adj Close':'btc'}, inplace=True)\nbtc['btc_return'] = btc['btc'].pct_change()\nbtc['btc_volatility_1m'] = (btc['btc_return'].rolling(20).std())*(20)**(1/2) \nbtc['btc_volatility_1y'] = (btc['btc_return'].rolling(252).std())*(252)**(1/2) \nbtc = btc[['Date','btc','btc_return','btc_volatility_1m','btc_volatility_1y']]\nbtc['one_month_forward_btc_return'] = btc['btc_return'][::-1].rolling(window=20, min_periods=1).sum()[::-1]\n\n# 3. Gold\ngold = series_dict['gold']\ngold['Date'] = pd.to_datetime(gold['DATE'])\ngold.rename(columns={'GOLDPMGBD228NLBM':'gold'}, inplace=True)\ngold['gold_lag1'] = gold['gold'].shift(1)\ngold['gold_lag2'] = gold['gold'].shift(2)\ngold['gold'] = gold['gold'].fillna(gold['gold_lag1'])\ngold['gold'] = gold['gold'].fillna(gold['gold_lag2'])\ngold[\"gold\"] = gold[\"gold\"].astype('float64')\ngold['gold_return'] = gold['gold'].pct_change()\ngold['gold_volatility_1m'] = (gold['gold_return'].rolling(20).std())*(20)**(1/2) \ngold['gold_volatility_1y'] = (gold['gold_return'].rolling(252).std())*(252)**(1/2) \ngold = gold[['Date','gold','gold_return','gold_volatility_1m','gold_volatility_1y']]\ngold['one_month_forward_gold_return'] = gold['gold_return'][::-1].rolling(window=20, min_periods=1).sum()[::-1]\n\n# 4. High Yield Bond\nhigh_yield_bond = series_dict['high_yield_bond']\nhigh_yield_bond['Date'] = pd.to_datetime(high_yield_bond['Date'])\nhigh_yield_bond.rename(columns={'Adj Close':'high_yield_bond'}, inplace=True)\nhigh_yield_bond['high_yield_bond_return'] = high_yield_bond['high_yield_bond'].pct_change()\nhigh_yield_bond['high_yield_bond_volatility_1m'] = (high_yield_bond['high_yield_bond_return'].rolling(20).std())*(20)**(1/2)\nhigh_yield_bond['high_yield_bond_volatility_1y'] = (high_yield_bond['high_yield_bond_return'].rolling(252).std())*(252)**(1/2)\nhigh_yield_bond = high_yield_bond[['Date','high_yield_bond','high_yield_bond_return','high_yield_bond_volatility_1m','high_yield_bond_volatility_1y']]\nhigh_yield_bond['one_month_forward_high_yield_bond_return'] = high_yield_bond['high_yield_bond_return'][::-1].rolling(window=20, min_periods=1).sum()[::-1]\n\n# 5. Investment Grade Bond\ninv_grade_bond = series_dict['inv_grade_bond']\ninv_grade_bond['Date'] = pd.to_datetime(inv_grade_bond['Date'])\ninv_grade_bond.rename(columns={'Adj Close':'inv_grade_bond'}, inplace=True)\ninv_grade_bond['inv_grade_bond_return'] = inv_grade_bond['inv_grade_bond'].pct_change()\ninv_grade_bond['inv_grade_bond_volatility_1m'] = (inv_grade_bond['inv_grade_bond_return'].rolling(20).std())*(20)**(1/2)\ninv_grade_bond['inv_grade_bond_volatility_1y'] = (inv_grade_bond['inv_grade_bond_return'].rolling(252).std())*(252)**(1/2)\ninv_grade_bond = inv_grade_bond[['Date','inv_grade_bond','inv_grade_bond_return','inv_grade_bond_volatility_1m',\n                                 'inv_grade_bond_volatility_1y']]\ninv_grade_bond['one_month_forward_inv_grade_bond_return'] = inv_grade_bond['inv_grade_bond_return'][::-1].rolling(window=20, min_periods=1).sum()[::-1]\n\n# 6. Crude Oil WTI\nwti = series_dict['wti']\nwti['Date'] = pd.to_datetime(wti['DATE'])\nwti.rename(columns={'WTISPLC':'wti'}, inplace=True)\nwti['wti_return'] = wti['wti'].pct_change()\nwti['wti_volatility_1m'] = wti['wti_return'].rolling(20).std()*(20)**(1/2)\nwti['wti_volatility_1y'] = wti['wti_return'].rolling(252).std()*(252)**(1/2)\nwti = wti[['Date','wti','wti_return','wti_volatility_1m','wti_volatility_1y']]\nwti['one_month_forward_wti_return'] = wti['wti_return'][::-1].rolling(window=20, min_periods=1).sum()[::-1]\n\n#7. Inflation\ncpi = series_dict['cpi']\ncpi['Date'] = pd.to_datetime(cpi['DATE'])\ncpi.rename(columns={'CUUR0000SEHE':'cpi'}, inplace=True)\ncpi = cpi[['Date','cpi']]\n\n#8. Employment\nemployment = series_dict['employment']\nemployment['Date'] = pd.to_datetime(employment['DATE'])\nemployment.rename(columns={'PAYEMS_CHG':'employment'}, inplace=True)\nemployment = employment[['Date','employment']]\n\n#9. US Fed's Balance Sheet\nfed_bs = series_dict['fed_bs']\nfed_bs['Date'] = pd.to_datetime(fed_bs['DATE'])\nfed_bs.rename(columns={'WALCL':'fed_bs'}, inplace=True)\nfed_bs = fed_bs[['Date','fed_bs']]\n\n#10. STI Index\nSTI['Date'] = pd.to_datetime(STI['Date'])\nSTI.rename(columns={' Close':'sti'}, inplace=True) # remember there is a space before \"Close\"\nSTI['sti_return'] = STI['sti'].pct_change()\nSTI['sti_volatility_1m'] = (STI['sti_return'].rolling(20).std())*(20)**(1/2) # Annualize daily standard deviation\nSTI['sti_volatility_1y'] = (STI['sti_return'].rolling(252).std())*(252)**(1/2) # 252 trading days per year\nSTI = STI[['Date','sti','sti_return','sti_volatility_1m','sti_volatility_1y']]\n# Calculate 1-month forward cumulative returns\nSTI['one_month_forward_sti_return'] = STI['sti_return'][::-1].rolling(window=20, min_periods=1).sum()[::-1]\n\n# 11. Tbill\ntbill[\"help_drop_na\"] = tbill[\"Close\"]/2 # create np.nan for future dropping and cleanning\ntbill = tbill.dropna()\ntbill['discount_rate'] = tbill ['Close']/100 + 1\ntbill['1plus_discount_rate'] = tbill ['discount_rate']**5\ntbill['tbill_close_price'] = 100/tbill['1plus_discount_rate']\n# try to change date\ntbill['Date'] = pd.to_datetime(tbill['Date'])\ntbill.rename(columns={'tbill_close_price':'tbill'}, inplace=True)\n# do operation\ntbill['tbill_return'] = tbill['tbill'].pct_change()\ntbill['tbill_volatility_1m'] = (tbill['tbill_return'].rolling(20).std())*(20)**(1/2)\ntbill['tbill_volatility_1y'] = (tbill['tbill_return'].rolling(252).std())*(252)**(1/2)\ntbill = tbill[['Date','tbill','tbill_return','tbill_volatility_1m','tbill_volatility_1y']]\ntbill['one_month_forward_tbill_return'] = tbill['tbill_return'][::-1].rolling(window=20, min_periods=1).sum()[::-1]\n\n\n### PBS_2:ADDING: sea limited and sia limited data\n# 12. SIA: Singapore Airline Clean, becasue of short timeframe, I put merge later\nsgairline['Date'] = pd.to_datetime(sgairline['Date'])\nsgairline.rename(columns={'Adj Close':'sia'}, inplace=True)\nsgairline['sia_return'] = sgairline['sia'].pct_change()\nsgairline['sia_volatility_1m'] = (sgairline['sia_return'].rolling(20).std())*(20)**(1/2)\nsgairline['sia_volatility_1y'] = (sgairline['sia_return'].rolling(252).std())*(252)**(1/2)\nsgairline = sgairline[['Date','sia','sia_return','sia_volatility_1m', 'sia_volatility_1y']]\nsgairline['one_month_forward_sia_return'] = sgairline['sia_return'][::-1].rolling(window=20, min_periods=1).sum()[::-1]\n\n# 13. SEALIMITED: Sea clean, becasue of short timeframe, I put merge later\nsealimited['Date'] = pd.to_datetime(sealimited['Date'])\nsealimited.rename(columns={'Adj Close':'sea'}, inplace=True)\nsealimited['sea_return'] = sealimited['sea'].pct_change()\nsealimited['sea_volatility_1m'] = (sealimited['sea_return'].rolling(20).std())*(20)**(1/2)\nsealimited['sea_volatility_1y'] = (sealimited['sea_return'].rolling(252).std())*(252)**(1/2)\nsealimited = sealimited[['Date','sea','sea_return','sea_volatility_1m', 'sea_volatility_1y']]\nsealimited['one_month_forward_sea_return'] = sealimited['sea_return'][::-1].rolling(window=20, min_periods=1).sum()[::-1]\n\n# 14. Robinhood data on SEALIMITED: becasue of short timeframe, I put merge later\nrobinhood = sealimited_volume\nrobinhood['Date'] = pd.to_datetime(robinhood['timestamp'], format='%Y-%m-%d %H:%M:%S')\nrobinhood.set_index('Date', inplace=True)\nrobinhood_daily = robinhood[['users_holding']].dropna().resample('D').mean().reset_index()\nrobinhood_daily['users_holding_pct_change'] = robinhood_daily[\"users_holding\"].pct_change()\nsea_robinhood = pd.merge(robinhood_daily,sealimited,how='left',on='Date')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sixth File: for nber base recession indicator\n\n# Import datasets with Pandas method read_csv\nnber_recession_indicator_month = pd.read_csv('../input/nber-based-recession-indicators-united-states/USRECM.csv')\nnber_recession_indicator_day = pd.read_csv('../input/nber-based-recession-indicators-united-states/USRECD.csv')\n\n# Convert data types\nnber_recession_indicator_day[\"Date\"] = pd.to_datetime(nber_recession_indicator_day[\"date\"])\nnber_recession_indicator_day[\"recession\"] = nber_recession_indicator_day[\"value\"].astype('bool')\n# Note that in Bootcamp2, prof use this but the effect is nearly equal to the line above except I keep the col call recession\n# nber_recession_indicator_day[\"value\"] = nber_recession_indicator_day[\"value\"].astype('bool')\n# nber_recession_indicator_day.rename(columns={'value':'recession'}, inplace=True)\n\n# Subset data columns\nnber_recession_indicator_day = nber_recession_indicator_day[[\"Date\",\"recession\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge 2nd and 3rd file together\n# Merge datasets together\nasset_classes = [btc,cpi,gold,high_yield_bond,inv_grade_bond,employment,fed_bs,wti,tbill]\n\nbaseline = pd.merge(snp,nber_recession_indicator_day,how='left',left_on='Date', right_on=\"Date\")\nbaseline = pd.merge(STI,baseline,how='left',left_on='Date', right_on=\"Date\")\n\nfor asset_class in asset_classes:\n    baseline = pd.merge(baseline,asset_class,how='left',left_on='Date', right_on=\"Date\")\n\n# Backfilling missing values,  \nbaseline.loc[baseline.Date >= '2020-03-01', \"recession\"] = 1\nbaseline[\"recession\"] = baseline[\"recession\"].fillna(0).astype(bool)\n# baseline.info()\n#2020 covid19 period\n\n### PBS_2: ADDING: to have this baseline 2020 portion (a smaller dataframe)\nbaseline2020 = baseline[baseline['Date'] >= '2020-01-01']\nbaseline2020 = pd.merge(baseline2020,full_grouped_SG_Only, how='left', on='Date') # this already make new cases as SG new cases\nbaseline2020['New cases'] = baseline2020['New cases'].fillna(0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n# 3. Pandemic Investigation"},{"metadata":{},"cell_type":"markdown","source":"![](http://)<a id=\"subsection-three-one\"></a>\n## 3.1 Singapore Pandemic Situation and Comparison to Similar Countries"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-three-one-one\"></a>\n### 3.1.1 Singapore General Situation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# How does virus spread around the world\n\ndef plot_map(df, col, pal):\n    df = df[df[col]>0]\n    \n    # choropleth draws maps based on shape files of different countries\n    fig = px.choropleth(df, locations=\"Country/Region\", locationmode='country names', \n                  color=col, hover_name=\"Country/Region\", \n                  title=col, hover_data=[col], color_continuous_scale=pal)\n    # 1:50m resolution\n    fig.update_geos (resolution=50)\n    fig.show()\n\nplot_map(worldometer_data, 'TotalCases', 'matter') # Total number spread around the world\nplot_map(worldometer_data, 'Tot Cases/1M pop', 'matter') # Total number that over 1 million population","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot log(Confirmed Cases) for each Country/Region Over time\nfig = px.choropleth(full_grouped, locations=\"Country/Region\", \n                    color=np.log(full_grouped[\"Confirmed\"]),\n                    locationmode='country names', hover_name=\"Country/Region\", \n                    animation_frame=full_grouped[\"Date\"].dt.strftime('%Y-%m-%d'),\n                    title='Cases over time', color_continuous_scale=px.colors.sequential.matter)\nfig.update(layout_coloraxis_showscale=False)\nfig.update_geos (resolution=50)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# THIS IS FROM PROBLEM SET ONE, RELEVANT TO SG GENERAL SITUATION: GENERAL TREND\n# Generate the global trend of active, recover and death chart\n# Collapse Country, Date observations to Date observations and reindex\nactive_total_trend = full_grouped.groupby('Date')['Recovered', 'Deaths', 'Active'].sum().reset_index()\n\n# Melt the data by the value_vars, bascially keep the date and make status as one column, cases become another column\nactive_total_trend = active_total_trend.melt(id_vars=\"Date\", value_vars=['Recovered', 'Deaths', 'Active'],\n                 var_name='Case', value_name='Count')\n\n# Plot the general chart in the ways that as time goes by, what is the case situation\nfig = px.area(active_total_trend, x=\"Date\", y=\"Count\", color='Case', height=600, width=700,\n             title='Cases over time', color_discrete_sequence = [rec, dth, act])\nfig.update_layout(xaxis_rangeslider_visible=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# THIS IS FROM PROBLEM SET ONE, RELEVANT TO SG GENERAL SITUATION: SMALL BAR ON NUMBERS\n# Generate Current Screenshot of cases expansion in Singapore\n# Graph out the chart\ncountry_wise_sg_only = country_wise_sg_only.melt(value_vars=['Active', 'Deaths', 'Recovered'])\nfig = px.treemap(country_wise_sg_only, path=[\"variable\"], values=\"value\", height=225)\nfig.data[0].textinfo = 'label+text+value'\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# THIS IS FROM PROBLEM SET ONE, RELEVANT TO SG GENERAL SITUATION: A STACKING CHART CAN OBSERVE THE ACTIVE ETC CASE DECLINE\n# Generate the global trend of active, recover and death chart for Singapore\n# Collapse Country, Date observations to Date observations and reindex\n# Similar to world chart, can wtire a function to do this and just call, but notebook not scrip so make it clear, write it out\nactive_singapore_trend = full_grouped_SG_Only.groupby('Date')['Recovered', 'Deaths', 'Active'].sum().reset_index()\n\n# Melt the data by the value_vars, bascially keep the date and make status as one column, cases become another column\nactive_singapore_trend = active_singapore_trend.melt(id_vars=\"Date\", value_vars=['Deaths', 'Active', 'Recovered'],\n                 var_name='Case', value_name='Count')\n\n# Plot the general chart in the ways that as time goes by, what is the case situation\nfig = px.area(active_singapore_trend, x=\"Date\", y=\"Count\", color='Case', height=600, width=700,\n             title='Cases over time', color_discrete_sequence = [rec, dth, act])\nfig.update_layout(xaxis_rangeslider_visible=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# THIS IS FROM PROBLEM SET ONE, RELEVANT TO SG GENERAL SITUATION: TOTAL TEST RATE 10TH IN THE WORLD, VERY HIGH\n# Use world meter to see what are the test rate zone Singapore located\nsg_testrate = worldometer[['Country/Region','WHO Region','TotalCases','TestRate']].dropna().sort_values('TestRate',ascending=False)\n\nsg_testrate.reset_index(inplace=True)\nsg_testrate.drop(['index'], axis=1,inplace=True)\nprint(sg_testrate.loc[sg_testrate['Country/Region'] == 'Singapore'])\n\nfig = px.scatter(sg_testrate,x='Country/Region', y='TestRate',size='TotalCases',color='WHO Region',color_discrete_sequence = px.colors.qualitative.Dark2)\nfig.update_layout(title='Test Rate', xaxis_title=\"\", yaxis_title=\"TestRate\",xaxis_categoryorder = 'total ascending',\n                  uniformtext_minsize=8, uniformtext_mode='hide',xaxis_rangeslider_visible=True)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# THIS IS FROM PROBLEM SET ONE, RELEVANT TO SG GENERAL SITUATION: TOTAL INFECTION RATE 15TH IN THE WORLD VERY HIGH\n# Use world meter to see what are the infection rate zone Singapore located\n\nsg_infection_rate = worldometer[['Country/Region','WHO Region','TotalCases','InfectionRate']].dropna().sort_values('InfectionRate',ascending=False)\n\nsg_infection_rate.reset_index(inplace=True)\nsg_infection_rate.drop(['index'], axis=1,inplace=True)\nprint(sg_infection_rate.loc[sg_infection_rate['Country/Region'] == 'Singapore'])\n\nfig = px.scatter(sg_infection_rate,x='Country/Region', y='InfectionRate',size='TotalCases',color='WHO Region',color_discrete_sequence = px.colors.qualitative.Dark2)\nfig.update_layout(title='Infection Rate', xaxis_title=\"\", yaxis_title=\"InfectionRate\",xaxis_categoryorder = 'total ascending',\n                  uniformtext_minsize=8, uniformtext_mode='hide',xaxis_rangeslider_visible=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# THIS IS FROM PROBLEM SET ONE, RELEVANT TO SG GENERAL SITUATION: TOTAL SERIOUS RATE NO DATA\n# Use worldometer to see the serious case rate for Singapore\nsg_seriousrate = worldometer[['Country/Region','WHO Region','TotalCases','SeriousRate']].dropna().sort_values('SeriousRate',ascending=False)\n\nsg_seriousrate.reset_index(inplace=True)\nsg_seriousrate.drop(['index'], axis=1,inplace=True)\nprint(sg_seriousrate.loc[sg_seriousrate['Country/Region'] == 'Singapore'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# THIS IS FROM PROBLEM SET ONE, RELEVANT TO SG GENERAL SITUATION: TOTAL DEATH RATE 170 IN THE WORLD, VERY LOW\n# Use world meter to see what are the death rate zone Singapore located\n\nsg_death_rate = worldometer[['Country/Region','WHO Region','TotalCases','DeathRate']].dropna().sort_values('DeathRate',ascending=False)\n\nsg_death_rate.reset_index(inplace=True)\nsg_death_rate.drop(['index'], axis=1,inplace=True)\nprint(sg_death_rate.loc[sg_death_rate['Country/Region'] == 'Singapore'])\n\nfig = px.scatter(sg_death_rate,x='Country/Region', y='DeathRate',size='TotalCases',color='WHO Region',color_discrete_sequence = px.colors.qualitative.Dark2)\nfig.update_layout(title='Death Rate', xaxis_title=\"\", yaxis_title=\"DeathRate\",xaxis_categoryorder = 'total ascending',\n                  uniformtext_minsize=8, uniformtext_mode='hide',xaxis_rangeslider_visible=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-three-one-two\"></a>\n### 3.1.2 Situation in Country Similar to Singapore"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Then we try to compare the similar country development curve (most similar countries)\n# And also, create a curve that as 100% incremental developing (some have higher and lower case, hard to compare but 100% can)\n\ndef gt_n(minimal,maximum,days_from_n_case):\n    # Identify countries with confirmed cases greater than days_from_n_case\n    # Then among these countries choose the unique set of countries\n    countries = full_grouped[full_grouped['Confirmed']>minimal]['Country/Region'].unique()\n    countries_maxlimit = full_grouped[full_grouped['Confirmed']>=maximum]['Country/Region'].unique()\n    \n    # Filter countries that are in the unique set of countries with confirmed cases greater than minimal\n    temp = full_table[full_table['Country/Region'].isin(countries)]\n    temp = temp[~ temp['Country/Region'].isin(countries_maxlimit)] # not in the region, so we filter out those very big number countries such as us\n    similar_country_sample_defined = temp\n    \n    # Aggregate (i.e., sum up) confirmed cases by Country/Region and Date\n    # Reset the index (it is no longer in running order)\n    temp = temp.groupby(['Country/Region', 'Date'])['Confirmed'].sum().reset_index()\n    \n    # Filter observations starting from the day n case is recorded\n    temp = temp[temp['Confirmed']>days_from_n_case]\n    # print(temp.head())\n\n    # Filter observations with confirmed cases more than minimal\n    temp = temp[temp['Confirmed']<maximum]\n    # print(temp.head())\n    \n    # Identify the start date when confirmed cases exceed minimal for each country\n    min_date = temp.groupby('Country/Region')['Date'].min().reset_index()\n    \n    # Name the columns in the dataframe min_date\n    min_date.columns = ['Country/Region', 'Min Date']\n    # print(min_date.head())\n\n    # Merge dataframe temp with dataframe min_date by 'Country/Region'\n    from_nth_case = pd.merge(temp, min_date, on='Country/Region')\n    \n    # Convert data type to datetime object\n    from_nth_case['Date'] = pd.to_datetime(from_nth_case['Date'])\n    from_nth_case['Min Date'] = pd.to_datetime(from_nth_case['Min Date'])\n    \n    # Create a variable that counts the number of days relative to the day when confirmed cases exceed N\n    from_nth_case['N days'] = (from_nth_case['Date'] - from_nth_case['Min Date']).dt.days\n    # print(from_nth_case.head())\n\n    # Plot a line graph from dataframe from_nth_case with column 'N days' and 'Confirmed' mapped to x-axis and y-axis, respectively.\n    # Distinguish each country by color (system-determined color)\n    # str converts n integer into string and \"'minimal days from '+ str(n) +' case'\" is the title \n    fig = px.line(from_nth_case, x='N days', y='Confirmed', color='Country/Region', \n                  title='N days from '+ str(days_from_n_case) +' case', height=600)\n    fig.show()\n    \n    return similar_country_sample_defined\n\n\n# Singapore has case = 50000, so we create this first function\n# So we compare countries with 10-100k case\nsimilar_country_sample_defined = gt_n(40000,60000,1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Draw histogram with two arguments\n# 1. variable of interest\n# 2. the number of bins\ndef plot_histogram_wm(col, bins):\n    fig = px.histogram(worldometer_data[col], x=col, nbins=bins)\n    fig.show()\n\n# Draw the histogram for case fatality rate (50 bins)\nplot_histogram_wm(\"Case Fatality\",50)\n\n# Plot those countries in same as SG, what is the bins they fall into\n# similar_country_sample_defined.info()\nsimilar_country_list = similar_country_sample_defined['Country/Region'].unique()\ndef plot_histogram_sg(list_similar_countries, col, bins):\n    temp = worldometer_data[worldometer_data['Country/Region'].isin(list_similar_countries)]\n    fig = px.histogram(temp[col], x=col, nbins=bins)\n    fig.show()\n    return temp\n\nLast_Chart = plot_histogram_sg(similar_country_list, \"Case Fatality\",50)\nLast_Chart[['Country/Region','Case Fatality']].sort_values('Case Fatality',ascending=False)\n# shows countries similar to Singapore has the same but higher fatality rate than Singapore, not print out, table shows in better format if not print out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-three-one-three\"></a>\n### 3.1.3 Where Singapore is Different and Why"},{"metadata":{},"cell_type":"markdown","source":"Singapore has the lowest death rate in similar countries, the reasons are:\n- Hospital/Quarantine place are abundant\n- Resources used to rescue those infected are in placed, which leads to the low death number\n- Quick response once a case is discovered"},{"metadata":{},"cell_type":"markdown","source":"![](http://)<a id=\"subsection-three-two\"></a>\n## 3.2 Are the Number Reported by Singapore Government Reliable"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-three-two-one\"></a>\n### 3.2.1 World Average Logic Apply to Singapore"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Draw a joint plot to diagnose the relationship between Tests/1M population vs Total Cases/1M population\n# Draw a regression line on the scatter plot\nsns.jointplot(x = 'Tests/1M pop', y = 'Tot Cases/1M pop', data = worldometer_data, kind='reg')\n\n# Show the descriptive statistics for case positivity bin (categorical variable)\nworldometer_data.groupby('Case Positivity Bin')['Case Positivity Bin'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Draw a Violin plot to diagnose the relationship betwen case positivity and case fatality rates\nfig = go.Figure()\n\n# Create a list of case positivity bin categories\nbins = ['low', 'medium', 'high']\n\n# Loop through each case positivity bin\nfor bin in bins:\n    \n    # worldometer_data['Case Positivity Bin'][worldometer_data['Case Positivity Bin'] == bin] means take the column 'Case Positivity Bin' and\n    # filter the column, such that Case Positivity Bin equals 'low', 'medium', or 'high'\n    fig.add_trace(go.Violin(x=worldometer_data['Case Positivity Bin'][worldometer_data['Case Positivity Bin'] == bin],\n                            y=worldometer_data['Case Fatality'][worldometer_data['Case Positivity Bin'] == bin],\n                            name=bin,\n                            box_visible=True,\n                            meanline_visible=True))\n    \nfig.update_layout(title='Case Fatality by Case Positivity Bins', \n                  yaxis_title=\"Case Fatality\", xaxis_title=\"Case Positivity Bins\", \n                  uniformtext_minsize=8, uniformtext_mode='hide')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Draw a joint plot to diagnose the relationship between fraction of population aged 65+ and case fatality rate\nsns.jointplot(x = 'Fraction age 65+ years', y = \"Case Fatality\", data = worldometer_data, kind='reg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the summary statistics of column case positivity\nworldometer_data[\"Case Positivity\"].describe()\n\n# Filter countries with Case Positivity less than 1% (i.e., 1 confirmed case out of 100 tests)\n# These are countries that go for rigorous testing regime\nbenchmark_countries = worldometer_data[worldometer_data[\"Case Positivity\"]<=0.01]\n#benchmark_countries.info()\n#benchmark_countries.head(20)\n\n\n# Assume that the number of confirmed cases are close to the true infections rates for countries with gold standard testing regimes \n# Thus, their case fatality rates are closer to the true infection fatality rates\ninfection_fatality_rate = benchmark_countries['TotalDeaths'].sum() / benchmark_countries['TotalCases'].sum()\n\n# Calculate the fraction of total Covid19 deaths for the population aged 65+ among the benchmark countries\nbenchmark_death_65y_pct = sum(benchmark_countries['TotalDeaths'] * benchmark_countries['Fraction age 65+ years']) / sum(benchmark_countries['TotalDeaths'])\n\nprint(infection_fatality_rate)\nprint(benchmark_death_65y_pct)\n\nprint('Estimated Infection Fatality Rate for a benchmark country with %.1f%s of population older than 65 years old \\\nis %.2f%s' %(100 * benchmark_death_65y_pct,'%',100 * infection_fatality_rate,'%'))\n\n\n# Estimate Infection Fatality Ratio using the estimated fraction of total Covid19 deaths for the population aged 65+\nworldometer_data['Estimated Infection Fatality Ratio'] \\\n    = ((worldometer_data['TotalDeaths'] * worldometer_data['Fraction age 65+ years']\n        /worldometer_data['TotalDeaths']) / benchmark_death_65y_pct) * infection_fatality_rate\n\n# Show descriptive statistics of the columns Estimated Infection Fatality Ratio and Case Fatality\nworldometer_data['Estimated Infection Fatality Ratio'].describe()\nworldometer_data['Case Fatality'].describe()\n\n# Plot histogram of Estimated Infection Fatality Ratio and Case Fatality\npx.histogram(worldometer_data, x='Estimated Infection Fatality Ratio', barmode=\"overlay\")\npx.histogram(worldometer_data, x='Case Fatality', barmode=\"overlay\")\n\n# Overlay both histograms for comparison\nfig = go.Figure()\n\nfig.add_trace(go.Histogram(x=worldometer_data['Estimated Infection Fatality Ratio'], \n    name = 'Estimated Infection Fatality Rate'\n))\n\nfig.add_trace(go.Histogram(x=worldometer_data['Case Fatality'], \n    name = 'Case Fatality Rate'\n))\n\nfig.update_layout(barmode='overlay', \n    title = 'Estimated Infection Fatality Rate vs. Case Fatality Rate',\n    xaxis_title_text='Value', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n)\n                  \nfig.update_traces(opacity=0.75)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test SG supposed Death Value\nworldometer_SG_Test = worldometer.loc[worldometer[\"Country/Region\"].isin(['Singapore'])]\nworldometer_SG_Test = worldometer_SG_Test.sort_values('TestRate',ascending=False)\nworldometerdata_SG_Test = worldometer_data.loc[worldometer_data[\"Country/Region\"].isin(['Singapore'])]\nworldometer_SG_Test.set_index('Country/Region', inplace=True)\nworldometerdata_SG_Test.set_index('Country/Region', inplace=True)\n\n# Singapore Test Number:\nSG_Test_Num = worldometer_SG_Test.loc['Singapore',\"TotalTests\"]\n\n# Singapore actual Positive Number\nSG_Actual_Pos_Num = worldometer_SG_Test.loc['Singapore',\"TotalCases\"]\n\n# Singapore use Universal Death Number Should be\nSG_Est_Fatality_Ratio = worldometerdata_SG_Test.loc['Singapore',\"Estimated Infection Fatality Ratio\"]\nSG_Supposed_Death_Num = SG_Actual_Pos_Num * SG_Est_Fatality_Ratio\n\n# Singapore actual Death Number\nSG_Actual_Death_Num = worldometer_SG_Test.loc['Singapore',\"TotalDeaths\"]\n\ncolumns_SG = ['Number']\nIndex_SG = ['SG TEST NUM in Thousands','SG POSITIVE NUM', 'SG SUPPOSED DEATH NUM', 'SG ACTUAL DEATH NUM']\nSG_Chart = pd.DataFrame(data = '', index = Index_SG, columns = columns_SG)\nSG_Chart.loc['SG TEST NUM in Thousands','Number'] = int(SG_Test_Num/1000)\nSG_Chart.loc['SG POSITIVE NUM','Number'] = int(SG_Actual_Pos_Num)\nSG_Chart.loc['SG SUPPOSED DEATH NUM','Number'] = int(SG_Supposed_Death_Num)\nSG_Chart.loc['SG ACTUAL DEATH NUM','Number'] = int(SG_Actual_Death_Num)\n\nSG_Chart","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-three-two-two\"></a>\n### 3.2.2 Why Singapore is Different"},{"metadata":{},"cell_type":"markdown","source":"Although calculated, and Singapore's number should be lower and death rate should be higher if we use the benchmark, however, we believe that the case in Singapore would not necessary to use the world average benchmark to normalized.\n\nThe reasons behind are:\n\n- Singapore is a very transparent and open country, with low corruption and political abussion data (Singapore could hid the number by counting not only the worker's camp number but it does)\n- Singapore publishes data everyday on: https://www.moh.gov.sg/covid-19, it's very clear that the government is telling residence everything\n- Strict locked down is implemented and the government are tracking things everywhere, direct all those close contact person, so it's unlikely that error will occur in tracking\n- Hospital/Quarantine place are abundant, so the resources used to rescue those infected are in placed, which leads to the low death number\n- Therefore, we have strong faith that Singapore is telling the truth and it performs much better than other countries"},{"metadata":{},"cell_type":"markdown","source":"![](http://)<a id=\"subsection-three-three\"></a>\n## 3.3 Economy Impact to Singapore"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-three-three-one\"></a>\n### 3.3.1 STI Index Performance & Helicopter Money"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n# Add traces to create subplots\nfig.add_trace(\n    go.Scatter(x=baseline2020['Date'], y=baseline2020['sti'], name = 'STI Index'),  \n    secondary_y=False,\n)\n\nfig.add_trace(\n    go.Scatter(x=baseline2020['Date'], y=baseline2020['New cases'], name = 'New COVID19 Cases'), \n    secondary_y=True,\n)\n\n# Add figure title\nfig.update_layout(\n    title_text=\"STI and New COVID19 Cases\"\n)\n\n# Set x-axis title\nfig.update_xaxes(title_text=\"Date\")\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"<b>STI</b>\", secondary_y=False)\nfig.update_yaxes(title_text=\"<b>New COVID19 Cases</b>\", secondary_y=True)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the covid period worse return is how many SD away from the mean SD\ndef plot_chart(series):\n    fig = px.scatter(baseline[baseline[series].notnull()], x=\"Date\", y=series, color='recession', width=1000)\n    fig.update_traces(mode='markers', marker_size=4)\n    fig.update_layout(title=series, xaxis_title=\"\", yaxis_title=\"\")\n    fig.show()\n\nprint(\"The worst single-day return in 2020 is \", str(round(abs(baseline2020['sti_return'].min()/baseline['sti_return'].std()),2)), \n      \" X standard deviations of STI historical returns!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Output the range of STI historical daily returns from 1999-08-31 onwards to 2020-09-03\nprint(\"STI historical daily returns from \" + str(baseline[baseline['sti_return'].notnull()]['Date'].min().date()) + ' to '\n       + str(baseline[baseline['sti_return'].notnull()]['Date'].max().date()))\n\nfig = px.histogram(baseline, x=\"sti_return\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotout the new cases to sti return (in wrangle data, new cases already change to SG only cases)\nsns.jointplot(x = 'New cases', y = 'sti_return', data = baseline2020, kind='reg')\n\n#SG Death Rate is so low so does not makes sense to draw death rate figure","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Helicopter Money? Singapore bill out only 2 times (umemployment CSV file is hard to find):\n- First time in 2009, 4.9 billion SGD fight for financial crisis, the market do not react much \n- Second on the 26th May, with 33 billion SGD this year, the market do not react much \n\n(unemployment rate is annual csv in SG,not representitive)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-three-three-two\"></a>\n### 3.3.2 SEA Group Stock Performance & FOMO Effect"},{"metadata":{},"cell_type":"markdown","source":"### FOMO Effects\n- May occur in Sea LImited, Robinhood data for SEA, import from data, search for robinhood and then SE.CSV\n- So we decided to track tech trend and use Tesla holding as comparison"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter Sea return series and robinhood users who invest in Sea for the year 2020\nsea_robinhood2020 = sea_robinhood[sea_robinhood['Date'] >= '2020-01-01']\n\n\n# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n# Add traces\nfig.add_trace(\n    go.Scatter(x=sea_robinhood2020['Date'], y=sea_robinhood2020['sea'], name = 'Sea Price'),  \n    secondary_y=False,\n)\n\nfig.add_trace(\n    go.Scatter(x=sea_robinhood2020['Date'], y=sea_robinhood2020['users_holding'], name = 'Robinhood Users\\' Holdings'), \n    secondary_y=True,\n)\n\n\n# Add figure title\nfig.update_layout(\n    title_text=\"Sea Price and Robinhood Users\\' Holdings\"\n)\n\n# Set x-axis title\nfig.update_xaxes(title_text=\"Date\")\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"<b>Sea Price</b>\", secondary_y=False)\nfig.update_yaxes(title_text=\"<b>Robinhood Users\\' Holdings</b>\", secondary_y=True)\n\nfig.show()\n\n\n\n#Turn infinity values (due to division by zero) into NaN and then drop all the NaN for the column 'users_holding_pct_change'\nsea_robinhood2020 = sea_robinhood2020.replace([np.inf, -np.inf], np.nan).dropna(subset=['users_holding_pct_change'], how=\"all\")\n#tesla_robinhood2020.describe()\n\n# Draw jointplot for testa's return and users_holding_pct_change\nsns.jointplot(x = 'sea_return', y = 'users_holding_pct_change', data = sea_robinhood2020, kind='reg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-three-three-three\"></a>\n### 3.3.3 Singapore Airline Stock Performance & Optimism on Recovery"},{"metadata":{},"cell_type":"markdown","source":"### Optimism about the coming economic recovery?\n- Singapore does not have stocks that produce vaccine etc\n- But we could exam SIA graph on Mordena Vaccine Announcement Date"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identify key milestone dates in vaccine developments by Moderna and later apply for SIA price\ndates = pd.to_datetime(['2020-7-27', '2020-5-6', '2020-5-1', '2020-4-27', '2020-4-16', '2020-3-16', '2020-1-13'])\n\n\nsgairline['vaccine_milestone_announced']  = sgairline['Date'].isin(dates)\nbaseline2020['vaccine_milestone_announced'] = baseline2020['Date'].isin(dates)\n\n# Let's create a function to plot graphs with vaccine milestones highlighted.\ndef plot_return_vaccine_milestone(data, asset):\n    fig = px.scatter(data, x='Date', y=asset, color='vaccine_milestone_announced', width=1000)\n    fig.update_traces(mode='markers', marker_size=4)\n    fig.update_layout(title=str(asset), xaxis_title='Date', yaxis_title=str(asset))\n    fig.show()\n\n# Draw a scatterplot of SIA's historical stock returns\nplot_return_vaccine_milestone(sgairline, 'sia_return')\n\n# Draw a scatterplot of STI's historical stock returns\nplot_return_vaccine_milestone(baseline2020, 'sti_return')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)<a id=\"subsection-three-three\"></a>\n## 3.4 Other Investigations"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-three-four-one\"></a>\n### 3.4.1 Matrix on Case vs All Assets and Stocks we Pick"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Draw scatter of asset returns during Covid19 pandemic\nbaseline_returns = baseline2020[[\"snp_return\", \"btc_return\", \"gold_return\", \"high_yield_bond_return\", \"inv_grade_bond_return\", \n                  \"wti_return\", \"sti_return\", \"New cases\"]]\nsns.pairplot(baseline_returns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Draw heatmap of correlation strength across asset classes (returns and volatilities) and Covid19 new cases and deaths during the pandemic period \nbaseline_corr = baseline2020[['sti_return', 'snp_return', 'snp_volatility_1y', 'btc_return', 'btc_volatility_1y',\n                         'gold_return', 'gold_volatility_1y', 'high_yield_bond_return', 'high_yield_bond_volatility_1y',\n                         'inv_grade_bond_return', 'inv_grade_bond_volatility_1y', 'wti_return', 'wti_volatility_1y',\n                         'New deaths', 'New cases']].corr() # just add one STI return as reference\n\nfig, ax = plt.subplots(figsize=(16,5)) \nsns.heatmap(baseline_corr, annot=True, ax = ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n# 4. Conclusion"},{"metadata":{},"cell_type":"markdown","source":"Singapore's COVID 19 is well under controlled\n- Compare with peers, it has lower death rate due to abundant hospital facilities, fast reaction, closed SG measures etc\n- World parameter is not proper to use and adjust SG number as government is transparent\n\nCOVID 19 and SG Stock Markets\n- No necessary related to STI Index (explosion happen when STI increase)\n- New cases does not link to STI Index performance\n- STI beggest lost only 5.02x historical SD in covid period, much better than US\n- Helicopter money drop on 26th May, but it has no impact on the market\n- Employment data is annul CSV file, therefore, not representitive\n\nSingle Stock Performance\n- SEA Limited: Price hike as FOMO effect happens, people keep buying even they see small growth, but data is only until Feb and listed in US\n- SIA Limited: Price volatility happen when vaccine news comes, but not necessary have relationship\n- STI Index vs Vaccine News: No clear relationship\n\nOthers: New cases strongly correlated with death and volatilities, but not other assets\nSuggestions: Singapore is a stable market for hedging, SEA has growth perspective but fundamentals needs to be investigate before you invest in the money\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}