{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.052512,"end_time":"2020-09-29T05:57:19.798481","exception":false,"start_time":"2020-09-29T05:57:19.745969","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# TABLE OF CONTENTS\n\n* [1. INTRODUCTION](#section-one)\n* [2. SETUP](#section-two)\n    - [2.1 Draw Packages](#subsection-two-one)\n    - [2.2 Import and Wrangle Data](#subsection-two-two)\n    - [2.3 Define Function](#subsection-two-three)\n* [3. Government Action and Next Epicenter](#section-three)\n    - [3.1 Do government actions matter globally](#subsection-three-one)\n    - [3.2 Do government actions matter in Singapore](#subsection-three-two)\n    - [3.3 Variables Affects the Infection Rate](#subsection-three-three)\n    - [3.4 Which countries may be the epicenters next?](#subsection-three-four)\n* [4. Stock and Index Correlation Analysis](#section-four)\n    - [4.1 SEA vs SPY](#subsection-four-one)\n    - [4.2 SIA vs STI](#subsection-four-two)\n    - [4.3 DBS vs STI](#subsection-four-three)\n* [5. CONCLUSION](#section-five)\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"papermill":{"duration":0.047872,"end_time":"2020-09-29T05:57:19.895244","exception":false,"start_time":"2020-09-29T05:57:19.847372","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# 1. INTRODUCTION\n\nIn this problem set, we are going to use inferential statistic to discuss globally and local issues. Firstly, we are going to discuss in the world region, if government intervention matters and predict the next epi center for virus explosion. Secondly, we are going to discuss the singapore government intervention does it matters and apply the inferential statistic to exam if SEA vs NYSE; SIA vs STI index closely correlated to each other."},{"metadata":{"_uuid":"323e2874cb99bdeea4a9c7e6bb8af7432f1c3c62","papermill":{"duration":0.04768,"end_time":"2020-09-29T05:57:19.991361","exception":false,"start_time":"2020-09-29T05:57:19.943681","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n# 2. SETUP"},{"metadata":{},"cell_type":"markdown","source":"1. Settings -> Environment -> \"Always use latest environment\"\n2. Settings -> Internet -> \"On\"\n3. Track Changes Adding for Section 1 and 2 from Previous Problem Set -> Ctrl+F -> \"### PBS_\" -> + X:Change/Add: Why"},{"metadata":{"papermill":{"duration":0.04796,"end_time":"2020-09-29T05:57:20.088169","exception":false,"start_time":"2020-09-29T05:57:20.040209","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"subsection-two-one\"></a>\n## 2.1 Draw Packages"},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T05:57:20.194457Z","iopub.status.busy":"2020-09-29T05:57:20.193691Z","iopub.status.idle":"2020-09-29T05:57:23.711293Z","shell.execute_reply":"2020-09-29T05:57:23.710359Z"},"papermill":{"duration":3.575048,"end_time":"2020-09-29T05:57:23.711427","exception":false,"start_time":"2020-09-29T05:57:20.136379","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Data Management\nfrom dateutil import relativedelta as rd ### PBS_3: ADDING: add the package\nimport datetime as dt\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport plotly as py ### PBS_3: ADDING: add the package\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.offline as pyo ### PBS_3: CHANGING,import entire package, other PBS only use partial function inside\npyo.init_notebook_mode()\nimport seaborn as sns\n\n# Regression \nimport statsmodels.api as sm ### PBS_3: ADDING: add the package\nfrom statsmodels.formula.api import ols ### PBS_3: ADDING: add the package\nimport statsmodels.graphics.api as smg ### PBS_3: ADDING: add the package\nfrom scipy.stats import t ### PBS_3: ADDING: add the package\nfrom scipy.stats import chi2_contingency ### PBS_3: ADDING: add the package\n\n!pip install yfinance ### PBS_3: ADDING: pip install package\nimport yfinance as yf ### PBS_3: ADDING: add the package","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.131935,"end_time":"2020-09-29T05:57:24.003399","exception":false,"start_time":"2020-09-29T05:57:23.871464","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"subsection-two-two\"></a>\n## 2.2 Import and Wrangle Data"},{"metadata":{"papermill":{"duration":0.13435,"end_time":"2020-09-29T05:57:24.271937","exception":false,"start_time":"2020-09-29T05:57:24.137587","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Please add these datasets (first DATASET is the new one):  ### PBS_3: ADDING: first dataset is new dataset\n1. countryinfo: https://www.kaggle.com/koryto/countryinfo   \n2. Covid19 datasets: https://www.kaggle.com/imdevskp/corona-virus-report   \n3. Covid19 worldometer: https://www.kaggle.com/selfishgene/covid19-worldometer-snapshots-since-april-18   \n4. STI index File from computer (local csv source, located at Hanye's computer at C:\\Users\\user\\Desktop\\NUS\\NUS MFin Course\\Sem 1\\BMF 5324 Statistic and Analytics\\Lecture Notes\\L2)\n5. SIA and Price Data (local zip source, located at Hanye's computer at C:\\Users\\user\\Desktop\\NUS\\NUS MFin Course\\Sem 1\\BMF 5324 Statistic and Analytics\\Lecture Notes\\L5), do not need to change anything here as the file is already uploaded\n"},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T05:57:24.56025Z","iopub.status.busy":"2020-09-29T05:57:24.559165Z","iopub.status.idle":"2020-09-29T05:57:24.639365Z","shell.execute_reply":"2020-09-29T05:57:24.638505Z"},"papermill":{"duration":0.233092,"end_time":"2020-09-29T05:57:24.639504","exception":false,"start_time":"2020-09-29T05:57:24.406412","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"### PBS_3: ADDING: ADD THIS SECTION\n\n# FIRST FILE\n\n# Read and rename column country\ncty_info = pd.read_csv('../input/countryinfo/covid19countryinfo.csv').rename(columns={'country':'Country'})\n\n# Filter observations with aggregate country-level information\n# The column region for region-level observations is populated\ncty_info = cty_info[cty_info.region.isnull()]\n\n# Convert string data type to floating data type\n# Remove comma from the fields\ncty_info['healthexp'] = cty_info[~cty_info['healthexp'].isnull()]['healthexp'].str.replace(',','').astype('float')\ncty_info['gdp2019'] = cty_info[~cty_info['gdp2019'].isnull()]['gdp2019'].str.replace(',','').astype('float')\n\n# Convert to date objects with to_datetime method\ngov_actions = ['quarantine', 'schools', 'gathering', 'nonessential', 'publicplace']\n\nfor gov_action in gov_actions:\n    cty_info[gov_action] = pd.to_datetime(cty_info[gov_action], format = '%m/%d/%Y')\n    \n# Filter columns of interest\n# Note: feel free to explore other variables or datasets\ncty_info = cty_info[['Country','avghumidity', 'avgtemp', 'fertility', 'medianage', 'urbanpop', 'quarantine', 'schools', \\\n                    'publicplace', 'gatheringlimit', 'gathering', 'nonessential', 'hospibed', 'smokers', \\\n                    'sex0', 'sex14', 'sex25', 'sex54', 'sex64', 'sex65plus', 'sexratio', 'lung', 'femalelung', \\\n                    'malelung', 'gdp2019', 'healthexp', 'healthperpop']]\n\n# can use the lines below to check on table inforamtion\n# cty_info.describe()\n# cty_info.info()\n# cty_info.head(20)\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T05:57:24.931332Z","iopub.status.busy":"2020-09-29T05:57:24.930425Z","iopub.status.idle":"2020-09-29T05:57:25.134689Z","shell.execute_reply":"2020-09-29T05:57:25.136258Z"},"papermill":{"duration":0.357864,"end_time":"2020-09-29T05:57:25.136711","exception":false,"start_time":"2020-09-29T05:57:24.778847","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# SECOND FILE INPUT\n\nworldometer_data = pd.read_csv('../input/corona-virus-report/worldometer_data.csv')\n\n# Replace missing values '' with NAN and then 0\nworldometer_data = worldometer_data.replace('', np.nan).fillna(0)\n\n# Transform variables and round them up to the two decimal points\n# Note that there are instances of division by zero issue when there are either zero total tests or total cases\nworldometer_data['Case Positivity'] = round(worldometer_data['TotalCases']/worldometer_data['TotalTests'],2)\nworldometer_data['Case Fatality'] = round(worldometer_data['TotalDeaths']/worldometer_data['TotalCases'],2)\n\n# Resolve the division by zero issue by replacing infinity value with zero\nworldometer_data[worldometer_data[\"Case Positivity\"] == np.inf] = 0\nworldometer_data[worldometer_data[\"Case Fatality\"] == np.inf] = 0 ### PBS_3: ADDING: add this item\n\n### PBS_3: ADDING: add the BINs\n# Place case positivity into three bins\nworldometer_data ['Case Positivity Bin']= pd.qcut(worldometer_data['Case Positivity'], q=3, labels=[\"low\", \"medium\", \"high\"])\n\n# Population Structure\nworldometer_pop_struc = pd.read_csv('../input/covid19-worldometer-snapshots-since-april-18/population_structure_by_age_per_contry.csv')\n\n# Replace missing values with zeros\nworldometer_pop_struc = worldometer_pop_struc.fillna(0)\n\n# Merge datasets by common key country\nworldometer_data = worldometer_data.merge(worldometer_pop_struc,how='inner',left_on='Country/Region', right_on='Country')\nworldometer_data = worldometer_data[worldometer_data[\"Country/Region\"] != 0]\n\n### PBS_3: ADDING: merge one more item and replace the item below (below all adding)\n# Country information\nworldometer_data = worldometer_data.merge(cty_info, how='left', on='Country')\n\n### PBS_3: ADDING: merge one more item and replace the item below (below all adding)\n# Replace space in variable names with '_'\nworldometer_data.columns = worldometer_data.columns.str.replace(' ', '_')\n\n# can use the lines below to check on table inforamtion\n# worldometer_data.describe()\n# worldometer_data.info()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T05:57:25.417782Z","iopub.status.busy":"2020-09-29T05:57:25.41673Z","iopub.status.idle":"2020-09-29T05:57:25.57176Z","shell.execute_reply":"2020-09-29T05:57:25.570997Z"},"papermill":{"duration":0.293621,"end_time":"2020-09-29T05:57:25.571908","exception":false,"start_time":"2020-09-29T05:57:25.278287","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# THIRD FILE INPUT \n\nfull_table = pd.read_csv('../input/corona-virus-report/covid_19_clean_complete.csv')\n### PBS_3: ADDING: only in PBS3 this line to change date appears\nfull_table['Date'] = pd.to_datetime(full_table['Date'])\n\n# Examine DataFrame (object type, shape, columns, dtypes)\n# full_table.info()\n\n#type(full_table)\n#full_table.shape\n#full_table.columns\n#full_table.dtypes\n\n# Deep dive into the DataFrame\n# full_table.head()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T05:57:25.865349Z","iopub.status.busy":"2020-09-29T05:57:25.864354Z","iopub.status.idle":"2020-09-29T05:57:27.882339Z","shell.execute_reply":"2020-09-29T05:57:27.881555Z"},"papermill":{"duration":2.169716,"end_time":"2020-09-29T05:57:27.882467","exception":false,"start_time":"2020-09-29T05:57:25.712751","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# FOURTH FILE INPUT\n# Grouped by day, country\n\nfull_grouped = pd.read_csv('../input/corona-virus-report/full_grouped.csv')\nfull_grouped['Date'] = pd.to_datetime(full_grouped['Date'])\n\n### PBS_3: ADDING: the rest are problem set adding\n\n# Correct country names in worldometer to make them consistent with dataframe full_grouped column Country/Region before merging \nworldometer_data['Country/Region'].replace({'USA':'US', 'UAE':'United Arab Emirates', 'S. Korea':'South Korea', \\\n                                           'UK':'United Kingdom'}, inplace=True)\n\n# Draw population and country-level data\nfull_grouped = full_grouped.merge(worldometer_data[['Country/Region', 'Population']], how='left', on='Country/Region')\nfull_grouped = full_grouped.merge(cty_info, how = 'left', left_on = 'Country/Region', right_on = 'Country')\nfull_grouped['Confirmed per 1000'] = full_grouped['Confirmed'] / full_grouped['Population'] * 1000\n\n# Backfill data\nfull_grouped = full_grouped.fillna(method='ffill')\n\n# Create post-invention indicators\ngov_actions = ['quarantine', 'schools', 'gathering', 'nonessential', 'publicplace']\n\nfor gov_action in gov_actions:\n    full_grouped['post_'+gov_action] = full_grouped['Date'] >= full_grouped[gov_action]\n    full_grouped['day_rel_to_' + gov_action] = (full_grouped['Date'] - full_grouped[gov_action]).dt.days\n\n# Create percent changes in covid19 outcomes\ncovid_outcomes = ['Confirmed', 'Deaths', 'Recovered', 'Active', 'Confirmed per 1000']\n\nfor covid_outcome in covid_outcomes:\n    full_grouped['pct_change_' + covid_outcome] = full_grouped.groupby(['Country/Region'])[covid_outcome].pct_change()\n    full_grouped[full_grouped['pct_change_' + covid_outcome] == np.inf] = 0\n\n# Replace space in variable names with '_'\nfull_grouped.columns = full_grouped.columns.str.replace(' ', '_')\nfull_grouped_SG_Only = full_grouped.loc[full_grouped[\"Country/Region\"].isin(['Singapore'])]\n\n# full_grouped.info()\n#full_grouped.tail(20)\n#print(full_grouped.iloc[0,0])\n# list(full_grouped.columns.values)\n# full_grouped.describe()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T05:57:28.162021Z","iopub.status.busy":"2020-09-29T05:57:28.161197Z","iopub.status.idle":"2020-09-29T05:57:28.621595Z","shell.execute_reply":"2020-09-29T05:57:28.620976Z"},"papermill":{"duration":0.604303,"end_time":"2020-09-29T05:57:28.621727","exception":false,"start_time":"2020-09-29T05:57:28.017424","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"### PBS_3: ADDING: the rest are problem set adding\n# Visualize the missingness isue in the dataset\nsns.heatmap(cty_info.isnull(), cbar=False)\n# We found lots of governmental item missing\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### PBS_3: ADDING: the rest are problem set adding\n# try singapore, missing all the data for quarantine etc, therefore we could not test SG\ncty_info_sg = cty_info.loc[cty_info[\"Country\"].isin(['Singapore'])]\nsns.heatmap(cty_info_sg.isnull(), cbar=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Then we adding Singapore quarantine day (knowing the index is 162 and row is those four)\ncty_info.iloc[162,6:12] = dt.date(2020,4,7) # quarantine start from 07042020","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.135146,"end_time":"2020-09-29T05:57:28.894595","exception":false,"start_time":"2020-09-29T05:57:28.759449","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"subsection-two-three\"></a>\n## 2.3 Define Function"},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T05:57:29.186135Z","iopub.status.busy":"2020-09-29T05:57:29.184394Z","iopub.status.idle":"2020-09-29T05:57:29.189129Z","shell.execute_reply":"2020-09-29T05:57:29.189708Z"},"papermill":{"duration":0.158201,"end_time":"2020-09-29T05:57:29.189898","exception":false,"start_time":"2020-09-29T05:57:29.031697","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# This function is from previous bootcamp, copy paste the one to exam SG here\n# Create a function to plot (reusing from the previous BootCamp)\n\ndef gt_n(n):\n    countries = full_grouped[full_grouped['Confirmed']>n]['Country/Region'].unique()\n    temp = full_table[full_table['Country/Region'].isin(countries)]\n    temp = temp.groupby(['Country/Region', 'Date'])['Confirmed'].sum().reset_index()\n    temp = temp[temp['Confirmed']>n]\n    temp['Log Confirmed'] = np.log(1 + temp['Confirmed'])\n    # print(temp.head())\n\n    min_date = temp.groupby('Country/Region')['Date'].min().reset_index()\n    min_date.columns = ['Country/Region', 'Min Date']\n    # print(min_date.head())\n\n    from_nth_case = pd.merge(temp, min_date, on='Country/Region')\n    from_nth_case['Date'] = pd.to_datetime(from_nth_case['Date'])\n    from_nth_case['Min Date'] = pd.to_datetime(from_nth_case['Min Date'])\n    from_nth_case['N days'] = (from_nth_case['Date'] - from_nth_case['Min Date']).dt.days\n    # print(from_nth_case.head())\n\n    fig = px.line(from_nth_case, x='N days', y='Confirmed', color='Country/Region', \\\n                  title='N days from '+str(n)+' case', height=600)\n    fig.show()\n    \n    fig = px.line(from_nth_case, x='N days', y='Log Confirmed', color='Country/Region', \\\n                  title='N days from '+str(n)+' case', height=600)\n    fig.show()\n\n    \n# Function to fatch SG similar countries\ndef gt_n_sg(minimal,maximum,days_from_n_case):\n    # Identify countries with confirmed cases greater than days_from_n_case\n    # Then among these countries choose the unique set of countries\n    countries = full_grouped[full_grouped['Confirmed']>minimal]['Country/Region'].unique()\n    countries_maxlimit = full_grouped[full_grouped['Confirmed']>=maximum]['Country/Region'].unique()\n    \n    # Filter countries that are in the unique set of countries with confirmed cases greater than minimal\n    temp = full_table[full_table['Country/Region'].isin(countries)]\n    temp = temp[~ temp['Country/Region'].isin(countries_maxlimit)] # not in the region, so we filter out those very big number countries such as us\n    similar_country_sample_defined = temp\n    \n    # Aggregate (i.e., sum up) confirmed cases by Country/Region and Date\n    # Reset the index (it is no longer in running order)\n    temp = temp.groupby(['Country/Region', 'Date'])['Confirmed'].sum().reset_index()\n    \n    # Filter observations starting from the day n case is recorded\n    temp = temp[temp['Confirmed']>days_from_n_case]\n    # print(temp.head())\n\n    # Filter observations with confirmed cases more than minimal\n    temp = temp[temp['Confirmed']<maximum]\n    # print(temp.head())\n    \n    # Identify the start date when confirmed cases exceed minimal for each country\n    min_date = temp.groupby('Country/Region')['Date'].min().reset_index()\n    \n    # Name the columns in the dataframe min_date\n    min_date.columns = ['Country/Region', 'Min Date']\n    # print(min_date.head())\n\n    # Merge dataframe temp with dataframe min_date by 'Country/Region'\n    from_nth_case = pd.merge(temp, min_date, on='Country/Region')\n    \n    # Convert data type to datetime object\n    from_nth_case['Date'] = pd.to_datetime(from_nth_case['Date'])\n    from_nth_case['Min Date'] = pd.to_datetime(from_nth_case['Min Date'])\n    \n    # Create a variable that counts the number of days relative to the day when confirmed cases exceed N\n    from_nth_case['N days'] = (from_nth_case['Date'] - from_nth_case['Min Date']).dt.days\n    # print(from_nth_case.head())\n\n    # Plot a line graph from dataframe from_nth_case with column 'N days' and 'Confirmed' mapped to x-axis and y-axis, respectively.\n    # Distinguish each country by color (system-determined color)\n    # str converts n integer into string and \"'minimal days from '+ str(n) +' case'\" is the title \n    fig = px.line(from_nth_case, x='N days', y='Confirmed', color='Country/Region', \n                  title='N days from '+ str(days_from_n_case) +' case', height=600)\n    fig.show()\n    \n    return similar_country_sample_defined\n\n\n# Singapore has case = 50000, so we create this first function\n# So we compare countries with 10-100k case\n\n# Calling\n# similar_country_sample_defined = gt_n_sg(40000,60000,1000)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.13913,"end_time":"2020-09-29T05:57:29.470454","exception":false,"start_time":"2020-09-29T05:57:29.331324","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n# 3. Government Action and Next Epicenter"},{"metadata":{"papermill":{"duration":0.138922,"end_time":"2020-09-29T05:57:29.749964","exception":false,"start_time":"2020-09-29T05:57:29.611042","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"subsection-three-one\"></a>\n## 3.1 Do government actions matter globally "},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T05:57:30.038552Z","iopub.status.busy":"2020-09-29T05:57:30.037456Z","iopub.status.idle":"2020-09-29T05:57:30.041457Z","shell.execute_reply":"2020-09-29T05:57:30.040773Z"},"papermill":{"duration":0.152541,"end_time":"2020-09-29T05:57:30.041589","exception":false,"start_time":"2020-09-29T05:57:29.889048","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def plot_gov_action (covid_outcome, gov_action):\n    fig = px.scatter(full_grouped[full_grouped[gov_action] != None], x = 'day_rel_to_' + gov_action \\\n                     , y=covid_outcome, color='Country/Region', \\\n                     title='N days from ' + gov_action, height=600)\n    fig.update_layout(yaxis=dict(range=[0,10]))\n    fig.show()\n\n# gov_actions = ['quarantine', 'schools', 'gathering', 'nonessential', 'publicplace']\nplot_gov_action('pct_change_Confirmed_per_1000', 'quarantine')","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.722329,"end_time":"2020-09-29T05:57:37.14804","exception":false,"start_time":"2020-09-29T05:57:36.425711","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"subsection-three-two\"></a>\n## 3.2 Do government actions matter in Singapore? "},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_gov_action_singapore (covid_outcome, gov_action):\n    fig = px.scatter(full_grouped_SG_Only[full_grouped_SG_Only[gov_action] != None], x = 'day_rel_to_' + gov_action \\\n                     , y=covid_outcome, color='Country/Region', \\\n                     title='N days from ' + gov_action, height=600)\n    fig.update_layout(yaxis=dict(range=[0,1]))\n    fig.show()\n\n# gov_actions = ['quarantine', 'schools', 'gathering', 'nonessential', 'publicplace']\nplot_gov_action_singapore('pct_change_Confirmed_per_1000', 'quarantine')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For Singapore, according to test on urban population etc, we do not need to worry about the correlation test\n# What we could do as we mentioned above, the correlation formula residual is not convincible, therefore, we are not apply it here","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-three-three\"></a>\n## 3.3 Variables Affects the Infection Rate"},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T05:57:40.112846Z","iopub.status.busy":"2020-09-29T05:57:40.111692Z","iopub.status.idle":"2020-09-29T05:57:40.124333Z","shell.execute_reply":"2020-09-29T05:57:40.124951Z"},"papermill":{"duration":0.749084,"end_time":"2020-09-29T05:57:40.125114","exception":false,"start_time":"2020-09-29T05:57:39.37603","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# full_grouped['Confirmed_per_1000'].describe()\nfull_grouped['log_Confirmed_per_1000'] = np.log(full_grouped['Confirmed_per_1000']+1) # avoid 0\n# full_grouped['log_Confirmed_per_1000'].describe()\n\n#Plot pairplot with countries organized by WHO regions, show if anything matters\ng = sns.pairplot(full_grouped[[\"log_Confirmed_per_1000\", \"avghumidity\", \"avgtemp\", \"urbanpop\", \"WHO_Region\"]], hue=\"WHO_Region\")","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T05:57:54.453492Z","iopub.status.busy":"2020-09-29T05:57:54.45263Z","iopub.status.idle":"2020-09-29T05:57:55.618096Z","shell.execute_reply":"2020-09-29T05:57:55.617471Z"},"papermill":{"duration":1.915966,"end_time":"2020-09-29T05:57:55.618235","exception":false,"start_time":"2020-09-29T05:57:53.702269","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# investigate if there are any relionship on two variables we input in\n# plt.matshow(full_grouped.corr())\n# plt.show()\n\nf, ax = plt.subplots(figsize=(10, 8))\ncorr = full_grouped.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Plot heatmap\nsns.heatmap(corr, mask=mask, cmap=cmap, square=True, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.796635,"end_time":"2020-09-29T05:57:57.165326","exception":false,"start_time":"2020-09-29T05:57:56.368691","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Note: sex ratio (i.e., amount of males per female) in each country and sex ratio by age groups. Lung disease data (i.e., death rate per 100k people) in each country and lung disease by sex."},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T05:58:00.165344Z","iopub.status.busy":"2020-09-29T05:58:00.156732Z","iopub.status.idle":"2020-09-29T05:58:00.304749Z","shell.execute_reply":"2020-09-29T05:58:00.304144Z"},"papermill":{"duration":0.8999,"end_time":"2020-09-29T05:58:00.304944","exception":false,"start_time":"2020-09-29T05:57:59.405044","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Create interaction term\nfull_grouped['quarXurbanpop'] = full_grouped['post_quarantine'] * full_grouped['urbanpop']\n\n# OLS regression\ny = full_grouped['log_Confirmed_per_1000']\nX = full_grouped[['post_quarantine', 'avghumidity', 'avgtemp', 'urbanpop', 'quarXurbanpop']]\nX = sm.add_constant(X)\n\nols_model=sm.OLS(y,X.astype(float), missing='drop')\nresult=ols_model.fit()\nprint(result.summary2())","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.729475,"end_time":"2020-09-29T05:58:01.776933","exception":false,"start_time":"2020-09-29T05:58:01.047458","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Only the dependent/response variable is log-transformed. Exponentiate the coefficient, subtract one from this number, and multiply by 100. This gives the percent increase (or decrease) in the response for every one-unit increase in the independent variable. The coefficient is -0.1455. (e^(-0.1455)-1) * 100 = -13.54%. So, the imposition of quarantine lowers confirmed cases per 1000 by 13.54%.\n\n T-statistic: The null hypothesis is the independent variable has coefficient of zero, as we reject it smoothly here.\n \n F-statistic: The null hypothesis states that the model with no independent variables (intercept-only) fits the data as well as your model. As F test is sufficient large, we pass F test.\n \n Omnibus and Jarque-Bera: The null hypothesis states that the residuals are normally distributed. Here the data is not normally distributed as Prob (Omnibus and JB aree small)\n \n Condition No: A test of potential multicollinearity issue. The informal rule of thumb: >=15 multicollinearity is an issue, > 30 a serious concern, here is 700 so we need to consider about it\n \n Durbin-Watson: The null hypothesis states that the residuals are not serially correlated. "},{"metadata":{"papermill":{"duration":0.731798,"end_time":"2020-09-29T05:58:04.703837","exception":false,"start_time":"2020-09-29T05:58:03.972039","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### A. Normality of residuals"},{"metadata":{"papermill":{"duration":0.730832,"end_time":"2020-09-29T05:58:06.179296","exception":false,"start_time":"2020-09-29T05:58:05.448464","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Q-Q Plot"},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T05:58:07.653676Z","iopub.status.busy":"2020-09-29T05:58:07.651428Z","iopub.status.idle":"2020-09-29T05:58:07.956615Z","shell.execute_reply":"2020-09-29T05:58:07.955791Z"},"papermill":{"duration":1.042664,"end_time":"2020-09-29T05:58:07.956741","exception":false,"start_time":"2020-09-29T05:58:06.914077","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from statsmodels.graphics.gofplots import ProbPlot\n\nmodel_norm_residuals = result.get_influence().resid_studentized_internal\n\nQQ = ProbPlot(model_norm_residuals)\nplot_lm_2 = QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1)\nplot_lm_2.axes[0].set_title('Normal Q-Q')\nplot_lm_2.axes[0].set_xlabel('Theoretical Quantiles')\nplot_lm_2.axes[0].set_ylabel('Standardized Residuals');\n\n# annotations\nabs_norm_resid = np.flip(np.argsort(np.abs(model_norm_residuals)), 0)\nabs_norm_resid_top_3 = abs_norm_resid[:3]\nfor r, i in enumerate(abs_norm_resid_top_3):\n    plot_lm_2.axes[0].annotate(i,\n                               xy=(np.flip(QQ.theoretical_quantiles, 0)[r],\n                                   model_norm_residuals[i]));","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.724605,"end_time":"2020-09-29T05:58:09.412672","exception":false,"start_time":"2020-09-29T05:58:08.688067","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Evidence of a violation of normality assumption of the residuals is present (visually). This is consistent with the inferential tests with Omnibus and Jarque-Bera tests.We see a part that is diviate to the straight line (normal distribution)"},{"metadata":{"papermill":{"duration":0.737291,"end_time":"2020-09-29T05:58:10.875734","exception":false,"start_time":"2020-09-29T05:58:10.138443","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### B. Influence test"},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T05:58:12.369584Z","iopub.status.busy":"2020-09-29T05:58:12.367322Z","iopub.status.idle":"2020-09-29T05:58:36.810843Z","shell.execute_reply":"2020-09-29T05:58:36.810026Z"},"papermill":{"duration":25.193961,"end_time":"2020-09-29T05:58:36.810989","exception":false,"start_time":"2020-09-29T05:58:11.617028","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from statsmodels.graphics.regressionplots import plot_leverage_resid2\n\nfig, ax = plt.subplots(figsize=(8,6))\nfig = plot_leverage_resid2(result, ax = ax)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.73277,"end_time":"2020-09-29T05:58:38.286106","exception":false,"start_time":"2020-09-29T05:58:37.553336","status":"completed"},"tags":[]},"cell_type":"markdown","source":"We see the presence of some strong residuals. "},{"metadata":{"papermill":{"duration":0.73627,"end_time":"2020-09-29T05:58:39.752766","exception":false,"start_time":"2020-09-29T05:58:39.016496","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### C. Heteroskedasticity Test"},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T05:58:41.282585Z","iopub.status.busy":"2020-09-29T05:58:41.281711Z","iopub.status.idle":"2020-09-29T05:58:41.297745Z","shell.execute_reply":"2020-09-29T05:58:41.297034Z"},"papermill":{"duration":0.799497,"end_time":"2020-09-29T05:58:41.297904","exception":false,"start_time":"2020-09-29T05:58:40.498407","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip\n\nname = ['Breusch-Pagan Lagrange multiplier statistic', 'p-value',\n        'f-value', 'f p-value']\ntest = sms.het_breuschpagan(result.resid, result.model.exog)\nlzip(name, test)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.929987,"end_time":"2020-09-29T05:58:42.985281","exception":false,"start_time":"2020-09-29T05:58:42.055294","status":"completed"},"tags":[]},"cell_type":"markdown","source":"The null hypothesis of homoskedascity is rejected in favour of heteroskedasticity assumption. Which means the error term is not a constant."},{"metadata":{"papermill":{"duration":0.743852,"end_time":"2020-09-29T05:58:44.475489","exception":false,"start_time":"2020-09-29T05:58:43.731637","status":"completed"},"tags":[]},"cell_type":"markdown","source":"In summary, we need to be careful with any inferences we make with this model specification because of the non-normal residual (Breusch Pagan test), heteroskedastic residual (Q-Q plot, Jaeque-Bera test, Omnibus test), multicollinearity concern (Condition number), and the presence of strong outliers (Influence test), even though there is no evidence of serial correlation in the residuals (Durbin-Watson test) and the model specification is better than an intercept-only model (F-test).\n\nThe result may be because of we are lacking some important variables here other than the stated variables"},{"metadata":{"papermill":{"duration":0.73937,"end_time":"2020-09-29T05:58:45.958807","exception":false,"start_time":"2020-09-29T05:58:45.219437","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"subsection-three-four\"></a>\n## 3.4 Which countries may be the epicenters next?"},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T05:58:47.459517Z","iopub.status.busy":"2020-09-29T05:58:47.454374Z","iopub.status.idle":"2020-09-29T05:58:47.463068Z","shell.execute_reply":"2020-09-29T05:58:47.462328Z"},"papermill":{"duration":0.761238,"end_time":"2020-09-29T05:58:47.463199","exception":false,"start_time":"2020-09-29T05:58:46.701961","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"## Function to get the pandemic countries\ndef get_explosive_countries(df, confirmed_cases, n_explosive_week):\n    explosive_country = []\n    confirmed_cases_per_1000_at_explosion = []\n    \n    # identify unique set of countries\n    for cty in df['Country/Region'].unique():\n        \n        # filter observations one country at a time\n        country = df[df['Country/Region']==cty]\n        #print(cty)\n        \n        #By plotting the confirmed cases over time,\n        #the confirmed cases takes exponential shape after critical mass of n confirmed cases\n        \n        country = country[country['Confirmed'] > confirmed_cases]\n\n        if len(country): \n        \n            # print(\"... confirmed cases more than \" + str(confirmed_cases))  \n            # print(\"... first day is \" + str(country.iloc[0,0]))  \n            \n            country.reset_index(drop=True, inplace=True)\n            spread_rate=country['Confirmed'].pct_change(7).values\n            explosive_spread_counter=0\n            tmp_list=[]\n\n            #Check if there is an explosive growth over one-week period\n            for i in range(7,len(spread_rate),7):\n                if spread_rate[i] > 1.0: #100% growth over one-week\n                    explosive_spread_counter += 1\n                    tmp_list.append(country.iloc[i,38]) #confirmed cases per 1000 population\n                    # print(tmp_list)\n                    \n            #Term a country pandemic if doubling effect continued for more than a week        \n            if explosive_spread_counter > n_explosive_week: #100% growth over one-week for at least one week\n                explosive_country.append(cty)\n                confirmed_cases_per_1000_at_explosion.extend(tmp_list)\n                \n        else: \n            \n            pass\n            # print(\"... confirmed cases less than\" + str(confirmed_cases))    \n    \n    \n    # print(confirmed_cases_per_1000_at_explosion)\n                     \n    median_rate = np.quantile(confirmed_cases_per_1000_at_explosion,0.5)\n    \n    return explosive_country, median_rate","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T05:58:48.947014Z","iopub.status.busy":"2020-09-29T05:58:48.946205Z","iopub.status.idle":"2020-09-29T05:58:50.530315Z","shell.execute_reply":"2020-09-29T05:58:50.530976Z"},"papermill":{"duration":2.332733,"end_time":"2020-09-29T05:58:50.531173","exception":false,"start_time":"2020-09-29T05:58:48.19844","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# identify current epicenters: at least 1000 confirmed cases and at least two weekly 100% surge in confirmed cases\nexplosive_country, median_rate = get_explosive_countries(full_grouped[full_grouped['Confirmed_per_1000'].notnull()],1000,2)\n\n# display the list of explosive countries\nprint(\"List of Explosive Countries: \", explosive_country)\n\n# display the median confirmed cases per 1000 population at explosion\nprint(\"Median Confirmed Cases per 1000 Population at Explosion: \", str(median_rate))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T05:58:52.08276Z","iopub.status.busy":"2020-09-29T05:58:52.082016Z","iopub.status.idle":"2020-09-29T05:58:52.131133Z","shell.execute_reply":"2020-09-29T05:58:52.13047Z"},"papermill":{"duration":0.838397,"end_time":"2020-09-29T05:58:52.131271","exception":false,"start_time":"2020-09-29T05:58:51.292874","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# create a variable 'explosive_country' indicating whether a country is in the list of explosive country\n# turn datatype boolean to integer\nworldometer_data['explosive_country'] = worldometer_data['Country/Region'].isin(explosive_country).astype('int')\n\n# filter observations that are not in the list of explosive countries\nworldometer_data[worldometer_data['explosive_country'] == 0]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T05:58:56.837931Z","iopub.status.busy":"2020-09-29T05:58:56.836557Z","iopub.status.idle":"2020-09-29T05:58:56.896632Z","shell.execute_reply":"2020-09-29T05:58:56.89768Z"},"papermill":{"duration":0.850337,"end_time":"2020-09-29T05:58:56.897938","exception":false,"start_time":"2020-09-29T05:58:56.047601","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# dependent/target/outcome variable\ny = worldometer_data['explosive_country']\n\n# independent/predictor/explanatory variable\nX = worldometer_data[['avghumidity', 'avgtemp', 'urbanpop', 'gdp2019', 'healthperpop', 'TotalTests']]\n\n# logit regression\n# turn independent variables into floating type (best practice)\n# \"missing='drop'\" drops rows with missing values from the regression\nlogit_model=sm.Logit(y,X.astype(float), missing='drop' )\n\n# fit logit model into the data\nresult=logit_model.fit()\n\n# summarize the logit model\nprint(result.summary2())","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-09-29T05:58:58.410015Z","iopub.status.busy":"2020-09-29T05:58:58.408195Z","iopub.status.idle":"2020-09-29T05:58:58.426147Z","shell.execute_reply":"2020-09-29T05:58:58.426755Z"},"papermill":{"duration":0.779785,"end_time":"2020-09-29T05:58:58.426953","exception":false,"start_time":"2020-09-29T05:58:57.647168","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# calculate the predicted probability with the parameter estimates and values of Xs (i.e., the independent variables)\ny_hats2 = result.predict(X)\n\n# assign the values of predicted probabilities to worldometer_data\nworldometer_data['explosive_country_probability'] = y_hats2\n\n# filter countries that are not in the earlier list of countries with explosive number of confirmed cases\nnext_explosive_countries = worldometer_data[~worldometer_data['Country/Region'].isin(explosive_country)]\n\n# sort dataframe next_explosive_countries by the predicted probabilities in a descending order\n# and then display the 20 countries with the highest probabilities of experiencing explosive number of confirmed cases \n# note our covid19 dataset (i.e., confirmed cases) was last updated on 27 July 2020\nnext_explosive_countries.sort_values(by='explosive_country_probability', ascending=False)[['Country/Region', 'explosive_country_probability']].head(20)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.748925,"end_time":"2020-09-29T05:58:59.937852","exception":false,"start_time":"2020-09-29T05:58:59.188927","status":"completed"},"tags":[]},"cell_type":"markdown","source":"The top 5 countries that are likely to experience explosive growth in confirmed cases are India, Japan, Iran, Canada, and Kazakhstan. As we mentioned in class, the model although missing some important variables, it's still quite predictive."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n# 4. Stock and Index Correlation Analysis\n"},{"metadata":{},"cell_type":"markdown","source":"To test the correlation, we first think about if crisis on SARS or 2008 can be duplicate here, but it seems from the graph that the trend is obviously not adoptable and SEA is also not listed. Therefore, we are only testing correlation of two stock with corresponding index and see whether the test is significant.\n\nThen we find a reason to explain the observation we seem."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-four-one\"></a>\n## 4.1 SEA vs SPY"},{"metadata":{"trusted":true},"cell_type":"code","source":"# find the symbol (i.e., google the instrument + \"yahoo finance\")\n# e.g., market/sector index ETF for your chosen country and various asset classes (e.g., Comex Gold's symbol is \"GC=F\")\nsymbols_list = [\"SE\", \"SPY\"]\nstart = dt.datetime(2019,8,30)\nend = dt.datetime(2020,8,30)\ndata = yf.download(symbols_list, start=start, end=end)\n# data.head()\n\ndf = data['Adj Close']\ndf =df.pct_change()[1:]\n\nplt.figure(figsize=(20,10))\ndf['SE'].plot()\ndf['SPY'].plot()\nplt.ylabel(\"Daily returns of SE and SPY\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[\"SPY\"]\ny = df[\"SE\"]\n\n# Note the difference in argument order\nX = sm.add_constant(X)\nmodel = sm.OLS(y.astype(float), X.astype(float), missing='drop').fit()\npredictions = model.predict(X.astype(float)) # make the predictions by the model\n\n# Print out the statistics\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows that SEA and SPY is quite closely correlated.\n\nT and F test are passed, but still, fail the Omnibus and JB test, which means the data is not commonly distributed."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-four-two\"></a>\n## 4.2 SIA vs STI"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import and merge these two data\nsgairline = pd.read_csv('../input/sgairline/SIA.csv')\nSTI = pd.read_csv('../input/stiidx/STI.csv')\n\nsgairline['Date'] = pd.to_datetime(sgairline['Date'])\nsgairline.rename(columns={'Adj Close':'sia'}, inplace=True)\nsgairline['sia_return'] = sgairline['sia'].pct_change()\n\nSTI['Date'] = pd.to_datetime(STI['Date'])\nSTI.rename(columns={' Close':'sti'}, inplace=True) # remember there is a space before \"Close\"\nSTI['sti_return'] = STI['sti'].pct_change()\n\nsia_sti = STI.merge(sgairline, on = \"Date\", how = \"left\")\nsia_sti.dropna(inplace = True)\nsia_sti['Date'] = pd.to_datetime(sia_sti['Date'])\nsia_sti = sia_sti[sia_sti['Date'] >= \"2019-08-30\"]\nsia_sti = sia_sti[[\"Date\",\"sia_return\",\"sti_return\"]]\nsia_sti.set_index('Date', inplace = True)\n\nplt.figure(figsize=(20,10))\nsia_sti['sia_return'].plot()\nsia_sti['sti_return'].plot()\nplt.ylabel(\"Daily returns of SIA and STI\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = sia_sti[\"sti_return\"]\ny = sia_sti[\"sia_return\"]\n\n# Note the difference in argument order\nX = sm.add_constant(X)\nmodel = sm.OLS(y.astype(float), X.astype(float), missing='drop').fit()\npredictions = model.predict(X.astype(float)) # make the predictions by the model\n\n# Print out the statistics\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It shows that STI could not explain SIA price at all."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-four-three\"></a>\n## 4.3 DBS vs STI"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import and merge these two data\ndbs = pd.read_csv('../input/dbsstock/D05.SI.csv')\n\ndbs['Date'] = pd.to_datetime(dbs['Date'])\ndbs.rename(columns={'Adj Close':'dbs'}, inplace=True)\ndbs['dbs_return'] = dbs['dbs'].pct_change()\n\ndbs_sti = STI.merge(dbs, on = \"Date\", how = \"left\")\ndbs_sti.dropna(inplace = True)\ndbs_sti['Date'] = pd.to_datetime(dbs_sti['Date'])\ndbs_sti = dbs_sti[dbs_sti['Date'] >= \"2019-08-30\"]\ndbs_sti = dbs_sti[[\"Date\",\"dbs_return\",\"sti_return\"]]\ndbs_sti.set_index('Date', inplace = True)\n\nplt.figure(figsize=(20,10))\ndbs_sti['dbs_return'].plot()\ndbs_sti['sti_return'].plot()\nplt.ylabel(\"Daily returns of DBS and STI\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dbs_sti[\"sti_return\"]\ny = dbs_sti[\"dbs_return\"]\n\n# Note the difference in argument order\nX = sm.add_constant(X)\nmodel = sm.OLS(y.astype(float), X.astype(float), missing='drop').fit()\npredictions = model.predict(X.astype(float)) # make the predictions by the model\n\n# Print out the statistics\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It shows that STI could not explain DBS price at all."},{"metadata":{"papermill":{"duration":0.785589,"end_time":"2020-09-29T05:59:17.805135","exception":false,"start_time":"2020-09-29T05:59:17.019546","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"section-five\"></a>\n# 5. CONCLUSION\n\nGovernment Intervention vs Infection Rate:\n\n- Observed the quarantine measures does allow countries to bring down the infection rate growth\n\n- Virus spread quickest at dense urban region with temperature around 20 degree and humidity around 50%\n\n- Although the infection rate graph is not normally distributed, the urban population, temperature, humidity and quarantine measures indicator does help to forecast future COVID Epicentre quite well\n\n\nMovement on Index vs Key Stock Price:\n\n- SEA’s stock price has relatively high correlation with SPY index\n\n- SIA’s stock price cannot be explained by STI index"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}