{"cells":[{"metadata":{"_uuid":"4c874ff0e5769ac66b661256a14892a262bb39e1","_cell_guid":"f3bcfcf0-6a0f-4ee9-b3d6-1962a2ea5dad","trusted":false,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c55d1ad37b2c31465d24d6e62235f4028a569c73"},"cell_type":"markdown","source":"## Import Standard Python Libraries"},{"metadata":{"_uuid":"ce57b8f4a2834298dd5970557ea20290c89742d4","collapsed":true,"_cell_guid":"687ad6bd-c467-4f60-a2b0-94b31569e8d4","trusted":false},"cell_type":"code","source":"import io, os, sys, types, time, datetime, math, random, requests, subprocess, tempfile","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40d1de8929693cd9c787cc4a28d4fd9a53b4ae22"},"cell_type":"markdown","source":"## Packages Import\n\nThese are all the packages we'll be using. Importing individual libraries make it easy for us to use them without having to call the parent libraries."},{"metadata":{"_uuid":"4792a643fc7b110673c134eedbea54e45dcebe82","collapsed":true,"_cell_guid":"d9eb413f-6b62-46a7-b662-068951163130","trusted":false},"cell_type":"code","source":"# Data Manipulation \nimport numpy as np\nimport pandas as pd\n\n# Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nfrom pandas.tools.plotting import scatter_matrix\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Feature Selection and Encoding\nfrom sklearn.feature_selection import RFE, RFECV\nfrom sklearn.svm import SVR\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\n\n# Machine learning \nimport sklearn.ensemble as ske\nfrom sklearn import datasets, model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport tensorflow as tf\n\n# Grid and Random Search\nimport scipy.stats as st\nfrom scipy.stats import randint as sp_randint\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Metrics\nfrom sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n\n# Managing Warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Plot the Figures Inline\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11b23e7f83a60a412f617d171368ae1ffecf0c38"},"cell_type":"markdown","source":"## Data  Loading"},{"metadata":{"_uuid":"765296d84a7373c8093a1657c5469f0d16ca4022","collapsed":true,"_cell_guid":"19d87494-e6e9-4ad5-963f-39d5360f0697","trusted":false},"cell_type":"code","source":"dataset_raw = pd.read_csv('../input/voice.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7fa5d24b867de833aebf7914384dd719f669dc96"},"cell_type":"markdown","source":"## Data Exploration - Univariate\n\nWhen exploring our dataset and its features, we have many options available to us. We can explore each feature individually, or compare pairs of features, finding the correlation between. Let's start with some simple Univariate (one feature) analysis.\n\nFeatures can be of multiple types:\n- **Nominal:**  is for mutual exclusive, but not ordered, categories.\n- **Ordinal:** is one where the order matters but not the difference between values.\n- **Interval:** is a measurement where the difference between two values is meaningful.\n- **Ratio:** has all the properties of an interval variable, and also has a clear definition of 0.0.\n\nThere are multiple ways of manipulating each feature type, but for simplicity, we'll define only two feature types:\n- **Numerical:** any feature that contains numeric values.\n- **Categorical:** any feature that contains categories, or text."},{"metadata":{"_uuid":"2918f2e61ff847c114d3d688ba484915c39295d3","_cell_guid":"6441685d-f884-4aa2-a9bc-0f5ac87c1ebb","trusted":false,"collapsed":true},"cell_type":"code","source":"# Describing all the Numerical Features\ndataset_raw.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cd5a850abd1aced43356850857aed2d089006f9","_cell_guid":"7c552831-4fad-4869-9154-f2b5469af6bc","trusted":false,"collapsed":true},"cell_type":"code","source":"# Describing all the Categorical Features\ndataset_raw.describe(include=['O'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eda40e819b0daf6d96e9653b992617a491ad1210","_cell_guid":"75fcf4b6-c956-4582-8a1b-cac345cb9caf","trusted":false,"collapsed":true},"cell_type":"code","source":"# Let's have a quick look at our data\ndataset_raw.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b380cc91f6e46aacb1cedb6a0adff92bda22b0e","_cell_guid":"f2cae72d-fe79-4440-a432-444731b8005f","trusted":false,"collapsed":true},"cell_type":"code","source":"# Let’s plot the distribution of each feature\ndef plot_distribution(dataset, cols=5, width=20, height=15, hspace=0.2, wspace=0.5):\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width,height))\n    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=wspace, hspace=hspace)\n    rows = math.ceil(float(dataset.shape[1]) / cols)\n    for i, column in enumerate(dataset.columns):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        ax.set_title(column)\n        if dataset.dtypes[column] == np.object:\n            g = sns.countplot(y=column, data=dataset)\n            substrings = [s.get_text()[:18] for s in g.get_yticklabels()]\n            g.set(yticklabels=substrings)\n            plt.xticks(rotation=25)\n        else:\n            g = sns.distplot(dataset[column])\n            plt.xticks(rotation=25)\n    \nplot_distribution(dataset_raw, cols=3, width=20, height=20, hspace=0.45, wspace=0.5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a12218a6404f86e40cd1b230c8ea5c073ae9a4f","_cell_guid":"15045b5b-4f44-4510-a901-fc9fd61b7d58","trusted":false,"collapsed":true},"cell_type":"code","source":"# How many missing values are there in our dataset?\nmissingno.matrix(dataset_raw, figsize = (30,5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad470070f02139047b375f081f350bb995ef17c2","_cell_guid":"7cf62737-23df-4c47-aed7-077a3b7d5834","trusted":false,"collapsed":true},"cell_type":"code","source":"missingno.bar(dataset_raw, sort='ascending', figsize = (30,5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98568ba4b498225ac3fea1258644f01bc45e1782"},"cell_type":"markdown","source":"# Feature Cleaning, Engineering, and Imputation\n\n**Cleaning:**\nTo clean our data, we'll need to work with:\n\n- **Missing values:** Either omit elements from a dataset that contain missing values or impute them (fill them in).\n- **Special values:** Numeric variables are endowed with several formalized special values including ±Inf, NA and NaN. Calculations involving special values often result in special values, and need to be handled/cleaned.\n- **Outliers:** They should be detected, but not necessarily removed. Their inclusion in the analysis is a statistical decision.\n- **Obvious inconsistencies:** A person's age cannot be negative, a man cannot be pregnant and an under-aged person cannot possess a drivers license. Find the inconsistencies and plan for them.\n\n**Engineering:**\nThere are multiple techniques for feature engineering:\n- **Decompose:** Converting 2014-09-20T20:45:40Z into categorical attributes like hour_of_the_day, part_of_day, etc.\n- **Discretization:** We can choose to either discretize some of the continuous variables we have, as some algorithms will perform faster. We are going to do both, and compare the results of the ML algorithms on both discretized and non discretised datasets. We'll call these datasets:\n\n- dataset_bin => where Continuous variables are Discretised\n- dataset_con => where Continuous variables are Continuous \n\n- **Reframe Numerical Quantities:** Changing from grams to kg, and losing detail might be both wanted and efficient for calculation\n- **Feature Crossing:** Creating new features as a combination of existing features. Could be multiplying numerical features, or combining categorical variables. This is a great way to add domain expertise knowledge to the dataset.\n    \n**Imputation:**\nWe can impute missing values in a number of different ways:\n- **Hot-Deck:**\tThe technique then finds the first missing value and uses the cell value immediately prior to the data that are missing to impute the missing value.\n- **Cold-Deck:** Selects donors from another dataset to complete missing data.\n- **Mean-substitution:** Another imputation technique involves replacing any missing value with the mean of that variable for all other cases, which has the benefit of not changing the sample mean for that variable.\n- **Regression:** A regression model is estimated to predict observed values of a variable based on other variables, and that model is then used to impute values in cases where that variable is missing."},{"metadata":{"_uuid":"0c2996cae084cc87bc99e1fa3029dfcd9924010f","collapsed":true,"_cell_guid":"fcb29b30-a47a-46ee-8f16-bad0cd59dd70","trusted":false},"cell_type":"code","source":"# To perform our data analysis, let's create new dataframes.\ndataset_bin = pd.DataFrame() # To contain our dataframe with our discretised continuous variables \ndataset_con = pd.DataFrame() # To contain our dataframe with our continuous variables ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4bf831b76e6fd1ae1ff7a1c7fefe98cc83a5d34"},"cell_type":"markdown","source":"### Feature Label\n\nThis is the feature we are trying to predict. We'll change the string to a binary 0/1. With 1 signifying male."},{"metadata":{"_uuid":"ccf076869a0b699f15056c065fe563caf761f2a0","collapsed":true,"_cell_guid":"9ed69421-b735-4687-8c65-c2a3989192c6","trusted":false},"cell_type":"code","source":"# Let's fix the Class Feature\ndataset_raw.loc[dataset_raw['label'] == 'male', 'label'] = 1\ndataset_raw.loc[dataset_raw['label'] == 'female', 'label'] = 0\n\ndataset_bin['label'] = dataset_raw['label']\ndataset_con['label'] = dataset_raw['label']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c9eb5e1b43ee48c05cf8c4a1bb02696097c964d","_cell_guid":"c9fe4b4a-3e3c-4097-87cd-d35f498b04e9","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(20,1)) \nsns.countplot(y=\"label\", data=dataset_bin);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5991d09c6d00d01d254aa73a6a63f65c7124c683"},"cell_type":"markdown","source":"### Feature: meanfreq\n\nWe will use the Pandas Cut function to bin the data in equally sized buckets. We will also add our original feature to the dataset_con dataframe."},{"metadata":{"_uuid":"de94e0deabbad4178cf82ea592ef90065f11997e","collapsed":true,"_cell_guid":"a0258ced-684b-4566-b223-ffee650682d4","trusted":false},"cell_type":"code","source":"dataset_bin['meanfreq'] = pd.cut(dataset_raw['meanfreq'], 10) # discretised \ndataset_con['meanfreq'] = dataset_raw['meanfreq'] # non-discretised","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6328675091fbeb648b3c34200d9ed6ddb7d5b6e8","_cell_guid":"6b0478ec-b640-4366-9e11-8aeded7c3f16","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(20,5)) \nplt.subplot(1, 2, 1)\nsns.countplot(y=\"meanfreq\", data=dataset_bin);\nplt.subplot(1, 2, 2)\nsns.distplot(dataset_con.loc[dataset_con['label'] == 1]['meanfreq'], kde_kws={\"label\": \"male\"});\nsns.distplot(dataset_con.loc[dataset_con['label'] == 0]['meanfreq'], kde_kws={\"label\": \"female\"});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d617e7bfadac84d8a83c73d1315e31156d7e302"},"cell_type":"markdown","source":"### Feature: sd\n\nWe will use the Pandas Cut function to bin the data in equally sized buckets. We will also add our original feature to the dataset_con dataframe."},{"metadata":{"_uuid":"d5e8fe5a2c1747c975931ed309779c8d19e656cd","collapsed":true,"_cell_guid":"8363d99a-27b7-48d8-bb29-142739c59135","trusted":false},"cell_type":"code","source":"dataset_bin['sd'] = pd.cut(dataset_raw['sd'], 10) # discretised \ndataset_con['sd'] = dataset_raw['sd'] # non-discretised","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e209a4a234a945e2ad8e2ecc51fdaee05efec48","_cell_guid":"aa0ad6de-48f3-4b8a-87ca-86c24c5573ff","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(20,5)) \nplt.subplot(1, 2, 1)\nsns.countplot(y=\"sd\", data=dataset_bin);\nplt.subplot(1, 2, 2)\nsns.distplot(dataset_con.loc[dataset_con['label'] == 1]['sd'], kde_kws={\"label\": \"male\"});\nsns.distplot(dataset_con.loc[dataset_con['label'] == 0]['sd'], kde_kws={\"label\": \"female\"});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"315fd27ba193f97fa2a5dabcc2b3224cc7994778"},"cell_type":"markdown","source":"### Feature: median\n\nWe will use the Pandas Cut function to bin the data in equally sized buckets. We will also add our original feature to the dataset_con dataframe."},{"metadata":{"_uuid":"7e8286e648af70fc1dcf4d7d19f1e7e68c8ae3e6","collapsed":true,"_cell_guid":"d3c42036-a465-4261-9edd-fdc57743e127","trusted":false},"cell_type":"code","source":"dataset_bin['median'] = pd.cut(dataset_raw['median'], 10) # discretised \ndataset_con['median'] = dataset_raw['median'] # non-discretised","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"378a971f66c89c0dd20d93922e56bdcfc9998593","_cell_guid":"81977563-2e4d-4fcf-96f2-de7f95165039","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(20,5)) \nplt.subplot(1, 2, 1)\nsns.countplot(y=\"median\", data=dataset_bin);\nplt.subplot(1, 2, 2)\nsns.distplot(dataset_con.loc[dataset_con['label'] == 1]['median'], kde_kws={\"label\": \"male\"});\nsns.distplot(dataset_con.loc[dataset_con['label'] == 0]['median'], kde_kws={\"label\": \"female\"});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5cec2baa320c11896607e879cf53e9c95d624b80"},"cell_type":"markdown","source":"### Feature: Q25\n\nWe will use the Pandas Cut function to bin the data in equally sized buckets. We will also add our original feature to the dataset_con dataframe."},{"metadata":{"_uuid":"9d5c44119c8839c5964a6110a3bfde51634cb2c2","_cell_guid":"5f423617-a22c-4e2a-bd9f-011df2e982d2","trusted":false,"collapsed":true},"cell_type":"code","source":"dataset_bin['Q25'] = pd.cut(dataset_raw['Q25'], 10) # discretised \ndataset_con['Q25'] = dataset_raw['Q25'] # non-discretised\n\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(20,5)) \nplt.subplot(1, 2, 1)\nsns.countplot(y=\"Q25\", data=dataset_bin);\nplt.subplot(1, 2, 2)\nsns.distplot(dataset_con.loc[dataset_con['label'] == 1]['Q25'], kde_kws={\"label\": \"male\"});\nsns.distplot(dataset_con.loc[dataset_con['label'] == 0]['Q25'], kde_kws={\"label\": \"female\"});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04a86ffee451ce13e9769ef1527271b083dbc264"},"cell_type":"markdown","source":"### Feature: Q75\n\nWe will use the Pandas Cut function to bin the data in equally sized buckets. We will also add our original feature to the dataset_con dataframe."},{"metadata":{"_uuid":"1483e90d61251d61bee67b6e204f7744fc945e4e","_cell_guid":"7102ff8e-9d11-414d-9de9-50a455226fa5","trusted":false,"collapsed":true},"cell_type":"code","source":"dataset_bin['Q75'] = pd.cut(dataset_raw['Q75'], 10) # discretised \ndataset_con['Q75'] = dataset_raw['Q75'] # non-discretised\n\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(20,5)) \nplt.subplot(1, 2, 1)\nsns.countplot(y=\"Q75\", data=dataset_bin);\nplt.subplot(1, 2, 2)\nsns.distplot(dataset_con.loc[dataset_con['label'] == 1]['Q75'], kde_kws={\"label\": \"male\"});\nsns.distplot(dataset_con.loc[dataset_con['label'] == 0]['Q75'], kde_kws={\"label\": \"female\"});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"909d981ba59c789cd9e4682064430a5eaffb490d"},"cell_type":"markdown","source":"### Feature: IQR\n\nWe will use the Pandas Cut function to bin the data in equally sized buckets. We will also add our original feature to the dataset_con dataframe."},{"metadata":{"_uuid":"aa873f86343700e9ffc67e67d99fd77e97fd8589","_cell_guid":"a1391bd6-9516-4733-a0ef-4a17ab65a8e0","trusted":false,"collapsed":true},"cell_type":"code","source":"dataset_bin['IQR'] = pd.cut(dataset_raw['IQR'], 10) # discretised \ndataset_con['IQR'] = dataset_raw['IQR'] # non-discretised\n\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(20,5)) \nplt.subplot(1, 2, 1)\nsns.countplot(y=\"IQR\", data=dataset_bin);\nplt.subplot(1, 2, 2)\nsns.distplot(dataset_con.loc[dataset_con['label'] == 1]['IQR'], kde_kws={\"label\": \"male\"});\nsns.distplot(dataset_con.loc[dataset_con['label'] == 0]['IQR'], kde_kws={\"label\": \"female\"});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"693278c695008565b79e2d37e38d3e706a2ca983"},"cell_type":"markdown","source":"### Feature: skew\n\nWe will use the Pandas Cut function to bin the data in equally sized buckets. We will also add our original feature to the dataset_con dataframe."},{"metadata":{"_uuid":"b85140eddfaa31b27dbce5edc235186d0a8d1b49","_cell_guid":"e51c1a07-9a96-4dae-b98e-db9a04254e00","trusted":false,"collapsed":true},"cell_type":"code","source":"dataset_bin['skew'] = pd.cut(dataset_raw['skew'], 10) # discretised \ndataset_con['skew'] = dataset_raw['skew'] # non-discretised\n\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(20,5)) \nplt.subplot(1, 2, 1)\nsns.countplot(y=\"skew\", data=dataset_bin);\nplt.subplot(1, 2, 2)\nsns.distplot(dataset_con.loc[dataset_con['label'] == 1]['skew'], kde_kws={\"label\": \"male\"});\nsns.distplot(dataset_con.loc[dataset_con['label'] == 0]['skew'], kde_kws={\"label\": \"female\"});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b58d713fcb18dcb5d50ca3e5d3dc040f40f9fbb6"},"cell_type":"markdown","source":"### Feature: kurt\n\nWe will use the Pandas Cut function to bin the data in equally sized buckets. We will also add our original feature to the dataset_con dataframe."},{"metadata":{"_uuid":"2b2b38573ab0c0613d36b306aef9de61377f3521","_cell_guid":"4bb247d2-46d4-49d3-82d1-e88b9dc2fb8d","trusted":false,"collapsed":true},"cell_type":"code","source":"dataset_bin['kurt'] = pd.cut(dataset_raw['kurt'], 10) # discretised \ndataset_con['kurt'] = dataset_raw['kurt'] # non-discretised\n\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(20,5)) \nplt.subplot(1, 2, 1)\nsns.countplot(y=\"kurt\", data=dataset_bin);\nplt.subplot(1, 2, 2)\nsns.distplot(dataset_con.loc[dataset_con['label'] == 1]['kurt'], kde_kws={\"label\": \"male\"});\nsns.distplot(dataset_con.loc[dataset_con['label'] == 0]['kurt'], kde_kws={\"label\": \"female\"});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44175531131e42f38bf9089256dca04748e6c06b"},"cell_type":"markdown","source":"### Feature: sp.ent\n\nWe will use the Pandas Cut function to bin the data in equally sized buckets. We will also add our original feature to the dataset_con dataframe."},{"metadata":{"_uuid":"f9e72f14308e05a4a951a8840f355276ca738614","_cell_guid":"8e39f2fa-6997-40e2-85c1-75ab429d9b58","trusted":false,"collapsed":true},"cell_type":"code","source":"dataset_bin['sp.ent'] = pd.cut(dataset_raw['sp.ent'], 10) # discretised \ndataset_con['sp.ent'] = dataset_raw['sp.ent'] # non-discretised\n\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(20,5)) \nplt.subplot(1, 2, 1)\nsns.countplot(y=\"sp.ent\", data=dataset_bin);\nplt.subplot(1, 2, 2)\nsns.distplot(dataset_con.loc[dataset_con['label'] == 1]['sp.ent'], kde_kws={\"label\": \"male\"});\nsns.distplot(dataset_con.loc[dataset_con['label'] == 0]['sp.ent'], kde_kws={\"label\": \"female\"});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dbdd82147bf0104d428afd353f7cc9a3b7012e14"},"cell_type":"markdown","source":"### Feature: sfm\n\nWe will use the Pandas Cut function to bin the data in equally sized buckets. We will also add our original feature to the dataset_con dataframe."},{"metadata":{"_uuid":"db5d4cb1f3f581d2649167fc57d547de91038d8d","_cell_guid":"21e269a5-86a0-4665-87a1-da7e6194ab61","trusted":false,"collapsed":true},"cell_type":"code","source":"dataset_bin['sfm'] = pd.cut(dataset_raw['sfm'], 10) # discretised \ndataset_con['sfm'] = dataset_raw['sfm'] # non-discretised\n\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(20,5)) \nplt.subplot(1, 2, 1)\nsns.countplot(y=\"sfm\", data=dataset_bin);\nplt.subplot(1, 2, 2)\nsns.distplot(dataset_con.loc[dataset_con['label'] == 1]['sfm'], kde_kws={\"label\": \"male\"});\nsns.distplot(dataset_con.loc[dataset_con['label'] == 0]['sfm'], kde_kws={\"label\": \"female\"});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88ce8773c3f37490314f413daba4edc8f0e8f7d0"},"cell_type":"markdown","source":"### Feature: mode\n\nWe will use the Pandas Cut function to bin the data in equally sized buckets. We will also add our original feature to the dataset_con dataframe."},{"metadata":{"_uuid":"81dc7f3b7b126a49420ef44e03b27ec363aa9ae7","_cell_guid":"88aecda7-645b-4300-ad45-63cddec8f1dc","trusted":false,"collapsed":true},"cell_type":"code","source":"dataset_bin['mode'] = pd.cut(dataset_raw['mode'], 10) # discretised \ndataset_con['mode'] = dataset_raw['mode'] # non-discretised\n\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(20,5)) \nplt.subplot(1, 2, 1)\nsns.countplot(y=\"mode\", data=dataset_bin);\nplt.subplot(1, 2, 2)\nsns.distplot(dataset_con.loc[dataset_con['label'] == 1]['mode'], kde_kws={\"label\": \"male\"});\nsns.distplot(dataset_con.loc[dataset_con['label'] == 0]['mode'], kde_kws={\"label\": \"female\"});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3123dec0435d0bd192fde4ca053f690a128a7076"},"cell_type":"markdown","source":"### Feature: centroid\n\nWe will use the Pandas Cut function to bin the data in equally sized buckets. We will also add our original feature to the dataset_con dataframe."},{"metadata":{"_uuid":"624afeab725847697d4735aa71b4815f3d0189c0","_cell_guid":"3b3f96cc-9ee8-4e8f-a7c5-a2afe524e943","trusted":false,"collapsed":true},"cell_type":"code","source":"dataset_bin['centroid'] = pd.cut(dataset_raw['centroid'], 10) # discretised \ndataset_con['centroid'] = dataset_raw['centroid'] # non-discretised\n\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(20,5)) \nplt.subplot(1, 2, 1)\nsns.countplot(y=\"centroid\", data=dataset_bin);\nplt.subplot(1, 2, 2)\nsns.distplot(dataset_con.loc[dataset_con['label'] == 1]['centroid'], kde_kws={\"label\": \"male\"});\nsns.distplot(dataset_con.loc[dataset_con['label'] == 0]['centroid'], kde_kws={\"label\": \"female\"});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0e0fd639d39277b56c99ba7c08b66963d2e7022"},"cell_type":"markdown","source":"### Feature: meanfun\n\nWe will use the Pandas Cut function to bin the data in equally sized buckets. We will also add our original feature to the dataset_con dataframe."},{"metadata":{"_uuid":"6f2598b36bdb2a484090daac806cc1cfeacc680c","_cell_guid":"a5014db8-aa2b-42a6-97c7-abb0285d0848","trusted":false,"collapsed":true},"cell_type":"code","source":"dataset_bin['meanfun'] = pd.cut(dataset_raw['meanfun'], 10) # discretised \ndataset_con['meanfun'] = dataset_raw['meanfun'] # non-discretised\n\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(20,5)) \nplt.subplot(1, 2, 1)\nsns.countplot(y=\"meanfun\", data=dataset_bin);\nplt.subplot(1, 2, 2)\nsns.distplot(dataset_con.loc[dataset_con['label'] == 1]['meanfun'], kde_kws={\"label\": \"male\"});\nsns.distplot(dataset_con.loc[dataset_con['label'] == 0]['meanfun'], kde_kws={\"label\": \"female\"});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca3b8813e02ef9e2e4063e139ba54c197a2c95b1"},"cell_type":"markdown","source":"### Feature: minfun\n\nWe will use the Pandas Cut function to bin the data in equally sized buckets. We will also add our original feature to the dataset_con dataframe."},{"metadata":{"_uuid":"7d339af47f586e4a4c05de02851623492098d8f4","_cell_guid":"0576b583-a54f-471e-9e20-51811faacc94","trusted":false,"collapsed":true},"cell_type":"code","source":"dataset_bin['minfun'] = pd.cut(dataset_raw['minfun'], 10) # discretised \ndataset_con['minfun'] = dataset_raw['minfun'] # non-discretised\n\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(20,5)) \nplt.subplot(1, 2, 1)\nsns.countplot(y=\"minfun\", data=dataset_bin);\nplt.subplot(1, 2, 2)\nsns.distplot(dataset_con.loc[dataset_con['label'] == 1]['minfun'], kde_kws={\"label\": \"male\"});\nsns.distplot(dataset_con.loc[dataset_con['label'] == 0]['minfun'], kde_kws={\"label\": \"female\"});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7317d544675c9239daea0304c8cbcfc023dfdf5f"},"cell_type":"markdown","source":"### Feature: maxfun\n\nWe will use the Pandas Cut function to bin the data in equally sized buckets. We will also add our original feature to the dataset_con dataframe."},{"metadata":{"_uuid":"ba30b69ac3115746965eae9b701ccceef68d1cc9","_cell_guid":"6bfa555b-48ec-4c79-921f-cd1272c52de4","trusted":false,"collapsed":true},"cell_type":"code","source":"dataset_bin['maxfun'] = pd.cut(dataset_raw['maxfun'], 10) # discretised \ndataset_con['maxfun'] = dataset_raw['maxfun'] # non-discretised\n\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(20,5)) \nplt.subplot(1, 2, 1)\nsns.countplot(y=\"maxfun\", data=dataset_bin);\nplt.subplot(1, 2, 2)\nsns.distplot(dataset_con.loc[dataset_con['label'] == 1]['maxfun'], kde_kws={\"label\": \"male\"});\nsns.distplot(dataset_con.loc[dataset_con['label'] == 0]['maxfun'], kde_kws={\"label\": \"female\"});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e190302f04729387d2088abc54bb4aa00ad68c9"},"cell_type":"markdown","source":"### Feature: meandom\n\nWe will use the Pandas Cut function to bin the data in equally sized buckets. We will also add our original feature to the dataset_con dataframe."},{"metadata":{"_uuid":"b7da11784a2ee698364448d20bb224fd4b08c9b1","_cell_guid":"82c8d6d6-cd55-4b23-9c02-cd4efc357991","trusted":false,"collapsed":true},"cell_type":"code","source":"dataset_bin['meandom'] = pd.cut(dataset_raw['meandom'], 10) # discretised \ndataset_con['meandom'] = dataset_raw['meandom'] # non-discretised\n\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(20,5)) \nplt.subplot(1, 2, 1)\nsns.countplot(y=\"meandom\", data=dataset_bin);\nplt.subplot(1, 2, 2)\nsns.distplot(dataset_con.loc[dataset_con['label'] == 1]['meandom'], kde_kws={\"label\": \"male\"});\nsns.distplot(dataset_con.loc[dataset_con['label'] == 0]['meandom'], kde_kws={\"label\": \"female\"});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2be0d2c04f45911040ef9721bc442c2afd64b9c"},"cell_type":"markdown","source":"### Feature: mindom\n\nWe will use the Pandas Cut function to bin the data in equally sized buckets. We will also add our original feature to the dataset_con dataframe."},{"metadata":{"_uuid":"723203e19b992be1acc71194ec7032d89b7b8719","_cell_guid":"96df33dd-8181-4f2a-b769-5dc27ed8811b","trusted":false,"collapsed":true},"cell_type":"code","source":"dataset_bin['mindom'] = pd.cut(dataset_raw['mindom'], 10) # discretised \ndataset_con['mindom'] = dataset_raw['mindom'] # non-discretised\n\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(20,5)) \nplt.subplot(1, 2, 1)\nsns.countplot(y=\"mindom\", data=dataset_bin);\nplt.subplot(1, 2, 2)\nsns.distplot(dataset_con.loc[dataset_con['label'] == 1]['mindom'], kde_kws={\"label\": \"male\"});\nsns.distplot(dataset_con.loc[dataset_con['label'] == 0]['mindom'], kde_kws={\"label\": \"female\"});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f48be5eeb50040b4ffa695d0d34700b8604499fd"},"cell_type":"markdown","source":"### Feature: maxdom\n\nWe will use the Pandas Cut function to bin the data in equally sized buckets. We will also add our original feature to the dataset_con dataframe."},{"metadata":{"_uuid":"bf46df17de6a01548a1d542f3a2cbb51b6156c64","_cell_guid":"d41232cd-e423-4bdf-aac5-4e131f945ffe","trusted":false,"collapsed":true},"cell_type":"code","source":"dataset_bin['maxdom'] = pd.cut(dataset_raw['maxdom'], 10) # discretised \ndataset_con['maxdom'] = dataset_raw['maxdom'] # non-discretised\n\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(20,5)) \nplt.subplot(1, 2, 1)\nsns.countplot(y=\"maxdom\", data=dataset_bin);\nplt.subplot(1, 2, 2)\nsns.distplot(dataset_con.loc[dataset_con['label'] == 1]['maxdom'], kde_kws={\"label\": \"male\"});\nsns.distplot(dataset_con.loc[dataset_con['label'] == 0]['maxdom'], kde_kws={\"label\": \"female\"});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c07671d8024ab26631a140f03e6f68e2732389e1"},"cell_type":"markdown","source":"### Feature: dfrange\n\nWe will use the Pandas Cut function to bin the data in equally sized buckets. We will also add our original feature to the dataset_con dataframe."},{"metadata":{"_uuid":"f812a80623ce7cb7d6d562a1e2f0fb7833d2f656","_cell_guid":"a341485b-5bf2-4e42-8606-d10685d67527","trusted":false,"collapsed":true},"cell_type":"code","source":"dataset_bin['dfrange'] = pd.cut(dataset_raw['dfrange'], 10) # discretised \ndataset_con['dfrange'] = dataset_raw['dfrange'] # non-discretised\n\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(20,5)) \nplt.subplot(1, 2, 1)\nsns.countplot(y=\"dfrange\", data=dataset_bin);\nplt.subplot(1, 2, 2)\nsns.distplot(dataset_con.loc[dataset_con['label'] == 1]['dfrange'], kde_kws={\"label\": \"male\"});\nsns.distplot(dataset_con.loc[dataset_con['label'] == 0]['dfrange'], kde_kws={\"label\": \"female\"});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8ad912c2aae1bf54ac565baa3dddd79e83611ab"},"cell_type":"markdown","source":"### Feature: modindx\n\nWe will use the Pandas Cut function to bin the data in equally sized buckets. We will also add our original feature to the dataset_con dataframe."},{"metadata":{"_uuid":"d91921609bee937f1740fbd305868071639072da","_cell_guid":"9f333d49-0054-417b-8421-700295899502","trusted":false,"collapsed":true},"cell_type":"code","source":"dataset_bin['modindx'] = pd.cut(dataset_raw['modindx'], 10) # discretised \ndataset_con['modindx'] = dataset_raw['modindx'] # non-discretised\n\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(20,5)) \nplt.subplot(1, 2, 1)\nsns.countplot(y=\"modindx\", data=dataset_bin);\nplt.subplot(1, 2, 2)\nsns.distplot(dataset_con.loc[dataset_con['label'] == 1]['modindx'], kde_kws={\"label\": \"male\"});\nsns.distplot(dataset_con.loc[dataset_con['label'] == 0]['modindx'], kde_kws={\"label\": \"female\"});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5aeec608330386342ce0269f869cb7d6536c1dc3"},"cell_type":"markdown","source":"## Bi-variate Analysis\n\nSo far, we have analised all features individually. Let's now start combining some of these features together to obtain further insight into the interactions between them."},{"metadata":{"_uuid":"ee5d7ad1e18a641176ef14fe0ce3c1fbd2b382fc","_cell_guid":"6bca7469-af76-4d81-8bb9-594fa2d1c8ea","trusted":true,"collapsed":true},"cell_type":"code","source":"# Interaction between pairs of features.\n#todo select some features\nsns.pairplot(dataset_con[['meanfreq','sd','median','Q25','Q75','IQR', 'skew', 'kurt', 'label']], \n             hue=\"label\", \n             diag_kind=\"kde\",\n             size=4);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94b2119a542742d702365c04346ae5b596efa2e5"},"cell_type":"markdown","source":"## Feature Encoding\n\nRemember that Machine Learning algorithms perform Linear Algebra on Matrices, which means all features need have numeric values. The process of converting Categorical Features into values is called Encoding. Let's perform both One-Hot and Label encoding."},{"metadata":{"_uuid":"d664dfdf1d55f1e6deaa8ba8ab7c43f9d4faf17e","_cell_guid":"45263114-43c4-4096-b2f0-1c2fbf3c9ce2","trusted":false,"collapsed":true},"cell_type":"code","source":"# One Hot Encodes all labels before Machine Learning\none_hot_cols = dataset_bin.columns.tolist()\none_hot_cols.remove('label')\ndataset_bin_enc = pd.get_dummies(dataset_bin, columns=one_hot_cols)\n\ndataset_bin_enc.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ebf01b756256db6f4aa26507fd7061396f4c554","_cell_guid":"d4d4ca41-8751-4f5e-a867-e29d79f09c03","trusted":false,"collapsed":true},"cell_type":"code","source":"# Label Encode all labels\ndataset_con_enc = dataset_con.apply(LabelEncoder().fit_transform)\n\ndataset_con_enc.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ab996a0cd99af785483f94d524f473abdea08db"},"cell_type":"markdown","source":"## Feature Reduction / Selection\n\nOnce we have our features ready to use, we might find that the number of features available is too large to be run in a reasonable timeframe by our machine learning algorithms. There's a number of options available to us for feature reduction and feature selection.\n\n- **Dimensionality Reduction:**\n    - **Principal Component Analysis (PCA):** Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.\n    - **Singular Value Decomposition (SVD):** SVD is a factorization of a real or complex matrix. It is the generalization of the eigendecomposition of a positive semidefinite normal matrix (for example, a symmetric matrix with positive eigenvalues) to any m×n  matrix via an extension of the polar decomposition. It has many useful applications in signal processing and statistics.\n\n\n- **Feature Importance/Relevance:**\n    - **Filter Methods:** Filter type methods select features based only on general metrics like the correlation with the variable to predict. Filter methods suppress the least interesting variables. The other variables will be part of a classification or a regression model used to classify or to predict data. These methods are particularly effective in computation time and robust to overfitting.\n    - **Wrapper Methods:** Wrapper methods evaluate subsets of variables which allows, unlike filter approaches, to detect the possible interactions between variables. The two main disadvantages of these methods are : The increasing overfitting risk when the number of observations is insufficient. AND. The significant computation time when the number of variables is large.\n    - **Embedded Methods:** Embedded methods try to combine the advantages of both previous methods. A learning algorithm takes advantage of its own variable selection process and performs feature selection and classification simultaneously."},{"metadata":{"_uuid":"3b3945b14d191a8dcb509d8fa81686b8753fdd7f"},"cell_type":"markdown","source":"### Feature Correlation\n\nCorrelation ia s measure of how much two random variables change together. Features should be uncorrelated with each other and highly correlated to the feature we’re trying to predict."},{"metadata":{"_uuid":"9abf4e94391f914aa727315c795ab4016c11203f","_cell_guid":"4110a222-07e9-40db-82d5-18cf77dd4307","trusted":false,"collapsed":true},"cell_type":"code","source":"# Create a correlation plot of both datasets.\nplt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(25,10)) \n\nplt.subplot(1, 2, 1)\n# Generate a mask for the upper triangle\nmask = np.zeros_like(dataset_bin_enc.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(dataset_bin_enc.corr(), \n            vmin=-1, vmax=1, \n            square=True, \n            cmap=sns.color_palette(\"RdBu_r\", 100), \n            mask=mask, \n            linewidths=.5);\n\nplt.subplot(1, 2, 2)\nmask = np.zeros_like(dataset_con_enc.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(dataset_con_enc.corr(), \n            vmin=-1, vmax=1, \n            square=True, \n            cmap=sns.color_palette(\"RdBu_r\", 100), \n            mask=mask, \n            linewidths=.5);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a59ff6557e312353b8416cd6fc0aede75a8d26db"},"cell_type":"markdown","source":"### Feature Importance\n\nRandom forest consists of a number of decision trees. Every node in the decision trees is a condition on a single feature, designed to split the dataset into two so that similar response values end up in the same set. The measure based on which the (locally) optimal condition is chosen is called impurity. When training a tree, it can be computed how much each feature decreases the weighted impurity in a tree. For a forest, the impurity decrease from each feature can be averaged and the features are ranked according to this measure. This is the feature importance measure exposed in sklearn’s Random Forest implementations."},{"metadata":{"_uuid":"b8bb779e902a57cc64bf3923ce710829b33a9b8d","_cell_guid":"a5779a0f-b57d-41c4-9ef9-1740abb6278c","trusted":false,"collapsed":true},"cell_type":"code","source":"# Using Random Forest to gain an insight on Feature Importance\nclf = RandomForestClassifier()\nclf.fit(dataset_con_enc.drop('label', axis=1), dataset_con_enc['label'])\n\nplt.style.use('seaborn-whitegrid')\nimportance = clf.feature_importances_\nimportance = pd.DataFrame(importance, index=dataset_con_enc.drop('label', axis=1).columns, columns=[\"Importance\"])\nimportance.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(20,len(importance)/2));","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98ca10d0450c70844fd908a620b7fec4e4deee86"},"cell_type":"markdown","source":"### PCA\n\nPrincipal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.\n\nWe can use PCA to reduce the number of features to use in our ML algorithms, and graphing the variance gives us an idea of how many features we really need to represent our dataset fully."},{"metadata":{"_uuid":"0424818ceab8d73c70dd256351e705a7be575324","_cell_guid":"1cb8b065-4478-4970-8873-6c11844c3fd1","trusted":false,"collapsed":true},"cell_type":"code","source":"# Calculating PCA for both datasets, and graphing the Variance for each feature, per dataset\nstd_scale = preprocessing.StandardScaler().fit(dataset_bin_enc.drop('label', axis=1))\nX = std_scale.transform(dataset_bin_enc.drop('label', axis=1))\npca1 = PCA(n_components=len(dataset_bin_enc.columns)-1)\nfit1 = pca1.fit(X)\n\nstd_scale = preprocessing.StandardScaler().fit(dataset_con_enc.drop('label', axis=1))\nX = std_scale.transform(dataset_con_enc.drop('label', axis=1))\npca2 = PCA(n_components=len(dataset_con_enc.columns)-2)\nfit2 = pca2.fit(X)\n\n# Graphing the variance per feature\nplt.style.use('seaborn-whitegrid')\nplt.figure(figsize=(25,7)) \n\nplt.subplot(1, 2, 1)\nplt.xlabel('PCA Feature')\nplt.ylabel('Variance')\nplt.title('PCA for Discretised Dataset')\nplt.bar(range(0, fit1.explained_variance_ratio_.size), fit1.explained_variance_ratio_);\n\nplt.subplot(1, 2, 2)\nplt.xlabel('PCA Feature')\nplt.ylabel('Variance')\nplt.title('PCA for Continuous Dataset')\nplt.bar(range(0, fit2.explained_variance_ratio_.size), fit2.explained_variance_ratio_);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3acec24962c601f9c1ab7457062005a06bec8c1d","_cell_guid":"679770d8-5481-4a64-832c-94a7ece07416","trusted":false,"collapsed":true},"cell_type":"code","source":"# PCA's components graphed in 2D and 3D\n# Apply Scaling \nstd_scale = preprocessing.StandardScaler().fit(dataset_con_enc.drop('label', axis=1))\nX = std_scale.transform(dataset_con_enc.drop('label', axis=1))\ny = dataset_con_enc['label']\n\n# Formatting\ntarget_names = [0,1]\ncolors = ['navy','darkorange']\nlw = 2\nalpha = 0.3\n# 2 Components PCA\nplt.style.use('seaborn-whitegrid')\nplt.figure(2, figsize=(20, 8))\n\nplt.subplot(1, 2, 1)\npca = PCA(n_components=2)\nX_r = pca.fit(X).transform(X)\nfor color, i, target_name in zip(colors, [0, 1], target_names):\n    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], \n                color=color, \n                alpha=alpha, \n                lw=lw,\n                label=target_name)\nplt.legend(loc='best', shadow=False, scatterpoints=1)\nplt.title('First two PCA directions');\n\n# 3 Components PCA\nax = plt.subplot(1, 2, 2, projection='3d')\n\npca = PCA(n_components=3)\nX_reduced = pca.fit(X).transform(X)\nfor color, i, target_name in zip(colors, [0, 1], target_names):\n    ax.scatter(X_reduced[y == i, 0], X_reduced[y == i, 1], X_reduced[y == i, 2], \n               color=color,\n               alpha=alpha,\n               lw=lw, \n               label=target_name)\nplt.legend(loc='best', shadow=False, scatterpoints=1)\nax.set_title(\"First three PCA directions\")\nax.set_xlabel(\"1st eigenvector\")\nax.set_ylabel(\"2nd eigenvector\")\nax.set_zlabel(\"3rd eigenvector\")\n\n# rotate the axes\nax.view_init(30, 10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5959077c74a7628faad1b0a04adebe3f1f16b66"},"cell_type":"markdown","source":"### Recursive Feature Elimination\n\nFeature ranking with recursive feature elimination and cross-validated selection of the best number of features."},{"metadata":{"_uuid":"2d2118b3425c8a78539cbe03629b96253599855a","_cell_guid":"682f2d80-c27b-43b7-a6fa-6c93463b075a","trusted":false,"collapsed":true},"cell_type":"code","source":"# Calculating RFE for non-discretised dataset, and graphing the Importance for each feature, per dataset\nselector1 = RFECV(LogisticRegression(), step=1, cv=5, n_jobs=1)\nselector1 = selector1.fit(dataset_con_enc.drop('label', axis=1).values, dataset_con_enc['label'].values)\nprint(\"Feature Ranking For Non-Discretised: %s\" % selector1.ranking_)\nprint(\"Optimal number of features : %d\" % selector1.n_features_)\n# Plot number of features VS. cross-validation scores\nplt.style.use('seaborn-whitegrid')\nplt.figure(figsize=(20,5)) \nplt.xlabel(\"Number of features selected - Non-Discretised\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(selector1.grid_scores_) + 1), selector1.grid_scores_);\n\n# Feature space could be subsetted like so:\ndataset_con_enc = dataset_con_enc[dataset_con_enc.columns[np.insert(selector1.support_, 0, True)]]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5cef9da3ed71ec619b5815c8cc07c714815781e9"},"cell_type":"markdown","source":"## Selecting Dataset\n\nWe now have two datasets to choose from to apply our ML algorithms. The one-hot-encoded, and the label-encoded. For now, we have decided not to use feature reduction or selection algorithms."},{"metadata":{"_uuid":"5da44005acc4344f554010d15f2849ec864c14b5","collapsed":true,"_cell_guid":"da9e1adb-b037-49c7-97aa-0f429548ee5e","trusted":false},"cell_type":"code","source":"selected_dataset = dataset_con_enc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"519978cf4d6e63cb13b8ea75e6bd287b5e201729","_cell_guid":"a9c8b129-fc66-488c-9fac-58318c2c22e0","trusted":false,"collapsed":true},"cell_type":"code","source":"selected_dataset.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbf8097ce18bb38c4140a43f96d52d1cae0e6600"},"cell_type":"markdown","source":"## Splitting Data into Training and Testing Datasets\n\nWe need to split the data back into the training and testing datasets. Remember we joined both right at the beginning."},{"metadata":{"_uuid":"bcc3b8b2b3e2f8617e7e664d85800bac431663bb","collapsed":true,"_cell_guid":"d1f02d33-e5f0-4860-be18-a15ca72ef442","trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = selected_dataset.drop(['label'], axis=1)\ny = selected_dataset['label'].astype('int64')\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fc0f79b1ed06342623eec1c91c4da24158759c6"},"cell_type":"markdown","source":"## Machine Learning Algorithms"},{"metadata":{"_uuid":"48420c8db280ed14b6a574cb98d5b1ae0f91e6c7"},"cell_type":"markdown","source":"### Data Review\n\nLet's take one last peek at our data before we start running the Machine Learning algorithms."},{"metadata":{"_uuid":"31256519d0a34f2d6deef8fe1dff6dff7eb26df3","_cell_guid":"547a41a0-c5f7-4544-a768-0619ac83913b","trusted":false,"collapsed":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c247580def5a640f995cfdec7c56417dec480fd3","_cell_guid":"b9a74ce6-00b1-4646-bcd2-f6d1711ce9dd","trusted":false,"collapsed":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6fa986745083f4285367999274dbc666f97c0bf1","_cell_guid":"d6da6c2c-be2e-4413-a864-1667934c8acc","trusted":false,"collapsed":true},"cell_type":"code","source":"y_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6418408dddd2fe007a07f3fab2e42fc79474726e","collapsed":true,"_cell_guid":"d68c7196-d39a-4883-ae63-a47191b7d772","trusted":false},"cell_type":"code","source":"random.seed(1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d45800f595dd5d07bc02cfffb34da3c467abe290"},"cell_type":"markdown","source":"### Algorithms\n\nFrom here, we will be running the following algorithms.\n\n- KNN\n- Logistic Regression\n- Random Forest\n- Naive Bayes\n- Stochastic Gradient Decent\n- Linear SVC\n- Decision Tree\n- Gradient Boosted Trees\n\nBecause there's a great deal of repetitiveness on the code for each, we'll create a custom function to analyse this.\n\nFor some algorithms, we have also chosen to run a Random Hyperparameter search, to select the best hyperparameters for a given algorithm."},{"metadata":{"_uuid":"21236054facb254c95ece3b855526db1828a9efd","collapsed":true,"_cell_guid":"fe53c23a-678a-4d46-9d6c-fd8e09ca0e70","trusted":false},"cell_type":"code","source":"# calculate the fpr and tpr for all thresholds of the classification\ndef plot_roc_curve(y_test, preds):\n    fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n    roc_auc = metrics.auc(fpr, tpr)\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([-0.01, 1.01])\n    plt.ylim([-0.01, 1.01])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55ba815ab59d12c3ff50f51a346aa0be2b14c722","collapsed":true,"_cell_guid":"0cf7e1c4-e69e-4c80-9dbf-d65eda3c4505","trusted":false},"cell_type":"code","source":"# Function that runs the requested algorithm and returns the accuracy metrics\ndef fit_ml_algo(algo, X_train, y_train, X_test, cv):\n    # One Pass\n    model = algo.fit(X_train, y_train)\n    test_pred = model.predict(X_test)\n    if (isinstance(algo, (LogisticRegression, \n                          KNeighborsClassifier, \n                          GaussianNB, \n                          DecisionTreeClassifier, \n                          RandomForestClassifier,\n                          GradientBoostingClassifier))):\n        probs = model.predict_proba(X_test)[:,1]\n    else:\n        probs = \"Not Available\"\n    acc = round(model.score(X_test, y_test) * 100, 2) \n    # CV \n    train_pred = model_selection.cross_val_predict(algo, \n                                                  X_train, \n                                                  y_train, \n                                                  cv=cv, \n                                                  n_jobs = -1)\n    acc_cv = round(metrics.accuracy_score(y_train, train_pred) * 100, 2)\n    return train_pred, test_pred, acc, acc_cv, probs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad03250e9d1490a0e299055eed3f60760aff80e2","_cell_guid":"2c499bff-6571-4546-ac82-d3aa6ff9c92d","trusted":false,"collapsed":true},"cell_type":"code","source":"# Logistic Regression - Random Search for Hyperparameters\n\n# Utility function to report best scores\ndef report(results, n_top=5):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")\n            \n# Specify parameters and distributions to sample from\nparam_dist = {'penalty': ['l2', 'l1'], \n                         'class_weight': [None, 'balanced'],\n                         'C': np.logspace(-20, 20, 10000), \n                         'intercept_scaling': np.logspace(-20, 20, 10000)}\n\n# Run Randomized Search\nn_iter_search = 10\nlrc = LogisticRegression()\nrandom_search = RandomizedSearchCV(lrc, \n                                   param_distributions=param_dist, \n                                   n_iter=n_iter_search)\n\nstart = time.time()\nrandom_search.fit(X_train, y_train)\nprint(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n      \" parameter settings.\" % ((time.time() - start), n_iter_search))\nreport(random_search.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98bfd277d21ed788b305d90739647b0ae98d3a5a","_cell_guid":"e41b1802-c105-4934-a8ab-1bd7f544ac50","trusted":false,"collapsed":true},"cell_type":"code","source":"# Logistic Regression\nstart_time = time.time()\ntrain_pred_log, test_pred_log, acc_log, acc_cv_log, probs_log = fit_ml_algo(LogisticRegression(), \n                                                                 X_train, \n                                                                 y_train, \n                                                                 X_test, \n                                                                 10)\nlog_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_log)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_log)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=log_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"805f801152e536596beaa822cce5b0c626b26426","_cell_guid":"4bf5099e-a37f-45f9-a199-593b886a0688","trusted":false,"collapsed":true},"cell_type":"code","source":"print(metrics.classification_report(y_train, train_pred_log)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85e40986c41f5a7f9d2673bcc6e4909b8f8df906","_cell_guid":"94b78c02-ef6a-4166-85c6-e762be759212","trusted":false,"collapsed":true},"cell_type":"code","source":"print(metrics.classification_report(y_test, test_pred_log)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"011412a5cd21ebde2342000e8d8103c6ee37d654","_cell_guid":"038499bc-8aa7-4fa6-822a-6a3206b25c9e","trusted":false,"collapsed":true},"cell_type":"code","source":"plot_roc_curve(y_test, probs_log)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd7a9a0060328e3f287f99943ab6215f787035cf","_cell_guid":"73adc086-bc9c-4b96-8e5a-837cccd58dcb","trusted":false,"collapsed":true},"cell_type":"code","source":"# k-Nearest Neighbors\nstart_time = time.time()\ntrain_pred_knn, test_pred_knn, acc_knn, acc_cv_knn, probs_knn = fit_ml_algo(KNeighborsClassifier(n_neighbors = 3), \n                                                                                                 X_train, \n                                                                                                 y_train, \n                                                                                                 X_test, \n                                                                                                 10)\nknn_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_knn)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_knn)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=knn_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6b38e3ed2281a4d118cc5fff14a76c15c06e976","_cell_guid":"14822748-4ccc-4b0e-96b6-1d8e65b21d8e","trusted":false,"collapsed":true},"cell_type":"code","source":"print(metrics.classification_report(y_train, train_pred_knn)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05c58347bc98dbafeda24ca7e653443165610f53","_cell_guid":"f36f2c42-6f73-41a4-80ad-b28ef958f01f","trusted":false,"collapsed":true},"cell_type":"code","source":"print(metrics.classification_report(y_test, test_pred_knn)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb0222ea120a46aeaf720192133f6ae082ec7390","_cell_guid":"02a6b9cc-9a45-454a-9485-66d7465a0010","trusted":false,"collapsed":true},"cell_type":"code","source":"plot_roc_curve(y_test, probs_knn)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81cda4cb5fccf3a62fd4a4cf45ffaa7d6bf03441","_cell_guid":"bbf154de-57c7-450a-9478-5ab42f2eaa08","trusted":false,"collapsed":true},"cell_type":"code","source":"# Gaussian Naive Bayes\nstart_time = time.time()\ntrain_pred_gaussian, test_pred_gaussian, acc_gaussian, acc_cv_gaussian, probs_gau = fit_ml_algo(GaussianNB(), \n                                                                                     X_train, \n                                                                                     y_train, \n                                                                                     X_test, \n                                                                                     10)\ngaussian_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_gaussian)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_gaussian)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=gaussian_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7dbd406af896ee493895644da4b6fc1c5ccbf17","_cell_guid":"01cb6fa2-7a1a-4a1c-9fdc-9c3441fdc430","trusted":false,"collapsed":true},"cell_type":"code","source":"print(metrics.classification_report(y_train, train_pred_gaussian)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9da5a35447eba8ee36ee1b86ffd9a26cc7e5521","_cell_guid":"65d957b1-4498-4625-adfe-a18f1127476e","trusted":false,"collapsed":true},"cell_type":"code","source":"print(metrics.classification_report(y_test, test_pred_gaussian))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df7f54e0467cb914f6e592a8e7071cf4f7d989c5","_cell_guid":"e368498e-8595-4747-af7e-91da25c08b85","trusted":false,"collapsed":true},"cell_type":"code","source":"plot_roc_curve(y_test, probs_gau)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b55b77f8440dff79efff8b6159c84135d6e56f8","_cell_guid":"862d920c-2ad2-4649-88b5-5f37606621c4","trusted":false,"collapsed":true},"cell_type":"code","source":"# Linear SVC\nstart_time = time.time()\ntrain_pred_svc, test_pred_svc, acc_linear_svc, acc_cv_linear_svc, _ = fit_ml_algo(LinearSVC(),\n                                                                                           X_train, \n                                                                                           y_train,\n                                                                                           X_test, \n                                                                                           10)\nlinear_svc_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_linear_svc)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_linear_svc)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=linear_svc_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"faaae83978c8a7f741497a049409320fa86c2f1c","_cell_guid":"57d61c23-636f-47cc-8e3b-0c5463361d9d","trusted":false,"collapsed":true},"cell_type":"code","source":"print(metrics.classification_report(y_train, train_pred_svc)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12c5eaad13b96949bf9c6d0dcbe604316b8a4c0f","_cell_guid":"cc5a702f-d2e4-4eaa-add5-ebd28d252f45","trusted":false,"collapsed":true},"cell_type":"code","source":"print(metrics.classification_report(y_test, test_pred_svc)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5f9259e23ce2d2bd761665ebfd78e4154ba4335","_cell_guid":"102389e0-8405-478e-ae13-a1283df3fa98","trusted":false,"collapsed":true},"cell_type":"code","source":"# Stochastic Gradient Descent\nstart_time = time.time()\ntrain_pred_sgd, test_pred_sgd, acc_sgd, acc_cv_sgd, _ = fit_ml_algo(SGDClassifier(), \n                                                                 X_train, \n                                                                 y_train, \n                                                                 X_test, \n                                                                 10)\nsgd_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_sgd)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_sgd)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=sgd_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"325142dd6fc5973a51999d96d8214b2f3784b57c","_cell_guid":"40b32cab-f65b-4955-9e11-9d7de6961d20","trusted":false,"collapsed":true},"cell_type":"code","source":"print(metrics.classification_report(y_train, train_pred_sgd)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bfe12dbc44a6a68f92fed8afa3b4d51801745a5","_cell_guid":"56ed3e5c-b0b3-4684-bba0-f04a1e56bb81","trusted":false,"collapsed":true},"cell_type":"code","source":"print(metrics.classification_report(y_test, test_pred_sgd)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e35c54675a068c05bbe929fd66d2120533c7a05c","_cell_guid":"bee9c09d-b44c-4684-bc6a-b5e9d34130a3","trusted":false,"collapsed":true},"cell_type":"code","source":"# Decision Tree Classifier\nstart_time = time.time()\ntrain_pred_dt, test_pred_dt, acc_dt, acc_cv_dt, probs_dt = fit_ml_algo(DecisionTreeClassifier(), \n                                                             X_train, \n                                                             y_train, \n                                                             X_test, \n                                                             10)\ndt_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_dt)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_dt)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=dt_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35ab8b1db3bc7b0c43d14c4bef39bc76d134401b","_cell_guid":"b1a56eb7-8261-40a3-abd4-03e8b488dc98","trusted":false,"collapsed":true},"cell_type":"code","source":"print(metrics.classification_report(y_train, train_pred_dt)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecf461dc5bf8f964d5d87428f5ecc75dd4edc784","_cell_guid":"dbefe5d6-1229-4141-aa46-1565c6518097","trusted":false,"collapsed":true},"cell_type":"code","source":"print(metrics.classification_report(y_test, test_pred_dt)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4b146a6822aad1f42b3257c46dd5de68168c074","_cell_guid":"5d957a7a-624b-47df-9339-fe53cdc26388","trusted":false,"collapsed":true},"cell_type":"code","source":"plot_roc_curve(y_test, probs_dt)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1239660a03af6ec4a68048ef907896556c99915a","_cell_guid":"f7024933-6b78-4fbb-8ed6-d4ab13b49ce7","trusted":false,"collapsed":true},"cell_type":"code","source":"# Random Forest Classifier - Random Search for Hyperparameters\n\n# Utility function to report best scores\ndef report(results, n_top=5):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")\n            \n# Specify parameters and distributions to sample from\nparam_dist = {\"max_depth\": [10, None],\n              \"max_features\": sp_randint(1, 7),\n              \"min_samples_split\": sp_randint(2, 20),\n              \"min_samples_leaf\": sp_randint(1, 11),\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# Run Randomized Search\nn_iter_search = 10\nrfc = RandomForestClassifier(n_estimators=10)\nrandom_search = RandomizedSearchCV(rfc, \n                                   param_distributions=param_dist, \n                                   n_iter=n_iter_search)\n\nstart = time.time()\nrandom_search.fit(X_train, y_train)\nprint(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n      \" parameter settings.\" % ((time.time() - start), n_iter_search))\nreport(random_search.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80278282edc1140b64abc2fb0e5264b19777ef71","_cell_guid":"d708d338-3886-40f2-bbab-eefd090a72f1","trusted":false,"collapsed":true},"cell_type":"code","source":"# Random Forest Classifier\nstart_time = time.time()\nrfc = RandomForestClassifier(n_estimators=10, \n                             min_samples_leaf=2,\n                             min_samples_split=17, \n                             criterion='gini', \n                             max_features=8)\ntrain_pred_rf, test_pred_rf, acc_rf, acc_cv_rf, probs_rf = fit_ml_algo(rfc, \n                                                             X_train, \n                                                             y_train, \n                                                             X_test, \n                                                             10)\nrf_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_rf)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_rf)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=rf_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6e33c03e18c1392d1121c887a6ec160a9d5f938","_cell_guid":"8e898ca0-b482-4413-9722-ba99ce4b1807","trusted":false,"collapsed":true},"cell_type":"code","source":"print(metrics.classification_report(y_train, train_pred_rf)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a02a516ca30163fc75183c2f24dfc9d1a57e612c","_cell_guid":"f252cf7f-1550-4f30-90d1-c5b64dc0f0f3","trusted":false,"collapsed":true},"cell_type":"code","source":"print(metrics.classification_report(y_test, test_pred_rf)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f74b114efc26a8a4d2a6b8cbaec73cb522d1f5a","_cell_guid":"d85416c7-c62b-466c-94dd-d2e290eae4c6","trusted":false,"collapsed":true},"cell_type":"code","source":"plot_roc_curve(y_test, probs_rf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98bae6a1a49ca2bb8489f19711fdf840ebd27aa4","_cell_guid":"e6db47c4-5737-41b9-aa24-ecd39362a4b1","trusted":false,"collapsed":true},"cell_type":"code","source":"# Gradient Boosting Trees\nstart_time = time.time()\ntrain_pred_gbt, test_pred_gbt, acc_gbt, acc_cv_gbt, probs_gbt = fit_ml_algo(GradientBoostingClassifier(), \n                                                                 X_train, \n                                                                 y_train, \n                                                                 X_test, \n                                                                 10)\ngbt_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_gbt)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_gbt)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=gbt_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b761476842ab04b88fddea3714723408ac7add8","_cell_guid":"737a7367-fc8a-4d3b-a262-5e2b65adb81c","trusted":false,"collapsed":true},"cell_type":"code","source":"print(metrics.classification_report(y_train, train_pred_gbt)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f314e5360ec0d2dd1a3b35e6e62272224a44eb4","_cell_guid":"d05620d3-4864-4bff-a1f1-6fedac58a6e2","trusted":false,"collapsed":true},"cell_type":"code","source":"print(metrics.classification_report(y_test, test_pred_gbt)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9656ba3f92dcf70b8a329a604eb0edf00a743f68","_cell_guid":"18a8dae2-7c36-4db4-a0ac-39aae76305e0","trusted":false,"collapsed":true},"cell_type":"code","source":"plot_roc_curve(y_test, probs_gbt)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a15ffcb1b2f91f16ab55a74b7bbfcfcf7562182"},"cell_type":"markdown","source":"## Ranking Results\n\nLet's rank the results for all the algorithms we have used"},{"metadata":{"_uuid":"39dfc0ad5f9d4688188f1a81c293620f7f19e047","_cell_guid":"942fef74-19fa-440f-8e78-16c0859a1d75","trusted":false,"collapsed":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Trees'],\n    'Score': [\n        acc_knn, \n        acc_log, \n        acc_rf, \n        acc_gaussian, \n        acc_sgd, \n        acc_linear_svc, \n        acc_dt,\n        acc_gbt\n    ]})\nmodels.sort_values(by='Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d07cf7b8811d731395f7cd04587c6fb6ad11525","_cell_guid":"589683e3-ae0f-49d1-8194-77fe7a89b646","trusted":false,"collapsed":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Trees'],\n    'Score': [\n        acc_cv_knn, \n        acc_cv_log,     \n        acc_cv_rf, \n        acc_cv_gaussian, \n        acc_cv_sgd, \n        acc_cv_linear_svc, \n        acc_cv_dt,\n        acc_cv_gbt\n    ]})\nmodels.sort_values(by='Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33d9ed70f14de40b9180f2934061f00fa130a188","_cell_guid":"5bd20bfe-0225-4a91-a8c2-14fc630f4f4c","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(10,10)) \n\nmodels = [\n    'KNN', \n    'Logistic Regression', \n    'Random Forest', \n    'Naive Bayes', \n    'Decision Tree', \n    'Gradient Boosting Trees'\n]\nprobs = [\n    probs_knn,\n    probs_log,\n    probs_rf,\n    probs_gau,\n    probs_dt,\n    probs_gbt\n]\ncolors = [\n    'blue',\n    'green',\n    'red',\n    'cyan',\n    'magenta',\n    'yellow',\n]\n    \nplt.title('Receiver Operating Characteristic')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([-0.01, 1.01])\nplt.ylim([-0.01, 1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\ndef plot_roc_curves(y_test, prob, model):\n    fpr, tpr, threshold = metrics.roc_curve(y_test, prob)\n    roc_auc = metrics.auc(fpr, tpr)\n    plt.plot(fpr, tpr, 'b', label = model + ' AUC = %0.2f' % roc_auc, color=colors[i])\n    plt.legend(loc = 'lower right')\n    \nfor i, model in list(enumerate(models)):\n    plot_roc_curves(y_test, probs[i], models[i])\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ebb2b797112ff286964444fe3ce00af4b7b7ef2"},"cell_type":"markdown","source":"## Tensorflow - Logistic Regression\n\nReference: https://www.tensorflow.org/tutorials/wide"},{"metadata":{"_uuid":"003fcafd08ccc7c3c0ec3537036228fa78aadb88"},"cell_type":"markdown","source":"### Converting Data into Tensors\nWhen building a TF.Learn model, the input data is specified by means of an Input Builder function. This builder function will not be called until it is later passed to TF.Learn methods such as fit and evaluate. The purpose of this function is to construct the input data, which is represented in the form of tf.Tensors or tf.SparseTensors. In more detail, the Input Builder function returns the following as a pair:\n\n- feature_cols: A dict from feature column names to Tensors or SparseTensors.\n- label: A Tensor containing the label column."},{"metadata":{"_uuid":"241b173db3ef260cf5fdcd8cf2242bed67c0ad26","_cell_guid":"1614a937-ac27-4dd9-ba5e-ca1ad06f4989","trusted":false,"collapsed":true},"cell_type":"code","source":"df1 = pd.DataFrame(dataset_con.dtypes, columns=['Continuous Type'])\ndf2 = pd.DataFrame(dataset_bin.dtypes, columns=['Discretised Type'])\npd.concat([df1, df2], axis=1).transpose()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d212a5afeea31262f0e3ce263427850b4b72fbe","collapsed":true,"_cell_guid":"f6ab8c3c-d5f4-494d-8e85-d2bc1bbc91b4","trusted":false},"cell_type":"code","source":"# Selecting the Continuous Dataset\nLABEL_COLUMN = \"label\"\ndataset_con[LABEL_COLUMN] = dataset_con[\"label\"].astype(int)\n\nCONTINUOUS_COLUMNS = dataset_con.select_dtypes(include=[np.number]).columns.tolist()\nCATEGORICAL_COLUMNS = [\n]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc1c8f9fad97b3db11da476cc2f23b1cb8d43ae3","_cell_guid":"b9f7dea7-d016-4d08-b5e1-f6e265942788","trusted":false,"collapsed":true},"cell_type":"code","source":"# Missing Values\nmissingno.matrix(dataset_con, figsize = (30,5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7e6cb27504177aacf71831470591906bb2aab86","collapsed":true,"_cell_guid":"ea441d99-f973-49a0-bb8e-fb7a6a3ec0f2","trusted":false},"cell_type":"code","source":"# Splitting the Training and Test data sets\ntrain = dataset_con.loc[0:2900,:]\ntest = dataset_con.loc[2900:,:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab259170b996490fe866b0b064e90b73fa854113","collapsed":true,"_cell_guid":"7a7f8341-99bb-46b5-b592-54340182b563","trusted":false},"cell_type":"code","source":"# Dropping rows with Missing Values\ntrain = train.dropna(axis=0)\ntest = test.dropna(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffb8afa1ba19be11244d175cc7c6594e0a90a576","collapsed":true,"_cell_guid":"1a050ac7-bb10-4e63-b6ff-8b24dd3413a9","trusted":false},"cell_type":"code","source":"# Coverting Dataframes into Tensors\ndef input_fn(df):\n  # Creates a dictionary mapping from each continuous feature column name (k) to\n  # the values of that column stored in a constant Tensor.\n  continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n  # Creates a dictionary mapping from each categorical feature column name (k)\n  # to the values of that column stored in a tf.SparseTensor.\n  categorical_cols = {k: tf.SparseTensor(\n      indices=[[i, 0] for i in range(df[k].size)],\n      values=df[k].values,\n      dense_shape=[df[k].size, 1]) for k in CATEGORICAL_COLUMNS\n                     }\n  # Merges the two dictionaries into one.\n  d = continuous_cols.copy()\n  d.update(categorical_cols)\n  feature_cols = d\n  # Converts the label column into a constant Tensor.\n  label = tf.constant(df[LABEL_COLUMN].values)\n  # Returns the feature columns and the label.\n  return feature_cols, label\n\ndef train_input_fn():\n  return input_fn(train)\n\ndef eval_input_fn():\n  return input_fn(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c567c78f945b6da5ec2df1f7b14715b335f5c72"},"cell_type":"markdown","source":"### Base Categorical Feature Columns\nTo define a feature column for a categorical feature, we can create a SparseColumn using the TF.Learn API. If you know the set of all possible feature values of a column and there are only a few of them, you can use sparse_column_with_keys. Each key in the list will get assigned an auto-incremental ID starting from 0. If we don't know the set of possible values in advance, we can use sparse_column_with_hash_bucket instead:"},{"metadata":{"_uuid":"8dfe9c19824f6a35a6c7d18f1bf3036209e9bc81","_cell_guid":"c18356b8-eb69-486c-aaac-3e2f3a86fa78","trusted":false,"collapsed":true},"cell_type":"code","source":"# Listing categorical classes for reference\ntrain.select_dtypes(include=[np.object]).columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74617103a419b1de60c6b12548d69503bad3c30b"},"cell_type":"markdown","source":"### Base Continuous Feature Columns\nSimilarly, we can define a RealValuedColumn for each continuous feature column that we want to use in the model:"},{"metadata":{"_uuid":"b7eb30ce5c283dc5cbff49061dbcedf8c89a902c","_cell_guid":"a0dda063-735b-4a9f-83f3-a0e1a844ae87","trusted":false,"collapsed":true},"cell_type":"code","source":"train.select_dtypes(include=[np.number]).columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae222a547b57e3d458588f9350d09470c31189c7","collapsed":true,"_cell_guid":"74d372ad-50d6-4196-8d14-ce8ff5bf907d","trusted":false},"cell_type":"code","source":"#IQR\tsfm\tmeanfun\tminfun\tmaxfun\tmindom\tmaxdom\tdfrange\nIQR = tf.contrib.layers.real_valued_column(\"IQR\")\nsfm = tf.contrib.layers.real_valued_column(\"sfm\")\nmeanfun = tf.contrib.layers.real_valued_column(\"meanfun\")\nminfun = tf.contrib.layers.real_valued_column(\"minfun\")\nmaxfun = tf.contrib.layers.real_valued_column(\"maxfun\")\nmindom = tf.contrib.layers.real_valued_column(\"mindom\")\nmaxdom = tf.contrib.layers.real_valued_column(\"maxdom\")\ndfrange = tf.contrib.layers.real_valued_column(\"dfrange\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcd27f429f8a1203be04dcece8a4cc399098a81b"},"cell_type":"markdown","source":"### Defining The Logistic Regression Model\n\nAfter processing the input data and defining all the feature columns, we're now ready to put them all together and build a Logistic Regression model."},{"metadata":{"_uuid":"5d5070dc1f36df91e2703f85a9c4e99384c60bd9","_cell_guid":"9f5df76e-518d-4726-9a10-7b48a6f21bc2","trusted":false,"collapsed":true},"cell_type":"code","source":"model_dir = tempfile.mkdtemp()\nm = tf.contrib.learn.LinearClassifier(feature_columns=[\nIQR,\nsfm,\nmeanfun,\nminfun,\nmaxfun,\nmindom,\nmaxdom,\ndfrange\n    ],\n    model_dir=model_dir)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"238d61203ca5c8ebc02c606f8dd512b4c4ef9694"},"cell_type":"markdown","source":"### Training and Evaluating Our Model\n\nAfter adding all the features to the model, now let's look at how to actually train the model. Training a model is just a one-liner using the TF.Learn API:"},{"metadata":{"_uuid":"ef91fb391b765bd5dfdfe09d17ad953205182a4f","_cell_guid":"e908faa3-9a4d-4aa3-850f-23cea9183a9a","trusted":false,"collapsed":true},"cell_type":"code","source":"m.fit(input_fn=train_input_fn, steps=200)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b8c2ba95634b8651e431c170559c114c955b07a","collapsed":true,"_cell_guid":"ff16ed9a-beef-443d-a761-f4ee1d8c2730","trusted":false},"cell_type":"code","source":"results = m.evaluate(input_fn=eval_input_fn, steps=1)\nfor key in sorted(results):\n    print(\"%s: %s\" % (key, results[key]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8bb6b521c3b629dff8dad1fd8bdf2cd374f1d37d","collapsed":true,"_cell_guid":"b5cb93bf-12c5-450e-98bd-bbfb872b6b75","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}