{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport string\nimport re\nfrom collections import Counter\nfrom keras.preprocessing.text import Tokenizer\nfrom nltk.corpus import stopwords\nfrom keras.preprocessing.text import Tokenizer\nfrom nltk import PorterStemmer as Stemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)\nsms = pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\",encoding = 'latin-1')\nsms = pd.DataFrame({'label' : sms['v1'],'text' : sms['v2']})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms.head(n=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms.groupby('label').describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms = sms.drop_duplicates(subset = 'text')\nsms.groupby('label').describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms['length'] = sms['text'].apply(len)\nsms.hist(column='length',by='label',bins=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_doc(docs): # Test with removing lowercase, filter 1 letter word, stemming\n    # Split into tokens\n    tokens = docs.split()\n    # Remove punctuation, in words or stand alone\n    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n    tokens = [re_punc.sub(' ',w) for w in tokens]\n    # Remove Stopwords\n    tokens = [i for i in tokens if i not in stopwords.words('english')]\n    # Lowercase\n    tokens = [i.lower() for i in tokens]\n    # Remove non-alphabetic entries:\n    tokens = [i for i in tokens if i.isalpha()]\n    # filter out 1 letter words\n    # tokens = [i for i in tokens if len(i)>1]\n    # Stemming\n    #st = Stemmer()\n    #tokens = [st.stem(t) for t in tokens]\n    return(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sms['text'][9])\nprint(clean_doc(sms['text'][9]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vec = CountVectorizer(analyzer=clean_doc)\nX = vec.fit_transform(sms['text'])\nprint('Size of vocabulary is: '+str(len(vec.get_feature_names())))\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(vec.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([\n('bow',CountVectorizer(analyzer=clean_doc)), \n# Since v0.21, if input is filename or file, the data is first read from the file \n# and then passed to the given callable analyzer.\n('classifier',MultinomialNB())\n])\n\npipe2 = Pipeline([\n('tfidf',TfidfVectorizer(analyzer=clean_doc)), \n# Since v0.21, if input is filename or file, the data is first read from the file \n# and then passed to the given callable analyzer.\n('classifier',BernoulliNB())\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(sms['text'],sms['label'],test_size = 0.2,random_state = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = pipe.predict(X_test)\nacc = sum(pred == y_test)\nprecent_acc = (acc/len(y_test)) * 100\nprecent_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(X_test[y_test != pred]))\nprint(len(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wrong_pred = pd.DataFrame({'text' : X_test[pred != y_test],'Prediction': pred[pred != y_test],'True_value':y_test[pred != y_test]})\nwrong_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe.predict(['WIN URGENT! Your mobile number has been awarded with a å£2000 prize GUARANTEED call 09061790121 from land line. claim 3030 valid 12hrs only 150ppm '])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}