{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Overview\n\nI completed this project as part of my Data Science course at the Flatiron School. Check it out! https://flatironschool.com/\n\nThis project uses a Kaggle dataset to predict gene classifications. In this dataset, we are given multiple genetic variants and various properties of each. Expert raters at different laboratories rated these variants based on their perceived clinical classifications, with ratings ranging from Benign to Pathogenic. The target variable is whether the raters have clinical classifications that are concordant, meaning that they are in the same clinical category.\n\nI approach this with the OMESN framework. Data cleaning turns out to be the most substantial part of this project. In the end, I test a few different modeling approaches and present the results of the highest scoring model."},{"metadata":{},"cell_type":"markdown","source":"## Initialization"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Obtaining the Data\n\nFor this project, I downloaded the dataset from the Kaggle page as a csv."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/clinvar_conflicting.csv')\ndf.head()","execution_count":3,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (0,38,40) have mixed types. Specify dtype option on import or set low_memory=False.\n  interactivity=interactivity, compiler=compiler, result=result)\n","name":"stderr"},{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"  CHROM     POS REF ALT   ...     LoFtool  CADD_PHRED  CADD_RAW BLOSUM62\n0     1  955563   G   C   ...       0.421      11.390  1.133255     -2.0\n1     1  955597   G   T   ...       0.421       8.150  0.599088      NaN\n2     1  955619   G   C   ...       0.421       3.288  0.069819      1.0\n3     1  957640   C   T   ...       0.421      12.560  1.356499      NaN\n4     1  976059   C   T   ...       0.421      17.740  2.234711      NaN\n\n[5 rows x 46 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CHROM</th>\n      <th>POS</th>\n      <th>REF</th>\n      <th>ALT</th>\n      <th>AF_ESP</th>\n      <th>AF_EXAC</th>\n      <th>AF_TGP</th>\n      <th>CLNDISDB</th>\n      <th>CLNDISDBINCL</th>\n      <th>CLNDN</th>\n      <th>CLNDNINCL</th>\n      <th>CLNHGVS</th>\n      <th>CLNSIGINCL</th>\n      <th>CLNVC</th>\n      <th>CLNVI</th>\n      <th>MC</th>\n      <th>ORIGIN</th>\n      <th>SSR</th>\n      <th>CLASS</th>\n      <th>Allele</th>\n      <th>Consequence</th>\n      <th>IMPACT</th>\n      <th>SYMBOL</th>\n      <th>Feature_type</th>\n      <th>Feature</th>\n      <th>BIOTYPE</th>\n      <th>EXON</th>\n      <th>INTRON</th>\n      <th>cDNA_position</th>\n      <th>CDS_position</th>\n      <th>Protein_position</th>\n      <th>Amino_acids</th>\n      <th>Codons</th>\n      <th>DISTANCE</th>\n      <th>STRAND</th>\n      <th>BAM_EDIT</th>\n      <th>SIFT</th>\n      <th>PolyPhen</th>\n      <th>MOTIF_NAME</th>\n      <th>MOTIF_POS</th>\n      <th>HIGH_INF_POS</th>\n      <th>MOTIF_SCORE_CHANGE</th>\n      <th>LoFtool</th>\n      <th>CADD_PHRED</th>\n      <th>CADD_RAW</th>\n      <th>BLOSUM62</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>955563</td>\n      <td>G</td>\n      <td>C</td>\n      <td>0.0000</td>\n      <td>0.00000</td>\n      <td>0.0000</td>\n      <td>MedGen:C3808739,OMIM:615120|MedGen:CN169374</td>\n      <td>NaN</td>\n      <td>Myasthenic_syndrome,_congenital,_8|not_specified</td>\n      <td>NaN</td>\n      <td>NC_000001.10:g.955563G&gt;C</td>\n      <td>NaN</td>\n      <td>single_nucleotide_variant</td>\n      <td>NaN</td>\n      <td>SO:0001583|missense_variant</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>C</td>\n      <td>missense_variant</td>\n      <td>MODERATE</td>\n      <td>AGRN</td>\n      <td>Transcript</td>\n      <td>NM_001305275.1</td>\n      <td>protein_coding</td>\n      <td>1/39</td>\n      <td>NaN</td>\n      <td>61</td>\n      <td>11</td>\n      <td>4</td>\n      <td>R/P</td>\n      <td>cGg/cCg</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.421</td>\n      <td>11.390</td>\n      <td>1.133255</td>\n      <td>-2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>955597</td>\n      <td>G</td>\n      <td>T</td>\n      <td>0.0000</td>\n      <td>0.42418</td>\n      <td>0.2826</td>\n      <td>MedGen:CN169374</td>\n      <td>NaN</td>\n      <td>not_specified</td>\n      <td>NaN</td>\n      <td>NC_000001.10:g.955597G&gt;T</td>\n      <td>NaN</td>\n      <td>single_nucleotide_variant</td>\n      <td>NaN</td>\n      <td>SO:0001819|synonymous_variant</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>T</td>\n      <td>synonymous_variant</td>\n      <td>LOW</td>\n      <td>AGRN</td>\n      <td>Transcript</td>\n      <td>NM_001305275.1</td>\n      <td>protein_coding</td>\n      <td>1/39</td>\n      <td>NaN</td>\n      <td>95</td>\n      <td>45</td>\n      <td>15</td>\n      <td>P</td>\n      <td>ccG/ccT</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.421</td>\n      <td>8.150</td>\n      <td>0.599088</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>955619</td>\n      <td>G</td>\n      <td>C</td>\n      <td>0.0000</td>\n      <td>0.03475</td>\n      <td>0.0088</td>\n      <td>MedGen:C3808739,OMIM:615120|MedGen:CN169374</td>\n      <td>NaN</td>\n      <td>Myasthenic_syndrome,_congenital,_8|not_specified</td>\n      <td>NaN</td>\n      <td>NC_000001.10:g.955619G&gt;C</td>\n      <td>NaN</td>\n      <td>single_nucleotide_variant</td>\n      <td>NaN</td>\n      <td>SO:0001583|missense_variant</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>C</td>\n      <td>missense_variant</td>\n      <td>MODERATE</td>\n      <td>AGRN</td>\n      <td>Transcript</td>\n      <td>NM_001305275.1</td>\n      <td>protein_coding</td>\n      <td>1/39</td>\n      <td>NaN</td>\n      <td>117</td>\n      <td>67</td>\n      <td>23</td>\n      <td>V/L</td>\n      <td>Gtc/Ctc</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.421</td>\n      <td>3.288</td>\n      <td>0.069819</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>957640</td>\n      <td>C</td>\n      <td>T</td>\n      <td>0.0318</td>\n      <td>0.02016</td>\n      <td>0.0328</td>\n      <td>MedGen:C3808739,OMIM:615120|MedGen:CN169374</td>\n      <td>NaN</td>\n      <td>Myasthenic_syndrome,_congenital,_8|not_specified</td>\n      <td>NaN</td>\n      <td>NC_000001.10:g.957640C&gt;T</td>\n      <td>NaN</td>\n      <td>single_nucleotide_variant</td>\n      <td>NaN</td>\n      <td>SO:0001819|synonymous_variant</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>T</td>\n      <td>synonymous_variant</td>\n      <td>LOW</td>\n      <td>AGRN</td>\n      <td>Transcript</td>\n      <td>NM_001305275.1</td>\n      <td>protein_coding</td>\n      <td>2/39</td>\n      <td>NaN</td>\n      <td>311</td>\n      <td>261</td>\n      <td>87</td>\n      <td>D</td>\n      <td>gaC/gaT</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.421</td>\n      <td>12.560</td>\n      <td>1.356499</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>976059</td>\n      <td>C</td>\n      <td>T</td>\n      <td>0.0000</td>\n      <td>0.00022</td>\n      <td>0.0010</td>\n      <td>MedGen:CN169374</td>\n      <td>NaN</td>\n      <td>not_specified</td>\n      <td>NaN</td>\n      <td>NC_000001.10:g.976059C&gt;T</td>\n      <td>NaN</td>\n      <td>single_nucleotide_variant</td>\n      <td>NaN</td>\n      <td>SO:0001819|synonymous_variant</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>T</td>\n      <td>synonymous_variant</td>\n      <td>LOW</td>\n      <td>AGRN</td>\n      <td>Transcript</td>\n      <td>NM_001305275.1</td>\n      <td>protein_coding</td>\n      <td>4/39</td>\n      <td>NaN</td>\n      <td>576</td>\n      <td>526</td>\n      <td>176</td>\n      <td>L</td>\n      <td>Ctg/Ttg</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.421</td>\n      <td>17.740</td>\n      <td>2.234711</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scrubbing the Data\n\nThere seem to be a number of feilds with missing data and incorrect types. In this section, I scrub the dataset squeaky-clean."},{"metadata":{},"cell_type":"markdown","source":"## Very Low Incidence Features\n\nHere I drop features with under 600 entries (1% of dataset)."},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.drop(['CLNDISDBINCL', 'CLNDNINCL', 'CLNSIGINCL', 'SSR', 'DISTANCE', 'MOTIF_NAME', 'MOTIF_POS', 'HIGH_INF_POS', 'MOTIF_SCORE_CHANGE'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Low Incidence Features\n\nHere I dichotomize features that are present for less than half the dataset, 1 indicating that data are present, 0 otherwise."},{"metadata":{"trusted":false},"cell_type":"code","source":"for var in ['CLNVI', 'INTRON', 'BAM_EDIT', 'SIFT', 'PolyPhen', 'BLOSUM62']:\n    df[var] = df[var].apply(lambda x: 1 if x == x else 0).astype('category')\n    print(df[var].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Target: CLASS\n\nThe CLASS vartible is the target variable, which indicates whether there were conflicting submissions."},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.rename({'CLASS': 'target'}, axis = 1)\ndf['target'] = df['target'].astype('category')\ndf['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CHROM\n\nThis variable captures the chromosome on which the variant is located. This should be a categorical variable. Strangely, there are two \"16\"s in this list, which should be combined. I do this by converting it to a strip and striping the spaces before making it into a category."},{"metadata":{"trusted":false},"cell_type":"code","source":"df['CHROM'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['CHROM'] = df['CHROM'].astype('str').apply(lambda x: x.strip())\ndf['CHROM'] = df['CHROM'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## POS\n\nThis variable captures position of the gene on the chromosome. Will need to treat this with care in analysis, since it depends on CHROM."},{"metadata":{"trusted":false},"cell_type":"code","source":"df['POS'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## REF, ALT, Allele\n\nThese variables are for capture variant alleles - should be categorical."},{"metadata":{"trusted":false},"cell_type":"code","source":"for var in ['REF', 'ALT', 'Allele']:\n    print(df[var].value_counts()[0:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a lot of low-frequency categories - I will lump them together into an \"other\" category."},{"metadata":{"trusted":false},"cell_type":"code","source":"for var in ['REF', 'ALT', 'Allele']:\n    df[var] = df[var].apply(lambda x: 'O' if x not in ['A', 'C', 'G', 'T'] else x).astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## AF_ESP, AF_EXAC, and AF_TGP\n\nThese variables capture the allele frequency as found in other datasets. They are almost all zero, so I dichotomize them into zero vs non-zero."},{"metadata":{"trusted":false},"cell_type":"code","source":"df[['AF_ESP', 'AF_EXAC', 'AF_TGP']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df[['AF_ESP', 'AF_EXAC', 'AF_TGP']].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['AF_ESP'] = df['AF_ESP'].apply(lambda x: 1 if x > 0 else 0).astype('category')\ndf['AF_EXAC'] = df['AF_EXAC'].apply(lambda x: 1 if x > 0 else 0).astype('category')\ndf['AF_TGP'] = df['AF_TGP'].apply(lambda x: 1 if x > 0 else 0).astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CLNDISDB\n\nThis variable contains IDs for diseases in other databases. This variable has a large number of values, so it will be difficult to use it. I see that different values for this variable often contain the same identifiedrs, making the values arguable not unique (e.g. 'MedGen:CN169374' appears in multiple values). I choose to drop it."},{"metadata":{"trusted":false},"cell_type":"code","source":"print(len(df['CLNDISDB'].unique()))\ndf['CLNDISDB'].value_counts()[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.drop('CLNDISDB', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CLNDN\n\nThis captures the preferred disease name using the identifiers from CLNDISDB. This may be cleaner than the other variable, and is probably important for prediction, so I will attempt to clean it."},{"metadata":{"trusted":false},"cell_type":"code","source":"print(len(df['CLNDN'].unique()))\ndf['CLNDN'].value_counts()[0:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each value is a list of diseases. It seems like I could clean this by creating dummy variables for specific common diseases in each list. I will create dummies for the top 100 diseases."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"name_df = df['CLNDN'].str.split(pat = '|', expand = True)\nname_df.head()\ntop_100_dn = name_df.apply(pd.value_counts).sum(axis=1).sort_values(ascending = False)[0:100]\nprint(top_100_dn[0:10])\n\ntop_100_dn_list = list(top_100_dn.index)\nprint(top_100_dn_list[0:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for dn in top_100_dn_list:\n    df[dn] = df['CLNDN'].apply(lambda x: 1 if dn in x else 0).astype('category')\ndf = df.drop('CLNDN', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CLNHGVS\n\nThis variable is all unique values that I don't understand related to HGVS expression. I choose to drop it."},{"metadata":{"trusted":false},"cell_type":"code","source":"print(len(df['CLNHGVS'].unique()))\ndf = df.drop('CLNHGVS', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CLNVC\n\nThis variant type variable is almost all one value - I will turn it into a categorical variable by consolidating low-incidence types."},{"metadata":{"trusted":false},"cell_type":"code","source":"print(df['CLNVC'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"clnvc_types = ['single_nucleotide_variant', 'Deletion', 'Duplication']\ndf['CLNVC'] = df['CLNVC'].apply(lambda x: x if x in clnvc_types else 'Other').astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MC\n\nMolecular consequence is a categorical variable, need to clean up rare values. Since values are lists of consequences, I will do this similarly to how I did it for the names, splitting up the series and coding dummies."},{"metadata":{"trusted":false},"cell_type":"code","source":"df['MC'].value_counts()[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"name_df = df['MC'].str.split(pat = '[|,]', expand = True)\nname_df.head()\ntop_mc = name_df.apply(pd.value_counts).sum(axis=1).sort_values(ascending = False)[0:20]\nprint(top_mc)\n\ntop_mc_list = [x for x in list(top_mc.index) if 'SO:' not in x]\nprint(top_mc_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['MC'] = df['MC'].fillna('unknown')\nfor mc in top_mc_list:\n    df[mc] = df['MC'].apply(lambda x: 1 if mc in x else 0).astype('category')\n    print(df[mc].value_counts())\ndf = df.drop('MC', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ORIGIN\n\nHere is the description: \"Allele origin. One or more of the following values may be added: 0 - unknown; 1 - germline; 2 - somatic; 4 - inherited; 8 - paternal; 16 - maternal; 32 - de-novo; 64 - biparental; 128 - uniparental; 256 - not-tested; 512 - tested-inconclusive; 1073741824 - other\" Since almost all have origin 1 (germline), I will recode this to have 0 for all other values to make it a dummy variable."},{"metadata":{"trusted":false},"cell_type":"code","source":"df['ORIGIN'] = df['ORIGIN'].fillna(0).apply(lambda x: 1 if x == 1.0 else 0).astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Consequence\n\nThis variable is similar to MC, but with slightly different values. I'm not sure why. I will use it to update the MC dummy variables from before."},{"metadata":{"trusted":false},"cell_type":"code","source":"name_df = df['Consequence'].str.split(pat = '&', expand = True)\nname_df.head()\ntop_mc = name_df.apply(pd.value_counts).sum(axis=1).sort_values(ascending = False)\nprint(top_mc[0:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for mc in top_mc_list:\n    mc2 = mc + '2'\n    df[mc2] = df['Consequence'].apply(lambda x: 1 if mc in x else 0).astype('category')\n    df[mc] = df[[mc, mc2]].apply(lambda x: max(x[mc], x[mc2]), axis = 1).astype('category')\n    print(df[mc].value_counts())\n    df=df.drop(mc2, axis = 1)\ndf = df.drop('Consequence', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## IMPACT\n\nCategorical variable capturing variant impact"},{"metadata":{"trusted":false},"cell_type":"code","source":"df['IMPACT'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['IMPACT'] = df['IMPACT'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SYMBOL\n\nThis variable is the Gene symbol/ID. It has many values - I will make it categorical, but only keep the top 100 values, recoding the rest as \"Other\"."},{"metadata":{"trusted":false},"cell_type":"code","source":"len(df['SYMBOL'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['SYMBOL'].value_counts()[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"top_100_symb = df['SYMBOL'].value_counts()[0:100].index\ndf['SYMBOL'] = df['SYMBOL'].apply(lambda x: x if x in top_100_symb else 'Other').astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['SYMBOL'].value_counts()[0:100]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature\n\nThis is an ID associated with gene name - deleting due to redundancy"},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.drop('Feature', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature_type and BIOTYPE\n\nThese features have little information (almost all records have same value), so I drop them."},{"metadata":{"trusted":false},"cell_type":"code","source":"for var in ['Feature_type', 'BIOTYPE']:\n    print(df[var].value_counts())\n    df = df.drop(var, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EXON\n\nThis captures the relative exon number. Given the very large numbers of unique values, I choose to drop it."},{"metadata":{"trusted":false},"cell_type":"code","source":"len(df['EXON'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.drop('EXON', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## cDNA_position, CDS_position, Protein_position\n\nThese represent relative positions of the base pair in various ways. These are all distance measures, which I think are irrelevant to the problem at hand, and difficult to clean so I drop them."},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.drop(['cDNA_position', 'CDS_position', 'Protein_position'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Amino_acids, Codons\n\nThese have a large number of unique values, so I drop them."},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.drop(['Amino_acids', 'Codons'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## STRAND\n\nCategorical: defined as + (forward) or - (reverse)"},{"metadata":{"trusted":false},"cell_type":"code","source":"df['STRAND'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['STRAND'] = df['STRAND'].fillna(df['STRAND'].mode())\ndf['STRAND'] = df['STRAND'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LoFtool\n\nNumeric variable: Loss of Function tolerance score for loss of function variants. Will fill missing values with median."},{"metadata":{"trusted":false},"cell_type":"code","source":"df['LoFtool'] = df['LoFtool'].fillna(df['LoFtool'].median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CADD_PHRED, CADD_RAW\n\nDifferent scores of deleteriousness - I keep them and fill missing values with medians."},{"metadata":{"trusted":false},"cell_type":"code","source":"df['CADD_PHRED'] = df['CADD_PHRED'].fillna(df['CADD_PHRED'].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['CADD_RAW'] = df['CADD_RAW'].fillna(df['CADD_RAW'].median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scaling numeric variables"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nnum_var_list = ['POS', 'LoFtool', 'CADD_PHRED', 'CADD_RAW']\nscl = StandardScaler()\ndf[num_var_list] = scl.fit_transform(df[num_var_list])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Separate target and features"},{"metadata":{"trusted":false},"cell_type":"code","source":"target = df['target']\nfeatures = df.drop('target', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring the Data"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"#Original columns\nlist(df.columns[0:23])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.iloc[:, 0:23].info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Original feature set\norig_feat = list(features.columns[0:22])\norig_feat_cat = [x for x in orig_feat if x not in num_var_list]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Numeric Variables\n\nIn this section I explore the numeric variables in the dataset. I find that there is a high correlation between CADD_PHRED and CADD_RAW, so I choose to drop one of them. I drop CADD_RAW due to the long right tail."},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"features[num_var_list].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"features[num_var_list].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.heatmap(features[num_var_list].corr(),\n            vmin=0,\n            vmax=1,\n            cmap='YlGnBu',\n            annot=np.round(features[num_var_list].corr(), 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"features = features.drop('CADD_RAW', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical Variables\n\nAssociations between categorical variables can be difficult to visualize. I use Cramer's V to understand the associations between each pair of categorical features, adapting this code: https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9"},{"metadata":{"trusted":false},"cell_type":"code","source":"import scipy.stats as ss\n\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n    rcorr = r-((r-1)**2)/(n-1)\n    kcorr = k-((k-1)**2)/(n-1)\n    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"num_feat = len(orig_feat_cat)\ncat_corr_arr = np.empty((num_feat, num_feat))\nfor i, row in enumerate(orig_feat_cat):\n    for j, col in enumerate(orig_feat_cat):\n        #print((i, j))\n        cat_corr_arr[i, j] = cramers_v(features[row], features[col])\nprint(cat_corr_arr[0:5, 0:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(16, 14))\nsns.heatmap(cat_corr_arr,\n            vmin=0,\n            vmax=1,\n            cmap='YlGnBu',\n            xticklabels = orig_feat_cat,\n            yticklabels = orig_feat_cat,\n            annot=np.round(cat_corr_arr, 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I choose to drop the Allele, IMPACT, SYMBOL and PolyPhen variables due to high correlations."},{"metadata":{"trusted":false},"cell_type":"code","source":"features = features.drop(['Allele', 'IMPACT', 'SYMBOL', 'PolyPhen'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling\n\nFor this problem, I choose to use multiple classifiers to see how they compare. I start with a dummy classifier as a baseline for comparison. Then I proceed to Random Forest Classifier, Naive Bayes, and AdaBoost. I will test the effects of various parameter spefications on model performance."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.dummy import DummyClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB\nfrom sklearn.model_selection import GridSearchCV, train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, make_scorer","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"features = pd.get_dummies(features, drop_first = True)\nprint(features.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation Metric\n\nFor this problem I choose the F1 score, which balances precision and recall. Since there are fewer positive cases than negative one, I want to include recall as part of my metric, but I want to include precision as well to avoid over-classification."},{"metadata":{"trusted":false},"cell_type":"code","source":"f1_scorer = make_scorer(f1_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dummy\n\nThe F1 score for the Dummy classifier is 0.253, providing a point of comparison for other models."},{"metadata":{"trusted":false},"cell_type":"code","source":"dm_clf = DummyClassifier(random_state = 42)\nmean_dm_cv_score = cross_val_score(dm_clf, features, target, scoring = f1_scorer, cv = 3).mean()\nprint(\"Mean Cross Validation F1 Score for Dummy Classifier: {:.3}\".format(mean_dm_cv_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive Bayes\n\nGaussian Naive Bayes doesn't seem like a natural fit, given that there are many one-hot encoded variables in this dataset, but I am curious whether its performance is better than the Dummy classifier - at 0.341 it does seem to be better. Bernoulli Naive Bayes does even better still."},{"metadata":{"trusted":false},"cell_type":"code","source":"gnb_clf = GaussianNB()\nmean_gnb_cv_score = cross_val_score(gnb_clf, features, target, scoring = f1_scorer, cv = 3).mean()\nprint(\"Mean Cross Validation F1 Score for Gaussian Naive Bayes Classifier: {:.3}\".format(mean_gnb_cv_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"bnb_clf = BernoulliNB()\nmean_bnb_cv_score = cross_val_score(bnb_clf, features, target, scoring = f1_scorer, cv = 3).mean()\nprint(\"Mean Cross Validation F1 Score for Bernoulli Naive Bayes Classifier: {:.3}\".format(mean_bnb_cv_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## AdaBoost\n\nI decide to fit AdaBooost next using decision tree and logistic regression classifiers. These models provide no improvement over Naive Bayes."},{"metadata":{"trusted":false},"cell_type":"code","source":"adb_clf = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(), random_state = 42)\nmean_adb_cv_score = cross_val_score(adb_clf, features, target, scoring = f1_scorer, cv = 3).mean()\nprint(\"Mean Cross Validation F1 Score for AdaBoost Decision Tree Classifier: {:.3}\".format(mean_adb_cv_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"adb_clf = AdaBoostClassifier(base_estimator = LogisticRegression(solver = 'lbfgs'), random_state = 42)\nmean_adb_cv_score = cross_val_score(adb_clf, features, target, scoring = f1_scorer, cv = 3).mean()\nprint(\"Mean Cross Validation F1 Score for AdaBoost Logistic Regression Classifier: {:.3}\".format(mean_adb_cv_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost\n\nI next decide to use XGBoost, a popular boosting algorithm. This does not seem to improve performance."},{"metadata":{"trusted":false},"cell_type":"code","source":"import xgboost as xgb\nxgb_clf = xgb.XGBClassifier(seed = 123)\nmean_xgb_cv_score = cross_val_score(xgb_clf, features, target, scoring = f1_scorer, cv = 3).mean()\nprint(\"Mean Cross Validation F1 Score for XGBoost Classifier: {:.3}\".format(mean_xgb_cv_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest\n\nLastly I fit a Random Forest model, which has an F1 score of 0.212."},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_clf = RandomForestClassifier(n_estimators = 100, random_state = 42)\nmean_rf_cv_score = cross_val_score(rf_clf, features, target, scoring = f1_scorer, cv = 3).mean()\nprint(\"Mean Cross Validation F1 Score for Random Forest Classifier: {:.3}\".format(mean_rf_cv_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bernoulli Naive Bayes Tuning\n\nHere I use grid search and random search to tune the highest performing model: Bernoulli Naive Bayes. The best hyperparameters yeild an F1 score of 0.437. This is not great, but substantially better than the dummy model."},{"metadata":{"trusted":false},"cell_type":"code","source":"bnb_param_grid = {\n'alpha': [0.1, 0.5, 1, 2, 5],\n'fit_prior': [True, False]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import time\nstart = time.time()\nbnb_grid_search = GridSearchCV(bnb_clf, bnb_param_grid, scoring = f1_scorer, cv = 3)\nbnb_grid_search.fit(features, target)\n\nprint(\"Cross Validation F1 Score: {:.3}\".format(bnb_grid_search.best_score_))\nprint(\"Total Runtime for Grid Search on Bernoulli Naive Bayes: {:.4} seconds\".format(time.time() - start))\nprint(\"\")\nprint(\"Optimal Parameters: {}\".format(bnb_grid_search.best_params_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Interpreting the Model\n\nBelow I explore the final model to better understand the important properties of the model."},{"metadata":{},"cell_type":"markdown","source":"## Performance\n\nLooking at the performance, I see that the unbalanced nature of the classes seem to yeild a fair amount of misclassification. Specifically, a number of cases where experts agreed were classified as being cases of disagreement (the upper right of the confusion matrix)."},{"metadata":{"trusted":false},"cell_type":"code","source":"best_bnb = bnb_grid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.33, random_state = 42)\nbest_bnb.fit(X_train, y_train)\ny_hat_test = best_bnb.predict(X_test) \nbnb_confusion_matrix = confusion_matrix(y_test, y_hat_test)\nprint(bnb_confusion_matrix)\nbnb_classification_report = classification_report(y_test, y_hat_test)\nprint(bnb_classification_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Probabilities\n\nWhile Naive Bayes doesn't yeild feature importances, I am able to look at which features have the largest difference in predicted probability of being present between the two target classes. The allele frequency variables jump out as having the largest differences, as do genes associated with unknown disease variants."},{"metadata":{"trusted":false},"cell_type":"code","source":"feat_df = pd.DataFrame()\nfeat_df['prob_0'] = np.exp(best_bnb.feature_log_prob_[0])\nfeat_df['prob_1'] = np.exp(best_bnb.feature_log_prob_[1])\nfeat_df.index = features.columns\nfeat_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"feat_df['ave_prob'] = feat_df.apply(lambda x: (x[0] + x[1])/2, axis = 1)\nfeat_df['prob_diff'] = feat_df.apply(lambda x: np.abs(x[0] - x[1]), axis = 1)\nfeat_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"feat_df.sort_values('prob_diff', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nIn this analysis, I find that I am able to predict when experts will disagree about gene severity moderately well, with an F1 score of 0.437 for my final Bernoulli Naive Bayes model. This is a notable improvement over the dummy model, with F1 score of 0.253. This model can be used to prioritize research on gene variants with debatable severity. However, there is still a fair amount of misclassification, specifically with concurrences being classified as disagreements more often than warranted. Future analysis could look for ways to better balance the overall accuracy with the recall of the model."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":1}