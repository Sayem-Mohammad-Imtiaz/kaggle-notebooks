{"cells":[{"metadata":{"_uuid":"a47a61b9c5b493252958611b73ad38f067991f09"},"cell_type":"markdown","source":"# King County, Washington State, USA - Housing Prices"},{"metadata":{"trusted":true,"_uuid":"a495663bc1983751db9336b9e6494e8d2e495cee","collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\nfrom scipy import optimize\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom scipy import optimize\nfrom matplotlib import pyplot\nimport sys\nfrom tqdm import tqdm as tqdm\nfrom matplotlib.pyplot import figure\n\nfrom sklearn.preprocessing import PolynomialFeatures\n#%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"012aed573b00cb93a5a1b94c6f4309e1201fa44c","collapsed":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/kc-house-datacsv/kc_house_data.csv\")\nplt.figure()\ndata.hist(figsize=(20, 15), bins=50)\n#scatter_matrix(data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b51fc077ae598ef9a51ec2e17a8dc1aed0390be"},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{"trusted":false,"_uuid":"92e29f5a3ddce2cd555dc39cdfc92792d607bdca","collapsed":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fb633cf5b62e54745d06e402a0993595dffec5db","collapsed":true},"cell_type":"code","source":"def data_preprocessing(data):\n    #Returns processed data in numpy matrix\n    #Drops ID field and converts date to year only\n    data_w = data\n    data_w = data.drop(columns=['id'])\n    data_w[\"date\"] = pd.to_datetime(data_w[\"date\"]).dt.year\n    \n    #move price to the end of df\n    price = data_w.pop('price')\n    data_w['price']=price\n    return data_w.values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6ec66f10d2e42f548eabd71374bebffb740762fd","collapsed":true},"cell_type":"code","source":"#Split data into train and test set\ndata_X = data_preprocessing(data)\nX, X_test, y, y_test = train_test_split(data_X[:, :18], data_X[:, 19], test_size=0.20, random_state=23)\n\n#Set aside test set\ntest_set = np.concatenate([X_test, y_test[:, None]], axis=1)\n#np.savetxt(\"Data/test_data.txt\", test_set, fmt='%s')\n\n#Split data into train and cross validation set\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=23)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90d0ffaf12b5be9796861bd83f86e6faacddebcd"},"cell_type":"markdown","source":"# Linear Regression Model"},{"metadata":{"trusted":false,"_uuid":"6a44d91fb625d9f3c090f0475ba3728b427374f0","collapsed":true},"cell_type":"code","source":"# Cost Function\ndef linearRegCostFunction(X, y, theta, lambda_):\n    \n    # Initialize some useful values\n    m, n = X.shape # X matrix shape\n    \n    \n    #Add intercept\n    #print(np.ones(m).reshape(m,1).shape, X)\n    X = np.concatenate([np.ones(m).reshape(m,1), X], axis=1)\n    \n    #Compute h\n    h = X @ theta\n    \n    #Regularized Cost Function\n    J = np.sum((h - y)**2)/(2*m)\n    reg_term = (lambda_/(2*m)) * np.sum(theta[1:]**2)\n    J = J + reg_term\n    \n    #Gradient Computation\n    #Simple Gradient without reg for bias\n    grad = (1/m) * ((h - y) @ X)\n    \n    # Compute gradient with reg for non-bias\n    grad[1:] = grad[1:] + (lambda_/m)*theta[1:]\n\n    return J, grad\n\ndef trainLinearReg(linearRegCostFunction, X, y, lambda_=0.0, maxiter=200):\n    # Useful values\n    m, n = X.shape # X matrix shape\n    # Initialize Theta\n    initial_theta = np.zeros(n+1)\n\n    # Create \"short hand\" for the cost function to be minimized\n    costFunction = lambda t: linearRegCostFunction(X, y, t, lambda_)\n\n    # Now, costFunction is a function that takes in only one argument\n    options = {'maxiter': maxiter}\n\n    # Minimize using scipy\n    res = optimize.minimize(costFunction, initial_theta, jac=True, method='TNC', options=options)\n    \n    return res.x\n\ndef learningCurve(X_train, y_train, X_val, y_val, lambda_=0):\n    # Number of training examples\n    m = y_train.size\n\n    # You need to return these values correctly\n    error_train = np.zeros(m)\n    error_val   = np.zeros(m)\n\n    #Compute error_train\n    for i in tqdm(range(1, m+1)):\n        theta_train = trainLinearReg(linearRegCostFunction, X_train[:i,:], y_train[:i], lambda_)\n        \n        error_train[i-1], _ = linearRegCostFunction(X_train[:i, :], y_train[:i], theta_train, 0)\n        error_val[i-1], _ = linearRegCostFunction(X_val, y_val, theta_train, 0)\n        \n    return error_train, error_val\n\ndef featureNormalize(X):\n\n    mu = np.mean(X, axis=0)\n    X_norm = X - mu\n\n    sigma = np.std(X_norm, axis=0, ddof=1)\n    X_norm /= sigma\n    return X_norm, mu, sigma\n\ndef predict(X, theta):\n    \n    # Initialize some useful values\n    m, n = X.shape # X matrix shape\n    \n    \n    #Add intercept\n    X = np.concatenate([np.ones(m).reshape(m,1), X], axis=1)\n    \n    #Compute h\n    h = X @ theta\n    \n    return h","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0783b1b9ab2927f34d67186c0ff653ef61d7c72c","collapsed":true},"cell_type":"code","source":"## testing cost function\n#test theta with ones\ntheta_test = np.ones(18+1)\nlambda_ = 0\n# Compute cost with test theta\nJ, grad = linearRegCostFunction(X_train, y_train, theta_test, lambda_)\nJ, grad","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b6e59366fb67f0b328d0a911f8dd043604403a9c","collapsed":true},"cell_type":"code","source":"#Train Model\nres = trainLinearReg(linearRegCostFunction, X_train, y_train, lambda_=0.0, maxiter=200)\ntrained_theta = res\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b5cd0c2a39d25a693aed8b4f61a27426af7abc67","collapsed":true},"cell_type":"code","source":"#Compute Training and Validation error for increasing number of training data used\nerror_train_n, error_val_n = learningCurve(X_train[:3000,:], y_train[:3000], X_val[:3000,:], y_val[:3000], lambda_=0)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bfe031276ef737cc2b2f2a07c36a17a69874a05"},"cell_type":"markdown","source":"# ML Diagnostics\n## Learning Curve"},{"metadata":{"trusted":false,"_uuid":"b78f6fe5183dd893712c19291a81089f72595d0c","collapsed":true},"cell_type":"code","source":"#Plot Learning Curves\nm = y_train[:3000].size\n\nprint('# Training Examples\\tTrain Error\\tCross Validation Error')\nfor i in range(10):\n    print('  \\t%d\\t\\t%f\\t%f' % (i+1, error_train_n[-i], error_val_n[-i]))\n\n\nfigure(num=1, figsize=(10, 5))\npyplot.plot(np.arange(1, m+1), error_train_n, np.arange(1, m+1), error_val_n, lw=0.6)\npyplot.title('Learning curve for linear regression')\npyplot.legend(['Train', 'Cross Validation'])\npyplot.xlabel('Number of training examples')\npyplot.ylabel('Error')\npyplot.axis([0, m, 0, 161361944543])\nplt.grid()\npyplot.show()\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cf0e56a10ac114cd941c724fcaded4c28e4b3e4"},"cell_type":"markdown","source":"## Learning Curve - Interpretation\n\n- Graph Properties\n    - Low training set size: J_CV is high & J_train is low\n    - Large training set size: J_CV â‰ƒ J_train\n    \n-> Model does not seem to suffer from high variance but from high bias\n\nSteps to address high bias situation:\n- Adding features: <span style=\"color:orange\">Not possible, look into feauture engineering</span> \n- Adding polynomial features: <span style=\"color:green\">Next Step</span> \n- After polynomials: determine best lambda: <span style=\"color:green\">Next Step</span> "},{"metadata":{"_uuid":"7b9b6321d1c4d83021c0f2ffe68ccd1b4ec98b08"},"cell_type":"markdown","source":"# Adding Polynomial Features"},{"metadata":{"trusted":false,"_uuid":"950fea61cefe7a29ba8720f89434e67a6b8f8c59","collapsed":true},"cell_type":"code","source":"#Polynomial dimensions\np = 2\n\n#Add polynomial features\npoly = PolynomialFeatures(p, include_bias=False)\n\nX_test_p = poly.fit_transform(X_test)\nX_train_p = poly.fit_transform(X_train)\nX_val_p = poly.fit_transform(X_val)\n\n#Normalize X_test_p and extract sigma & mu\nX_train_p, mu, sigma = featureNormalize(X_train_p)\n\n#Normalize X_test_p & X_val_p using sigma & mu\nX_test_p -= mu\nX_test_p /= sigma\nX_val_p -= mu\nX_val_p /= sigma","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cab474920cafece0f162f4659e02372cafe4b653"},"cell_type":"markdown","source":"# Plot learning curve"},{"metadata":{"trusted":false,"_uuid":"27073d4ef050f62b37adb71b3b994d0a1c3c6cd2","collapsed":true},"cell_type":"code","source":"#Compute Training and Validation error for increasing number of training data used\nerror_train, error_val = learningCurve(X_train_p[:3000,:], y_train[:3000], X_val_p[:3000,:], y_val[:3000], lambda_=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"20972301d79fd6610dda7024cef1819fb88b8f05","collapsed":true},"cell_type":"code","source":"#Plot Learning Curves\nm = y_train[:3000].size\n\nprint('# Training Examples\\tTrain Error Norm\\tCross Error Norm\\tTrain Error Poly\\tCross Error Poly')\nfor i in range(2999, 0, -500):\n    print('  \\t%d\\t\\t%f\\t%f\\t%f\\t%f' % (i+1, error_train_n[i], error_val_n[i],error_train[i], error_val[i]))\n\n\nfigure(num=1, figsize=(15, 5))\npyplot.subplot(121)\npyplot.plot(np.arange(1, m+1), error_train_n, np.arange(1, m+1), error_val_n, lw=0.6)\npyplot.title('Learning curve for linear regression: Normal')\npyplot.legend(['Train', 'Cross Validation'])\npyplot.xlabel('Number of training examples')\npyplot.ylabel('Error')\npyplot.axis([0, m, 0, 161361944543])\nplt.grid()    \n\n\n#figure(num=1, figsize=(10, 5))\npyplot.subplot(122)\npyplot.plot(np.arange(1, m+1), error_train, np.arange(1, m+1), error_val, lw=0.6)\npyplot.title('Learning curve for linear regression: Poly')\npyplot.legend(['Train', 'Cross Validation'])\npyplot.xlabel('Number of training examples')\npyplot.ylabel('Error')\npyplot.axis([0, m, 0, 161361944543])\nplt.grid()\nplt.tight_layout()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1a12a92d4a817967f7899c42d1c448f1c6d026d2","collapsed":true},"cell_type":"code","source":"#Full dataset learning curve#Compute Training and Validation error for increasing number of training data used\nerror_train_fu, error_val_fu = learningCurve(X_train_p, y_train, X_val_p, y_val, lambda_=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e37a9b514677b41bac0035a4eb76ee0179966a35"},"cell_type":"markdown","source":"### Learning curve for full training set"},{"metadata":{"trusted":false,"_uuid":"e85e167ede7dd0fc535d4d9582f1c2a30f1b524d","collapsed":true},"cell_type":"code","source":"m = y_train.size\n\nfigure(num=1, figsize=(15, 5))\npyplot.plot(np.arange(1, m+1), error_train_fu, np.arange(1, m+1), error_val_fu, lw=0.6)\npyplot.title('Learning curve for linear regression: Poly')\npyplot.legend(['Train', 'Cross Validation'])\npyplot.xlabel('Number of training examples')\npyplot.ylabel('Error')\npyplot.axis([0, m, 0, 161361944543])\nplt.grid()\nplt.tight_layout()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1feae050b265589ff4e8f9a15f8a51cc8b678ea"},"cell_type":"markdown","source":"# Lambda Selection"},{"metadata":{"trusted":false,"_uuid":"ca3ce8de677a594ebb58bb88c8ba37e835ff0773","collapsed":true},"cell_type":"code","source":"def validationCurve(X, y, Xval, yval):\n\n \n    # Selected values of lambda \n    lambda_vec = [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]\n    \n    # Init vectors containing errors\n    error_train = np.zeros(len(lambda_vec))\n    error_val = np.zeros(len(lambda_vec))\n    \n    for i in range(len(lambda_vec)):\n        lambda_ = lambda_vec[i]\n        \n        #Train model based on i-th lambda\n        theta_train = trainLinearReg(linearRegCostFunction, X, y, lambda_)\n        \n        #Compute error for train & validation set\n        error_train[i], _ = linearRegCostFunction(X, y, theta_train, 0)\n        error_val[i], _ = linearRegCostFunction(Xval, yval, theta_train, 0)\n        \n\n\n    return lambda_vec, error_train, error_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9697550d72cd114da20a9058806bb2c448889b82","collapsed":true},"cell_type":"code","source":"lambda_vec, error_train, error_val = validationCurve(X_train_p, y_train, X_val_p, y_val)\n\npyplot.plot(lambda_vec, error_train, '-o', lambda_vec, error_val, '-o', lw=2)\npyplot.legend(['Train', 'Cross Validation'])\npyplot.xlabel('lambda')\npyplot.ylabel('Error')\npyplot.grid()\n\nprint('lambda\\t\\tTrain Error\\tValidation Error')\nfor i in range(len(lambda_vec)):\n    print(' %f\\t%f\\t%f' % (lambda_vec[i], error_train[i], error_val[i]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5c1f74799f045044fdb8654c8f546881c3b6d01"},"cell_type":"markdown","source":"Note: Shouldn't Train error constantly increase?"},{"metadata":{"_uuid":"b530851facd93d9900e256a0fc683ccb5fdcf208"},"cell_type":"markdown","source":"# Final model and prediction"},{"metadata":{"trusted":false,"_uuid":"1815c311578f6eed60d7f3efff2aae287bbd7a0c","collapsed":true},"cell_type":"code","source":"#Final Model theta:\ntheta_final = trainLinearReg(linearRegCostFunction, X_train_p, y_train, lambda_=3.0, maxiter=200)\n\n#Predict\n\nresult = predict(X_test_p, theta_final)\n\ncost = (1/(2*m))*np.sum((result - y_test)**2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8f67f5cc57cdb474fc85208d5b7d9909f03b930d","collapsed":true},"cell_type":"code","source":"result2 = result.reshape(4320,1)\ny_test2 = y_test.reshape(4320,1)\ncomparison = np.concatenate([result2, y_test2], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"503da6bc786e242e56e93653e03e272a8d5cef7a","collapsed":true},"cell_type":"code","source":"#np.savetxt(\"result.csv\", comparison, fmt='%10.5f')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d904e7fef4084011d9ef1d4b9622b303e5b4007a","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}