{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nfrom sklearn.preprocessing import PowerTransformer\npt = PowerTransformer()\n\nfrom scipy import stats\n\nfrom sklearn.model_selection import train_test_split\n\n#Import (Z-Scaler) StandardScaler\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read dataset from the kaggle\n\ndf= pd.read_csv('/kaggle/input/health-insurance-cross-sell-prediction/train.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#some varaibles which is categorical feature but has datatype as int, so changing those datatype to \n#object\n\ndf[['Driving_License', 'Previously_Insured', 'Response']] = df[['Driving_License', 'Previously_Insured', 'Response']].astype('object')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Shape\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy = df.copy(deep=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Remove insignificant variable id","metadata":{}},{"cell_type":"code","source":"# Remove insignificant variable id\ndf.drop('id', axis=1, inplace=True)\n\ndf.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Duplicate records ","metadata":{}},{"cell_type":"code","source":"#Remove duplicates\ndf.duplicated(keep='first').value_counts(normalize=True) * 100\n\n#There are totally 0.07% of duplicate records","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.duplicated(keep='first').value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We are keeping the first duplicate record and delete the rest\ndf.drop_duplicates(keep='first', inplace=True)\n\ndf.shape\n\n#Shape after removing duplicates","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Outlier treatment","metadata":{}},{"cell_type":"code","source":"#As seen in eda, only Annual_Premium variable had significant number of outliers\n\nq1 = df['Annual_Premium'].quantile(0.25)\nq3 = df['Annual_Premium'].quantile(0.75)\n\niqr = q3-q1\n\nll = q1 - 1.5*iqr\nul = q3 + 1.5*iqr\n\ndf[(df['Annual_Premium']<ll)|(df['Annual_Premium']>ul)].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#There are 10331 outliers\n\n10331/len(df_copy) * 100\n\n#2.71% outliers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We treat the outlier by Power transforming the Annual_Premium feature\n\nprint('Skewness of Annual_premium variable before Power transformation :', df['Annual_Premium'].skew())\n\ndf['Annual_Premium'] = pt.fit_transform(df[['Annual_Premium']])\n\nprint('\\nSkewness of Annual_premium variable after Power transformation :', df['Annual_Premium'].skew())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Skewness is reduced after applying Power transformation\n\n#distribution plot\nplt.figure(figsize=(12,8))\n\nplt.subplots_adjust(hspace=0.3)\n\nplt.subplot(2,1,1)\nsns.distplot(df_copy['Annual_Premium'])\nplt.title('Distribution before transformation')\n\nplt.subplot(2,1,2)\nsns.distplot(df['Annual_Premium'])\nplt.title('Distribution after transformation')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Statistical Analysis for feature important","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dividing the dataset to customers whose Response = 1 as res_1 and customers whose Response = 0 as res_0\n\n#Customer who have not responded\nres_0 = df[df['Response']==0]\n\n#Customer who have responded\nres_1 = df[df['Response']==1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Statistical analysis for Numerical columns","metadata":{}},{"cell_type":"code","source":"#Features whose datatype is 'numeric'\nnum_cols = list(df.select_dtypes(include='number'))\n\nnum_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Test of Normality for numerical data (Shapiro test)","metadata":{}},{"cell_type":"code","source":"# Test of normality\n# Ho: skew=0 (normal)\n# Ha : skew !=0(not normal)\n\n#Shapiro test\n\nfor col in num_cols:\n    print(f'\\nShapiro test for {col} feature :')\n    print('Response = 0 :',stats.shapiro(res_0[col]))\n    print('Response = 1 :',stats.shapiro(res_1[col]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For all the numerical features<br>\n>pval = 0<br>\nsig lvl = 0.05<br>\npval < sig lvl<br>\nWe reject Null hypothesis<br>\nNone of the Data is not normally distributed","metadata":{}},{"cell_type":"markdown","source":"#### Test for equality of variances (levene test)","metadata":{}},{"cell_type":"code","source":"#equality of variances\n# Ho: Variance is equal\n# Ha : Variance is not equal\n\nfor col in num_cols:\n    print(f'\\nLevene test for {col} feature :')\n    print(stats.levene(res_0[col], res_1[col]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For features ==> 'Age', 'Region_Code', 'Annual_Premium', 'Policy_Sales_Channel'\n>   pval = 0<br>\n    sig lvl = 0.05<br>\n    pval < sig lvl<br>\n    We reject Null hypothesis\n Population variances are not equal<br>\n\nFor 'Vintage' Feature :\n>pval = 0.89<br>\nsig lvl = 0.05<br>\npval > sig lvl<br>\nWe fail to reject Null hypothesis <br>\nPopulation variances are equal","metadata":{}},{"cell_type":"markdown","source":"#### Mannwhitneyu (non-parametric ttest)","metadata":{}},{"cell_type":"code","source":"#As all the features are not normal, we cannot perform parametric test\n#We will perform non-parametric test (Mannwhitneyu)\n\n# Ho : mu1 = mu2(no relation)\n# Ha : mu1 != mu (relation)\n\nfor col in num_cols:\n    print(f'\\nNon-parametric 2-sample Unpaired test for {col} feature and Response feature :')\n    print(stats.mannwhitneyu(res_0[col], res_1[col]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For features :  'Age', 'Annual_Premium', 'Policy_Sales_Channel'<br>\n>pval = 0<br>\nsig lvl = 0.05<br>\npval < sig lvl<br>\nWe reject Null hypothesis\n###### There is a relation between ('Age', 'Annual_Premium', 'Policy_Sales_Channel) and 'Response'\n\nFor features : 'Region_Code', 'Vintage'\n>pval = 0.22, 0.26 (Region_Code and Vintage respectively)<br>\nsig lvl = 0.05<br>\npval > sig lvl<br>\nWe fail to reject Null hypothesis<br>\n######  'There is a no relation between (Region_code, Vintage) and Response'","metadata":{}},{"cell_type":"markdown","source":"## Statistical analysis for Categorical columns","metadata":{}},{"cell_type":"code","source":"#Chi-Square Test for Independence : It is a non-parametric test (hence no assumptions)\n#H0 : The variables are independent\n#H1 : The variables are not independent (i.e. variables are dependent)\n\n#List of categorical features\ncat_cols = list(df.select_dtypes(exclude='number'))\n\n#Remove the target feature from the list\ncat_cols.remove('Response')\n\ncat_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#perform chi2_contingency for all the categorical features and Response target\nfor col in cat_cols:\n    print(f'{col} vs Response :')\n    obs = pd.crosstab(index=df['Response'], columns=df[col]) #create a cross-tab for feature and target\n    print('Observed values :\\n',obs )\n    print(stats.chi2_contingency(obs))\n    print('Pvalue =',stats.chi2_contingency(obs)[1] )\n    print('\\n\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For features : 'Gender', 'Driving_License', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage'\n>pval = 0<br>\nsig lvl = 0.05<br>\npval < sig lvl<br>\nWe reject Null hypothesis<br>\n\nThere is relationship between these ('Gender',  'Driving_License',  'Previously_Insured', 'Vehicle_Age',  'Vehicle_Damage') and 'RESPONSE' variable<br>\n##### Response (target variable) is dependent on all the categorical variable.\n\n#Except Region_Code and Vintage, Response is dependent on all other variables","metadata":{}},{"cell_type":"markdown","source":"## Label encoding / One hot encoding","metadata":{}},{"cell_type":"code","source":"df.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Driving_License, Previously_Insured, Response are already encoded but Data-type is object\n#Change the datatype to int\n\ndf[['Driving_License', 'Previously_Insured', 'Response']] = df[['Driving_License', 'Previously_Insured', 'Response']].astype('int')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#One hot encoding the rest of categorical variables, by droping the first column after encoding\n\ncat_cols = list(df.select_dtypes(exclude='number'))\n\ndf = pd.get_dummies(df, columns=cat_cols, drop_first=True)\ndf.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train-Test split","metadata":{}},{"cell_type":"code","source":"# X y split\ny = df['Response']\nX = df.drop('Response', axis=1)\n\n#Train & test split with 0.3%\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=4)\n\nprint('X train Shape :',X_train.shape)\nprint('X test Shape :',X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model evaluation metrics","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score, roc_curve, precision_score, recall_score\n\nresult_df = pd.DataFrame(columns=['Model_Name', 'Accuracy_score_train', 'roc_auc_score_train','f1_score_train','precision_score_train', \n                                  'recall_score_train','Accuracy_score_test', 'roc_auc_score_test','f1_score_test', 'precision_score_test', 'recall_score_test'  ])\n\nresult_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Defining a function to append metrics in dataframe\n\ndef model_score_card(algo,  name, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test):\n    algo.fit(X_train, y_train)\n    \n    #train datset\n    y_train_pred = algo.predict(X_train)\n    y_train_proba = algo.predict_proba(X_train)[::, 1]\n    \n    #test datset\n    y_test_pred = algo.predict(X_test)\n    y_test_proba = algo.predict_proba(X_test)[::, 1]\n    \n    global result_df\n    \n    result_df = result_df.append({'Model_Name' : name,\n                                    \n                                    'Accuracy_score_train' :accuracy_score(y_train, y_train_pred) ,\n                                    'roc_auc_score_train' : roc_auc_score(y_train, y_train_proba),\n                                    'f1_score_train' : f1_score(y_train, y_train_pred), \n                                    'precision_score_train' : precision_score(y_train, y_train_pred), \n                                    'recall_score_train' : recall_score(y_train, y_train_pred),\n                                    \n                                    'Accuracy_score_test':accuracy_score(y_test, y_test_pred),\n                                      'f1_score_test' :f1_score(y_test, y_test_pred) ,\n                                    'roc_auc_score_test' : roc_auc_score(y_test, y_test_proba),\n                                     \n                                    'precision_score_test' : precision_score(y_test, y_test_pred), \n                                    'recall_score_test' : recall_score(y_test, y_test_pred)}, ignore_index = True)\n    \n    return result_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Defining a function to get evaluation metrics\n\ndef model_eval(algo,  X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test):\n    algo.fit(X_train, y_train)\n    \n    #train datset\n    y_train_pred = algo.predict(X_train)\n    y_train_proba = algo.predict_proba(X_train)[::, 1]\n\n    print('Train dataset :')\n    print('Confusion matrix :\\n', confusion_matrix(y_train, y_train_pred))\n    print('Accuracy :',accuracy_score(y_train, y_train_pred) )\n    print('AUC score :', roc_auc_score(y_train, y_train_proba))\n    print('F1-score :', f1_score(y_train, y_train_pred))\n    print('Precision score :', precision_score(y_train, y_train_pred))\n    print('Recall score :', recall_score(y_train, y_train_pred))\n    \n    #test datset\n    y_test_pred = algo.predict(X_test)\n    y_test_proba = algo.predict_proba(X_test)[::, 1]\n    print('\\n\\nTest dataset :')\n    print('Confusion matrix :\\n', confusion_matrix(y_test, y_test_pred))\n    print('Accuracy :',accuracy_score(y_test, y_test_pred) )\n    print('AUC score :', roc_auc_score(y_test, y_test_proba))\n    print('F1-score :', f1_score(y_test, y_test_pred))\n    print('Precision score :', precision_score(y_test, y_test_pred))\n    print('Recall score :', recall_score(y_test, y_test_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model building","metadata":{}},{"cell_type":"markdown","source":"## 1.Logistic Regression as a base model","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlor = LogisticRegression(solver='liblinear',random_state=4)\n\nmodel_eval(lor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Appending the evaluation metrics in a DataFrame for further reference\n\nlor = LogisticRegression(solver='liblinear',random_state=4)\n\nmodel_score_card(lor, 'Logistic Regression')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lor = LogisticRegression(solver='liblinear',random_state=4)\nlor.fit(X_train, y_train)\n    \n    #train datset\ny_train_pred = lor.predict(X_train)\ny_train_proba = lor.predict_proba(X_train)[::, 1]\n    \n    #test datset\ny_test_pred = lor.predict(X_test)\ny_test_proba = lor.predict_proba(X_test)[::, 1]\n\nfpr_train,tpr_train,threshold_train = roc_curve(y_train,y_train_proba )\nfpr_test,tpr_test,threshold_test = roc_curve(y_test,y_test_proba )\n\nprint('Logistic Regression Base model :')\n\nplt.figure(figsize=(14,5))\nplt.subplot(1,2,1)\nplt.plot(fpr_train,fpr_train)\nplt.plot(fpr_train,tpr_train)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('Train Data')\n\nplt.subplot(1,2,2)\nplt.plot(fpr_test,fpr_test)\nplt.plot(fpr_test,tpr_test)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('Test Data')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Improving the base model (Logistic Regression):\n>2.1 Data Transformation (Power transformation / Standard Scaler)<br>\n2.2 Feature selection (Recursive Feature Elimination)<br>\n2.3 SMOTE analysis","metadata":{}},{"cell_type":"markdown","source":"> ### 2.1 Data Transformation (Power transformation / Standard Scaler)","metadata":{}},{"cell_type":"code","source":"#First we check for skewness & then transform the data to reduce the skewness \n#We are checking skewness for numerical columns & only for Train data and transform test data to avoid Data-leakage\n\nX_train[num_cols].skew()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#As Age feature is right skewed, we will use Power transformation to reduce the reduce\n\nprint('Skewness before transformation :', X_train['Age'].skew())\n\nX_train['Age'] = pt.fit_transform(X_train[['Age']])\nX_test['Age'] = pt.transform(X_test[['Age']])\n\nprint('\\nSkewness after Power transformation :', X_train['Age'].skew())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Skewness of Region Code is -0.113, which is almost equal to 0\n#So we will avoid transformation for Region_code feature","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Policy_Sales_Channel is left skewed, so Power tranformation will not work properly\n#So, after trail & error, (To the power of 6) gives best results\n\nprint('Skewness before transformation :', X_train['Policy_Sales_Channel'].skew())\n\nX_train['Policy_Sales_Channel'] = X_train['Policy_Sales_Channel']**6\nX_test['Policy_Sales_Channel'] = X_test['Policy_Sales_Channel']**6\n\nprint('\\nSkewness after Power transformation :', X_train['Policy_Sales_Channel'].skew())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Scaling the whole data using StandardScaler\n# Fit on Train data and tranform it on Test data to avoid Data-Leakage\n\nX_train[num_cols] = ss.fit_transform(X_train[num_cols])\n\nX_test[num_cols] = ss.transform(X_test[num_cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Building the Logistic regression on transformed data to check the improvement\n\nlor = LogisticRegression(solver='liblinear',random_state=4)\n\nmodel_eval(lor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##### There is no change in base model after data transformation\n\n# X and Y dataset after transformation\n\n#concat train and test dataset for variable Y\ny_full = pd.concat([y_train, y_test], axis=0)\ny_full.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#concat train and test dataset for variable X\n\nX_full = pd.concat([X_train, X_test], axis=0)\nX_full.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">### 2.2 Feature selection (Recursive Feature Elimination)","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFE, RFECV\n\n#estimator used is LogisticRegression\nlor = LogisticRegression(solver='liblinear',random_state=4)\n\n#RUN RFECV to find out the best number of features to be selected\nrfe_n = RFECV(estimator=lor, cv=3, scoring='roc_auc', verbose=2, n_jobs=-1)\nrfe_n.fit(X_full, y_full)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Number\nprint('Number of features selected :', rfe_n.n_features_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Selected features\nselected = list(X_full.columns[rfe_n.support_])\nprint('\\nSelected features :',selected)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#selecting only features from RFE in both train & test dataset\n\nX_train_sel = X_train[selected]\nX_test_sel = X_test[selected]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lor = LogisticRegression(solver='liblinear',random_state=4)\n\nmodel_eval(lor, X_train_sel, X_test_sel, y_train, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### There is no change in base model after Feature selection (RFE)","metadata":{}},{"cell_type":"markdown","source":">### 2.3 SMOTE analysis","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(sampling_strategy='minority')\n\n#SMOTE analysis on train dataset\nX_train_sm, y_train_sm = smote.fit_resample(X_train_sel, y_train)\n\nprint('Shape of X train', X_train_sm.shape)\n\nprint('\\nCount of target variable :')\nprint(y_train_sm.value_counts())\n#after smote analysis, target variable is equally distributed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#building LogisticRegression on smote analysed data\nlor = LogisticRegression(solver='liblinear',random_state=4)\n\n#Model evaluation\nmodel_eval(lor, X_train_sm,X_test_sel, y_train_sm, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even though there is huge change in Accuracy<br>\nRecall, Precision and F1-score has been increased<br>\nBut the model is Over-fitting","metadata":{}},{"cell_type":"code","source":"## Appending the evaluation metrics in a DataFrame for further reference\n##This is the final base model\nmodel_score_card(lor, 'Logistic Regression Final base_model', X_train_sm,X_test_sel, y_train_sm, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lor = LogisticRegression(solver='liblinear',random_state=4)\nlor.fit(X_train_sm, y_train_sm)\n    \n    #train datset\ny_train_sm_pred = lor.predict(X_train_sm)\ny_train_sm_proba = lor.predict_proba(X_train_sm)[::, 1]\n    \n    #test datset\ny_test_pred = lor.predict(X_test_sel)\ny_test_proba = lor.predict_proba(X_test_sel)[::, 1]\n\nfpr_train,tpr_train,threshold_train = roc_curve(y_train_sm,y_train_sm_proba )\nfpr_test,tpr_test,threshold_test = roc_curve(y_test,y_test_proba )\n\nprint('Logistic Regression Final-Base_model :')\n\nplt.figure(figsize=(14,5))\nplt.subplot(1,2,1)\nplt.plot(fpr_train,fpr_train)\nplt.plot(fpr_train,tpr_train)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('Train Data')\n\nplt.subplot(1,2,2)\nplt.plot(fpr_test,fpr_test)\nplt.plot(fpr_test,tpr_test)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('Test Data')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Decision Tree Classifier\n>  3.1 Decision Tree Classifier<br>\n>  3.2 Decision Tree Classifier with Hyper-parameter tuning","metadata":{}},{"cell_type":"markdown","source":">### 3.1 Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Build DTC model on Dataset which are : RFE selected features, smote analysed\n\ndtc = DecisionTreeClassifier()\n\nmodel_eval(dtc, X_train_sm,X_test_sel, y_train_sm, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">### 3.2 Decision Tree Classifier with Hyper-parameter tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import randint as sp_randint","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GridSearchCV & RandomizedSearchCV results were almost similar\n# We are considering RandomizedSearchCV for Hyper-parameter tuning\n\ndtc = DecisionTreeClassifier(random_state=4)\n\nparams = {'max_depth' : sp_randint(2,10),\n         'min_samples_leaf' : sp_randint(1,12),\n         'criterion' : ['gini', 'entropy']}\n\nrsearch = RandomizedSearchCV(dtc, param_distributions=params, n_iter=100, n_jobs=-1, \n                             cv=3, scoring='roc_auc', random_state=4)\n\n# RandomizedSearchCV on overall transformed datasets\nrsearch.fit(X_full,y_full)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Best parameters\nprint(rsearch.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtc = DecisionTreeClassifier(**rsearch.best_params_, random_state=4)\n\nmodel_eval(dtc, X_train_sm, X_test_sel, y_train_sm, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Decision Tree Classifier after hyper-parameter tuning is giving better results. It is better fit model compared to previous model\n##### Recall , Precision , F1-score, AUC score is increased compared to previous model.","metadata":{}},{"cell_type":"code","source":"## Appending the evaluation metrics in a DataFrame for further reference\n\ndtc = DecisionTreeClassifier(**rsearch.best_params_, random_state=4)\n\nmodel_score_card(dtc, 'DecisionTreeClassifier HyperParameter-tuning', X_train_sm,X_test_sel, y_train_sm, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtc = DecisionTreeClassifier(**rsearch.best_params_, random_state=4)\ndtc.fit(X_train_sm, y_train_sm)\n    \n    #train datset\ny_train_sm_pred = dtc.predict(X_train_sm)\ny_train_sm_proba = dtc.predict_proba(X_train_sm)[::, 1]\n    \n    #test datset\ny_test_pred = dtc.predict(X_test_sel)\ny_test_proba = dtc.predict_proba(X_test_sel)[::, 1]\n\nfpr_train,tpr_train,threshold_train = roc_curve(y_train_sm,y_train_sm_proba )\nfpr_test,tpr_test,threshold_test = roc_curve(y_test,y_test_proba )\n\nprint('DecisionTreeClassifier HyperParameter-tuning :')\n\nplt.figure(figsize=(14,5))\nplt.subplot(1,2,1)\nplt.plot(fpr_train,fpr_train)\nplt.plot(fpr_train,tpr_train)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('Train Data')\n\nplt.subplot(1,2,2)\nplt.plot(fpr_test,fpr_test)\nplt.plot(fpr_test,tpr_test)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('Test Data')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Random forest Classifier\n>  4.1 Random forest Classifier<br>\n>  4.2 Random forest Classifier with Hyper-parameter tuning","metadata":{}},{"cell_type":"markdown","source":">### 4.1 Random forest Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Build RFC model on Dataset which are : RFE selected features, smote analysed\n\nrfc = RandomForestClassifier(random_state=4)\n\n#model evaluation\nmodel_eval(rfc, X_train_sm, X_test_sel, y_train_sm, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">### 4.2 Random forest Classifier with Hyper-parameter tuning","metadata":{}},{"cell_type":"code","source":"# GridSearchCV & RandomizedSearchCV results were almost similar\n# We are considering RandomizedSearchCV for Hyper-parameter tuning\n\nrfc = RandomForestClassifier(random_state=4)\n\nparams = {'n_estimators': sp_randint(50,200),\n         'max_features': sp_randint(1,15),\n         'min_samples_leaf' : sp_randint(1,25),\n          'max_depth' : sp_randint(1,10),\n         'criterion' : ['gini', 'entropy']}\n\nrsearch = RandomizedSearchCV(rfc, param_distributions=params, cv=3, n_iter=10,verbose=2, \n                             scoring='roc_auc', random_state=4, n_jobs=-1)\nrsearch.fit(X_full, y_full)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Best parameters\nprint(rsearch.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc = RandomForestClassifier(**rsearch.best_params_, random_state=4)\n\nmodel_eval(rfc, X_train_sm, X_test_sel, y_train_sm, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Random Forest Classifier after hyper-parameter tuning is giving better results. It is better fit model compared to previous model\n##### Recall , F1-score, AUC score is increased compared to previous model.","metadata":{}},{"cell_type":"code","source":"## Appending the evaluation metrics in a DataFrame for further reference\n\nrfc = RandomForestClassifier(**rsearch.best_params_, random_state=4)\n\nmodel_score_card(rfc, 'RandomForestClassifier HyperParameter-tuning', X_train_sm,X_test_sel, y_train_sm, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc = RandomForestClassifier(**rsearch.best_params_, random_state=4)\nrfc.fit(X_train_sm, y_train_sm)\n    \n    #train datset\ny_train_sm_pred = rfc.predict(X_train_sm)\ny_train_sm_proba = rfc.predict_proba(X_train_sm)[::, 1]\n    \n    #test datset\ny_test_pred = rfc.predict(X_test_sel)\ny_test_proba = rfc.predict_proba(X_test_sel)[::, 1]\n\nfpr_train,tpr_train,threshold_train = roc_curve(y_train_sm,y_train_sm_proba )\nfpr_test,tpr_test,threshold_test = roc_curve(y_test,y_test_proba )\n\nprint('RandomForestClassifier HyperParameter-tuning :')\n\nplt.figure(figsize=(14,5))\nplt.subplot(1,2,1)\nplt.plot(fpr_train,fpr_train)\nplt.plot(fpr_train,tpr_train)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('Train Data')\n\nplt.subplot(1,2,2)\nplt.plot(fpr_test,fpr_test)\nplt.plot(fpr_test,tpr_test)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('Test Data')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. LGBMClassifier\n>  5.1 LGBMClassifier<br>\n>  5.2 LGBMClassifier with Hyper-parameter tuning","metadata":{}},{"cell_type":"markdown","source":">### 5.1 LGBMClassifier","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Build RFC model on Dataset which are : RFE selected features, smote analysed\n\nlgbc = lgb.LGBMClassifier()\n\n#model evaluation\nmodel_eval(lgbc, X_train_sm, X_test_sel, y_train_sm, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">### 5.2 LGBMClassifier with Hyper-parameter tuning","metadata":{}},{"cell_type":"code","source":"from scipy.stats import uniform as sp_uniform","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbc = lgb.LGBMClassifier()\n\nparams = {'n_estimators':sp_randint(50,250),\n         'max_depth' : sp_randint(1,50),\n         'learning_rate' : sp_uniform(0,0.5)}\n\nrsearch = RandomizedSearchCV(lgbc, param_distributions=params, scoring='roc_auc', cv=3, n_iter=10,\n                             n_jobs=-1, random_state=4)\nrsearch.fit(X_full, y_full)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Best parameters\nprint(rsearch.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbc = lgb.LGBMClassifier(**rsearch.best_params_, random_state=4)\n\nmodel_eval(lgbc,  X_train_sm, X_test_sel, y_train_sm, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### LGBMClassifier model results before and after hyper-parameter tuning is same, Model is slightly over-fit","metadata":{}},{"cell_type":"code","source":"## Appending the evaluation metrics in a DataFrame for further reference\n\nlgbc = lgb.LGBMClassifier(random_state=4)\n\nmodel_score_card(lgbc, 'LGBMClassifier', X_train_sm,X_test_sel, y_train_sm, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbc = lgb.LGBMClassifier(random_state=4)\nlgbc.fit(X_train_sm, y_train_sm)\n    \n    #train datset\ny_train_sm_pred = lgbc.predict(X_train_sm)\ny_train_sm_proba = lgbc.predict_proba(X_train_sm)[::, 1]\n    \n    #test datset\ny_test_pred = lgbc.predict(X_test_sel)\ny_test_proba = lgbc.predict_proba(X_test_sel)[::, 1]\n\nfpr_train,tpr_train,threshold_train = roc_curve(y_train_sm,y_train_sm_proba )\nfpr_test,tpr_test,threshold_test = roc_curve(y_test,y_test_proba )\n\nprint('LGBMClassifier :')\n\nplt.figure(figsize=(14,5))\nplt.subplot(1,2,1)\nplt.plot(fpr_train,fpr_train)\nplt.plot(fpr_train,tpr_train)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('Train Data')\n\nplt.subplot(1,2,2)\nplt.plot(fpr_test,fpr_test)\nplt.plot(fpr_test,tpr_test)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('Test Data')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Naive Bayes\n>  6.1 GaussianNB<br>\n>  6.2 GaussianNB SMOTE analysis<br>\n>  6.3 GaussianNB Hyper-parameter Tuning<br>","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.metrics import roc_curve","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">###  6.1 GaussianNB<br>","metadata":{}},{"cell_type":"code","source":"#Model built on transformed data\nnb = GaussianNB()\n\nmodel_eval(nb, X_train_sel, X_test_sel, y_train, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb = GaussianNB()\nnb.fit(X_train_sm, y_train_sm)\n    \n    #train datset\ny_train_sm_pred = nb.predict(X_train_sm)\ny_train_sm_proba = nb.predict_proba(X_train_sm)[::, 1]\n    \n    #test datset\ny_test_pred = nb.predict(X_test_sel)\ny_test_proba = nb.predict_proba(X_test_sel)[::, 1]\n\nfpr_train,tpr_train,threshold_train = roc_curve(y_train_sm,y_train_sm_proba )\nfpr_test,tpr_test,threshold_test = roc_curve(y_test,y_test_proba )\n\nprint(' GaussianNB :')\n\nplt.figure(figsize=(14,5))\nplt.subplot(1,2,1)\nplt.plot(fpr_train,fpr_train)\nplt.plot(fpr_train,tpr_train)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('Train Data')\n\nplt.subplot(1,2,2)\nplt.plot(fpr_test,fpr_test)\nplt.plot(fpr_test,tpr_test)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('Test Data')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ###  6.2 GaussianNB SMOTE analysis","metadata":{}},{"cell_type":"code","source":"nb = GaussianNB()\n\nmodel_eval(nb, X_train_sm, X_test_sel, y_train_sm, y_test)\n\n#Recall is increased but the model is over fitted, so we cant consider this model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### 6.3 GaussianNB Hyper-parameter Tuning","metadata":{}},{"cell_type":"code","source":"nb_classifier = GaussianNB()\n\n#default var_smoothing is 1e-09\n#We can try a range between 1e-0.15 to 1e-0.5\n\nparams_NB = {'var_smoothing': np.logspace(-5, -15, num=200)}\ngs_NB = GridSearchCV(estimator=nb_classifier, \n                 param_grid=params_NB, \n                 cv=3,   # use any cross validation technique \n                 verbose=1, \n                 scoring='accuracy') \ngs_NB.fit(X_full, y_full)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Best parameters\nprint(gs_NB.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gnb = GaussianNB(**gs_NB.best_params_)\n\n#As we got better result on GaussianNB for data without SMOTE analysis, we will use the same dataset\nmodel_eval(gnb,  X_train_sel, X_test_sel, y_train, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### GaussianNB model results before and after hyper-parameter tuning is same, Model is better fit to rest of the models","metadata":{}},{"cell_type":"code","source":"## Appending the evaluation metrics in a DataFrame for further reference\n## We can consider GaussianNB before hyper-parameter tuning\n\ngnb = GaussianNB()\n\nmodel_score_card(gnb, 'GaussianNB', X_train_sel, X_test_sel, y_train, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Overall results of all the models built","metadata":{}},{"cell_type":"code","source":"result_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final model selected","metadata":{}},{"cell_type":"code","source":"result_df.iloc[5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb = GaussianNB()\nnb.fit(X_train_sm, y_train_sm)\n    \n    #train datset\ny_train_sm_pred = nb.predict(X_train_sm)\ny_train_sm_proba = nb.predict_proba(X_train_sm)[::, 1]\n    \n    #test datset\ny_test_pred = nb.predict(X_test_sel)\ny_test_proba = nb.predict_proba(X_test_sel)[::, 1]\n\nfpr_train,tpr_train,threshold_train = roc_curve(y_train_sm,y_train_sm_proba )\nfpr_test,tpr_test,threshold_test = roc_curve(y_test,y_test_proba )\n\nprint(' GaussianNB :')\n\nplt.figure(figsize=(14,5))\nplt.subplot(1,2,1)\nplt.plot(fpr_train,fpr_train)\nplt.plot(fpr_train,tpr_train)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('Train Data')\n\nplt.subplot(1,2,2)\nplt.plot(fpr_test,fpr_test)\nplt.plot(fpr_test,tpr_test)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('Test Data')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### This model is better fit compared to other models","metadata":{}},{"cell_type":"markdown","source":"### Feature importance","metadata":{}},{"cell_type":"code","source":"from sklearn.inspection import permutation_importance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imps = permutation_importance(gnb, X_test_sel, y_test)\nprint(imps.importances_mean)\n\nfeatures = list(X_test_sel.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the feature ranking\nimportances = imps.importances_mean\nstd = imps.importances_std\nindices = np.argsort(importances)[::-1]\n\nprint(\"Feature ranking:\")\nfor f in range(X_test_sel.shape[1]):\n    print(\"%d. %s (%f)\" % (f + 1, features[indices[f]], importances[indices[f]]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(19, 8))\nplt.title(\"Feature importances\")\nplt.bar(range(X_test_sel.shape[1]), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X_test_sel.shape[1]), [features[indices[i]] for i in range(9)])\nplt.xlim([-1, X_test_sel.shape[1]])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#If we remove the less important features, then recall and AUC_score will be reduced","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#                                                     Thank you","metadata":{}},{"cell_type":"markdown","source":"#### I have tried other models also and have updated only the model with best performances\n##### Other models tried :\n>Logistic Regression<br>\nLogistic Regression Data_transformed<br>\nLogistic Regression SMOTE<br>\nLogistic Regression RFE<br>\nDecisionTreeClassifier<br>\nDecisionTreeClassifier HP-tuning<br>\nRandomForestClassifier<br>\nRandomForestClassifier HP-tuning<br>\nKNeighborsClassifier<br>\nKNeighborsClassifier Hp-tuning<br>\nLGBMClassifier<br>\nLGBMClassifier HP-tuning<br>\nGaussianNB w/o SMOTE<br>\nGradientBoostingClassifier<br>\nXGBoost<br>\nAdaBoost<br>\n","metadata":{}},{"cell_type":"markdown","source":"#### Please let me know, if i need to upload a notebook with these models.","metadata":{}},{"cell_type":"markdown","source":"# Thank You","metadata":{}}]}