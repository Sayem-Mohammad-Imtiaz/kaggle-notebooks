{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n%matplotlib inline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abb4ed74dfa99a7de35758d8021a727ecc836d5b"},"cell_type":"markdown","source":"**Importing Data**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"data = pd.read_csv(u'../input/500_Person_Gender_Height_Weight_Index.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"344462fb38f42b4716cd9c4f9327e07553babe4f"},"cell_type":"markdown","source":"**Checking for Missing Values(Any thin horizontal white lines in this image means a data is missing).**No missing values hence we are good to proceed "},{"metadata":{"trusted":true,"_uuid":"954dbae49d6ccf81b86e97dae4762ebdcebfc78c","collapsed":true},"cell_type":"code","source":"import missingno as msno\nmsno.matrix(data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"686802e999a88c409a36fa475ab6c224dea36a1a"},"cell_type":"markdown","source":"**Converting Weight from retarded units(lbs) to kg and visualising a scatterplot**"},{"metadata":{"trusted":true,"_uuid":"24d79b73b2a2e3370d87ceac288e250633df4fdf","collapsed":true},"cell_type":"code","source":"data['Weight'] = data['Weight'].apply(lambda x:0.453592*x)\ndata.plot.scatter(x='Weight', y='Height')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3af72bd45b094c05f23659ea4cc45757799b7a3"},"cell_type":"markdown","source":"**A hexbin density based plot would be more intuitive than scatterplot due high overlapping of data**"},{"metadata":{"trusted":true,"_uuid":"27ab20ab55565436525f00c43ed79244f94c311c","collapsed":true},"cell_type":"code","source":"#data.plot.hexbin(x=\"Weight\", y='Height',gridsize=15)\nimport seaborn as sns\nsns.jointplot(x='Weight', y='Height', data=data, kind='hex', gridsize=15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5739e3d304ca1805a056379f513e2beaefb7d411"},"cell_type":"markdown","source":"**Amount of people who are  0 - Extremely Weak, 1 - Weak, 2 - Normal, 3 - Overweight, 4 - Obesity, 5 - Extreme Obesity**"},{"metadata":{"trusted":true,"_uuid":"8bc4d739046af9501d2145d38304b1e3a53c5536","collapsed":true},"cell_type":"code","source":"#Index Reference : 0 - Extremely Weak, 1 - Weak, 2 - Normal, 3 - Overweight, 4 - Obesity, 5 - Extreme Obesity\ndata['Index'].value_counts().sort_index().plot.bar(\n    figsize=(12, 6),\n    fontsize=16,\n    title='Frequency of Body Height-Weight Index',\n    stacked=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f43975c688c18057026579ceb8f0080bb4a9cf3b","collapsed":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5775597ca29b28cc8812e7793260bde77fe36ff"},"cell_type":"markdown","source":"**Categorising data as 1 for male and 0 for female(Not trying to be offensive :P)**"},{"metadata":{"trusted":true,"_uuid":"cd704423eba0f7140b851d2df2a56d90e6cf0879","collapsed":true},"cell_type":"code","source":"data['Gender'].replace({'Male':1,'Female':0},inplace = True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0828fa6607a443643aa7bbed4f6d247d6713d3df"},"cell_type":"markdown","source":"Now moving onto Classification models we will try 4 basic methods of classification:\n1. K-NN Classification.\n2. Naive Bayes\n3. Random Forest\n4. XGB Classfier\nAfter classification we will compare all the models performance from it's confusion matrix and through k-fold crosss validation method."},{"metadata":{"_uuid":"5c9ef01c9e6eb00c238140644bee470a644d2d97"},"cell_type":"markdown","source":"**Preparing and splitting data for training and testing**"},{"metadata":{"trusted":true,"_uuid":"76d3ad7ff63aef3a67bb44a505d7e8226ca4f0f6","collapsed":true},"cell_type":"code","source":"X = data.iloc[:, [0,1,2]].values\ny = data.iloc[:, 3].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82b6b6f514d33f9e119b4de10bdd4ac4fe5e5a7d"},"cell_type":"markdown","source":"**The data will have 400 training data and 100 testing data**"},{"metadata":{"trusted":true,"_uuid":"6af08765a061f4253f6c9b0461b581f000def5f5","collapsed":true},"cell_type":"code","source":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8d5893e5b7df7fadc9311a2a13f3d32e732c62a"},"cell_type":"markdown","source":"Trying out the K-Nearest Neighbour classifier model and visualising the confusion matrix.\n\n**K-NN gives 86 % right prediction on the test set .**\n\n**The k-fold cross validation removes the data bias and gives an accuarcy of 86.51% ± 3.63% **"},{"metadata":{"trusted":true,"_uuid":"41ccd0df9c75171d32b32d337de8c3993cecde36","collapsed":true},"cell_type":"code","source":"# Fitting K-NN to the Training set\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a05f6ea32b546824a86b6df519a802b46840b737","collapsed":true},"cell_type":"code","source":"# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n# Making the Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\ncorrect = cm.diagonal().sum()/cm.sum()\nprint(\"% of right predictions {0}\".format(correct*100))\n# Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(accuracies.mean())\nprint(accuracies.std())\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nconf_arr = cm\n\nnorm_conf = []\nfor i in conf_arr:\n    a = 0\n    tmp_arr = []\n    a = sum(i, 0)\n    for j in i:\n        tmp_arr.append(float(j)/float(a))\n    norm_conf.append(tmp_arr)\n\nfig = plt.figure()\nplt.clf()\nax = fig.add_subplot(111)\nax.set_aspect(1)\nres = ax.imshow(np.array(norm_conf), cmap=plt.cm.jet, \n                interpolation='nearest')\n\nwidth, height = conf_arr.shape\n\nfor x in range(width):\n    for y in range(height):\n        ax.annotate(str(conf_arr[x][y]), xy=(y, x), \n                    horizontalalignment='center',\n                    verticalalignment='center')\n\ncb = fig.colorbar(res)\nalphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\nplt.xticks(range(width), alphabet[:width])\nplt.yticks(range(height), alphabet[:height])\nplt.savefig('confusion_matrix.png', format='png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7db46fccd107716ea882995549aa4ac95e4d050"},"cell_type":"markdown","source":"Trying out the Naive Bayes classifier model and visualising the confusion matrix.\n\n**Naive Bayes gives 67 % right prediction on the test set .**\n\n**The k-fold cross validation  gives an accuarcy of 71.54% ± 6.05% **"},{"metadata":{"trusted":true,"_uuid":"44ad5bae10a5ddbe1befe0ee67395a7bcc5fcf09","collapsed":true},"cell_type":"code","source":"# Fitting Naive Bayes to the Training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4aa6af10edcece7c378d225a68de651c68465897","collapsed":true},"cell_type":"code","source":"# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n# Making the Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\ncorrect = cm.diagonal().sum()/cm.sum()\nprint(\"% of right predictions {0}\".format(correct*100))\n# Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(accuracies.mean())\nprint(accuracies.std())\nconf_arr = cm\n\nnorm_conf = []\nfor i in conf_arr:\n    a = 0\n    tmp_arr = []\n    a = sum(i, 0)\n    for j in i:\n        tmp_arr.append(float(j)/float(a))\n    norm_conf.append(tmp_arr)\n\nfig = plt.figure()\nplt.clf()\nax = fig.add_subplot(111)\nax.set_aspect(1)\nres = ax.imshow(np.array(norm_conf), cmap=plt.cm.jet, \n                interpolation='nearest')\n\nwidth, height = conf_arr.shape\n\nfor x in range(width):\n    for y in range(height):\n        ax.annotate(str(conf_arr[x][y]), xy=(y, x), \n                    horizontalalignment='center',\n                    verticalalignment='center')\n\ncb = fig.colorbar(res)\nalphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\nplt.xticks(range(width), alphabet[:width])\nplt.yticks(range(height), alphabet[:height])\nplt.savefig('confusion_matrix.png', format='png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16fee139cd31bd85417a1ebae8233075ca394de2"},"cell_type":"markdown","source":"Trying out the Random Forest classifier model and visualising the confusion matrix.\n\n**Random Forest Classifier gives 89 % right prediction on the test set .**\n\n**The k-fold cross validation gives an accuarcy of 84.78% ± 4.82% **"},{"metadata":{"trusted":true,"_uuid":"f64bd3e50b46411e9ae6681f408201e01864f5ac","collapsed":true},"cell_type":"code","source":"# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 50, criterion = 'gini', random_state = 0)\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22a130146f4cbff398dd482f9da30678dbad20b9","collapsed":true},"cell_type":"code","source":"# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n# Making the Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\ncorrect = cm.diagonal().sum()/cm.sum()\nprint(\"% of right predictions {0}\".format(correct*100))\n# Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(accuracies.mean())\nprint(accuracies.std())\nconf_arr = cm\n\nnorm_conf = []\nfor i in conf_arr:\n    a = 0\n    tmp_arr = []\n    a = sum(i, 0)\n    for j in i:\n        tmp_arr.append(float(j)/float(a))\n    norm_conf.append(tmp_arr)\n\nfig = plt.figure()\nplt.clf()\nax = fig.add_subplot(111)\nax.set_aspect(1)\nres = ax.imshow(np.array(norm_conf), cmap=plt.cm.jet, \n                interpolation='nearest')\n\nwidth, height = conf_arr.shape\n\nfor x in range(width):\n    for y in range(height):\n        ax.annotate(str(conf_arr[x][y]), xy=(y, x), \n                    horizontalalignment='center',\n                    verticalalignment='center')\n\ncb = fig.colorbar(res)\nalphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\nplt.xticks(range(width), alphabet[:width])\nplt.yticks(range(height), alphabet[:height])\nplt.savefig('confusion_matrix.png', format='png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50d8a8400ffb3b6c63fdb9be9a11274a4878199a"},"cell_type":"markdown","source":"Trying out the XGB Classifier model and visualising the confusion matrix.\n\n**XGB Classifier gives 82 % right prediction on the test set .**\n\n**The k-fold cross validation gives an accuarcy of 84.90% ± 4.75% **"},{"metadata":{"trusted":true,"_uuid":"86d3ecb1693a2edb2bd6cf919949a09681e17afb","collapsed":true},"cell_type":"code","source":"# Fitting XGBoost to the Training set\nfrom xgboost import XGBClassifier\nclassifier = XGBClassifier()\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84c8a266f911fff3dbffeceeb13f1aba67da552c","collapsed":true},"cell_type":"code","source":"# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n# Making the Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\ncorrect = cm.diagonal().sum()/cm.sum()\nprint(\"% of right predictions {0}\".format(correct*100))\n# Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(accuracies.mean())\nprint(accuracies.std())\nconf_arr = cm\n\nnorm_conf = []\nfor i in conf_arr:\n    a = 0\n    tmp_arr = []\n    a = sum(i, 0)\n    for j in i:\n        tmp_arr.append(float(j)/float(a))\n    norm_conf.append(tmp_arr)\n\nfig = plt.figure()\nplt.clf()\nax = fig.add_subplot(111)\nax.set_aspect(1)\nres = ax.imshow(np.array(norm_conf), cmap=plt.cm.jet, \n                interpolation='nearest')\n\nwidth, height = conf_arr.shape\n\nfor x in range(width):\n    for y in range(height):\n        ax.annotate(str(conf_arr[x][y]), xy=(y, x), \n                    horizontalalignment='center',\n                    verticalalignment='center')\n\ncb = fig.colorbar(res)\nalphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\nplt.xticks(range(width), alphabet[:width])\nplt.yticks(range(height), alphabet[:height])\nplt.savefig('confusion_matrix.png', format='png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2adcd3653c9f813452b49ffc5aeea08f972bc61"},"cell_type":"markdown","source":"Now it is upto us to choose the model which is most suited.\n\nBoth Random Forest and XGB Classifiers are well suited.\n\nI would go wtih Random forest as I find it more intuitive."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1de8f21ee4cbd76ccfccb30d5c94c73e90dc36d9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}