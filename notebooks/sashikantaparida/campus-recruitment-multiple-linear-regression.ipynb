{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Campus Recruitment\n\n## Multiple Linear Regression","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading and Understanding the data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#reading the data\n\ndf = pd.read_csv('../input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv')\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* All the columns except 'salary' have no null values. \n* Salary for the students who are not placed has been mentioned as null. We can impute those values as zero.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# changing null values in the salary column to zero\n\ndf['salary'].fillna(0,inplace=True)\n\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now there is no null value in the dataframe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping the variable sl_no since it has no impact on the dependent variable\n\ndf.drop('sl_no',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the distribution of the data\n\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* All the columns except salary & etest_p are normally distributed. Distribution looks good.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for duplicate rows\n\ndf.loc[df.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting the distribution plot\n\ndf_num = df.select_dtypes(include=[np.number])\n\ncol_num = list(df_num.columns)\n\nc = len(col_num)\nm = 1\nn = 0\n\nplt.figure(figsize=(20,30))\n\nfor i in col_num:\n  if m in range(1,c+1):\n    plt.subplot(8,4,m)\n    sns.distplot(df_num[df_num.columns[n]])\n    m=m+1\n    n=n+1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are no duplicate rows.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the pairplot\n\nsns.heatmap(df.corr(),linewidth=0.5,cmap='YlGnBu',annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It is quite intresting to find out that salary does not depend on the MBA percentage or the employbility test percentage but there is a fair chance of getting a good salary if the student scores well in Secondary education.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data Preparation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* The value 'Others' have been used in three categorical columns, ssc_b,hsc_b & degree_t\n* It will create problem while converting the values to dummy variable and might result in same column name 'Others' for all these three columns.\n* We must make that value Unique.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.ssc_b.replace('Others','sscb_other',inplace=True)\ndf.hsc_b.replace('Others','hscb_other',inplace=True)\ndf.degree_t.replace('Others','deg_other',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for creating dummy variables for categorical variables\n\ndef dummy(x,df):\n    temp = pd.get_dummies(df[x],drop_first = True)\n    df =pd.concat([df,temp],axis=1)\n    df.drop(x,axis=1,inplace=True)\n    return df\n\n#Getting dummy variables for the categorical variables in df\ndf = dummy('status',df)\ndf = dummy('specialisation',df)\ndf = dummy('workex',df)\ndf = dummy('degree_t',df)\ndf = dummy('hsc_s',df)\ndf = dummy('hsc_b',df)\ndf = dummy('ssc_b',df)\ndf = dummy('gender',df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Lets rename the column 'Yes' to 'Workex' for better understanding.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.rename(columns={'Yes':'Workex','M':'Male'},inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Building","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Dividing the data into train and test sets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# We specify this so that the train and test data set always have the same rows, respectively\nnp.random.seed(1001)\n\ndf_train,df_test = train_test_split(df,test_size=0.2,random_state=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature Scaling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\ndf_train[col_num] = scaler.fit_transform(df_train[col_num])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Q1.  Develop an estimated multiple linear regression equation with mbap as response variable and sscp & hscp as the two predictor variables. Interpret the regression coefficients and check whether they are significant based on the summary","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Model 1","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Dividing the train and test set into X&y variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = df_train['mba_p']\nX_train = df_train[['ssc_p','hsc_p']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\n\n#Adding constant to X_train since by default statsmodel fits a regression line passing through the origin.\nX_train = sm.add_constant(X_train)\n\n#Fitting linear model\n\nlm = sm.OLS(y_train,X_train).fit()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#printing paramaters\n\nprint(lm.params)\n\n#Printing the summary\n\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The R-squared value is 0.193 which means only 19.3% variance in mba_p is explained by ssc_p and hsc_p.\n* Coefficients of both the independent variables has a very low p-value which means these are statistically significant.\n* But the F-statistics is very low which explains overall fit of the model is not statistically significant. We can do better by adding more variables.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Q2. Estimate a multiple regression equation for each of the below scenarios and based on the modelâ€™s R-square comment which model is better. \n\n#### (i) Use mbap as outcome variable and sscp & degreep as the two predictor variables. (Model 2)\n#### (ii) Use mbap as outcome variable and hscp & degreep as the two predictor variables. (Model 3)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Model 2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train2 = df_train['mba_p']\nX_train2 = df_train[['ssc_p','degree_p']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adding constant\nX_train2 = sm.add_constant(X_train2)\n\n#fitting linear model\n\nlm2 = sm.OLS(y_train2,X_train2).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Printing model parameters\n\nprint(lm2.params)\n\n#printing model summary\nprint(lm2.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The R-squared value and F-statistics have improved slightly.\n* The Independent variables have very low p-value which means ssc_p and degree_p are important features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Model 3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train3 = df_train['mba_p']\nX_train3 = df_train[['hsc_p','degree_p']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adding constant to X_tarin3\n\nX_train3 = sm.add_constant(X_train3)\n\n#fitting the linear model\n\nlm3 = sm.OLS(y_train3,X_train3).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lm3.params)\nprint(lm3.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The results are quite similar to Model 2. Lets take all three independent variables and see if we get any improvement.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Q3. Show the functional form of a multiple regression model. Build a regression model with mbap as dependent variable and sscp, hscp and degree_p as three independent variables.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Model 4","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train4 = df_train['mba_p']\nX_train4 = df_train[['ssc_p','hsc_p','degree_p']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adding constant to X_train4\n\nX_train4= sm.add_constant(X_train4)\n\n#Fitting the linear model\n\nlm4 = sm.OLS(y_train4,X_train4).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Printing coefficients and statistical summary\nprint(lm4.params)\n\nprint(lm4.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There isn't much improvement in the R2 score with ssc_p,hsc_p,degree_p as independent variable.\n* The F-statistics has dropped to 16.56 which means overall fit of the model with three variables is worse than the previous model.\n* The best model among these four models is Model 3. Lets do the residual analysis of the model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Residual Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting on the train data\n\ny_train_pred = lm4.predict(X_train4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the histogram of the error terms\n\nfig = plt.figure()\nsns.distplot((y_train4 - y_train_pred), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                \nplt.xlabel('Errors', fontsize = 18)   \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The residuals are normally distributed with mean zero, which satisfies our assumptions of Linear regression.\n* Lets predict on the test data now.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Prediction and evaluation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Applying scaling on test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transforming the numerical varianles of test data\ndf_test[col_num] = scaler.transform(df_test[col_num])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extracting X_test and y_test from the df_test  \n\nX_test = df_test[['ssc_p','hsc_p','degree_p']]\ny_test = df_test['mba_p']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adding constant\n\nX_test = sm.add_constant(X_test)\n\n#Predicting on th emodel\n\ny_pred = lm4.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluating R2 score on the predictions\n\nfrom sklearn.metrics import r2_score\n\nprint(r2_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Getting a R2 score which is slightly less than the training R2 score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting y_test and y_pred to understand the spread.\n\nfig = plt.figure()\nsns.scatterplot(y_test,y_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)   \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Putting y_test and y_pred to a dataframe.\n\ncompare_pred = pd.DataFrame(columns=['y_test','y_pred'])\ncompare_pred['y_test'] = y_test\ncompare_pred['y_pred'] = y_pred\n\ncompare_pred.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion:\n\n* As we can see above the R2 score of test data is very low and the predicted values on test data is far from the actual values.\n* Hence Secondary school percentage and Higher secondary school percentage are not valid factors of deciding MBA percentage of a student.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}