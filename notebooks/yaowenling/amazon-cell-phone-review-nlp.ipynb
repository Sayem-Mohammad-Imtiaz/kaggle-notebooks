{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Intro\n\nThis dataset contains 82,815 reviews from Amazon about cell phones from 2004 up until Sep 2019. Each review can be associated with an item and brand name and comes with a rating ranging from 1 to 5. This makes the dataset a perfect sample for text analytics and sentiment classification.\n\nWith multiple wordclouds and bar charts we found out that the two most popular brands are Apple and Samsung, and 96% iPhone offerings are renewed/refurbished models sold by carriers. Among all brands Xiaomi has the highest rating. Therefore we would deep dive into these three brands.\n\n🍎 Since almost all iPhones sold on Amazon are refurbished phones sold by carriers, buyers are satisfied when their purchases are in a (nearly) new condition and would complain if they find any scratches on the screen or the battery health is bad.\n\n𝙎 Samsung buyers are in general happy about the balance between quality and price. Their complaints are also mostly about battery and screen.\n\n\n🌾 Xiaomi buyers show specific appreciation about camera and the price-quality ratio - they really believe it's a good deal! However, they also complain about battery problem and some users have troubles in making phone calls.\n\nThe project also includes a non-supervised sentiment classification using VADER in the end. There is a strong positive correlation between the sentiment and the rating, which shows that VADER model is actually a very powerful tool.\n\nUpvote me if you like my solution! This means a lot for me and will keep me motivated for my next project!\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Step 0: Load datasets\nimport pandas as pd\nitems = pd.read_csv(\"/kaggle/input/amazon-cell-phones-reviews/20190928-items.csv\")\nreviews = pd.read_csv(\"/kaggle/input/amazon-cell-phones-reviews/20190928-reviews.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 0.1 Items overview\nprint(\"The dataset contains {0[0]: .0f} rows and {0[1]: .0f} variables.\".format(items.shape))\nitems.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.describe(include=\"all\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"ggplot\")\n# Brand distribution\nax = items.groupby(\"brand\").count()[\"asin\"].plot(kind=\"pie\", \n                                                 figsize=(8, 5),\n                                                 title=\"Number of Offerings grouped by Brand\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Average rating per brand\nax = items.groupby(\"brand\").mean()[\"rating\"].sort_values().plot(kind=\"barh\",\n                                                                figsize=(8,5), \n                                                                title=\"Average rating per Brand\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 0.2 Reviews overview\nprint(\"The dataset contains {0[0]: ,.0f} rows and {0[1]: .0f} variables.\".format(reviews.shape))\nreviews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 0.4 Link review data to item data bsed on common column \"asin\"\nreviews = pd.merge(reviews, items, how=\"left\", left_on=\"asin\", right_on=\"asin\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 0.5 Rename columns\nreviews.rename(columns={\"rating_x\": \"rating\", \"title_x\": \"title\", \"title_y\": \"item_title\", \"rating_y\": \"overall_rating\"}, inplace=True)\nreviews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 0.6 Convert string into datetime\nfrom datetime import datetime\nreviews[\"date\"] = reviews[\"date\"].apply(lambda x: datetime.strptime(x, '%B %d, %Y'))\nreviews[\"date\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 0.7 Truncate date column to month\nreviews[\"month\"] = reviews[\"date\"].apply(lambda x: x.replace(day=1))\nreviews[\"month\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 0.8 Plot reviews over time\nax = pd.pivot_table(reviews, \n                    index=\"month\", \n                    columns=\"brand\", \n                    values=\"asin\", \n                    aggfunc=\"count\", \n                    fill_value=0).plot.area(title=\"Monthly Number of Reviews per Brand\", figsize=(10, 6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Samsung and Apple are most rated brands, while Xiaomi has the highest average rating. Hence we will look deep into these three brands later."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 0.9 Add posivity label\nreviews[\"positivity\"] = reviews[\"rating\"].apply(lambda x: 1 if x>3 else(0 if x==3 else -1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Cleaning\n\nThis section creates functions to clean text, which includes:\n* Convert the text into lowercase\n* Remove punctuation\n* Remove stopwords (English, from nltk corpus)\n* Remove other keywords like \"phone\" and brand name"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 1: Preprocess review text\n# 1.1 Define preprocess function\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport string\nstop = set(stopwords.words('english'))\npunc = set(string.punctuation)\nkeywords = reviews[\"brand\"].apply(lambda x: x.lower()).unique().tolist()\nkeywords.append(\"phone\")\nlemma = WordNetLemmatizer()\ndef clean_text(text):\n    # Convert the text into lowercase\n    text = text.lower()\n    # Split into list\n    wordList = text.split()\n    # Remove punctuation\n    wordList = [\"\".join(x for x in word if (x==\"'\")|(x not in punc)) for word in wordList]\n    # Remove stopwords\n    wordList = [word for word in wordList if word not in stop]\n    # Remove other keywords\n    wordList = [word for word in wordList if word not in keywords]\n    # Lemmatisation\n    wordList = [lemma.lemmatize(word) for word in wordList]\n    return \" \".join(wordList)\nclean_text(\"I love reading books.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1.2 Apply preprocess function to the dataframe\nreviews[\"body\"] = reviews[\"body\"].astype(\"str\")\nreviews[\"clean_text\"] = reviews[\"body\"].apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at first 5 cleaned reviews:"},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews[\"clean_text\"].head().values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word Cloud\n\nGiven the size of the dataset, we created wordclouds only for the latest 1000 positive and negative reviews for Apple, Samsung and Xiaomi separately. The wordclouds are based on the frequency of words."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 2: Create a wordcloud\n# 2.1 Define word frequency function\ndef word_freq_dict(text):\n    # Convert text into word list\n    wordList = text.split()\n    # Generate word freq dictionary\n    wordFreqDict = {word: wordList.count(word) for word in wordList}\n    return wordFreqDict\nword_freq_dict(\"I love reading books. I love music.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.2 Create brand subsets\napple = reviews[reviews[\"brand\"]==\"Apple\"].sort_values(by=[\"date\"], ascending=False)\nsamsung = reviews[reviews[\"brand\"]==\"Samsung\"].sort_values(by=[\"date\"], ascending=False)\nxiaomi = reviews[reviews[\"brand\"]==\"Xiaomi\"].sort_values(by=[\"date\"], ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.3 Initializer\nfrom wordcloud import WordCloud, ImageColorGenerator\n\n# Define a function to create a wordcloud from dictionary of word frequency\ndef wordcloud_from_frequency(word_freq_dict, title, figure_size=(10, 6)):\n    wordcloud.generate_from_frequencies(word_freq_dict)\n    plt.figure(figsize=figure_size)\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.title(title)\n    plt.show()\n    \n# Define a function to plot top10 positive words and top10 negative words in a grouped bar plot (from dictionaries)\ndef topn_wordfreq_bar_both(pos_word_freq_dict, neg_word_freq_dict, pos_num_doc, neg_num_doc, topn, title, palette, height=6, aspect=2):\n    # Transform positive word frequency into DF\n    df_pos = pd.DataFrame.from_dict(pos_word_freq_dict, orient=\"index\").sort_values(by=0, ascending=False).head(topn)\n    df_pos.columns = [\"frequency\"]\n    df_pos[\"frequency\"] = df_pos[\"frequency\"] / pos_num_doc\n    df_pos[\"label\"] = \"Positive\"\n    # Transform negative word frequency into DF\n    df_neg = pd.DataFrame.from_dict(neg_word_freq_dict, orient=\"index\").sort_values(by=0, ascending=False).head(topn)\n    df_neg.columns = [\"frequency\"]\n    df_neg[\"frequency\"] = df_neg[\"frequency\"] / neg_num_doc\n    df_neg[\"label\"] = \"Negative\"\n    # Append two dataframes\n    df_append = df_pos.append(df_neg)\n    df_append.reset_index(inplace=True)\n    # Plot\n    sns.catplot(x=\"index\", y=\"frequency\", hue=\"label\", data=df_append, \n                kind=\"bar\",\n                palette=palette,\n                height=height, aspect=aspect, \n                legend_out=False)\n    plt.title(title)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is a list of color code for param \"colormap\": https://matplotlib.org/examples/color/colormaps_reference.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.4 Plot wordclouds for latest 1000 reviews for Apple\napple_pos = \" \".join(apple[apple[\"positivity\"]==1][\"clean_text\"][0:1000])\napple_pos_word_freq = word_freq_dict(apple_pos)\nwordcloud = WordCloud(width=5000, \n                      height=3000, \n                      max_words=200, \n                      colormap=\"Blues\",\n                      background_color=\"white\")\nwordcloud_from_frequency(apple_pos_word_freq, \"Most Frequent Words in the Latest 1000 Positive Reviews for Apple\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we check apple reviews that mention \"new\", they are most renewed phones - actually 96% of Apple reviews are for renewed iPhones that are sold on Amazon. Buyers are satisfied when they find their purchases are almost in the new condition."},{"metadata":{"trusted":true},"cell_type":"code","source":"apple[apple[\"clean_text\"].apply(lambda x: \"new\" in x)][\"item_title\"].value_counts().sort_values(ascending=True).tail(10).plot(kind=\"barh\")\nplt.title(\"Most reviews that mention 'new' are from renewed iPhone buyers\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"apple[\"renewed\"] = apple[\"item_title\"].apply(lambda x: (\"Renewed\" in x) | (\"Reburshied\" in x))\nprint(\"{0: 0.1%} iPhones that were sold on Amazon are renewed/reburshied.\".format(apple[\"renewed\"].sum() / len(apple[\"renewed\"])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"apple_neg = \" \".join(apple[apple[\"positivity\"]==-1][\"clean_text\"][0:1000])\napple_neg_word_freq = word_freq_dict(apple_neg)\nwordcloud = WordCloud(width=5000, \n                      height=3000, \n                      max_words=200, \n                      colormap=\"Blues\",\n                      background_color=\"black\")\nwordcloud_from_frequency(apple_neg_word_freq, \"Most Frequent Words in the Latest 1000 Negative Reviews for Apple\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topn_wordfreq_bar_both(apple_pos_word_freq, apple_neg_word_freq, \n                       min(sum(apple[\"positivity\"]==1), 1000), \n                       min(sum(apple[\"positivity\"]==-1), 1000), \n                       10, \n                       \"Top10 Frequent Words in Latest Positive and Negative Reviews for Apple\", \n                       [\"lightblue\", \"lightcoral\"], \n                       height=6, aspect=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.5 Plot wordclouds for latest 1000 reviews for Samsung\nsamsung_pos = \" \".join(samsung[samsung[\"positivity\"]==1][\"clean_text\"][0:1000])\nsamsung_pos_word_freq = word_freq_dict(samsung_pos)\nwordcloud = WordCloud(width=5000, \n                      height=3000, \n                      max_words=200, \n                      colormap=\"Greens\",\n                      background_color=\"white\")\nwordcloud_from_frequency(samsung_pos_word_freq, \"Most Frequent Words in the Latest 1000 Positive Reviews for Samsung\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samsung_neg = \" \".join(samsung[samsung[\"positivity\"]==-1][\"clean_text\"][0:1000])\nsamsung_neg_word_freq = word_freq_dict(samsung_neg)\nwordcloud = WordCloud(width=5000, \n                      height=3000, \n                      max_words=200, \n                      colormap=\"Greens\",\n                      background_color=\"black\")\nwordcloud_from_frequency(samsung_neg_word_freq, \"Most Frequent Words in the Latest 1000 Negative Reviews for Samsung\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topn_wordfreq_bar_both(samsung_pos_word_freq, samsung_neg_word_freq, \n                       min(sum(samsung[\"positivity\"]==1), 1000), \n                       min(sum(samsung[\"positivity\"]==-1), 1000), \n                       10, \n                       \"Top10 Frequent Words in Latest Positive and Negative Reviews for Samsung\", \n                       [\"steelblue\", \"orange\"], \n                       height=6, aspect=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.6 Plot wordclouds for latest 1000 reviews for Xiaomi\nxiaomi_pos = \" \".join(xiaomi[xiaomi[\"positivity\"]==1][\"clean_text\"][0:1000])\nxiaomi_pos_word_freq = word_freq_dict(xiaomi_pos)\nwordcloud = WordCloud(width=5000, \n                      height=3000, \n                      max_words=200, \n                      colormap=\"Oranges\",\n                      background_color=\"white\")\nwordcloud_from_frequency(xiaomi_pos_word_freq, \"Most Frequent Words in the Latest 1000 Positive Reviews for Xiaomi\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xiaomi_neg = \" \".join(xiaomi[xiaomi[\"positivity\"]==-1][\"clean_text\"][0:1000])\nxiaomi_neg_word_freq = word_freq_dict(xiaomi_neg)\nwordcloud = WordCloud(width=5000, \n                      height=3000, \n                      max_words=200, \n                      colormap=\"Oranges\",\n                      background_color=\"black\")\nwordcloud_from_frequency(xiaomi_neg_word_freq, \"Most Frequent Words in the Latest 1000 Negative Reviews for Xiaomi\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topn_wordfreq_bar_both(xiaomi_pos_word_freq, xiaomi_neg_word_freq, \n                       min(sum(xiaomi[\"positivity\"]==1), 1000), \n                       min(sum(xiaomi[\"positivity\"]==-1), 1000), \n                       10, \n                       \"Top10 Frequent Words in Latest Positive and Negative Reviews for Xiaomi\", \n                       [\"darkgreen\", \"pink\"], \n                       height=6, aspect=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The visulisations above show that:\n* The top topics that cell phone buyers focus on are battery health and screen condition.\n* Most iPhones that are sold on Amazon are renewed/refurbished. Apple buyers are satisfied if their purchases are in a (nearly) new condition, and would complain mostly when there is scratch on the screen or the battery health is bad.\n* Samsung buyers are in general positive about the overall performance of their purchases, but also complained about the screen. Some buyers also complained about unlocked phones sold by carriers.\n* Xiaomi buyers show specific appreciation for the good price and camera, and complained about the screen and making phone calls. Some buyers also have troubles when making phone calls."},{"metadata":{},"cell_type":"markdown","source":"# Vectorization and Topic Modelling\n\nTo extract features from each text, we need to convet the original text into a document-term matrix (https://en.wikipedia.org/wiki/Document-term_matrix), where the feature names are words and values are the scaled frequency of the words in each document.\n\nWhy \"scaled frequency\"? If we simply count the occurence of each word in each document, the highly frequent words start to dominate yet those words may not contain that much \"informational\" content (e.g. \"the\"). Therefore we use TF-IDF model to rescale the word frequency.\n* Term frequency: the frequency of a certain word in a document\n* Inverse document frequency: the scoring of how rare a certain word in all documents"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 3: Vectorization and Topic Modelling\n# 3.1 Initialize TF-IDF vectorizer\nimport time\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.05, stop_words=\"english\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For topic modelling we employ LDA model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3.2 Initalize LDA model\nfrom sklearn.decomposition import LatentDirichletAllocation\nn_topics=10\nlda = LatentDirichletAllocation(n_components=n_topics, \n                                max_iter=50, \n                                learning_method='online',\n                                learning_offset=50.,\n                                random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3.3 Define a function to print LDA topics\ndef print_topn_words(model, feature_names, topn):\n    for topic_idx, topic in enumerate(model.components_):\n        message = \"Topic #%d: \" % topic_idx\n        message += \" \".join([feature_names[i]\n                             for i in topic.argsort()[:-topn - 1:-1]])\n        print(message)\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3.4 Run LDA model for Apple\nt0 = time.time()\napple_tfidf = tfidf_vectorizer.fit_transform(apple[\"clean_text\"])\napple_tfidf_feature_names = tfidf_vectorizer.get_feature_names()\nlda.fit(apple_tfidf)\nprint(\"Below is the output from LDA model with {} topics (each includes Top10 words) for Apple.\".format(n_topics))\nprint_topn_words(lda, apple_tfidf_feature_names, 10)\nprint(\"Done in %0.3fs.\" % (time.time() - t0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The LDA topic modelling confirms our findings from the wordcloud:\n* Apple buyers complain about battery life.\n* If the buyer bought a refurbished iphone, he would pay particular attention to batter health and screen condition."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run the model for Samsung\nt0 = time.time()\nsamsung_tfidf = tfidf_vectorizer.fit_transform(samsung[\"clean_text\"])\nsamsung_tfidf_feature_names = tfidf_vectorizer.get_feature_names()\nlda.fit(samsung_tfidf)\nprint(\"Below is the output from LDA model with {} topics (each includes Top10 words) for Samsung.\".format(n_topics))\nprint_topn_words(lda, samsung_tfidf_feature_names, 10)\nprint(\"Done in %0.3fs.\" % (time.time() - t0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Samsung buyers are in general happy about the value-price combination. They also complained abbout battery life and screen condition."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run the model for Xiaomi\nt0 = time.time()\nxiaomi_tfidf = tfidf_vectorizer.fit_transform(xiaomi[\"clean_text\"])\nxiaomi_tfidf_feature_names = tfidf_vectorizer.get_feature_names()\nlda.fit(xiaomi_tfidf)\nprint(\"Below is the output from LDA model with {} topics (each includes Top10 words) for Xiaomi.\".format(n_topics))\nprint_topn_words(lda, xiaomi_tfidf_feature_names, 10)\nprint(\"Done in %0.3fs.\" % (time.time() - t0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Consistent with the wordcloud, Xiaomi buyers show highest satisfaction about the quality especially the camera."},{"metadata":{},"cell_type":"markdown","source":"# Feature Importance for Sentiment Classification\n\nThe document-term matrix we obtain fter running text vectorization can be used as a feature set. In the following example, we use it as the features to predict "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 4: Plot feature importance using XGBoost\n# 4.1 for Apple\nimport xgboost as xgb\nxgb_clf = xgb.XGBClassifier()\nxgb_clf.fit(apple_tfidf, apple[\"positivity\"])\nfeatureImport = pd.DataFrame(xgb_clf.feature_importances_, index=apple_tfidf_feature_names)\nfeatureImport.columns = [\"Importance\"]\nfeatureImport.sort_values([\"Importance\"], ascending=True).tail(20).plot(kind=\"barh\", figsize=(10, 6))\nplt.title(\"XGBoost Relative Feature Importance (from all reviews for Apple)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sentiment Analysis using VADER\n\nThere are two common approaches for text sentiment analysis: the lexical method and the machine learning methhod.\n\nThe lexcial method maps the new text to a pre-defined \"dictionary of sentiment\". VADER is one example of such method. Wtihe VADER, the sentiment score of a sentence is the normalised sum of sentiment scores of each word in that sentence.\n\nThe machine learning method is applied when an pre-labeled dataset is available and uses it as a training dataset to predict the sentiment of a new text. This is essentially a prediction/classification problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 1: Filter English reviews\n# 1.1 Add language labels (This part can take ~15 minutes)\n# from langdetect import detect\n# def lang_detect(text):\n#     try:\n#         return detect(text)\n#     except:\n#         return None\n# import time\n# start_time = time.time()\n# reviews[\"lang\"] = reviews[\"body\"].apply(lang_detect)\n# print(\"It takes %s seconds for the code to finish.\" % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1.2 Plot distribution of reviews into languages\n# reviews[\"lang\"].value_counts()[:10].plot(kind=\"barh\", title=\"Number of Reviews grouped by Top10 Language\")\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1.3 Only take English reviews\n# reviews = reviews[reviews[\"lang\"]==\"en\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 2: Sentiment analysis using Vader\n# 2.1 Load packages\n# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n# analyzer = SentimentIntensityAnalyzer()\n# analyzer.polarity_scores(\"The weather is nice today.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.2 Create sentiment score columns (It takes roughly 5 minutes)\n# start_time = time.time()\n# reviews[\"body\"] = reviews[\"body\"].astype(\"str\")\n# reviews[\"sent_neg\"] = reviews[\"body\"].apply(lambda x: analyzer.polarity_scores(x)[\"neg\"])\n# reviews[\"sent_neu\"] = reviews[\"body\"].apply(lambda x: analyzer.polarity_scores(x)[\"neu\"])\n# reviews[\"sent_pos\"] = reviews[\"body\"].apply(lambda x: analyzer.polarity_scores(x)[\"pos\"])\n# reviews[\"sent_comp\"] = reviews[\"body\"].apply(lambda x: analyzer.polarity_scores(x)[\"compound\"])\n# print(\"It takes %s seconds for the code to finish.\" % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.3 Save the datasets into csv\n# reviews.to_csv(\"reviews_with_sentiment_scores.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# After the steps above we will get a pre-proceessed dataset\nreviews_en = pd.read_csv(\"/kaggle/input/amazon-cell-phones-reviews-with-sentiment-scores/reviews_with_sentiment_scores.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.4 Plot the distribution of sentiment scores\nplt.figure()\n\nplt.subplot(2, 2, 1)\nreviews_en[\"sent_neg\"].hist(figsize=(10, 8), color=\"lightblue\")\nplt.title(\"Negative Sentiment Score\")\nplt.subplot(2, 2, 2)\nreviews_en[\"sent_neu\"].hist(figsize=(10, 8), color=\"grey\")\nplt.title(\"Neutral Sentiment Score\")\nplt.subplot(2, 2, 3)\nreviews_en[\"sent_pos\"].hist(figsize=(10, 8), color=\"lightgreen\")\nplt.title(\"Positive Sentiment Score\")\nplt.subplot(2, 2, 4)\nreviews_en[\"sent_comp\"].hist(figsize=(10, 8), color=\"lightcoral\")\nplt.title(\"Compound Sentiment Score\")\n\nplt.suptitle('Sentiment Analysis of Amazom Cell Phone Reviews', fontsize=12, fontweight='bold');\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The charts show that the majority of the reviews are positive (i.e. compoung score > 0). "},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.5 Check the correlation between sentiment score (compound) and rating\nimport numpy as np\nimport scipy.stats as stats\nprint(\"The correlation coefficient between sentiment score (compound) and rating is {0[0]: .4f} with a p-value of {0[1]: .4f}.\".format(stats.pearsonr(reviews_en[\"rating\"], reviews_en[\"sent_comp\"])))\nreviews_en.groupby(\"rating\").mean()[\"sent_comp\"].plot(kind=\"bar\", figsize=(10, 6))\nplt.title(\"Avg. Sentiment Score (Compound) per Rating\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a strong correlation between the sentiment score derived by VADER and the actual rating from the reviewers. The VADER model proves to be a very powerful tool for sentiment classification."},{"metadata":{},"cell_type":"markdown","source":"# Appendix\n\nWord frequency:\n* https://programminghistorian.org/en/lessons/counting-frequencies\n\nSentiment anlaysis: \n* Python implementation of VADER: https://medium.com/analytics-vidhya/simplifying-social-media-sentiment-analysis-using-vader-in-python-f9e6ec6fc52f\n* VADER method explaind: http://datameetsmedia.com/vader-sentiment-analysis-explained/\n* Documentation: https://github.com/cjhutto/vaderSentiment\n\nTopic modelling: \n* TF-IDF model: https://machinelearningmastery.com/gentle-introduction-bag-words-model/\n* Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation: https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py\n\nSentiment classification:\n* Feature extraction using TF-IDF: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.fit_transform\n* Classification: https://towardsdatascience.com/latent-semantic-analysis-sentiment-classification-with-python-5f657346f6a3"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}