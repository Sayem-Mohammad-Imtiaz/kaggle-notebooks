{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Importing libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport numpy as np\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom gensim.models import Word2Vec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading data\ndata = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.isnull().values.any()) # any NaN values\nprint(data.shape) # (row, column)\nprint(data['sentiment'].value_counts()) # check distribution of data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# change positive to 1, negative to 0\ndata['sentiment'] = data['sentiment'].replace({'positive':1, 'negative':0})\ndata['sentiment'] = data['sentiment'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print stopwords\n# A stop word is a commonly used word (such as “the”, “a”, “an”, “in”)\n# Ignore these words as it takes up space and processing time\nstop_words = set(stopwords.words('english'))\nprint(stop_words)\n\n# Stemming words - convert to base/root word by reducing the word to their stems, faster method for good enough results\nstemmer = SnowballStemmer(\"english\")\nprint(\"\\nStemming words: \"+stemmer.stem('studies')+','+stemmer.stem('studying'))\n\n# Lemmatization - convert to base/root word, considers part of speech as well, more complex and slower\nlemmatizer = WordNetLemmatizer()\nprint(\"\\nLemmatization: \"+lemmatizer.lemmatize('studies')+','+lemmatizer.lemmatize('studying', pos='v'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_review(review):\n    review = review.lower() # set to lower case\n    review = re.sub(\"<.*?>|'s\", '', review) # remove html tags and apostrophe\n    # convert short forms back to their respective words\n    review = re.sub(\"n't\", ' not ', review) \n    review = re.sub(\"'ve\", \" have \", review)\n    review = re.sub(\"can't\", \"cannot \", review)\n    review = re.sub(\"i'm\", \"i am \", review)\n    review = re.sub(\"'re\", \" are \", review)\n    review = re.sub(\"'d\", \" would \", review)\n    review = re.sub(\"'ll\", \" will \", review)\n    # convert sentences to tokens - splitting them into their words\n    tokens = word_tokenize(review)\n    filtered_tokens = [token for token in tokens if token not in stop_words and token.isalpha()] # remove stopwords and non-alphanumeric words\n    stemmed = [stemmer.stem(token) for token in filtered_tokens] # stemming the words\n    return stemmed#\" \".join(stemmed)\n\ndata['cleaned_review'] = data['review'].apply(clean_review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = max(data['cleaned_review'].apply(len))\nprint(MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting the data into train and test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# random_state for reproducibility, shuffle to shuffle data before splitting, stratify to maintain data distribution\n# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\nx_train, x_test, y_train, y_test = train_test_split(data['cleaned_review'], data['sentiment'], test_size=0.3, random_state=10,\n                                                    shuffle=True, stratify=data['sentiment'])\n\nprint(y_train.value_counts())\nprint(y_test.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word2Vec Embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"skip_gram = 1\nmin_count = 1\nwindow = 3\nword2vec = Word2Vec(x_train.values, window=window, min_count=min_count, sg=skip_gram, size=MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Convert texts into numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# word2vec.wv.syn0 is the word2vec matrix\nvocab_size, emdedding_size = word2vec.wv.syn0.shape\nprint(\"Vocab size: {}\".format(vocab_size))\n\n# this maps every word to a unique number\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, lower=True)\ntokenizer.fit_on_texts(x_train)\ntrain_sequence = tokenizer.texts_to_sequences(x_train)\n# need to pad the sequences so that every row of data has the same number of input\ntrain_sequence = tf.keras.preprocessing.sequence.pad_sequences(train_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n\ntest_sequence = tokenizer.texts_to_sequences(x_test)\ntest_sequence = tf.keras.preprocessing.sequence.pad_sequences(test_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n\nprint(\"First row of input data: {}\".format(train_sequence[0])) # padding of zeros added to match MAX_SEQUENCE_LENGTH\nword_list = list(tokenizer.word_index.items())\nprint(\"Mapping of vocab to their respective number: {} ... {}\".format(word_list[0], word_list[-1]))\n\n# getting the word2vec weights for the embedding layer\n# we have 1 to n number of vocab\n# due to padding, zeros are added and have to cater for it too, 0 to n number of values\n# expand the weights by 1 row filled with 0\nembedding_weights = np.vstack([np.zeros((1,MAX_SEQUENCE_LENGTH)), word2vec.wv.syn0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input_dim is vocab+1\n# set embedding layer to be not trainable as we have already trained it when building the word2vec\nembedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size+1, output_dim=emdedding_size, weights=[embedding_weights], trainable=False)\nlstm_layer = tf.keras.layers.LSTM(128)\ndense_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n\nmodel = tf.keras.Sequential([embedding_layer, lstm_layer, dense_layer])\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_sequence, y_train, epochs=100, batch_size=32, steps_per_epoch=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict on test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"probabilities = model.predict(test_sequence)\npredictions = np.where(probabilities>0.5,1,0)\nprint(\"Accuracy: {}\".format(accuracy_score(y_test, predictions)))\nprint(\"Confusion Matrix:\\n{}\".format(confusion_matrix(y_test, predictions)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}