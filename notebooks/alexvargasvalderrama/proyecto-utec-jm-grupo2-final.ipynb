{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Dirección en Kaggle: https://www.kaggle.com/alexvargasvalderrama/proyecto-utec-jm-grupo2-final\n!pip install pyspark\n!pip install langdetect\n!pip install nltk","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:08:06.014308Z","iopub.execute_input":"2021-08-29T00:08:06.014709Z","iopub.status.idle":"2021-08-29T00:08:54.647246Z","shell.execute_reply.started":"2021-08-29T00:08:06.014637Z","shell.execute_reply":"2021-08-29T00:08:54.646528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport plotly.express as px\nimport warnings \nfrom pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nwarnings.filterwarnings('ignore')\n\nspark = (\n    SparkSession.builder.appName(\"covid\")\n    .master(\"local[*]\")\n    .config(\"spark.driver.memory\", \"16g\")\n    .config(\"spark.executor.memory\", \"16g\")\n    .config(\"spark.driver.maxResultSize\", \"4g\")\n    .getOrCreate()\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:09:10.131746Z","iopub.execute_input":"2021-08-29T00:09:10.13204Z","iopub.status.idle":"2021-08-29T00:09:16.737548Z","shell.execute_reply.started":"2021-08-29T00:09:10.132013Z","shell.execute_reply":"2021-08-29T00:09:16.736646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parte 1. Carga y Limpeza de datos\n## Por el gran volumen de la data esta parte se realizó en el Khipu","metadata":{}},{"cell_type":"code","source":"#Cargamos el metadata del dataset CORD-19-research-challenge\ndf_csv = spark.read\\\n            .format(\"csv\")\\\n            .option(\"header\", \"true\")\\\n            .load(\"../input/CORD-19-research-challenge/metadata.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:40:53.221389Z","iopub.execute_input":"2021-08-28T07:40:53.221798Z","iopub.status.idle":"2021-08-28T07:40:53.359265Z","shell.execute_reply.started":"2021-08-28T07:40:53.221763Z","shell.execute_reply":"2021-08-28T07:40:53.358128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df_csv.toPandas() \ndf = df[['cord_uid','title','doi','abstract','publish_time','authors','journal','doi','pmcid','pubmed_id','pdf_json_files']]","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:41:13.676761Z","iopub.execute_input":"2021-08-28T07:41:13.677321Z","iopub.status.idle":"2021-08-28T07:41:49.000636Z","shell.execute_reply.started":"2021-08-28T07:41:13.677285Z","shell.execute_reply":"2021-08-28T07:41:48.999471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Eliminar artículos sin resúmenes\ndf = df[~df['abstract'].isnull()]","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:42:52.791147Z","iopub.execute_input":"2021-08-28T07:42:52.791603Z","iopub.status.idle":"2021-08-28T07:42:53.278351Z","shell.execute_reply.started":"2021-08-28T07:42:52.791569Z","shell.execute_reply":"2021-08-28T07:42:53.277213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reemplazamos las palabras clave de la sección de los abstract\ndf['abstract'] = df['abstract'].apply(lambda x: \n                                          x.replace('BACKGROUND:','').replace('BACKGROUNDS:','').replace('OBJECTIVES:','')\n                                          .replace('OBJECTIVE:','').replace('METHODS:','').replace('METHOD:','')\n                                          .replace('RESULTS:','').replace('RESULT:','')\n                                          .replace('CONCLUSION:','').replace('CONCLUSIONS:',''))","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:43:56.439053Z","iopub.execute_input":"2021-08-28T07:43:56.439449Z","iopub.status.idle":"2021-08-28T07:43:59.422573Z","shell.execute_reply.started":"2021-08-28T07:43:56.439404Z","shell.execute_reply":"2021-08-28T07:43:59.421469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convertimos el abstract a minúsculas\ndf['abstract'] = df['abstract'].apply(lambda x: x.lower())\n# Esto reemplaza las líneas que contienen el texto \"this article is protected by copyright. all rights reserved\"\ndf['abstract'] = df['abstract'].apply(lambda x: x.replace('this article is protected by copyright. all rights reserved',''))","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:44:28.05507Z","iopub.execute_input":"2021-08-28T07:44:28.055448Z","iopub.status.idle":"2021-08-28T07:44:31.727616Z","shell.execute_reply.started":"2021-08-28T07:44:28.055403Z","shell.execute_reply":"2021-08-28T07:44:31.726547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Conversión de la cadena timestamp a formato de fecha, que Python puede procesar\ndf['publish_time_new'] = pd.to_datetime(df['publish_time'], format='%Y-%m-%d',errors='coerce')","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:44:38.592178Z","iopub.execute_input":"2021-08-28T07:44:38.592603Z","iopub.status.idle":"2021-08-28T07:44:38.830883Z","shell.execute_reply.started":"2021-08-28T07:44:38.592565Z","shell.execute_reply":"2021-08-28T07:44:38.829904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removemos los artículos que fueron publicados antes del 01/01/2020\nimport datetime\ndf= df[df['publish_time_new']>'2020-01-01']","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:45:26.689973Z","iopub.execute_input":"2021-08-28T07:45:26.690398Z","iopub.status.idle":"2021-08-28T07:45:26.957663Z","shell.execute_reply.started":"2021-08-28T07:45:26.690366Z","shell.execute_reply":"2021-08-28T07:45:26.956556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detectamos y removemos artículos con abstracts escritos en otro idioma que no sea inglés\nfrom langdetect import detect, DetectorFactory\nDetectorFactory.seed = 0\ndef langdet (x):\n    try:\n        return detect(x)\n    except:\n        return \"NA\"\ndf['lang'] = df['abstract'].apply(lambda x: langdet(x))\ndf = df[df['lang'].str.contains('en')]","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:47:52.483246Z","iopub.execute_input":"2021-08-28T07:47:52.483702Z","iopub.status.idle":"2021-08-28T09:04:27.943382Z","shell.execute_reply.started":"2021-08-28T07:47:52.483665Z","shell.execute_reply":"2021-08-28T09:04:27.942029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Convertimos el abstract limpio a tokens y encontramos los unigramas y bigramas\nimport re\nimport nltk\nimport string\nfrom textblob import TextBlob\nstopword = nltk.corpus.stopwords.words('english')\nmy_file = open(\"/kaggle/input/stopword/stopwords.txt\", \"r\")\ncontent = my_file.read().split('\\n')\nstopword.extend(content)\nstopword = list(set(stopword))\nstopword = [w.strip() for w in stopword]\nstopword = set(stopword)\nwn = nltk.WordNetLemmatizer()\nps = nltk.PorterStemmer()\nfrom nltk import bigrams\n\ndef tokenization(text):\n    text = text.split()\n    text = ','.join(set(text))\n    return text\ndef clean_text(text):\n    text_lc = \" \".join([word.lower() for word in text.split() if word not in string.punctuation]) \n    text_rc = re.sub('[0-9]+', '', text_lc)\n    tokens = re.split('\\W+', text_rc)   \n    text = [word for word in tokens if word not in stopword]  \n    text = ' '.join(text)\n    return text\ndf['title'] = df['title'].apply(str)\ndf['title'] = df['title'].apply(lambda x: x.lower())\ndf['clean_text'] = df['abstract'].apply(lambda x: clean_text(x))\ndf['unigram'] = df['clean_text'].apply(lambda x: tokenization(x))\ndf['bigram']  = df['unigram'].apply(lambda x: ','.join([st[0].strip()+\" \"+st[1].strip() for st in list(bigrams(x.split(',')))]))","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:04:27.960895Z","iopub.execute_input":"2021-08-28T09:04:27.96137Z","iopub.status.idle":"2021-08-28T09:06:39.378832Z","shell.execute_reply.started":"2021-08-28T09:04:27.961323Z","shell.execute_reply":"2021-08-28T09:06:39.37755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Buscamos los términos que hacen referencia al coronavirus\ndf = df[((df['abstract'].str.contains('coronavirus|covid|2019-ncov|sars-cov'))|\n         (df['title'].str.contains('coronavirus|covid|2019-ncov|sars-cov')))]","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:06:39.380588Z","iopub.execute_input":"2021-08-28T09:06:39.380914Z","iopub.status.idle":"2021-08-28T09:06:46.192192Z","shell.execute_reply.started":"2021-08-28T09:06:39.380881Z","shell.execute_reply":"2021-08-28T09:06:46.191205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Buscamos los términos que hacen referencia a nuestro tema: Secuelas de Covid\ndf = df[((df['abstract'].str.contains('post-acute COVID-19 syndrome|complications|sequelae|consequence|hauler|long-term |chronic'))|\n         (df['title'].str.contains('post-acute COVID-19 syndrome|complications|sequelae|consequence|hauler|long-term |chronic')))]\n","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:55:59.933041Z","iopub.execute_input":"2021-08-28T03:55:59.933681Z","iopub.status.idle":"2021-08-28T03:56:14.122955Z","shell.execute_reply.started":"2021-08-28T03:55:59.933638Z","shell.execute_reply":"2021-08-28T03:56:14.12152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Eliminamos duplicados basados en el mismo título\nhas_dup = df.duplicated(subset =\"title\", keep=False)\ndup = df[has_dup]\ndf = df[~has_dup]\ndup = dup.fillna('-999')","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:56:28.271223Z","iopub.execute_input":"2021-08-28T03:56:28.271846Z","iopub.status.idle":"2021-08-28T03:56:28.517814Z","shell.execute_reply.started":"2021-08-28T03:56:28.271787Z","shell.execute_reply":"2021-08-28T03:56:28.516739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Llenamos los datos faltantes con -999 para facilitar la búsqueda\ndup1 = dup[~((dup['journal'].str.contains('-999'))|((dup['pmcid'].str.contains('-999')))|((dup['pubmed_id'].astype(str).str.contains('-999')))|((dup['doi'].str.contains('-999'))))]\ndup1 = dup1.drop_duplicates(subset =\"title\", keep='first')","metadata":{"execution":{"iopub.status.busy":"2021-08-28T04:34:44.088669Z","iopub.execute_input":"2021-08-28T04:34:44.089298Z","iopub.status.idle":"2021-08-28T04:34:44.190034Z","shell.execute_reply.started":"2021-08-28T04:34:44.089258Z","shell.execute_reply":"2021-08-28T04:34:44.188486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Combinamos los artículos únicos de estrads duplicadas y artículos no duplicados\ndf1 = pd.concat([df,dup1])","metadata":{"execution":{"iopub.status.busy":"2021-08-28T04:36:38.512812Z","iopub.execute_input":"2021-08-28T04:36:38.513393Z","iopub.status.idle":"2021-08-28T04:36:38.61106Z","shell.execute_reply.started":"2021-08-28T04:36:38.513355Z","shell.execute_reply":"2021-08-28T04:36:38.609746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#Finalmente guardamos la data limpia en un archivo separado\ndf1 = df1.reset_index()\ndf1 = df1.drop(['doi.1'], axis=1)\ndf1.to_csv('Selected_articles_clean_text_eng_duplicate_removed_01.csv', index=None)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T04:36:46.4057Z","iopub.execute_input":"2021-08-28T04:36:46.406186Z","iopub.status.idle":"2021-08-28T04:36:46.496486Z","shell.execute_reply.started":"2021-08-28T04:36:46.406149Z","shell.execute_reply":"2021-08-28T04:36:46.493403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parte 2: Aplicación de Machine Learning para agruparlos por clusters","metadata":{}},{"cell_type":"code","source":"#Cargamos la data limpia\ndf = pd.read_csv('../input/data-procesada/Selected_articles_clean_text_eng_duplicate_removed_01.csv').fillna('')","metadata":{"execution":{"iopub.status.busy":"2021-08-28T04:38:38.48494Z","iopub.execute_input":"2021-08-28T04:38:38.485469Z","iopub.status.idle":"2021-08-28T04:38:42.263455Z","shell.execute_reply.started":"2021-08-28T04:38:38.485425Z","shell.execute_reply":"2021-08-28T04:38:42.262268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Para la ingeniería de variables usamos TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk\nimport re\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(\"english\")\ndef tokenize_and_stem(text):\n    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n    filtered_tokens = []\n    for token in tokens:\n        if re.search('[a-zA-Z]', token):\n            filtered_tokens.append(token)\n    stems = [stemmer.stem(t) for t in filtered_tokens]\n    return stems\n\n#Definimos los parametros del TfidfVectorizer\n# Parametros: max_df=0.90 and min_df=10\ntfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=10, stop_words='english',\n                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,2))\n\n\n%time tfidf_matrix = tfidf_vectorizer.fit_transform(df['clean_text'].tolist()) \n\nprint(tfidf_matrix.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T04:43:50.737261Z","iopub.execute_input":"2021-08-28T04:43:50.737729Z","iopub.status.idle":"2021-08-28T04:45:53.397662Z","shell.execute_reply.started":"2021-08-28T04:43:50.737694Z","shell.execute_reply":"2021-08-28T04:45:53.396694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Buscamos el posible número de clusters usando el método Elbow\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.decomposition import TruncatedSVD\n\npca = TruncatedSVD(n_components=200)\nX = pca.fit_transform(tfidf_matrix)\n\n\n# Iniciamos el modelo de clusters y visualizamos\nmodel = KMeans()\nvisualizer = KElbowVisualizer(model, k=(2,50))\n\nvisualizer.fit(X)        # Encaja el modelo en el visualizador\nvisualizer.show()        # Genera la figura y para guardar la figura usamos: outpath=\"kelbow_kmeans.png\" ","metadata":{"execution":{"iopub.status.busy":"2021-08-28T04:47:04.826592Z","iopub.execute_input":"2021-08-28T04:47:04.827033Z","iopub.status.idle":"2021-08-28T05:16:13.924984Z","shell.execute_reply.started":"2021-08-28T04:47:04.826999Z","shell.execute_reply":"2021-08-28T05:16:13.923429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Usamos 21 como input del KMeans clustering como sugerencia del método Elbow que indica 21 clusters\nX = tfidf_matrix\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters = 21, init = 'k-means++', random_state = 0)\nkm.fit(X)\npredict = km.predict(X)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T05:16:37.28365Z","iopub.execute_input":"2021-08-28T05:16:37.28412Z","iopub.status.idle":"2021-08-28T05:18:13.938264Z","shell.execute_reply.started":"2021-08-28T05:16:37.284061Z","shell.execute_reply":"2021-08-28T05:18:13.937137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Asignamos el nro. de cluster al abstract\ndf['cluster'] = pd.Series(predict, index = df.index)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T05:19:05.025012Z","iopub.execute_input":"2021-08-28T05:19:05.025493Z","iopub.status.idle":"2021-08-28T05:19:05.035348Z","shell.execute_reply.started":"2021-08-28T05:19:05.025455Z","shell.execute_reply":"2021-08-28T05:19:05.033212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Grabamos en un archivo el resultado final con la asignación del número de cluster\ndf.to_csv('kmeans_resultados.csv',sep=\"|\",index=None)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T05:19:07.537172Z","iopub.execute_input":"2021-08-28T05:19:07.5376Z","iopub.status.idle":"2021-08-28T05:19:13.766013Z","shell.execute_reply.started":"2021-08-28T05:19:07.537566Z","shell.execute_reply":"2021-08-28T05:19:13.764487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extraemos los top de unigramas y bigramas de cada cluster\n# para que sea evaluado por los expertos y validar las etiquetas de cada cluster\nimport matplotlib.pyplot as plt; plt.rcdefaults()\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport math\nclusters = []\nfor i in range(21):\n#   unigramas  \n    print('Topic '+str(i)+' :')\n    terms = df[df['cluster']==i]['unigram'].tolist()\n    stats = 'Topic '+str(i)+' stats: '+ str(len(terms)/len(df)*100)+\"% (\"+str(len(terms))+\"/\"+str(len(df))+\")\"\n    print(stats)\n    print(i)\n    cnt = Counter([x.strip() for st in terms for x in st.split(',')])\n    del cnt['']\n    counter = cnt.most_common(100)\n    uni = ', '.join([val[0] for val in counter])\n    print('Topic'+str(i)+' top unigrams : '+uni)\n    print('\\n')\n    \n#   bigramas\n    terms = df[df['cluster']==i]['bigram'].tolist()\n    cnt = Counter([x.strip() for st in terms for x in st.split(',')])\n    del cnt['']\n    counter = cnt.most_common(100)\n    bi = ', '.join([val[0] for val in counter])\n    print('Topic'+str(i)+' top bigrams : '+bi)\n    print('\\n')\n    \n    clusters.append([stats,uni,bi])\n    \n    print('\\n\\n\\n\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-28T05:19:29.035911Z","iopub.execute_input":"2021-08-28T05:19:29.036363Z","iopub.status.idle":"2021-08-28T05:19:32.212709Z","shell.execute_reply.started":"2021-08-28T05:19:29.036328Z","shell.execute_reply":"2021-08-28T05:19:32.211638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Grabamos un archivo con el top 100 de los unigramas y bigramas para el analisis de expertos \ncluster_results = pd.DataFrame(clusters, columns=['Stats','Top100Unigrams','Top100Bigrams'])\ncluster_results['ClusterNumber'] = cluster_results.reset_index().index\ncluster_results= cluster_results[['ClusterNumber','Stats','Top100Unigrams','Top100Bigrams']]\ncluster_results.to_csv('top_terms_in_clusters_new.csv',index=None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Luego del analisis de expertos, se procede a unir algunos clusters\n# Para ello cargamos el archivo con los cluster actuales\ndf = pd.read_csv('../input/dataresultado/kmeans_resultados.csv',sep=\"|\").fillna('')","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:12:07.592093Z","iopub.execute_input":"2021-08-29T00:12:07.592431Z","iopub.status.idle":"2021-08-29T00:12:08.833694Z","shell.execute_reply.started":"2021-08-29T00:12:07.592381Z","shell.execute_reply":"2021-08-29T00:12:08.832539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Luego del análisis de expertos, se procede a unir algunos clusters\n#Definimos la función que cambia los cluster luego del análisis de expertos y luego la ejecutamos\n\ndef reclasificar(x):\n    if x==19:\n        return 4\n    elif x==15:\n        return 5\n    elif x==13:\n        return 11\n    elif x==18:\n        return 12\n    elif x==14 or x==17:\n        return 13\n    elif x==16:\n        return 14\n    elif x==20:\n        return 15\n    else:\n        return x\ndf['cluster'] = df['cluster'].apply(lambda x: reclasificar(x))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:13:27.56222Z","iopub.execute_input":"2021-08-29T00:13:27.562578Z","iopub.status.idle":"2021-08-29T00:13:28.811047Z","shell.execute_reply.started":"2021-08-29T00:13:27.56255Z","shell.execute_reply":"2021-08-29T00:13:28.810026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Grabamos en un archivo el resultado final con la asignación del número de cluster actualizado luego\n# del análisis de expertos\ndf.to_csv('kmeans_resultados_final.csv',sep=\"|\",index=None)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:22:16.458986Z","iopub.execute_input":"2021-08-29T00:22:16.459295Z","iopub.status.idle":"2021-08-29T00:22:19.774261Z","shell.execute_reply.started":"2021-08-29T00:22:16.459265Z","shell.execute_reply":"2021-08-29T00:22:19.773702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extraemos la versión final de los top de unigramas y bigramas de cada cluster\n# luego del análisis de los expertos\nimport matplotlib.pyplot as plt; plt.rcdefaults()\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport math\nclusters = []\nfor i in range(16):\n#   unigramas  \n    print('Topic '+str(i)+' :')\n    terms = df[df['cluster']==i]['unigram'].tolist()\n    stats = 'Topic '+str(i)+' stats: '+ str(len(terms)/len(df)*100)+\"% (\"+str(len(terms))+\"/\"+str(len(df))+\")\"\n    print(stats)\n    print(i)\n    cnt = Counter([x.strip() for st in terms for x in st.split(',')])\n    del cnt['']\n    counter = cnt.most_common(100)\n    uni = ', '.join([val[0] for val in counter])\n    print('Topic'+str(i)+' top unigrams : '+uni)\n    print('\\n')\n    \n#   bigramas\n    terms = df[df['cluster']==i]['bigram'].tolist()\n    cnt = Counter([x.strip() for st in terms for x in st.split(',')])\n    del cnt['']\n    counter = cnt.most_common(100)\n    bi = ', '.join([val[0] for val in counter])\n    print('Topic'+str(i)+' top bigrams : '+bi)\n    print('\\n')\n    \n    clusters.append([stats,uni,bi])\n    \n    print('\\n\\n\\n\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:37:59.593128Z","iopub.execute_input":"2021-08-29T00:37:59.593418Z","iopub.status.idle":"2021-08-29T00:38:01.615289Z","shell.execute_reply.started":"2021-08-29T00:37:59.593379Z","shell.execute_reply":"2021-08-29T00:38:01.614231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Grabamos un archivo con el top 100 de los unigramas y bigramas luego del análisis de expertos \ncluster_results = pd.DataFrame(clusters, columns=['Stats','Top100Unigrams','Top100Bigrams'])\ncluster_results['ClusterNumber'] = cluster_results.reset_index().index\ncluster_results= cluster_results[['ClusterNumber','Stats','Top100Unigrams','Top100Bigrams']]\ncluster_results.to_csv('top_terms_in_clusters_new_final.csv',index=None)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:38:29.737863Z","iopub.execute_input":"2021-08-29T00:38:29.738137Z","iopub.status.idle":"2021-08-29T00:38:29.74918Z","shell.execute_reply.started":"2021-08-29T00:38:29.738114Z","shell.execute_reply":"2021-08-29T00:38:29.748446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parte 3: Análisis descriptivo con SPARK SQL","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import isnan, when, count, col, year, month, to_date","metadata":{"execution":{"iopub.status.busy":"2021-08-28T06:10:53.104912Z","iopub.execute_input":"2021-08-28T06:10:53.1053Z","iopub.status.idle":"2021-08-28T06:10:53.110809Z","shell.execute_reply.started":"2021-08-28T06:10:53.105264Z","shell.execute_reply":"2021-08-28T06:10:53.109808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cargo los datos reultantes del punto anterior\ndf_consultas = spark.read\\\n            .format(\"csv\")\\\n            .option(\"header\", \"true\")\\\n            .option(\"sep\",\"|\")\\\n            .load(\"../input/data-resultado-final/kmeans_resultados_final.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:38:45.400919Z","iopub.execute_input":"2021-08-29T00:38:45.401205Z","iopub.status.idle":"2021-08-29T00:38:49.369225Z","shell.execute_reply.started":"2021-08-29T00:38:45.40118Z","shell.execute_reply":"2021-08-29T00:38:49.368356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Agregamos la columna country para coloar el pais del autor del artículo\ndf_consultas = df_consultas.withColumn(\"country\", lit(\"\"))\ndf_consultas_j=df_consultas.toPandas() ","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:38:51.691439Z","iopub.execute_input":"2021-08-29T00:38:51.691763Z","iopub.status.idle":"2021-08-29T00:38:55.429865Z","shell.execute_reply.started":"2021-08-29T00:38:51.691735Z","shell.execute_reply":"2021-08-29T00:38:55.428826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Como el país del autor del artículo no se encuentra en el metadata se está buscando\n#en el archivo json a fin, en caso existiese, identificar el país del autor, en caso de ser varios\n#se toma el que aparece en primer orden\nimport json\nfor s in range(len(df_consultas_j)):\n    if(df_consultas_j['pdf_json_files'][s]==None):\n        df_consultas_j['country'][s]=''\n    else:\n        try:\n            with open('../input/CORD-19-research-challenge/'+df_consultas_j['pdf_json_files'][s],'r') as miarchivo:\n                datos=miarchivo.read()\n            objeto=json.loads(datos)\n            if(len(objeto['metadata']['authors'])==0):\n               df_consultas_j['country'][s]=''\n            for p in range(len(objeto['metadata']['authors'])):\n                try:\n                    if objeto['metadata']['authors'][p]['affiliation']['location']['country'].strip()!='':\n                        df_consultas_j['country'][s]=objeto['metadata']['authors'][p]['affiliation']['location']['country'].strip()\n                        p=len(objeto['metadata']['authors'])\n                except:\n                    p=p+1;\n        except:\n            df_consultas_j['country'][s]='';      ","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:38:57.760458Z","iopub.execute_input":"2021-08-29T00:38:57.760737Z","iopub.status.idle":"2021-08-29T00:42:45.088343Z","shell.execute_reply.started":"2021-08-29T00:38:57.760713Z","shell.execute_reply":"2021-08-29T00:42:45.087439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_consultas=spark.createDataFrame(df_consultas_j) ","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:44:28.268057Z","iopub.execute_input":"2021-08-29T00:44:28.268428Z","iopub.status.idle":"2021-08-29T00:44:31.421342Z","shell.execute_reply.started":"2021-08-29T00:44:28.268377Z","shell.execute_reply":"2021-08-29T00:44:31.4203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Agregamos las colulmas publish_Year (Año de publicación), publish_Month (Mes de publicación) y\n# publish_Week (Semana de publicación)\ndf_consultas = df_consultas.withColumn(\"publish_Year\", year(to_date(\"publish_time\")))\\\n                        .withColumn(\"publish_Month\", month(to_date(\"publish_time\")))\\\n                        .withColumn(\"publish_Week\", weekofyear(to_date(\"publish_time\")))\\\n                        .withColumn(\"ones\", lit(1)).createOrReplaceTempView(\"consultas\")","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:44:33.294291Z","iopub.execute_input":"2021-08-29T00:44:33.294625Z","iopub.status.idle":"2021-08-29T00:44:33.643112Z","shell.execute_reply.started":"2021-08-29T00:44:33.294598Z","shell.execute_reply":"2021-08-29T00:44:33.642558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reporte por tipos de publicaciones\n# Como revisión sistemática, revisión de alcance, etc.\nspark.sql(\"select 'Systematic review' Tipo_de_Publicaciones,count(*) Cantidad from consultas where \"\\\n\"abstract like '%systematic review%' or abstract like '%systematic literature review%'\"\\\n\" or title like '%systematic review%' or title like '%systematic literature review%' group by 'Systematic review'\"\\\n\" union all \"\\\n\"select 'Meta-analysis' Tipo_de_Publicaciones,count(*) Cantidad from consultas where \"\\\n\"abstract like '%meta-analysis%' or abstract like '%metaanalysis%'\"\\\n\" or title like '%meta-analysis%' or title like '%metaanalysis%' group by 'Meta-analysis'\"\\\n\" union all \"\\\n\"select 'Scoping review' Tipo_de_Publicaciones,count(*) Cantidad from consultas where \"\\\n\"abstract like '%scoping review%' or abstract like '%scoping literature review%'\"\\\n\"or title like '%scoping review%' or title like '%scoping literature review%' group by 'Scoping review'\"\\\n\" union all \"\\\n\"select 'Randomised control trial' Tipo_de_Publicaciones,count(*) Cantidad from consultas where \"\\\n\"abstract like '%randomised control trial%' or abstract like '%randomized control trial%'\"\\\n\" or abstract like '%randomised controlled trial%' or abstract like '%randomized controlled trial%'\"\\\n\" or abstract like '%randomized clinical trial%' or abstract like '%randomised clinical trial%'\"\\\n\" or title like '%randomised control trial%' or title like '%randomized control trial%'\"\\\n\" or title like '%randomised controlled trial%' or title like '%randomized controlled trial%'\"\\\n\" or title like '%randomized clinical trial%' or title like '%randomised clinical trial%' group by 'Randomised control trial'\"\\\n\" union all \"\\\n\"select 'Survey' Tipo_de_Publicaciones,count(*) Cantidad from consultas where \"\\\n\"abstract like '%survey%'\"\\\n\" or title like '%survey%' group by 'Survey'\"\\\n\" union all \"\\\n\"select 'Case-control study' Tipo_de_Publicaciones,count(*) Cantidad from consultas where \"\\\n\"abstract like '%case-control study%' or abstract like '%case control study%'\"\\\n\" or title like '%case-control study%' or title like '%case control study%' group by 'Case-control study'\"\\\n\" union all \"\\\n\"select 'Cohort study' Tipo_de_Publicaciones,count(*) Cantidad from consultas where \"\\\n\"abstract like '%cohort study%'\"\\\n\" or title like '%cohort study%' group by 'Cohort study'\"\\\n\" union all \"\\\n\"select 'Case study' Tipo_de_Publicaciones,count(*) Cantidad from consultas where \"\\\n\"abstract like '%case study%'\"\\\n\" or title like '%case study%' group by 'Case study'\"\\\n\" order by Cantidad desc\").show() \n\n#Guardamos el reporte en csv\nspark.sql(\"select 'Systematic review' Tipo_de_Publicaciones,count(*) Cantidad from consultas where \"\\\n\"abstract like '%systematic review%' or abstract like '%systematic literature review%'\"\\\n\" or title like '%systematic review%' or title like '%systematic literature review%' group by 'Systematic review'\"\\\n\" union all \"\\\n\"select 'Meta-analysis' Tipo_de_Publicaciones,count(*) Cantidad from consultas where \"\\\n\"abstract like '%meta-analysis%' or abstract like '%metaanalysis%'\"\\\n\" or title like '%meta-analysis%' or title like '%metaanalysis%' group by 'Meta-analysis'\"\\\n\" union all \"\\\n\"select 'Scoping review' Tipo_de_Publicaciones,count(*) Cantidad from consultas where \"\\\n\"abstract like '%scoping review%' or abstract like '%scoping literature review%'\"\\\n\"or title like '%scoping review%' or title like '%scoping literature review%' group by 'Scoping review'\"\\\n\" union all \"\\\n\"select 'Randomised control trial' Tipo_de_Publicaciones,count(*) Cantidad from consultas where \"\\\n\"abstract like '%randomised control trial%' or abstract like '%randomized control trial%'\"\\\n\" or abstract like '%randomised controlled trial%' or abstract like '%randomized controlled trial%'\"\\\n\" or abstract like '%randomized clinical trial%' or abstract like '%randomised clinical trial%'\"\\\n\" or title like '%randomised control trial%' or title like '%randomized control trial%'\"\\\n\" or title like '%randomised controlled trial%' or title like '%randomized controlled trial%'\"\\\n\" or title like '%randomized clinical trial%' or title like '%randomised clinical trial%' group by 'Randomised control trial'\"\\\n\" union all \"\\\n\"select 'Survey' Tipo_de_Publicaciones,count(*) Cantidad from consultas where \"\\\n\"abstract like '%survey%'\"\\\n\" or title like '%survey%' group by 'Survey'\"\\\n\" union all \"\\\n\"select 'Case-control study' Tipo_de_Publicaciones,count(*) Cantidad from consultas where \"\\\n\"abstract like '%case-control study%' or abstract like '%case control study%'\"\\\n\" or title like '%case-control study%' or title like '%case control study%' group by 'Case-control study'\"\\\n\" union all \"\\\n\"select 'Cohort study' Tipo_de_Publicaciones,count(*) Cantidad from consultas where \"\\\n\"abstract like '%cohort study%'\"\\\n\" or title like '%cohort study%' group by 'Cohort study'\"\\\n\" union all \"\\\n\"select 'Case study' Tipo_de_Publicaciones,count(*) Cantidad from consultas where \"\\\n\"abstract like '%case study%'\"\\\n\" or title like '%case study%' group by 'Case study'\"\\\n\" order by Cantidad desc\").toPandas().to_csv('reporte_tipo_publicaciones.csv',sep=\"|\",index=None)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:44:35.915838Z","iopub.execute_input":"2021-08-29T00:44:35.9161Z","iopub.status.idle":"2021-08-29T00:45:07.046578Z","shell.execute_reply.started":"2021-08-29T00:44:35.916077Z","shell.execute_reply":"2021-08-29T00:45:07.045338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reporte estadístico agrupado por día de publicación\nspark.sql(\"Select publish_time_new,count(ones) cantidad,mean(ones) media,\"\\\n          \"stddev(ones) std,min(ones) min,percentile(ones,0.25) Q1_25,\"\\\n          \"percentile(ones,0.5) Q2_50,percentile(ones,0.75) Q3_75,max(ones) max,\"\\\n          \"sum(ones) suma from consultas group by publish_time_new order by publish_time_new\").show()\n#Guardamos el reporte en CSV\nspark.sql(\"Select publish_time_new,count(ones) cantidad,mean(ones) media,\"\\\n          \"stddev(ones) std,min(ones) min,percentile(ones,0.25) Q1_25,\"\\\n          \"percentile(ones,0.5) Q2_50,percentile(ones,0.75) Q3_75,max(ones) max,\"\\\n          \"sum(ones) suma from consultas group by publish_time_new order by publish_time_new\").toPandas().to_csv('reporte_dia_publicacion.csv',sep=\"|\",index=None)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:45:07.048325Z","iopub.execute_input":"2021-08-29T00:45:07.048922Z","iopub.status.idle":"2021-08-29T00:45:16.686604Z","shell.execute_reply.started":"2021-08-29T00:45:07.048884Z","shell.execute_reply":"2021-08-29T00:45:16.685617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reporte estadístico agrupado por semana de publicación\nspark.sql(\"Select publish_Year,publish_Week,sum(ones) cantidad,round(mean(ones),2) media,\"\\\n          \"round(stddev(ones),2) std,min(ones) min,round(percentile(ones,0.25),2) Q1_25,\"\\\n          \"round(percentile(ones,0.5),2) Q2_50,round(percentile(ones,0.75),2) Q3_75,max(ones) max,\"\\\n          \"sum(ones) suma from (Select publish_time_new,publish_Year,publish_Week,count(ones) \"\\\n          \"ones from consultas group by publish_time_new,publish_Year,publish_Week) \"\\\n          \"group by publish_Year,publish_Week order by publish_Year,int(publish_Week)\").show()\n#Guardamos el reporte en CSV\nspark.sql(\"Select publish_Year,publish_Week,sum(ones) cantidad,round(mean(ones),2) media,\"\\\n          \"round(stddev(ones),2) std,min(ones) min,round(percentile(ones,0.25),2) Q1_25,\"\\\n          \"round(percentile(ones,0.5),2) Q2_50,round(percentile(ones,0.75),2) Q3_75,max(ones) max,\"\\\n          \"sum(ones) suma from (Select publish_time_new,publish_Year,publish_Week,count(ones) \"\\\n          \"ones from consultas group by publish_time_new,publish_Year,publish_Week) \"\\\n          \"group by publish_Year,publish_Week order by publish_Year,int(publish_Week)\").toPandas().to_csv('reporte_semana_publicacion.csv',sep=\"|\",index=None)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:45:24.55654Z","iopub.execute_input":"2021-08-29T00:45:24.556832Z","iopub.status.idle":"2021-08-29T00:45:33.418108Z","shell.execute_reply.started":"2021-08-29T00:45:24.556805Z","shell.execute_reply":"2021-08-29T00:45:33.417435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reporte estadístico agrupado por mes de publicación\nspark.sql(\"Select publish_Year,publish_Month,sum(ones) cantidad,round(mean(ones),2) media,\"\\\n          \"round(stddev(ones),2) std,min(ones) min,round(percentile(ones,0.25),2) Q1_25,\"\\\n          \"round(percentile(ones,0.5),2) Q2_50,round(percentile(ones,0.75),2) Q3_75,max(ones) max,\"\\\n          \"sum(ones) suma from (Select publish_time_new,publish_Year,publish_Month,count(ones) \"\\\n          \"ones from consultas group by publish_time_new,publish_Year,publish_Month) \"\\\n          \"group by publish_Year,publish_Month order by publish_Year,int(publish_Month)\").show()\n#Guardamos el reporte en CSV\nspark.sql(\"Select publish_Year,publish_Month,sum(ones) cantidad,round(mean(ones),2) media,\"\\\n          \"round(stddev(ones),2) std,min(ones) min,round(percentile(ones,0.25),2) Q1_25,\"\\\n          \"round(percentile(ones,0.5),2) Q2_50,round(percentile(ones,0.75),2) Q3_75,max(ones) max,\"\\\n          \"sum(ones) suma from (Select publish_time_new,publish_Year,publish_Month,count(ones) \"\\\n          \"ones from consultas group by publish_time_new,publish_Year,publish_Month) \"\\\n          \"group by publish_Year,publish_Month order by publish_Year,int(publish_Month)\").toPandas().to_csv('reporte_mes_publicacion.csv',sep=\"|\",index=None)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:45:38.443901Z","iopub.execute_input":"2021-08-29T00:45:38.44424Z","iopub.status.idle":"2021-08-29T00:45:44.516423Z","shell.execute_reply.started":"2021-08-29T00:45:38.44421Z","shell.execute_reply":"2021-08-29T00:45:44.515336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reporte estadístico agrupado por año de publicación\nspark.sql(\"Select publish_Year,sum(ones) cantidad,round(mean(ones),2) media,\"\\\n          \"round(stddev(ones),2) std,min(ones) min,round(percentile(ones,0.25),2) Q1_25,\"\\\n          \"round(percentile(ones,0.5),2) Q2_50,round(percentile(ones,0.75),2) Q3_75,max(ones) max,\"\\\n          \"sum(ones) suma from (Select publish_time_new,publish_Year,count(ones) \"\\\n          \"ones from consultas group by publish_time_new,publish_Year) \"\\\n          \"group by publish_Year order by publish_Year,int(publish_Year)\").show()\n#Guardamos el reporte en CSV\nspark.sql(\"Select publish_Year,sum(ones) cantidad,round(mean(ones),2) media,\"\\\n          \"round(stddev(ones),2) std,min(ones) min,round(percentile(ones,0.25),2) Q1_25,\"\\\n          \"round(percentile(ones,0.5),2) Q2_50,round(percentile(ones,0.75),2) Q3_75,max(ones) max,\"\\\n          \"sum(ones) suma from (Select publish_time_new,publish_Year,count(ones) \"\\\n          \"ones from consultas group by publish_time_new,publish_Year) \"\\\n          \"group by publish_Year order by publish_Year,int(publish_Year)\").toPandas().to_csv('reporte_anio_publicacion.csv',sep=\"|\",index=None)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:45:50.538428Z","iopub.execute_input":"2021-08-29T00:45:50.538842Z","iopub.status.idle":"2021-08-29T00:45:56.38983Z","shell.execute_reply.started":"2021-08-29T00:45:50.538809Z","shell.execute_reply":"2021-08-29T00:45:56.388946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Identificamos la cantidad de cluster\ncantidad_cluster=spark.sql(\"Select max(int(cluster)) cantidad from consultas\").toPandas()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:45:59.670784Z","iopub.execute_input":"2021-08-29T00:45:59.671079Z","iopub.status.idle":"2021-08-29T00:46:00.333247Z","shell.execute_reply.started":"2021-08-29T00:45:59.671055Z","shell.execute_reply":"2021-08-29T00:46:00.332498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reporte estadístico agrupado por mes y clasificado por cluster\nfor i in range(cantidad_cluster['cantidad'][0]+1):\n    print(\"Cluster: \"+str(i)+\"\\n\")\n    spark.sql(\"Select publish_Year,publish_Month,sum(ones) cantidad,round(mean(ones),2) media,\"\\\n          \"round(stddev(ones),2) std,min(ones) min,round(percentile(ones,0.25),2) Q1_25,\"\\\n          \"round(percentile(ones,0.5),2) Q2_50,round(percentile(ones,0.75),2) Q3_75,max(ones) max,\"\\\n          \"sum(ones) suma from (Select publish_time_new,publish_Year,publish_Month,count(ones) \"\\\n          \"ones from consultas where cluster=\"+str(i)+\" group by publish_time_new,publish_Year,publish_Month) \"\\\n          \"group by publish_Year,publish_Month order by publish_Year,int(publish_Month)\").show()\n#Guardamos el reporte en CSV\nspark.sql(\"Select cluster,publish_Year,publish_Month,sum(ones) cantidad,round(mean(ones),2) media,\"\\\n          \"round(stddev(ones),2) std,min(ones) min,round(percentile(ones,0.25),2) Q1_25,\"\\\n          \"round(percentile(ones,0.5),2) Q2_50,round(percentile(ones,0.75),2) Q3_75,max(ones) max,\"\\\n          \"sum(ones) suma from (Select publish_time_new,cluster,publish_Year,publish_Month,count(ones) \"\\\n          \"ones from consultas group by publish_time_new,cluster,publish_Year,publish_Month) \"\\\n          \"group by cluster,publish_Year,publish_Month order by cluster,publish_Year,int(publish_Month)\").toPandas().to_csv('reporte_cluster_publicacion.csv',sep=\"|\",index=None)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:46:03.22762Z","iopub.execute_input":"2021-08-29T00:46:03.227918Z","iopub.status.idle":"2021-08-29T00:46:47.42653Z","shell.execute_reply.started":"2021-08-29T00:46:03.22789Z","shell.execute_reply":"2021-08-29T00:46:47.424945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autor=spark.sql(\"Select authors from consultas where authors is not null\").toPandas()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:46:47.428175Z","iopub.execute_input":"2021-08-29T00:46:47.428489Z","iopub.status.idle":"2021-08-29T00:46:48.015796Z","shell.execute_reply.started":"2021-08-29T00:46:47.428461Z","shell.execute_reply":"2021-08-29T00:46:48.01465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#identificamos los autores de los artículos, pueden ser varios autores para un solo artículo\nlista_autor=[]\nfor a in autor['authors'].tolist():\n    for i in a.split(sep=';'):\n        lista_autor.append(i.strip())\ndf_autor=pd.DataFrame(lista_autor,columns=[\"autor\"])\nsp_autor=spark.createDataFrame(df_autor) \nsp_autor.createOrReplaceTempView(\"consulta_autor\")","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:46:50.33066Z","iopub.execute_input":"2021-08-29T00:46:50.330944Z","iopub.status.idle":"2021-08-29T00:46:53.355335Z","shell.execute_reply.started":"2021-08-29T00:46:50.33092Z","shell.execute_reply":"2021-08-29T00:46:53.354076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reportes por journal y autor, top 10\nspark.sql(\"Select count(distinct journal) total_journal from consultas\").show()\nspark.sql(\"Select journal,count(*) cantidad from consultas where journal is not null group by journal order by count(*) desc limit 10\").show()\nspark.sql(\"Select count(distinct autor) total_autores from consulta_autor\").show()\nspark.sql(\"Select autor,count(*) cantidad from consulta_autor group by autor order by count(*) desc limit 10\").show()\n#Guardamos el reporte en CSV\nspark.sql(\"Select journal,count(*) cantidad from consultas where journal is not null group by journal order by count(*) desc\").toPandas().to_csv('reporte_journal.csv',sep=\"|\",index=None)\nspark.sql(\"Select autor,count(*) cantidad from consulta_autor group by autor order by count(*) desc\").toPandas().to_csv('reporte_autor.csv',sep=\"|\",index=None)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:46:56.841431Z","iopub.execute_input":"2021-08-29T00:46:56.841775Z","iopub.status.idle":"2021-08-29T00:47:04.751386Z","shell.execute_reply.started":"2021-08-29T00:46:56.841746Z","shell.execute_reply":"2021-08-29T00:47:04.749867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reporte de autores clasificado por cluster, top 10 y lo grabamos en un archivo csv\nfor i in range(cantidad_cluster['cantidad'][0]+1):\n    print(\"Cluster: \"+str(i)+\"\\n\")\n    autor=spark.sql(\"Select authors from consultas where cluster=\"+str(i)+\" and authors is not null\").toPandas()\n    lista_autor=[]\n    for a in autor['authors'].tolist():\n        for j in a.split(sep=';'):\n            lista_autor.append(j.strip())\n    df_autor=pd.DataFrame(lista_autor,columns=[\"autor\"])    \n    sp_autor=spark.createDataFrame(df_autor) \n    sp_autor.createOrReplaceTempView(\"consulta_autor\")\n    spark.sql(\"Select journal,count(*) cantidad from consultas where cluster=\"+str(i)+\" and journal is not null group by journal order by count(*) desc limit 10\").show()\n    spark.sql(\"Select autor,count(*) cantidad from consulta_autor group by autor order by count(*) desc limit 10\").show()   \n    spark.sql(\"Select autor,count(*) cantidad from consulta_autor group by autor order by count(*) desc limit 10\").toPandas().to_csv('reporte_autor_cluster_'+str(i)+'.csv',sep=\"|\",index=None)\nspark.sql(\"Select cluster,journal,count(*) cantidad from consultas where journal is not null group by cluster,journal order by count(*) desc\").toPandas().to_csv('reporte_journal_por_cluster.csv',sep=\"|\",index=None)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:47:09.751718Z","iopub.execute_input":"2021-08-29T00:47:09.751992Z","iopub.status.idle":"2021-08-29T00:47:51.822319Z","shell.execute_reply.started":"2021-08-29T00:47:09.751968Z","shell.execute_reply":"2021-08-29T00:47:51.820874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reporte por país de autor de artículo\nspark.sql(\"Select country,count(*) cantidad from consultas where country <>'No se encontro' \"\\\n          \"and country <>'' group by country order by count(*) desc limit 10\").show()\n#Guardamos el reporte en CSV\nspark.sql(\"Select country,count(*) cantidad from consultas \"\\\n          \"group by country order by count(*) desc\").toPandas().to_csv('reporte_paises.csv',sep=\"|\",index=None)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:48:00.238354Z","iopub.execute_input":"2021-08-29T00:48:00.238682Z","iopub.status.idle":"2021-08-29T00:48:02.343202Z","shell.execute_reply.started":"2021-08-29T00:48:00.238653Z","shell.execute_reply":"2021-08-29T00:48:02.341903Z"},"trusted":true},"execution_count":null,"outputs":[]}]}