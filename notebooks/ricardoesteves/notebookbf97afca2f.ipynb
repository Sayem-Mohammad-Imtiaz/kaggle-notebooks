{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# IBM HR Analytics Employee Attrition","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This project was developed using a fictional dataset created by IBM data scientists and aims to investigate factors that lead to employee attrition, as well as to develop a Machine Learning model capable of predicting whether employees tend to leave the company or not. \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Exploring the data\n\nLet's start by importing the necessary libraries, loading the data, and checking the data set.\n\nThe second column of this data set, 'Attrition', will be our target variable, that is, the one we want to predict (whether an employee will leave the company or not). All other columns are characteristics of each employee in the company's database.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading the dataset\nemployee_df = pd.read_csv(\"/kaggle/input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing the first lines\n\nemployee_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"employee_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The dataset has 35 features (columns), 26 of which are numeric and 9 are categorical, in addition to 1470 rows.\n* The dataset does not have null values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyzing statistical information about numerical variables\nemployee_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transforming some categorical variables with YES / NO content to numeric 0/1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"employee_df['Attrition'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"employee_df['OverTime'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"employee_df['Over18'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"employee_df['Attrition'] = employee_df['Attrition'].apply(lambda x: 1 if x == 'Yes' else 0)\nemployee_df['OverTime'] = employee_df['OverTime'].apply(lambda x: 1 if x == 'Yes' else 0)\nemployee_df['Over18'] = employee_df['Over18'].apply(lambda x: 1 if x == 'Y' else 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"employee_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting a histogram to visualize how each feature is distributed into dataset\n\nemployee_df.hist(bins = 30, figsize = (20,20), color = 'b');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Most of the employees are around between 27 and 40 years old\n* Most of the employees live close to work\n* Most of the employees have Education level 3\n* Most of the employees have less than 10 years working in the company\n* Several features such as 'MonthlyIncome' and 'TotalWorkingYears' are tail heavy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# It makes sense to drop 'EmployeeCount' , 'Standardhours' and 'Over18' since they do not change from one employee to the other\n# Let's drop 'EmployeeNumber' as well\nemployee_df.drop(['EmployeeCount', 'StandardHours', 'Over18', 'EmployeeNumber'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"employee_df.head()\n# Now we have 31 columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see how many employees left the company! \nleft_df = employee_df[employee_df['Attrition'] == 1]\nstayed_df = employee_df[employee_df['Attrition'] == 0]\n\n# Count the number of employees who stayed and left\n# It seems that we are dealing with an imbalanced dataset \n\nprint(\"Total =\", len(employee_df))\n\nprint(\"Number of employees who left the company:\", len(left_df))\nprint(f\"Percentage of employees who left the company: {1.*len(left_df)/len(employee_df)*100.0:.2f}%\") \nprint(\"Number of employees who did not leave the company (stayed) =\", len(stayed_df))\nprint(f\"Percentage of employees who did not leave the company (stayed): {1.*len(stayed_df)/len(employee_df)*100.0:.2f}%\") \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets have a look in the statistics of the employees who stayed and left to make some comparisions\n\nleft_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stayed_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nAfter comparing the mean and std of the employees who stayed and left we can conclude: \n* Age: mean age of the employees who stayed is higher compared to who left (37.5 x 33.6)\n* DailyRate: Rate of employees who stayed is higher (812 x 750)\n* DistanceFromHome: Employees who stayed live closer to home (8.9km x 10.6km)\n* EnvironmentSatisfaction and JobSatisfaction: Employees who stayed are generally more satisifed with their jobs\n* StockOptionLevel: Employees who stayed tend to have higher stock option level\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets have a look in the different correlations between the features\n\ncorrelations = employee_df.corr()\nf, ax = plt.subplots(figsize = (20, 20))\nsns.heatmap(correlations, annot = True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Verifying the correlation between variables is extremely important to achieve a broader view of the data and how they relate to each other.\n\nThe lighter the color the more positive it correlates\n\n* Job level is strongly correlated with total working years\n* Monthly income is strongly correlated with Job level\n* Monthly income is strongly correlated with total working years\n\n* Age is stongly correlated with monthly income","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets investigate if there is any correlation between people who left the company with some specific variables such as 'Age', 'JobRole', 'MaritalStatus', 'JobInvolvement' and 'JobLevel'\n\nplt.figure(figsize=[25, 12])\nsns.countplot(x = 'Age', hue = 'Attrition', data = employee_df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Blue is represented by employees who stayed, orange by those who left the company.\n* Up to 31 years of age, the largest number of employees who left the company is concentrated compared to those who stayed; Between 18 to 21 years of age are concentrated the largest number of employees that leave proportionally the amount that remains.\n* After the 31's, as age increases, there is a decrease in the number of employees who left the company;","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[20,20])\n\nplt.subplot(411)\n\nsns.countplot(x = 'JobRole', hue = 'Attrition', data = employee_df)\nplt.title(\"In which position the Attrition is higher / lower?\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Almost half of the team who work in Sales Representative left the company. However a very small number of Reseach Director left.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the Monthly Income vs. Job Role\n\nplt.figure(figsize=(10, 10))\nsns.boxplot(x = 'MonthlyIncome', y = 'JobRole', data = employee_df);\nplt.title(\"How is the distribution of wages among the different positions?\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Sales Representative, Laboratory Technician and Research Scientist are the least paid, while Research Director and Manager are best paid.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = 'MaritalStatus', hue = 'Attrition', data = employee_df);\nplt.title(\"Marital Status Vs Attrition\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Single employees tend to leave compared to married and divorced","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = 'JobInvolvement', hue = 'Attrition', data = employee_df);\nplt.title(\"How does the level of involvement at work affect the Attrition?\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The less employees are involved, the more they tend to leave the company","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = 'JobLevel', hue = 'Attrition', data = employee_df)\nplt.title(\"Job level Vs Attrition\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Less experienced (low job level) tend to leave the company ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's use KDE (Kernel Density Estimate) to visualize the probability density of a continuous variable.\n\n# Investigating DistanceFromHome\n\nplt.figure(figsize=(12,7))\nsns.kdeplot(left_df['DistanceFromHome'], label = 'Employees who left', shade = True, color = 'r')\nsns.kdeplot(stayed_df['DistanceFromHome'], label = 'Employees who Stayed', shade = True, color = 'b')\nplt.xlabel('Distance From Home');\nplt.ylabel('Attrition');\nplt.title(\"Does the distance from home to work impact Attrition?\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As the distance from home increases, the number of employees who tends to leave is higher.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Investigating YearsWithCurrManager\n\nplt.figure(figsize=(12,7))\nsns.kdeplot(left_df['YearsWithCurrManager'], label = 'Employees who left', shade = True, color = 'r')\nsns.kdeplot(stayed_df['YearsWithCurrManager'], label = 'Employees who Stayed', shade = True, color = 'b')\nplt.xlabel('Years With Current Manager');\nplt.title(\"Does the length of stay as a Current Manager influence the departure of employees?\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The shorter the time as a Current Manager, the greater the tendency for employees to leave.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Investigating TotalWorkingYears\n\nplt.figure(figsize=(12,7))\nsns.kdeplot(left_df['TotalWorkingYears'], shade = True, label = 'Employees who left', color = 'r')\nsns.kdeplot(stayed_df['TotalWorkingYears'], shade = True, label = 'Employees who Stayed', color = 'b')\nplt.xlabel('Total Working Years');\nplt.ylabel('Attrition');\nplt.title(\"Is there a relationship between total working time in the company and Attrition?\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The critical period that employees most tend to leave is up to about 7 years working at the company. From there they tend to stay.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2. Performing data cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this process, the main objective is to ensure that the data is correct, consistent and usable, identifying any errors or corruptions in the data, correcting or deleting them, or manually processing them as needed to prevent the error from happening again.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the types of each feature\nemployee_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separating categorical data from the rest of the dataframe to then convert it to numeric\nX_cat = employee_df[['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus']]\nX_cat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are several different ways to convert categorical to numeric values. In this project we will use the One Hot Encoder from the Scikit Learn library.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the categorical features into numbers using OneHotEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nonehotencoder = OneHotEncoder()\nX_cat = onehotencoder.fit_transform(X_cat).toarray()\nX_cat.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting into dataframe\nX_cat = pd.DataFrame(X_cat)\nX_cat ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separating the numerical data\nX_numerical = employee_df[['Age', 'DailyRate', 'DistanceFromHome','Education', 'EnvironmentSatisfaction', 'HourlyRate', 'JobInvolvement','JobLevel','JobSatisfaction','MonthlyIncome','MonthlyRate','NumCompaniesWorked',\t'OverTime',\t'PercentSalaryHike', 'PerformanceRating','RelationshipSatisfaction','StockOptionLevel','TotalWorkingYears'\t,'TrainingTimesLastYear', 'WorkLifeBalance','YearsAtCompany','YearsInCurrentRole', 'YearsSinceLastPromotion','YearsWithCurrManager']]\nX_numerical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concatenating the categorical dataset X_cat and the numerical dataset X_numerical into a unique dataset\n\nX_all = pd.concat([X_cat, X_numerical], axis = 1)\nX_all","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets use sklearn's MinMaxScaler to transform the data by scaling each resource to an interval between 0 and 1 to ensure that our machine learning model handles the features equally","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X_all)\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separating the feature that we want to predict\n\ny = employee_df['Attrition']\ny","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Creating Testing and Training datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Building, training and evaluating different Machine Learning models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Logistic Regression Classifier","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression is a Machine Learning algorithm which is used for the classification problems, it is a predictive analysis algorithm and based on the concept of probability.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nmodel = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the data\n\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions and visualizing the accuracy\n\nLRC_pred = model.predict(X_test)\n\n\nprint(\"Accuracy: {}%\".format( 100 * accuracy_score(LRC_pred, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comparing the results using Confusion Matrix\n\nfrom sklearn.metrics import confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing Set Performance\n\ncm = confusion_matrix(LRC_pred, y_test)\nsns.heatmap(cm, annot=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The model was able to correctly classify around 3,000 registers and erroneously classify a very small number of employees","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyzing the KPI (Key Performance Indicator)\n\nprint(classification_report(y_test, LRC_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Random Forest Classifier\n\nIt is also widely used in classification problems and like its name implies, consists of a large number of individual decision trees that operate as an ensemble.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the data\n\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions and visualizing the accuracy\n\nRFC_pred = model.predict(X_test)\nprint(\"Accuracy: {}%\".format( 100 * accuracy_score(RFC_pred, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing Set Performance\n\ncm = confusion_matrix(RFC_pred, y_test)\nsns.heatmap(cm, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyzing the KPI (Key Performance Indicator)\n\nprint(classification_report(y_test, RFC_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3 K-Nearest Neighbors Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = KNeighborsClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"KNNC_pred = model.predict(X_test)\nprint(\"Accuracy: {}%\".format( 100 * accuracy_score(KNNC_pred, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing Set Performance\n\ncm = confusion_matrix(KNNC_pred, y_test)\nsns.heatmap(cm, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyzing the KPI (Key Performance Indicator)\n\nprint(classification_report(y_test, KNNC_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.4 Artificial Neural Network Classifier","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In summary, a Neural Network consists of units (neurons), arranged in layers, which convert an input vector into some output","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the layers\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(units=500, activation='relu', input_shape=(50, )))\nmodel.add(tf.keras.layers.Dense(units=500, activation='relu'))\nmodel.add(tf.keras.layers.Dense(units=500, activation='relu'))\nmodel.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='Adam', loss='binary_crossentropy', metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the model\n\nepochs_hist = model.fit(X_train, y_train, epochs = 100, batch_size = 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ANNC_pred = model.predict(X_test)\nANNC_pred = (ANNC_pred > 0.5)\nprint(\"Accuracy: {}%\".format( 100 * accuracy_score(ANNC_pred, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs_hist.history.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(epochs_hist.history['loss'])\nplt.title('Model Loss Progress During Training')\nplt.xlabel('Epoch')\nplt.ylabel('Training Loss')\nplt.legend(['Training Loss']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(epochs_hist.history['accuracy'])\nplt.title('Model Accuracy Progress During Training')\nplt.xlabel('Epoch')\nplt.ylabel('Training Accuracy')\nplt.legend(['Training Accuracy']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing Set Performance\ncm = confusion_matrix(y_test, ANNC_pred)\nsns.heatmap(cm, annot=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, ANNC_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Model evaluation\n\nAfter testing the four models we came to the conclusion that the best model is the Logistic Regression Classifier with an accuracy of 91.85%","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing the results\n\nprint(\"Logistic Regression Classifier: {:.2f}% Accuracy\".format( 100 * accuracy_score(LRC_pred, y_test)))\nprint(\"Random Forest Classifier: {:.2f}% Accuracy\".format( 100 * accuracy_score(RFC_pred, y_test)))\nprint(\"K-Nearest Neighbors Classifier: {:.2f}% Accuracy\".format( 100 * accuracy_score(KNNC_pred, y_test)))\nprint(\"Artificial Neural Network Classifier: {:.2f}% Accuracy\".format( 100 * accuracy_score(ANNC_pred, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}