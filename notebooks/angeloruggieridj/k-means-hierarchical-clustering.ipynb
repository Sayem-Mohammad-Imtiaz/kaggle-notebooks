{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Esercitazione su k-Means e Hierarchical Clustering"},{"metadata":{},"cell_type":"markdown","source":"## Indice contenuti\n- [Obiettivo esercitazione](#Obiettivo-esercitazione)\n- [Descrizione ed analisi del dataset](#Descrizione-ed-analisi-del-dataset)\n- [Analisi esplorativa del dataset](#Analisi-esplorativa-del-dataset)\n    - [Caricamento in memoria del dataset](#Caricamento-in-memoria-del-dataset)\n    - [Pulizia del dataset](#Pulizia-del-dataset)\n- [Visualizzazione dei dati](#Visualizzazione-dei-dati)\n    - [Istogramma](#Istogramma)\n    - [CountPlot per Genere](#CountPlot-per-Genere)\n    - [Studio delle features disponibili](#Studio-delle-features-disponibili)\n- [Clustering via k-Means](#Clustering-via-k-Means)\n    - [k-Means++](#kMeans++)\n    - [Segmentazione dei clienti Age vs Spending Score](#Segmentazione-dei-clienti-Age-vs-Spending-Score)\n    - [Trovare il numero ottimale di clusters](#Trovare-il-numero-ottimale-di-clusters)\n    - [Plot delle regioni identificate](#Plot-delle-regioni-identificate)\n    - [Segmentazione dei clienti Annual Income vs Spending Score](#Segmentazione-dei-clienti-Annual-Income-vs-Spending-Score)\n    - [Plot n2 delle regioni identificate](#Plot-n2-delle-regioni-identificate)\n    - [Segmentazione dei clienti Age vs Annual Income vs Spending Score](#Segmentazione-dei-clienti-Age-vs-Annual-Income-vs-Spending-Score)\n        - [Analisi di Silhouette](#Analisi-di-Silhouette)\n    - [BoxPlot ottenuti con k-Means](BoxPlot-ottenuti-con-k-Means)\n- [Clustering gerarchico](#Clustering-gerarchico)\n    - [Agglomerative clustering](#Agglomerative-Clustering)\n- [DBSCAN](#DBSCAN)\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"## Obiettivo esercitazione\nL'esercitazione ha l'obiettivo di applicare su un dataset reale i differenti algoritmi di clustering, in particolare k-Means e Hierarchical Clustering.\n\nSi effettueranno, inoltre, differenti variazioni all'applicazione standard degli algoritmi per comprendere l'utilizzo dei differenti iper-parametri a seconda delle documentazioni ufficiali dei metodi utilizzati."},{"metadata":{},"cell_type":"markdown","source":"## Descrizione ed analisi del dataset\nIl dataset che verrà utilizzato tratta informazioni circa la fedeltà dei clienti che hanno sottoscritto una tessera punti. \nTali informazioni raccolte potrebbero ritornare utili, ai fini di business, per pianificare potenziali strategie economiche ed attuare delle promozioni mirate in base alla propria clientela.\n\nLe features presenti nel dataset sono le seguenti:\n- <b>CustomerID</b>: Codice univoco identificativo del cliente.\n- <b>Gender</b>: Sesso dell'utente.\n- <b>Age</b>: Età dell'utente.\n- <b>Annual Income (k$)</b>: Spesa annuale dell'utente fatta presso il centro commerciale di riferimento\n- <b>Spending Score (1-100)</b>: Punteggio assegnato dal centro commerciale in base al comportamento del cliente e alla natura della spesa"},{"metadata":{},"cell_type":"markdown","source":"## Analisi esplorativa del dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import delle l'analisi esplorativa dei dati\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport datetime as dt\nimport matplotlib.pyplot as plt\n\n# import delle librerie richieste per l'applicazione di algoritmi di clustering\nimport sklearn\nfrom sklearn.cluster import KMeans\nfrom scipy.cluster.hierarchy import linkage\nfrom sklearn.metrics import silhouette_score\nfrom scipy.cluster.hierarchy import cut_tree\nfrom scipy.cluster.hierarchy import dendrogram\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Caricamento in memoria del dataset"},{"metadata":{},"cell_type":"markdown","source":"Con il seguente comando si effettua il caricamento in memoria di quanto contenuto nel dataset _'OnlineRetail.csv'_.\n\nPer condurre una prima fase di analisi esplorativa e comprendere la natura dei dati a disposizione, si stampano di seguito i primi cinque esempi presenti nel dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"store = pd.read_csv('../input/customer-segmentation-tutorial-in-python/Mall_Customers.csv')\nstore.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Per ottenere informazioni statistiche inerenti ciascuna feature a disposizione, mediante il metodo _describe()_ si è provveduto al calcolo delle seguenti informazioni:\n- <b>count</b>: conteggio del numero di esempi per la feature selezionata\n- <b>mean</b>: media aritmetica per la feature selezionata\n- <b>std</b>: deviazione standard per la feature selezionata\n- <b>min</b>: valore minimo presentato dagli esempi per la feature selezionata\n- <b>25%</b>: primo quartile calcolato sugli esempi per la feature selezionata\n- <b>50%</b>: secondo quartile calcolato sugli esempi per la feature selezionata\n- <b>75%</b>: terzo quartile calcolato sugli esempi per la feature selezionata\n- <b>max</b>: valore massimo presentato dagli esempi per la feature selezionata"},{"metadata":{"trusted":true},"cell_type":"code","source":"store.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Successivamente, al fine di comprendere le dimensioni (in termini di esempi e di features a disposizione), mediante apposito attributo si stampano il numero di righe e di colonne del DataFrame:"},{"metadata":{"trusted":true},"cell_type":"code","source":"store.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Al fine di ottenere una descrizione complessiva del Dataframe (e dunque del relativo dataset) caricato, mediante il metodo _info()_ si sono ottenute le seguenti informazioni:\n- <b>#</b>: numero di feature presente nel DataFrame\n- <b>Column</b>: intestazione delle features nel DataFrame\n- <b>Non-Null Count</b>: contatore di valori non nulli per ogni feature presente nel DataFrame\n- <b>Dtype</b>: tipo di dato memorizzato per ogni feature presente nel DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"store.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"store.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizzazione dei dati"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Set dello stile dei grafici\nplt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Istogramma"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1 , figsize = (10 , 5))\nn = 0 \nfor x in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    n += 1\n    plt.subplot(1 , 3 , n)\n    plt.subplots_adjust(hspace =0.5 , wspace = 0.5)\n    sns.distplot(store[x] , bins = 20)\n    plt.title('Distplot of {}'.format(x))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CountPlot per Genere"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1 , figsize = (10 , 5))\nsns.countplot(y = 'Gender' , data = store)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Studio delle features disponibili"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1 , figsize = (10 , 5))\nn = 0 \nfor x in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    for y in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n        n += 1\n        plt.subplot(3 , 3 , n)\n        plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n        sns.regplot(x = x , y = y , data = store)\n        plt.ylabel(y.split()[0]+' '+y.split()[1] if len(y.split()) > 1 else y )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1 , figsize = (10 , 5))\nfor gender in ['Male' , 'Female']:\n    plt.scatter(x = 'Age' , y = 'Annual Income (k$)' , data = store[store['Gender'] == gender] ,\n                s = 200 , alpha = 0.5 , label = gender)\nplt.xlabel('Age'), plt.ylabel('Annual Income (k$)') \nplt.title('Age vs Annual Income suddiviso per Gender')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1 , figsize = (10 , 5))\nfor gender in ['Male' , 'Female']:\n    plt.scatter(x = 'Annual Income (k$)',y = 'Spending Score (1-100)' ,\n                data = store[store['Gender'] == gender] ,s = 200 , alpha = 0.5 , label = gender)\nplt.xlabel('Annual Income (k$)'), plt.ylabel('Spending Score (1-100)') \nplt.title('Annual Income vs Spending Score suddiviso per Gender')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1 , figsize = (15 , 7))\nn = 0 \nfor cols in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    n += 1 \n    plt.subplot(1 , 3 , n)\n    plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n    sns.violinplot(x = cols , y = 'Gender' , data = store , palette = 'vlag')\n    sns.swarmplot(x = cols , y = 'Gender' , data = store)\n    plt.ylabel('Gender' if n == 1 else '')\n    plt.title('Boxplots & Swarmplots' if n == 2 else '')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clustering via k-Means\n\nk-Means è un algoritmo di clustering non supervisionato, tra i più semplici e popolari messi a disposizione dalla libreria _Sci-Kit_.\n\nUn cluster è definito come un insieme di punti dati che il clustering è uno degli algoritmi di apprendimento automatico non supervisionati più semplici e popolari.\n\nDefinito il valore del parametro k, che esplica il numero di centroidi da identificare nel dataset. Un centroide è la posizione immaginaria o reale che rappresenta il centro di ciascun cluster.\n\nL'algoritmo prevede l'assegnazione di ogni punto dati viene a ciascuno dei cluster utilizzando la nozione di distanza. In altre parole, l'algoritmo k-Means identifica il numero k di centroidi e quindi assegna ogni punto dati al cluster più vicino, mantenendo i centroidi i più piccoli possibili.\n\nPer clusterizzare i dati presenti nel dataset, l'algoritmo k-Means identifica randomicamente un primo gruppo di centroidi e tali sono utilizzati come punti iniziali per ogni cluster. Successivamente, si effettua il ricalcolo dei centroidi ogni qualvolta un nuovo esempio è assegnato al cluster, al fine di ottimizzare le posizioni dei centroidi.\nIl processo di ottimizzazione termina quando si raggiunge il numero delle iterazioni massime (definite) oppure quando si è giunti alla convergenza del metodo."},{"metadata":{},"cell_type":"markdown","source":"### kMeans++\nNel k-means classico, si utilizza un seme casuale per posizionare i centroidi iniziali, che a volte può provocare cattivi raggruppamenti o una lenta convergenza se i centroidi iniziali sono scelti male. Un modo per risolvere questo problema è eseguire l'algoritmo k-mean più volte su un set di dati e scegliere il modello con le migliori prestazioni in termini di SSE.\n\nUn'altra strategia è quella di posizionare i centroidi iniziali molto distanti tra loro tramite l'algoritmo k-means ++, che porta a risultati migliori e più coerenti rispetto ai classici k-mean \n\nPer utilizzare il k-Means++ basterà porre l'attributo init = 'k-means++' (che è già posto di default). Per utilizzare il k-Means classico bisognerà porre l'attributo init = 'random'."},{"metadata":{},"cell_type":"markdown","source":"## Segmentazione dei clienti Age vs Spending Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Metodo k-Means con un numero di clusters arbitrario.\n# - n_clusters: numero di cluster desiderati - limitazione del k-Means;\n# - n_init: esegue l'algoritmo n volte in modo indipendente, con diversi centroidi casuali per scegliere il modello finale come quello con il SSE più basso.\n# - max_iter: indica il numero massimo di iterazioni per ogni singola esecuzione. \n\nX1 = store[['Age' , 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    method = (KMeans(n_clusters = n, init='k-means++', n_init = 10, max_iter=300, tol=0.0001, random_state= 1))\n    method.fit(X1)\n    inertia.append(method.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stampa delle etichette relative ai cluster\nmethod.labels_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Trovare il numero ottimale di clusters"},{"metadata":{},"cell_type":"markdown","source":"#### Metodo Elbow\nAl fine di identificare il giusto numero per il parametro _n_clusters_ è possibile definire un metodo grafico che consenta, variando il parametro mediante una lista di valori espressi, di poter valutare l'attributo _intertia_ (ovvero la somma della radice delle distanze dei campioni dal centro del cluster più vicino)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot del valore della somma della radice delle distanze al crescere del numero dei cluster\n\nplt.figure(1 , figsize = (10 ,5))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Numero dei cluster') , plt.ylabel('Sum of Squared Distance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ottenuti i risultati visibili dal precedente grafico, si sceglie il valore 4 per il parametro _n_clusters_ e si riesegue nuovamente l'algoritmo k-Means registrando le relative etichette."},{"metadata":{"trusted":true},"cell_type":"code","source":"method = (KMeans(n_clusters = 4, init='k-means++', n_init = 10, max_iter=300, tol=0.0001, random_state= 1))\nmethod.fit(X1)\nlabels1 = method.labels_\ncentroids1 = method.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stampa delle etichette predette\nmethod.labels_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot delle regioni identificate"},{"metadata":{"trusted":true},"cell_type":"code","source":"h = 0.02\nx_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\ny_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = method.predict(np.c_[xx.ravel(), yy.ravel()]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1 , figsize = (10 , 5) )\nplt.clf()\nZ = Z.reshape(xx.shape)\nplt.imshow(Z, interpolation='nearest', extent=(xx.min(), xx.max(), yy.min(), yy.max()), cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = store, c = labels1, s = 50 )\nplt.scatter(x = centroids1[: , 0], y =  centroids1[: , 1], s = 50, c = 'red', alpha = 0.5)\nplt.ylabel('Spending Score (1-100)'), plt.xlabel('Age')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Segmentazione dei clienti Annual Income vs Spending Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"X2 = store[['Annual Income (k$)', 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    method = (KMeans(n_clusters = n, init='k-means++', n_init = 10, max_iter=300, tol=0.0001, random_state= 1))\n    method.fit(X2)\n    inertia.append(method.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot del valore della somma della radice delle distanze al crescere del numero dei cluster\n\nplt.figure(1 , figsize = (10 ,5))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Numero dei cluster') , plt.ylabel('Sum of Squared Distance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ottenuti i risultati visibili dal precedente grafico, si sceglie il valore 5 per il parametro _n_clusters_ e si riesegue nuovamente l'algoritmo k-Means registrando le relative etichette."},{"metadata":{"trusted":true},"cell_type":"code","source":"method = (KMeans(n_clusters = 5, init='k-means++', n_init = 10, max_iter=300, tol=0.0001, random_state= 1))\nmethod.fit(X2)\nlabels2 = method.labels_\ncentroids2 = method.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stampa delle etichette predette\nmethod.labels_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot n2 delle regioni identificate"},{"metadata":{"trusted":true},"cell_type":"code","source":"h = 0.02\nx_min, x_max = X2[:, 0].min() - 1, X2[:, 0].max() + 1\ny_min, y_max = X2[:, 1].min() - 1, X2[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ2 = method.predict(np.c_[xx.ravel(), yy.ravel()]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1, figsize = (10 , 6))\nplt.clf()\nZ2 = Z2.reshape(xx.shape)\nplt.imshow(Z2 , interpolation='nearest', extent=(xx.min(), xx.max(), yy.min(), yy.max()), cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter(x = 'Annual Income (k$)', y = 'Spending Score (1-100)', data = store , c = labels2, s = 50)\nplt.scatter(x = centroids2[: , 0], y =  centroids2[: , 1], s = 50, c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)'), plt.xlabel('Annual Income (k$)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Segmentazione dei clienti Age vs Annual Income vs Spending Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"X3 = store[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    method = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, tol=0.0001,  random_state= 1))\n    method.fit(X3)\n    inertia.append(method.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot del valore della somma della radice delle distanze al crescere del numero dei cluster\n\nplt.figure(1 , figsize = (10 ,5))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Numero dei cluster') , plt.ylabel('Sum of Squared Distance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Analisi di Silhouette\n\nL'analisi di Silhouette si riferisce a un metodo di interpretazione e convalida della coerenza dei dati rispetto ai cluster identificati.\n\nIl valore dek coefficiente di Silhouette è una misura di quanto un oggetto sia simile al proprio cluster (coesione) rispetto ad altri cluster (separazione). Tale valore è espresso in un intervallo [-1, +1], dove un valore alto indica che l'esempio è ben adattato al proprio cluster e scarsamente abbinato ai cluster vicini. Se la maggior parte degli oggetti ha un valore elevato, la suddivisione degli esempi nei rispettivi cluster è appropriata. Se molti punti, invece, hanno un valore basso o negativo, la suddivisione degli esempi nei cluster definiti potrebbe risultare inappropriata.\n\nL'analisi di Silhouette può essere condotta utilizzando una qualsiasi metrica di distanza, come la distanza euclidea o la distanza di Manhattan.\nIn particolare, può essere espressa come segue:\n$$\\text{silhouette score}=\\frac{p-q}{max(p,q)}$$\n\n$p$ è la distanza media tra il punto e il centroide del cluster più vicino.\n\n$q$ è la distanza media intra-cluster definita su tutti i punti presenti nel proprio cluster.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definizione della lista del numero di cluster da testare\nrange_n_clusters = list(x for x in range (2,10+1))\n\nfor num_clusters in range_n_clusters:\n    method = KMeans(n_clusters = num_clusters ,init='k-means++', n_init = 10 ,max_iter=300, tol=0.0001,  random_state= 1)\n    method.fit(X3)\n    cluster_labels = method.labels_\n    # Calcolo coefficiente di silhouette\n    silhouette_avg = silhouette_score(X3, cluster_labels)\n    print(\"Per n_clusters={0}, il coefficiente di Silhouette è pari a {1}\".format(num_clusters, silhouette_avg))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ottenuti i risultati visibili dal precedente grafico, si sceglie il valore 6 per il parametro _n_clusters_ e si riesegue nuovamente l'algoritmo k-Means registrando le relative etichette."},{"metadata":{"trusted":true},"cell_type":"code","source":"method = (KMeans(n_clusters = 6, init='k-means++', n_init = 10, max_iter=300, tol=0.0001, random_state= 1))\nmethod.fit(X3)\nlabels3 = method.labels_\ncentroids3 = method.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stampa delle etichette predette\nmethod.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assegnazione delle etichette a ciascun esempio presente nel DataFrame\nstore['Cluster_Id'] = method.labels_\n# Stampa dei primi 5 esempi\nstore.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BoxPlot ottenuti con k-Means"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_list = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n\nfor feature in features_list:\n    sns.boxplot(x='Cluster_Id', y=feature, data=store)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clustering gerarchico\n\nGli algoritmi gerarchici in genere clusterizzano i dati usando le misure di distanza. Tuttavia, l'uso delle funzioni di distanza non è obbligatorio. Molti algoritmi gerarchici utilizzano altri metodi di clustering, ad esempio metodi density-based o graph-based, come subroutine per la costruzione della gerarchia.\n\nUno dei motivi principali di utiulizzo di tale modalità di clustering è che diversi livelli di granularità del clustering forniscono dei dettagli specifici per l'applicazione. Ciò fornisce una tassonomia di cluster, che possono essere esplorati sulla base di tali dettagli semantici.\n\nL'organizzazione gerarchica consente la navigazione manuale molto conveniente per un utente, specialmente quando il contenuto dei cluster può essere descritto in modo semanticamente comprensibile. In altri casi, tali organizzazioni gerarchiche possono essere utilizzate dagli algoritmi di indicizzazione, rispetto alle macroaree di riferimento.\nInoltre, tali metodi possono talvolta essere utilizzati anche per creare cluster \"piatti\" migliori (dove tutte le categorie sono posto allo stesso livello). Alcuni metodi gerarchici agglomerativi e metodi di divisione, possono fornire cluster di qualità migliore rispetto ai metodi di partizionamento come k-Means, sebbene con un costo computazionale più elevato.\n\nEsistono due tipi di algoritmi gerarchici, a seconda di come viene costruito l'albero gerarchico dei cluster:\n- Metodi bottom-up (agglomerativi): i singoli punti dati vengono successivamente agglomerati in cluster di livello superiore. La principale variazione tra i diversi metodi è nella scelta della funzione obiettivo utilizzata per fondere i cluster.\n- Metodi top-down (divisivi): un approccio top-down viene utilizzato per partizionare successivamente i punti in una struttura ad albero. Un algoritmo di clustering piatto può essere utilizzato per il partizionamento in un determinato passo. Tale approccio offre un'enorme flessibilità in termini di scelta del compromesso tra l'equilibrio nella struttura ad albero e l'equilibrio nel numero di punti in ciascun nodo della struttura ad albero."},{"metadata":{},"cell_type":"markdown","source":"**Single Linkage:<br>**\n\nNel clustering che sfrutta la modalità di collegamento _single linkage_, la distanza tra due cluster è definita come la più piccola distanza calcolabile tra due punti in ciascun cluster. Per esempio, la distanza tra il cluster “r” e “s” è uguale alla lunghezza dell'arco tra i due punti più vicini, così come visibile dalla figura riportata.\n\n![](https://www.saedsayad.com/images/Clustering_single.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applicazione del metodo Single Linkage\nplt.figure(figsize = (10,5))\nsingle_linkage = linkage(X3, method=\"single\", metric='euclidean')\ndendrogram(single_linkage)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Complete Linkage<br>**\n\nNel metodo _Complete Linkage_, la distanza tra due cluster è definita come la più grande distanza tra due punti in ciascun cluster.\n\nPer esempio, la distanza tra i cluster “r” e “s” è uguale alla lunghrzza dell'arco tra i due punti più distanti dei due cluster.\n\n![](https://www.saedsayad.com/images/Clustering_complete.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applicazione del metodo Complete linkage\nplt.figure(figsize = (10,5))\ncomplete_linkage = linkage(X3, method=\"complete\", metric='euclidean')\ndendrogram(complete_linkage)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Average Linkage:<br>**\n\nCon il metodo _Average Linkage_, la distanza tra due cluster è definita come la distanza media presente tra ciascun punto di un cluster con tutti i punti dell'altro cluster.\n\nPer esempio, la distanza tra i cluster “r” e “s” è uguale alla lunghezza mediata dell'arco che connette i punti di un cluster all'altro.\n\n![](https://www.saedsayad.com/images/Clustering_average.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applicazione del metodo Average linkage\nplt.figure(figsize = (10,5))\navg_linkage = linkage(X3, method=\"average\", metric='euclidean')\ndendrogram(avg_linkage)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Taglio del Dendrogramma in base al valore di K"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Desiderando un numero di cluster pari a 4, si inizializza il parametro n_clusters=4\ncluster_labels = cut_tree(complete_linkage, n_clusters=4).reshape(-1, )\n#Stampa delle etichette dei cluster\ncluster_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assegnazione delle etichette a ciascun esempio presente nel DataFrame\nstore['Cluster_Labels'] = cluster_labels\n# Stampa dei primi 5 elementi presenti nel DataFrame\nstore.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot delle Features\n\nfeatures_list = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n\nfor feature in features_list:\n    sns.boxplot(x='Cluster_Labels', y=feature, data=store)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Numero dei clienti in ciascun cluster\nstore['Cluster_Labels'].value_counts(ascending=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Agglomerative Clustering"},{"metadata":{},"cell_type":"markdown","source":"Nel clustering agglomerativo, come già accennato, i singoli punti dati vengono agglomerati iterativamente in cluster di livello superiore.\nNel primo step, ogni singolo punto costituisce un cluster. Successivamente, si agglomerano insieme via via sempre più punti, andando a costruire cluster sempre più popolati.\nIl metodo si arresta quando si raggiunge un certo numero di cluster.\nNel metodo seguente vengono utilizzati i seguenti parametri:\n- <b>n_clusters=3</b>: si desiderano tre cluster come suggerito dal metodo Elbow\n- <b>affinity</b>: metrica utilizzata per computare il linkage. Si utilizza la distanza euclidea.\n    - <b>euclidean</b>\n    - <b>l1</b>\n    - <b>l2</b>\n    - <b>manhattan</b>\n    - <b>cosine</b>\n    - <b>precomputed</b>\n- <b>linkage</b>: criterio di collegamento da utilizzare. Ne esistono diversi:\n    - <b>ward</b>: minimizza la varianza dei cluster che devono essere fusi insieme\n    - <b>average</b>: usa la media delle distanze di ogni osservazione nei due insiemi\n    - <b>complete</b>: usa la distanza massima tra due punti negli insiemi\n    - <b>single</b>: usa la distanza minima tra due insiemi"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\n\nac = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='complete')\nagglomerative_cluster_labels = ac.fit_predict(X3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assegnazione delle etichette a ciascun esempio presente nel DataFrame\nstore['Agglomerative_Clustering'] = agglomerative_cluster_labels\n# Stampa dei primi 5 elementi presenti nel DataFrame\nstore.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot delle Features\n\nfeatures_list = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n\nfor feature in features_list:\n    sns.boxplot(x='Agglomerative_Clustering', y=feature, data=store)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DBSCAN\n\nDBSCAN è un algoritmo di clustering Density-Based utilizzabile su dataset che presentano punti rumorosi. È un algoritmo non parametrico di clustering basato sulla densità: dato un insieme di punti in uno spazio, raggruppa i punti che sono altamente vicini, contrassegnando come punti anomali i punti che si trovano da soli in regioni a bassa densità. \n\nDBSCAN è uno degli algoritmi di clustering più comuni e anche i più citati nella letteratura scientifica e presenta i seguenti vantaggi:\n- Non richiede la specifica a priori di un numero di cluster, a differenza di k-Means\n- Gestione accurata dei punti rumorosi\n- Robusto in presenza degli outliers\n\nPer DBSCAN, invece, si identificano i seguenti svantaggi:\n- Non deterministico: i punti presenti sulle frontiere possono essere assegnati a cluster differenti, in base all'ordine in cui i dati sono processati\n- La qualità dei risultati restituiti da DBSCAN dipende dalla misura di distanza usata\n- Sensibile al fenomeno della \"Curse of dimensionality\" in presenza di dataset con un numero di features elevato"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import DBSCAN\nX3 = StandardScaler().fit_transform(X3)\n\ndbscan = DBSCAN(eps=0.3, min_samples=5, metric = 'euclidean')\ndbscan.fit(X3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dbscan_labels = dbscan.labels_\n#Stampa delle etichette dei cluster\ndbscan_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Identificazione numero di cluster e punti rumorosi\nn_clusters_ = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\nn_noise_ = list(dbscan_labels).count(-1)\n\nprint('Numero di cluster stimati: %d' % n_clusters_)\nprint('Numero di punti rumorosi identificati: %d' % n_noise_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assegnazione delle etichette a ciascun esempio presente nel DataFrame\nstore['DensityBased_Labels'] = dbscan_labels\n# Stampa dei primi 5 elementi presenti nel DataFrame\nstore.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot delle Features\n\nfeatures_list = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n\nfor feature in features_list:\n    sns.boxplot(x='DensityBased_Labels', y=feature, data=store)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}