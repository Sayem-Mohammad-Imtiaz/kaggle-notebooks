{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Business problem\nWe want to Identify wheater the job posting given to us is the geniune or a fraudulent"},{"metadata":{},"cell_type":"markdown","source":"# Machine learning problem\nThis one is a classical Classifiation problem were we have to predict weater the given input belongs to the class 0 or class 1 here the input is the job post and the we have to clasify is it a geniune opening or a fraud opening."},{"metadata":{},"cell_type":"markdown","source":"# Performance metric\nThe performance metric that I am going to use it in this is F1 score."},{"metadata":{},"cell_type":"markdown","source":"# importing the libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom prettytable import PrettyTable\nfrom scipy.sparse import hstack\nfrom sklearn.metrics import confusion_matrix\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_curve\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import SGDClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"job_data = pd.read_csv('../input/real-or-fake-fake-jobposting-prediction/fake_job_postings.csv', index_col = 'job_id')\njob_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the shape of the data\njob_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\nThere are around 18k data with 17 feature"},{"metadata":{},"cell_type":"markdown","source":"### How balance the dataset is"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 0 is not fake and 1 is fake\n# checking for Imbalance\njob_data['fraudulent'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"percentage of data with class a 0: \",job_data['fraudulent'].value_counts()[0] /job_data.shape[0] *100)\nprint(\"percentage of data with class a 1: \",job_data['fraudulent'].value_counts()[1] /job_data.shape[0] *100)\nsns.set(style=\"darkgrid\")\nax = sns.countplot(x=\"fraudulent\", data=job_data)\nax.set_title(\"count plot of the classes\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking at the count of number of data point belongs to each class we can clearly see that the dataset is highly imbalaced in nature and hence we have to balance it using various balancing techniques"},{"metadata":{"trusted":true},"cell_type":"code","source":"job_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking the missing value "},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ntotal= job_data.isnull().sum()\nmissing_percent =  job_data.isnull().sum()* 100 / len(job_data)\nmissing_data = pd.concat([total,missing_percent],axis=1,keys=['Total','Percentage'])\nf,ax = plt.subplots(figsize=(15,6))\nxlocs=plt.xticks(rotation='90')\nbars = plt.bar(missing_data.index,missing_data['Total'])\n\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x(), yval + .005, round((yval*100)/len(job_data),2), color='red',fontweight='bold')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mejority of the columns in this data has missing value as we can see the percent of missing value in each columns/feature so we have to decide either we have to imuputate the missing data or we have to drop the feature"},{"metadata":{},"cell_type":"markdown","source":"Now since the dataset contains missing value and it has 17 columns and feature we have to first identitfy which of them is catgorical and which of them are not to do so I am going to count the number of unique value in the cols and will set some threshold and if the col have unique values less than the threshold means that column is categorical"},{"metadata":{},"cell_type":"markdown","source":"### Identiyfing the categorical features "},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_feature = []\nfor col in job_data.columns:\n    print(f'Unique rows in {col}:', job_data[col].nunique())\n    if job_data[col].nunique() < 15:\n        categorical_feature.append(col)\nprint('Categorical feature:',categorical_feature)\nprint('Total cat feature',len(categorical_feature))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can se that we have total of 6 categorical feature I am not counting the fraudulent col as that is our target class."},{"metadata":{},"cell_type":"markdown","source":"## Imputation of missing values"},{"metadata":{},"cell_type":"markdown","source":"Now It is time to imputate the missing value data. You must remember that we have calculated the percenteage of mising value of data so we set a threshold using that and drop the column having percentage more than threshold."},{"metadata":{"trusted":true},"cell_type":"code","source":"# job_data.dropna(thresh = threshold, axis=1)\njob_data.drop(columns = ['salary_range'],axis=1,inplace = True)\njob_data['department'].fillna('other',inplace=True)\njob_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"job_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# total= job_data.isnull().sum()\n# missing_percent =  job_data.isnull().sum()* 100 / len(job_data)\n# missing_data = pd.concat([total,missing_percent],axis=1,keys=['Total','Percentage'])\n# f,ax = plt.subplots(figsize=(15,6))\n# xlocs=plt.xticks(rotation='90')\n# bars = plt.bar(missing_data.index,missing_data['Total'])\n\n# for bar in bars:\n#     yval = bar.get_height()\n#     plt.text(bar.get_x(), yval + .005, round((yval*100)/len(job_data),2), color='red',fontweight='bold')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# empty_columns = job_data.loc[:,job_data.isnull().sum()>0]\n# empty_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Imputating the categorical feature\nnow to all the categorical data we will replace it with the most frequent occuring data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# filling he categorical missing data\nfor feature in categorical_feature:\n#     print(\"feature name: \",feature)\n    if job_data[feature].isnull().sum() > 0 :\n        print(\"find the empyt value in feature:\",feature)\n        job_data[feature].fillna(value= job_data[feature].mode()[0],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Imputating the non categorical feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"# filling the non categorical data\nnon_categorical_data =  list(set(job_data.columns) - set(categorical_feature))\nfor feature in non_categorical_data:\n    if job_data[feature].isnull().sum() > 0 :\n        print(\"find the empyt value in feature:\",feature)\n        job_data[feature].fillna(value= 'Not specified',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking the data balance after Imputation"},{"metadata":{"trusted":true},"cell_type":"code","source":"job_data.isnull().sum(), job_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are done with our missing value and now we have no missing data feature so we can move futher in our analysis"},{"metadata":{},"cell_type":"markdown","source":"## Univarient Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"job_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"job_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_feature.pop()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Employment type on job post fraud"},{"metadata":{"trusted":true},"cell_type":"code","source":"# categorical feature effect on the fraudulent classes\nplt.figure(1,figsize=(20,8))\nsns.countplot(hue=job_data.fraudulent,x=job_data.employment_type);\nplt.title('Which type of jobs have more fraudulent postings');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\n\nBy observing the count plot of the employment type we can make a conclusion that expect of employment type full time there is no other types that contribute to the fraudulent job post."},{"metadata":{},"cell_type":"markdown","source":"### Required Experience on job post fraud"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(20,8))\nsns.countplot(hue=job_data.fraudulent,x=job_data.required_experience);\nplt.title('Which required experience of jobs have more fraudulent postings');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observation\n\nThe mid senior level work exprience job posting have more fraudulent job post then any other"},{"metadata":{},"cell_type":"markdown","source":"### Required Education on job post fraud"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=job_data.fraudulent,x=job_data.required_education)\nplt.legend(loc='upper right')\nplt.title('Which required education of jobs have more fraudulent postings')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observation\n\nWe can see in the plot that the job post which have education requirement as bachelors degree contribute more to the fraudulent post"},{"metadata":{},"cell_type":"markdown","source":"### Telecommuting Education on job post fraud"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=job_data.fraudulent,x=job_data.telecommuting)\nplt.legend(loc='upper right')\nplt.title('How telecommuting jobs effect contribute towards the fraudulent postings.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\nFor the non telecomunicating position there is fraudulent post then the telecomunicating position."},{"metadata":{},"cell_type":"markdown","source":"### Presence of company logo on job post on fraud post"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=job_data.fraudulent,x=job_data.has_company_logo)\nplt.legend(loc='upper right')\nplt.title('Company logo presence effect on fraudulent postings')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\nJob post which have the company logo in it has less number of faudulent casses then the one which do not have the company logo which is like a natural thing to see."},{"metadata":{},"cell_type":"markdown","source":"### Presence of screening question on job post fraud"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=job_data.fraudulent,x=job_data.has_questions)\nplt.legend(loc='upper right')\nplt.title('Screening Question effect fraudulent postings')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\nWe can see that is the screening questions are present then there is less number of fraudulent job compare to the job posting where not screening questions are present."},{"metadata":{},"cell_type":"markdown","source":"### DATA CLEANING AND FEATURE ENGINEEING OF LOCATION COLUMN"},{"metadata":{},"cell_type":"markdown","source":"#### Getting the country name from the job location"},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_country = []\n# not_specified_country_index = []\n# processed_state = []\n# not_specified_state_index = []\n# processed_city = []\ndef location_separator(splitted_location_name):\n    for idx, j in enumerate(splitted_location_name):\n        if j.isspace() :\n            if idx == 0:\n                processed_country.append('not given')\n#             elif idx == 1:\n# #                 not_specified_state_index.append(idx)\n#                 processed_state.append('not given')\n#             else:\n#                 processed_city.append('not given')\n        else:\n            if idx == 0:\n                processed_country.append(j.replace(\" \", \"\"))\n#             elif idx == 1:\n#                 processed_state.append(j.replace(\" \", \"\"))\n#             else:\n#                 processed_city.append(j.replace(\" \", \"\")) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_location = []\nfor idx,value in enumerate(job_data.location.to_list()):\n    if '\\t' in value:\n        processed_location_name = value.replace('\\t','')\n        processed_location.append(processed_location_name)\n        splitted_location_name = processed_location_name.split(',')\n        location_separator(splitted_location_name) \n    else:\n        \n        if value == 'Not specified':\n            value = 'NA, NA, NA'\n        processed_location.append(value)\n        location_separator(value.split(','))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(processed_country),len(processed_location)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Getting the state name from the location"},{"metadata":{"trusted":true},"cell_type":"code","source":"state_name = []\nimport re\nfor idx,value in enumerate(job_data.location.to_list()):\n    value = value.replace('\\t','')\n    if len(value.split(',')) > 3:\n        state_value = list(filter(str.strip, value.split(',')))\n        state_value = [re.sub('[0-9]','',word) for word in state_value if len(word.replace(\" \", \"\"))>1]\n        state_value = list(filter(str.strip,state_value))\n        state_value = list(set(state_value))\n        state_value = state_value[:3]\n        state_name.append(state_value[1].replace(\" \", \"\"))  \n    elif len(value.split(',')) == 1:\n        state_name.append('NOT GIVEN')      \n    else:\n        if len(value.split(',')[1].replace(\" \", \"\"))==1 or value.split(',')[1].isspace():\n#             print(\"original_value: \",value,\"<---list_value:--> \",value.split(','))\n#             print('idx',idx,'lenght: ',len(value.split(',')))\n            state_name.append('NOT GIVEN')\n        elif value.split(',')[1].replace(\" \", \"\").isdigit():\n            state_name.append('NOT GIVEN')\n        else:\n#             print(\"original_value: \",value,\"<---list_value:--> \",value.split(','))\n#             print('idx',idx,'lenght: ',len(value.split(',')))\n            \n            state_name.append(value.split(',')[1].replace(\" \", \"\")) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(state_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# job_data[job_data.duplicated(keep=False)].reset_index()\n# state_name\n# len('01')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Getting the city name from the location"},{"metadata":{"trusted":true},"cell_type":"code","source":"city_name = []\nimport re\nfor idx,value in enumerate(job_data.location.to_list()):\n    value = value.replace('\\t','')\n    if len(value.split(',')) > 3:\n        state_value = list(filter(str.strip, value.split(',')))\n        state_value = [re.sub('[0-9]','',word) for word in state_value if len(word.replace(\" \", \"\"))>1]\n        state_value = list(filter(str.strip,state_value))\n        state_value = list(set(state_value))\n        city_name.append(state_value[2].replace(\" \", \"\"))  \n    elif len(value.split(',')) == 1:\n        city_name.append('NOT GIVEN')      \n    else:\n        if len(value.split(',')[2].replace(\" \", \"\"))==1 or value.split(',')[2].isspace():\n            city_name.append('NOT GIVEN')\n        elif value.split(',')[2].replace(\" \", \"\").isdigit():\n            city_name.append('NOT GIVEN')\n        else:\n            city_name.append(value.split(',')[2].replace(\" \", \"\"))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(processed_country), len(state_name),len(city_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_job_data = job_data.copy()\nprocessed_job_data['country'] = processed_country\nprocessed_job_data['state'] = state_name\nprocessed_job_data['city'] = city_name\nprocessed_job_data.drop(columns=['location'],axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Separating all the fraud and no fraud data for some analysis on our newly created feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"fraud_data = processed_job_data[processed_job_data.fraudulent==1]\n# fraud_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Which Department is the most common in the fraud job posting"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=fraud_data.fraudulent,x=fraud_data.department, data=fraud_data, order= fraud_data.department.value_counts().iloc[:].index)\nplt.legend(loc='upper right')\nplt.title('which department contribute towards the fraudulent postings.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\nBy lookig at the plot of department of fraudulent post we can clearly see that the most fraud post there is no deparment specified more than 64% of total fraudlent post dont specify there department."},{"metadata":{"trusted":true},"cell_type":"code","source":"no_fraud_data = processed_job_data[processed_job_data.fraudulent==0]\n# no_fraud_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Which department is most common in the non fraud job post"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=no_fraud_data.fraudulent,x=no_fraud_data.department, data=no_fraud_data, order= no_fraud_data.department.value_counts().iloc[:100].index)\nplt.legend(loc='upper right')\nplt.title('which department contribute towards the non fraudulent postings.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\nIn the non fraudulent job post also more than 59% of total job post do not specify there department so here we cannot clearly say that if the department is specified as other then the job post is fraud."},{"metadata":{},"cell_type":"markdown","source":"### Which industry has most common in fraud job post"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=no_fraud_data.fraudulent,x=no_fraud_data.industry,data=no_fraud_data,order=no_fraud_data.industry.value_counts().iloc[:50].index)\nplt.legend(loc='upper right')\nplt.title('which department contribute towards the non fraudulent postings.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\n\nLooking at the count plot of the industry type in the non fraudulent job post we can see that the high number of job post does not specify the industry count."},{"metadata":{},"cell_type":"markdown","source":"### Which industry is most common in the non fraud job post"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=fraud_data.fraudulent,x=fraud_data.industry,data=fraud_data,order=fraud_data.industry.value_counts().iloc[:50].index)\nplt.legend(loc='upper right')\nplt.title('which department contribute towards the non fraudulent postings.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\nThe fraudulent post also do not specify the industry type and it is vast in number but if we see carefully the number of post with oil and energy , Accounting etc is higher than that of non fraudulent post hence we can use industry as feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_job_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Which function is most common in the fraud job post and non fraud job post"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=processed_job_data.fraudulent,x=processed_job_data.function ,data=processed_job_data, order = processed_job_data.function.value_counts().iloc[:].index )\nplt.legend(loc='upper right')\nplt.title('function effect on fraudulent postings')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\nWe cannot make a descision after looking at the countplot of the function in case of the job post to be fraud or geniuine."},{"metadata":{},"cell_type":"markdown","source":"### Which country is most common in the fraud job post"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=fraud_data.fraudulent,x=fraud_data.country ,data=fraud_data, order = fraud_data.country.value_counts().iloc[:].index )\nplt.legend(loc='upper right')\nplt.title('state effect on fraudulent postings')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\nUS is the country with maximum fraud post."},{"metadata":{},"cell_type":"markdown","source":"### Which state is the most common in the fraud job post"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=fraud_data.fraudulent,x=fraud_data.state ,data=fraud_data, order = fraud_data.state.value_counts().iloc[:].index )\nplt.legend(loc='upper right')\nplt.title('state effect on fraudulent postings')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\nIn US also the texas is the state were most fraud job post were present"},{"metadata":{},"cell_type":"markdown","source":"### Which country is most common in the non fraud job post"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=no_fraud_data.fraudulent,x=no_fraud_data.country ,data=no_fraud_data, order = no_fraud_data.country.value_counts().iloc[:].index )\nplt.legend(loc='upper right')\nplt.title('Country effect on non fraudulent postings')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\n Also US is the country with most geniune job post hence we can say that most of the job post were posted in the US then any other country in the world."},{"metadata":{},"cell_type":"markdown","source":"### Which state is the most common in the non fraud job post"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=no_fraud_data.fraudulent,x=no_fraud_data.state ,data=no_fraud_data, order = no_fraud_data.state.value_counts().iloc[:100].index )\nplt.legend(loc='upper right')\nplt.title('state effect on non fraudulent postings')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\nThis is intereseting the to see that most of the authentic job post in the us does not specify the state in there location."},{"metadata":{},"cell_type":"markdown","source":"### Which title of the job is common in the most fraud job post"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=fraud_data.fraudulent,x=fraud_data.title ,data=fraud_data, order = fraud_data.title.value_counts().iloc[:100].index )\nplt.legend(loc='upper right')\nplt.title('title effect on fraudulent postings')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\nIn the plot of titles in the fraud post we can see the titles like URGENT, Data Entry or customer care have most the count towards fraud post."},{"metadata":{},"cell_type":"markdown","source":"### Which title of the job is common in the most non fraud job post"},{"metadata":{"trusted":true},"cell_type":"code","source":"no_fraud_data.title.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Which title of the job is common in the most fraud job post"},{"metadata":{"trusted":true},"cell_type":"code","source":" fraud_data.title.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(20,8))\nplt.xticks(rotation='90')\nsns.countplot(hue=no_fraud_data.fraudulent,x=no_fraud_data.title ,data=no_fraud_data, order = no_fraud_data.title.value_counts().iloc[:100].index )\nplt.legend(loc='upper right')\nplt.title('title effect on non fraudulent postings')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\nMost geniune job post have the title as the english teacher."},{"metadata":{},"cell_type":"markdown","source":"## Text Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import re\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://gist.github.com/sebleier/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_job_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Description"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessed_description = []\nfor sentance in tqdm(processed_job_data['description'].values):\n    sent = sentance.lower()\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = re.sub(r'\\w*[0-9]\\w*', '', sent, flags=re.MULTILINE)\n    sent = re.sub('[0-9]', ' ', sent)\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    \n    sent = ' '.join(e for e in sent.split(' ') if e not in stopwords)\n    sent = decontracted(sent)\n    # https://gist.github.com/sebleier/554280\n    preprocessed_description.append(sent)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_job_data.drop(['description'], axis=1,inplace=True)\nprocessed_job_data['preprocessed_description'] = preprocessed_description","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Benefits"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessed_benefits = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(processed_job_data['benefits'].values):\n    sent = sentance.lower()\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = re.sub(r'\\w*[0-9]\\w*', '', sent, flags=re.MULTILINE)\n    sent = re.sub('[0-9]', ' ', sent)\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    \n    sent = ' '.join(e for e in sent.split(' ') if e not in stopwords)\n    sent = decontracted(sent)\n    # https://gist.github.com/sebleier/554280\n#     sent = ' '.join(e for e in sent.split() if e not in stopwords)\n#     preprocessed_essays.append(sent.lower().strip())\n    preprocessed_benefits.append(sent)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_job_data.drop(['benefits'], axis=1,inplace=True)\nprocessed_job_data['preprocessed_benefits'] = preprocessed_benefits\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Requirements"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessed_requirements = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(processed_job_data['requirements'].values):\n    sent = sentance.lower()\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = re.sub(r'\\w*[0-9]\\w*', '', sent, flags=re.MULTILINE)\n    sent = re.sub('[0-9]', ' ', sent)\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    \n    sent = ' '.join(e for e in sent.split(' ') if e not in stopwords)\n    sent = decontracted(sent)\n    # https://gist.github.com/sebleier/554280\n#     sent = ' '.join(e for e in sent.split() if e not in stopwords)\n#     preprocessed_essays.append(sent.lower().strip())\n    preprocessed_requirements.append(sent)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_job_data.drop(['requirements'], axis=1,inplace=True)\nprocessed_job_data['preprocessed_requirements'] = preprocessed_requirements","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Title"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessed_title = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(processed_job_data['title'].values):\n    sent = sentance.lower()\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = re.sub(r'\\w*[0-9]\\w*', '', sent, flags=re.MULTILINE)\n    sent = re.sub('[0-9]', ' ', sent)\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    sent = ' '.join(e for e in sent.split(' ') if e not in stopwords)\n    sent = sent.strip()\n    sent = decontracted(sent)\n    # https://gist.github.com/sebleier/554280\n#     sent = ' '.join(e for e in sent.split() if e not in stopwords)\n#     preprocessed_essays.append(sent.lower().strip())\n    preprocessed_title.append(sent)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_job_data.drop(['title'], axis=1,inplace=True)\nprocessed_job_data['preprocessed_title'] = preprocessed_title","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Company Profile"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessed_company_profile = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(processed_job_data['company_profile'].values):\n    sent = sentance.lower()\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = re.sub(r'\\w*[0-9]\\w*', '', sent, flags=re.MULTILINE)\n    sent = re.sub('[0-9]', ' ', sent)\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    sent = ' '.join(e for e in sent.split(' ') if e not in stopwords)\n    sent = sent.strip()\n    sent = decontracted(sent)\n    # https://gist.github.com/sebleier/554280\n#     sent = ' '.join(e for e in sent.split() if e not in stopwords)\n#     preprocessed_essays.append(sent.lower().strip())\n    preprocessed_company_profile.append(sent)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_job_data.drop(['company_profile'], axis=1,inplace=True)\nprocessed_job_data['preprocessed_company_profile'] = preprocessed_company_profile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_job_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_job_data.drop(['department'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# processed_job_data\nprocessed_job_data.reset_index(inplace=True)\nprocessed_job_data.drop(['job_id'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = processed_job_data['fraudulent'].values\nX = processed_job_data.drop(['fraudulent'], axis=1)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install imbalanced-learn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check version number\nimport imblearn\nprint(imblearn.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # balancig the Imbalanced dataset https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18\n# from sklearn.model_selection import train_test_split\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y)\n# X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.33, stratify=y_train)\n\n# print(X_train.shape)\n# print(y_train.shape)\n# print(X_cv.shape)\n# print(y_cv.shape)\n# print(X_test.shape)\n# print(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Onehot encoding of the categorical data"},{"metadata":{},"cell_type":"markdown","source":"### Employment type"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.feature_extraction.text import CountVectorizer\n# vectorizer = CountVectorizer(vocabulary=set(processed_job_data['employment_type']),lowercase=False, binary=True)\n# X_train_employment_type_one_hot = vectorizer.fit_transform(X_train['employment_type'].values)\n# X_cv_employment_type_one_hot = vectorizer.transform(X_cv['employment_type'].values)\n# X_test_employment_type_one_hot = vectorizer.transform(X_test['employment_type'].values)\n\n# print(vectorizer.get_feature_names())\n# print(\"Shape of X_train matrix after one hot encodig \",X_train_employment_type_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_employment_type_one_hot.shape)\n# print(\"Shape of X_test matrix after one hot encodig \",X_test_employment_type_one_hot.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Required Experience"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.feature_extraction.text import CountVectorizer\n# vectorizer = CountVectorizer(vocabulary=set(processed_job_data['required_experience']),lowercase=False, binary=True)\n# X_train_required_experience_one_hot = vectorizer.fit_transform(X_train['required_experience'].values)\n# X_cv_required_experience_one_hot = vectorizer.transform(X_cv['required_experience'].values)\n# X_test_required_experience_one_hot = vectorizer.transform(X_test['required_experience'].values)\n\n# print(vectorizer.get_feature_names())\n# print(\"Shape of X_train matrix after one hot encodig \",X_train_required_experience_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_required_experience_one_hot.shape)\n# print(\"Shape of X_test matrix after one hot encodig \",X_test_required_experience_one_hot.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Required Education"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.feature_extraction.text import CountVectorizer\n# vectorizer = CountVectorizer(vocabulary=set(processed_job_data['required_education']),lowercase=False, binary=True)\n# X_train_required_education_one_hot = vectorizer.fit_transform(X_train['required_education'].values)\n# X_cv_required_education_one_hot = vectorizer.transform(X_cv['required_education'].values)\n# X_test_required_education_one_hot = vectorizer.transform(X_test['required_education'].values)\n\n# print(vectorizer.get_feature_names())\n# print(\"Shape of X_train matrix after one hot encodig \",X_train_required_education_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_required_education_one_hot.shape)\n# print(\"Shape of X_test matrix after one hot encodig \",X_test_required_education_one_hot.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Industry"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.feature_extraction.text import CountVectorizer\n# vectorizer = CountVectorizer(vocabulary=set(processed_job_data['industry']),lowercase=False, binary=True)\n# X_train_industry_one_hot = vectorizer.fit_transform(X_train['industry'].values)\n# X_cv_industry_one_hot = vectorizer.transform(X_cv['industry'].values)\n# X_test_industry_one_hot = vectorizer.transform(X_test['industry'].values)\n\n# print(vectorizer.get_feature_names())\n# print(\"Shape of X_train matrix after one hot encodig \",X_train_industry_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_industry_one_hot.shape)\n# print(\"Shape of X_test matrix after one hot encodig \",X_test_industry_one_hot.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### function"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.feature_extraction.text import CountVectorizer\n# vectorizer = CountVectorizer(vocabulary=set(processed_job_data['function']),lowercase=False, binary=True)\n# X_train_function_one_hot = vectorizer.fit_transform(X_train['function'].values)\n# X_cv_function_one_hot = vectorizer.transform(X_cv['function'].values)\n# X_test_function_one_hot = vectorizer.transform(X_test['function'].values)\n\n# print(vectorizer.get_feature_names())\n# print(\"Shape of X_train matrix after one hot encodig \",X_train_function_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_function_one_hot.shape)\n# print(\"Shape of X_test matrix after one hot encodig \",X_test_function_one_hot.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Country"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.feature_extraction.text import CountVectorizer\n# vectorizer = CountVectorizer(vocabulary=set(processed_job_data['country']),lowercase=False, binary=True)\n# X_train_country_one_hot = vectorizer.fit_transform(X_train['country'].values)\n# X_cv_country_one_hot = vectorizer.transform(X_cv['country'].values)\n# X_test_country_one_hot = vectorizer.transform(X_test['country'].values)\n\n# print(vectorizer.get_feature_names())\n# print(\"Shape of X_train matrix after one hot encodig \",X_train_country_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_country_one_hot.shape)\n# print(\"Shape of X_test matrix after one hot encodig \",X_test_country_one_hot.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### State"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.feature_extraction.text import CountVectorizer\n# vectorizer = CountVectorizer(vocabulary=set(processed_job_data['state']),lowercase=False, binary=True)\n# X_train_state_one_hot = vectorizer.fit_transform(X_train['state'].values)\n# X_cv_state_one_hot = vectorizer.transform(X_cv['state'].values)\n# X_test_state_one_hot = vectorizer.transform(X_test['state'].values)\n\n# print(vectorizer.get_feature_names())\n# print(\"Shape of X_train matrix after one hot encodig \",X_train_state_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_state_one_hot.shape)\n# print(\"Shape of X_test matrix after one hot encodig \",X_test_state_one_hot.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vectorizing the preprocessed text data"},{"metadata":{},"cell_type":"markdown","source":"### Description"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.feature_extraction.text import TfidfVectorizer\n# vectorizer = TfidfVectorizer(min_df=10)\n# X_train_description_tfidf = vectorizer.fit_transform(X_train['preprocessed_description'])\n# X_cv_description_tfidf = vectorizer.transform(X_cv['preprocessed_description'])\n# X_test_description_tfidf = vectorizer.transform(X_test['preprocessed_description'])\n# print(\"Shape of X_train_essay_tfidf matrix after \",X_train_description_tfidf.shape)\n# print(\"Shape of X_cv_essay_tfidf matrix after \",X_cv_description_tfidf.shape)\n# print(\"Shape of X_test_essay_tfidf matrix after \",X_test_description_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Benifits"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.feature_extraction.text import TfidfVectorizer\n# vectorizer = TfidfVectorizer(min_df=10)\n# X_train_denifits_tfidf = vectorizer.fit_transform(X_train['preprocessed_benefits'])\n# X_cv_denifits_tfidf = vectorizer.transform(X_cv['preprocessed_benefits'])\n# X_test_denifits_tfidf = vectorizer.transform(X_test['preprocessed_benefits'])\n# print(\"Shape of X_train_essay_tfidf matrix after \",X_train_denifits_tfidf.shape)\n# print(\"Shape of X_cv_essay_tfidf matrix after \",X_cv_denifits_tfidf.shape)\n# print(\"Shape of X_test_essay_tfidf matrix after \",X_test_denifits_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Requirement"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.feature_extraction.text import TfidfVectorizer\n# vectorizer = TfidfVectorizer(min_df=10)\n# X_train_requirements_tfidf = vectorizer.fit_transform(X_train['preprocessed_requirements'])\n# X_cv_requirements_tfidf = vectorizer.transform(X_cv['preprocessed_requirements'])\n# X_test_requirements_tfidf = vectorizer.transform(X_test['preprocessed_requirements'])\n# print(\"Shape of X_train_essay_tfidf matrix after \",X_train_requirements_tfidf.shape)\n# print(\"Shape of X_cv_essay_tfidf matrix after \",X_cv_requirements_tfidf.shape)\n# print(\"Shape of X_test_essay_tfidf matrix after \",X_test_requirements_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### title"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.feature_extraction.text import TfidfVectorizer\n# vectorizer = TfidfVectorizer(min_df=10)\n# X_train_title_tfidf = vectorizer.fit_transform(X_train['preprocessed_title'])\n# X_cv_title_tfidf = vectorizer.transform(X_cv['preprocessed_title'])\n# X_test_title_tfidf = vectorizer.transform(X_test['preprocessed_title'])\n# print(\"Shape of X_train_essay_tfidf matrix after \",X_train_title_tfidf.shape)\n# print(\"Shape of X_cv_essay_tfidf matrix after \",X_cv_title_tfidf.shape)\n# print(\"Shape of X_test_essay_tfidf matrix after \",X_test_title_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Company profile"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.feature_extraction.text import TfidfVectorizer\n# vectorizer = TfidfVectorizer(min_df=10)\n# X_train_profile_tfidf = vectorizer.fit_transform(X_train['preprocessed_company_profile'])\n# X_cv_profile_tfidf = vectorizer.transform(X_cv['preprocessed_company_profile'])\n# X_test_profile_tfidf = vectorizer.transform(X_test['preprocessed_company_profile'])\n# print(\"Shape of X_train_essay_tfidf matrix after \",X_train_profile_tfidf.shape)\n# print(\"Shape of X_cv_essay_tfidf matrix after \",X_cv_profile_tfidf.shape)\n# print(\"Shape of X_test_essay_tfidf matrix after \",X_test_profile_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # merge two sparse matrices: https://stackoverflow.com/a/19710648/4084039\n\n# # with the same hstack function we are concatinating a sparse matrix and a dense matirx :)\n# # X = hstack((categories_one_hot, sub_categories_one_hot, text_bow, price_standardized))\n# X_tr = hstack((X_train_employment_type_one_hot,X_train_required_experience_one_hot,X_train_required_education_one_hot,X_train_industry_one_hot,\n#               X_train_function_one_hot,X_train_country_one_hot,X_train_state_one_hot,X_train_description_tfidf,\n#               X_train_denifits_tfidf,X_train_requirements_tfidf,X_train_title_tfidf,X_train_profile_tfidf,\n#               X_train['telecommuting'].values.reshape(-1,1),X_train['has_company_logo'].values.reshape(-1,1),\n#                X_train['has_questions'].values.reshape(-1,1))).tocsr()\n\n# X_cr = hstack((X_cv_employment_type_one_hot,X_cv_required_experience_one_hot,X_cv_required_education_one_hot,\n#               X_cv_industry_one_hot,X_cv_function_one_hot,X_cv_country_one_hot,X_cv_state_one_hot,\n#               X_cv_description_tfidf,X_cv_denifits_tfidf,X_cv_requirements_tfidf,X_cv_title_tfidf,\n#               X_cv_profile_tfidf,X_cv['telecommuting'].values.reshape(-1,1),\n#               X_cv['has_company_logo'].values.reshape(-1,1),X_cv['has_questions'].values.reshape(-1,1))).tocsr()\n\n# X_te = hstack((X_test_employment_type_one_hot,X_test_required_experience_one_hot,X_test_required_education_one_hot,\n#               X_test_industry_one_hot,X_test_function_one_hot,X_test_country_one_hot,X_test_state_one_hot,\n#               X_test_description_tfidf,X_test_denifits_tfidf,X_test_requirements_tfidf,X_test_title_tfidf,\n#               X_test_profile_tfidf,X_test['telecommuting'].values.reshape(-1,1),\n#               X_test['has_company_logo'].values.reshape(-1,1),X_test['has_questions'].values.reshape(-1,1))).tocsr()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #For memory issue batch wise prediction\n# def batch_predict(clf, data):\n#     y_data_pred = []\n#     tr_loop = data.shape[0] - data.shape[0]%1000\n#     for i in range(0, tr_loop, 1000):\n#         y_data_pred.extend(clf.predict(data[i:i+1000])[:,1])\n#     if data.shape[0]%1000 !=0:\n#         y_data_pred.extend(clf.predict(data[tr_loop:])[:,1])\n#     return y_data_pred\n\n# def find_best_threshold(threshould, fpr, tpr):\n#     t = threshould[np.argmax(tpr*(1-fpr))]\n#     # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n#     print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n#     return t\n\n# def predict_with_best_t(proba, threshould):\n#     predictions = []\n#     for i in proba:\n#         if i>=threshould:\n#             predictions.append(1)\n#         else:\n#             predictions.append(0)\n#     return predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Please write all the code with proper documentation\n# #selecting the hyperparameter using RandomSearch\n# from sklearn.metrics import f1_score, make_scorer\n# from sklearn.model_selection import GridSearchCV\n# from sklearn.metrics import roc_curve\n# from xgboost import XGBClassifier\n# from sklearn.linear_model import SGDClassifier\n\n# # from sklearn.model_selection import GridSearchCV\n\n# log = SGDClassifier(loss = 'log', class_weight= 'balanced')\n# # parameters = {'alpha':sp_randint(50, 100)}\n# parameters = {'alpha':[10**x for x in range(-4,4)]}\n# clf = GridSearchCV(log, parameters, cv=3, scoring=make_scorer(f1_score),return_train_score=True)\n# clf.fit(X_tr, y_train)\n\n# results = pd.DataFrame.from_dict(clf.cv_results_)\n# # results3 = results3.sort_values(['param_alpha'])\n\n# train_f1= results['mean_train_score']\n# train_f1_std= results['std_train_score']\n# cv_f1 = results['mean_test_score'] \n# cv_f1_std= results['std_test_score']\n# # alpha3 =  results3['param_alpha']\n# # alpha3 = alpha3.astype(np.int64) # https://stackoverflow.com/questions/46995041/why-does-this-array-has-no-attribute-log10?rq=1\n# alpha = np.log10(parameters['alpha'])\n\n# plt.plot(alpha, train_f1, label='Train F1')\n# # this code is copied from here: https://stackoverflow.com/a/48803361/4084039\n# # plt.gca().fill_between(K, train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.2,color='darkblue')\n\n# plt.plot(alpha, cv_f1, label='CV F1')\n# # this code is copied from here: https://stackoverflow.com/a/48803361/4084039\n# # plt.gca().fill_between(K, cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.2,color='darkorange')\n\n# plt.scatter(alpha, train_f1, label='Train F1 points')\n# plt.scatter(alpha, cv_f1, label='CV F1 points')\n\n\n\n# plt.legend()\n# plt.xlabel(\"alpha: hyperparameter\")\n# plt.ylabel(\"f1\")\n# plt.title(\"Hyper parameter Vs f1 score plot\")\n# plt.grid()\n# plt.show()\n\n# results.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Please write all the code with proper documentation\n# best_alpha = 0.001\n\n\n# log_reg = SGDClassifier(loss = 'log',alpha=best_alpha, class_weight= 'balanced')\n# log_reg.fit(X_tr, y_train)\n# # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# # not the predicted outputs\n\n# y_train_pred = log_reg.predict(X_tr) \n# y_cv_pred = log_reg.predict(X_cr)    \n# y_test_pred = log_reg.predict(X_te)\n\n# # train_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\n# # test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\n# # plt.plot(train_fpr_3, train_tpr_3, label=\"train AUC =\"+str(auc(train_fpr_3, train_tpr_3)))\n# # plt.plot(test_fpr_3, test_tpr_3, label=\"test AUC =\"+str(auc(test_fpr_3, test_tpr_3)))\n# # plt.legend()\n# # plt.xlabel(\"alpha_3: hyperparameter\")\n# # plt.ylabel(\"AUC\")\n# # plt.title(\"ERROR PLOTS\")\n# # plt.grid()\n# # plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_train_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n\n# print(\"Train confusion matrix\")\n\n# f1_score_ = f1_score(y_train,y_train_pred)\n# print(\"F1_score\",f1_score_)\n# cm_train_set = confusion_matrix(y_train, y_train_pred)\n# print(cm_train_set)\n# df_cm = pd.DataFrame(cm_train_set, columns=np.unique(y_test), index = np.unique(y_test))\n# df_cm.index.name = 'Actual'\n# df_cm.columns.name = 'Predicted'\n# plt.figure(figsize = (10,7))\n# sns.set(font_scale=1.4)#for label size\n# sns.heatmap(df_cm,fmt='d', cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})# font size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n\n# print(\"cv confusion matrix\")\n# f1_score_ = f1_score(y_cv,y_cv_pred)\n# print(\"F1_score\",f1_score_)\n# cm_cv_set = confusion_matrix(y_cv,y_cv_pred)\n# print(cm_cv_set)\n# df_cm = pd.DataFrame(cm_cv_set, columns=np.unique(y_cv), index = np.unique(y_cv))\n# df_cm.index.name = 'Actual'\n# df_cm.columns.name = 'Predicted'\n# plt.figure(figsize = (10,7))\n# sns.set(font_scale=1.4)#for label size\n# sns.heatmap(df_cm,fmt='d', cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})# font size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n\n# print(\"Test confusion matrix\")\n# f1_score_ = f1_score(y_test,y_test_pred)\n# print(\"F1_score\",f1_score_)\n# cm_test_set = confusion_matrix(y_test,y_test_pred)\n# print(cm_test_set)\n# df_cm = pd.DataFrame(cm_test_set, columns=np.unique(y_test), index = np.unique(y_test))\n# df_cm.index.name = 'Actual'\n# df_cm.columns.name = 'Predicted'\n# plt.figure(figsize = (10,7))\n# sns.set(font_scale=1.4)#for label size\n# sns.heatmap(df_cm,fmt='d', cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})# font size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBOOST Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# #selecting the hyperparameter using GridSearch\n# # https://www.kaggle.com/arindambanerjee/grid-search-simplified\n# from sklearn.metrics import f1_score, make_scorer\n# from sklearn.model_selection import GridSearchCV\n\n# xg_clf = XGBClassifier()\n# parameters = {'n_estimators':[5, 10, 50, 100, 200],'max_depth':[2,3, 4, 5, 6, 7]}\n# xg_clf_ = GridSearchCV(xg_clf, parameters, n_jobs= -1, verbose=10, cv=2, scoring=make_scorer(f1_score),return_train_score=True)\n# xg_clf_.fit(X_tr, y_train)\n\n# results2 = pd.DataFrame.from_dict(xg_clf_.cv_results_)\n# # results4 = results4.sort_values(['param_alpha'])\n# max_depth_list = list(xg_clf_.cv_results_['param_max_depth'].data)\n# n_estimator_list = list(xg_clf_.cv_results_['param_n_estimators'].data)\n\n\n# sns.set_style(\"whitegrid\")\n# plt.figure(figsize=(16,6))\n# plt.subplot(1,2,1)\n\n# data = pd.DataFrame(data={'Number of Estimator':n_estimator_list, 'Max Depth':max_depth_list, 'f1_score':xg_clf_.cv_results_['mean_train_score']})\n# data = data.reset_index().pivot_table(index='Max Depth', columns='Number of Estimator', values='f1_score')\n# sns.heatmap(data, annot=True, cmap=\"YlGnBu\").set_title('AUC for Training data')\n# plt.subplot(1,2,2)\n\n# # Testing Heatmap\n# data = pd.DataFrame(data={'Number of Estimator':n_estimator_list, 'Max Depth':max_depth_list, 'f1_score':xg_clf_.cv_results_['mean_test_score']})\n# data = data.reset_index().pivot_table(index='Max Depth', columns='Number of Estimator', values='f1_score')\n# sns.heatmap(data, annot=True, cmap=\"YlGnBu\").set_title('f1_score for Test data')\n# plt.show()\n\n# results2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Please write all the code with proper documentation\n# n_estimators = 50\n# max_depth = 6\n\n\n# xg_clf = XGBClassifier(max_depth = max_depth, n_estimators = n_estimators , n_jobs=1)\n# xg_clf.fit(X_tr, y_train)\n# # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# # not the predicted outputs\n\n# y_train_pred = xg_clf.predict(X_tr)    \n# y_cv_pred = xg_clf.predict(X_cr)    \n# y_test_pred = xg_clf.predict(X_te)\n\n# # train_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\n# # test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n\n# print(\"Train confusion matrix\")\n# f1_score_ = f1_score(y_train,y_train_pred)\n# print(\"F1_score\",f1_score_)\n# cm_train_set = confusion_matrix(y_train, y_train_pred)\n# print(cm_train_set)\n# df_cm = pd.DataFrame(cm_train_set)\n# df_cm.index.name = 'Actual'\n# df_cm.columns.name = 'Predicted'\n# plt.figure(figsize = (10,7))\n# sns.set(font_scale=1.4)#for label size\n# sns.heatmap(df_cm,fmt='d', cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})# font size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n\n# print(\"cv confusion matrix\")\n# f1_score_ = f1_score(y_cv,y_cv_pred)\n# print(\"F1_score\",f1_score_)\n# cm_train_set = confusion_matrix(y_cv, y_cv_pred)\n# print(cm_train_set)\n# df_cm = pd.DataFrame(cm_train_set)\n# df_cm.index.name = 'Actual'\n# df_cm.columns.name = 'Predicted'\n# plt.figure(figsize = (10,7))\n# sns.set(font_scale=1.4)#for label size\n# sns.heatmap(df_cm,fmt='d', cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})# font size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n\n# print(\"Test confusion matrix\")\n# f1_score_1 = f1_score(y_test,y_test_pred)\n# print(\"F1_score\",f1_score_1)\n# cm_test_set = confusion_matrix(y_test, y_test_pred)\n# print(cm_test_set)\n# df_cm = pd.DataFrame(cm_test_set)\n# df_cm.index.name = 'Actual'\n# df_cm.columns.name = 'Predicted'\n# plt.figure(figsize = (10,7))\n# sns.set(font_scale=1.4)#for label size\n# sns.heatmap(df_cm,fmt='d', cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})# font size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets try balancing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# balancig the Imbalanced dataset https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18\nfrom sklearn.model_selection import train_test_split\nX_train_, X_test_, y_train_, y_test_ = train_test_split(X, y, test_size=0.33, stratify=y)\n# X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.33, stratify=y_train)\n\nprint(X_train_.shape)\nprint(y_train_.shape)\n# print(X_cv.shape)\n# print(y_cv.shape)\nprint(X_test_.shape)\nprint(y_test_.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Onehot encoding "},{"metadata":{},"cell_type":"markdown","source":"### Eemployment type"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(vocabulary=set(processed_job_data['employment_type']),lowercase=False, binary=True)\nX_train_employment_type_one_hot = vectorizer.fit_transform(X_train_['employment_type'].values)\n# X_cv_employment_type_one_hot = vectorizer.transform(X_cv['employment_type'].values)\nX_test_employment_type_one_hot = vectorizer.transform(X_test_['employment_type'].values)\n\nprint(vectorizer.get_feature_names())\nprint(\"Shape of X_train matrix after one hot encodig \",X_train_employment_type_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_employment_type_one_hot.shape)\nprint(\"Shape of X_test matrix after one hot encodig \",X_test_employment_type_one_hot.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### required_experience"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(vocabulary=set(processed_job_data['required_experience']),lowercase=False, binary=True)\nX_train_required_experience_one_hot = vectorizer.fit_transform(X_train_['required_experience'].values)\n# X_cv_required_experience_one_hot = vectorizer.transform(X_cv['required_experience'].values)\nX_test_required_experience_one_hot = vectorizer.transform(X_test_['required_experience'].values)\n\nprint(vectorizer.get_feature_names())\nprint(\"Shape of X_train matrix after one hot encodig \",X_train_required_experience_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_required_experience_one_hot.shape)\nprint(\"Shape of X_test matrix after one hot encodig \",X_test_required_experience_one_hot.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### required_education"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(vocabulary=set(processed_job_data['required_education']),lowercase=False, binary=True)\nX_train_required_education_one_hot = vectorizer.fit_transform(X_train_['required_education'].values)\n# X_cv_required_education_one_hot = vectorizer.transform(X_cv['required_education'].values)\nX_test_required_education_one_hot = vectorizer.transform(X_test_['required_education'].values)\n\nprint(vectorizer.get_feature_names())\nprint(\"Shape of X_train matrix after one hot encodig \",X_train_required_education_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_required_education_one_hot.shape)\nprint(\"Shape of X_test matrix after one hot encodig \",X_test_required_education_one_hot.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### industry"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(vocabulary=set(processed_job_data['industry']),lowercase=False, binary=True)\nX_train_industry_one_hot = vectorizer.fit_transform(X_train_['industry'].values)\n# X_cv_industry_one_hot = vectorizer.transform(X_cv['industry'].values)\nX_test_industry_one_hot = vectorizer.transform(X_test_['industry'].values)\n\nprint(vectorizer.get_feature_names())\nprint(\"Shape of X_train matrix after one hot encodig \",X_train_industry_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_industry_one_hot.shape)\nprint(\"Shape of X_test matrix after one hot encodig \",X_test_industry_one_hot.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### function"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(vocabulary=set(processed_job_data['function']),lowercase=False, binary=True)\nX_train_function_one_hot = vectorizer.fit_transform(X_train_['function'].values)\n# X_cv_function_one_hot = vectorizer.transform(X_cv['function'].values)\nX_test_function_one_hot = vectorizer.transform(X_test_['function'].values)\n\nprint(vectorizer.get_feature_names())\nprint(\"Shape of X_train matrix after one hot encodig \",X_train_function_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_function_one_hot.shape)\nprint(\"Shape of X_test matrix after one hot encodig \",X_test_function_one_hot.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### country"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(vocabulary=set(processed_job_data['country']),lowercase=False, binary=True)\nX_train_country_one_hot = vectorizer.fit_transform(X_train_['country'].values)\n# X_cv_country_one_hot = vectorizer.transform(X_cv['country'].values)\nX_test_country_one_hot = vectorizer.transform(X_test_['country'].values)\n\nprint(vectorizer.get_feature_names())\nprint(\"Shape of X_train matrix after one hot encodig \",X_train_country_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_country_one_hot.shape)\nprint(\"Shape of X_test matrix after one hot encodig \",X_test_country_one_hot.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### state"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(vocabulary=set(processed_job_data['state']),lowercase=False, binary=True)\nX_train_state_one_hot = vectorizer.fit_transform(X_train_['state'].values)\n# X_cv_state_one_hot = vectorizer.transform(X_cv['state'].values)\nX_test_state_one_hot = vectorizer.transform(X_test_['state'].values)\n\nprint(vectorizer.get_feature_names())\nprint(\"Shape of X_train matrix after one hot encodig \",X_train_state_one_hot.shape)\n# print(\"Shape of X_cv matrix after one hot encodig \",X_cv_state_one_hot.shape)\nprint(\"Shape of X_test matrix after one hot encodig \",X_test_state_one_hot.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vectorizing the processed text data"},{"metadata":{},"cell_type":"markdown","source":"### preprocessed_description"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10)\nX_train_description_tfidf = vectorizer.fit_transform(X_train_['preprocessed_description'])\n# X_cv_description_tfidf = vectorizer.transform(X_cv['preprocessed_description'])\nX_test_description_tfidf = vectorizer.transform(X_test_['preprocessed_description'])\nprint(\"Shape of X_train_essay_tfidf matrix after \",X_train_description_tfidf.shape)\n# print(\"Shape of X_cv_essay_tfidf matrix after \",X_cv_description_tfidf.shape)\nprint(\"Shape of X_test_essay_tfidf matrix after \",X_test_description_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Benifits"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10)\nX_train_denifits_tfidf = vectorizer.fit_transform(X_train_['preprocessed_benefits'])\n# X_cv_denifits_tfidf = vectorizer.transform(X_cv['preprocessed_benefits'])\nX_test_denifits_tfidf = vectorizer.transform(X_test_['preprocessed_benefits'])\nprint(\"Shape of X_train_essay_tfidf matrix after \",X_train_denifits_tfidf.shape)\n# print(\"Shape of X_cv_essay_tfidf matrix after \",X_cv_denifits_tfidf.shape)\nprint(\"Shape of X_test_essay_tfidf matrix after \",X_test_denifits_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Requirements"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10)\nX_train_requirements_tfidf = vectorizer.fit_transform(X_train_['preprocessed_requirements'])\n# X_cv_requirements_tfidf = vectorizer.transform(X_cv['preprocessed_requirements'])\nX_test_requirements_tfidf = vectorizer.transform(X_test_['preprocessed_requirements'])\nprint(\"Shape of X_train_essay_tfidf matrix after \",X_train_requirements_tfidf.shape)\n# print(\"Shape of X_cv_essay_tfidf matrix after \",X_cv_requirements_tfidf.shape)\nprint(\"Shape of X_test_essay_tfidf matrix after \",X_test_requirements_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Title"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10)\nX_train_title_tfidf = vectorizer.fit_transform(X_train_['preprocessed_title'])\n# X_cv_title_tfidf = vectorizer.transform(X_cv['preprocessed_title'])\nX_test_title_tfidf = vectorizer.transform(X_test_['preprocessed_title'])\nprint(\"Shape of X_train_essay_tfidf matrix after \",X_train_title_tfidf.shape)\n# print(\"Shape of X_cv_essay_tfidf matrix after \",X_cv_title_tfidf.shape)\nprint(\"Shape of X_test_essay_tfidf matrix after \",X_test_title_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### company profile"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10)\nX_train_profile_tfidf = vectorizer.fit_transform(X_train_['preprocessed_company_profile'])\n# X_cv_profile_tfidf = vectorizer.transform(X_cv['preprocessed_company_profile'])\nX_test_profile_tfidf = vectorizer.transform(X_test_['preprocessed_company_profile'])\nprint(\"Shape of X_train_essay_tfidf matrix after \",X_train_profile_tfidf.shape)\n# print(\"Shape of X_cv_essay_tfidf matrix after \",X_cv_profile_tfidf.shape)\nprint(\"Shape of X_test_essay_tfidf matrix after \",X_test_profile_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge two sparse matrices: https://stackoverflow.com/a/19710648/4084039\n\n# with the same hstack function we are concatinating a sparse matrix and a dense matirx :)\n# X = hstack((categories_one_hot, sub_categories_one_hot, text_bow, price_standardized))\nX_tr = hstack((X_train_employment_type_one_hot,X_train_required_experience_one_hot,X_train_required_education_one_hot,X_train_industry_one_hot,\n              X_train_function_one_hot,X_train_country_one_hot,X_train_state_one_hot,X_train_description_tfidf,\n              X_train_denifits_tfidf,X_train_requirements_tfidf,X_train_title_tfidf,X_train_profile_tfidf,\n              X_train_['telecommuting'].values.reshape(-1,1),X_train_['has_company_logo'].values.reshape(-1,1),\n               X_train_['has_questions'].values.reshape(-1,1))).tocsr()\n\nX_te = hstack((X_test_employment_type_one_hot,X_test_required_experience_one_hot,X_test_required_education_one_hot,\n              X_test_industry_one_hot,X_test_function_one_hot,X_test_country_one_hot,X_test_state_one_hot,\n              X_test_description_tfidf,X_test_denifits_tfidf,X_test_requirements_tfidf,X_test_title_tfidf,\n              X_test_profile_tfidf,X_test_['telecommuting'].values.reshape(-1,1),\n              X_test_['has_company_logo'].values.reshape(-1,1),X_test_['has_questions'].values.reshape(-1,1))).tocsr()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_upsample, y_train_upsample = SMOTE(random_state=42).fit_sample(X_tr, y_train_)\ny_train_upsample.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nprint(Counter(y_train_upsample))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=10, random_state=42, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'alpha':[0.0001,0.001,0.01,0.1,1]}\ndef score_model(model, params, cv=None):\n    \"\"\"\n    Creates folds manually, and upsamples within each fold.\n    Returns an array of validation (recall) scores\n    \"\"\"\n    if cv is None:\n        cv = KFold(n_splits=5, random_state=42)\n\n    smoter = SMOTE(random_state=42)\n    \n    scores = []\n\n    for train_fold_index, val_fold_index in cv.split(X_tr, y_train_):\n        # Get the training data\n        X_train_fold, y_train_fold = X_tr[train_fold_index], y_train_[train_fold_index]\n        # Get the validation data\n        X_val_fold, y_val_fold = X_tr[val_fold_index], y_train_[val_fold_index]\n\n        # Upsample only the data in the training section\n        X_train_fold_upsample, y_train_fold_upsample = smoter.fit_resample(X_train_fold,\n                                                                           y_train_fold)\n        # Fit the model on the upsampled training data\n        model_obj = model(**params).fit(X_train_fold_upsample, y_train_fold_upsample)\n        # Score the model on the (non-upsampled) validation data\n        score = f1_score(y_val_fold, model_obj.predict(X_val_fold))\n        scores.append(score)\n    return np.array(scores)\n\n\n# Example of the model in action\n# score_model(RandomForestClassifier, example_params, cv=kf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_tracker = []\nfor alpha in params['alpha']:\n        example_params = {\n            'alpha': alpha,\n            'random_state': 13,\n            'loss': 'log',\n        }\n        example_params['f1_score'] = score_model(SGDClassifier, \n                                               example_params, cv=kf).mean()\n        score_tracker.append(example_params)\n     \n# What's the best model?\n# print(score_tracker)\nsorted(score_tracker, key=lambda x: x['f1_score'], reverse=True)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clf = SGDClassifier(alpha=0.0001,loss='log',random_state=13)\n# clf.fit(X_train_upsample, y_train_upsample)\nprint(\"Train f1 score\",f1_score(y_train_,clf.predict(X_tr))),(\"Test f1 score: \",f1_score(y_test_,clf.predict(X_te)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'n_estimators':[5, 10, 50, 100, 200],'max_depth':[2,3, 4, 5, 6, 7]}\ndef score_model(model, params, cv=None):\n    \"\"\"\n    Creates folds manually, and upsamples within each fold.\n    Returns an array of validation (recall) scores\n    \"\"\"\n    if cv is None:\n        cv = KFold(n_splits=5, random_state=42)\n\n    smoter = SMOTE(random_state=42)\n    \n    scores = []\n\n    for train_fold_index, val_fold_index in cv.split(X_tr, y_train_):\n        # Get the training data\n        X_train_fold, y_train_fold = X_tr[train_fold_index], y_train_[train_fold_index]\n#         print('X_train_fold',X_train_fold.shape)\n        # Get the validation data\n        X_val_fold, y_val_fold = X_tr[val_fold_index], y_train_[val_fold_index]\n#         print('X_val_fold',X_val_fold.shape)\n\n        # Upsample only the data in the training section\n        X_train_fold_upsample, y_train_fold_upsample = smoter.fit_resample(X_train_fold,\n                                                                           y_train_fold)\n#         print(\"X_train_fold_upsample\",X_train_fold_upsample.shape)\n        # Fit the model on the upsampled training data\n#         print(model)\n#         clf = model()\n#         print(params)\n        model_obj = model(**params).fit(X_train_fold_upsample, y_train_fold_upsample)\n        # Score the model on the (non-upsampled) validation data\n#         print('X_val_fold',X_val_fold.shape)\n#         print('y_val_fold',y_val_fold.shape)\n        score = f1_score(y_val_fold, model_obj.predict(X_val_fold))\n        scores.append(score)\n    return np.array(scores)\n\n# Example of the model in action\n# score_model(RandomForestClassifier, example_params, cv=kf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# score_tracker = []\n# for n_estimator in params['n_estimators']:\n#     for max_depth in params['max_depth']:\n#         example_params = {\n#             'n_estimators': n_estimator,\n#             'max_depth': max_depth,\n#             'random_state': 13,\n            \n#         }\n#         example_params['f1_score'] = score_model(XGBClassifier, \n#                                                example_params, cv=kf).mean()\n#         score_tracker.append(example_params)\n     \n# # What's the best model?\n# print(score_tracker)\n# sorted(score_tracker, key=lambda x: x['f1_score'], reverse=True)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split, KFold\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=False)\n# from sklearn.neighbors import KNeighborsClassifier\nimba_pipeline = Pipeline([('smote',SMOTE(random_state=42)),('classifier',SGDClassifier())])\n\nimba_pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imba_pipeline.get_params().keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_train_upsample,clf.predict(X_train_upsample)\n# cross_val_score(imba_pipeline, X_tr, y_train_, scoring=make_scorer(f1_score), cv=kf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'alpha': [0.0001,0.001,0.01,0.1,1],\n    'loss':['log'],\n    'penalty':['l1','l2'],\n    'random_state': [13]\n}\nnew_params = {'classifier__' + key: params[key] for key in params}\ngrid_imba = GridSearchCV(imba_pipeline, param_grid=new_params, cv=kf, scoring=make_scorer(f1_score),\n                        return_train_score=True)\n\ngrid_imba.fit(X_tr, y_train_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# estimator.get_params().keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_imba.cv_results_['mean_test_score'], grid_imba.cv_results_['mean_train_score']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#selecting the hyperparameter using GridSearch\n# https://www.kaggle.com/arindambanerjee/grid-search-simplified\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\n\nresults = pd.DataFrame.from_dict(grid_imba.cv_results_)\n\n# results4 = results4.sort_values(['param_alpha'])\nalpha = list(grid_imba.cv_results_['param_classifier__alpha'].data)\n# loss = list(grid_imba.cv_results_['param_classifier__loss'].data)\n\ntrain_f1= results['mean_train_score']\n# train_auc_std= results['std_train_score']\ncv_f1 = results['mean_test_score'] \n# cv_auc_std= results['std_test_score']\n# K =  results['param_classifier__alpha']\nalpha = np.log10(alpha)\nprint(params['alpha'])\nprint(alpha_)\nprint(train_f1)\n\nplt.plot(alpha, train_f1, label='Train f1 score')\n# this code is copied from here: https://stackoverflow.com/a/48803361/4084039\n# plt.gca().fill_between(K, train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.2,color='darkblue')\n\nplt.plot(alpha, cv_f1, label='CV f1 score')\n# this code is copied from here: https://stackoverflow.com/a/48803361/4084039\n# plt.gca().fill_between(K, cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.2,color='darkorange')\n\nplt.scatter(alpha, train_f1, label='Train f1 score')\nplt.scatter(alpha, cv_f1, label='CV f1 score')\n\n\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"F1 SCORE\")\nplt.title(\"Hyper parameter Vs F1 plot\")\nplt.grid()\nplt.show()\n\n\nsns.set_style(\"whitegrid\")\n# plt.figure(figsize=(16,6))\n# plt.subplot(1,2,1)\n\n# data = pd.DataFrame(data={'alpha':alpha, 'loss':loss, 'f1_score':grid_imba.cv_results_['mean_train_score']})\n\n# data = data.reset_index().pivot_table(index='alpha', columns='loss', values='f1_score')\n# sns.heatmap(data, annot=True, cmap=\"YlGnBu\").set_title('f1_score for Training data')\n# plt.subplot(1,2,2)\n\n# # Testing Heatmap\n# data = pd.DataFrame(data={'alpha':alpha, 'loss':loss, 'f1_score':grid_imba.cv_results_['mean_test_score']})\n# data = data.reset_index().pivot_table(index='alpha', columns='loss', values='f1_score')\n# sns.heatmap(data, annot=True, cmap=\"YlGnBu\").set_title('f1_score for Test data')\n# plt.show()\n\nresults.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_imba.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_imba.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = SGDClassifier(alpha=0.001,loss='log',random_state=13)\nclf.fit(X_train_upsample, y_train_upsample)\nprint(\"Train f1 score\",f1_score(y_train_upsample,clf.predict(X_train_upsample))),(\"Test f1 score: \",f1_score(y_test_,clf.predict(X_te)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_predict = grid_imba.best_estimator_.predict(X_te)\ny_train_predict = grid_imba.best_estimator_.predict(X_tr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_test_, y_test_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_train_, y_train_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_train_upsample,clf.predict(X_train_upsample))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=False)\nfrom sklearn.neighbors import KNeighborsClassifier\nimba_pipeline = Pipeline([('smote',SMOTE(random_state=42)),('xgbclassifier',XGBClassifier())])\n\nimba_pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'n_estimators':[5, 10, 50, 100, 200],\n    'max_depth':[2,3, 4, 5, 6, 7]\n}\nnew_params = {'xgbclassifier__' + key: params[key] for key in params}\ngrid_imba = GridSearchCV(imba_pipeline, param_grid=new_params, cv=kf, scoring=make_scorer(f1_score),\n                        return_train_score=True)\n\ngrid_imba.fit(X_tr, y_train_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_imba.cv_results_['mean_test_score'], grid_imba.cv_results_['mean_train_score']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#selecting the hyperparameter using GridSearch\n# https://www.kaggle.com/arindambanerjee/grid-search-simplified\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV\n\nresults2 = pd.DataFrame.from_dict(grid_imba.cv_results_)\n\n# results4 = results4.sort_values(['param_alpha'])\nmax_depth_list = list(grid_imba.cv_results_['param_xgbclassifier__max_depth'].data)\nn_estimator_list = list(grid_imba.cv_results_['param_xgbclassifier__n_estimators'].data)\n\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,6))\nplt.subplot(1,2,1)\n\ndata = pd.DataFrame(data={'Number of Estimator':n_estimator_list, 'Max Depth':max_depth_list, 'f1_score':grid_imba.cv_results_['mean_train_score']})\ndata = data.reset_index().pivot_table(index='Max Depth', columns='Number of Estimator', values='f1_score')\nsns.heatmap(data, annot=True, cmap=\"YlGnBu\").set_title('f1_score for Training data')\nplt.subplot(1,2,2)\n\n# Testing Heatmap\ndata = pd.DataFrame(data={'Number of Estimator':n_estimator_list, 'Max Depth':max_depth_list, 'f1_score':grid_imba.cv_results_['mean_test_score']})\ndata = data.reset_index().pivot_table(index='Max Depth', columns='Number of Estimator', values='f1_score')\nsns.heatmap(data, annot=True, cmap=\"YlGnBu\").set_title('f1_score for Test data')\nplt.show()\n\nresults2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_imba.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_imba.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_predict = grid_imba.best_estimator_.predict(X_te)\ny_train_predict = grid_imba.best_estimator_.predict(X_tr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_test_, y_test_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_train_, y_train_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_train_upsample,clf.predict(X_train_upsample))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import recall_score, roc_auc_score\nprint(roc_auc_score(y_train_,grid_imba.predict_proba(X_tr)[:,1]))\n# print ('Cross validation AUC for Random Forest model : ',np.mean(cross_val_score(grid_imba,X_tr,y_train_,scoring='roc_auc',cv=10)))\nprint(roc_auc_score(y_test_,grid_imba.predict_proba(X_te)[:,1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### lightgbm"},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=False)\n# from sklearn.neighbors import KNeighborsClassifier\nimport lightgbm as lgb\nimba_pipeline = Pipeline([('smote',SMOTE(random_state=42)),('lgbclassifier',lgb.LGBMClassifier())])\n\nimba_pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves':[6,8,12,16],'learning_rate':[0.01,0.03,0.05,0.1,0.15,0.2],\n          'n_estimators':[5, 10, 50, 100, 200],'max_depth':[3,5,10]}\n\n# params = {\n#     'n_estimators':[5, 10, 50, 100, 200],\n#     'max_depth':[2,3, 4, 5, 6, 7]\n# }\nnew_params = {'lgbclassifier__' + key: params[key] for key in params}\ngrid_imba = GridSearchCV(imba_pipeline, param_grid=new_params, cv=kf, scoring=make_scorer(f1_score),\n                        return_train_score=True)\n\ngrid_imba.fit(X_tr, y_train_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}