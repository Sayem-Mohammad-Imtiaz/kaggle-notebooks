{"cells":[{"metadata":{},"cell_type":"markdown","source":"**The purpose of this notebook is to create a DATASET that includes**\n\n** Characteristics of patients like - **\n\n*age\n\n*sex\n\n*Country\n\nand so\n\n\n\n\n**The condition of the patients and their characteristics - **\n\n* Disease time (from diagnosis date)\n\n* Have been cured\n \n* Deaths\n\n\nThe database is designed to allow easy exploration of the data\n\nAnyone interested can use and donate"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport  numpy as np\nfrom ds_exam import *","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"france = pd.read_csv(\"/kaggle/input/coronavirus-france-dataset/patient.csv\")\ntunisia = pd.read_csv(\"../input/coronavirus-tunisia/Coronavirus_Tunisia.csv\")\njapan = pd.read_csv(\"/kaggle/input/close-contact-status-of-corona-in-japan/COVID-19_Japan_Mar_07th_2020.csv\")\nindonesia = pd.read_csv(\"/kaggle/input/indonesia-coronavirus-cases/patient.csv\")\nkorea = pd.read_csv(\"/kaggle/input/coronavirusdataset/PatientInfo.csv\")\nHubei = pd.read_csv(\"/kaggle/input/covid19official/Hubei.csv\")\noutside_Hubei = pd.read_csv(\"/kaggle/input/covid19official/outside_Hubei.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\ndatasets_name = [\"france\", \"tunisia\", \"japan\", \"indonesia\", \"korea\", \"Hubei\", \"outside_Hubei\"]\n\ngarbge = [print(\"\\n\"+datasets_name[i], [i for i in datasets[i].columns]) for i in range(len(datasets_name))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"change name of col"},{"metadata":{"trusted":true},"cell_type":"code","source":"france.rename(columns={\"health\":\"severity_illness\",\"status\":\"treatment\",\"infection_reason\":\"infection_place\"}\n              , inplace = True)\n\ntunisia.rename(columns={\"date\":\"confirmed_date\", \"gender\":\"sex\", \"situation\":\"severity_illness\", \n                        \"return_from\":\"infection_place\", \"health\":\"background_diseases\"}, inplace = True)\n\njapan.rename(columns={\"No.\":\"id\", \"Fixed date\":\"confirmed_date\",\"Age\":\"age\", \"residence\":\"region\",\n                      \"Surrounding patients *\":\"infected_by\"}, inplace = True)\n\nindonesia.rename(columns={\"patient_id\":\"id\",\"gender\": \"sex\", \"province\":\"region\", \"hospital\":\"hospital_name\",\n                          \"contacted_with\":\"infected_by\", \"current_state\":\"status\"}, inplace = True)\n\nkorea.rename(columns={\"patient_id\":\"id\", \"disease\":\"background_diseases_binary\", \"state\":\"severity_illness\",\n                      \"province\":\"region\", \"infection_case\" :\"infection_place\",\n                      \"symptom_onset_date\":\"date_onset_symptoms\"}, inplace = True)\n\nHubei = Hubei.rename(columns={ \"province\":\"region\",\"date_confirmation\": \"confirmed_date\",\n                              \"chronic_disease_binary\":\"background_diseases_binary\", \n                              \"chronic_disease\":\"background_diseases\", \"outcome\":\"severity_illness\"})\n\noutside_Hubei = outside_Hubei.rename(columns={ \"province\":\"region\", \"date_confirmation\": \"confirmed_date\",\n                              \"chronic_disease_binary\":\"background_diseases_binary\", \n                              \"chronic_disease\":\"background_diseases\", \"outcome\":\"severity_illness\" })\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# datasets.shape"},{"metadata":{"trusted":true},"cell_type":"code","source":"o = []\nfor i in range(len(datasets)):\n    print(datasets_name[i],datasets[i].shape)\n    o.append(datasets[i].shape[0])\nprint(\"\\nnum of i \" + str(sum(o)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# exam_df = columns vs dfs"},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\ncolumns_name = Exam.build_columns_name_ls(datasets)\nexam_df = Exam.df_exam_columns_dfs(datasets,datasets_name,columns_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def full_common(exam_df):\n    \"\"\"\n    Returns columns that all DATASETS have\n    \"\"\"\n    full_common = []\n    for j in exam_df.columns:\n        boolyan = exam_df[j].all()\n        if boolyan == True:\n            full_common.append(j)\n    return full_common","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"common = []\nunique = []\nblank = []\nfor i in exam_df.columns:\n    if exam_df[i].value_counts()[True]>1:\n        common.append(i)\n    elif exam_df[i].value_counts()[True]==1:\n        unique.append(i)\n    else:\n        blank.append(i)\n        \n        \nprint(common)\nprint(unique)\nprint(blank)   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# drop feature"},{"metadata":{},"cell_type":"markdown","source":"Tests for columns' usefulness before drop"},{"metadata":{},"cell_type":"markdown","source":"country_new"},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in [outside_Hubei, Hubei]:\n    l = x.index[x.country_new.notnull() == True]\n    p = []\n    for i in l:\n        if x.country_new[i] == x.country[i]:\n            p.append(i)\n\n    print(\"country_new == country\",len(p))\n    print(\"country_new.notnull\",len(l),\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The column has no new information to give"},{"metadata":{},"cell_type":"markdown","source":"ID"},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in [outside_Hubei, Hubei]:\n    m =[]\n    for i in range(len(x)):\n        if x.ID[i] != str(i+1):\n            m.append(i)\n    print(m[0],x.ID[m[0]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see the ID is not arranged in any numerical order.\nand because there is no column that needs another row identifier\nI drop ID"},{"metadata":{},"cell_type":"markdown","source":"# drop"},{"metadata":{"trusted":true},"cell_type":"code","source":"france = france.drop([\"departement\",\"source\",\"comments\",\"contact_number\"],axis=1)\n\n\nindonesia = indonesia.drop([\"origin\"],axis=1)\n\nkorea = korea.drop([\"age\",\"contact_number\"],axis=1)\n\nHubei = Hubei.drop([\"ID\",'location', 'admin3', 'admin2', \"admin1\" ,'latitude', 'longitude',\n                    'geo_resolution','admin_id', \"country_new\",\"source\",\"additional_information\",\"geo_resolution\"\n                    ,\"notes_for_discussion\"],axis=1)\n\noutside_Hubei = outside_Hubei.drop([\"ID\",'location', 'admin3', 'admin2', \"admin1\" ,'latitude', 'longitude',\n                                    'geo_resolution', 'admin_id', \"country_new\", \"data_moderator_initials\",\n                                    \"source\",\"additional_information\",\"geo_resolution\",\n                                    \"notes_for_discussion\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Examining values - v1 "},{"metadata":{"trusted":true},"cell_type":"code","source":"def examining_values_by_col (dfs, dfs_name, col):\n    \"\"\"\n    Prints values of each DF per column\n    \"\"\"\n    counter = 0\n    \n    for i in datasets:\n        if col in i.columns:\n            print(\"\\n\" + dfs_name[counter])\n            print(i[col].value_counts())\n        counter =counter + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\ncolumns_name = Exam.build_columns_name_ls(datasets)\nexam_df = Exam.df_exam_columns_dfs(datasets,datasets_name,columns_name)\n\nfor j in exam_df.columns[1:len(exam_df.columns)]:\n    print(j)\n    examining_values_by_col (datasets , datasets_name , j) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# format col"},{"metadata":{},"cell_type":"markdown","source":">datetime"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## TODO make FUN to_date\n\ncols = [\"confirmed_date\",\"released_date\", \"deceased_date\"]\n\nfrance[cols] = france[cols].apply(pd.to_datetime)\nindonesia[cols] = france[cols].apply(pd.to_datetime)\njapan[cols] = france[cols].apply(pd.to_datetime)\nkorea[cols] = korea[cols].apply(pd.to_datetime)\n\n# different\ntunisia_col = [\"confirmed_date\"]\n#  tunisia_col  = \"return_date\" problame\nkorea_col = [\"date_onset_symptoms\"] + cols\n\ntunisia[tunisia_col] = tunisia[tunisia_col].apply(pd.to_datetime)\nkorea[korea_col] = korea[korea_col].apply(pd.to_datetime)\n\n# לא עובד . בעיה אם השעות אם אני יעשה את זה - יותר נוח מאחרי בניית התכונות\nHubei_col = [\"confirmed_date\", \"date_death_or_discharge\", \"date_onset_symptoms\"]\nHubei[Hubei_col] = Hubei[Hubei_col].apply(pd.to_datetime)\n\n# \"confirmed_date\", \"date_onset_symptoms\"\nHubei_col = [\"date_death_or_discharge\"]\noutside_Hubei[Hubei_col] = outside_Hubei[Hubei_col].apply(pd.to_datetime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"examining_values_by_col(datasets, datasets_name, \"travel_history_dates\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_ls_of_str_datatime(ls):\n        for i in range(len(ls)):\n            ls[i] = pd.to_datetime(ls[i], errors='ignore').date()\n        return ls\n        \ndef  time_range_extremity(ls, earliest=False):\n    df = pd.DataFrame({\"series\":ls})\n    if earliest == True:\n        value = df.series.max()\n    else:\n        value = df.series.min()\n    return value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outside_Hubei.travel_history_dates[7957]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for indx in outside_Hubei.index[outside_Hubei.travel_history_dates.notnull()]:\n    print(indx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for indx in outside_Hubei.index[outside_Hubei.travel_history_dates.notnull()]:\n    i = outside_Hubei.travel_history_dates[indx] \n    i_split_1 = i.split(',')\n    \n    if len(i_split_1) > 1:\n        i_split_1 = make_ls_of_str_datatime(i_split_1)\n        value = time_range_extremity(i_split_1)\n        outside_Hubei.loc[indx, 'travel_history_dates'] = value\n    \n    i_split_2 = i_split_1[0].split('-')\n    if len(p) >1:\n        for i in range(len(p)):\n            p[i] = pd.to_datetime(p[i], errors='ignore').date()\n        d = pd.DataFrame({\"selfs\":p})\n        print(p,indx)\n        print(d.selfs.max())\n        print(p[0],p[1])\n        print(\"------\")\n    else:\n        p[0] = pd.to_datetime(p[0], errors='ignore').date()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date_string = '2019-04-17'\ntime_stamp = pd.to_datetime(date_string)\nprint(time_stamp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"examining_values_by_col(datasets, datasets_name, \"travel_history_dates\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"sex"},{"metadata":{"trusted":true},"cell_type":"code","source":"tunisia_sex = {\"F\":\"female\", \"M\":\"male\",np.nan:np.nan}\ntunisia.sex = [tunisia_sex[item] for item in  tunisia.sex] \n\njapan_sex = {\"Woman\":\"female\", \"Man\":\"male\",np.nan:np.nan, \"Checking\":np.nan, \"investigating\":np.nan}\njapan.sex = [japan_sex[item] for item in  japan.sex] \n\nfrance_sex = {\"female\":\"female\", \"male\":\"male\",\"Female\":\"female\", \"Male\":\"male\", \"male\\xa0?\":\"male\", \n              np.nan:np.nan }\nfrance.sex = [france_sex[item] for item in  france.sex] \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"examining_values_by_col(datasets, datasets_name, \"sex\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_index(df, col, index, data):\n    \"\"\"\n    Value change according index\n    \n    df: df\n    col : col you want to change\n    index: pd.index\n    data: data you want to into\n    \n    \"\"\"\n    for i in index:\n        df[col][i]=data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"background_diseases_binary"},{"metadata":{"trusted":true},"cell_type":"code","source":"# indexs = korea.index[korea.background_diseases_binary == True]\n# p = [i for i in indexs]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"examining_values_by_col (datasets, datasets_name, \"background_diseases_binary\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tunisia[\"background_diseases_binary\"] = np.nan\nindexs = tunisia.index[tunisia.background_diseases.notnull()]\nupdate_index(tunisia,\"background_diseases_binary\",indexs,1) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"background_diseases"},{"metadata":{"trusted":true},"cell_type":"code","source":"words =[]\nfor i in outside_Hubei.background_diseases[outside_Hubei.background_diseases.notnull()]:\n    [words.append(i) for i in i.split(\" \")]\n\nwords = list(dict.fromkeys(words))\nprint(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import nltk\n# from nltk.stem import PorterStemmer\n# from nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words =[]\nfor i in outside_Hubei.severity_illness[outside_Hubei.severity_illness.notnull()]:\n    [words.append(i) for i in i.split(\" \")]\n\nwords = list(dict.fromkeys(words))\nprint(words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#                                                     Complete features"},{"metadata":{},"cell_type":"markdown","source":"severity_illness"},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in [indonesia,japan]:   \n    x[\"severity_illness\"] = np.nan\n\n    indexs = x.index[x.deceased_date.notnull()]\n    update_index(x,\"severity_illness\",indexs,\"deceased\") \n\n    indexs = x.index[x.released_date.notnull()]\n    update_index(x,\"severity_illness\",indexs,\"cured\") \n\n\n\nseverity_illness_tunisia = {\"Critical\":\"critical\", \"In progress\":\"good\", \"Stable\":\"good\", \"Cured\":\"cured\" }\ntunisia.Severity_illness = [severity_illness_tunisia[item] for item in  tunisia.severity_illness]\n\n\nindexs = korea.index[korea.severity_illness.isnull()] \nupdate_index(korea,\"severity_illness\",indexs,np.nan)\nseverity_illness_korea = {\"deceased\":\"deceased\", \"isolated\": np.nan, \"released\":\"cured\", np.nan : np.nan }\nkorea.severity_illness = [severity_illness_korea[item] for item in  korea.severity_illness]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets2 = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\nexamining_values_by_col(datasets2, datasets_name, \"severity_illness\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"released_date"},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in [outside_Hubei, Hubei]:\n    x[\"released_date\"] = np.nan \n    indexs_released = x.index[x.severity_illness ==\"discharged\"]\n\n    for i in indexs_released:\n        x[\"released_date\"][i] = x[\"date_death_or_discharge\"][i]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"deceased_date"},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in [outside_Hubei, Hubei]:\n    x[\"deceased_date\"] = np.nan \n    indexs_released = x.index[x.severity_illness ==\"died\"]\n\n    for i in indexs_released:\n        x[\"deceased_date\"][i] = x[\"date_death_or_discharge\"][i]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"format date"},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in [outside_Hubei, Hubei]:\n    type(Hubei.deceased_date[2])\n    col = [\"released_date\",\"deceased_date\"]\n    x[col] = x[col].apply(pd.to_datetime)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"test"},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in [outside_Hubei, Hubei]:\n    l = x.date_death_or_discharge.notnull().sum()\n    y = x.severity_illness.notnull().sum()\n    p = x.released_date.notnull().sum() +x.deceased_date.notnull().sum()\n    print(l,y,p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in [outside_Hubei]:\n    complete_features =list(x.index[x.released_date.notnull()]) + list(x.index[x.deceased_date.notnull()])\n    date_death_or_discharge = list(x.index[x.date_death_or_discharge.notnull()])\n    severity_illness = list(x.index[x.severity_illness.notnull()])\n\n    if complete_features == date_death_or_discharge:\n        print(\"==\")\n    else:\n        print(\"not ==\")\n    \n    for i in severity_illness:\n        if i not in complete_features:\n            print(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"age"},{"metadata":{},"cell_type":"markdown","source":"exam "},{"metadata":{"trusted":true},"cell_type":"code","source":"# outside_Hubei_age = {\"investigating\":np.nan, \"Checking\":np.nan, \"Under 10\":\"0s\", \"Under teens\":\"0s\",\"305\":\"30s\",\n#             \"10s\":\"10s\",\"20s\":\"20s\", \"30-39\":\"30s\", \"40-49\":\"40s\", \"50-59\":\"50s\", \"60-69\":\"60s\", \"70s\":\"70s\" ,\n#              \"80s\":\"80s\",\"90s\":\"90s\" }\n# outside_Hubei.age = [outside_Hubei_age[item] for item in outside_Hubei.age] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Hubei.age.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hubei_age = {\"15-88\":np.nan, \"25-89\":np.nan, \"21-39\":np.nan, \"40-49\":\"40s\", \"50-59\":\"50s\", \"60-69\":\"60s\",\n#              \"70-82\":\"70s\" }\n# Hubei.age = [Hubei_age[item] for item in Hubei.age] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"japan.age.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"japan_age = {\"investigating\":np.nan, \"Checking\":np.nan, \"Under 10\":\"0s\", \"Under teens\":\"0s\",\"305\":\"30s\",\n            \"10s\":\"10s\",\"20s\":\"20s\", \"30s\":\"30s\", \"40s\":\"40s\", \"50s\":\"50s\", \"60s\":\"60s\", \"70s\":\"70s\" ,\n             \"80s\":\"80s\",\"90s\":\"90s\" }\njapan.age = [japan_age[item] for item in japan.age] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def birth_year_to_age(data):\n    age_ls = []\n\n    for i in range(len(data)):\n        age_ls.append(data.confirmed_date[i].year - data.birth_year[i])\n    return age_ls\n\nkorea[\"age\"] = birth_year_to_age(korea)\nfrance[\"age\"] = birth_year_to_age(france)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"country"},{"metadata":{"trusted":true},"cell_type":"code","source":"tunisia[\"country\"] = [\"tunisia\" for i in range(len(tunisia))]\njapan[\"country\"] = [\"japan\" for i in range(len(japan))]\nindonesia[\"country\"] = [\"indonesia\" for i in range(len(indonesia))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"outside_Hubei data VS country data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(korea))\nprint(outside_Hubei.country.value_counts()[\"South Korea\"])\nprint()\n\nprint(len(france))\nprint(outside_Hubei.country.value_counts()[\"France\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"status"},{"metadata":{"trusted":true},"cell_type":"code","source":"france[\"status\"] =[np.nan for i in np.arange(len(france))]\nfrance.status = france.status.astype(\"object\")\nfrance[\"status\"].dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"deceased = france.index[france.deceased_date.notnull() == True]\nreleased = france.index[france.released_date.notnull() == True]\nisolated = france.index[(france.released_date.notnull() == False)&(france.deceased_date.notnull() == False)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in deceased:\n    france.at[ i, 'status'] = \"deceased\"\n\nfor i in released:\n    france.at[ i, 'status'] = \"released\" \n    \nfor i in isolated:\n    france.at[ i, 'status'] = \"isolated\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"france.status.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"infection_place"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Hubei[\"wuhan(0)_not_wuhan(1)\"].value_counts())\nprint(outside_Hubei[\"travel_history_location\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"infection_case\n\n= Community \\abroad \\ Nan"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Garbage drop \n- Features that have only one dataset or  built with Engineered another feature with them"},{"metadata":{"trusted":true},"cell_type":"code","source":"france = france.drop([\"birth_year\", \"treatment\",\"group\"],axis=1)\ntunisia = tunisia.drop([\"hospital_place\"],axis=1)\njapan = japan.drop([\"Close contact situation\"],axis=1)\nkorea = korea.drop([\"birth_year\",\"global_num\"],axis=1)\nHubei = Hubei.drop([\"date_death_or_discharge\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# drop Non-baked features"},{"metadata":{"trusted":true},"cell_type":"code","source":"france = france.drop([\"infection_place\", \"infected_by\",\"infection_order\"],axis=1)\ntunisia = tunisia.drop([\"hospital_name\"],axis=1)\nindonesia = indonesia.drop([\"infected_by\",\"hospital_name\"],axis=1)\nkorea = korea.drop([\"infection_place\", \"infected_by\",\"infection_order\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**> Feature sum**"},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets2 = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\ncolumns_name = Exam.build_columns_name_ls(datasets2)\nexam_df = Exam.df_exam_columns_dfs(datasets2, datasets_name, columns_name)\nprint(columns_name)\nexam_df.infection_place","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"unipue"},{"metadata":{},"cell_type":"markdown","source":"common feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets3 = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\ncolumns_name = Exam.build_columns_name_ls(datasets3)\nexam_df2 = Exam.df_exam_columns_dfs(datasets3,datasets_name,columns_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in exam_df2.columns:\n    print(\"\\n\"+i)\n    examining_values_by_col (datasets, datasets_name, i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# build final DS"},{"metadata":{"trusted":true},"cell_type":"code","source":"exam_df.sex","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in dfs:\n    print(i[col].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in datasets:\n    print(i.sex.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets_final = [france, tunisia, japan, indonesia, korea]\nfinal_DS = pd.concat(datasets_final, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_DS.status.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# orgnaze DS"},{"metadata":{},"cell_type":"markdown","source":"index"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_DS.index = range(len(final_DS))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_DS.to_csv(r'/kaggle/working/Characteristics_Corona_patients1.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_DS.to_csv()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"infected by"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}