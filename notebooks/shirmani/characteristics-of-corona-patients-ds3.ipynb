{"cells":[{"metadata":{},"cell_type":"markdown","source":"**The purpose of this notebook is to create a DATASET that includes**\n\n** Characteristics of patients like - **\n\n*age\n\n*sex\n\n*Country\n\nand so\n\n\n\n\n**The condition of the patients and their characteristics - **\n\n* Disease time (from diagnosis date)\n\n* Have been cured\n \n* Deaths\n\n\nThe database is designed to allow easy exploration of the data\n\nAnyone interested can use and donate"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import nltk\nimport pandas as pd \nimport  numpy as np\nfrom collections import Counter\nfrom ds_exam import *\nfrom update_time import *\nfrom bag_words import *","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"france = pd.read_csv(\"/kaggle/input/coronavirus-france-dataset/patient.csv\")\ntunisia = pd.read_csv(\"../input/coronavirus-tunisia/Coronavirus_Tunisia.csv\")\njapan = pd.read_csv(\"/kaggle/input/close-contact-status-of-corona-in-japan/COVID-19_Japan_Mar_07th_2020.csv\")\nindonesia = pd.read_csv(\"/kaggle/input/indonesia-coronavirus-cases/patient.csv\")\nkorea = pd.read_csv(\"/kaggle/input/coronavirusdataset/PatientInfo.csv\")\nHubei = pd.read_csv(\"/kaggle/input/covid19official/Hubei.csv\")\noutside_Hubei = pd.read_csv(\"/kaggle/input/covid19official/outside_Hubei.csv\")\nworld_a = pd.read_csv(\"/kaggle/input/covid19-outbreak-realtime-case-information/latestdata.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei, world]\ndatasets_name = [\"france\", \"tunisia\", \"japan\", \"indonesia\", \"korea\", \"Hubei\", \"outside_Hubei\", \"world\"]\n\ngarbge = [print(\"\\n\"+datasets_name[i], [i for i in datasets[i].columns]) for i in range(len(datasets_name))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = ['age', 'sex', 'city', 'province', 'country',\"age\", 'date_confirmation', 'date_onset_symptoms',\n     'date_admission_hospital', 'symptoms', 'source']\nl = ['ID', 'age', 'sex', 'city', 'province', 'country', 'wuhan(0)_not_wuhan(1)', 'geo_resolution',\n     'date_onset_symptoms', 'date_admission_hospital', 'date_confirmation', 'symptoms',\n     'lives_in_Wuhan', 'travel_history_dates', \n      'chronic_disease_binary']\n\n\ndf1 = Hubei.loc[:, l]\n\ndf2 =  outside_Hubei.loc[:, l]\n\ndf_diff = pd.concat([df2,df1]).drop_duplicates()\n\nwww =  df_diff\n\n\ndf2 =  world_a.loc[:, l]\n\ndf_diff = pd.concat([df2,www]).drop_duplicates()\n\nmm = df_diff\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"e = []\nfor ind in range(len(mm['date_confirmation'])):\n    if type(mm['date_confirmation'][ind]) == pd.core.series.Series:\n        e.append(ind)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mm['date_confirmation'][(mm['date_confirmation'][21164].notnull()) == ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ls_i = [i  for i in mm.date_confirmation[mm['date_confirmation'].notnull()]]\n\n# indexs = mm.index[mm['date_confirmation'].notnull()]\n# dataset =mm\n# input_col = 'date_confirmation'\n# output_col ='date_confirmation'\n# date_character= \".\"\n# character_separator =  \"-\"\n\n# d = []\n\n# for i in ls_i:\n\n#     p = i.split(character_separator)\n#     p = UpdateTime.del_str_equal_x_from_ls(p, \"\")\n#     boo = UpdateTime.redundant_numbers_date(p, date_character)\n#     if len(p) == 1:\n#             d.append(p)\n            \n# mm['date_confirmation']            \n# # mm['date_confirmation'].max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# datasets.shape"},{"metadata":{"trusted":true},"cell_type":"code","source":"o = []\nfor i in range(len(datasets)):\n    print(datasets_name[i],datasets[i].shape)\n    o.append(datasets[i].shape[0])\nprint(\"\\nnum of i \" + str(sum(o)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# change name of col"},{"metadata":{"trusted":true},"cell_type":"code","source":"france.rename(columns={\"health\":\"severity_illness\",\"status\":\"treatment\",\"infection_reason\":\"infection_place\"}\n              , inplace = True)\n\ntunisia.rename(columns={\"date\":\"confirmed_date\", \"gender\":\"sex\", \"situation\":\"severity_illness\", \n                        \"return_from\":\"infection_place\", \"health\":\"background_diseases\"}, inplace = True)\n\njapan.rename(columns={\"No.\":\"id\", \"Fixed date\":\"confirmed_date\",\"Age\":\"age\", \"residence\":\"region\",\n                      \"Surrounding patients *\":\"infected_by\"}, inplace = True)\n\nindonesia.rename(columns={\"patient_id\":\"id\",\"gender\": \"sex\", \"province\":\"region\", \"hospital\":\"hospital_name\",\n                          \"contacted_with\":\"infected_by\", \"current_state\":\"severity_illness\"}, inplace = True)\n\nkorea.rename(columns={\"patient_id\":\"id\", \"disease\":\"background_diseases_binary\", \"state\":\"severity_illness\",\n                      \"province\":\"region\", \"infection_case\" :\"infection_place\",\n                      \"symptom_onset_date\":\"date_onset_symptoms\"}, inplace = True)\n\nHubei = Hubei.rename(columns={ \"province\":\"region\",\"date_confirmation\": \"confirmed_date\",\n                              \"chronic_disease_binary\":\"background_diseases_binary\", \n                              \"chronic_disease\":\"background_diseases\", \"outcome\":\"severity_illness\"})\n\noutside_Hubei = outside_Hubei.rename(columns={ \"province\":\"region\", \"date_confirmation\": \"confirmed_date\",\n                              \"chronic_disease_binary\":\"background_diseases_binary\", \n                              \"chronic_disease\":\"background_diseases\", \"outcome\":\"severity_illness\" })\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_datatime(dataset, input_col, output_col, indexs, date_character, character_separator, earliest=False):\n\n    drop = []\n    indexs_error = []\n    for indx in indexs:\n        i = dataset.loc[indx, input_col]\n        print(i)\n#         ls = i.split(character_separator)\n#         ls = UpdateTime.del_str_equal_x_from_ls(ls, \"\")\n#         boo = UpdateTime.redundant_numbers_date(ls, date_character)\n#         if boo == False:\n#                 indexs_error.append(indx)\n#         else:\n#             if len(ls) > 1:\n#                 ls = UpdateTime.make_ls_of_str_datatime(ls)\n#                 value = UpdateTime.time_range_extremity(ls, earliest)\n#                 dataset.loc[indx, output_col] = value\n#                 drop.append(indx)\n#     indexs = indexs.drop(drop)\n#      return indexs, indexs_error\n\nformat_datatime(mm,'date_confirmation','date_confirmation' ,indexs ,\".\",[ \"-\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'travel_history_dates'\n\n# world['date_confirmation'] = world['date_confirmation'].apply(pd.to_datetime)\nindexs = world.index[world['date_confirmation'].notnull()]\nindexs_ , error = UpdateTime.updte_time(world,'date_confirmation','date_confirmation' ,[1],\".\",[ \"-\"])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# exam_df = columns vs dfs"},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei, world]\ncolumns_name = Exam.build_columns_name_ls(datasets)\nexam_df = Exam.df_exam_columns_dfs(datasets,datasets_name,columns_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def full_common(exam_df):\n    \"\"\"\n    Returns columns that all DATASETS have\n    \"\"\"\n    full_common = []\n    for j in exam_df.columns:\n        boolyan = exam_df[j].all()\n        if boolyan == True:\n            full_common.append(j)\n    return full_common","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"common = []\nunique = []\nblank = []\nfor i in exam_df.columns:\n    if exam_df[i].value_counts()[True]>1:\n        common.append(i)\n    elif exam_df[i].value_counts()[True]==1:\n        unique.append(i)\n    else:\n        blank.append(i)\n        \n        \nprint(common)\nprint(unique)\nprint(blank)   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# drop feature"},{"metadata":{},"cell_type":"markdown","source":"Tests for columns' usefulness before drop"},{"metadata":{},"cell_type":"markdown","source":"country_new"},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in [outside_Hubei, Hubei]:\n    l = x.index[x.country_new.notnull() == True]\n    p = []\n    for i in l:\n        if x.country_new[i] == x.country[i]:\n            p.append(i)\n\n    print(\"country_new == country\",len(p))\n    print(\"country_new.notnull\",len(l),\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The column has no new information to give"},{"metadata":{},"cell_type":"markdown","source":"ID"},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in [outside_Hubei, Hubei]:\n    m =[]\n    for i in range(len(x)):\n        if x.ID[i] != str(i+1):\n            m.append(i)\n    print(m[0],x.ID[m[0]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see the ID is not arranged in any numerical order.\nand because there is no column that needs another row identifier\nI drop ID"},{"metadata":{},"cell_type":"markdown","source":"# drop"},{"metadata":{"trusted":true},"cell_type":"code","source":"france = france.drop([\"departement\",\"source\",\"comments\",\"contact_number\"],axis=1)\n\n\nindonesia = indonesia.drop(['nationality'],axis=1)\n\nkorea = korea.drop([\"age\",\"contact_number\"],axis=1)\n\nHubei = Hubei.drop([\"ID\",'location', 'admin3', 'admin2', \"admin1\" ,'latitude', 'longitude',\n                    'geo_resolution','admin_id', \"country_new\",\"source\",\"additional_information\",\"geo_resolution\"\n                    ,\"notes_for_discussion\"],axis=1)\n\noutside_Hubei = outside_Hubei.drop([\"ID\",'location', 'admin3', 'admin2', \"admin1\" ,'latitude', 'longitude',\n                                    'geo_resolution', 'admin_id', \"country_new\", \"data_moderator_initials\",\n                                    \"source\",\"additional_information\",\"geo_resolution\",\n                                    \"notes_for_discussion\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Examining values - v1 "},{"metadata":{"trusted":true},"cell_type":"code","source":"def examining_values_by_col (datasets, datasets_name, col):\n    \"\"\"\n    Prints values of each DF per column\n    \"\"\"\n    counter = 0\n    \n    for i in datasets:\n        if col in i.columns:\n            print(\"\\n\" + datasets_name[counter])\n            print(i[col].value_counts())\n        counter =counter + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei, world]\ncolumns_name = Exam.build_columns_name_ls(datasets)\nexam_df = Exam.df_exam_columns_dfs(datasets,datasets_name,columns_name)\n\nfor j in exam_df.columns[1:len(exam_df.columns)]:\n    print(j)\n    examining_values_by_col (datasets , datasets_name , j) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# format col"},{"metadata":{},"cell_type":"markdown","source":">datetime"},{"metadata":{"trusted":true},"cell_type":"code","source":"l1= tunisia.index[tunisia[\"return_date\"] == \"Local\"]\nl2 = tunisia.index[ tunisia[\"return_date\"].notnull()]\n\nindex = l2.drop(l1)\n\nfor indx in index:\n    tunisia.loc[indx,\"return_date\"] = pd.to_datetime(tunisia.loc[indx,\"return_date\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cols = [\"confirmed_date\",\"released_date\", \"deceased_date\"]\n\n# france[cols] = france[cols].apply(pd.to_datetime)\n# indonesia[cols] = france[cols].apply(pd.to_datetime)\n# japan[cols] = france[cols].apply(pd.to_datetime)\n# korea[cols] = korea[cols].apply(pd.to_datetime)\n\n# #### different#####\n\n# # korea\n# korea_col = [\"date_onset_symptoms\"]\n# korea[korea_col] = korea[korea_col].apply(pd.to_datetime)\n\n# #  tunisia\n# tunisia_col = [\"confirmed_date\"]\n# tunisia[tunisia_col] = tunisia[tunisia_col].apply(pd.to_datetime)\n\n# # Hubei\n# Hubei_col = [\"confirmed_date\", \"date_death_or_discharge\", \"date_onset_symptoms\"]\n# Hubei[Hubei_col] = Hubei[Hubei_col].apply(pd.to_datetime)\n\n# # outside_Hubei\n# outside_Hubei_col = [\"date_death_or_discharge\"]\n# outside_Hubei[outside_Hubei_col] = outside_Hubei[outside_Hubei_col].apply(pd.to_datetime)\n\n# # 'travel_history_dates'\n# for j in [\"confirmed_date\", \"date_onset_symptoms\"]:\n#     indexs = outside_Hubei.index[outside_Hubei[j].notnull()]\n#     indexs_ , error = UpdateTime.updte_time(outside_Hubei, j, j, indexs,\".\",[ \"-\", ','])\n#     print(j , error)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# error\noutside_Hubei.index[outside_Hubei[\"date_onset_symptoms\"] == \"- 25.02.2020\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"examining_values_by_col (datasets , datasets_name , \"date_onset_symptoms\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"sex"},{"metadata":{"trusted":true},"cell_type":"code","source":"tunisia_sex = {\"F\":\"female\", \"M\":\"male\",np.nan:np.nan}\ntunisia.sex = [tunisia_sex[item] for item in  tunisia.sex] \n\njapan_sex = {\"Woman\":\"female\", \"Man\":\"male\",np.nan:np.nan, \"Checking\":np.nan, \"investigating\":np.nan}\njapan.sex = [japan_sex[item] for item in  japan.sex] \n\nfrance_sex = {\"female\":\"female\", \"male\":\"male\",\"Female\":\"female\", \"Male\":\"male\", \"male\\xa0?\":\"male\", \n              np.nan:np.nan }\nfrance.sex = [france_sex[item] for item in  france.sex] \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"examining_values_by_col(datasets, datasets_name, \"sex\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"city"},{"metadata":{"trusted":true},"cell_type":"code","source":"# indexs = Hubei.index[Hubei.city.notnull()]\n# for indx in indexs:\n#     i = Hubei.loc[indx, \"city\"]\n#     i = i.split(\" \")\n#     print(i)\n#     if len(i)> 1:\n#         ls_value = del_str_equal_x_from_ls(i, \"City\")\n#         for i in ls_value:\n#         Hubei.loc[indx, \"city\"] = value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Hubei.loc[5, \"city\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"examining_values_by_col(datasets, datasets_name, \"city\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_index(dataset, col, indexs, data):\n    \"\"\"\n    Value change according index\n    \n    dataset: df\n    \n    col : str\n        name of col you want to change\n        \n    indexs: pd.index\n    \n    data: int/ str/ float\n        data you want to into\n    \n    \"\"\"\n    for indx in indexs:\n        dataset.loc[indx,col] = data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"background_diseases_binary"},{"metadata":{"trusted":true},"cell_type":"code","source":"indexs = korea.index[korea.background_diseases_binary == True]\nupdate_index(korea, \"background_diseases_binary\", indexs, 1.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tunisia[\"background_diseases_binary\"] = np.nan\n\nfor dataset in [tunisia, Hubei, outside_Hubei]:\n    indexs = dataset.index[dataset.background_diseases.notnull()]\n    update_index(dataset,\"background_diseases_binary\",indexs,1.0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"examining_values_by_col (datasets, datasets_name, \"background_diseases_binary\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"background_diseases"},{"metadata":{"trusted":true},"cell_type":"code","source":"examining_values_by_col (datasets, datasets_name, \"background_diseases\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ps = nltk.stem.SnowballStemmer('english')\n\n# for i in outside_Hubei.background_diseases[outside_Hubei.background_diseases.notnull()]:\n#     print(\"\\n\"+i)\n#     i = BagWords.clean_str(i)\n    \n#     print([ps.stem(x) for x in i ])\n    \n# # chronic obstructive pulmonary disease  'obstruct', 'pulmonari', 'diseas'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a_bag_words= {\"hypertension\":[\"hypertens\",  ],\n            \n#             \"coronary heart disease\":[\"stenocardia\" ],\n            \n#             \"diabetes\":['diabet', \"mellitus\"],\n               \n#             \"tuberculosis\": [\"tuberculosi\"],\n            \n#             \"parkinson\": [\"parkinso\", \"madopar\"],\n           \n#             \"hypertriglyceridemia\": ['hypertriglyceridemia',],\n            \n#             \"obesity\": ['obes',],\n             \n#             \"Chronic obstructive pulmonary\": ['copd',],\n            \n#             \"hiv\": [\"hiv\",],\n#            \"asthma\": [\"asthma\",],}\n\n\n\n# a_sentences_bag = {\"hypertension\":[['high', 'blood', 'pressur'],],\n\n#                 \"coronary heart disease\": [['coronari', 'heart'], ['coronari', 'stent']],\n\n#                 \"chronic bronchitis\":[['chronic', 'bronchiti'],],\n\n#                 \"chronic renal insufficiency\":[[ 'chronic', 'renal', \"insuffici\"],],\n\n#                 \"hemorrhage of digestive tract\":[['hemorrhag', 'of', 'digest', \"tra\"],],\n\n#                 \"colon cancer\":[['colon', 'cancer'],],\n                \n#                  \"lung cancer\":[['lung', 'cancer'],],\n\n#                 \"prostate hypertrophy\": [['prostat', 'hypertrophi'],],\n                 \n#                 \"hip replacement\": [['hip', \"replac\"],],\n\n#                 \"cerebral infarction\":[['hypertens', 'cerebr', 'infarct'],],\n                \n#                  \"hepatitis B\": [[\"hepat\", 'b'],]}\n                                 \n# # 'encephalomalacia'  encephalomalacia   coronary bypass                               \n# # time = {['20', 'year'], ['four', 'year'], ['five', 'year']9 years}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# datasets2 = [tunisia, Hubei, outside_Hubei]\n# datasets_name1 = [ \"tunisia\", \"Hubei\", \"outside_Hubei\"]\n\n# for ind in range(len(datasets_name1)):\n#     dataset = datasets2[ind]\n#     print(ind)\n#     dataset[\"guess\"]= [np.nan for i in range(len(dataset.background_diseases)) ]\n#     indexs = dataset.index[dataset.background_diseases.notnull()]\n#     no_guess,multi_guess = BagWords.guess_category(dataset, \"background_diseases\", \"guess\",indexs, ps, a_bag_words, a_sentences_bag)\n    \n#     print(datasets_name1[ind])\n#     print(no_guess)\n#     print(multi_guess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# indexs = Hubei.index[Hubei.background_diseases.notnull()]\n\n# indexs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"symptoms"},{"metadata":{"trusted":true},"cell_type":"code","source":"# bag_words= {\"pneumonia\":[\"pneumonia\",\"pneumon\"],\n#             \"fever\":[\"fever\"], \n#             \"cough\": [\"cough\" ],\n#             \"fatigue\":[\"fatigu\"],\n#             \"discomfort\": [\"discomfort\"],\n#            \"weakness\": [\"weak\", ['lack', 'of', 'energi]],\n#            \"dizziness\": [\"dizzi\"],\n#            \"rhinorrhoea\": [\"rhinorrhoea\",['runni', \"nose\"]],\n#            \"sneezing\": [\"sneez\"],\n#            \"diarrhea\": [\"diarrhea\"],\n#             \"expectoration\": [\"expector\"],\n#             \"headache\": [\"headach\"],\n#             \"diarrhea\": [\"diarrhea\"],\n#             \"chills\": [\"chill\"],\n#             \"dyspnea\": [\"dyspnea\"],\n#             \"rigor\": [\" rigor\"],\n#                                  \"pharyngalgia\": [\"pharyngalgia\",],\n#                                  \"no_symptom\": [\"asymptomat\"],\n#                                  \"nausea\": [\"nausea\"],\n                                 \n            \n#            }\n\n# sentences_bag = {\"nasal congestion\":[['nasal', 'congest'],['in', 'progress']],\n#                 \"sore throat\":[['sore', \"throat\"],], \n#                 \"pleuritic chest pain\": [ [ 'pleurit', 'chest', 'pain']],\n#                 \"muscular soreness\":[['muscular',\"sore\"]],\n#                  \"chest distress\":[[' chest', 'distress']],\n#                  \"muscular stiffness\":[['muscular', 'stiff']],\n#                  \"muscular soreness\":[['muscular',\"sore\"],[ \"muscl\", 'ach'],['muscl', \"pain\"], [\"myalgia\"]],\n#                  \"joint pain\":[['muscular', 'stiff']],\n#                  \"sore limbs\":[['muscular', 'stiff']],\n                 \n                 \n#                }\n                 \n# # pleuritic chest pain , ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getKeysByValue(dictOfElements, valueToFind):\n    listOfKeys = list()\n    listOfItems = dictOfElements.items()\n    for item  in listOfItems:\n        if item[1] == valueToFind:\n            listOfKeys.append(item[0])\n    return  listOfKeys\n\ndef remove(dict_a, keys_remove ):\n    for key in keys_remove:\n        if key in dict_a.keys():\n            dict_a.pop(key)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ps = nltk.stem.SnowballStemmer('english')\nr = []\no = []\nfor ind in outside_Hubei.index[outside_Hubei.symptoms.notnull()]:\n    i = outside_Hubei.loc[ind, \"symptoms\"]\n\n    i = BagWords.clean_str(i)\n    l = [ps.stem(x) for x in i]\n\n    for x in l:\n        if x.isalpha():\n            r.append(x)\n            \nkeys_remove = ['to', 'a','like',  'no', 'and',  'yes', 'then','complaint',\"great\", \"even\", \n         \"for\", \"the\", \"non\",  'of' , \"this\",  'on' ,'with', \"was\", 'c',\n         \"cannot\", \"recommend\", \"as\", \"a\", \"i\", \"did\", \"not\", \"want\", \"to\", \"have\", \"to\", \"do\", \"this\"]\n\n\n            \n# r = columns_name = list(dict.fromkeys(o))\ntest_dict = dict(Counter(r))\nremove(test_dict, keys_remove )\nprint(test_dict)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for key in test_dict.keys():\n    \n    o = test_dict[key]\n    print(key, o)\n    if o < 9:\n        test_dict.pop(key)\n        \nprint(test_dict)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"_____________________________________________________lab"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def remover(my_string =\"\"):\n#     values = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 \")\n#     for item in my_string:\n#         if item not in values:\n#             my_string = my_string.replace(item,\"\")\n#     return my_string\n\n\n\n\n# ps = nltk.stem.SnowballStemmer('english')\n\n# for i in Hubei.severity_illness[Hubei.severity_illness.notnull()]:\n#     i = remover(i)\n#     i = i.split(\" \")\n#     print(i)\n# #     print([ps.stem(x) for x in i ])\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#                                                     Complete features"},{"metadata":{},"cell_type":"markdown","source":"severity_illness"},{"metadata":{"trusted":true},"cell_type":"code","source":"bag_words= {\"good\":[\"good\",\"stabl\", \"follow\"],\n            \"critical\":[\"critic\", \"intens\", \"sever\"], \n            \"deceased\": [\"death\",\"dead\", \"die\", \"deceas\" ],\n            \"cured\":[\"discharg\", \"releas\", \"cure\", \"recov\", 'health'],\n            np.nan: [\"isol\"]}\n\nsentences_bag = {\"good\":[['not', 'hospit'],['in', 'progress']],\n                \"critical\":[], \n                \"deceased\": [ ],\n                \"cured\":[]}\n\n\n\ndatasets2 = [france, tunisia, indonesia, korea, Hubei, outside_Hubei]\ndatasets_name1 = [\"france\", \"tunisia\", \"indonesia\", \"korea\", \"Hubei\", \"outside_Hubei\"]\n\nfor ind in range(len(datasets_name1)):\n    dataset = datasets2[ind]\n    indexs = dataset.index[dataset.severity_illness.notnull()]\n    no_guess,multi_guess = BagWords.guess_category(dataset, \"severity_illness\", \"severity_illness\",indexs, ps, bag_words, sentences_bag)\n    \n    print(datasets_name1[ind])\n    print(no_guess)\n    print(multi_guess)\n\nfor x in [indonesia, france]:  \n    indexs = x.index[x.deceased_date.notnull()]\n    update_index(x,\"severity_illness\",indexs,\"deceased\") \n\n    indexs = x.index[x.released_date.notnull()]\n    update_index(x,\"severity_illness\",indexs,\"cured\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets2 = [france, tunisia, indonesia, korea, Hubei, outside_Hubei]\ndatasets_name1 = [\"france\", \"tunisia\", \"indonesia\", \"korea\", \"Hubei\", \"outside_Hubei\"]\nexamining_values_by_col(datasets2, datasets_name1,  \"severity_illness\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"released_date / deceased_date"},{"metadata":{"trusted":true},"cell_type":"code","source":"categories = [\"deceased\", \"cured\"]\ncols = [\"deceased_date\",\"released_date\"]\nfor indx_j in range(len(cols)) :\n    j  = cols[indx_j]\n    category = categories[indx_j]\n    \n    for x in [outside_Hubei, Hubei]:\n        x[j] = np.nan \n        indexs = x.index[x[\"severity_illness\"] == category]\n\n        for i in indexs:\n            x.loc[i, j]= pd.to_datetime(x.loc[ i, \"date_death_or_discharge\"])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"released_date exam"},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets2 = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\nexamining_values_by_col(datasets2, datasets_name, \"released_date\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"deceased_date exam"},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets2 = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\nexamining_values_by_col(datasets2, datasets_name, \"deceased_date\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"test"},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in [outside_Hubei, Hubei]:\n    l = x.date_death_or_discharge.notnull().sum()\n    y = x.severity_illness.notnull().sum()\n    p = x.released_date.notnull().sum() +x.deceased_date.notnull().sum()\n    print(l,y,p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in [outside_Hubei]:\n    complete_features =list(x.index[x.released_date.notnull()]) + list(x.index[x.deceased_date.notnull()])\n    date_death_or_discharge = list(x.index[x.date_death_or_discharge.notnull()])\n    severity_illness = list(x.index[x.severity_illness.notnull()])\n\n    if complete_features == date_death_or_discharge:\n        print(\"==\")\n    else:\n        print(\"not ==\")\n    \n    for i in severity_illness:\n        if i not in complete_features:\n            print(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"age"},{"metadata":{},"cell_type":"markdown","source":"exam "},{"metadata":{"trusted":true},"cell_type":"code","source":"indexs =  outside_Hubei.index[outside_Hubei.age.notnull()]\n\ndef int_num(dataset, col, indexs):\n    to_float = []\n\n    for i in indexs:\n        if dataset.loc[i, col].isdigit() == True:\n            to_float.append(i)\n\n        \n        if \".\" in i:\n            to_float.append(i)\n\n#     for indx in  to_float:\n#         dataset.loc[indx, col] = int(dataset.loc[indx, col])\n    \n    return to_float\n        \n\nto_float= int_num(outside_Hubei, \"age\",indexs)\nindexs = indexs.drop(to_float)\n\nprint(indexs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"float_indx = []\n\nfor indx in indexs:\n    \n   print(type(outside_Hubei.loc[indx, \"age\"]))\n\n\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in indexs:\n    ls = i.split(\"-\")\n    if len(ls)>1:\n        y = ls[0]-ls[1]\n        print(y)\n    print(outside_Hubei.loc[i, \"age\"], i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\noutside_Hubei.head(11714)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# outside_Hubei_age = {\"investigating\":np.nan, \"Checking\":np.nan, \"Under 10\":\"0s\", \"Under teens\":\"0s\",\"305\":\"30s\",\n#             \"10s\":\"10s\",\"20s\":\"20s\", \"30-39\":\"30s\", \"40-49\":\"40s\", \"50-59\":\"50s\", \"60-69\":\"60s\", \"70s\":\"70s\" ,\n#              \"80s\":\"80s\",\"90s\":\"90s\" }\n# outside_Hubei.age = [outside_Hubei_age[item] for item in outside_Hubei.age] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hubei.age.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hubei_age = {\"15-88\":np.nan, \"25-89\":np.nan, \"21-39\":np.nan, \"40-49\":\"40s\", \"50-59\":\"50s\", \"60-69\":\"60s\",\n#              \"70-82\":\"70s\" }\n# Hubei.age = [Hubei_age[item] for item in Hubei.age] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# japan.age.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# japan_age = {\"investigating\":np.nan, \"Checking\":np.nan, \"Under 10\":\"0s\", \"Under teens\":\"0s\",\"305\":\"30s\",\n#             \"10s\":\"10s\",\"20s\":\"20s\", \"30s\":\"30s\", \"40s\":\"40s\", \"50s\":\"50s\", \"60s\":\"60s\", \"70s\":\"70s\" ,\n#              \"80s\":\"80s\",\"90s\":\"90s\" }\n# japan.age = [japan_age[item] for item in japan.age] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def birth_year_to_age(data):\n    age_ls = []\n\n    for i in range(len(data)):\n        age_ls.append(data.confirmed_date[i].year - data.birth_year[i])\n    return age_ls\n\nkorea[\"age\"] = birth_year_to_age(korea)\nfrance[\"age\"] = birth_year_to_age(france)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"country"},{"metadata":{"trusted":true},"cell_type":"code","source":"tunisia[\"country\"] = [\"tunisia\" for i in range(len(tunisia))]\njapan[\"country\"] = [\"japan\" for i in range(len(japan))]\nindonesia[\"country\"] = [\"indonesia\" for i in range(len(indonesia))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"outside_Hubei data VS country data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(korea))\nprint(outside_Hubei.country.value_counts()[\"South Korea\"])\nprint()\n\nprint(len(france))\nprint(outside_Hubei.country.value_counts()[\"France\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"infection_place"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Hubei[\"wuhan(0)_not_wuhan(1)\"].value_counts())\nprint(outside_Hubei[\"travel_history_location\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"infection_case\n\n= Community \\abroad \\ Nan"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Garbage drop \n- Features that have only one dataset or  built with Engineered another feature with them"},{"metadata":{"trusted":true},"cell_type":"code","source":"france = france.drop([\"birth_year\", \"treatment\",\"group\"],axis=1)\ntunisia = tunisia.drop([\"hospital_place\"],axis=1)\njapan = japan.drop([\"Close contact situation\"],axis=1)\nkorea = korea.drop([\"birth_year\",\"global_num\"],axis=1)\nHubei = Hubei.drop([\"date_death_or_discharge\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# drop Non-baked features"},{"metadata":{"trusted":true},"cell_type":"code","source":"france = france.drop([\"infection_place\", \"infected_by\",\"infection_order\"],axis=1)\ntunisia = tunisia.drop([\"hospital_name\"],axis=1)\nindonesia = indonesia.drop([\"infected_by\",\"hospital_name\"],axis=1)\nkorea = korea.drop([\"infection_place\", \"infected_by\",\"infection_order\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**> Feature sum**"},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets2 = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\ncolumns_name = Exam.build_columns_name_ls(datasets2)\nexam_df = Exam.df_exam_columns_dfs(datasets2, datasets_name, columns_name)\nprint(columns_name)\nexam_df.infection_place","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"unipue"},{"metadata":{},"cell_type":"markdown","source":"common feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets3 = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\ncolumns_name = Exam.build_columns_name_ls(datasets3)\nexam_df2 = Exam.df_exam_columns_dfs(datasets3,datasets_name,columns_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in exam_df2.columns:\n    print(\"\\n\"+i)\n    examining_values_by_col (datasets, datasets_name, i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# build final DS"},{"metadata":{"trusted":true},"cell_type":"code","source":"exam_df.sex","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in dfs:\n    print(i[col].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in datasets:\n    print(i.sex.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets_final = [france, tunisia, japan, indonesia, korea]\nfinal_DS = pd.concat(datasets_final, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_DS.status.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# orgnaze DS"},{"metadata":{},"cell_type":"markdown","source":"index"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_DS.index = range(len(final_DS))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_DS.to_csv(r'/kaggle/working/Characteristics_Corona_patients1.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_DS.to_csv()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"infected by"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}