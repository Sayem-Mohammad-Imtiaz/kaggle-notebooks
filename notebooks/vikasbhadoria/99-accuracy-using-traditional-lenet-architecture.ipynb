{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Downloading the dataset from github. ","execution_count":null},{"metadata":{"id":"mODSRy74h4eQ","outputId":"8cd5dd3e-8b9f-4602-d8da-f063489f1914","trusted":true},"cell_type":"code","source":"!git clone https://bitbucket.org/jadslim/german-traffic-signs","execution_count":null,"outputs":[]},{"metadata":{"id":"w738vkqviAou","outputId":"87fa61b8-dd91-4616-a864-da20e91b93c9","trusted":true},"cell_type":"code","source":"!ls german-traffic-signs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing all the libraries","execution_count":null},{"metadata":{"id":"buTmcmLbiR00","outputId":"fa7187e6-c1cd-483f-f510-1884b201168e","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.layers import Dense\nfrom keras.layers import Flatten, Dropout\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nimport random\nimport pickle\nimport pandas as pd\nimport cv2","execution_count":null,"outputs":[]},{"metadata":{"id":"EWPJX0tuiYzH","trusted":true},"cell_type":"code","source":"np.random.seed(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading the data as train, test & valadition.","execution_count":null},{"metadata":{"id":"Zj8HPX4RigRx","outputId":"9cd6063c-cb77-499c-c818-48a52d730720","trusted":true},"cell_type":"code","source":"with open('german-traffic-signs/train.p', 'rb') as f:\n    train_data = pickle.load(f)\nwith open('german-traffic-signs/valid.p', 'rb') as f:\n    val_data = pickle.load(f)\n# TODO: Load test data\nwith open('german-traffic-signs/test.p', 'rb') as f:\n    test_data = pickle.load(f)\n    \nX_train, y_train = train_data['features'], train_data['labels']\nX_val, y_val = val_data['features'], val_data['labels']\nX_test, y_test = test_data['features'], test_data['labels']\n    \nprint(X_train.shape)\nprint(X_test.shape)\nprint(X_val.shape)\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This step is just to make sure that I can come to know about any errors easily. One can skip this step if wanted","execution_count":null},{"metadata":{"id":"skd24mXbi2K-","trusted":true},"cell_type":"code","source":"assert(X_train.shape[0] == y_train.shape[0]), \"The number of images is not equal to the number of labels.\"\nassert(X_train.shape[1:] == (32,32,3)), \"The dimensions of the images are not 32 x 32 x 3.\"\nassert(X_val.shape[0] == y_val.shape[0]), \"The number of images is not equal to the number of labels.\"\nassert(X_val.shape[1:] == (32,32,3)), \"The dimensions of the images are not 32 x 32 x 3.\"\nassert(X_test.shape[0] == y_test.shape[0]), \"The number of images is not equal to the number of labels.\"\nassert(X_test.shape[1:] == (32,32,3)), \"The dimensions of the images are not 32 x 32 x 3.\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualising the data","execution_count":null},{"metadata":{"id":"YDp8qOSRi4p3","outputId":"937bdb6a-6bab-4b48-f70e-bc958f00d3c7","trusted":true},"cell_type":"code","source":"data = pd.read_csv('german-traffic-signs/signnames.csv')\nnum_of_samples=[]\n \ncols = 5\nnum_classes = 43\n \nfig, axs = plt.subplots(nrows=num_classes, ncols=cols, figsize=(5,50))\nfig.tight_layout()\n \nfor i in range(cols):\n    for j, row in data.iterrows():\n      x_selected = X_train[y_train == j]\n      axs[j][i].imshow(x_selected[random.randint(0,(len(x_selected) - 1)), :, :], cmap=plt.get_cmap('gray'))\n      axs[j][i].axis(\"off\")\n      if i == 2:\n        axs[j][i].set_title(str(j) + \" - \" + row[\"SignName\"])\n        num_of_samples.append(len(x_selected))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As we can see in graph below that this dataset is a little bit imbalanced.","execution_count":null},{"metadata":{"id":"OSDCGKohi9gm","outputId":"dc35fbe1-3fda-42e1-90f2-32b0c9160327","trusted":true},"cell_type":"code","source":"print(num_of_samples)\nplt.figure(figsize=(12, 4))\nplt.bar(range(0, num_classes), num_of_samples)\nplt.title(\"Distribution of the train dataset\")\nplt.xlabel(\"Class number\")\nplt.ylabel(\"Number of images\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using OpenCV2 for preprocessing the image. ","execution_count":null},{"metadata":{"id":"TDTLLZDSjTJi","outputId":"2a103222-435f-4c77-865f-4b49a9fe96bf","trusted":true},"cell_type":"code","source":"\nimport cv2\n \nplt.imshow(X_train[1000])\nplt.axis(\"off\")\nprint(X_train[1000].shape)\nprint(y_train[1000])\ndef grayscale(img):\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"id":"JiP5f0ttjVhK","outputId":"44e37137-1bf1-4a80-a01c-40c765ab4ecf","trusted":true},"cell_type":"code","source":"img = grayscale(X_train[1000])\nplt.imshow(img)\nplt.axis(\"off\")\nprint(img.shape)\ndef equalize(img):\n    img = cv2.equalizeHist(img)\n    return img\nimg = equalize(img)\nplt.imshow(img)\nplt.axis(\"off\")\nprint(img.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"4bLLEoNIjfHS","trusted":true},"cell_type":"code","source":"def preprocess(img):\n    img = grayscale(img)\n    img = equalize(img)\n    img = img/255\n    return img","execution_count":null,"outputs":[]},{"metadata":{"id":"kr57der3jo9a","outputId":"4c75d963-d1ff-4499-cb50-f6d80c7f9291","trusted":true},"cell_type":"code","source":"X_train = np.array(list(map(preprocess, X_train)))\nX_test = np.array(list(map(preprocess, X_test)))\nX_val = np.array(list(map(preprocess, X_val)))\n \nplt.imshow(X_train[random.randint(0, len(X_train) - 1)])\nplt.axis('off')\nprint(X_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"wx3YvThXjrvW","trusted":true},"cell_type":"code","source":"X_train = X_train.reshape(34799, 32, 32, 1)\nX_test = X_test.reshape(12630, 32, 32, 1)\nX_val = X_val.reshape(4410, 32, 32, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applying the preprocessing function to every image in dataset","execution_count":null},{"metadata":{"id":"kiSejXJ7jwik","outputId":"c836e5f9-df50-43c0-8342-14ae6fd0d69c","trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n \ndatagen = ImageDataGenerator(width_shift_range=0.1,\n                            height_shift_range=0.1,\n                            zoom_range=0.2,\n                            shear_range=0.1,\n                            rotation_range=10.)\n \ndatagen.fit(X_train)\nbatches = datagen.flow(X_train, y_train, batch_size = 15)\nX_batch, y_batch = next(batches)\n \nfig, axs = plt.subplots(1, 15, figsize=(20, 5))\nfig.tight_layout()\n \nfor i in range(15):\n    axs[i].imshow(X_batch[i].reshape(32, 32))\n    axs[i].axis(\"off\")\n \nprint(X_batch.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"usRPlfGfj2g1","trusted":true},"cell_type":"code","source":"y_train = to_categorical(y_train, 43)\ny_test = to_categorical(y_test, 43)\ny_val = to_categorical(y_val, 43)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining the CNN model. I have taken inspiration from the traditional LeNet architecture as I am curious how this old model performs on such a huge dataset.","execution_count":null},{"metadata":{"id":"1dIxBUdaj5e_","trusted":true},"cell_type":"code","source":" def modified_model():\n  model = Sequential()\n  model.add(Conv2D(60, (5, 5), input_shape=(32, 32, 1), activation='relu'))\n  model.add(Conv2D(60, (5, 5), activation='relu'))\n  model.add(MaxPooling2D(pool_size=(2, 2)))\n  \n  model.add(Conv2D(30, (3, 3), activation='relu'))\n  model.add(Conv2D(30, (3, 3), activation='relu'))\n  model.add(MaxPooling2D(pool_size=(2, 2)))\n  \n  model.add(Flatten())\n  model.add(Dense(500, activation='relu'))\n  model.add(Dropout(0.5))\n  model.add(Dense(43, activation='softmax'))\n  \n  model.compile(Adam(lr = 0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n  return model","execution_count":null,"outputs":[]},{"metadata":{"id":"-oAjOm9HkAnt","outputId":"589318ed-a105-4152-e141-aa75c88de214","trusted":true},"cell_type":"code","source":"model = modified_model()\nprint(model.summary())\n \nhistory = model.fit_generator(datagen.flow(X_train, y_train, batch_size=50),\n                            epochs=10,\n                            validation_data=(X_val, y_val), shuffle = 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"47NI0IqxkEWc","outputId":"df077b3e-20f8-44c3-ff41-b8b2c6cc3ea5","trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss')\nplt.xlabel('epoch')\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.legend(['training','test'])\nplt.title('Accuracy')\nplt.xlabel('epoch')\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Taking a random image from Internet to test my model","execution_count":null},{"metadata":{"id":"ZUBNQoavoYae","trusted":true},"cell_type":"code","source":"import requests\nfrom PIL import Image\nurl = 'https://thumbs.dreamstime.com/t/road-signs-main-road-sign-blue-background-road-signs-main-road-sign-blue-background-109436823.jpg'\nr = requests.get(url, stream=True)\nimg = Image.open(r.raw)\nplt.imshow(img, cmap=plt.get_cmap('gray'))","execution_count":null,"outputs":[]},{"metadata":{"id":"FTq6z_1pobMf","trusted":true},"cell_type":"code","source":"img = np.asarray(img)\nimg = cv2.resize(img, (32, 32))\nimg = preprocess(img)\nplt.imshow(img, cmap = plt.get_cmap('gray'))\nprint(img.shape)\nimg = img.reshape(1, 32, 32, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As seen below that the model is accuretly able to classify a random google image. This image belongs to the category 12, one can scroll up and find it in the data visualization part of this notebook","execution_count":null},{"metadata":{"id":"3GzjV1w0odny","trusted":true},"cell_type":"code","source":"print(\"predicted sign: \"+ str(model.predict_classes(img)))\n# the prediction is correct.","execution_count":null,"outputs":[]},{"metadata":{"id":"DlIINUepogv_"},"cell_type":"markdown","source":"**I used the traditional LeNet model architecture for classifing road symbols. The model seems to work very well as I achieved validation accuracy of around 99% which is really amazing. Also I tried certain random images from the Internet to classify, the model was accuretly able to classify them. \nI added only a single dropout layer which produced a very good accuracy. The best part of the model is that it has generalize the data very well. Would come back here and try to fine tune the model a little more to achieve 100% valadition accuracy.** \nAnyone who is interested can copy and edit my notebook and let me know the changes. If you like my work then please upvote. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### I am curious as to how this model can be deployed as an end node. I already am able to create a UI app for this but if any one have an idea as to how this can be deployed in heroku using flask then please let me know. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}