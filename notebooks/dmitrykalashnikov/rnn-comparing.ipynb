{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Simple RNN, LSTM and GRU implementation. \n# Pipeline for spam detection task solving, using pretrained glove embedding.","metadata":{}},{"cell_type":"markdown","source":"## Data preparing","metadata":{"execution":{"iopub.status.busy":"2021-09-03T10:34:42.325679Z","iopub.execute_input":"2021-09-03T10:34:42.326003Z","iopub.status.idle":"2021-09-03T10:34:42.332125Z","shell.execute_reply.started":"2021-09-03T10:34:42.325975Z","shell.execute_reply":"2021-09-03T10:34:42.331204Z"}}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport torch\nimport torch.nn as nn\nimport re\nimport fasttext.util\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader\nfrom tqdm.notebook import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-04T07:35:15.79273Z","iopub.execute_input":"2021-09-04T07:35:15.79307Z","iopub.status.idle":"2021-09-04T07:35:17.134711Z","shell.execute_reply.started":"2021-09-04T07:35:15.793038Z","shell.execute_reply":"2021-09-04T07:35:17.133788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_PATH = '/kaggle/input/sms-spam-collection-dataset/spam.csv'\ndata = pd.read_csv(INPUT_PATH)[['v1', 'v2']]\ndata['target'] = (data['v1'] == 'spam').astype(int)\ndata['v2'] = data['v2'].apply(lambda w : w.lower())\ndata['v2'] = data['v2'].apply(lambda w : re.sub(r'[^a-z]+', ' ', w))","metadata":{"execution":{"iopub.status.busy":"2021-09-04T07:35:17.790982Z","iopub.execute_input":"2021-09-04T07:35:17.791346Z","iopub.status.idle":"2021-09-04T07:35:17.884699Z","shell.execute_reply.started":"2021-09-04T07:35:17.791316Z","shell.execute_reply":"2021-09-04T07:35:17.883929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T07:35:18.466382Z","iopub.execute_input":"2021-09-04T07:35:18.466701Z","iopub.status.idle":"2021-09-04T07:35:18.482468Z","shell.execute_reply.started":"2021-09-04T07:35:18.466672Z","shell.execute_reply":"2021-09-04T07:35:18.481547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"not_null_ind = data['v2'].apply(lambda w : len(w.strip()) > 0)\nsentences = data['v2'][not_null_ind].values\npre_target = data['target'][not_null_ind].values.astype(int)\nwords = map(lambda w : w.split(), sentences)\nwords = list(filter(lambda w : len(w) > 0, words))","metadata":{"execution":{"iopub.status.busy":"2021-09-04T07:35:19.06049Z","iopub.execute_input":"2021-09-04T07:35:19.060802Z","iopub.status.idle":"2021-09-04T07:35:19.082213Z","shell.execute_reply.started":"2021-09-04T07:35:19.06077Z","shell.execute_reply":"2021-09-04T07:35:19.08136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda'","metadata":{"execution":{"iopub.status.busy":"2021-09-04T07:35:19.729188Z","iopub.execute_input":"2021-09-04T07:35:19.7295Z","iopub.status.idle":"2021-09-04T07:35:19.733675Z","shell.execute_reply.started":"2021-09-04T07:35:19.72947Z","shell.execute_reply":"2021-09-04T07:35:19.73266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nembeddings_dict = {}\nwith open(\"/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.50d.txt\", 'r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vector = np.asarray(values[1:], \"float32\")\n        embeddings_dict[word] = vector","metadata":{"execution":{"iopub.status.busy":"2021-09-04T07:35:20.136284Z","iopub.execute_input":"2021-09-04T07:35:20.1366Z","iopub.status.idle":"2021-09-04T07:35:29.387221Z","shell.execute_reply.started":"2021-09-04T07:35:20.13657Z","shell.execute_reply":"2021-09-04T07:35:29.386272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings = []\ntarget = []\nfor i, sentence in enumerate(words):\n    sent_emb = []\n    for word in sentence:\n        embedding_vector = embeddings_dict.get(word)\n        if embedding_vector is not None:\n            sent_emb.append([embedding_vector])\n    if len(sent_emb) > 0:\n        embeddings.append(torch.Tensor(sent_emb).to(device))\n        target.append(pre_target[i])","metadata":{"execution":{"iopub.status.busy":"2021-09-04T07:35:29.388665Z","iopub.execute_input":"2021-09-04T07:35:29.38919Z","iopub.status.idle":"2021-09-04T07:35:34.987544Z","shell.execute_reply.started":"2021-09-04T07:35:29.389148Z","shell.execute_reply":"2021-09-04T07:35:34.9867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(embeddings), len(target), embeddings[0].shape","metadata":{"execution":{"iopub.status.busy":"2021-09-04T07:35:36.471428Z","iopub.execute_input":"2021-09-04T07:35:36.471766Z","iopub.status.idle":"2021-09-04T07:35:36.476894Z","shell.execute_reply.started":"2021-09-04T07:35:36.471733Z","shell.execute_reply":"2021-09-04T07:35:36.476086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = torch.Tensor(y).to(device)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]","metadata":{"execution":{"iopub.status.busy":"2021-09-04T07:35:36.770825Z","iopub.execute_input":"2021-09-04T07:35:36.77115Z","iopub.status.idle":"2021-09-04T07:35:36.77722Z","shell.execute_reply.started":"2021-09-04T07:35:36.771119Z","shell.execute_reply":"2021-09-04T07:35:36.776256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data = CustomDataset(embeddings, target)\ntrain_dataloader = DataLoader(training_data, batch_size=1, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T07:35:38.367197Z","iopub.execute_input":"2021-09-04T07:35:38.367545Z","iopub.status.idle":"2021-09-04T07:35:38.372481Z","shell.execute_reply.started":"2021-09-04T07:35:38.367512Z","shell.execute_reply":"2021-09-04T07:35:38.371506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Simple RNN","metadata":{}},{"cell_type":"code","source":"class Head(nn.Module):\n    def __init__(self, input_size):\n        super(Head, self).__init__()\n        self.layer = nn.Linear(input_size, 1)\n        self.act = nn.Sigmoid()\n        \n    def forward(self, x):\n        return self.act(self.layer(x))","metadata":{"execution":{"iopub.status.busy":"2021-09-04T07:35:39.891384Z","iopub.execute_input":"2021-09-04T07:35:39.891732Z","iopub.status.idle":"2021-09-04T07:35:39.897009Z","shell.execute_reply.started":"2021-09-04T07:35:39.8917Z","shell.execute_reply":"2021-09-04T07:35:39.896005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RNN(nn.Module):\n    \n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNN, self).__init__()\n        self.Lo = nn.Linear(input_size + hidden_size, output_size)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x, hidden):\n        return self.tanh( self.Lo( torch.cat([x, hidden], dim=1) ) ) ","metadata":{"execution":{"iopub.status.busy":"2021-09-04T07:35:41.191026Z","iopub.execute_input":"2021-09-04T07:35:41.191394Z","iopub.status.idle":"2021-09-04T07:35:41.197198Z","shell.execute_reply.started":"2021-09-04T07:35:41.191362Z","shell.execute_reply":"2021-09-04T07:35:41.195905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hidden_size = 50\nepochs = 3\nrnn_model = RNN(embeddings[0][0][0].size(0), hidden_size, hidden_size).to(device)\nhead_model = Head(hidden_size).to(device)\noptimizer = torch.optim.Adam( list(rnn_model.parameters()) + list(head_model.parameters()) )\ncriterion = nn.BCEWithLogitsLoss()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T07:33:27.270918Z","iopub.status.idle":"2021-09-04T07:33:27.271676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(epochs):\n    \n    losses = []\n    i = 0\n    for X_in, y_in in train_dataloader:\n        \n        rnn_model.zero_grad()\n        head_model.zero_grad()\n\n        ht = torch.zeros(1, hidden_size).to(device)\n\n        for i, x_in in enumerate(X_in[0]):\n            ht = rnn_model(x_in, ht)\n\n        out = head_model(ht)\n        loss = criterion( out, y_in.unsqueeze(1) )\n        \n        loss.backward()\n        optimizer.step()\n        \n        if not np.isnan(loss.item()):\n            losses.append(loss.item())\n        \n    print(np.mean(losses))","metadata":{"execution":{"iopub.status.busy":"2021-09-03T09:47:07.283213Z","iopub.execute_input":"2021-09-03T09:47:07.283568Z","iopub.status.idle":"2021-09-03T09:48:28.233174Z","shell.execute_reply.started":"2021-09-03T09:47:07.283535Z","shell.execute_reply":"2021-09-03T09:48:28.231385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LSTM","metadata":{}},{"cell_type":"code","source":"class LSTM(nn.Module):\n    \n    def __init__(self, input_dim, hidden_dim):\n        super(LSTM, self).__init__()\n        \n        self.forget_gate = nn.Sequential(\n            nn.Linear( input_dim + hidden_dim, hidden_dim ),\n            nn.Sigmoid()\n        )\n        self.input_gate = nn.Sequential(\n            nn.Linear( input_dim + hidden_dim, hidden_dim ),\n            nn.Sigmoid()\n        )\n        self.new_info = nn.Sequential(\n            nn.Linear( input_dim + hidden_dim, hidden_dim ),\n            nn.Tanh()\n        )\n        self.output_gate = nn.Sequential(\n            nn.Linear( input_dim + hidden_dim, hidden_dim ),\n            nn.Sigmoid()\n        )\n        self.new_hidden = nn.Sequential(\n            nn.Linear( hidden_dim, hidden_dim ),\n            nn.Tanh()\n        )\n        \n        self.head = nn.Sequential(\n            nn.Linear( hidden_dim, 1 ),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x, hidden, cell):\n        combined = torch.cat( [x, hidden], 1 )\n        old_info = self.forget_gate(combined) * cell\n        new_info = self.input_gate(combined) * self.new_info(combined)\n        new_cell_state = old_info + new_info\n        new_hidden = self.output_gate(combined) * self.new_hidden(new_cell_state)\n        output = self.head(new_hidden)\n        return new_hidden, new_cell_state, output","metadata":{"execution":{"iopub.status.busy":"2021-09-04T07:35:44.283968Z","iopub.execute_input":"2021-09-04T07:35:44.284331Z","iopub.status.idle":"2021-09-04T07:35:44.293326Z","shell.execute_reply.started":"2021-09-04T07:35:44.284299Z","shell.execute_reply":"2021-09-04T07:35:44.292289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hidden_size = 50\nepochs = 3\nlstm_model = LSTM(embeddings[0][0][0].size(0), hidden_size).to(device)\noptimizer = torch.optim.Adam( lstm_model.parameters() )\ncriterion = nn.BCEWithLogitsLoss()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T09:49:00.630263Z","iopub.execute_input":"2021-09-03T09:49:00.630628Z","iopub.status.idle":"2021-09-03T09:49:00.639496Z","shell.execute_reply.started":"2021-09-03T09:49:00.630593Z","shell.execute_reply":"2021-09-03T09:49:00.63871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, optimizer, train_dataloader):\n    for epoch in range(epochs):\n        losses = []\n        for X_in, y_in in train_dataloader:\n\n            model.zero_grad()\n            ht = torch.zeros(1, hidden_size).to(device)\n            ct = torch.zeros(1, hidden_size).to(device)\n\n            for i, x_in in enumerate(X_in[0]):\n                ht, ct, output = model(x_in, ht, ct)\n\n            loss = criterion( output, y_in.unsqueeze(1) )\n\n            loss.backward()\n            optimizer.step()\n\n            if not np.isnan(loss.item()):\n                losses.append(loss.item())\n            else:\n                print('nan')\n\n        print(np.mean(losses))\n\n    return model, optimizer, losses","metadata":{"execution":{"iopub.status.busy":"2021-09-03T10:06:28.967216Z","iopub.execute_input":"2021-09-03T10:06:28.967574Z","iopub.status.idle":"2021-09-03T10:06:28.974804Z","shell.execute_reply.started":"2021-09-03T10:06:28.967541Z","shell.execute_reply":"2021-09-03T10:06:28.973855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = train(lstm_model, optimizer, train_dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T10:06:29.785074Z","iopub.execute_input":"2021-09-03T10:06:29.785419Z","iopub.status.idle":"2021-09-03T10:12:10.011791Z","shell.execute_reply.started":"2021-09-03T10:06:29.78537Z","shell.execute_reply":"2021-09-03T10:12:10.010969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GRU","metadata":{}},{"cell_type":"code","source":"class GRU(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(GRU, self).__init__()\n        self.reset_gate = nn.Sequential(\n            nn.Linear(input_dim + hidden_dim, hidden_dim),\n            nn.Sigmoid()\n        )\n        self.new_info = nn.Sequential(\n            nn.Linear(input_dim + hidden_dim, hidden_dim),\n            nn.Tanh()\n        )\n        self.update_gate = nn.Sequential(\n            nn.Linear(input_dim + hidden_dim, hidden_dim),\n            nn.Sigmoid()\n        )\n        \n        self.head = nn.Sequential(\n            nn.Linear(hidden_dim, 1),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x, hidden):\n        combined = torch.cat( [x, hidden], 1 )\n        forget_hidden = self.reset_gate(combined) * hidden\n        new_combined = torch.cat( [x, forget_hidden], 1 )\n        updated = self.update_gate(combined)\n        new_hidden = hidden * (1 - updated) + self.new_info(new_combined) * updated\n        output = self.head(new_hidden)\n        return new_hidden, output","metadata":{"execution":{"iopub.status.busy":"2021-09-04T07:44:46.80746Z","iopub.execute_input":"2021-09-04T07:44:46.807801Z","iopub.status.idle":"2021-09-04T07:44:46.815451Z","shell.execute_reply.started":"2021-09-04T07:44:46.807769Z","shell.execute_reply":"2021-09-04T07:44:46.814649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hidden_size = 50\nepochs = 3\ngru_model = GRU(embeddings[0][0][0].size(0), hidden_size).to(device)\noptimizer = torch.optim.Adam( gru_model.parameters() )\ncriterion = nn.BCEWithLogitsLoss()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T10:26:27.746884Z","iopub.execute_input":"2021-09-03T10:26:27.747256Z","iopub.status.idle":"2021-09-03T10:26:27.75714Z","shell.execute_reply.started":"2021-09-03T10:26:27.747226Z","shell.execute_reply":"2021-09-03T10:26:27.756275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, optimizer, train_dataloader):\n    for epoch in range(epochs):\n        losses = []\n        for X_in, y_in in train_dataloader:\n\n            model.zero_grad()\n            ht = torch.zeros(1, hidden_size).to(device)\n\n            for i, x_in in enumerate(X_in[0]):\n                ht, output = model(x_in, ht)\n\n            loss = criterion( output, y_in.unsqueeze(1) )\n\n            loss.backward()\n            optimizer.step()\n\n            if not np.isnan(loss.item()):\n                losses.append(loss.item())\n            else:\n                print('nan')\n\n        print(np.mean(losses))\n\n    return model, optimizer, losses","metadata":{"execution":{"iopub.status.busy":"2021-09-03T10:26:28.099138Z","iopub.execute_input":"2021-09-03T10:26:28.099472Z","iopub.status.idle":"2021-09-03T10:26:28.106686Z","shell.execute_reply.started":"2021-09-03T10:26:28.09944Z","shell.execute_reply":"2021-09-03T10:26:28.105588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = train(gru_model, optimizer, train_dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T10:26:28.926014Z","iopub.execute_input":"2021-09-03T10:26:28.926354Z","iopub.status.idle":"2021-09-03T10:31:03.720165Z","shell.execute_reply.started":"2021-09-03T10:26:28.926324Z","shell.execute_reply":"2021-09-03T10:31:03.719303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Attention","metadata":{}},{"cell_type":"markdown","source":"Just a scheme for attention in RNN implementation. You need to specify vocabulary, get_word_by_prob function, get_embedding_by_word function and reform train data to sequences of words.","metadata":{}},{"cell_type":"code","source":"def get_word_by_prob(probs):\n    pass\ndef get_embedding_by_word(word):\n    pass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionGRU(nn.Module):\n    def __init__(self, input_size, hidden_size, seq_len, vocab_size):\n        super(AttentionLSTM, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.seq_len = seq_len\n        self.vocab_size = vocab_size\n        \n        self.enc = GRU(input_size, hidden_size)\n        self.dec = GRU(input_size, hidden_size)\n        self.proj = nn.Sequential(\n            nn.Linear(hidden_size, seq_len),\n            nn.Softmax(seq_len)\n        )\n        self.vocab_proj = nn.Sequential(\n            nn.Linear(hidden_size, vocab_size),\n            nn.Softmax(vocab_size)\n        )\n        \n    def forward(self, seq_x, start_emb):\n        assert len(seq_x) == self.seq_len\n        hs = []\n        hidden = torch.zeros((1, self.hidden_size)).to(device)\n        \n        for x in seq_x:\n            hidden = self.enc(x, hidden)\n            hs.append(hidden)\n        \n        hs = torch.Tensor(hs).to(device)\n        \n        hidden = torch.zeros((1, self.hidden_size)).to(device)\n        prev = start_emb\n        outputs = []\n        for i in range(self.seq_len):\n            hidden = self.dec(prev, hidden)\n            input_mask = self.proj(hidden)\n            att_vector = (input_mask * hs).sum(dim=-1)\n            output = self.vocab_proj(att_vector)\n            \n            word = get_word_by_prob(output)\n            outputs.append(word)\n            \n            prev = get_embedding_by_word(word)","metadata":{},"execution_count":null,"outputs":[]}]}