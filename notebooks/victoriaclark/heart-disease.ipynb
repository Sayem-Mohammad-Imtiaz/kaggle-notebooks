{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Heart Disease Prediction","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries and Load Data","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.inspection import permutation_importance\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Load data\ndf = pd.read_csv(\"../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\")\ndf.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-22T14:44:58.913664Z","iopub.execute_input":"2021-05-22T14:44:58.914158Z","iopub.status.idle":"2021-05-22T14:45:00.519199Z","shell.execute_reply.started":"2021-05-22T14:44:58.914025Z","shell.execute_reply":"2021-05-22T14:45:00.518446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clean the Data","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:45:00.520649Z","iopub.execute_input":"2021-05-22T14:45:00.521173Z","iopub.status.idle":"2021-05-22T14:45:00.543338Z","shell.execute_reply.started":"2021-05-22T14:45:00.521135Z","shell.execute_reply":"2021-05-22T14:45:00.541878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop rows with NaN values\ndf = df.dropna()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:45:00.545555Z","iopub.execute_input":"2021-05-22T14:45:00.546202Z","iopub.status.idle":"2021-05-22T14:45:00.577175Z","shell.execute_reply.started":"2021-05-22T14:45:00.546146Z","shell.execute_reply":"2021-05-22T14:45:00.575983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#drop variables that will not be used in model\ndf =df.drop(['id','work_type','ever_married'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:45:00.579425Z","iopub.execute_input":"2021-05-22T14:45:00.579888Z","iopub.status.idle":"2021-05-22T14:45:00.589815Z","shell.execute_reply.started":"2021-05-22T14:45:00.57984Z","shell.execute_reply":"2021-05-22T14:45:00.58852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View statistics of numerical data\ndf.describe(include = 'all')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:45:00.591893Z","iopub.execute_input":"2021-05-22T14:45:00.592619Z","iopub.status.idle":"2021-05-22T14:45:00.648047Z","shell.execute_reply.started":"2021-05-22T14:45:00.592567Z","shell.execute_reply":"2021-05-22T14:45:00.64691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To include categorical data into the model. use dummies to turn these fields into binary true false values with dummies. This changes the data from 9 columns to 15 columns.","metadata":{}},{"cell_type":"code","source":"#Create non-numerical data dummies\ndf = pd.get_dummies(df)\n\ndf.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:45:00.649704Z","iopub.execute_input":"2021-05-22T14:45:00.65014Z","iopub.status.idle":"2021-05-22T14:45:00.739127Z","shell.execute_reply.started":"2021-05-22T14:45:00.650093Z","shell.execute_reply":"2021-05-22T14:45:00.737672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To get a better image of the data, this look creates a distribution plot of all the features. It shows the frequency histogram of our numerical variables and how our categorical data is separated.","metadata":{}},{"cell_type":"code","source":"#Plotting the distribution plot.\nplt.figure(figsize=(20,25))\nplotnumber=1\n\nfor column in df:\n  if plotnumber<15:\n    ax=plt.subplot(4,4,plotnumber)\n    sns.distplot(df[column])\n    plt.xlabel(column,fontsize=20)\n    plt.ylabel('Values',fontsize=20)\n  plotnumber+=1\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:45:00.741799Z","iopub.execute_input":"2021-05-22T14:45:00.74221Z","iopub.status.idle":"2021-05-22T14:45:04.003572Z","shell.execute_reply.started":"2021-05-22T14:45:00.742161Z","shell.execute_reply":"2021-05-22T14:45:04.001907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before we build our model, it is important to check the correlation coefficients of our features. When a correlation coefficient is close to 0, there is a weaker relationship. When a correlation is strong, it affects the models' ability to estimate the relationship between each independent and dependant variable","metadata":{}},{"cell_type":"code","source":"#Correlation matrix\n\nplt.figure(figsize = (16, 8))\n\ncorr = df.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\nsns.heatmap(corr, cmap='YlGnBu',mask = mask, annot = True, fmt = '.2g', linewidths = 1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:45:04.00677Z","iopub.execute_input":"2021-05-22T14:45:04.007319Z","iopub.status.idle":"2021-05-22T14:45:04.888077Z","shell.execute_reply.started":"2021-05-22T14:45:04.00726Z","shell.execute_reply":"2021-05-22T14:45:04.887058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To train and test the accuracy of our model, we split the data. 80% of the records will be used to train our model to evaluate if an individual has heart disease or not. Then the remaining 20% of records are used to test how well the model performs.","metadata":{}},{"cell_type":"markdown","source":"To ensure our numeric features are being measured on a common scale, normalize the data set to before training the model.","metadata":{}},{"cell_type":"code","source":"# Split data into inputs and target result\ninput = df.drop([\"heart_disease\"], axis=1)\ntarget = df[\"heart_disease\"]","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:45:04.889538Z","iopub.execute_input":"2021-05-22T14:45:04.889814Z","iopub.status.idle":"2021-05-22T14:45:04.896738Z","shell.execute_reply.started":"2021-05-22T14:45:04.889785Z","shell.execute_reply":"2021-05-22T14:45:04.89579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize the data\nscaler = StandardScaler()\ninput_scaled = scaler.fit_transform(input)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:45:04.898112Z","iopub.execute_input":"2021-05-22T14:45:04.898486Z","iopub.status.idle":"2021-05-22T14:45:04.916386Z","shell.execute_reply.started":"2021-05-22T14:45:04.898449Z","shell.execute_reply":"2021-05-22T14:45:04.915122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into train and test sets with a 80/20 ratio\nX_train, X_test, y_train, y_test = train_test_split(input_scaled, target, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:45:04.91818Z","iopub.execute_input":"2021-05-22T14:45:04.91873Z","iopub.status.idle":"2021-05-22T14:45:04.932166Z","shell.execute_reply.started":"2021-05-22T14:45:04.918676Z","shell.execute_reply":"2021-05-22T14:45:04.930856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Classifier","metadata":{}},{"cell_type":"markdown","source":"Train the model, because our target output is to determine if an indivdual has heart disease or not, we use a classifier to predict which category to assign. After training the model, we can see which features we included have the most significant impact. features being closer to 1 having the most importance. this can be used to simplify the model if a feature appears irrelevant.","metadata":{}},{"cell_type":"code","source":"# Fit a random forest classifier on train set and predict on test set\nclf = RandomForestClassifier(random_state=0)\nclf.fit(X_train, y_train)\n\ntarget_predict = clf.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:45:04.933538Z","iopub.execute_input":"2021-05-22T14:45:04.9346Z","iopub.status.idle":"2021-05-22T14:45:05.529422Z","shell.execute_reply.started":"2021-05-22T14:45:04.934539Z","shell.execute_reply":"2021-05-22T14:45:05.528164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Show Feature Importance\nclf_summary = pd.DataFrame(input.columns.values, columns=['Features'])\nclf_summary['weight'] = clf.feature_importances_\nclf_summary.sort_values","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:45:05.530923Z","iopub.execute_input":"2021-05-22T14:45:05.531244Z","iopub.status.idle":"2021-05-22T14:45:05.554721Z","shell.execute_reply.started":"2021-05-22T14:45:05.531214Z","shell.execute_reply":"2021-05-22T14:45:05.553653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Accuracy","metadata":{}},{"cell_type":"markdown","source":"Now test the accuracy of our logic on the test data. we can see the current model is 95% accurate","metadata":{}},{"cell_type":"code","source":"# Output classification metrics\nprint(classification_report(y_test, target_predict))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:45:05.556163Z","iopub.execute_input":"2021-05-22T14:45:05.556464Z","iopub.status.idle":"2021-05-22T14:45:05.569576Z","shell.execute_reply.started":"2021-05-22T14:45:05.556434Z","shell.execute_reply":"2021-05-22T14:45:05.568276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below we can see where the model miscategorized the data. This model was wrong on 4 records. 3 of the records were labeled for no heart disease and only 1 recorded was wrongly predicted to have heart disease.","metadata":{}},{"cell_type":"code","source":"conf_matrix = pd.crosstab(y_test,target_predict)\nconf_matrix","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:45:05.571301Z","iopub.execute_input":"2021-05-22T14:45:05.571646Z","iopub.status.idle":"2021-05-22T14:45:05.605376Z","shell.execute_reply.started":"2021-05-22T14:45:05.571612Z","shell.execute_reply":"2021-05-22T14:45:05.604251Z"},"trusted":true},"execution_count":null,"outputs":[]}]}