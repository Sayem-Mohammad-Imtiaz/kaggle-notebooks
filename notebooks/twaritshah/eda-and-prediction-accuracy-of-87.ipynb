{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df= pd.read_csv('/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dividing dataset into numeric and class dataset**","metadata":{}},{"cell_type":"code","source":"num_cols= ['age', 'trtbps', 'chol', 'thalachh', 'oldpeak']\nclass_cols= ['sex', 'cp', 'fbs', 'restecg', 'exng', 'slp', 'caa', 'thall']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize= (15,10))\nsns.heatmap(df.corr(),annot= True, cmap= 'coolwarm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking the Distribution of Data**","metadata":{}},{"cell_type":"code","source":"df.hist(figsize=(20,20))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating BoxPlot to check outliers in highly co-related data","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\n\nplt.subplot(4,2,1)\nsns.boxplot(x= df['output'], y=df['cp'] )\nplt.title('Chest Pain vs Output')\n\nplt.subplot(4,2,2)\nsns.boxplot(x= df['output'], y= df['thalachh'])\nplt.title('Maximum Heart Rate vs Output')\n\nplt.subplot(4,2,3)\nsns.boxplot(x= df['output'], y= df['oldpeak'])\nplt.title('Old Peak vs Output')\n\nplt.subplot(4,2,4)\nsns.boxplot(x= df['output'], y= df['exng'])\nplt.title('exercise induced angina vs Output')\n\nplt.subplot(4,2,5)\nsns.boxplot(x= df['output'], y= df['sex'])\nplt.title('Sex vs Output')\n\nplt.subplot(4,2,6)\nsns.boxplot(x= df['output'], y= df['age'])\nplt.title('Age vs Output')\n\nplt.subplot(4,2,7)\nsns.boxplot(x= df['output'], y= df['trtbps'])\nplt.title('Blood Pressure vs Output')\n\nplt.subplot(4,2,8)\nsns.boxplot(x= df['output'], y= df['slp'])\nplt.title('Slope vs Output')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Heart Attack is highly positively co-related to Chest Pain(cp)\n* Heart Attack is positively co-related to Maximum Heart Rate achieved(thalachh)\n* Heart Attack is negatively co-related to OldPeak and EXNG\n* People between Age 50-70 more prone to heart attack\n* People with chest pain type 0 have highest risk of heart attack\n* People with cholestrol level 200-300 have very high risk\n* People with max heart rate 150-175 have high risk\n* People with thaal rate 2 and 3 have high risk","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Creating Models","metadata":{}},{"cell_type":"code","source":"X= df.drop('output', axis=1)\ny= df['output']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test= train_test_split(X, y, test_size= 0.2, stratify=y, random_state= 100)\n\nss= StandardScaler()\nX_train= ss.fit_transform(X_train)\nX_test= ss.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"key= ['LogisticRegression', 'DecisionTreeRegressor', 'DecisionTreeClassifier',  'RandomForestClassifier', 'KNeighborsClassifier', ]\n\nvalue= [LogisticRegression(), DecisionTreeRegressor() , DecisionTreeClassifier() ,  RandomForestClassifier() ,  KNeighborsClassifier() ]\nmodels= dict(zip(key, value))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores= []\nfor keys, value in models.items():\n    score= -1*cross_val_score(value, X, y,  cv=5, scoring= 'neg_mean_absolute_error' )\n    scores.append(score)\n    print(value, score.mean())\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_scores= []\nfor key, value in models.items():\n    value.fit(X_train, y_train)\n    y_pred= value.predict(X_test)\n    accuracy= accuracy_score(y_test, y_pred)\n    accuracy_scores.append(accuracy)\n    print(key, accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" **Logistic Regression and RandomForest Classifier are the best models to predict **","metadata":{}},{"cell_type":"code","source":"lr= LogisticRegression()\n\nlr.fit(X_train, y_train)\ny_pred= lr.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc= RandomForestClassifier(random_state= 50)\nrfc.fit(X_train, y_train)\ny_pred= rfc.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**IT IS CLEAR THAT RANDOMFORESTCLASSIFIER IS BEST MODEL FOR THIS PROBLEM**","metadata":{}},{"cell_type":"markdown","source":"# Now we do some Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"params= {'max_depth':np.arange(2,10,1),\n        'n_estimators': [10,100,200],\n        'max_features':[10,100,500],\n        }\n\ngrid= GridSearchCV(rfc, param_grid= params, cv= 5)\ngrid.fit(X_train, y_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid.best_estimator_.fit(X_train, y_train)\ny_predcv= grid.best_estimator_.predict(X_test)\n\nprint(classification_report(y_test, y_predcv))\nprint(confusion_matrix(y_test, y_predcv))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Hence, we get an accuracy of 87% on test set with RandomForestClassifier","metadata":{}}]}