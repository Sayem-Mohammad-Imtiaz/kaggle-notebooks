{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID-19 Risk Factors Literature Clustering"},{"metadata":{},"cell_type":"markdown","source":"## Authors:\n\n* **Anthony Mazzulli** (4278070) - anthony.mazzulli@roche.com\n* **Ju Zhang** (177660) - ju.zhang.jz1@roche.com\n* **Mark Yan** (4742456) - mark.yan@roche.com\n* **Yanling Jin** (1908340) - yanling.jin@roche.com"},{"metadata":{},"cell_type":"markdown","source":"## Objective:\n\n**Identify available literature on risk factors and cluster risk papers based on risk factors discussed.**"},{"metadata":{},"cell_type":"markdown","source":"## Approach:\n\n* Build a supervised machine learning model to predict whether a given article discusses COVID-19 risk factors\n    * Data processing\n        * Selected articles published within 2019-2020\n        * Selected articles with keywords “coronavirus” or “COVID”\n        * Tokenized abstracts\n        * Removed punctuation and numbers\n        * Transformed words to lowercase\n        * Removed stop words\n        * Created bag of words and 2-grams\n    * Manually label a subset of 352 articles based on whether they discuss risk factors in their abstracts\n        * Separated the articles into training (N=281) and testing (N=71)\n    * Train different models using the 352 labelled articles to predict whether the rest of the papers discuss risk factors and label those articles accordingly, select the best performed model to conduct clustering, and re-train the model with labelled data\n        * Selected models include\n            * Logistic regression\n            * Random forest\n            * Multi-layer perceptron classifier\n            * Tf-idf adjusted model\n\n\n* Use LDA clustering, an unsupervised machine learning model, to cluster risk-factor related articles based on discussed risk factor categories and visualize the top salient terms\n    * Targeted risk-factor categories include\n        * Data on potential risks factors\n            * Smoking, pre-existing pulmonary disease\n            * Co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities\n            * Neonates and pregnant women\n            * Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\n        * Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\n        * Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\n        * Susceptibility of populations\n        * Public health mitigation measures that could be effective for control"},{"metadata":{},"cell_type":"markdown","source":"## Summary:\n\n**MLP with Tf-idf adjusted uni- and bi-gram model performance the best. Its accuracy on the test dataset reached 87.3%. LDA clustering results, based on clustering targeted risk-factor groups, showed promising results.** Some clusters we identified are directly associated with the key questions of interests: \n\n* Data on potential risks factors\n  * Smoking, pre-existing pulmonary disease  ---- ***Cluster 8***\n  * Co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities\n  * Neonates and pregnant women ---- ***Cluster 9***\n  * Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\n* Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors  ---- ***Cluster 4***\n* Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups    ---- ***Cluster 2 and 5***\n* Susceptibility of populations  ---- ***Cluster 3***\n* Public health mitigation measures that could be effective for control   ---- ***Cluster 3 and 8***"},{"metadata":{},"cell_type":"markdown","source":"","attachments":{}},{"metadata":{},"cell_type":"markdown","source":"## Future steps for model performance improvement:\n\n* Remove common words, but not related to the risk-factors of COVID-19, and re-train the model \n    * E.g., name of months, 'study' , 'year' from the datasets\n* Deliberately include rare articles of interest into the model training process to help program pick up the rare cases more accurately, instead of random sampling\n    * E.g., economic impact of COVID-19"},{"metadata":{},"cell_type":"markdown","source":"## Table of Contents:\n\n<p> 1 Loading Data </p>\n<p> 2 Data pre-processing </p>\n<p style=\"text-indent: 40px\"> a Filtering and cleaning </p>\n<p style=\"text-indent: 40px\"> b Tokenization, Lemmatization, and 2-grams  </p>\n<p style=\"text-indent: 40px\"> c Manual Annotation  </p>\n3 Classifying Articles - Risk or No Risk </p>\n<p style=\"text-indent: 40px\"> a Testing Models </p>\n<p style=\"text-indent: 80px\"> i Vectorization </p>\n<p style=\"text-indent: 80px\"> ii Logistic Regression </p>\n<p style=\"text-indent: 80px\"> iii Random Forest </p>\n<p style=\"text-indent: 80px\"> iv MLP </p>\n<p style=\"text-indent: 80px\"> v Tf-idf Adjusted Models </p>\n<p style=\"text-indent: 40px\"> b Classifying Remaining Articles with Tf-idf Adjusted MLP Model </p>\n<p>4 Clustering Articles by Risk Factors using LDA </p>\n<p>5 Visualizing LDA Clustering </p>"},{"metadata":{},"cell_type":"markdown","source":"# 1 Loading the Data:"},{"metadata":{},"cell_type":"markdown","source":"**Import nucessary packages:**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re # regular expressions\nimport nltk\nimport random # for random sampling 300 abstracts for annotation. Can be removed later\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neural_network import MLPClassifier\nimport matplotlib.pyplot as plt\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**View file directory:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls /kaggle/input/CORD-19-research-challenge/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Import Metadata:**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"root_path = '/kaggle/input/CORD-19-research-challenge/'\nmetadata_path = f'{root_path}/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, parse_dates=['publish_time'], dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2 Data Pre-Processing:"},{"metadata":{},"cell_type":"markdown","source":"## a Filtering and Cleaning:"},{"metadata":{},"cell_type":"markdown","source":"**Filter on Years 2019, 2020:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df_new = meta_df[meta_df['publish_time'] >= '2019-01-01']\nmeta_df_new.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Filter on articles that talk about coronavirus or COVID only:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"abstracts_new = meta_df_new[['cord_uid', 'abstract']].dropna() # create new df with only id and abstract\nabout_coronavirus = abstracts_new['abstract'].apply(lambda x: ('coronavirus' in x.lower() or 'covid' in x.lower())) # create condition that abstract contains 'coronavirus' or 'COVID'\nabstracts_new = abstracts_new[about_coronavirus] # filter abstracts based on about_coronavirus condition\nabstracts_new.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abstracts_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Remove punctuation and numbers from abstracts:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"abstracts_new['tokens'] = abstracts_new['abstract'].apply(lambda x: re.sub('[^a-zA-z\\s]',' ',x)) # remove punctuation and numbers from abstract text\nabstracts_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tokenize abstracts into list of lowercase words:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"abstracts_new['tokens'] = abstracts_new['tokens'].apply(lambda x: word_tokenize(x.lower())) #tokenize lowercase words\nabstracts_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Remove stop words:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a function to remove stop words from a list of words\ndef remove_stopwords(text):\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abstracts_new['tokens'] = abstracts_new['tokens'].apply(lambda x: remove_stopwords(x)) #remove stopwords\nabstracts_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Remove word 'abstract' from token list (many abstracts begin with this word)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a function to remove the word 'abstract' from a list of words\ndef remove_abstract(text):\n    words = [w for w in text if w != 'abstract']\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abstracts_new['tokens'] = abstracts_new['tokens'].apply(lambda x: remove_abstract(x)) #remove the word 'abstract' from token list\nabstracts_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## b Tokenization, Lemmatization, and 2-grams:"},{"metadata":{},"cell_type":"markdown","source":"**Lemmatize tokens:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a function to lematize a list of tokens\nlemmatizer = WordNetLemmatizer() # instantiate a lemmatizer\ndef lemmatize_tokens(tokens):\n    lemmatized_tokens = [lemmatizer.lemmatize(t) for t in tokens]\n    return lemmatized_tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abstracts_new['tokens'] = abstracts_new['tokens'].apply(lambda x: lemmatize_tokens(x)) #lemmatize words\nabstracts_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Create a bag of words for each abstract:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"abstracts_new['bow'] = abstracts_new['tokens'].apply(lambda x: Counter(x)) # create new column called 'bow' that translates lemmatized tokens into bag of words \nabstracts_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Create a column for 2-grams:**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a function that converts a list of tokens into n-grams\ndef getNGrams(tokens, n):\n    return [tokens[i : i + n] for i in range(len(tokens) - (n - 1))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abstracts_new['2-grams'] = abstracts_new['tokens'].apply(lambda x: getNGrams(x, 2))\nabstracts_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abstracts_new.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## c Manual annotation\nWe randomly sampled 352 abstracts from the collection of filtered abstracts. The manual annotation was done by reading the 352 abstracts and label them as either related to this risk-factor task or not. The criteria we used in the manual process for classifying an abstract as risk-factor related are based on the Task Details, i.e.: \n* Data on potential risks factors\n  * Smoking, pre-existing pulmonary disease\n  * Co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities\n  * Neonates and pregnant women\n  * Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\n* Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\n* Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\n* Susceptibility of populations\n* Public health mitigation measures that could be effective for control\n\nAfter manual annotation, we have also split the 352 abstracts into training (N = 281) and test (N = 71) sets stratified by the annotation label. The annotation labels and and training/test identifiers are stored in [this dataset](https://www.kaggle.com/qingxiangyan/covid19-data-for-modeling). This dataset is used for model building.  "},{"metadata":{},"cell_type":"markdown","source":"# 3 Classifying Articles - Risk or No Risk "},{"metadata":{},"cell_type":"markdown","source":"**Read in data and get training and test samples:**  \n\nThe variable \"risk-factor\" contains the annotation label ('Yes' = risk-factor related, and 'No' = otherwise). And the variable \"split_label\" contains the training/test identifier. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# read in data\nmodel_data_df = pd.read_csv('../input/covid19-data-for-modeling/Metadata_for_modeling.csv',encoding = \"ISO-8859-1\")\n# get training and test samples\ntrain_test = model_data_df[model_data_df['split_label'].notna()]\ntrain_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training set\ntrain = train_test[train_test['split_label']==\"Training\"]\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test set\ntest = train_test[train_test['split_label']==\"Test\"]\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## a Testing Models"},{"metadata":{},"cell_type":"markdown","source":"During the model building phase, we have tried different ways of engineering features from text:\n1. Bag of words (unigram)\n2. A mix of unigram and bigram (unigram + bigram) \n3. Tf-idf weighted unigram and unigram + bigram \n\nDifferent classification methods we have tried are: \n1. Logistic regression\n2. Random forrest\n3. Multi-layer Perceptron classifier \n\nModels will be trained in the training set and their performances will be compared in the test set. "},{"metadata":{},"cell_type":"markdown","source":"### I Vectorization"},{"metadata":{},"cell_type":"markdown","source":"**Bag of word**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test_tmp = train_test\n\n#train Vectorizer the entire training+test set\nwords = set(nltk.corpus.words.words())\n\nvectorizer = CountVectorizer(analyzer = \"word\",   \\\n                             tokenizer = None,    \\\n                             preprocessor = None, \\\n                             stop_words = None,   \\\n                             max_features = 2000) \n                             \nvectorizer.fit(train_test_tmp['tokens'])\n\n#Vectorizing training set\nx_train = vectorizer.transform(train['tokens'])\nx_train = x_train.toarray()\n\n# Top words in the trianing set\nword_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(x_train.sum(axis=0))})\nword_count.sort_values('count', ascending=False).set_index('word')[:30].sort_values('count', ascending=True).plot(kind='barh')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vectorizing test set\nx_test = vectorizer.transform(test['tokens'])\nx_test = x_test.toarray()\n\n# Top words in the test set\nword_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(x_test.sum(axis=0))})\nword_count.sort_values('count', ascending=False).set_index('word')[:30].sort_values('count', ascending=True).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### II Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train a simple logistic regression\nlogisticCV = LogisticRegressionCV(cv=5, random_state=19, max_iter = 10000).fit(x_train, train['risk_factor'])\n# Training performance\nbow_lr_train = logisticCV.score(x_train, train['risk_factor']) \nprint(bow_lr_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test performance\nbow_lr_test = logisticCV.score(x_test, test['risk_factor'])  \nprint(bow_lr_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### III Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train a random forest model\n\n# Select hyperparameter through cross-validation. To shorten the process time of this notebook, this step is only run once and the results will be used directly.  \n# param_grid = {\n#                 'n_estimators': [50,100,150,200,500],\n#                 'max_depth': list(range(1,20, 2))\n#             }\n\n# clf = RandomForestClassifier(random_state=19)\n\n# grid_clf = GridSearchCV(clf, param_grid, cv=5)\n\n# grid_clf.fit(x_train, train['risk_factor'])\n\n# grid_clf.best_estimator_\n\nforestCV = RandomForestClassifier(n_estimators = 100, max_depth=17, random_state=19) \nforestCV = forestCV.fit(x_train, train['risk_factor'])\n\n# Training performance\nbow_rf_train  = forestCV.score(x_train, train['risk_factor']) \nprint(bow_rf_train)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test performance\nbow_rf_test = forestCV.score(x_test, test['risk_factor']) \nprint(bow_rf_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### IV MLP"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train an MLP model\n\n\n# Select hyper parameter through cross-validation. To shorten the process time of this notebook, this step is only run once and the results will be used directly.  \n\n# param_grid = {'hidden_layer_sizes': [(20,10),(10,5),(4,2)]}\n\n# clf = MLPClassifier(random_state=19, max_iter = 10000)\n\n# grid_clf = GridSearchCV(clf, param_grid, cv=5)\n\n# grid_clf.fit(x_train, train['risk_factor'])\n\n# grid_clf.best_estimator_\n\n\nNN = MLPClassifier(solver='lbfgs', alpha=0.0001,activation='relu',\n                    hidden_layer_sizes=(4, 2), random_state=19, max_iter = 1000)\n\nNN = NN.fit(x_train, train['risk_factor'])\n\n# Training performance\nbow_mlp_train = NN.score(x_train, train['risk_factor'])\nprint(bow_mlp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test performance\nbow_mlp_test = NN.score(x_test, test['risk_factor'])  \nprint(bow_mlp_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Unigram and bigram:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test_tmp = train_test\n\n#train Vectorizer the entire training+test set\nwords = set(nltk.corpus.words.words())\n\nvectorizer = CountVectorizer(analyzer = \"word\",   \\\n                             tokenizer = None,    \\\n                             preprocessor = None, \\\n                             stop_words = None,   \\\n                             ngram_range = (1,2),    # <- include 1 and 2-grams\n                             max_features = 2000) \n                             \nvectorizer.fit(train_test_tmp['tokens'])\n\n#Vectorizing training set\nx_train = vectorizer.transform(train['tokens'])\nx_train = x_train.toarray()\n\n# Top words in the trianing set\nword_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(x_train.sum(axis=0))})\nword_count.sort_values('count', ascending=False).set_index('word')[:30].sort_values('count', ascending=True).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vectorizing test set\nx_test = vectorizer.transform(test['tokens'])\nx_test = x_test.toarray()\n\n# Top words in the test set\nword_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(x_test.sum(axis=0))})\nword_count.sort_values('count', ascending=False).set_index('word')[:30].sort_values('count', ascending=True).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model building:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train a logistic regression\nlogisticCV = LogisticRegressionCV(cv=5, random_state=19, max_iter = 10000).fit(x_train, train['risk_factor'])\n# Training performance\nngram_lr_train = logisticCV.score(x_train, train['risk_factor']) \nprint(ngram_lr_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test performance\nngram_lr_test = logisticCV.score(x_test, test['risk_factor'])  \nprint(ngram_lr_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train a random forest model\n\n# Select hyperparameter through cross-validation. To shorten the process time of this notebook, this step is only run once and the results will be used directly.  \n# param_grid = {\n#                 'n_estimators': [50,100,150,200,500],\n#                 'max_depth': list(range(1,20, 2))\n#             }\n\n# clf = RandomForestClassifier(random_state=19)\n\n# grid_clf = GridSearchCV(clf, param_grid, cv=5)\n\n# grid_clf.fit(x_train, train['risk_factor'])\n\n# grid_clf.best_estimator_\n\nforestCV = RandomForestClassifier(n_estimators = 100,max_depth=13, random_state=19) \nforestCV = forestCV.fit(x_train, train['risk_factor'])\n\n# Training performance\nngram_rf_train  = forestCV.score(x_train, train['risk_factor']) \nprint(ngram_rf_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test performance\nngram_rf_test = forestCV.score(x_test, test['risk_factor']) \nprint(ngram_rf_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train an MLP model\n\n# Select hyper parameter through cross-validation. To shorten the process time of this notebook, this step is only run once and the results will be used directly.  \n\n#param_grid = {'hidden_layer_sizes': [(20,10),(10,5),(4,2)]}\n\n#clf = MLPClassifier(random_state=19, max_iter = 10000)\n\n#grid_clf = GridSearchCV(clf, param_grid, cv=5)\n\n#grid_clf.fit(x_train, train['risk_factor'])\n\n#grid_clf.best_estimator_\n\nNN = MLPClassifier(solver='lbfgs', alpha=0.0001,activation='relu',\n                    hidden_layer_sizes=(10, 5), random_state=19, max_iter = 1000)\n\nNN = NN.fit(x_train, train['risk_factor'])\n\n# Training performance\nngram_mlp_train = NN.score(x_train, train['risk_factor'])\nprint(ngram_mlp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test performance\nngram_mlp_test = NN.score(x_test, test['risk_factor'])  \nprint(ngram_mlp_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Bag of words adjusted by Tf-idf weighting:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test_tmp = train_test\n\n#train Vectorizer the entire training+test set\nwords = set(nltk.corpus.words.words())\n\nvectorizer = TfidfVectorizer(analyzer = \"word\",   \\\n                             tokenizer = None,    \\\n                             preprocessor = None, \\\n                             stop_words = None,   \\\n                             ngram_range = (1,1),\n                             max_features = 2000)\n\nvectorizer.fit(train_test_tmp['tokens'])\n\n#Vectorizing training set\nx_train = vectorizer.transform(train['tokens'])\nx_train = x_train.toarray()\n\nword_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(x_train.sum(axis=0))})\nword_count.sort_values('count', ascending=False).set_index('word')[:30].sort_values('count', ascending=True).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vectorizing test set\nx_test = vectorizer.transform(test['tokens'])\nx_test = x_test.toarray()\n\n# Top words in the test set\nword_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(x_test.sum(axis=0))})\nword_count.sort_values('count', ascending=False).set_index('word')[:30].sort_values('count', ascending=True).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model Building:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train a simple logistic regression\nlogisticCV = LogisticRegressionCV(cv=5, random_state=19, max_iter = 10000).fit(x_train, train['risk_factor'])\n# Training performance\nTfidf_bow_lr_train = logisticCV.score(x_train, train['risk_factor']) \nprint(Tfidf_bow_lr_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test performance\nTfidf_bow_lr_test = logisticCV.score(x_test, test['risk_factor'])  \nprint(Tfidf_bow_lr_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train a random forest model\n\n# Select hyper parameter through cross-validation. To shorten the process time of this notebook, this step is only run once and the results will be used directly.  \n# param_grid = {\n#                 'n_estimators': [50,100,150,200,500],\n#                 'max_depth': list(range(1,20, 2))\n#             }\n\n# clf = RandomForestClassifier(random_state=19)\n\n# grid_clf = GridSearchCV(clf, param_grid, cv=5)\n\n# grid_clf.fit(x_train, train['risk_factor'])\n\n# grid_clf.best_estimator_\nforestCV = RandomForestClassifier(n_estimators = 50,max_depth=9, random_state=19) \nforestCV = forestCV.fit(x_train, train['risk_factor'])\n\n# Training performance\nTfidf_bow_rf_train  = forestCV.score(x_train, train['risk_factor']) \nprint(Tfidf_bow_rf_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test performance\nTfidf_bow_rf_test = forestCV.score(x_test, test['risk_factor']) \nprint(Tfidf_bow_rf_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train an MLP model\n# Select hyper parameter through cross-validation. To shorten the process time of this notebook, this step is only run once and the results will be used directly.  \n\n#param_grid = {'hidden_layer_sizes': [(20,10),(10,5),(4,2)]}\n\n#clf = MLPClassifier(random_state=19, max_iter = 10000)\n\n#grid_clf = GridSearchCV(clf, param_grid, cv=5)\n\n#grid_clf.fit(x_train, train['risk_factor'])\n\n#grid_clf.best_estimator_\n\nNN = MLPClassifier(solver='lbfgs', alpha=0.0001,activation='relu',\n                    hidden_layer_sizes=(10, 5), random_state=19, max_iter = 1000)\n\nNN = NN.fit(x_train, train['risk_factor'])\n\n# Training performance\nTfidf_bow_mlp_train = NN.score(x_train, train['risk_factor'])\nprint(Tfidf_bow_mlp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test performance\nTfidf_bow_mlp_test = NN.score(x_test, test['risk_factor'])  \nprint(Tfidf_bow_mlp_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Uni- and bi-gram adjusted by Tf-idf weighting:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test_tmp = train_test\n\n#train Vectorizer the entire training+test set\nwords = set(nltk.corpus.words.words())\n\nvectorizer = TfidfVectorizer(analyzer = \"word\",   \n                             tokenizer = None,    \n                             preprocessor = None, \n                             stop_words = None,   \n                             ngram_range = (1,2),  \n                             max_features = 2000)\n\nvectorizer.fit(train_test_tmp['tokens'])\n\n#Vectorizing training set\nx_train = vectorizer.transform(train['tokens'])\nx_train = x_train.toarray()\n\nword_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(x_train.sum(axis=0))})\nword_count.sort_values('count', ascending=False).set_index('word')[:30].sort_values('count', ascending=True).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vectorizing test set\nx_test = vectorizer.transform(test['tokens'])\nx_test = x_test.toarray()\n\n# Top words in the test set\nword_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(x_test.sum(axis=0))})\nword_count.sort_values('count', ascending=False).set_index('word')[:30].sort_values('count', ascending=True).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model Building:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train a simple logistic regression\nlogisticCV = LogisticRegressionCV(cv=5, random_state=19, max_iter = 10000).fit(x_train, train['risk_factor'])\n# Training performance\nTfidf_ngram_lr_train = logisticCV.score(x_train, train['risk_factor']) \nprint(Tfidf_ngram_lr_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test performance\nTfidf_ngram_lr_test = logisticCV.score(x_test, test['risk_factor'])  \nprint(Tfidf_ngram_lr_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train a random forest model\n\n# Select hyper parameter through cross-validation. To shorten the process time of this notebook, this step is only run once and the results will be used directly.  \n# param_grid = {\n#                 'n_estimators': [50,100,150,200,500],\n#                 'max_depth': list(range(1,20, 2))\n#             }\n\n# clf = RandomForestClassifier(random_state=19)\n\n# grid_clf = GridSearchCV(clf, param_grid, cv=5)\n\n# grid_clf.fit(x_train, train['risk_factor'])\n\n# grid_clf.best_estimator_\n\nforestCV = RandomForestClassifier(n_estimators = 50,max_depth=11, random_state=19) \nforestCV = forestCV.fit(x_train, train['risk_factor'])\n\n# Training performance\nTfidf_ngram_rf_train  = forestCV.score(x_train, train['risk_factor']) \nprint(Tfidf_ngram_rf_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test performance\nTfidf_ngram_rf_test = forestCV.score(x_test, test['risk_factor']) \nprint(Tfidf_ngram_rf_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train an MLP model\n# Select hyper parameter through cross-validation. To shorten the process time of this notebook, this step is only run once and the results will be used directly.  \n\n#param_grid = {'hidden_layer_sizes': [(20,10),(10,5),(4,2)]}\n\n#clf = MLPClassifier(random_state=19, max_iter = 10000)\n\n#grid_clf = GridSearchCV(clf, param_grid, cv=5)\n\n#grid_clf.fit(x_train, train['risk_factor'])\n\n#grid_clf.best_estimator_\n\nNN = MLPClassifier(solver='lbfgs', alpha=0.0001,activation='relu',\n                    hidden_layer_sizes=(10, 5), random_state=19, max_iter = 1000)\n\nNN = NN.fit(x_train, train['risk_factor'])\n\n# Training performance\nTfidf_ngram_mlp_train = NN.score(x_train, train['risk_factor'])\nprint(Tfidf_ngram_mlp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test performance\nTfidf_ngram_mlp_test = NN.score(x_test, test['risk_factor'])  \nprint(Tfidf_ngram_mlp_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model Summary:**\n\nThe best model is MLP on Tf-idf adjusted uni- and bi-gram. Its accuracy on the test dataset is 87.3%. "},{"metadata":{},"cell_type":"markdown","source":"**Training performance:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# summary table for training performance:\ntrain_res = [['Bow',bow_lr_train, bow_rf_train, bow_mlp_train],\n             ['1,2-gram', ngram_lr_train, ngram_rf_train, ngram_mlp_train],\n             ['Tfidf-Bow', Tfidf_bow_lr_train, Tfidf_bow_rf_train, Tfidf_bow_mlp_train], \n             ['Tfidf-1,2-gram', Tfidf_ngram_lr_train, Tfidf_ngram_rf_train, Tfidf_ngram_mlp_train]]\ntrain_res = pd.DataFrame(train_res, columns = ['Features','Logistic','Random Forest','MLP'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Test performance:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# summary table for testing performance:\ntest_res = [['Bow',bow_lr_test, bow_rf_test, bow_mlp_test],\n             ['1,2-gram', ngram_lr_test, ngram_rf_test, ngram_mlp_test],\n             ['Tfidf-Bow', Tfidf_bow_lr_test, Tfidf_bow_rf_test, Tfidf_bow_mlp_test], \n             ['Tfidf-1,2-gram', Tfidf_ngram_lr_test, Tfidf_ngram_rf_test, Tfidf_ngram_mlp_test]]\ntest_res = pd.DataFrame(test_res, columns = ['Features','Logistic','Random Forest','MLP'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## b Classifying Remaining Articles with Tf-idf Adjusted MLP Model\n\nAfter identify the best model, i.e., the MLP model on Tf-idf adjusted uni- and bi-gram, we are now ready to make predictions. Before that, we will first re-train our model on the training and test combined dataset (n = 352). "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge previously annotated labels back to the collection of filtered abstracts: \n\nlabels = train_test[[\"cord_uid\",\"risk_factor\"]]\n\nabstracts_new_merged = pd.merge(abstracts_new,labels, how = 'left', on = ['cord_uid'])\nabstracts_new_merged.info()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abstracts_new_merged.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Re-train MLP model on the training/test combined dataset:**"},{"metadata":{},"cell_type":"markdown","source":"**Prepare dataset:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize vectorizer on the entire dataset\n\ntrain_test_tmp = abstracts_new_merged\n\n# Note： The direct use of abstracts_new_merged['token'] dosen't work so I repeated the preoprocessing here. \n\ntrain_test_tmp['newtokens'] = train_test_tmp['abstract'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n\n# Removing punctuation\ntrain_test_tmp['newtokens'] = train_test_tmp['newtokens'].apply(lambda x: re.sub(r'[^a-zA-z\\w\\s]','',x))\n\n# Stop word removal\nstop = stopwords.words('english')\nstop.append('abstract')\ntrain_test_tmp['newtokens'] = train_test_tmp['newtokens'].apply(lambda x: \" \".join(w for w in x.split() if not w in stop))\n\n#Stemming\nlemmatizer = WordNetLemmatizer()\ntrain_test_tmp['newtokens'] = train_test_tmp['newtokens'].apply(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split()]))\n\n\n#train Vectorizer the entire training+test set\nwords = set(nltk.corpus.words.words())\n\nvectorizer = TfidfVectorizer(analyzer = \"word\",   \\\n                             tokenizer = None,    \\\n                             preprocessor = None, \\\n                             stop_words = None,   \\\n                             ngram_range = (1,2),    # <- indicate 1 and 2-grams\n                             max_features = 2000)\n\nvectorizer.fit(train_test_tmp['newtokens'])\n\n# This is our new training set\ntrain = train_test_tmp[train_test_tmp['risk_factor'].notna()]\ntrain.info()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is our new test set that includes all the unlabeled abstracts\ntest = train_test_tmp[train_test_tmp['risk_factor'].isna()]\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#Vectorizing training set\nx_train = vectorizer.transform(train['newtokens'])\nx_train = x_train.toarray()\n\nword_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(x_train.sum(axis=0))})\nword_count.sort_values('count', ascending=False).set_index('word')[:30].sort_values('count', ascending=True).plot(kind='barh')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vectorizing testing set\nx_test = vectorizer.transform(test['newtokens'])\nx_test = x_test.toarray()\n\nword_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(x_test.sum(axis=0))})\nword_count.sort_values('count', ascending=False).set_index('word')[:30].sort_values('count', ascending=True).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Build Final Model:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the final MLP model\n# Select hyper parameter through cross-validation. To shorten the process time of this notebook, this step is only run once and the results will be used directly.  \n\n# param_grid = {'hidden_layer_sizes': [(50,20),(10,5),(10,2)]}\n# clf = MLPClassifier(random_state=19, max_iter = 10000)\n\n# grid_clf = GridSearchCV(clf, param_grid, cv=5)\n\n# grid_clf.fit(x_train, train['risk_factor'])\n\n# grid_clf.best_estimator_\n\nNN = MLPClassifier(solver='adam', alpha=0.0001,activation='relu',\n                    hidden_layer_sizes=(10, 5), random_state=19, max_iter = 1000)\n\nfinal_model = NN.fit(x_train, train['risk_factor'])\n\n# Training performance\nTfidf_ngram_mlp_final = final_model.score(x_train, train['risk_factor'])\nprint(Tfidf_ngram_mlp_final)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Classify the remaining abstracts:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge results and obtain all risk-factor related articles\nresults = final_model.predict(x_test)\n\noutput = pd.DataFrame( data={\"cord_uid\":test[\"cord_uid\"], \"Predicted_label\":results})\n\noutput.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = pd.merge(abstracts_new_merged,output,how = 'left', on = ['cord_uid'])\n\nRiskFactor_df = tmp[(tmp['Predicted_label'] == \"Yes\" ) | (tmp['risk_factor']== \"Yes\")]\n\nRiskFactor_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RiskFactor_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4 Clustering Articles by Risk Factors using LDA"},{"metadata":{},"cell_type":"markdown","source":"**LDA clustering:**"},{"metadata":{},"cell_type":"markdown","source":"**Building dictionary and corpus:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"LDA_data = RiskFactor_df['tokens']\n\n# Reformatting tokens for LDA\n\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\ntexts = list(sent_to_words(LDA_data))\n\ndictionary = corpora.Dictionary(texts)\ndict(list(dictionary.token2id.items())[0:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = [dictionary.doc2bow(item) for item in texts]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Run LDA model:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                            id2word=dictionary,\n                                            num_topics=10,\n                                            random_state=100,\n                                            update_every=1,\n                                            chunksize=100,\n                                            passes=10,\n                                            alpha='auto',\n                                            per_word_topics=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Calculate perplexity and coherence score:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5 Visualizing LDA Clustering"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pyLDAvis\nimport pyLDAvis.gensim  \n# Visualize the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary, R = 50)\nvis","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}