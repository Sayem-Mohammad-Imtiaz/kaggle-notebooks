{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import cv2\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras as tfk\nimport tensorflow_datasets as tfds\nimport keras\nfrom keras import layers\nfrom keras.preprocessing import image\ntfkl = tfk.layers","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = \"/kaggle/input/fashion-product-images-dataset/fashion-dataset/fashion-dataset/\"\nprint(os.listdir(path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(path + \"styles.csv\", nrows=5000, error_bad_lines=False)\ndf['image'] = df.apply(lambda row: str(row['id']) + \".jpg\", axis=1)\ndf = df.reset_index(drop=True)\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **We will use the product image 15970.jpg as for example**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id = 15970","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step1. Retrieval"},{"metadata":{},"cell_type":"markdown","source":"Here I directly use the product of the same subcategory, but in our real case"},{"metadata":{"trusted":true},"cell_type":"code","source":"def retrieval(df,image_id):\n    selected_rows = df.loc[df['image'] == str(image_id)+'.jpg']\n    sub_cat = selected_rows['subCategory'].iloc[0]\n    gender = selected_rows['gender'].iloc[0]\n    retr_dt = df.loc[(df['subCategory']==sub_cat) & (df['gender']==gender)]#retrived dataset\n    return retr_dt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retr_dt = retrieval(df,test_id)\nretr_dt.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step2. Ranking based on similarity of product images"},{"metadata":{},"cell_type":"markdown","source":"### read in images and make plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_image(image_id):\n    img = str(image_id)+'.jpg'\n    img = cv2.imread(path+\"images/\"+str(img))\n    #print(img.shape)\n    if img.shape != (2400,1800,3):\n        img = image.load_img(path+\"images/\"+str(image_id)+'.jpg', target_size=(2400,1800,3))\n        img = image.img_to_array(img)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_image(image_id):\n    img = str(image_id)+'.jpg'\n    img = cv2.imread(path+\"images/\"+str(img))\n    # If directly use cv2.imshow(img)m, the color is in wrong order\n    b,g,r = cv2.split(img)\n    frame_rgb = cv2.merge((r,g,b))\n    plt.imshow(frame_rgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_image(test_id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### instance segmentation\nSome pictures are with models while others are not, and perhaps some photos does not have a clear background. Therefore, it's important to apply instance segmentation for identifying the part of product image."},{"metadata":{},"cell_type":"markdown","source":"Unfortunately, the pre-trained model Resnet used the COCO dataset, which is generally for custom items (e.g person, car, etc), not specifically for clothes image segmentation, and we lack the training dataset to specify the part of clothes/products. \n\nTherefore, the rough solution here is: use ResNet to identify if there's a person. If there is a model, we would recommend similar products which are always with models, and select some product images without models for add-up."},{"metadata":{},"cell_type":"markdown","source":"**image-processing difficulties & Future Work:**\n\n(1) semantic segmentation *(eliminate the background/models, only keep products)*\n\n(2) resize picture matricies\n\n(3) cope with image distortion / different filming angles\n\n(3) compute similarities"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from imageai.Detection import ObjectDetection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def with_without_model(test_id):\n    execution_path = \"/kaggle/input/fashion-product-images-dataset/fashion-dataset/images/\"\n    detector = ObjectDetection()\n    detector.setModelTypeAsRetinaNet()\n    detector.setModelPath(\"/kaggle/input/imageai/resnet50_coco_best_v2.0.1.h5\")\n    detector.loadModel()\n    \n    detections = detector.detectObjectsFromImage(input_image=os.path.join(execution_path,str(test_id)+\".jpg\"), output_image_path=os.path.join(os.getcwd(),str(test_id)+\".jpg\"))\n    \n    for eachObject in detections:\n        if eachObject[\"name\"]=='person' and eachObject[\"percentage_probability\"]>50:\n            return 1\n        else:\n            continue\n    return 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's a dataset ***model_dat*** which is generated from the process of mapping the function ***with_without_model*** to every image."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"#with_without_model(test_id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### image embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.resnet50 import ResNet50\n\ntfkl = tfk.layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remember the input_shape set for this model is in_shape, which is a tuple, so the image should be resized\ndef build_model(in_shape,high_d=True):\n    #build model for embedding\n    resnet_base = ResNet50(weights='imagenet', \n                      include_top=False, \n                      input_shape = in_shape)\n    resnet_base.trainable = False\n    \n    model = tfk.Sequential()\n    model.add(resnet_base)\n    if high_d==True:\n        model.add(tfkl.GlobalMaxPooling2D()) #add layer embedding\n    else:\n        model.add(tfkl.Dense(100, activation=tf.nn.relu))\n    \n    print(model.summary())\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"model.compile(\n    optimizer=tfk.optimizers.RMSprop(),\n    loss=tfk.losses.CategoricalCrossentropy(),\n    metrics=[\"acc\"]\n)\n\nresults = model.fit(ds_train, batch_size=32, steps_per_epoch=30, epochs=20,verbose=1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"in_shape = [2400,1800,3]\nmodel = build_model(tuple(in_shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we have to reshape the images, because not every image is in the same shape"},{"metadata":{"trusted":true},"cell_type":"code","source":"img = read_image(test_id)\nemb = model.predict(img.reshape(tuple([1]+in_shape))) #(1, 2400, 1800, 3)\n# emb\n## convert the shape (1,2048) to (2048,)\nemb = emb.reshape(-1)\n\nemb.shape\nemb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_embedding(mod, image_name, in_shape):\n    # Reshape and load image\n    img = image.load_img(path+\"images/\"+str(image_name), target_size=in_shape)\n    img = image.img_to_array(img)\n    ## img = cv2.imread(path+\"images/\"+str(image_name))\n    return mod.predict(img.reshape(tuple([1]+in_shape))).reshape(-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Attach a column *embedding* to store image embedding for every photo. "},{"metadata":{"trusted":true},"cell_type":"code","source":"numRows = df.shape[0]\nnumCols = 2048 #representing dimensions for embedding, see the output dim of model\nemb_matrix = pd.DataFrame(index=range(numRows),columns=range(numCols))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The output embedding for every image is 2048, is there a curse of dimensionality?\n\nNo problem arises, and 2048 dimension works better than 100 dimension."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Compute every image's embedding in df, and attach it as a column\nfor r in range(0,df.shape[0]):\n    im = df['image'][r]\n    emb = get_embedding(model,im,in_shape)\n    emb_matrix.iloc[r,:]=emb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emb_matrix.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#store emb_matrixï¼Œinstead as a ram\nemb_matrix.to_csv(\"emb_matrix.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emb_store = pd.concat([emb_matrix, df[[\"image\",\"id\"]]],axis=1,ignore_index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emb_store.to_csv(\"emb_store.csv\",index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### compute similarity for all retrieved images"},{"metadata":{"trusted":true},"cell_type":"code","source":"#retr_dt.index\ndt = emb_store.loc[retr_dt.index,]\ndt.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_similarity(dt,test_id):\n    dt.index = dt[\"id\"].apply(str)\n    dt[\"sim\"] = np.nan\n    try:\n        dt = dt.drop([\"image\",\"id\"],axis=1)\n    except:\n        dt = dt\n    target_vec = dt.loc[dt.index==str(test_id)]\n    target_vec = list(target_vec.iloc[0,0:2048])\n    #again, 2048 represents dimensions for embedding, see the output dim of model\n    \n    from scipy import spatial\n    for i in dt.index:\n        vec = dt.loc[dt.index==i, dt.columns!=\"sim\"]\n        vec = list(vec.iloc[0,:])\n        cosine_similarity = 1 - spatial.distance.cosine(target_vec, vec)\n        dt.loc[dt.index==i,\"sim\"] = round(cosine_similarity,3)\n    \n    sort_dt = dt.sort_values('sim',ascending=False)\n    \n    return sort_dt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_dat = compute_similarity(dt, test_id)\nsorted_dat.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the top 10 recommendations for test_id\nfig=plt.figure(figsize=(10, 10))\ncolumns = 5\nrows = 2\ni = 1\nfor img in sorted_dat.iloc[0:10,i].index:\n    im = cv2.imread(path+\"images/\"+str(img)+\".jpg\")\n    b,g,r = cv2.split(im)\n    frame_rgb = cv2.merge((r,g,b))\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(frame_rgb)\n    i+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Add-up note, but not necessary:**\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html\nfrom sklearn.metrics.pairwise import pairwise_distances\n\nCalculate distance Matrix\ncosine_sim = 1-pairwise_distances(dt.loc[:,dt.columns!=\"sim\"], metric='cosine')\ncosine_sim[:4, :4]"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Try another test_id"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id_2 = 58183","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retr_dt_2 = retrieval(df,test_id_2)\nretr_dt_2.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_image(test_id_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_shape_2 = [2400,1800,3]\nmodel_2 = build_model(tuple(in_shape_2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_2 = read_image(test_id_2)\nemb_2 = model_2.predict(img_2.reshape(tuple([1]+in_shape_2))) #(1, 2400, 1800, 3)\n# emb\n## convert the shape (1,2048) to (2048,)\nemb_2 = emb_2.reshape(-1)\n\nemb_2.shape\nemb_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_2 = emb_store.loc[retr_dt_2.index,]\ndt_2.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_dat_2 = compute_similarity(dt_2, test_id_2)\nsorted_dat_2.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the top 10 recommendations for test_id\nfig=plt.figure(figsize=(10, 10))\ncolumns = 5\nrows = 2\ni = 1\nfor img in sorted_dat_2.iloc[0:10,i].index:\n    im = cv2.imread(path+\"images/\"+str(img)+\".jpg\")\n    b,g,r = cv2.split(im)\n    frame_rgb = cv2.merge((r,g,b))\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(frame_rgb)\n    i+=1\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}