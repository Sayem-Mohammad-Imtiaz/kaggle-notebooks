{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Spam Detection using Natural Language Toolkit\n\n## Libraries\n\n#### 1. stopwords - NLTK has a list of stopwords from 16 languages. Stopwords are removed so the algorithm can process input faster. \n\n{‘ourselves’, ‘hers’, ‘between’, ‘yourself’, ‘but’, ‘again’, ‘there’, ‘about’, ‘once’, ‘during’, ‘out’, ‘very’, ‘having’, ‘with’, ‘they’, ‘own’, ‘an’, ‘be’, ‘some’, ‘for’, ‘do’, ‘its’, ‘yours’, ‘such’, ‘into’, ‘of’, ‘most’, ‘itself’, ‘other’, ‘off’, ‘is’, ‘s’, ‘am’, ‘or’, ‘who’, ‘as’, ‘from’, ‘him’, ‘each’, ‘the’, ‘themselves’, ‘until’, ‘below’, ‘are’, ‘we’, ‘these’, ‘your’, ‘his’, ‘through’, ‘don’, ‘nor’, ‘me’, ‘were’, ‘her’, ‘more’, ‘himself’, ‘this’, ‘down’, ‘should’, ‘our’, ‘their’, ‘while’, ‘above’, ‘both’, ‘up’, ‘to’, ‘ours’, ‘had’, ‘she’, ‘all’, ‘no’, ‘when’, ‘at’, ‘any’, ‘before’, ‘them’, ‘same’, ‘and’, ‘been’, ‘have’, ‘in’, ‘will’, ‘on’, ‘does’, ‘yourselves’, ‘then’, ‘that’, ‘because’, ‘what’, ‘over’, ‘why’, ‘so’, ‘can’, ‘did’, ‘not’, ‘now’, ‘under’, ‘he’, ‘you’, ‘herself’, ‘has’, ‘just’, ‘where’, ‘too’, ‘only’, ‘myself’, ‘which’, ‘those’, ‘i’, ‘after’, ‘few’, ‘whom’, ‘t’, ‘being’, ‘if’, ‘theirs’, ‘my’, ‘against’, ‘a’, ‘by’, ‘doing’, ‘it’, ‘how’, ‘further’, ‘was’, ‘here’, ‘than’} \n\n#### 2. WordNetLemmatizer - Lemmatization is the grouping together of different forms of the same word. Lemmatization considers the context and converts the word to its meaningful base form. Wordnet is a database that offers lemmatization capabilities. \n\nEx: 'caring' -> lemmatization -> 'care'\n\n\n#### 3. TF-IDF Vectorizer - Transform text into a meaningful representation of numbers which is used to fit machine algorithm for prediction. Provides a weightage to each word.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\n#from nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(color_codes = True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-22T03:48:39.360114Z","iopub.execute_input":"2021-09-22T03:48:39.360493Z","iopub.status.idle":"2021-09-22T03:48:41.153301Z","shell.execute_reply.started":"2021-09-22T03:48:39.360397Z","shell.execute_reply":"2021-09-22T03:48:41.152445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv',encoding='ISO-8859-1')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-22T03:48:41.154887Z","iopub.execute_input":"2021-09-22T03:48:41.155129Z","iopub.status.idle":"2021-09-22T03:48:41.212499Z","shell.execute_reply.started":"2021-09-22T03:48:41.155101Z","shell.execute_reply":"2021-09-22T03:48:41.211383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.iloc[:,:2]\ndf.rename(columns = {'v1':'label','v2':'message'},inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T03:48:41.213774Z","iopub.execute_input":"2021-09-22T03:48:41.214388Z","iopub.status.idle":"2021-09-22T03:48:41.226725Z","shell.execute_reply.started":"2021-09-22T03:48:41.214351Z","shell.execute_reply":"2021-09-22T03:48:41.225384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus = []\nlm = WordNetLemmatizer()\nfor i in range(0, len(df)):\n    review = re.sub('[^a-zA-Z]', ' ', df['message'][i])\n    review = review.lower()\n    review = review.split()\n    review = [lm.lemmatize(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T03:48:41.228103Z","iopub.execute_input":"2021-09-22T03:48:41.228379Z","iopub.status.idle":"2021-09-22T03:48:55.516909Z","shell.execute_reply.started":"2021-09-22T03:48:41.228349Z","shell.execute_reply":"2021-09-22T03:48:55.516263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ncv = TfidfVectorizer()\nX = cv.fit_transform(corpus).toarray()\n\n\n#TF-IDF stands for “Term Frequency — Inverse Document Frequency”. \n#This is a technique to quantify a word in documents, \n#we generally compute a weight to each word which signifies the importance of the word in the document and corpus. \n#This method is a widely used technique in Information Retrieval and Text Mining.\n\ny=pd.get_dummies(df['label'])\ny=y.iloc[:,1].values","metadata":{"execution":{"iopub.status.busy":"2021-09-22T03:48:55.519189Z","iopub.execute_input":"2021-09-22T03:48:55.519435Z","iopub.status.idle":"2021-09-22T03:48:55.804102Z","shell.execute_reply.started":"2021-09-22T03:48:55.519409Z","shell.execute_reply":"2021-09-22T03:48:55.803156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train Test Split\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T03:48:55.805655Z","iopub.execute_input":"2021-09-22T03:48:55.806049Z","iopub.status.idle":"2021-09-22T03:48:55.994141Z","shell.execute_reply.started":"2021-09-22T03:48:55.806006Z","shell.execute_reply":"2021-09-22T03:48:55.99314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import svm\n\nclf = svm.SVC()\nclf.fit(X_train,y_train)\ny_predicted = clf.predict(X_test)\nscore = clf.score(X_test,y_test)\n\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T03:48:55.995856Z","iopub.execute_input":"2021-09-22T03:48:55.996146Z","iopub.status.idle":"2021-09-22T03:51:41.292033Z","shell.execute_reply.started":"2021-09-22T03:48:55.996114Z","shell.execute_reply":"2021-09-22T03:51:41.290851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n#Confusion Matrix\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, y_predicted)\nnp.set_printoptions(precision=2)\ncnf_matrix","metadata":{"execution":{"iopub.status.busy":"2021-09-22T03:51:41.293418Z","iopub.execute_input":"2021-09-22T03:51:41.29366Z","iopub.status.idle":"2021-09-22T03:51:41.303664Z","shell.execute_reply.started":"2021-09-22T03:51:41.293635Z","shell.execute_reply":"2021-09-22T03:51:41.302828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-09-22T03:51:41.304853Z","iopub.execute_input":"2021-09-22T03:51:41.305463Z","iopub.status.idle":"2021-09-22T03:51:41.316851Z","shell.execute_reply.started":"2021-09-22T03:51:41.305424Z","shell.execute_reply":"2021-09-22T03:51:41.31584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = df[\"label\"].value_counts()\n#With Normalization\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=classes.index,\n                      title='Confusion matrix, without normalization')\n# With normalization\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes= classes.index, normalize=True,\n                      title='Normalized confusion matrix')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-22T03:51:41.318223Z","iopub.execute_input":"2021-09-22T03:51:41.318636Z","iopub.status.idle":"2021-09-22T03:51:41.809598Z","shell.execute_reply.started":"2021-09-22T03:51:41.318604Z","shell.execute_reply":"2021-09-22T03:51:41.808711Z"},"trusted":true},"execution_count":null,"outputs":[]}]}