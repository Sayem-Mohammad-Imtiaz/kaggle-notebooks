{"cells":[{"metadata":{"id":"ffHizKZ40Sec","trusted":true},"cell_type":"code","source":"import re    # for regular expressions \nimport nltk  # for text manipulation \nimport string \nimport warnings \nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"id":"jHSqHngL0Se1","trusted":true},"cell_type":"code","source":"pd.set_option(\"display.max_colwidth\", 200) \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"xDtA7_AM0Se3","trusted":true},"cell_type":"code","source":"import io\n\ntrain = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/train.csv')\ntest  = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"DfKaUCZA0Se4"},"cell_type":"markdown","source":"## Data Inspection"},{"metadata":{"id":"JVFtLq7e0Se5"},"cell_type":"markdown","source":"Let’s check out a few non racist/sexist tweets."},{"metadata":{"id":"ZVRpU9yo0Se6","outputId":"72c76e95-c43c-4187-9195-93315096b223","trusted":true},"cell_type":"code","source":"train[train['label'] == 0].head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"56Na6YWx0Se-"},"cell_type":"markdown","source":"Now check out a few racist/sexist tweets."},{"metadata":{"id":"Y6dr2Hf30Se_","outputId":"156f2360-0e4f-4cbc-9bd4-99a3952c553e","trusted":true},"cell_type":"code","source":"train[train['label'] == 1].head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"Wxd5T0FU0SfA"},"cell_type":"markdown","source":"There are quite a many words and characters which are not really required. So, we will try to keep only those words which are important and add value.\n\nLet’s check dimensions of the train and test dataset."},{"metadata":{"id":"jHHHpzUv0SfB","outputId":"0ea8de5c-093f-4349-ab2b-9b574ab02e44","trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"0vJ73PYZ0SfD"},"cell_type":"markdown","source":"Train set has 31,962 tweets and test set has 17,197 tweets.\n\nLet’s have a glimpse at label-distribution in the train dataset."},{"metadata":{"id":"dF2Gq2x_0SfE","outputId":"e3a7ba2e-1977-4b84-8f94-75e271845bed","trusted":true},"cell_type":"code","source":"train[\"label\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"lWe2krem0SfF"},"cell_type":"markdown","source":"In the train dataset, we have 2,242 (7%) tweets labeled as racist or sexist, and 29,720 (93%) tweets labeled as non racist/sexist. So, it is an imbalanced classification challenge.\n\nNow we will check the distribution of length of the tweets, in terms of words, in both train and test data."},{"metadata":{"id":"fMlzjmz80SfG","outputId":"d3fe5f91-cd37-4216-c1a0-fb69ea66d6d9","trusted":true},"cell_type":"code","source":"length_train = train['tweet'].str.len() \nlength_test = test['tweet'].str.len() \nplt.hist(length_train, bins=20, label=\"train_tweets\") \nplt.hist(length_test, bins=20, label=\"test_tweets\") \nplt.legend() \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"AmdhXar-0SfH"},"cell_type":"markdown","source":"## Data Cleaning"},{"metadata":{"id":"b41gLHF00SfI"},"cell_type":"markdown","source":"Before we begin cleaning, let’s first combine train and test datasets. Combining the datasets will make it convenient for us to preprocess the data. Later we will split it back into train and test data."},{"metadata":{"id":"A9eZ18gc0SfJ","outputId":"427aba55-a041-4bbe-b214-324f4075fbfc","trusted":true},"cell_type":"code","source":"combi = train.append(test, ignore_index=True) \ncombi.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"YzPtZOBY0SfK"},"cell_type":"markdown","source":"Given below is a user-defined function to remove unwanted text patterns from the tweets."},{"metadata":{"id":"J91ckmoC0SfL","trusted":true},"cell_type":"code","source":"def remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n    return input_txt ","execution_count":null,"outputs":[]},{"metadata":{"id":"UyteQMJi0SfM"},"cell_type":"markdown","source":"We will be following the steps below to clean the raw tweets in out data.\n\nWe will remove the twitter handles as they are already masked as @user due to privacy concerns. These twitter handles hardly give any information about the nature of the tweet.\n\nWe will also get rid of the punctuations, numbers and even special characters since they wouldn’t help in differentiating different types of tweets.\n\nMost of the smaller words do not add much value. For example, ‘pdx’, ‘his’, ‘all’. So, we will try to remove them as well from our data.\n\nLastly, we will normalize the text data. For example, reducing terms like loves, loving, and lovable to their base word, i.e., ‘love’.are often used in the same context. If we can reduce them to their root word, which is ‘love’. It will help in reducing the total number of unique words in our data without losing a significant amount of information."},{"metadata":{"id":"IpCXcRgh0SfN"},"cell_type":"markdown","source":"### 1. Removing Twitter Handles (@user)"},{"metadata":{"id":"OVD7R3ri0SfO"},"cell_type":"markdown","source":"Let’s create a new column tidy_tweet, it will contain the cleaned and processed tweets. Note that we have passed “@[]*” as the pattern to the remove_pattern function. It is actually a regular expression which will pick any word starting with ‘@’."},{"metadata":{"id":"j3qiwn9k0SfP","outputId":"61c6e29d-8e12-46b6-9bdb-cf9859f48f0f","trusted":true},"cell_type":"code","source":"combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\") \ncombi.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"6sg8fZJd0SfQ"},"cell_type":"markdown","source":"### 2. Removing Punctuations, Numbers, and Special Characters"},{"metadata":{"id":"ukDp0BEp0SfR"},"cell_type":"markdown","source":"Here we will replace everything except characters and hashtags with spaces. The regular expression “[^a-zA-Z#]” means anything except alphabets and ‘#’."},{"metadata":{"id":"V9mz59PE0SfT","outputId":"eecc49dd-2ddc-4939-e4a6-6a4c1aebd385","trusted":true},"cell_type":"code","source":"combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \") \ncombi.head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"GPTx_38G0SfU"},"cell_type":"markdown","source":"### 3. Removing Short Words"},{"metadata":{"id":"SWmW_jM40SfW"},"cell_type":"markdown","source":"We have to be a little careful here in selecting the length of the words which we want to remove. So, I have decided to remove all the words having length 3 or less. For example, terms like “hmm”, “oh” are of very little use. It is better to get rid of them."},{"metadata":{"id":"jH1rPXnT0SfX","outputId":"34aa5170-3b40-44bf-c822-c29fb9daeec1","trusted":true},"cell_type":"code","source":"combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ncombi.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"Di1R0cCZ0Sfa"},"cell_type":"markdown","source":"You can see the difference between the raw tweets and the cleaned tweets (tidy_tweet) quite clearly. Only the important words in the tweets have been retained and the noise (numbers, punctuations, and special characters) has been removed."},{"metadata":{"id":"gLbTyrTP0Sfb"},"cell_type":"markdown","source":"### 4. Text Normalization "},{"metadata":{"id":"RF-AbMsD0Sfc"},"cell_type":"markdown","source":"Here we will use nltk’s PorterStemmer() function to normalize the tweets. But before that we will have to tokenize the tweets. Tokens are individual terms or words, and tokenization is the process of splitting a string of text into tokens."},{"metadata":{"id":"N3KhkTag0Sfe","outputId":"06f0c2bc-0f87-46ce-b7c4-1fce9fd56d83","trusted":true},"cell_type":"code","source":"tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split()) # tokenizing \ntokenized_tweet.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"wGPkns1d0Sfg"},"cell_type":"markdown","source":"Now we can normalize the tokenized tweets."},{"metadata":{"id":"NWsyeuwK0Sfj","outputId":"c19b0eb9-e8dd-4df1-9838-f9d305ef7e3b","trusted":true},"cell_type":"code","source":"from nltk.stem.porter import * \nstemmer = PorterStemmer() \ntokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\ntokenized_tweet.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"XFYKhjDP0Sfk"},"cell_type":"markdown","source":"Now let’s stitch these tokens back together. It can easily be done using nltk’s MosesDetokenizer function."},{"metadata":{"id":"ialCRZMW0Sfm","outputId":"72936df5-3a8a-42d8-a2fa-850785beec87","trusted":true},"cell_type":"code","source":"for i in range(len(tokenized_tweet)):\n    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])    \ncombi['tidy_tweet'] = tokenized_tweet\ncombi['tidy_tweet'].head()","execution_count":null,"outputs":[]},{"metadata":{"id":"YDX1cvQp0Sfp"},"cell_type":"markdown","source":"## Data Visualization"},{"metadata":{"id":"bwtMiSnD0SgE"},"cell_type":"markdown","source":"Before we begin exploration, we must think and ask questions related to the data in hand. A few probable questions are as follows:\n\n- What are the most common words in the entire dataset?\n- What are the most common words in the dataset for negative and positive tweets, respectively?\n- How many hashtags are there in a tweet?\n- Which trends are associated with my dataset?\n- Which trends are associated with either of the sentiments? Are they compatible with the sentiments?"},{"metadata":{"id":"Icdxq2Ic0SgH"},"cell_type":"markdown","source":"### A) Understanding the common words used in the tweets: WordCloud"},{"metadata":{"id":"6oFltYET0SgJ"},"cell_type":"markdown","source":"A wordcloud is a visualization wherein the most frequent words appear in large size and the less frequent words appear in smaller sizes.\n\nLet’s visualize all the words our data using the wordcloud plot"},{"metadata":{"id":"2ZcsH0pm0SgL","outputId":"114833b6-0083-4488-b9d2-168c3c6c7c48","trusted":true},"cell_type":"code","source":"all_words = ' '.join([text for text in combi['tidy_tweet']]) \n\nfrom wordcloud import WordCloud \n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words) \nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"s1zlIxFT0SgN"},"cell_type":"markdown","source":"We can see most of the words are positive or neutral. Words like love, great, friend, life are the most frequent ones. It doesn’t give us any idea about the words associated with the racist/sexist tweets. Hence, we will plot separate wordclouds for both the classes (racist/sexist or not) in our train data."},{"metadata":{"id":"3Nb-MnEB0SgR"},"cell_type":"markdown","source":"### B) Words in non racist/sexist tweets"},{"metadata":{"id":"owBU9C3k0SgS","outputId":"f1133c43-0fc6-4990-fc2b-49d1295e6837","trusted":true},"cell_type":"code","source":"normal_words =' '.join([text for text in combi['tidy_tweet'][combi['label'] == 0]]) \nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words) \nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"bIvnTfWV0SgU"},"cell_type":"markdown","source":"Most of the frequent words are compatible with the sentiment, i.e, non-racist/sexists tweets. Similarly, we will plot the word cloud for the other sentiment. Expect to see negative, racist, and sexist terms."},{"metadata":{"id":"ToKc44ub0SgW"},"cell_type":"markdown","source":"### C) Racist/Sexist Tweets"},{"metadata":{"id":"4_DX3U6W0SgX","outputId":"0040320a-cbd5-4cef-aa1a-ffbe0d8cf31f","trusted":true},"cell_type":"code","source":"negative_words = ' '.join([text for text in combi['tidy_tweet'][combi['label'] == 1]]) \nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(negative_words) \nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"GUN8OrgQ0SgY"},"cell_type":"markdown","source":"As we can clearly see, most of the words have negative connotations. So, it seems we have a pretty good text data to work on. Next we will the hashtags/trends in our twitter data."},{"metadata":{"id":"MEPfDyy70SgZ"},"cell_type":"markdown","source":"### D) Understanding the impact of Hashtags on tweets sentiment"},{"metadata":{"id":"a1WR0UWf0SgZ"},"cell_type":"markdown","source":"Hashtags in twitter are synonymous with the ongoing trends on twitter at any particular point in time. We should try to check whether these hashtags add any value to our sentiment analysis task, i.e., they help in distinguishing tweets into the different sentiments."},{"metadata":{"id":"4mU5jBaT0Sgb"},"cell_type":"markdown","source":"The tweet seems sexist in nature and the hashtags in the tweet convey the same feeling.\n\nWe will store all the trend terms in two separate lists — one for non-racist/sexist tweets and the other for racist/sexist tweets."},{"metadata":{"id":"mkaw8Csn0Sgc","trusted":true},"cell_type":"code","source":"# function to collect hashtags \ndef hashtag_extract(x):    \n    hashtags = []    \n    # Loop over the words in the tweet    \n    for i in x:        \n        ht = re.findall(r\"#(\\w+)\", i)        \n        hashtags.append(ht)     \n    return hashtags","execution_count":null,"outputs":[]},{"metadata":{"id":"78u732bc0Sgd","trusted":true},"cell_type":"code","source":"# extracting hashtags from non racist/sexist tweets \nHT_regular = hashtag_extract(combi['tidy_tweet'][combi['label'] == 0]) \n\n# extracting hashtags from racist/sexist tweets \nHT_negative = hashtag_extract(combi['tidy_tweet'][combi['label'] == 1]) \n\n# unnesting list \nHT_regular = sum(HT_regular,[]) \nHT_negative = sum(HT_negative,[])","execution_count":null,"outputs":[]},{"metadata":{"id":"RgtEEQbI0Sge"},"cell_type":"markdown","source":"Now that we have prepared our lists of hashtags for both the sentiments, we can plot the top ‘n’ hashtags. So, first let’s check the hashtags in the non-racist/sexist tweets."},{"metadata":{"id":"_2C2jUD20Sgf"},"cell_type":"markdown","source":"#### Non-Racist/Sexist Tweets"},{"metadata":{"id":"skNriJPn0Sgg","outputId":"afaa2f8a-835d-40cf-b7fc-0ecb4939ad16","trusted":true},"cell_type":"code","source":"a = nltk.FreqDist(HT_regular) \nd = pd.DataFrame({'Hashtag': list(a.keys()), 'Count': list(a.values())}) \n\n# selecting top 20 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 20)\n\nplt.figure(figsize=(16,5)) \nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\") \nax.set(ylabel = 'Count') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"6KfNF0940Sgi"},"cell_type":"markdown","source":"All these hashtags are positive and it makes sense. I am expecting negative terms in the plot of the second list. Let’s check the most frequent hashtags appearing in the racist/sexist tweets."},{"metadata":{"id":"B2ayLxvx0Sgj"},"cell_type":"markdown","source":"#### Racist/Sexist Tweets"},{"metadata":{"id":"PRg-nO_p0Sgk","outputId":"869d66d6-608a-4f07-f595-91d8a3926e7e","trusted":true},"cell_type":"code","source":"b = nltk.FreqDist(HT_negative) \ne = pd.DataFrame({'Hashtag': list(b.keys()), 'Count': list(b.values())}) \n\n# selecting top 20 most frequent hashtags \ne = e.nlargest(columns=\"Count\", n = 20)   \nplt.figure(figsize=(16,5)) \nax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")","execution_count":null,"outputs":[]},{"metadata":{"id":"KiXn-a7G0Sgl"},"cell_type":"markdown","source":"As expected, most of the terms are negative with a few neutral terms as well. So, it’s not a bad idea to keep these hashtags in our data as they contain useful information. Next, we will try to extract features from the tokenized tweets."},{"metadata":{"id":"g0uPT_m20Sgm"},"cell_type":"markdown","source":"## Bag-of-Words Features"},{"metadata":{"id":"6selqjXH0Sgn"},"cell_type":"markdown","source":"To analyse a preprocessed data, it needs to be converted into features. Depending upon the usage, text features can be constructed using assorted techniques – Bag of Words, TF-IDF, and Word Embeddings."},{"metadata":{"id":"XHNKw8ga0Sgo","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \nimport gensim","execution_count":null,"outputs":[]},{"metadata":{"id":"B0PqGi7p0Sgp","outputId":"a33e2e2b-3419-49c6-a742-4b14fb51f2b4","trusted":true},"cell_type":"code","source":"bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english') \nbow = bow_vectorizer.fit_transform(combi['tidy_tweet']) \nbow.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"E2vYPu-c0Sgr"},"cell_type":"markdown","source":"## TF-IDF Features"},{"metadata":{"id":"80UsRcY10Sgs"},"cell_type":"markdown","source":"TF-IDF works by penalising the common words by assigning them lower weights while giving importance to words which are rare in the entire corpus but appear in good numbers in few documents.\n\nLet’s have a look at the important terms related to TF-IDF:\n\nTF = (Number of times term t appears in a document)/(Number of terms in the document)\n\nIDF = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n\nTF-IDF = TF*IDF"},{"metadata":{"id":"QWhXBDDa0Sgt","outputId":"d2f466a6-c80d-4913-bbc9-4ddde3cd68c3","trusted":true},"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english') \ntfidf = tfidf_vectorizer.fit_transform(combi['tidy_tweet']) \ntfidf.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"L2jMUchb0Sgu"},"cell_type":"markdown","source":"## Word2Vec Features"},{"metadata":{"id":"OuIX9y_v0Sgv"},"cell_type":"markdown","source":"Word embeddings are the modern way of representing words as vectors. The objective of word embeddings is to redefine the high dimensional word features into low dimensional feature vectors by preserving the contextual similarity in the corpus. They are able to achieve tasks like King -man +woman = Queen"},{"metadata":{"id":"mLTr3tBP0Sgw"},"cell_type":"markdown","source":"### 1. Word2Vec Embeddings"},{"metadata":{"id":"AADbQ5iF0Sgy"},"cell_type":"markdown","source":"Word2Vec is not a single algorithm but a combination of two techniques – CBOW (Continuous bag of words) and Skip-gram model. Both of these are shallow neural networks which map word(s) to the target variable which is also a word(s). Both of these techniques learn weights which act as word vector representations.\n\nCBOW tends to predict the probability of a word given a context. A context may be a single adjacent word or a group of surrounding words. The Skip-gram model works in the reverse manner, it tries to predict the context for a given word."},{"metadata":{"id":"oT4Tk9eY0Sgz"},"cell_type":"markdown","source":"Let’s train a Word2Vec model on our corpus."},{"metadata":{"id":"xTa4Ppo80Sg0","outputId":"dc107b43-ac16-4cfb-9dfd-9362ba3a23fc","trusted":true},"cell_type":"code","source":"tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split()) # tokenizing \nmodel_w2v = gensim.models.Word2Vec(\n            tokenized_tweet,\n            size=200, # desired no. of features/independent variables\n            window=5, # context window size\n            min_count=2,\n            sg = 1, # 1 for skip-gram model\n            hs = 0,\n            negative = 10, # for negative sampling\n            workers= 2, # no.of cores\n            seed = 34) \n\nmodel_w2v.train(tokenized_tweet, total_examples= len(combi['tidy_tweet']), epochs=20)","execution_count":null,"outputs":[]},{"metadata":{"id":"6JXASTv10Sg2"},"cell_type":"markdown","source":"Let’s play a bit with our Word2Vec model and see how does it perform. We will specify a word and the model will pull out the most similar words from the corpus"},{"metadata":{"id":"PqAt9CIb0Sg3","outputId":"42c9cb0e-9493-4e90-bbec-2320641dcd5e","trusted":true},"cell_type":"code","source":"model_w2v.wv.most_similar(positive=\"dinner\")","execution_count":null,"outputs":[]},{"metadata":{"id":"hF8398G80Sg4","outputId":"3b621040-ad1e-4426-c5f5-f3aeab4d1b8e","trusted":true},"cell_type":"code","source":"model_w2v.wv.most_similar(positive=\"trump\")","execution_count":null,"outputs":[]},{"metadata":{"id":"1PW1lTXQ0Sg6"},"cell_type":"markdown","source":"From the above two examples, we can see that our word2vec model does a good job of finding the most similar words for a given word. But how is it able to do so? That’s because it has learned vectors for every unique word in our data and it uses cosine similarity to find out the most similar vectors (words)."},{"metadata":{"id":"Q6diZEQU0Sg7"},"cell_type":"markdown","source":"#### Preparing Vectors for Tweets"},{"metadata":{"id":"yBfdDRGW0Sg8"},"cell_type":"markdown","source":"Since our data contains tweets and not just words, we’ll have to figure out a way to use the word vectors from word2vec model to create vector representation for an entire tweet. There is a simple solution to this problem, we can simply take mean of all the word vectors present in the tweet. The length of the resultant vector will be the same, i.e. 200. We will repeat the same process for all the tweets in our data and obtain their vectors. Now we have 200 word2vec features for our data.\n\nWe will use the below function to create a vector for each tweet by taking the average of the vectors of the words present in the tweet."},{"metadata":{"id":"gWzjHOMI0Sg8","trusted":true},"cell_type":"code","source":"def word_vector(tokens, size):\n    vec = np.zeros(size).reshape((1, size))\n    count = 0.\n    for word in tokens:\n        try:\n            vec += model_w2v[word].reshape((1, size))\n            count += 1.\n        except KeyError: # handling the case where the token is not in vocabulary                                     \n            continue\n    if count != 0:\n        vec /= count\n    return vec","execution_count":null,"outputs":[]},{"metadata":{"id":"R_aZMPwg0Sg9","outputId":"f35a772d-fe8c-49fc-a2bf-2e00d08b13bc","trusted":true},"cell_type":"code","source":"wordvec_arrays = np.zeros((len(tokenized_tweet), 200)) \nfor i in range(len(tokenized_tweet)):\n    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)\n    wordvec_df = pd.DataFrame(wordvec_arrays) \n\nwordvec_df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"vbk2cKe30Sg-"},"cell_type":"markdown","source":"Now we have 200 new features, whereas in Bag of Words and TF-IDF we had 1000 features."},{"metadata":{"id":"NDzfdgGr0Sg_"},"cell_type":"markdown","source":"### 2. Doc2Vec Embedding"},{"metadata":{"id":"0SQcunpn0Sg_"},"cell_type":"markdown","source":"Doc2Vec model is an unsupervised algorithm to generate vectors for sentence/paragraphs/documents. This approach is an extension of the word2vec. The major difference between the two is that doc2vec provides an additional context which is unique for every document in the corpus. This additional context is nothing but another feature vector for the whole document. This document vector is trained along with the word vectors."},{"metadata":{"id":"OCtaxsDH0ShA","trusted":true},"cell_type":"code","source":"from gensim.models.doc2vec import LabeledSentence","execution_count":null,"outputs":[]},{"metadata":{"id":"njYHFxUb0ShB"},"cell_type":"markdown","source":"To implement doc2vec, we have to labelise or tag each tokenised tweet with unique IDs. We can do so by using Gensim’s LabeledSentence() function."},{"metadata":{"id":"6vjnvFMA0ShC","trusted":true},"cell_type":"code","source":"def add_label(twt):\n    output = []\n    for i, s in zip(twt.index, twt):\n        output.append(LabeledSentence(s, [\"tweet_\" + str(i)]))\n    return output\nlabeled_tweets = add_label(tokenized_tweet) # label all the tweets","execution_count":null,"outputs":[]},{"metadata":{"id":"99ALjM3f0ShE"},"cell_type":"markdown","source":"Let’s have a look at the result."},{"metadata":{"id":"Ps9LdVgq0ShF","outputId":"e1d15d38-75fd-4588-8690-9deb6ee1f639","trusted":true},"cell_type":"code","source":"labeled_tweets[:6]","execution_count":null,"outputs":[]},{"metadata":{"id":"doEpTVu70ShH"},"cell_type":"markdown","source":"Now let’s train a doc2vec model."},{"metadata":{"id":"F7AK_xlc0ShI","trusted":true},"cell_type":"code","source":"model_d2v = gensim.models.Doc2Vec(dm=1, # dm = 1 for ‘distributed memory’ model                                   \n                                  dm_mean=1, # dm = 1 for using mean of the context word vectors                                  \n                                  vector_size=200, # no. of desired features                                  \n                                  window=5, # width of the context window                                  \n                                  negative=7, # if > 0 then negative sampling will be used                                 \n                                  min_count=5, # Ignores all words with total frequency lower than 5.                                  \n                                  workers=3, # no. of cores                                  \n                                  alpha=0.1, # learning rate                                  \n                                  seed = 23)\nmodel_d2v.build_vocab([i for i in labeled_tweets])\nmodel_d2v.train(labeled_tweets, total_examples= len(combi['tidy_tweet']), epochs=15)","execution_count":null,"outputs":[]},{"metadata":{"id":"1Yne56KR0ShK"},"cell_type":"markdown","source":"Preparing doc2vec Feature Set"},{"metadata":{"id":"su8xZGCB0ShL","outputId":"08c3e353-cd5e-4810-d34a-fb4a2befd0d2","trusted":true},"cell_type":"code","source":"docvec_arrays = np.zeros((len(tokenized_tweet), 200)) \nfor i in range(len(combi)):\n    docvec_arrays[i,:] = model_d2v.docvecs[i].reshape((1,200))    \n\ndocvec_df = pd.DataFrame(docvec_arrays) \ndocvec_df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"tFawKZ1O0ShN"},"cell_type":"markdown","source":"## Modeling"},{"metadata":{"id":"CUHtOYOE0ShO"},"cell_type":"markdown","source":"We are now done with all the pre-modeling stages required to get the data in the proper form and shape. We will be building models on the datasets with different feature sets prepared in the earlier sections — Bag-of-Words, TF-IDF, word2vec vectors, and doc2vec vectors. We will use the following algorithms to build models:\n\n- Logistic Regression\n- Support Vector Machine\n- RandomForest\n- XGBoost"},{"metadata":{"id":"n1VkHCnj0ShP"},"cell_type":"markdown","source":"> Evaluation Metric: \n       **F1 score** is being used as the evaluation metric. It is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. It is suitable for uneven class distribution problems."},{"metadata":{"id":"Qoa1xLwl0ShQ"},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"id":"zKVg1vDs0ShQ","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"id":"fUlms4p60ShR"},"cell_type":"markdown","source":"### Bag-of-Words Features"},{"metadata":{"id":"0-sl72fl0ShR","outputId":"b40a6800-6605-40b4-b776-54db41411327","trusted":true},"cell_type":"code","source":"# Extracting train and test BoW features \ntrain_bow = bow[:31962,:] \ntest_bow = bow[31962:,:] \n\n# splitting data into training and validation set \nxtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'], random_state=42, test_size=0.3)\nlreg = LogisticRegression() \n\n# training the model \nlreg.fit(xtrain_bow, ytrain) \n\nprediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set \nprediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0 \nprediction_int = prediction_int.astype(np.int) \n\nf1_score(yvalid, prediction_int) # calculating f1 score for the validation set","execution_count":null,"outputs":[]},{"metadata":{"id":"u9m6ekSR0ShS"},"cell_type":"markdown","source":"### TF-IDF Features"},{"metadata":{"id":"GaAURnSs0ShU","outputId":"dc4a8a8e-253e-436c-eaf8-68d37884abee","trusted":true},"cell_type":"code","source":"train_tfidf = tfidf[:31962,:] \ntest_tfidf = tfidf[31962:,:] \n\nxtrain_tfidf = train_tfidf[ytrain.index] \nxvalid_tfidf = train_tfidf[yvalid.index]\n\nlreg.fit(xtrain_tfidf, ytrain) \n\nprediction = lreg.predict_proba(xvalid_tfidf) \nprediction_int = prediction[:,1] >= 0.3 \nprediction_int = prediction_int.astype(np.int) \n\nf1_score(yvalid, prediction_int) # calculating f1 score for the validation set","execution_count":null,"outputs":[]},{"metadata":{"id":"QnosxnQx0ShV"},"cell_type":"markdown","source":"### Word2Vec Features"},{"metadata":{"id":"QcsyzYao0ShW","outputId":"05325c8b-0126-4c51-b260-4963155e6ec5","trusted":true},"cell_type":"code","source":"train_w2v = wordvec_df.iloc[:31962,:] \ntest_w2v = wordvec_df.iloc[31962:,:] \n\nxtrain_w2v = train_w2v.iloc[ytrain.index,:]\nxvalid_w2v = train_w2v.iloc[yvalid.index,:]\n\nlreg.fit(xtrain_w2v, ytrain) \n\nprediction = lreg.predict_proba(xvalid_w2v) \nprediction_int = prediction[:,1] >= 0.3 \nprediction_int = prediction_int.astype(np.int) \n\nf1_score(yvalid, prediction_int)","execution_count":null,"outputs":[]},{"metadata":{"id":"5prUKlDy0ShY"},"cell_type":"markdown","source":"### Doc2Vec Features"},{"metadata":{"id":"KGNDidtd0ShZ","outputId":"dd1974ea-4142-4180-fecd-441a4347d102","trusted":true},"cell_type":"code","source":"train_d2v = docvec_df.iloc[:31962,:] \ntest_d2v = docvec_df.iloc[31962:,:] \n\nxtrain_d2v = train_d2v.iloc[ytrain.index,:] \nxvalid_d2v = train_d2v.iloc[yvalid.index,:]\n\nlreg.fit(xtrain_d2v, ytrain) \n\nprediction = lreg.predict_proba(xvalid_d2v) \nprediction_int = prediction[:,1] >= 0.3 \nprediction_int = prediction_int.astype(np.int) \n\nf1_score(yvalid, prediction_int)","execution_count":null,"outputs":[]},{"metadata":{"id":"mb0TIfFL0Sha"},"cell_type":"markdown","source":"## Support Vector Machine (SVM)"},{"metadata":{"id":"_yyFlZdi0Shb","trusted":true},"cell_type":"code","source":"from sklearn import svm","execution_count":null,"outputs":[]},{"metadata":{"id":"7_VGHdJA0Shc"},"cell_type":"markdown","source":"### Bag-of-Words Features"},{"metadata":{"id":"3iMty7gI0Shc","outputId":"496b0bf4-0f8d-4800-af8d-7fa18568cf2c","trusted":true},"cell_type":"code","source":"svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_bow, ytrain) \n\nprediction = svc.predict_proba(xvalid_bow) \nprediction_int = prediction[:,1] >= 0.3 \nprediction_int = prediction_int.astype(np.int)\n\nf1_score(yvalid, prediction_int)","execution_count":null,"outputs":[]},{"metadata":{"id":"Nm0H7RN30Shd"},"cell_type":"markdown","source":"### TF-IDF Features"},{"metadata":{"id":"GeOVMtZe0She","outputId":"8dd19482-c522-4361-d049-69a5a8934f61","trusted":true},"cell_type":"code","source":"svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_tfidf, ytrain) \n\nprediction = svc.predict_proba(xvalid_tfidf) \nprediction_int = prediction[:,1] >= 0.3 \nprediction_int = prediction_int.astype(np.int) \n\nf1_score(yvalid, prediction_int)","execution_count":null,"outputs":[]},{"metadata":{"id":"ccU8RWfe0Shf"},"cell_type":"markdown","source":"### Word2Vec Features"},{"metadata":{"id":"dON4qVBO0Shh","outputId":"c1569902-8c24-4e1e-bada-37db97421669","trusted":true},"cell_type":"code","source":"svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_w2v, ytrain) \n\nprediction = svc.predict_proba(xvalid_w2v) \nprediction_int = prediction[:,1] >= 0.3 \nprediction_int = prediction_int.astype(np.int) \n\nf1_score(yvalid, prediction_int)","execution_count":null,"outputs":[]},{"metadata":{"id":"n1FzfoNs0Shi"},"cell_type":"markdown","source":"### Doc2Vec Features"},{"metadata":{"id":"z3NCw9eq0Shj","outputId":"fd46b864-05fb-4cb5-ef9e-dc7fce7fea83","trusted":true},"cell_type":"code","source":"svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_d2v, ytrain) \n\nprediction = svc.predict_proba(xvalid_d2v) \nprediction_int = prediction[:,1] >= 0.3 \nprediction_int = prediction_int.astype(np.int) \n\nf1_score(yvalid, prediction_int)","execution_count":null,"outputs":[]},{"metadata":{"id":"gx22jc2X0Shl"},"cell_type":"markdown","source":"## RandomForest"},{"metadata":{"id":"sgWalkjm0Shm","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"id":"uETnXraz0Shn"},"cell_type":"markdown","source":"### Bag-of-Words Features"},{"metadata":{"id":"lYoPx75t0Sho","outputId":"52c8c225-ec75-44f5-8a79-c00a93ebf3b1","trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_bow, ytrain) \n\nprediction = rf.predict(xvalid_bow) \n\nf1_score(yvalid, prediction)","execution_count":null,"outputs":[]},{"metadata":{"id":"A3S9T-O-0Shq"},"cell_type":"markdown","source":"### TF-IDF Features"},{"metadata":{"id":"LdnzWyQQ0Shr","outputId":"2e2ec772-a17d-411f-dc1d-6108e9e550e3","trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_tfidf, ytrain) \n\nprediction = rf.predict(xvalid_tfidf) \n\nf1_score(yvalid, prediction)","execution_count":null,"outputs":[]},{"metadata":{"id":"6_nT6OHm0Shu"},"cell_type":"markdown","source":"### Word2Vec Features"},{"metadata":{"id":"F8wvEbWi0Shv","outputId":"cdf194c8-01da-40c1-de40-d353a6ae3432","trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_w2v, ytrain) \n\nprediction = rf.predict(xvalid_w2v) \n\nf1_score(yvalid, prediction)","execution_count":null,"outputs":[]},{"metadata":{"id":"3ccaLE1e0Shw"},"cell_type":"markdown","source":"### Doc2Vec Features"},{"metadata":{"id":"zfT2Cenx0Shx","outputId":"8aaf6bb5-ad41-4ef6-b9e7-c5217d30d5b6","trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_d2v, ytrain) \n\nprediction = rf.predict(xvalid_d2v) \n\nf1_score(yvalid, prediction)","execution_count":null,"outputs":[]},{"metadata":{"id":"hX5jBnvc0Sh0"},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"id":"vAEx9ZVE0Sh1","trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"id":"FFaBdJv_0Sh2"},"cell_type":"markdown","source":"### Bag-of-Words Features"},{"metadata":{"id":"To1dbZxC0Sh4","outputId":"b2eb3080-cdb2-46d5-d4ed-2d84ba2e87d0","trusted":true},"cell_type":"code","source":"xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_bow, ytrain) \n\nprediction = xgb_model.predict(xvalid_bow) \n\nf1_score(yvalid, prediction)","execution_count":null,"outputs":[]},{"metadata":{"id":"PQoaLO3b0Sh6"},"cell_type":"markdown","source":"### TF-IDF Features"},{"metadata":{"id":"iHPRewAp0Sh8","outputId":"cd087e53-ab2b-46ae-e331-c54e101c8c47","trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_tfidf, ytrain)\n\nprediction = xgb.predict(xvalid_tfidf) \n\nf1_score(yvalid, prediction)","execution_count":null,"outputs":[]},{"metadata":{"id":"RzUuO0fr0Sh-"},"cell_type":"markdown","source":"### Word2Vec Features"},{"metadata":{"id":"nkm9piA30SiA","outputId":"e1e9af4e-bbe5-4d80-c246-31bcab570298","trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(max_depth=6, n_estimators=1000, nthread= 3).fit(xtrain_w2v, ytrain)\n\nprediction = xgb.predict(xvalid_w2v) \n\nf1_score(yvalid, prediction)","execution_count":null,"outputs":[]},{"metadata":{"id":"se8EcAKI0SiD"},"cell_type":"markdown","source":"### Doc2Vec Features"},{"metadata":{"id":"vjDwWMSt0SiE","outputId":"f0135b61-231e-4970-9b3d-6d1eb9a55c38","trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(max_depth=6, n_estimators=1000, nthread= 3).fit(xtrain_d2v, ytrain)\n\nprediction = xgb.predict(xvalid_d2v)\n\nf1_score(yvalid, prediction)","execution_count":null,"outputs":[]},{"metadata":{"id":"-rWXxcld0SiF"},"cell_type":"markdown","source":"## Results :"},{"metadata":{"id":"WgcLbKaN0SiG"},"cell_type":"markdown","source":"|  | Logistic Regression | SVM      | RandomForest       | XGBoost       |\n|:--------:|  :-----------:  |  :-----------:  |  :-----------:  |  :-----------:  |\n| Bag of words      | 0.53       | 0.51       | 0.55       | 0.51       |\n| TF-IDF   | 0.55        | 0.51        | 0.56        | 0.52        |\n| Word2Vec | 0.61       | 0.62        | 0.50        | 0.65        |\n| Doc2Vec  | 0.39        | 0.16        | 0.07        | 0.34       |"},{"metadata":{"id":"0kqF-Mvn0SiH"},"cell_type":"markdown","source":"## FineTuning XGBoost + Word2Vec"},{"metadata":{"id":"6jR0kgA-0SiJ"},"cell_type":"markdown","source":"XGBoost with Word2Vec model has given us the best performance so far. Let’s try to tune it further to extract as much from it as we can. XGBoost has quite a many tuning parameters and sometimes it becomes tricky to properly tune them. This is what we are going to do in the following steps."},{"metadata":{"id":"E7rqAwnc0SiL","trusted":true},"cell_type":"code","source":"import xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"id":"CU0F2T8Z0SiM"},"cell_type":"markdown","source":"Here we will use DMatrices. A DMatrix can contain both the features and the target."},{"metadata":{"id":"vycX_imG0SiO","trusted":true},"cell_type":"code","source":"dtrain = xgb.DMatrix(xtrain_w2v, label=ytrain) \ndvalid = xgb.DMatrix(xvalid_w2v, label=yvalid) \ndtest = xgb.DMatrix(test_w2v)\n# Parameters that we are going to tune \nparams = {\n    'objective':'binary:logistic',\n    'max_depth':6,\n    'min_child_weight': 1,\n    'eta':.3,\n    'subsample': 1,\n    'colsample_bytree': 1\n }","execution_count":null,"outputs":[]},{"metadata":{"id":"Un-byr2F0SiP"},"cell_type":"markdown","source":"We will prepare a custom evaluation function to calculate F1 score."},{"metadata":{"id":"uYPcH4ot0SiQ","trusted":true},"cell_type":"code","source":"def custom_eval(preds, dtrain):\n    labels = dtrain.get_label().astype(np.int)\n    preds = (preds >= 0.3).astype(np.int)\n    return [('f1_score', f1_score(labels, preds))]","execution_count":null,"outputs":[]},{"metadata":{"id":"emL2O2-G0SiR"},"cell_type":"markdown","source":"### General Approach for Parameter Tuning\n\nWe will follow the steps below to tune the parameters:\n\n- Choose a relatively high learning rate. Usually a learning rate of 0.3 is used at this stage.\n\n- Tune tree-specific parameters such as max_depth, min_child_weight, subsample, colsample_bytree keeping the learning rate fixed.\n\n- Tune the learning rate.\n\n- Finally tune gamma to avoid overfitting."},{"metadata":{"id":"FzSPGI3b0SiS"},"cell_type":"markdown","source":"#### Tuning max_depth and min_child_weight"},{"metadata":{"id":"J_LreCqm0SiT","trusted":true},"cell_type":"code","source":"gridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in range(6,10)\n     for min_child_weight in range(5,8)\n ]","execution_count":null,"outputs":[]},{"metadata":{"id":"Nnl29IVv-Gwt","outputId":"3bb5566d-ff9e-43ea-b73b-13d40714acb9","trusted":true},"cell_type":"code","source":"max_f1 = 0.\nbest_params = None \nfor max_depth, min_child_weight in gridsearch_params:\n  print(\"CV with max_depth={}, min_child_weight={}\".format(max_depth, min_child_weight))\n  params['max_depth'] = max_depth\n  params['min_child_weight'] = min_child_weight\n  cv_results = xgb.cv(params, dtrain, feval= custom_eval, num_boost_round=200, maximize=True, seed=16, nfold=5, early_stopping_rounds=10)     \n  mean_f1 = cv_results['test-f1_score-mean'].max()\n  boost_rounds = cv_results['test-f1_score-mean'].argmax()    \n  print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))    \n  if mean_f1 > max_f1:\n    max_f1 = mean_f1\n    best_params = (max_depth,min_child_weight) \n\nprint(\"Best params: {}, {}, F1 Score: {}\".format(best_params[0], best_params[1], max_f1))","execution_count":null,"outputs":[]},{"metadata":{"id":"pQSuoJjFUuyl","trusted":true},"cell_type":"code","source":"params['max_depth'] = 7 \nparams['min_child_weight'] = 6","execution_count":null,"outputs":[]},{"metadata":{"id":"Nwk7ZA0nf2Ou","trusted":true},"cell_type":"code","source":"gridsearch_params = [\n    (subsample, colsample)\n    for subsample in [i/10. for i in range(5,10)]\n    for colsample in [i/10. for i in range(5,10)] ]","execution_count":null,"outputs":[]},{"metadata":{"id":"eNBwRoHKf5Ar","outputId":"ce895173-10ef-43d3-885f-9b1ad5497756","trusted":true},"cell_type":"code","source":"max_f1 = 0. \nbest_params = None \nfor subsample, colsample in gridsearch_params:\n  print(\"CV with subsample={}, colsample={}\".format(\n                             subsample,\n                             colsample))\n  params['colsample'] = colsample\n  params['subsample'] = subsample\n  cv_results = xgb.cv(params, dtrain, feval= custom_eval, num_boost_round=200, maximize=True, seed=16, nfold=5, early_stopping_rounds=10)\n  mean_f1 = cv_results['test-f1_score-mean'].max()\n  boost_rounds = cv_results['test-f1_score-mean'].argmax()\n  print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n  if mean_f1 > max_f1:\n    max_f1 = mean_f1\n    best_params = (subsample, colsample) \n\nprint(\"Best params: {}, {}, F1 Score: {}\".format(best_params[0], best_params[1], max_f1))","execution_count":null,"outputs":[]},{"metadata":{"id":"D5Fp_IgVf7XM","trusted":true},"cell_type":"code","source":"params['subsample'] = 0.8 \nparams['colsample_bytree'] = 0.5","execution_count":null,"outputs":[]},{"metadata":{"id":"V26gzVq0g-12"},"cell_type":"markdown","source":"#### Now let’s tune the learning rate."},{"metadata":{"id":"qxSwvKOphAHT","outputId":"61912ce9-6bb5-4a3b-c03c-4175da3302ee","trusted":true},"cell_type":"code","source":"max_f1 = 0. \nbest_params = None \nfor eta in [.3, .2, .1, .05, .01, .005]:\n    print(\"CV with eta={}\".format(eta))\n     # Update ETA\n    params['eta'] = eta\n\n     # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        feval= custom_eval,\n        num_boost_round=1000,\n        maximize=True,\n        seed=16,\n        nfold=5,\n        early_stopping_rounds=20\n    )\n\n     # Finding best F1 Score\n    mean_f1 = cv_results['test-f1_score-mean'].max()\n    boost_rounds = cv_results['test-f1_score-mean'].argmax()\n    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n    if mean_f1 > max_f1:\n        max_f1 = mean_f1\n        best_params = eta \nprint(\"Best params: {}, F1 Score: {}\".format(best_params, max_f1))","execution_count":null,"outputs":[]},{"metadata":{"id":"rMV9-_PvhJfZ","trusted":true},"cell_type":"code","source":"params['eta'] = 0.1","execution_count":null,"outputs":[]},{"metadata":{"id":"u6WMktZ_hIwb"},"cell_type":"markdown","source":"Let’s have a look at the final list of tuned parameters.\n\n{'colsample': 0.8,\n\n'colsample_bytree': 0.5, 'eta': 0.1,\n\n'max_depth': 7, 'min_child_weight': 6,\n\n'objective': 'binary:logistic',\n\n'subsample': 0.8}\n\nFinally we can now use these tuned parameters in our xgboost model. We have used early stopping of 10 which means if the model’s performance doesn’t improve under 10 rounds, then the model training will be stopped."},{"metadata":{"id":"oSNfY9wK1Vst","outputId":"822001c5-116f-481a-8616-1a662deddd04","trusted":true},"cell_type":"code","source":"xgb_model = xgb.train(\n    params,\n    dtrain,\n    feval= custom_eval,\n    num_boost_round= 1000,\n    maximize=True,\n    evals=[(dvalid, \"Validation\")],\n    early_stopping_rounds=10\n )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}