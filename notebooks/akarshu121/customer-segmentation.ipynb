{"cells":[{"metadata":{},"cell_type":"markdown","source":"Task : \n1. Analyzing the content of the E-Commerce database that contains the purchases made by approximate ~4000 customers over a period of one year\n2. To develop a model that allows to anticipate or predict the purchases that will be made by a new customer during the following & this year, based on the data available from the previous purchases.\n\nSteps : \n1. Data Preparation\n2. Exploring the content of variables (in short, Exploratory Data Analysis, not inlcuding the product category)\n3. Intuitions about the categories of the product, the customer buys\n4. Categories of the Customer that buys the product\n5. Classifying the Customers based on some parameters\n6. Testing the Predictions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Understanding the content of Data\n### \"This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers. \n\n#  1. Data Preparation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing all the required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime, nltk, warnings\nimport matplotlib.cm as cm\nimport itertools\nfrom pathlib import Path\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn import preprocessing, model_selection, metrics, feature_selection\nfrom sklearn.model_selection import GridSearchCV, learning_curve\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import neighbors, linear_model, svm, tree, ensemble\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.decomposition import PCA\nfrom IPython.display import display, HTML\nimport plotly\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode,iplot\ninit_notebook_mode(connected=True)\nwarnings.filterwarnings(\"ignore\")                                    # Ignoring the warnings\nplt.rcParams[\"patch.force_edgecolor\"] = True                         # Setting the default style of Matplotlib\nplt.style.use('fivethirtyeight')\nmpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":false,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Loading the Data\ndf_initial = pd.read_csv(r'../input/ecommerce-data/data.csv',encoding=\"ISO-8859-1\",\n                         dtype={'CustomerID': str,'InvoiceID': str})\nprint('Dataframe dimensions:', df_initial.shape)\n#------------------------------------------------\ndf_initial['InvoiceDate'] = pd.to_datetime(df_initial['InvoiceDate'])\n#------------------------------------------------\n# getting the information about the column type and the null values\ntab_info=pd.DataFrame(df_initial.dtypes).T.rename(index={0:'column type'})\ntab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()).T.rename(index={0:'null values (no.)'}))\ntab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()/df_initial.shape[0]*100).T.\n                         rename(index={0:'null values (%)'}))\ndisplay(tab_info)\n#---------------------------------------------------\n# show first 5 lines\ndisplay(df_initial[:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Observations:\n* Since null values, play an important role in the analysis, we can observe that from the 'CustomerID' about 25 % data is missing. So, an simple and easy way to handle those missing values is to remove thpose missing values. And, I think that removing those customers, we will be able to remove the missing values that are present in the 'Description' column.\n\n\n# 2. Exploring the content of variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_initial.dropna(axis = 0, subset = ['CustomerID'], inplace = True)\nprint('Dataframe dimensions:', df_initial.shape)\n#--------------------------------------------------\n# gives some information on columns types and numer of null values in each column\ntab_info=pd.DataFrame(df_initial.dtypes).T.rename(index={0:'column type'})\ntab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()).T.rename(index={0:'null values (no)'}))\ntab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()/df_initial.shape[0]*100).T.\n                         rename(index={0:'null values (%)'}))\ndisplay(tab_info)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* So, we have removed the missing values, and as intuited the missing values present in the 'Description' cloumn has also been removed. So, now the next step would be as mentioned below:\n\n* Some orders have been cancelled and hence, in order to nullify those experience, the thing that the retailer have done is to create a new order in which the quantites have been the negative of the previously ordered quantities.\n\n* So, now let us go for the analysis of the duplicate enteries.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Duplicate Entries: {}'.format(df_initial.duplicated().sum()))\ndf_initial.drop_duplicates(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Ok, the dupicated enteries are 5225. We will go for the analysis and the dealing procedure for handling the duplicate enteries.\n\n* But before that, let us see that which are the primary countries from which the orders are coming.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = df_initial[['CustomerID', 'InvoiceNo', 'Country']].groupby(['CustomerID', 'InvoiceNo', 'Country']).count()\ntemp = temp.reset_index(drop = False)\ncountries = temp['Country'].value_counts()\nprint('Total number of countries: {}'.format(len(countries)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = dict(type='choropleth',\nlocations = countries.index,\nlocationmode = 'country names', z = countries,\ntext = countries.index, colorbar = {'title':'Order nb.'},\ncolorscale=[[0, 'rgb(224,255,255)'],\n            [0.01, 'rgb(166,206,227)'], [0.02, 'rgb(31,120,180)'],\n            [0.03, 'rgb(178,223,138)'], [0.05, 'rgb(51,160,44)'],\n            [0.10, 'rgb(251,154,153)'], [0.20, 'rgb(255,255,0)'],\n            [1, 'rgb(227,26,28)']],    \nreversescale = False)\n#--------------------\nlayout = dict(title='Number of orders per country',\ngeo = dict(showframe = True, projection={'type':'mercator'}))\n#--------------------\nchoromap = go.Figure(data = [data], layout = layout)\niplot(choromap, validate=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* So, the primary country from which the orders are coming is mainly United Kingdoms with 19.5K orders approximately, followed by France and Germany\n\n# 3. Intuitons about the categories of the product, the customer buys.\n\n* Now, let us see that the orders that are being processed have how many different values coresponding to different categories.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame([{'products': len(df_initial['StockCode'].value_counts()),    \n               'transactions': len(df_initial['InvoiceNo'].value_counts()),\n               'customers': len(df_initial['CustomerID'].value_counts()),  \n              }], columns = ['products', 'transactions', 'customers'], index = ['quantity'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* I can see in the dataframe that some orders have the InvoiceNo to be starting with the letter 'C' which indicates that the order have been cancelled. So, let us see those orders","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = df_initial.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate'].count()\nno_products_per_basket = temp.rename(columns = {'InvoiceDate':'Number of products'})\nno_products_per_basket[:10].sort_values('CustomerID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_products_per_basket['order_canceled'] = no_products_per_basket['InvoiceNo'].apply(lambda x:int('C' in x))\ndisplay(no_products_per_basket[:5])\n#---------------------------------------\nn1 = no_products_per_basket['order_canceled'].sum()\nn2 = no_products_per_basket.shape[0]\nprint('Number of orders cancelled: {}/{} ({:.2f}%) '.format(n1, n2, n1/n2*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(df_initial.sort_values('CustomerID')[:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point, I decide to create a new variable in the dataframe that indicate if part of the command has been canceled. For the cancellations without counterparts, a few of them are probably due to the fact that the buy orders were performed before December 2010 (the point of entry of the database). Below, I make a census of the cancel orders and check for the existence of counterparts:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cleaned = df_initial.copy(deep = True)\ndf_cleaned['QuantityCanceled'] = 0\n\nentry_to_remove = [] ; doubtfull_entry = []\n\nfor index, col in  df_initial.iterrows():\n    if (col['Quantity'] > 0) or col['Description'] == 'Discount': continue        \n    df_test = df_initial[(df_initial['CustomerID'] == col['CustomerID']) &\n                         (df_initial['StockCode']  == col['StockCode']) & \n                         (df_initial['InvoiceDate'] < col['InvoiceDate']) & \n                         (df_initial['Quantity']   > 0)].copy()\n    #-----------------------------\n    # Cancelation WITHOUT counterpart\n    if (df_test.shape[0] == 0): \n        doubtfull_entry.append(index)\n    #-----------------------\n    # Cancelation WITH a counterpart\n    elif (df_test.shape[0] == 1): \n        index_order = df_test.index[0]\n        df_cleaned.loc[index_order, 'QuantityCanceled'] = -col['Quantity']\n        entry_to_remove.append(index)        \n    #--------------------------\n    # Various counterparts exist in orders: we delete the last one\n    elif (df_test.shape[0] > 1): \n        df_test.sort_index(axis=0 ,ascending=False, inplace = True)        \n        for ind, val in df_test.iterrows():\n            if val['Quantity'] < -col['Quantity']: continue\n            df_cleaned.loc[ind, 'QuantityCanceled'] = -col['Quantity']\n            entry_to_remove.append(index) \n            break     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above function, I checked the two cases:\n\n1. a cancel order exists without counterpart\n2. there's at least one counterpart with the exact same quantity\n\nThe index of the corresponding cancel order are respectively kept in the doubtfull_entry and entry_to_remove lists whose sizes are:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"entry_to_remove: {}\".format(len(entry_to_remove)))\nprint(\"doubtfull_entry: {}\".format(len(doubtfull_entry)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, now as decided we will be dropping the rows belonging to 'entry_to_remove' as well as 'doubtfull_entry'\n\nBut wait, what if some orders which corresponds to discount did not come into the function adn did not got deleted. So, let us check for the same.!!!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cleaned.drop(entry_to_remove, axis = 0, inplace = True)\ndf_cleaned.drop(doubtfull_entry, axis = 0, inplace = True)\nremaining_entries = df_cleaned[(df_cleaned['Quantity'] < 0) & (df_cleaned['StockCode'] != 'D')]\nprint(\"no of entries to delete: {}\".format(remaining_entries.shape[0]))\nremaining_entries[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, now we have performed the Filtering of the Data, let us go for the stock-code and analysis what are the various categories encrypted in the 'StockCode' column!!!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"list_special_codes = df_cleaned[df_cleaned['StockCode'].str.contains('^[a-zA-Z]+', regex=True)]['StockCode'].unique()\nlist_special_codes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, as we know what are the codes for the stockcodes, let us see what do they really stand for, I mean to say that what are th fullforms or the terminology for the stockcode that is given.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for code in list_special_codes:\n    print(\"{:<15} -> {:<30}\".format(code, df_cleaned[df_cleaned['StockCode'] == code]['Description'].unique()[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we have seen, the description of the stockcode, now it is time for creating a column named TotalPrice, which will be calculating:\n\nUnit Price * Quantities","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cleaned['TotalPrice'] = df_cleaned['UnitPrice'] * (df_cleaned['Quantity'] - df_cleaned['QuantityCanceled'])\ndf_cleaned.sort_values('CustomerID')[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each entry of the dataframe indicates prices for a single kind of product. Hence, orders are split on several lines. I collect all the purchases made during a single order to recover the total order price. in short, creating a new column which includes the total basket price of an individual customer which simplifies the analysis.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-------------------------------\n# sum of purchases / user & order\ntemp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['TotalPrice'].sum()\nbasket_price = temp.rename(columns = {'TotalPrice':'Basket Price'})\n#-----------------\n# date of the order\ndf_cleaned['InvoiceDate_int'] = df_cleaned['InvoiceDate'].astype('int64')\ntemp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate_int'].mean()\ndf_cleaned.drop('InvoiceDate_int', axis = 1, inplace = True)\nbasket_price.loc[:, 'InvoiceDate'] = pd.to_datetime(temp['InvoiceDate_int'])\n#-------------------\n# selection of significant entries:\nbasket_price = basket_price[basket_price['Basket Price'] > 0]\nbasket_price.sort_values('CustomerID')[:6]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since, Pie-Chart is an important visualization for describing the total breakdown of the single entity, ehich in our cases are the total categories from which the revenues come (i.e the various segment budgets ), we will visualizae it now.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Purchase statement\nprice_range = [0, 50, 100, 200, 500, 1000, 5000, 50000]\ncount_price = []\nfor i, price in enumerate(price_range):\n    if i == 0: continue\n    val = basket_price[(basket_price['Basket Price'] < price) &\n                       (basket_price['Basket Price'] > price_range[i-1])]['Basket Price'].count()\n    count_price.append(val)\n#-----------------------------------------\n# Representation of the number of purchases / amount       \nplt.rc('font', weight='bold')\nf, ax = plt.subplots(figsize=(11, 6))\ncolors = ['yellowgreen', 'gold', 'wheat', 'c', 'violet', 'royalblue','firebrick']\nlabels = [ '{}<.<{}'.format(price_range[i-1], s) for i,s in enumerate(price_range) if i != 0]\nsizes  = count_price\nexplode = [0.0 if sizes[i] < 100 else 0.0 for i in range(len(sizes))]\nax.pie(sizes, explode = explode, labels=labels, colors = colors,\n       autopct = lambda x:'{:1.0f}%'.format(x) if x > 1 else '',\n       shadow = False, startangle=0)\nax.axis('equal')\nf.text(0.5, 1.01, \"Breakdown of order amounts\", ha='center', fontsize = 18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, the most revenues are generated from the segmented which are ∼ 65% of purchases give prices in excess of £ 200.\n\nNow for the word analysis, we would extract from the Description variable the information that will prove useful. To do this, we will use the following function:\n\nThis function takes as input the dataframe and analyzes the content of the Description column by performing the following operations:\n\n* extract the names (proper, common) appearing in the products description\n* for each name, I extract the root of the word and aggregate the set of names associated with this particular root\n* count the number of times each root appears in the dataframe\n* when several words are listed for the same root, I consider that the keyword associated with this root is the shortest name (this systematically selects the singular when there are singular/plural variants)\n* The first step of the analysis is to retrieve the list of products:\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"is_noun = lambda pos: pos[:2] == 'NN'\n\ndef keywords_inventory(dataframe, colonne = 'Description'):\n    stemmer = nltk.stem.SnowballStemmer(\"english\")\n    keywords_roots  = dict()  # collect the words / root\n    keywords_select = dict()  # association: root <-> keyword\n    category_keys   = []\n    count_keywords  = dict()\n    icount = 0\n    for s in dataframe[colonne]:\n        if pd.isnull(s): continue\n        lines = s.lower()\n        tokenized = nltk.word_tokenize(lines)\n        nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n        \n        for t in nouns:\n            t = t.lower() ; racine = stemmer.stem(t)\n            if racine in keywords_roots:                \n                keywords_roots[racine].add(t)\n                count_keywords[racine] += 1                \n            else:\n                keywords_roots[racine] = {t}\n                count_keywords[racine] = 1\n    \n    for s in keywords_roots.keys():\n        if len(keywords_roots[s]) > 1:  \n            min_length = 1000\n            for k in keywords_roots[s]:\n                if len(k) < min_length:\n                    clef = k ; min_length = len(k)            \n            category_keys.append(clef)\n            keywords_select[s] = clef\n        else:\n            category_keys.append(list(keywords_roots[s])[0])\n            keywords_select[s] = list(keywords_roots[s])[0]\n                   \n    print(\"No. of keywords in variable '{}': {}\".format(colonne,len(category_keys)))\n    return category_keys, keywords_roots, keywords_select, count_keywords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_products = pd.DataFrame(df_initial['Description'].unique()).rename(columns = {0:'Description'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Categories of the Customer that buys the product","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#nltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let us see the number of keywords in variable 'Description'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords, keywords_roots, keywords_select, count_keywords = keywords_inventory(df_products)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The execution of this function returns three variables:\n\n* keywords: the list of extracted keywords\n* keywords_roots: a dictionary where the keys are the keywords roots and the values are the lists of words associated with those roots\n* count_keywords: dictionary listing the number of times every word is used\n\nAt this point, I convert the count_keywords dictionary into a list, to sort the keywords according to their occurences:\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"list_products = []\nfor k,v in count_keywords.items():\n    list_products.append([keywords_select[k],v])\nlist_products.sort(key = lambda x:x[1], reverse = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will create a graph of the most frequently occuring words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"liste = sorted(list_products, key = lambda x:x[1], reverse = True)\n#--------------------------\nplt.rc('font', weight='normal')\nfig, ax = plt.subplots(figsize=(7, 25))\ny_axis = [i[1] for i in liste[:125]]\nx_axis = [k for k,i in enumerate(liste[:125])]\nx_label = [i[0] for i in liste[:125]]\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 13)\nplt.yticks(x_axis, x_label)\nplt.xlabel(\"No. of occurences\", fontsize = 18, labelpad = 10)\nax.barh(x_axis, y_axis, align = 'center')\nax = plt.gca()\nax.invert_yaxis()\n#---------------------------------------------------\nplt.title(\"Words occurence\",bbox={'facecolor':'k', 'pad':5}, color='w',fontsize = 25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The list that was obtained contains more than 1400 keywords and the most frequent ones appear in more than 200 products. However, while examinating the content of the list, I note that some names are useless. Others are do not carry information, like colors. Therefore, I discard these words from the analysis that follows and also, I decide to consider only the words that appear more than 13 times.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"list_products = []\nfor k,v in count_keywords.items():\n    word = keywords_select[k]\n    if word in ['pink', 'blue', 'tag', 'green', 'orange']: continue\n    if len(word) < 3 or v < 13: continue\n    if ('+' in word) or ('/' in word): continue\n    list_products.append([word, v])\n#-------------------------------------------  \nlist_products.sort(key = lambda x:x[1], reverse = True)\nprint('preserved words:', len(list_products))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"liste_produits = df_cleaned['Description'].unique()\nX = pd.DataFrame()\nfor key, occurence in list_products:\n    X.loc[:, key] = list(map(lambda x:int(key.upper() in x), liste_produits))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = [0, 1, 2, 3, 5, 10]\nlabel_col = []\nfor i in range(len(threshold)):\n    if i == len(threshold)-1:\n        col = '.>{}'.format(threshold[i])\n    else:\n        col = '{}<.<{}'.format(threshold[i],threshold[i+1])\n    label_col.append(col)\n    X.loc[:, col] = 0\n\nfor i, prod in enumerate(liste_produits):\n    prix = df_cleaned[ df_cleaned['Description'] == prod]['UnitPrice'].mean()\n    j = 0\n    while prix > threshold[j]:\n        j+=1\n        if j == len(threshold): break\n    X.loc[i, label_col[j-1]] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The  X  matrix indicates the words contained in the description of the products using the one-hot-encoding principle. In practice, I have found that introducing the price range results in more balanced groups in terms of element numbers. Hence, I add 6 extra columns to this matrix, where I indicate the price range of the products:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"{:<8} {:<20} \\n\".format('range', 'no. of products') + 20*'-')\nfor i in range(len(threshold)):\n    if i == len(threshold)-1:\n        col = '.>{}'.format(threshold[i])\n    else:\n        col = '{}<.<{}'.format(threshold[i],threshold[i+1])    \n    print(\"{:<10}  {:<20}\".format(col, X.loc[:, col].sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this section, I will group the products into different classes. In the case of matrices with binary encoding, the most suitable metric for the calculation of distances is the Hamming's metric. Note that the kmeans method of sklearn uses a Euclidean distance that can be used, but it is not to the best choice in the case of categorical variables. However, in order to use the Hamming's metric, we need to use the kmodes package which is not available on the current plateform. Hence, I use the kmeans method even if this is not the best choice.\n\nIn order to define (approximately) the number of clusters that best represents the data, I use the silhouette score:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = X.values\nfor n_clusters in range(3,10):\n    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n    kmeans.fit(matrix)\n    clusters = kmeans.predict(matrix)\n    silhouette_avg = silhouette_score(matrix, clusters)\n    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In practice, the scores obtained above can be considered equivalent since, depending on the run, scores of  0.1±0.05  will be obtained for all clusters with n_clusters  >  3 (we obtain slightly lower scores for the first cluster). On the other hand, I found that beyond 5 clusters, some clusters contained very few elements. I therefore choose to separate the dataset into 5 clusters. In order to ensure a good classification at every run of the notebook, I iterate untill we obtain the best possible silhouette score, which is, in the present case, around 0.15:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_clusters = 5\nsilhouette_avg = -1\nwhile silhouette_avg < 0.145:\n    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n    kmeans.fit(matrix)\n    clusters = kmeans.predict(matrix)\n    silhouette_avg = silhouette_score(matrix, clusters)\n    \n    #km = kmodes.KModes(n_clusters = n_clusters, init='Huang', n_init=2, verbose=0)\n    #clusters = km.fit_predict(matrix)\n    #silhouette_avg = silhouette_score(matrix, clusters)\n    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I check the number of elements in every class:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(clusters).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"a / Silhouette intra-cluster score\n\nIn order to have an insight on the quality of the classification, we can represent the silhouette scores of each element of the different clusters. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def graph_component_silhouette(n_clusters, lim_x, mat_size, sample_silhouette_values, clusters):\n    plt.rcParams[\"patch.force_edgecolor\"] = True\n    plt.style.use('fivethirtyeight')\n    mpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\n    #------------------------------\n    fig, ax1 = plt.subplots(1, 1)\n    fig.set_size_inches(8, 8)\n    ax1.set_xlim([lim_x[0], lim_x[1]])\n    ax1.set_ylim([0, mat_size + (n_clusters + 1) * 10])\n    y_lower = 10\n    for i in range(n_clusters):\n        #-------------------------------------------------\n        # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n        ith_cluster_silhouette_values = sample_silhouette_values[clusters == i]\n        ith_cluster_silhouette_values.sort()\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n        cmap = cm.get_cmap(\"Spectral\")\n        color = cmap(float(i) / n_clusters)        \n        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values,\n                           facecolor=color, edgecolor=color, alpha=0.8)\n        #--------------------------------------------------------------\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.03, y_lower + 0.5 * size_cluster_i, str(i), color = 'red', fontweight = 'bold',\n                bbox=dict(facecolor='white', edgecolor='black', boxstyle='round, pad=0.3'))\n        #------------------------------\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define individual silouhette scores\nsample_silhouette_values = silhouette_samples(matrix, clusters)\n#--------------------------\n# and do the graph\ngraph_component_silhouette(n_clusters, [-0.07, 0.33], len(X), sample_silhouette_values, clusters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"b/ Word Cloud\n\nNow we can have a look at the type of objects that each cluster represents. In order to obtain a global view of their contents, I determine which keywords are the most frequent in each of them","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"liste = pd.DataFrame(liste_produits)\nliste_words = [word for (word, occurence) in list_products]\n\noccurence = [dict() for _ in range(n_clusters)]\n\nfor i in range(n_clusters):\n    liste_cluster = liste.loc[clusters == i]\n    for word in liste_words:\n        if word in ['art', 'set', 'heart', 'pink', 'blue', 'tag']: continue\n        occurence[i][word] = sum(liste_cluster.loc[:, 0].str.contains(word.upper()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making the wordcloud","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#--------------------------------\ndef random_color_func(word=None, font_size=None, position=None,\n                      orientation=None, font_path=None, random_state=None):\n    h = int(360.0 * tone / 255.0)\n    s = int(100.0 * 255.0 / 255.0)\n    l = int(100.0 * float(random_state.randint(70, 120)) / 255.0)\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n#-------------------------------------------\ndef make_wordcloud(liste, increment):\n    ax1 = fig.add_subplot(4,2,increment)\n    words = dict()\n    trunc_occurences = liste[0:150]\n    for s in trunc_occurences:\n        words[s[0]] = s[1]\n    #----------------------------------------------\n    wordcloud = WordCloud(width=1000,height=400, background_color='lightgrey', \n                          max_words=1628,relative_scaling=1,\n                          color_func = random_color_func,\n                          normalize_plurals=False)\n    wordcloud.generate_from_frequencies(words)\n    ax1.imshow(wordcloud, interpolation=\"bilinear\")\n    ax1.axis('off')\n    plt.title('cluster n{}'.format(increment-1))\n#----------------------------------\nfig = plt.figure(1, figsize=(14,14))\ncolor = [0, 160, 130, 95, 280, 40, 330, 110, 25]\nfor i in range(n_clusters):\n    list_cluster_occurences = occurence[i]\n\n    tone = color[i] # define the color of the words\n    liste = []\n    for key, value in list_cluster_occurences.items():\n        liste.append([key, value])\n    liste.sort(key = lambda x:x[1], reverse = True)\n    make_wordcloud(liste, i+1)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"c / Principal Component Analysis\n\nIn order to ensure that these clusters are truly distinct, I look at their composition. Given the large number of variables of the initial matrix, I first perform a PCA:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA()\npca.fit(matrix)\npca_samples = pca.transform(matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"and then check for the amount of variance explained by each component:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14, 5))\nsns.set(font_scale=1)\nplt.step(range(matrix.shape[1]), pca.explained_variance_ratio_.cumsum(), where='mid',\n         label='cumulative explained variance')\nsns.barplot(np.arange(1,matrix.shape[1]+1), pca.explained_variance_ratio_, alpha=0.5, color = 'g',\n            label='individual explained variance')\nplt.xlim(0, 100)\n\nax.set_xticklabels([s if int(s.get_text())%2 == 0 else '' for s in ax.get_xticklabels()])\n\nplt.ylabel('Explained variance', fontsize = 14)\nplt.xlabel('Principal components', fontsize = 14)\nplt.legend(loc='upper left', fontsize = 13);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the number of components required to explain the data is extremely important: we need more than 100 components to explain 90% of the variance of the data. In practice, I decide to keep only a limited number of components since this decomposition is only performed to visualize the data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=50)\nmatrix_9D = pca.fit_transform(matrix)\nmat = pd.DataFrame(matrix_9D)\nmat['cluster'] = pd.Series(clusters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.patches as mpatches\n\nsns.set_style(\"white\")\nsns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2.5})\n\nLABEL_COLOR_MAP = {0:'r', 1:'gold', 2:'b', 3:'k', 4:'c', 5:'g'}\nlabel_color = [LABEL_COLOR_MAP[l] for l in mat['cluster']]\n\nfig = plt.figure(figsize = (15,8))\nincrement = 0\nfor ix in range(4):\n    for iy in range(ix+1, 4):    \n        increment += 1\n        ax = fig.add_subplot(2,3,increment)\n        ax.scatter(mat[ix], mat[iy], c= label_color, alpha=0.4) \n        plt.ylabel('PCA {}'.format(iy+1), fontsize = 12)\n        plt.xlabel('PCA {}'.format(ix+1), fontsize = 12)\n        ax.yaxis.grid(color='lightgray', linestyle=':')\n        ax.xaxis.grid(color='lightgray', linestyle=':')\n        ax.spines['right'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n        \n        if increment == 9: break\n    if increment == 9: break\n        \n#------------------------------\n# I set the legend: abreviation -> airline name\ncomp_handler = []\nfor i in range(5):\n    comp_handler.append(mpatches.Patch(color = LABEL_COLOR_MAP[i], label = i))\n\nplt.legend(handles=comp_handler, bbox_to_anchor=(1.1, 0.97), \n           title='Cluster', facecolor = 'lightgrey',\n           shadow = True, frameon = True, framealpha = 1,\n           fontsize = 13, bbox_transform = plt.gcf().transFigure)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the previous section, the different products were grouped in five clusters. In order to prepare the rest of the analysis, a first step consists in introducing this information into the dataframe. To do this, I create the categorical variable categ_product where I indicate the cluster of each product :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corresp = dict()\nfor key, val in zip (liste_produits, clusters):\n    corresp[key] = val \n#-------------------------------\ndf_cleaned['categ_product'] = df_cleaned.loc[:, 'Description'].map(corresp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In a second step, I decide to create the categ_N variables (with  N∈[0:4] ) that contains the amount spent in each product category:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(5):\n    col = 'categ_{}'.format(i)        \n    df_temp = df_cleaned[df_cleaned['categ_product'] == i]\n    price_temp = df_temp['UnitPrice'] * (df_temp['Quantity'] - df_temp['QuantityCanceled'])\n    price_temp = price_temp.apply(lambda x:x if x > 0 else 0)\n    df_cleaned.loc[:, col] = price_temp\n    df_cleaned[col].fillna(0, inplace = True)\n#-------------------------------------\ndf_cleaned[['InvoiceNo', 'Description', 'categ_product', 'categ_0', 'categ_1', 'categ_2', 'categ_3','categ_4']][:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Up to now, the information related to a single order was split over several lines of the dataframe (one line per product). I decide to collect the information related to a particular order and put in in a single entry. I therefore create a new dataframe that contains, for each order, the amount of the basket, as well as the way it is distributed over the 5 categories of products:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#---------------------------\n# sum of purchases / user & order\ntemp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['TotalPrice'].sum()\nbasket_price = temp.rename(columns = {'TotalPrice':'Basket Price'})\n#---------------------------------\n# percentage of order price / product category\nfor i in range(5):\n    col = 'categ_{}'.format(i) \n    temp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)[col].sum()\n    basket_price.loc[:, col] = temp \n#-----------------------\n# date of the order\ndf_cleaned['InvoiceDate_int'] = df_cleaned['InvoiceDate'].astype('int64')\ntemp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate_int'].mean()\ndf_cleaned.drop('InvoiceDate_int', axis = 1, inplace = True)\nbasket_price.loc[:, 'InvoiceDate'] = pd.to_datetime(temp['InvoiceDate_int'])\n#______________________________________\n# selection of significant entries:\nbasket_price = basket_price[basket_price['Basket Price'] > 0]\nbasket_price.sort_values('CustomerID', ascending = True)[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataframe basket_price contains information for a period of 12 months. Later, one of the objectives will be to develop a model capable of characterizing and anticipating the habits of the customers visiting the site and this, from their first visit. In order to be able to test the model in a realistic way, I split the data set by retaining the first 10 months to develop the model and the following two months to test it:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(basket_price['InvoiceDate'].min(), '->',  basket_price['InvoiceDate'].max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_entrainement = basket_price[basket_price['InvoiceDate'] < pd.to_datetime(datetime.date(2011,10,1))]\nset_test         = basket_price[basket_price['InvoiceDate'] >= pd.to_datetime(datetime.date(2011,10,1))]\nbasket_price = set_entrainement.copy(deep = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.1.3 Consumer Order Combinations\n\nIn a second step, I group together the different entries that correspond to the same user. I thus determine the number of purchases made by the user, as well as the minimum, maximum, average amounts and the total amount spent during all the visits:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of visits and stats on the amount of the cart / users\ntransactions_per_user=basket_price.groupby(by=['CustomerID'])['Basket Price'].agg(['count','min','max','mean','sum'])\nfor i in range(5):\n    col = 'categ_{}'.format(i)\n    transactions_per_user.loc[:,col] = basket_price.groupby(by=['CustomerID'])[col].sum() /\\\n                                            transactions_per_user['sum']*100\n\ntransactions_per_user.reset_index(drop = False, inplace = True)\nbasket_price.groupby(by=['CustomerID'])['categ_0'].sum()\ntransactions_per_user.sort_values('CustomerID', ascending = True)[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, I define two additional variables that give the number of days elapsed since the first purchase ( FirstPurchase ) and the number of days since the last purchase ( LastPurchase ):","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"last_date = basket_price['InvoiceDate'].max().date()\n\nfirst_registration = pd.DataFrame(basket_price.groupby(by=['CustomerID'])['InvoiceDate'].min())\nlast_purchase      = pd.DataFrame(basket_price.groupby(by=['CustomerID'])['InvoiceDate'].max())\n\ntest  = first_registration.applymap(lambda x:(last_date - x.date()).days)\ntest2 = last_purchase.applymap(lambda x:(last_date - x.date()).days)\n\ntransactions_per_user.loc[:, 'LastPurchase'] = test2.reset_index(drop = False)['InvoiceDate']\ntransactions_per_user.loc[:, 'FirstPurchase'] = test.reset_index(drop = False)['InvoiceDate']\n\ntransactions_per_user[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A customer category of particular interest is that of customers who make only one purchase. One of the objectives may be, for example, to target these customers in order to retain them. In part, I find that this type of customer represents 1/3 of the customers listed:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n1 = transactions_per_user[transactions_per_user['count'] == 1].shape[0]\nn2 = transactions_per_user.shape[0]\nprint(\"Number of customers with a single purchase: {:<2}/{:<5} ({:<2.2f}%)\".format(n1,n2,n1/n2*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataframe transactions_per_user contains a summary of all the commands that were made. Each entry in this dataframe corresponds to a particular client. I use this information to characterize the different types of customers and only keep a subset of variables:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"list_cols = ['count','min','max','mean','categ_0','categ_1','categ_2','categ_3','categ_4']\n#---------------------------------------\nselected_customers = transactions_per_user.copy(deep = True)\nmatrix = selected_customers[list_cols].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In practice, the different variables I selected have quite different ranges of variation and before continuing the analysis, I create a matrix where these data are standardized:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(matrix)\nprint('variables mean values: \\n' + 90*'-' + '\\n' , scaler.mean_)\nscaled_matrix = scaler.transform(matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the following, I will create clusters of customers. In practice, before creating these clusters, it is interesting to define a base of smaller dimension allowing to describe the scaled_matrix matrix. In this case, I will use this base in order to create a representation of the different clusters and thus verify the quality of the separation of the different groups. I therefore perform a PCA beforehand:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA()\npca.fit(scaled_matrix)\npca_samples = pca.transform(scaled_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"and I represent the amount of variance explained by each of the components:\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14, 5))\nsns.set(font_scale=1)\nplt.step(range(matrix.shape[1]), pca.explained_variance_ratio_.cumsum(), where='mid',\n         label='cumulative explained variance')\nsns.barplot(np.arange(1,matrix.shape[1]+1), pca.explained_variance_ratio_, alpha=0.5, color = 'g',\n            label='individual explained variance')\nplt.xlim(0, 10)\n\nax.set_xticklabels([s if int(s.get_text())%2 == 0 else '' for s in ax.get_xticklabels()])\n\nplt.ylabel('Explained variance', fontsize = 14)\nplt.xlabel('Principal components', fontsize = 14)\nplt.legend(loc='best', fontsize = 13);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point, I define clusters of clients from the standardized matrix that was defined earlier and using the k-means algorithm fromscikit-learn. I choose the number of clusters based on the silhouette score and I find that the best score is obtained with 11 clusters:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_clusters = 11\nkmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=100)\nkmeans.fit(scaled_matrix)\nclusters_clients = kmeans.predict(scaled_matrix)\nsilhouette_avg = silhouette_score(scaled_matrix, clusters_clients)\nprint('score of silhouette: {:<.3f}'.format(silhouette_avg))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(pd.Series(clusters_clients).value_counts(), columns = ['nb. de clients']).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=6)\nmatrix_3D = pca.fit_transform(scaled_matrix)\nmat = pd.DataFrame(matrix_3D)\nmat['cluster'] = pd.Series(clusters_clients)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"a / Report via the PCA\n\nThere is a certain disparity in the sizes of different groups that have been created. Hence I will now try to understand the content of these clusters in order to validate (or not) this particular separation. At first, I use the result of the PCA:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.patches as mpatches\n\nsns.set_style(\"white\")\nsns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2.5})\n\nLABEL_COLOR_MAP = {0:'r', 1:'tan', 2:'b', 3:'k', 4:'c', 5:'g', 6:'deeppink', 7:'skyblue', 8:'darkcyan', 9:'orange',\n                   10:'yellow', 11:'tomato', 12:'seagreen'}\nlabel_color = [LABEL_COLOR_MAP[l] for l in mat['cluster']]\n\nfig = plt.figure(figsize = (12,10))\nincrement = 0\nfor ix in range(6):\n    for iy in range(ix+1, 6):   \n        increment += 1\n        ax = fig.add_subplot(4,3,increment)\n        ax.scatter(mat[ix], mat[iy], c= label_color, alpha=0.5) \n        plt.ylabel('PCA {}'.format(iy+1), fontsize = 12)\n        plt.xlabel('PCA {}'.format(ix+1), fontsize = 12)\n        ax.yaxis.grid(color='lightgray', linestyle=':')\n        ax.xaxis.grid(color='lightgray', linestyle=':')\n        ax.spines['right'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n        \n        if increment == 12: break\n    if increment == 12: break\n        \n#------------------------------------------\n# I set the legend: abreviation -> airline name\ncomp_handler = []\nfor i in range(n_clusters):\n    comp_handler.append(mpatches.Patch(color = LABEL_COLOR_MAP[i], label = i))\n\nplt.legend(handles=comp_handler, bbox_to_anchor=(1.1, 0.9), \n           title='Cluster', facecolor = 'lightgrey',\n           shadow = True, frameon = True, framealpha = 1,\n           fontsize = 13, bbox_transform = plt.gcf().transFigure)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this representation, it can be seen, for example, that the first principal component allow to separate the tiniest clusters from the rest. More generally, we see that there is always a representation in which two clusters will appear to be distinct.\n\nb/ Score de silhouette intra-cluster\n\nAs with product categories, another way to look at the quality of the separation is to look at silouhette scores within different clusters:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_silhouette_values = silhouette_samples(scaled_matrix, clusters_clients)\n#----------------------------\n# define individual silouhette scores\nsample_silhouette_values = silhouette_samples(scaled_matrix, clusters_clients)\n#-------------------------\n# and do the graph\ngraph_component_silhouette(n_clusters, [-0.15, 0.55], len(scaled_matrix), sample_silhouette_values, clusters_clients)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"c/ Customers morphotype\n\nAt this stage, I have verified that the different clusters are indeed disjoint (at least, in a global way). It remains to understand the habits of the customers in each cluster. To do so, I start by adding to the selected_customers dataframe a variable that defines the cluster to which each client belongs:\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_customers.loc[:, 'cluster'] = clusters_clients","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_df = pd.DataFrame()\nfor i in range(n_clusters):\n    test = pd.DataFrame(selected_customers[selected_customers['cluster'] == i].mean())\n    test = test.T.set_index('cluster', drop = True)\n    test['size'] = selected_customers[selected_customers['cluster'] == i].shape[0]\n    merged_df = pd.concat([merged_df, test])\n#---------------------------------------\nmerged_df.drop('CustomerID', axis = 1, inplace = True)\nprint('number of customers:', merged_df['size'].sum())\n\nmerged_df = merged_df.sort_values('sum')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, I re-organize the content of the dataframe by ordering the different clusters: first, in relation to the amount wpsent in each product category and then, according to the total amount spent:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"liste_index = []\nfor i in range(5):\n    column = 'categ_{}'.format(i)\n    liste_index.append(merged_df[merged_df[column] > 45].index.values[0])\n#------------------------------\nliste_index_reordered = liste_index\nliste_index_reordered += [ s for s in merged_df.index if s not in liste_index]\n#------------------------------------\nmerged_df = merged_df.reindex(index = liste_index_reordered)\nmerged_df = merged_df.reset_index(drop = False)\ndisplay(merged_df[['cluster', 'count', 'min', 'max', 'mean', 'sum', 'categ_0',\n                   'categ_1', 'categ_2', 'categ_3', 'categ_4', 'size']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"d / Customers morphology\n\nFinally, I created a representation of the different morphotypes. To do this, I define a class to create \"Radar Charts\"?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def _scale_data(data, ranges):\n    (x1, x2) = ranges[0]\n    d = data[0]\n    return [(d - y1) / (y2 - y1) * (x2 - x1) + x1 for d, (y1, y2) in zip(data, ranges)]\n\nclass RadarChart():\n    def __init__(self, fig, location, sizes, variables, ranges, n_ordinate_levels = 6):\n\n        angles = np.arange(0, 360, 360./len(variables))\n\n        ix, iy = location[:] ; size_x, size_y = sizes[:]\n        \n        axes = [fig.add_axes([ix, iy, size_x, size_y], polar = True, \n        label = \"axes{}\".format(i)) for i in range(len(variables))]\n\n        _, text = axes[0].set_thetagrids(angles, labels = variables)\n        \n        for txt, angle in zip(text, angles):\n            if angle > -1 and angle < 181:\n                txt.set_rotation(angle - 90)\n            else:\n                txt.set_rotation(angle - 270)\n        \n        for ax in axes[1:]:\n            ax.patch.set_visible(False)\n            ax.xaxis.set_visible(False)\n            ax.grid(\"off\")\n        \n        for i, ax in enumerate(axes):\n            grid = np.linspace(*ranges[i],num = n_ordinate_levels)\n            grid_label = [\"\"]+[\"{:.0f}\".format(x) for x in grid[1:-1]]\n            ax.set_rgrids(grid, labels = grid_label, angle = angles[i])\n            ax.set_ylim(*ranges[i])\n        \n        self.angle = np.deg2rad(np.r_[angles, angles[0]])\n        self.ranges = ranges\n        self.ax = axes[0]\n                \n    def plot(self, data, *args, **kw):\n        sdata = _scale_data(data, self.ranges)\n        self.ax.plot(self.angle, np.r_[sdata, sdata[0]], *args, **kw)\n\n    def fill(self, data, *args, **kw):\n        sdata = _scale_data(data, self.ranges)\n        self.ax.fill(self.angle, np.r_[sdata, sdata[0]], *args, **kw)\n\n    def legend(self, *args, **kw):\n        self.ax.legend(*args, **kw)\n        \n    def title(self, title, *args, **kw):\n        self.ax.text(0.9, 1, title, transform = self.ax.transAxes, *args, **kw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,12))\n\nattributes = ['count', 'mean', 'sum', 'categ_0', 'categ_1', 'categ_2', 'categ_3', 'categ_4']\nranges = [[0.01, 10], [0.01, 1500], [0.01, 10000], [0.01, 75], [0.01, 75], [0.01, 75], [0.01, 75], [0.01, 75]]\nindex  = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n\nn_groups = n_clusters ; i_cols = 3\ni_rows = n_groups//i_cols\nsize_x, size_y = (1/i_cols), (1/i_rows)\n\nfor ind in range(n_clusters):\n    ix = ind%3 ; iy = i_rows - ind//3\n    pos_x = ix*(size_x + 0.05) ; pos_y = iy*(size_y + 0.05)            \n    location = [pos_x, pos_y]  ; sizes = [size_x, size_y] \n    #______________________________________________________\n    data = np.array(merged_df.loc[index[ind], attributes])    \n    radar = RadarChart(fig, location, sizes, attributes, ranges)\n    radar.plot(data, color = 'b', linewidth=2.0)\n    radar.fill(data, alpha = 0.2, color = 'b')\n    radar.title(title = 'cluster n {}'.format(index[ind]), color = 'r')\n    ind += 1 ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"t can be seen, for example, that the first 5 clusters correspond to a strong preponderance of purchases in a particular category of products. Other clusters will differ from basket averages ( mean ), the total sum spent by the clients ( sum ) or the total number of visits made ( count ).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Class_Fit(object):\n    def __init__(self, clf, params=None):\n        if params:            \n            self.clf = clf(**params)\n        else:\n            self.clf = clf()\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def grid_search(self, parameters, Kfold):\n        self.grid = GridSearchCV(estimator = self.clf, param_grid = parameters, cv = Kfold)\n        \n    def grid_fit(self, X, Y):\n        self.grid.fit(X, Y)\n        \n    def grid_predict(self, X, Y):\n        self.predictions = self.grid.predict(X)\n        print(\"Precision: {:.2f} % \".format(100*metrics.accuracy_score(Y, self.predictions)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Classifying the Customers based on some parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['mean', 'categ_0', 'categ_1', 'categ_2', 'categ_3', 'categ_4' ]\nX = selected_customers[columns]\nY = selected_customers['cluster']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train-test Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, train_size = 0.8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scalar Vector Machine","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = Class_Fit(clf = svm.LinearSVC)\nsvc.grid_search(parameters = [{'C':np.logspace(-2,2,10)}], Kfold = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc.grid_fit(X = X_train, Y = Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc.grid_predict(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Confusion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    #------------------------------\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n    #-----------------------------\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    #----------------------------------\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names = [i for i in range(11)]\ncnf_matrix = confusion_matrix(Y_test, svc.predictions) \nnp.set_printoptions(precision=2)\nplt.figure(figsize = (8,8))\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize = False, title='Confusion matrix')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 10)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = plot_learning_curve(svc.grid.best_estimator_,\n                        \"SVC learning curves\", X_train, Y_train, ylim = [1.01, 0.6],\n                        cv = 5,  train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5,\n                                                0.6, 0.7, 0.8, 0.9, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = Class_Fit(clf = linear_model.LogisticRegression)\nlr.grid_search(parameters = [{'C':np.logspace(-2,2,20)}], Kfold = 5)\nlr.grid_fit(X = X_train, Y = Y_train)\nlr.grid_predict(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"g = plot_learning_curve(lr.grid.best_estimator_, \"Logistic Regression learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.7], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# kNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = Class_Fit(clf = neighbors.KNeighborsClassifier)\nknn.grid_search(parameters = [{'n_neighbors': np.arange(1,50,1)}], Kfold = 5)\nknn.grid_fit(X = X_train, Y = Y_train)\nknn.grid_predict(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision tree Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tr = Class_Fit(clf = tree.DecisionTreeClassifier)\ntr.grid_search(parameters = [{'criterion' : ['entropy', 'gini'], 'max_features' :['sqrt', 'log2']}], Kfold = 5)\ntr.grid_fit(X = X_train, Y = Y_train)\ntr.grid_predict(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = plot_learning_curve(tr.grid.best_estimator_, \"Decision tree learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.7], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Random Forest Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = Class_Fit(clf = ensemble.RandomForestClassifier)\nparam_grid = {'criterion' : ['entropy', 'gini'], 'n_estimators' : [20, 40, 60, 80, 100],\n               'max_features' :['sqrt', 'log2']}\nrf.grid_search(parameters = param_grid, Kfold = 5)\nrf.grid_fit(X = X_train, Y = Y_train)\nrf.grid_predict(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = plot_learning_curve(rf.grid.best_estimator_, \"Random Forest learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.7], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ada Boost Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ada = Class_Fit(clf = AdaBoostClassifier)\nparam_grid = {'n_estimators' : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]}\nada.grid_search(parameters = param_grid, Kfold = 5)\nada.grid_fit(X = X_train, Y = Y_train)\nada.grid_predict(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = plot_learning_curve(ada.grid.best_estimator_, \"AdaBoost learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.4], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient Boosting Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gb = Class_Fit(clf = ensemble.GradientBoostingClassifier)\nparam_grid = {'n_estimators' : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]}\ngb.grid_search(parameters = param_grid, Kfold = 5)\ngb.grid_fit(X = X_train, Y = Y_train)\ngb.grid_predict(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = plot_learning_curve(gb.grid.best_estimator_, \"Gradient Boosting learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.7], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. testing the Predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_best  = ensemble.RandomForestClassifier(**rf.grid.best_params_)\ngb_best  = ensemble.GradientBoostingClassifier(**gb.grid.best_params_)\nsvc_best = svm.LinearSVC(**svc.grid.best_params_)\ntr_best  = tree.DecisionTreeClassifier(**tr.grid.best_params_)\nknn_best = neighbors.KNeighborsClassifier(**knn.grid.best_params_)\nlr_best  = linear_model.LogisticRegression(**lr.grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"votingC = ensemble.VotingClassifier(estimators=[('rf', rf_best),('gb', gb_best),\n                                                ('knn', knn_best)], voting='soft')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"votingC = votingC.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = votingC.predict(X_test)\nprint(\"Precision: {:.2f} % \".format(100*metrics.accuracy_score(Y_test, predictions)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"basket_price = set_test.copy(deep = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transactions_per_user=basket_price.groupby(by=['CustomerID'])['Basket Price'].agg(['count','min','max','mean','sum'])\nfor i in range(5):\n    col = 'categ_{}'.format(i)\n    transactions_per_user.loc[:,col] = basket_price.groupby(by=['CustomerID'])[col].sum() /\\\n                                            transactions_per_user['sum']*100\n\ntransactions_per_user.reset_index(drop = False, inplace = True)\nbasket_price.groupby(by=['CustomerID'])['categ_0'].sum()\n\n#-------------------------\n# Correcting time range\ntransactions_per_user['count'] = 5 * transactions_per_user['count']\ntransactions_per_user['sum']   = transactions_per_user['count'] * transactions_per_user['mean']\n\ntransactions_per_user.sort_values('CustomerID', ascending = True)[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_cols = ['count','min','max','mean','categ_0','categ_1','categ_2','categ_3','categ_4']\n#---------------------------------\nmatrix_test = transactions_per_user[list_cols].values\nscaled_test_matrix = scaler.transform(matrix_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = kmeans.predict(scaled_test_matrix)\n\ncolumns = ['mean', 'categ_0', 'categ_1', 'categ_2', 'categ_3', 'categ_4' ]\nX = transactions_per_user[columns]\nclassifiers = [(svc, 'Support Vector Machine'),\n                (lr, 'Logostic Regression'),\n                (knn, 'k-Nearest Neighbors'),\n                (tr, 'Decision Tree'),\n                (rf, 'Random Forest'),\n                (gb, 'Gradient Boosting')]\n#--------------------------\nfor clf, label in classifiers:\n    print(30*'-', '\\n{}'.format(label))\n    clf.grid_predict(X, Y)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}