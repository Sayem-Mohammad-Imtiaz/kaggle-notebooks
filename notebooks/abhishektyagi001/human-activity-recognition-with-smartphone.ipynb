{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-16T02:27:45.725875Z","iopub.execute_input":"2021-09-16T02:27:45.726218Z","iopub.status.idle":"2021-09-16T02:27:46.703334Z","shell.execute_reply.started":"2021-09-16T02:27:45.726188Z","shell.execute_reply":"2021-09-16T02:27:46.702295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\nimport os\n#Data Path has to be set as per the file location in your system\n#data_path = ['..', 'data']\ndata_path = ['../input/human-activity-recognition-with-smartphones/test.csv']","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-09-16T02:27:49.385891Z","iopub.execute_input":"2021-09-16T02:27:49.386265Z","iopub.status.idle":"2021-09-16T02:27:49.391234Z","shell.execute_reply.started":"2021-09-16T02:27:49.386235Z","shell.execute_reply":"2021-09-16T02:27:49.390184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n#The filepath is dependent on the data_path set in the previous cell \nfilepath = os.sep.join(data_path)\ndata = pd.read_csv(filepath, sep=',')","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:27:50.872667Z","iopub.execute_input":"2021-09-16T02:27:50.873088Z","iopub.status.idle":"2021-09-16T02:27:51.645075Z","shell.execute_reply.started":"2021-09-16T02:27:50.87305Z","shell.execute_reply":"2021-09-16T02:27:51.644175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.dtypes.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:27:52.257017Z","iopub.execute_input":"2021-09-16T02:27:52.257398Z","iopub.status.idle":"2021-09-16T02:27:52.271608Z","shell.execute_reply.started":"2021-09-16T02:27:52.257361Z","shell.execute_reply":"2021-09-16T02:27:52.270615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.dtypes.tail()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:27:53.208884Z","iopub.execute_input":"2021-09-16T02:27:53.209459Z","iopub.status.idle":"2021-09-16T02:27:53.219915Z","shell.execute_reply.started":"2021-09-16T02:27:53.209402Z","shell.execute_reply":"2021-09-16T02:27:53.21868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The data are all scaled from -1 (minimum) to 1.0 (maximum).\ndata.iloc[:, :-1].min().value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:27:54.019164Z","iopub.execute_input":"2021-09-16T02:27:54.019548Z","iopub.status.idle":"2021-09-16T02:27:54.091381Z","shell.execute_reply.started":"2021-09-16T02:27:54.019504Z","shell.execute_reply":"2021-09-16T02:27:54.090311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.iloc[:, :-1].max().value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:27:54.59883Z","iopub.execute_input":"2021-09-16T02:27:54.599399Z","iopub.status.idle":"2021-09-16T02:27:54.65623Z","shell.execute_reply.started":"2021-09-16T02:27:54.599346Z","shell.execute_reply":"2021-09-16T02:27:54.655012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Examine the breakdown of activities--they are relatively balanced.\ndata.Activity.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:27:55.146109Z","iopub.execute_input":"2021-09-16T02:27:55.1465Z","iopub.status.idle":"2021-09-16T02:27:55.155476Z","shell.execute_reply.started":"2021-09-16T02:27:55.146468Z","shell.execute_reply":"2021-09-16T02:27:55.154548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndata['Activity'] = le.fit_transform(data.Activity)\ndata['Activity'].sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:27:55.810727Z","iopub.execute_input":"2021-09-16T02:27:55.81127Z","iopub.status.idle":"2021-09-16T02:27:55.822769Z","shell.execute_reply.started":"2021-09-16T02:27:55.811237Z","shell.execute_reply":"2021-09-16T02:27:55.821643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Question 2\nCalculate the correlations between the dependent variables.\nCreate a histogram of the correlation values\nIdentify those that are most correlated (either positively or negatively).\"\"\"\n# Calculate the correlation values\nfeature_cols = data.columns[:-1]\ncorr_values = data[feature_cols].corr()\n\n# Simplify by emptying all the data below the diagonal\ntril_index = np.tril_indices_from(corr_values)\n\n# Make the unused values NaNs\nfor coord in zip(*tril_index):\n    corr_values.iloc[coord[0], coord[1]] = np.NaN\n    \n# Stack the data and convert to a data frame\ncorr_values = (corr_values.stack().to_frame().reset_index().rename(columns={'level_0':'feature1','level_1':'feature2',0:'correlation'}))\n\n# Get the absolute values for sorting\ncorr_values['abs_correlation'] = corr_values.correlation.abs()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:27:57.008537Z","iopub.execute_input":"2021-09-16T02:27:57.009096Z","iopub.status.idle":"2021-09-16T02:28:11.328089Z","shell.execute_reply.started":"2021-09-16T02:27:57.009062Z","shell.execute_reply":"2021-09-16T02:28:11.327025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:28:11.329911Z","iopub.execute_input":"2021-09-16T02:28:11.330199Z","iopub.status.idle":"2021-09-16T02:28:11.42777Z","shell.execute_reply.started":"2021-09-16T02:28:11.330171Z","shell.execute_reply":"2021-09-16T02:28:11.426932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_context('talk')\nsns.set_style('white')\nsns.set_palette('dark')\n\nax = corr_values.abs_correlation.hist(bins=50)\n\nax.set(xlabel='Absolute Correlation', ylabel='Frequency');","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:28:11.428776Z","iopub.execute_input":"2021-09-16T02:28:11.429169Z","iopub.status.idle":"2021-09-16T02:28:11.845285Z","shell.execute_reply.started":"2021-09-16T02:28:11.429142Z","shell.execute_reply":"2021-09-16T02:28:11.844556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The most highly correlated values\ncorr_values.sort_values('correlation', ascending=False).query('abs_correlation>0.8')","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:28:11.846295Z","iopub.execute_input":"2021-09-16T02:28:11.846714Z","iopub.status.idle":"2021-09-16T02:28:11.912478Z","shell.execute_reply.started":"2021-09-16T02:28:11.846682Z","shell.execute_reply":"2021-09-16T02:28:11.911752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Split the data into train and test data sets. This can be done using any method, but consider using Scikit-learn's StratifiedShuffleSplit to maintain the same ratio of predictor classes.\nRegardless of methods used to split the data, compare the ratio of classes in both the train and test splits.\"\"\"\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# Get the split indexes\nstrat_shuf_split = StratifiedShuffleSplit(n_splits=1,test_size=0.3, random_state=42)\n\ntrain_idx, test_idx = next(strat_shuf_split.split(data[feature_cols], data.Activity))\n\n# Create the dataframes\nX_train = data.loc[train_idx, feature_cols]\ny_train = data.loc[train_idx, 'Activity']\n\nX_test  = data.loc[test_idx, feature_cols]\ny_test  = data.loc[test_idx, 'Activity']","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:28:32.194948Z","iopub.execute_input":"2021-09-16T02:28:32.195569Z","iopub.status.idle":"2021-09-16T02:28:32.234623Z","shell.execute_reply.started":"2021-09-16T02:28:32.195504Z","shell.execute_reply":"2021-09-16T02:28:32.233695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Fit a logistic regression model without any regularization using all of the features. Be sure to read the documentation about fitting a multi-class model so you understand the coefficient output. Store the model.\nUsing cross validation to determine the hyperparameters, fit models using L1, and L2 regularization. Store each of these models as well. Note the limitations on multi-class models, solvers, and regularizations. The regularized models, in particular the L1 model, will probably take a while to fit.\"\"\"\nfrom sklearn.linear_model import LogisticRegression\n\n# Standard logistic regression\nlr = LogisticRegression().fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:28:37.344002Z","iopub.execute_input":"2021-09-16T02:28:37.344356Z","iopub.status.idle":"2021-09-16T02:28:37.7288Z","shell.execute_reply.started":"2021-09-16T02:28:37.344323Z","shell.execute_reply":"2021-09-16T02:28:37.726461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegressionCV\n\n# L1 regularized logistic regression\nlr_l1 = LogisticRegressionCV(Cs=10, cv=4, penalty='l1', solver='liblinear').fit(X_train, y_train)\n#Try with different solvers like ‘newton-cg’, ‘lbfgs’, ‘sag’, ‘saga’ and give your observations","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:28:46.369351Z","iopub.execute_input":"2021-09-16T02:28:46.369761Z","iopub.status.idle":"2021-09-16T02:29:14.918636Z","shell.execute_reply.started":"2021-09-16T02:28:46.369725Z","shell.execute_reply":"2021-09-16T02:29:14.917644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# L2 regularized logistic regression\nlr_l2 = LogisticRegressionCV(Cs=10, cv=4, penalty='l2').fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:29:21.419976Z","iopub.execute_input":"2021-09-16T02:29:21.420324Z","iopub.status.idle":"2021-09-16T02:29:29.788104Z","shell.execute_reply.started":"2021-09-16T02:29:21.420294Z","shell.execute_reply":"2021-09-16T02:29:29.787013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install pandas==0.25.3\n# Combine all the coefficients into a dataframe\nimport pandas as pd\ncoefficients = list()\n\ncoeff_labels = ['lr', 'l1', 'l2']\ncoeff_models = [lr, lr_l1, lr_l2]\n\nfor lab,mod in zip(coeff_labels, coeff_models):\n    coeffs = mod.coef_\n    coeff_label = pd.MultiIndex(levels=[[lab], [0,1,2,3,4,5]], \n                                 codes=[[0,0,0,0,0,0], [0,1,2,3,4,5]])\n    coefficients.append(pd.DataFrame(coeffs.T, columns=coeff_label))\n\ncoefficients = pd.concat(coefficients, axis=1)\n\ncoefficients.sample(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:37:11.964589Z","iopub.execute_input":"2021-09-16T02:37:11.964951Z","iopub.status.idle":"2021-09-16T02:37:12.009205Z","shell.execute_reply.started":"2021-09-16T02:37:11.964921Z","shell.execute_reply":"2021-09-16T02:37:12.008267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axList = plt.subplots(nrows=3, ncols=2)\naxList = axList.flatten()\nfig.set_size_inches(10,10)\n\n\nfor ax in enumerate(axList):\n    loc = ax[0]\n    ax = ax[1]\n    \n    data = coefficients.xs(loc, level=1, axis=1)\n    data.plot(marker='o', ls='', ms=2.0, ax=ax, legend=False)\n    \n    if ax is axList[0]:\n        ax.legend(loc=4)\n        \n    ax.set(title='Coefficient Set '+str(loc))\n\nplt.tight_layout()\n","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:37:14.673304Z","iopub.execute_input":"2021-09-16T02:37:14.673759Z","iopub.status.idle":"2021-09-16T02:37:16.374532Z","shell.execute_reply.started":"2021-09-16T02:37:14.673725Z","shell.execute_reply":"2021-09-16T02:37:16.373615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Predict and store the class for each model.\nAlso store the probability for the predicted class for each model.\"\"\"\n# Predict the class and the probability for each\n\ny_pred = list()\ny_prob = list()\n\ncoeff_labels = ['lr', 'l1', 'l2']\ncoeff_models = [lr, lr_l1, lr_l2]\n\nfor lab,mod in zip(coeff_labels, coeff_models):\n    y_pred.append(pd.Series(mod.predict(X_test), name=lab))\n    y_prob.append(pd.Series(mod.predict_proba(X_test).max(axis=1), name=lab))\n    \ny_pred = pd.concat(y_pred, axis=1)\ny_prob = pd.concat(y_prob, axis=1)\n\ny_pred.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:37:18.890367Z","iopub.execute_input":"2021-09-16T02:37:18.890696Z","iopub.status.idle":"2021-09-16T02:37:18.959163Z","shell.execute_reply.started":"2021-09-16T02:37:18.89067Z","shell.execute_reply":"2021-09-16T02:37:18.958066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_prob.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:37:20.330261Z","iopub.execute_input":"2021-09-16T02:37:20.330647Z","iopub.status.idle":"2021-09-16T02:37:20.342187Z","shell.execute_reply.started":"2021-09-16T02:37:20.33061Z","shell.execute_reply":"2021-09-16T02:37:20.341475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"For each model, calculate the following error metrics:\n\naccuracy\nprecision\nrecall\nfscore\nconfusion matrix\nDecide how to combine the multi-class metrics into a single value for each model.\"\"\"\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\nfrom sklearn.preprocessing import label_binarize\n\nmetrics = list()\ncm = dict()\n\nfor lab in coeff_labels:\n\n    # Preciision, recall, f-score from the multi-class support function\n    precision, recall, fscore, _ = score(y_test, y_pred[lab], average='weighted')\n    \n    # The usual way to calculate accuracy\n    accuracy = accuracy_score(y_test, y_pred[lab])\n    \n    # ROC-AUC scores can be calculated by binarizing the data\n    auc = roc_auc_score(label_binarize(y_test, classes=[0,1,2,3,4,5]),\n              label_binarize(y_pred[lab], classes=[0,1,2,3,4,5]), \n              average='weighted')\n    \n    # Last, the confusion matrix\n    cm[lab] = confusion_matrix(y_test, y_pred[lab])\n    \n    metrics.append(pd.Series({'precision':precision, 'recall':recall, \n                              'fscore':fscore, 'accuracy':accuracy,\n                              'auc':auc}, \n                             name=lab))\n\nmetrics = pd.concat(metrics, axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:37:21.273181Z","iopub.execute_input":"2021-09-16T02:37:21.273713Z","iopub.status.idle":"2021-09-16T02:37:21.311491Z","shell.execute_reply.started":"2021-09-16T02:37:21.273675Z","shell.execute_reply":"2021-09-16T02:37:21.310672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Run the metrics\nmetrics","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:37:26.449761Z","iopub.execute_input":"2021-09-16T02:37:26.450094Z","iopub.status.idle":"2021-09-16T02:37:26.460264Z","shell.execute_reply.started":"2021-09-16T02:37:26.450068Z","shell.execute_reply":"2021-09-16T02:37:26.459149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfig, axList = plt.subplots(nrows=2, ncols=2)\naxList = axList.flatten()\nfig.set_size_inches(12, 10)\n\naxList[-1].axis('off')\n\nfor ax,lab in zip(axList[:-1], coeff_labels):\n    sns.heatmap(cm[lab], ax=ax, annot=True, fmt='d');\n    ax.set(title=lab);\n    \nplt.tight_layout()\n#Display or plot the confusion matrix for each model.","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:37:28.144613Z","iopub.execute_input":"2021-09-16T02:37:28.145188Z","iopub.status.idle":"2021-09-16T02:37:29.918986Z","shell.execute_reply.started":"2021-09-16T02:37:28.145143Z","shell.execute_reply":"2021-09-16T02:37:29.917819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import VarianceThreshold\n\n#threshold with .7\n\nsel = VarianceThreshold(threshold=(.7 * (1 - .7)))\n\ndata2 = pd.concat([X_train,X_test])\ndata_new = pd.DataFrame(sel.fit_transform(data2))\n\n\ndata_y = pd.concat([y_train,y_test])\n\nfrom sklearn.model_selection import train_test_split\n\nX_new,X_test_new = train_test_split(data_new)\nY_new,Y_test_new = train_test_split(data_y)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T02:37:29.920952Z","iopub.execute_input":"2021-09-16T02:37:29.921541Z","iopub.status.idle":"2021-09-16T02:37:30.032446Z","shell.execute_reply.started":"2021-09-16T02:37:29.92148Z","shell.execute_reply":"2021-09-16T02:37:30.031419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}