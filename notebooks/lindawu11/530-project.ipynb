{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Understanding Customer Using Reviews Data\n\nThe purpose of this project is to try out Python concepts and techniques I've learned from Computational Concept in HCDE to understand the customers in a women's clothing ecommerce webiste. To understand the customers, I will calculate and visualize descriptive statistics to gain a basic understanding of the dataset, conduct sentiment analysis on review text to uncover key words in positive and negative sentiments, and conduct topic modeling to find customer segments. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Dataset\n\nThe dataset used in this project is [the women's ecommerce clothing reviews](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews) dataset found on Kaggle. This data set is real reviews from an anonymized women’s clothing e-commerce platform. The data is a collection of 22641 Rows and 10 column variables. Each row consists of a written review as well as an additional feature of the customer information. All 10 variables are clothing ID, age, title, review text, rating, recommended IND, positive feedback count, division name, department name, and class name. \n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%pylab inline\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndata = pd.read_csv('../input/womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv')\ndata = data.drop(['Unnamed: 0'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove na in data and store as a new csv file","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.dropna(how='any') \ndata.to_csv('Womens_Clothing_clean.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use clean data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/womens-clothing-cleancsv/Womens_Clothing_clean.csv')\ndf = df.drop(['Unnamed: 0'], axis=1)\n\nprint('Number of rows of original dataset: %d' % len(data))\nprint('Number of rows after dropping na: %d' % len(df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Descriptive statistics","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Mean age and mean rating: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean reviewer age: ' , round(df['Age'].mean(), 3))\nprint('Mean rating: ', round(df['Rating'].mean(), 3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting distribution of age and rating","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# histogram for age\nage = pd.Series(df['Age'])\n# age.value_counts()\nage_plot = age.plot.hist(bins=20, rwidth=0.9, color='#607c8e', title = 'Age distribution')\nplt.xlabel('Age')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The age distribution plot indicates a slightly left skewed age distribution with median value around 35 - 40. Most of the customers who shop at this website are younger than 60 years old.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# histogram for rating\nrating = pd.Series(df['Rating'])\n# rating.value_counts()\nrating_plot = rating.plot.hist(color='#607c8e')\nplt.title('Rating distribution')\nplt.xlabel('Rating')\nplt.ylabel('Frequency')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The biggest group of people give the highest rating of 5. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_by_age = df.groupby('Age')['Rating'].mean()\n\nrating_by_age_plot = rating_by_age.plot(kind='bar', title='Ratings by Age', color='#4290be')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Rating by Age graph shows that the reviewers who are less than or equal to 76-year-old have similar mean ratings between 4 to about 4.5. Mean ratings of reviewers older than 76 fluctuate more. However, whether those reviewers who identified themselves as above 80 years old were reporting their real age remains questionable. Upon reading the reviews of those in the higher age group, I did not find any evidence of fake age, so I decided to keep data of those age groups. \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_by_department = df.groupby('Department Name')['Rating'].mean()\n\nrating_by_department_plot = rating_by_department.plot(kind='bar', title='Ratings by Department', color='#4290be')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There isn't any big difference among ratings on clothing in different departments. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_by_class = df.groupby('Class Name')['Rating'].mean()\nrating_by_class_plot = rating_by_class.plot(kind='bar', title='Ratings by Class', color='#4290be')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarly, there isn't much difference among ratings on clothing in different class.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Sentiment Analysis on review text","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Labeling reviews as positive or negative\nSince there are way more customers rated items as 5, and in those reviews with ratings of less than 5, there are always some things the customers are not satisfied with, I will label reviews with a rating of 5 as positive and anything below as negative.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_df = df\nsentiment_df['label'] = ['pos' if rating == 5 else 'neg' for rating in df['Rating']] \nsentiment_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### split training and testing set for final model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntarget = [1 if label == 'pos'  else 0 for label in df['label']] # use 1 and 0 to represent labels\nreview_train, review_test, target_train, target_test = train_test_split(sentiment_df['Review Text'].values, target, test_size=0.20, random_state=1)\nprint(review_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vectorization\n\nIn order to feed the review text into machine learning algorithms, we need to convert the text into numbers. \n\nAfter testing out combinations of different text processing techniques such as tf-idf, bi-gram, stemming and others, using CountVectorizer with tri-gram and stop words yields the best accuracy.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nstop_words=['in','of','at','a','the']\ntrigram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 3), stop_words=stop_words) #initialize vectorizer using tri-gram and removing stop words\ntrigram_vectorizer.fit(review_train)\nX = trigram_vectorizer.transform(review_train) # convert training data\nX_test = trigram_vectorizer.transform(review_test) # convert testing data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build classifier\n\nIn comparison to logistic regression, support vector machine (svm) yields higher accuracy and has shorter computation time. So I decided to use svm to fit the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.svm import LinearSVC\n\n# using svm\nX_train, X_val, y_train, y_val = train_test_split(\n    X, target_train, train_size = 0.75, random_state=1) # set random state to retain the same data split each time.  \n\n# Tuning hyperparameter c to adjust regularization and see which one yields the highest accuracy\nfor c in [0.001, 0.005, 0.01, 0.05, 0.1]:\n    \n    svm = LinearSVC(C=c)\n    svm.fit(X_train, y_train)\n    print (\"Accuracy for C=%s: %s\" \n           % (c, accuracy_score(y_val, svm.predict(X_val))))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above results we see that C=0.005 yields the highest accuracy. Therefore, we will deploy our final model using C=0.005","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = LinearSVC(C=0.005)\nfinal_model.fit(X, target_train)\nprint (\"Final Accuracy: %s\" \n       % accuracy_score(target_test, final_model.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let’s look at the 10 most discriminating words for both positive and negative reviews. We’ll do this by looking at the largest and smallest coefficients, respectively.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_to_coef = {\n    word: coef for word, coef in zip(\n        trigram_vectorizer.get_feature_names(), final_model.coef_[0]\n    )\n}\nprint('positive:')\n\nfor best_positive in sorted(\n    feature_to_coef.items(), \n    key=lambda x: x[1], \n    reverse=True)[:10]:\n    print (best_positive)\n\nprint('\\nnegative:')\n    \nfor best_negative in sorted(\n    feature_to_coef.items(), \n    key=lambda x: x[1])[:10]:\n    print (best_negative)\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the output we can see that words that strongly associate with positive reivews are words that usually express positive emotions such as 'perfect' and 'love.' In negative words, besides adjective that usually describe negative emotions, words related with the action of returning items are high on the list. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Clustering review text - Topic modeling using LDA\n\nTopic modeling is used to extract topics from large collections of documents. Latent Dirichlet Allocation (LDA) is one way to do topic modeling. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Prepare data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews = df[['Review Text']]\nreviews['index'] = reviews.index\ndocuments = reviews\nreviews","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing and vectorization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Lemmatizing, stemming, removing stop words\n\n* Stopwords are removed.\n* Lemmatizing — words in third person are changed to first person and verbs in past and future tenses are changed into present.\n* Stemming — words are reduced to their root form.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading libraries for text processing\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nimport nltk\nnltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# functions for stokenizing, stemming, lemmatizing, and removing stop words\nstemmer = SnowballStemmer('english')\ndef lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            result.append(lemmatize_stemming(token))\n    return result\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A sample review being processed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"doc_sample = documents[documents['index'] == 4].values[0][0]\nprint('original document: ')\nwords = []\nfor word in doc_sample.split(' '):\n    words.append(word)\nprint(words)\nprint('\\n\\n tokenized and lemmatized document: ')\nprint(preprocess(doc_sample))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Process all review text","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_docs = documents['Review Text'].map(preprocess)\nprocessed_docs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Compute Bi/Tri-gram\n\nBigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# using gensim library to generate bi-grams and tri-grams\nfrom gensim.models import Phrases\n\n# Add bigrams and trigrams to docs (only ones that appear 10 times or more).\nbigram = Phrases(processed_docs, min_count=10)\ntrigram = Phrases(bigram[processed_docs])\n\nfor idx in range(len(processed_docs)):\n    for token in bigram[processed_docs[idx]]:\n        if '_' in token:\n            # Token is a bigram, add to document.\n            processed_docs[idx].append(token)\n    for token in trigram[processed_docs[idx]]:\n        if '_' in token:\n            # Token is a bigram, add to document.\n            processed_docs[idx].append(token)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Corpus and Dictionary \n\nThe two main inputs to the LDA topic model are the dictionary and the corpus.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dictionary = gensim.corpora.Dictionary(processed_docs)\nprint('Number of unique words in initital documents:', len(dictionary))\ncount = 0\nfor k, v in dictionary.iteritems():\n    print(k, v)\n    count += 1\n    if count > 10:\n        break\n\n# Filter out words that occur less than 10 documents.\ndictionary.filter_extremes(no_below=10)\nprint('Number of unique words after removing rare and common words:', len(dictionary))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preview sample processed documents:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bow_doc_4 = bow_corpus[4]\nfor i in range(len(bow_doc_4)):\n    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4[i][0], \n                                               dictionary[bow_doc_4[i][0]], \nbow_doc_4[i][1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of unique tokens: %d' % len(dictionary))\nprint('Number of documents: %d' % len(bow_corpus))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train Baseline LDA model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import LdaModel\n\nnum_topics = 9\nlda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=4, workers=3, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look at terms in each topics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pprint import pprint\n# Print the Keyword in the 10 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[bow_corpus]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Compute Baseline Coherence Score\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import CoherenceModel\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nBaseline Coherence Score: ', coherence_lda)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameter Tuning\n\nFor the simplicity of the project, I will only tune the number of topics (K).\n\n\nC_v will be used as metric for performance comparison.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_coherence_values(dictionary, corpus, texts, max_topics = 25, min_topics=3, step_size=2):\n    \"\"\"\n    Compute c_v coherence for various number of topics\n\n    Parameters:\n    ----------\n    dictionary : Gensim dictionary\n    corpus : Gensim corpus\n    texts : List of input texts\n    limit : Max num of topics\n\n    Returns:\n    -------\n    model_list : List of LDA topic models\n    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n    \"\"\"\n    coherence_values = []\n    model_list = []\n    for num_topics in range(min_topics, max_topics, step_size):\n        model = gensim.models.LdaMulticore(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=4, workers=3, random_state=100) #workers = 3 to increase computation power\n        model_list.append(model)\n        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n        coherence_values.append(coherence_model.get_coherence())\n\n    return model_list, coherence_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=bow_corpus, texts=processed_docs, max_topics = 25, min_topics=3, step_size=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Took about 15 min to run models with different numbers of topic*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show graph\nmax_topics = 25\nmin_topics=3\nstep_size=2\nx = range(min_topics, max_topics, step_size)\nplt.plot(x, coherence_values)\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend((\"coherence_values\"), loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the coherence scores\nfor m, cv in zip(x, coherence_values):\n    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Running final model\n\nBased on the coherence score above, I choose 17 topics for the final model. In previous model, I set the passes to be lower in order to save time, but in the final model we increases passes to 10 for a better training results.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"optimal_num_topics = 17\nlda_model = gensim.models.LdaMulticore(bow_corpus, num_topics= 17, id2word=dictionary, passes=10, workers=3, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nFinal Model Coherence Score: ', coherence_lda)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compared to the baseline model, the coherence score has increased. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Finding the dominant topic in each review\n\nTo determine what topic a given document is about, we find the topic number that has the highest percentage contribution in that document.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_topics_sentences(ldamodel=lda_model, corpus=bow_corpus, texts=processed_docs):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row in enumerate(ldamodel[corpus]):\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=bow_corpus, texts=processed_docs) #aggregates this information in a presentable table.\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show\ndf_dominant_topic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews.iloc[3497,0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Finding the most representative document for each topic \n\nTo help with understanding the topic, we find the documents a given topic has contributed to the most and infer the topic by reading that document.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group top 5 sentences under each topic\nsent_topics_sorteddf = pd.DataFrame()\n\nsent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n\nfor i, grp in sent_topics_outdf_grpd:\n    sent_topics_sorteddf = pd.concat([sent_topics_sorteddf, \n                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n                                            axis=0) # Perc_Contribution is percentage contribution of the topic in the given document.\n\n# Reset Index    \nsent_topics_sorteddf.reset_index(drop=True, inplace=True)\n\n# Format\nsent_topics_sorteddf.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n\n# Show\nsent_topics_sorteddf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking age and rating differences for each topic group","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"age_rating_by_topics = df_dominant_topic\nage_rating_by_topics['Rating'] = df['Rating']\nage_rating_by_topics['Age'] = df['Age']\n\n\nage_rating_by_topics.groupby('Dominant_Topic')['Age', 'Rating'].mean()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no major difference in age among different topic groups. Group 15 has the lowest rating. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Visualizing topics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pyLDAvis.gensim\npyLDAvis.enable_notebook()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The interactive visualization allows you to explore the most relevant terms in each topic. By comparing the estimated term frequency with overall term frequency we can also identify words that are unique to the topic.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Interpreting topics\n\nSome topics are harder to interpret than others. By combining all the information above and based on my subjective interpretation, the following topic groups are formed:\n* Topic 1: petite buyer\n* Topic 2: loves to report back compliment they receive when wearing\n* Topic 3: recommender, also loves to try in store\n* Topic 4: loves the purchase because it fits very well.\n* Topic 5: the size run large for them and loves to tell that to other reviewers\n* Topic 6: loves reading reviews\n* Topic 7: buys during sales and usually happy purchase\n* Topic 8: recommender, buying fall and winter clothes\n* Topic 9: review reader\n* Topic 10: loves the high quality\n* Topic 11: jeans/pants buyer, cares about the fit\n* Topic 12: intimacy clothing buyer, loves light weight\n* Topic 13: cami buyer, cares most about color\n* Topic 14: skirt buyer \n* Topic 15: disappointed buyer \n* Topic 16: bath suit buyer\n* Topic 17: (difficult to interpret)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nThere were a lot of assumptions made when exploring the data. For example, we assume that people honestly reported their age. Besides, not all customers would have leave reviews for the products they've purchased. So the exploration of customer segmentation only captures those who write reviews. Even though I was able to interpret some of the topics, but the interpretation is very subjective and it's not the most obvious. When doing customer segmentation in the real world (not just exploring python like me), it would be best to have other features about the customers. Features may come from other data collection methods such as large scale surveys. \n\nThank you for reading my exploration of the women's clothing reviews dataset!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}