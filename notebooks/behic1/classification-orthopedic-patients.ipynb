{"cells":[{"metadata":{"_uuid":"99099bea021728ee4748255fb702c298804d34a3"},"cell_type":"markdown","source":"**[DATA ANALYSIS](#1)**\n\n\n**[CLASSIFICATION  WITH MY LOGISTIC REGRESSION MODEL](#2)**\n1. [Data preparation](#3)\n2. [Parameter Initialize](#4)\n3. [Foward and Backward Propagation](#5)\n4. [Learning Algorithm(Updating Parameters)](#6)\n5. [Predict](#7)\n6. [Score](#8)\n7. [Classification](#9)\n\n**[CLASSIFICATION ALGORITHMS WITH SKLEARN](#10)**\n1. [Logistic Regression Algorithm](#11)\n2. [KNN Algorithm](#12)\n3. [SVM (Support Vector Machine) Algorithm](#13)\n4. [Naive Bayes Algorithm](#14)\n5. [Desicon Tree Algorithm](#15)\n6. [Random Forest Algotihm](#16)\n\n**[ACCURACY COMPARISON THE ALL ALGORITHMS](#17)**\n\n     \n\n    "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport warnings\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected = True)\nimport plotly.graph_objs as go\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88e763e186a42276a09f111b445ed4a64141f31e"},"cell_type":"markdown","source":"<a id=\"1\"></a>\n**DATA ANALYSIS**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#import data\n\ndata = pd.read_csv(\"../input/column_2C_weka.csv\")\nprint(data.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"66cffd554cdbbf9d255755cd6174c5a66d8a60c5"},"cell_type":"code","source":"#split data to x, y \nx = data.drop([\"class\"], axis = 1)\ny = data[\"class\"].values\n#normalized data\nx = (x - np.min(x)) / (np.max(x) - np.min(x)).values\n\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c54d8f98d21282f1e112ad449b9a35c60545bc22"},"cell_type":"markdown","source":"**Show the ratio of normal/abnormal as a pie chart **"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"791b90269d509e92af36d7451aedff0f18a1b26f"},"cell_type":"code","source":"#%% Show the ratio of normal/abnormal\nimport seaborn as sns\n\nrate = pd.Series(y).value_counts()\nplt.figure(figsize=[5,5])\nplt.pie(rate.values, explode = [0, 0], labels = rate.index,  autopct = \"%1.1f%%\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9289e94c6e040b48aabf020a3f032cb4f51a7c6d"},"cell_type":"markdown","source":"**Show the values spreading according to their class**"},{"metadata":{"trusted":true,"_uuid":"63999df6e042caa3be38336ad8d019421ccc9d45"},"cell_type":"code","source":"plt.figure(figsize=[15,5])\n\n# Create dataframe and reshape\ncolumns = list(x.columns) #column names\n\ndf = x.copy()\ndf[\"class\"] = y #df = x_data + y_data\ndf = pd.melt(df, value_vars=columns, id_vars='class') #id = class olsun,  diğer columnları variable olarak dağıt\n\n\n#Plot\nplt.figure(figsize=(16,6))\npal = sns.cubehelix_palette(2, rot=.5, dark=.3)\nsns.swarmplot(x=\"variable\",y=\"value\", hue=\"class\", palette=pal, data=df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2029925159521d966d3c178d43a79fb3f1ff8a61"},"cell_type":"markdown","source":"<a id=\"2\"></a>\n**CLASSIFICATION  WITH MY LOGISTIC REGRESSION MODEL**"},{"metadata":{"_uuid":"42a672ddc4c64e396e51bc9230eabe3c5ff89290"},"cell_type":"markdown","source":"<a id=\"3\"></a>\n**1. Data preparation **"},{"metadata":{"trusted":true,"_uuid":"5c50e755d76c06e9bbe67c64dcfa01eb465ab945"},"cell_type":"code","source":"#change y values abnormal/normal to 0/1\ny = np.array( [1 if each == \"Abnormal\" else 0 for each in y] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c31ef35fede842e0d2decb0287c6a5da97b5c46"},"cell_type":"code","source":"#  SPLIT DATA TO train and test\nfrom sklearn.model_selection import train_test_split\n\n#x = checkup, y = classes\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state=42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c93d29f72430a284cdf3f4d53f07a71e9d7398c0"},"cell_type":"markdown","source":"<a id=\"4\"></a>\n**2.  Parameter Initialize**"},{"metadata":{"trusted":true,"_uuid":"ecf5a30b740c72bf203c740ae6853d3d4e3fe751"},"cell_type":"code","source":"#%% PARAMETER INITIALIZE \ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1), 0.01)\n    b = 0.0\n    return w,b\n\ndef sigmoid(z):\n    y_head = 1/(1 + np.exp(-z))\n    return y_head","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11d59be4e6cffb5c00eef5d9a46d2a634b6679cc"},"cell_type":"markdown","source":"<a id=\"5\"></a>\n**3. Foward and Backward Propagation**"},{"metadata":{"trusted":true,"_uuid":"9a3fe7890805d30b6322bdd3d9086b2d865cb79d"},"cell_type":"code","source":"def forward_backward_propagation(w, b, x_train, y_train):\n    #foward propagation\n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    loss = - y_train * np.log(y_head) - (1-y_train) * np.log(1-y_head)\n    cost = (np.sum(loss)) / x_train.shape[1] # Bölme sebebi çıkan sonucu normalize etmek\n\n    #backward propagation   \n    derivative_weight = (np.dot(x_train, ((y_head-y_train).T))) / x_train.shape[1]\n    derivative_bias = np.sum(y_head - y_train) / x_train.shape[1]\n    \n    gradients = {\"derivative_weight\" : derivative_weight, \"derivative_bias\" : derivative_bias}\n    \n    return cost, gradients","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ccc817baaf369ea6f6cbc8f48901c01cb55779c"},"cell_type":"markdown","source":"<a id=\"6\"></a>\n**4. Learning Algorithm(Updating Parameters) **"},{"metadata":{"trusted":true,"_uuid":"f73cd862bb38f8f6234acd9ad924ef13132618d3"},"cell_type":"code","source":"def update(w, b, x_train, y_train, learning_rate, number_of_iteration):\n    cost_list = [] #Tüm costları depolamak için, analiz için\n    cost_list2 = [] #Her 10 adımda bir cost değerlerini depolar\n    index = [] # Cost2'nin kaçıncı i değerlerine denk geldiğini gösterir\n    \n    #updating parameters\n    for i in range(number_of_iteration):\n        #make forward and backward propagation and find cost and gradients\n        cost, gradients = forward_backward_propagation(w, b, x_train, y_train)\n        cost_list.append(cost) \n        #Update et\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 100 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iteration %i : %f\" %(i, cost))\n    \n    parameters = {\"weight\" : w, \"bias\" : b} #Elimdeki son weight ve bias değerleri\n    \n    #Parametrelerin güncelleme çizimleri\n    plt.plot(index, cost_list2)\n    plt.xticks(index, rotation='vertical')\n    plt.xlabel(\"Number of iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22a4322f0ae60e2798dd9722531a1784f1d4b32c"},"cell_type":"markdown","source":"<a id=\"7\"></a>\n**5.  Predict**"},{"metadata":{"trusted":true,"_uuid":"02f25041591ab34ee85c654b08de7d3b16766dba"},"cell_type":"code","source":"#%% PREDICT, TEST İÇİN VERİLEN DATA'NIN SONUÇLARINI TAHMİN ET\n\ndef predict(w, b, x_test):\n    # test için verilen data x_test\n    z = sigmoid(np.dot(w.T, x_test) + b)\n    Y_prediction = np.zeros((1, x_test.shape[1])) #Tahmin sonuçları için bir array oluştur. Ör : 1,150...\n    \n    for i in range(z.shape[1]): #her sütün için gezilecek\n        if z[0,i] <= 0.5:\n            Y_prediction[0, i] = 0\n        else:\n            Y_prediction[0, i] = 1\n    \n    return Y_prediction","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b3b1b9473016c17fb0de8fd3779535c529d477f"},"cell_type":"markdown","source":"<a id=\"8\"></a>\n**6. Score**"},{"metadata":{"trusted":true,"_uuid":"678aa7be6a741c7e63793f3b08570400644e675d"},"cell_type":"code","source":"#%% NE KADAR DOĞRU TAHMİN EDİLDİ\n    \ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iterations):\n    #initialize\n    dimension = x_train.shape[0] #that is 4096\n    w,b = initialize_weights_and_bias(dimension)\n    \n    #W ve b değerlerini güncelle. Train ve Test datalarını tahmin et\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate, num_iterations)\n    \n    #y_prediction_train = predict(parameters[\"weight\"], parameters[\"bias\"], x_train)\n    y_prediction_test = predict(parameters[\"weight\"], parameters[\"bias\"], x_test)\n    \n    #Ne kadar yanlış var\n    print(\"My Test Accuracy : {} %\" .format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return y_prediction_test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ba3f45a0f0d45c607a0a252749506d173ddc135"},"cell_type":"markdown","source":"<a id=\"9\"></a>\n**7. Classification**"},{"metadata":{"trusted":true,"_uuid":"5a8e81263a71e6875dd2d6bedfbbe16d7640a0da"},"cell_type":"code","source":"#%% CLASSIFICATION WITH MY LOGISTIC REGRESYON\n\n#BENIM REGRESSION TAHMINLERIM\nmy_predict =logistic_regression(x_train, y_train, x_test, y_test, learning_rate = 5, num_iterations = 1000).reshape(-1,1)\n\n\n#CONFUSION MATRIX, TAHMINLER NE KADAR DOGRU\nfrom sklearn.metrics import confusion_matrix\nmy_cm = confusion_matrix(y_test, my_predict)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#MY LR CONFUSION MATRIX PLOT \nplt.figure(figsize=(5,5))\nsns.heatmap(my_cm, annot = True, linewidth = 0.5, linecolor=\"red\", fmt = \".0f\")\nplt.xlabel(\"Predict Values\")\nplt.ylabel(\"True Values\")\nplt.title(\"MY CONFUSION MATRIX PLOT\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb5da75cea8a046bae8e8d684ef9b60c6802c04f"},"cell_type":"markdown","source":"<a id=\"10\"></a>\n**CLASSIFICATION ALGORITHMS WITH SKLEARN**"},{"metadata":{"_uuid":"b5096b2cd2efd890d0ba708ff51752cd943bf60b"},"cell_type":"markdown","source":"<a id=\"11\"></a>\n**1. Logistic Regression Algorithm**"},{"metadata":{"trusted":true,"_uuid":"27ef6790192effe246788c91ce64322c17734bcc"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T, y_train.T)\n\n#SKLEARN REGRESSION PREDICTS\ny_sk_predict =  lr.predict(x_test.T)\n\n#ACCURACY\nlr_score = lr.score(x_test.T, y_test.T) * 100\nprint(\"Test Accuracy According To (Sklearn)Logistic Reg: {}\".format(lr_score))\n\n#CONFUSION MATRIX\nsk_cm = confusion_matrix(y_test, y_sk_predict)\n\n#SKLEARN lR CONFUSİON MATRİX PLOT\nplt.figure(figsize=(5,5))\nsns.heatmap(sk_cm, annot = True, linewidth = 0.5, linecolor=\"red\", fmt = \".0f\")\nplt.xlabel(\"Predict Values\")\nplt.ylabel(\"True Values\")\nplt.title(\"SK CONFUSİON MATRİX PLOT\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef5a2602c5ea2040d27a52c7da8f008067b24bd4"},"cell_type":"markdown","source":"<a id=\"12\"></a>\n**2. KNN Algorithm**"},{"metadata":{"trusted":true,"_uuid":"c93c4f47eceb3eef66c36e797f78d55acf5d2e33"},"cell_type":"code","source":"#%% CLASSIFICATION WITH KNN\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_score = []\n#k degelerine gore score'ları bul\nfor i in range(1, 40):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train.T, y_train.T)\n    knn_score.append( knn.score(x_test.T, y_test.T) )\n\ndf = pd.DataFrame(knn_score)\n#K DEGERLERINE GORE DOGRULUK ORANLARINI CIZ\nplt.figure(figsize=(7,5))\nplt.plot(df.index+1, df.values, color=\"blue\")\nplt.title(\"K Degerlerine Göre Accuracy\")\nplt.xlabel(\"K value\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\n#K = 15 EN IYI DEGER (K= 14 ICIN TAHMINLER YAP)\nknn = KNeighborsClassifier(n_neighbors=15)\nknn.fit(x_train.T, y_train.T)\ny_knn_predict = knn.predict(x_test.T)\n\n#ACCURACY YAZ\nknn_score = knn.score(x_test.T, y_test.T) * 100\nprint(\"Test Accuracy According To KNN(K=15): {}\".format(knn_score))\n\n##CONFUSION MATRIX, TAHMINLER NE KADAR DOGRU\nknn_cm = confusion_matrix(y_test, y_knn_predict)\n\n#KNN CONFUSİON MATRİX PLOT\nplt.figure(figsize=(6,5))\nsns.heatmap(knn_cm, annot = True, linewidth = 0.5, linecolor=\"red\", fmt = \".0f\")\nplt.xlabel(\"Predict Values\")\nplt.ylabel(\"True Values\")\nplt.title(\"K=15 CONFUSİON MATRİX PLOT\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ee931003b4ab8133c5f0ade7813e10f6cae971a"},"cell_type":"markdown","source":"<a id=\"13\"></a>\n**3. SVM (Support Vector Machine) Algorithm**"},{"metadata":{"trusted":true,"_uuid":"31e46fd60ad4de5bd1e633a5991228303305f71b"},"cell_type":"code","source":"#%% CLASSIFICATION WITH SVM  (SUPPORT VECTOR MACHINE)\nfrom sklearn.svm import SVC\n\nsvm = SVC(random_state = 42)\nsvm.fit(x_train.T, y_train.T)\n\n#ACCURACY YAZ\nsvm_score = svm.score(x_test.T, y_test.T) * 100\nprint(\"Test Accuracy According To SVM : {}\".format(svm_score))\n\n#PREDICT WITH SVM\nsvm_predict = svm.predict(x_test.T)\n\n#CONFUSION MATRIX\nsvm_cm = confusion_matrix(y_test, svm_predict)\n\n#SVM CONFUSİON MATRIX PLOT\nplt.figure(figsize=(5,5))\nsns.heatmap(svm_cm, annot = True, linewidth = 0.5, linecolor=\"red\", fmt = \".0f\")\nplt.xlabel(\"Predict Values\")\nplt.ylabel(\"True Values\")\nplt.title(\"SK CONFUSİON MATRİX PLOT\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"505a26e806ed657bca3d2b7e3e42275d50746297"},"cell_type":"markdown","source":"<a id=\"14\"></a>\n**4. Bayes Algorithm**"},{"metadata":{"trusted":true,"_uuid":"85eb541f360c4d2dc3714cbb1d42a0dd48ef1a54"},"cell_type":"code","source":"# CLASSIFICATION WITH NAIVE BAYES\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train.T, y_train.T)\n\n#ACCURACY\nnb_score = nb.score(x_test.T, y_test.T) * 100\nprint(\"Test Accuracy According To Naive Bayes : {}\".format(nb_score))\n\n#PREDICT WITH NAIVE BAYES\nnb_predict = nb.predict(x_test.T)\n\n#CONFUSION MATRIX\nnb_cm = confusion_matrix(y_test, nb_predict)\n\n#NAIVE BAYES CONFUSİON MATRIX PLOT\nplt.figure(figsize=(5,5))\nsns.heatmap(nb_cm, annot = True, linewidth = 0.5, linecolor=\"red\", fmt = \".0f\")\nplt.xlabel(\"Predict Values\")\nplt.ylabel(\"True Values\")\nplt.title(\"NAIVE BAYES CONFUSİON MATRİX PLOT\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a238902fd6dfb545dc76f7ec52cf38fbed59de70"},"cell_type":"markdown","source":"<a id=\"15\"></a>\n**5. Desicon Tree Algorithm**"},{"metadata":{"trusted":true,"_uuid":"60967b8793c8c855b530eba1d115ed49de55ed99"},"cell_type":"code","source":"#%% CLASSIFICATION WITH DESCION TREE\n\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train.T, y_train.T)\n\n#ACCURACY\ndt_score = dt.score(x_test.T, y_test.T) * 100\nprint(\"Test Accuracy According To Decision Tree : {}\".format(dt_score))\n\n#PREDICT WITH decision tree\ndt_predict = dt.predict(x_test.T)\n\n#CONFUSION MATRIX\ndt_cm = confusion_matrix(y_test, nb_predict)\n\n#DESCION TREE CONFUSİON MATRIX PLOT\nplt.figure(figsize=(5,5))\nsns.heatmap(dt_cm, annot = True, linewidth = 0.5, linecolor=\"red\", fmt = \".0f\")\nplt.xlabel(\"Predict Values\")\nplt.ylabel(\"True Values\")\nplt.title(\"DECISION TREE CONFUSİON MATRİX\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3819a731e26e7c39687a81434268481859da78eb"},"cell_type":"markdown","source":"<a id=\"16\"></a>\n**6. Random Forest Algotihm**"},{"metadata":{"trusted":true,"_uuid":"65c88ca2862c701b9d7ab9227c40a1a4cf64035e"},"cell_type":"code","source":"#%% CLASSIFICATION WITH RANDOM FOREST\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=300, random_state=1)\nrf.fit(x_train.T, y_train.T)\n\n#ACCURACY YAZ\nrf_score = rf.score(x_test.T, y_test.T) * 100\nprint(\"Test Accuracy According To Random Forest Algorithm : {}\".format(rf_score))\n\n#PREDICT WITH RANDOM FOREST\nrf_predict = rf.predict(x_test.T)\n\n#CONFUSION MATRIX\nrf_cm = confusion_matrix(y_test, rf_predict)\n\n#RANDOM FOREST CONFUSİON MATRIX PLOT\nplt.figure(figsize=(5,5))\nsns.heatmap(rf_cm, annot = True, linewidth = 0.5, linecolor=\"red\", fmt = \".0f\")\nplt.xlabel(\"Predict Values\")\nplt.ylabel(\"True Values\")\nplt.title(\" RANDOM FOREST ALGORITHM CONFUSİON MATRİX\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45c32a97c11a280b4407e3aab4e89a0a0d53e6e3"},"cell_type":"markdown","source":"<a id=\"17\"></a>\n**ACCURACY COMPARISON THE ALL ALGORITHMS**"},{"metadata":{"trusted":true,"_uuid":"0f172f89074f217c0a72a5993833a2b7f42ea8ac"},"cell_type":"code","source":"trace = go.Bar(\n    x=['Logistic Regression', 'KNN', 'SVM', 'Naive Bayes', 'Decision Tree', 'Random Forest'],\n    y=[lr_score, knn_score, svm_score, nb_score, dt_score, rf_score],\n    marker=dict(color=['#008BF8', '#0FFF95', '#EE6C4D', '#A30000', '#2081C3', '#FF7700']),\n)\n\nlayout = go.Layout(\n    title='Accuracy Comparison The All Algorithms',\n)\n\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}