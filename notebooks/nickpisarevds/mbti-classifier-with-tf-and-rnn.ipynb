{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import *\nimport seaborn as sns\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')\n\n%matplotlib inline\nfrom nltk import tokenize\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.engine.topology import Layer\nfrom keras import initializers as initializers, regularizers, constraints\nfrom keras.utils.np_utils import to_categorical\nfrom keras import optimizers\nfrom keras.models import Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/mbti-type/mbti_1.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords \nfrom nltk import word_tokenize\n\nstemmer = PorterStemmer()\nlemmatiser = WordNetLemmatizer() \ncachedStopWords = stopwords.words(\"english\")\n\ndef cleaning_data(data, remove_stop_words=True):\n    list_posts = []\n    i=0   \n    for row in data.iterrows():\n        posts = row[1].posts\n        temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', posts) #remove urls\n        temp = re.sub(\"[^a-zA-Z.]\", \" \", temp) #remove all punctuations except fullstops.\n        temp = re.sub(' +', ' ', temp).lower() \n        temp=re.sub(r'\\.+', \".\", temp) #remove multiple fullstops.\n        if remove_stop_words:\n            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in cachedStopWords])\n        else:\n            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ')])\n        list_posts.append(temp)\n\n    text = np.array(list_posts)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_text = cleaning_data(data, remove_stop_words=True)\ndata['clean_text']=clean_text\ndata = data[['clean_text', 'type']]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"types=data['type']\ntext=data['clean_text']\ntps=data.groupby('type')\nprint(\"total types:\",tps.ngroups)\nprint(tps.size())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len=200   # maximum words in a sentence\nVAL_SPLIT = 0.2\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(text)\nmax_features = len(tokenizer.word_index) + 1 # maximum number of unique words\n\n\ninput_sequences = []\nfor line in (data):\n    token_list = tokenizer.texts_to_sequences([line])[0]\n    for i in range(1, len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n        input_sequences.append(n_gram_sequence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_seq_length = max([len(x) for x in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences, maxlen = max_seq_length, padding='pre'))\n\nxs, labels = input_sequences[:,:-1],input_sequences[:,-1]\nys = tf.keras.utils.to_categorical(labels, num_classes=max_features, dtype='float64')\n\nx_val = xs * VAL_SPLIT\ny_val = ys * VAL_SPLIT","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_features, 64, input_length = max_seq_length - 1))\nmodel.add(tf.keras.layers.Conv1D(32, (1), padding='same', activation='relu'))\nmodel.add(tf.keras.layers.Conv1D(32, (1), activation='relu'))\nmodel.add(tf.keras.layers.Dropout(0.5)) \n\nmodel.add(tf.keras.layers.Conv1D(64, (1), padding='same', activation='relu'))\nmodel.add(tf.keras.layers.Conv1D(64, (1), activation='relu'))\nmodel.add(tf.keras.layers.Dropout(0.5)) \n\nmodel.add(Bidirectional(LSTM(64)))\nmodel.add(Dense(max_features, activation = 'softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['acc'])\nhistory = model.fit(xs, ys, epochs = 500, validation_data=(x_val, y_val), verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('mbti_rnn.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}