{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np \nimport random as rand\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features=[\"anaemia\",\"diabetes\",\"high_blood_pressure\",\"sex\",\"smoking\",\"DEATH_EVENT\"]\ncat_without_label=[\"anaemia\",\"diabetes\",\"high_blood_pressure\",\"sex\",\"smoking\"]\nnum_features=pd.Series(df.columns)\nnum_features=num_features[~num_features.isin(cat_features)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"DEATH_EVENT\"].value_counts().plot(kind='bar')\nplt.title='Death Events Distribution'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(data=df[num_features].corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Learned this next part from: https://www.kaggle.com/ksvmuralidhar/heart-failure-prediction-auc-0-98 "},{"metadata":{"trusted":true},"cell_type":"code","source":"r=c=0\nfig,ax = plt.subplots(4,2,figsize=(14,25))\nfor n, i in enumerate(num_features):\n    sns.boxplot(x='DEATH_EVENT', y=i, data=df, ax=ax[r,c])\n    ax[r,c].set_title(i.upper()+\" by \"+\"DEATH_EVENT\")\n    c+=1\n    if (n+1)%2==0:\n        r+=1\n        c=0\nax[r,c].axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[num_features].hist(figsize=(10,10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r=c=0\nfig,ax = plt.subplots(3,2,figsize=(14, 14))\nfor n, i in enumerate(cat_without_label):\n    sns.countplot(x=i, hue='DEATH_EVENT', data=df, ax=ax[r,c])\n    ax[r,c].set_title(i.upper())\n    c+=1\n    if (n+1)%2==0:\n        r+=1\n        c=0\nax[r,c].axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df.iloc[:, :-1]\nY=df.iloc[:, -1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature Importance Reference: https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = RandomForestClassifier(n_estimators=5000,random_state=42)\nclassifier.fit(x_train,y_train)\n\nprint('Train Accuracy: %f' % classifier.score(x_train, y_train))\nprint('Test Accuracy: %f' % classifier.score(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = cat_features\nfeature_names = np.r_[feature_names, num_features]\n\nfeat_imp = pd.DataFrame(classifier.feature_importances_)\nfeat_imp.index = pd.Series(df.iloc[:,:-1].columns)\nfeat_imp = (feat_imp*100).copy().sort_values(by=0,ascending=False)\nfeat_imp = feat_imp.reset_index()\nfeat_imp.columns = [\"Feature\",\"Importance_score\"]\n\nfig = plt.figure(figsize=(6,10))\nsns.scatterplot(data=feat_imp,x=5,y=np.linspace(100,0,12),size=\"Importance_score\",sizes=(200,2000),legend=False)\nfor i,feat,imp in zip(np.linspace(100,0,12),feat_imp[\"Feature\"],feat_imp[\"Importance_score\"]):\n    plt.text(x=5.05,y=i-1,s=feat)\n    plt.text(x=4.89,y=i-1,s=np.round(imp,2))\nplt.axis(\"off\")\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good resource for SMOTE: https://towardsdatascience.com/how-to-effortlessly-handle-class-imbalance-with-python-and-smote-9b715ca8e5a7"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom imblearn.over_sampling import SMOTE\n\n\nfor var in np.arange(feat_imp.shape[0],6,-1):\n    X_new = X[feat_imp.iloc[:var,0]].copy()\n    X_train, X_test, y_train,y_test = train_test_split(X_new,Y,test_size=0.2,random_state=11)\n    smote = SMOTE(random_state = 11) \n    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n    final_rf = RandomForestClassifier(random_state=11)\n    \n    \n    gscv = GridSearchCV(estimator=final_rf,param_grid={\n        \"n_estimators\":[100,500,1000,5000],\n        \"criterion\":[\"gini\",\"entropy\"]\n    },cv=5,n_jobs=-1,scoring=\"f1_weighted\")\n\n    gscv.fit(X_train_smote,y_train_smote)\n    print(str(var)+\" variables:  \"+str(gscv.best_estimator_)+\"  F1 score: \"+str(gscv.best_score_))\n    \n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_new = X[feat_imp.iloc[:8,0]].copy()\nX_train, X_test, y_train,y_test = train_test_split(X_new,Y,test_size=0.2,random_state=11)\nsmote = SMOTE(random_state = 11) \nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\nfinal_rf = RandomForestClassifier(random_state=11)\ngscv = GridSearchCV(estimator=final_rf,param_grid={\n    \"n_estimators\":[100,500,1000,5000],\n    \"criterion\":[\"gini\",\"entropy\"]\n},cv=5,n_jobs=-1,scoring=\"f1_weighted\")\n\ngscv.fit(X_train_smote,y_train_smote)\nfinal_model = gscv.best_estimator_\n\nfinal_model.score(X_train_smote, y_train_smote)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=final_model.predict(X_test)\nprint(pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use learning curve to see bias and variance and see if additional data will help the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import learning_curve\n\ntrain_size, train_acc, test_acc = learning_curve(final_model, X_train_smote, y_train_smote, cv=5)\nlearn_df = pd.DataFrame({\"Train_size\":train_size,\"Train_Accuracy\":train_acc.mean(axis=1),\"Test_Accuracy\":test_acc.mean(axis=1)}).melt(id_vars=\"Train_size\")\nsns.lineplot(x='Train_size', y='value', data=learn_df, hue='variable')\nplt.ylabel('Accuracy');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Learning curve confirms the model is overfitting the data. Let's use confusion matrix to confirm overfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, plot_roc_curve, plot_precision_recall_curve\n\ncm = confusion_matrix(y_test, pred)\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, annot=True, cmap='Blues')\nplt.xlabel('Prediction')\nplt.ylabel('Actual');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we are tuning some hyperparameters to overcome the overfitting. \n\n1. Use large number of n_estimators - grows more trees, prevents overfitting\n2. Use low number for max_depth - prevents model from growing\n3. Use large number for max_samples - ensures the leaf has good amount of samples"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_new = X[feat_imp.iloc[:8,0]].copy()\nX_train, X_test, y_train,y_test = train_test_split(X_new,Y,test_size=0.2,random_state=11)\nsmote = SMOTE(random_state = 11) \nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\nfinal_rf = RandomForestClassifier(random_state=11)\ngscv = GridSearchCV(estimator=final_rf,param_grid={\n    \"n_estimators\":[5000,7000],\n    \"criterion\":[\"gini\",\"entropy\"],\n    \"max_depth\":[3,5,7],\n    \"min_samples_split\":[80,100],\n    \"min_samples_leaf\":[40,50],\n},cv=5,n_jobs=-1,scoring=\"f1_weighted\")\n\ngscv.fit(X_train_smote,y_train_smote)\nfinal_model = gscv.best_estimator_\n\nfinal_model.score(X_train_smote, y_train_smote)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred = final_model.score(X_train_smote)\n\nprint(classification_report(y_train_smote, train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the training score and test score are similar, so the model isn't overfitting anymore."},{"metadata":{"trusted":true},"cell_type":"code","source":"final_pred = final_model.predict(X_test)\nprint(final_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, final_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(final_model, X_test, y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_precision_recall_curve(final_model, X_test, y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good true positive rate!"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, final_pred)\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, annot=True, cmap='Blues')\nplt.xlabel('Prediction')\nplt.ylabel('Actual');","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}