{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Breast Cancer Detection"},{"metadata":{},"cell_type":"markdown","source":"# Using Machine Learning To Predict Diagnosis of a Breast Cancer\n  \n\n## 1. Identify the problem\nBreast cancer is the most common malignancy among women, accounting for nearly 1 in 3 cancers diagnosed among women in the United States, and it is the second leading cause of cancer death among women. Breast Cancer occurs as a results of abnormal growth of cells in the breast tissue, commonly referred to as a Tumor. A tumor does not mean cancer - tumors can be benign (not cancerous), pre-malignant (pre-cancerous), or malignant (cancerous). Tests such as MRI, mammogram, ultrasound and biopsy are commonly used to diagnose breast cancer performed.\n\n### 1.1 Expected outcome\nGiven breast cancer results from breast fine needle aspiration (FNA) test (is a quick and simple procedure to perform, which removes some fluid or cells from a breast lesion or cyst (a lump, sore or swelling) with a fine needle similar to a blood sample needle). Since this build a model that can classify a breast cancer tumor using two training classification:\n* 1= Malignant (Cancerous) - Present\n* 0= Benign (Not Cancerous) -Absent\n\n### 1.2 Objective \nSince the labels in the data are discrete, the predication falls into two categories, (i.e. Malignant or benign). In machine learning this is a classification problem. \n        \n> *Thus, the goal is to classify whether the breast cancer is benign or malignant and predict the recurrence and non-recurrence of malignant cases after a certain period.  To achieve this we have used machine learning classification methods to fit a function that can predict the discrete class of new input.*\n\n### 1.3 Identify data sources\nThe [Breast Cancer](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29) datasets is available machine learning repository maintained by the University of California, Irvine. The dataset contains **569 samples of malignant and benign tumor cells**. \n* The first two columns in the dataset store the unique ID numbers of the samples and the corresponding diagnosis (M=malignant, B=benign), respectively. \n* The columns 3-32 contain 30 real-value features that have been computed from digitized images of the cell nuclei, which can be used to build a model to predict whether a tumor is benign or malignant. \n\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load dataset\ndf = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count of malignant and benignate\ndf['diagnosis'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['diagnosis'], label = 'count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encoding Categorical data\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf.iloc[:,1] = le.fit_transform(df.iloc[:,1].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Separate columns into smaller dataframes to perform visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_mean=df.iloc[:,1:11]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot histograms of CUT1 variables\nhist_mean=data_mean.hist(bins=10, figsize=(15, 10),grid=False,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Heatmap\nplt.figure(figsize=(20,20))\nsns.heatmap(df.corr(),annot=True, fmt = '.0%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Density Plots\nplt = data_mean.plot(kind= 'density', subplots=True, layout=(4,3), sharex=False, \n                     sharey=False,fontsize=12, figsize=(15,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Spliting the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train test split\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.drop(['diagnosis'], axis=1)\ny = df['diagnosis'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*## Feature Scaling*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state= 0 )\nclassifier.fit(x_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nreg = LogisticRegression()\nreg.fit(x_train,y_train)\nprint(\"Logistic Regression accuracy : {:.2f}%\".format(reg.score(x_test,y_test)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Support Vactor \nfrom sklearn.svm import SVC\nsvm = SVC(random_state=10)\nsvm1 = SVC(kernel='linear',gamma='scale',random_state=10)\nsvm2 = SVC(kernel='rbf',gamma='scale',random_state=10)\nsvm3 = SVC(kernel='poly',gamma='scale',random_state=10)\nsvm4 = SVC(kernel='sigmoid',gamma='scale',random_state=10)\n\nsvm.fit(x_train,y_train)\nsvm1.fit(x_train,y_train)\nsvm2.fit(x_train,y_train)\nsvm3.fit(x_train,y_train)\nsvm4.fit(x_train,y_train)\n\nprint('SVC Accuracy : {:,.2f}%'.format(svm.score(x_test,y_test)*100))\n\nprint('SVC Liner Accuracy : {:,.2f}%'.format(svm1.score(x_test,y_test)*100))\n\nprint('SVC RBF Accuracy : {:,.2f}%'.format(svm2.score(x_test,y_test)*100))\n\nprint('SVC Ploy Accuracy : {:,.2f}%'.format(svm3.score(x_test,y_test)*100))\n\nprint('SVC Sigmoid Accuracy : {:,.2f}%'.format(svm4.score(x_test,y_test)*100))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(\" Naive Bayes accuracy : {:.2f}%\".format(nb.score(x_test,y_test)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=1000,random_state=1)\nrf.fit(x_train,y_train)\nprint(\"Random Forest Classifier accuracy : {:.2f}%\".format(rf.score(x_test,y_test)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost\nxg = xgboost.XGBClassifier()\nxg.fit(x_train,y_train)\nprint(\"XGboost accuracy : {:.2f}%\".format(xg.score(x_test,y_test)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=100)\nknn.fit(x_train,y_train)\nprint('KNN Accuracy {:.2f}%'.format(knn.score(x_test,y_test)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(criterion='entropy',max_depth=4, random_state=10)\ndt.fit(x_train,y_train)\nprint(\"Decision Tree Accuracy : {:,.2f}%\".format(dt.score(x_test,y_test)*100))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}