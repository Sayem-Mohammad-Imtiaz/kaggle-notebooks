{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Retail data analytics\n\nThe dataset, available at https://www.kaggle.com/manjeetsingh/retaildataset, contains:\n\"\"\"\nhistorical sales data for 45 stores located in different regions - each store contains a number of departments. The company also runs several promotional markdown events throughout the year. These markdowns precede prominent holidays, the four largest of which are the Super Bowl, Labor Day, Thanksgiving, and Christmas. The weeks including these holidays are weighted five times higher in the evaluation than non-holiday weeks.\n\"\"\"","metadata":{}},{"cell_type":"markdown","source":"## Contents \nThe following activities will be conducted: \n\n1. Description of the dataset\n\n2. Data exploration\n\n3. Data cleaning and feature engineering\n\n4. Key findings and insights\n\n5. Hypothesis about this data:\n    1. Are sales larger or smaller in a given week if preceded by a holiday week?\n    2. Are sales larger or smaller on normal weeks or holiday weeks?\n    3. Are sales larger or smaller on weekends or on weekdays? \n\n6. Statistical test for one of the hypotheses\n\n7. Next steps\n\n8. Data quality","metadata":{}},{"cell_type":"markdown","source":"## 1. Data description\nThere are three .csv files:\n\n### Features\n\nContains additional data related to the store, department, and regional activity for the given dates.\n\n    Store - the store number\n    Date - the week\n    Temperature - average temperature in the region\n    Fuel_Price - cost of fuel in the region\n    MarkDown1-5 - anonymized data related to promotional markdowns. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA\n    CPI - the consumer price index\n    Unemployment - the unemployment rate\n    IsHoliday - whether the week is a special holiday week\n\n### Sales\n\nHistorical sales data, which covers to 2010-02-05 to 2012-11-01. Within this tab you will find the following fields:\n\n    Store - the store number\n    Dept - the department number\n    Date - the week\n    Weekly_Sales -  sales for the given department in the given store\n    IsHoliday - whether the week is a special holiday week\n\n### Stores\n\nContains additional data related to the store, department, and regional activity for the given dates.\n\n    Store - the store number\n    Type - store type\n    Size - store size\n    ","metadata":{}},{"cell_type":"code","source":"# Import python packages to be used\n# %pylab inline\n# %config InlineBackend.figure_formats = ['svg']\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:70% !important; }</style>\"))\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats \nimport math\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:36:59.182148Z","iopub.execute_input":"2021-07-01T20:36:59.182715Z","iopub.status.idle":"2021-07-01T20:36:59.188951Z","shell.execute_reply.started":"2021-07-01T20:36:59.18268Z","shell.execute_reply":"2021-07-01T20:36:59.188009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read data sources and convert them into dataframes\nfiles = ['features', 'sales', 'stores']\nfilenames = ['Features data set', 'sales data-set', 'stores data-set']\n\ndatasets = dict()\nfor x, y in zip(files, filenames):\n    datasets[x] = pd.read_csv('../input/retaildataset/'+y+'.csv') # Cleaner names\n\nfeatures, sales, stores = datasets['features'], datasets['sales'], datasets['stores']","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:36:59.190103Z","iopub.execute_input":"2021-07-01T20:36:59.190383Z","iopub.status.idle":"2021-07-01T20:36:59.382961Z","shell.execute_reply.started":"2021-07-01T20:36:59.190356Z","shell.execute_reply":"2021-07-01T20:36:59.382051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Data exploration\nWe will see:\n\n2.1. If there are invalid values.\n\n2.2. Store characterization.\n\n2.3. Range for weekly sales by store.\n\n2.4. Evolution of fuel price, CPI, unemployment, and weekly sales in the region.","metadata":{}},{"cell_type":"markdown","source":"### 2.1. Invalid values\nLet's see which columns have empty, zero, negative values, or incorrect data types (e.g. strings instead of numbers)","metadata":{}},{"cell_type":"code","source":"# Show each of the dataframes to see how many records and fields there are in each\nfor f in files:\n    print('\\n'+'#'*10+' {0} '.format(f.upper())+'#'*10)\n    display(datasets[f])\n    print(datasets[f].dtypes)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:43.554721Z","iopub.execute_input":"2021-07-01T20:37:43.555195Z","iopub.status.idle":"2021-07-01T20:37:43.605383Z","shell.execute_reply.started":"2021-07-01T20:37:43.555161Z","shell.execute_reply":"2021-07-01T20:37:43.604441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dates are being interpreted as 'object', so they have to be converted to datetimes.","metadata":{}},{"cell_type":"code","source":"for f in ['features', 'sales']:\n    datasets[f].Date = pd.to_datetime(datasets[f].Date)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:36:59.435445Z","iopub.execute_input":"2021-07-01T20:36:59.435817Z","iopub.status.idle":"2021-07-01T20:36:59.506115Z","shell.execute_reply.started":"2021-07-01T20:36:59.435778Z","shell.execute_reply":"2021-07-01T20:36:59.505316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for f in files:\n    print('\\n'+'#'*10+' {0} '.format(f.upper())+'#'*10)\n    columns = datasets[f].columns.values.tolist()\n    for c in columns:\n        if datasets[f][c].isnull().values.any():\n            print('{0}: {1} invalid values found'.format(c, datasets[f][c].isnull().sum()))\n        else:\n            print('{0}: ok'.format(c))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:36:59.507196Z","iopub.execute_input":"2021-07-01T20:36:59.507449Z","iopub.status.idle":"2021-07-01T20:36:59.523253Z","shell.execute_reply.started":"2021-07-01T20:36:59.507425Z","shell.execute_reply":"2021-07-01T20:36:59.522458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No columns in our sales and stores dataframes are incomplete. However, there is a number of rows with incomplete values in the features dataframe. For the time being no action will be taken because we do not plan to check the markdowns, consumer price index or unemployment.\n\nNext we should find negative and zero values in some columns where this might be nonsensical.","metadata":{}},{"cell_type":"code","source":"# Columns in each dataframe that will be checked for unusual values\ncols_negcheck = {'features': ['Fuel_Price', 'CPI', 'Unemployment'],\n                 'sales': ['Weekly_Sales'],\n                 'stores': ['Size'],\n                }\n# Additional columns added to a temporary dataframe to count negative or zero values\nfor f in files: \n    df = datasets[f].copy()\n    for c in cols_negcheck[f]:\n        df.loc[df[c] == 0, c+'_zero'] = 1\n        df.loc[df[c] != 0, c+'_zero'] = 0\n        df.loc[df[c] < 0, c+'_negative'] = 1\n        df.loc[df[c] > 0, c+'_negative'] = 0\n        print('\\nRows in {0}/{1}:'.format(f, c))\n        rows = df.shape[0]\n        print('  All: {0}'.format(rows))\n        neg_rows = df[c+'_negative'].sum()\n        print('  Negative: {0} ({1})'.format(neg_rows, '{:.2%}'.format(neg_rows/rows)))\n        zero_rows = df[c+'_zero'].sum()\n        print('  Zero: {0} ({1})'.format(zero_rows, '{:.2%}'.format(zero_rows/rows)))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:36:59.524395Z","iopub.execute_input":"2021-07-01T20:36:59.524633Z","iopub.status.idle":"2021-07-01T20:36:59.590829Z","shell.execute_reply.started":"2021-07-01T20:36:59.524609Z","shell.execute_reply":"2021-07-01T20:36:59.589918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 1285 rows where sales are negative and 73 where these are zero. Negative rows might happen if there are returns, but this is difficult to determine from the data.\n\nAs the number of negative or zero rows is very small, they will be removed before conducting the preliminary analyses, but this phenomenon does warrant further investigation.","metadata":{}},{"cell_type":"markdown","source":"### 2.2. Store characterization","metadata":{}},{"cell_type":"code","source":"stores = datasets['stores']\nsns.set(style='darkgrid')\nax = sns.countplot(x='Type', data=stores)\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")\nax.set(title='Stores by Type')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:36:59.591944Z","iopub.execute_input":"2021-07-01T20:36:59.592202Z","iopub.status.idle":"2021-07-01T20:36:59.743165Z","shell.execute_reply.started":"2021-07-01T20:36:59.592173Z","shell.execute_reply":"2021-07-01T20:36:59.74202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style='darkgrid')\ng = sns.histplot(stores, x='Size', hue='Type', element='step').set(title='Stores by Size')\nplt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:36:59.746007Z","iopub.execute_input":"2021-07-01T20:36:59.7464Z","iopub.status.idle":"2021-07-01T20:36:59.968179Z","shell.execute_reply.started":"2021-07-01T20:36:59.746366Z","shell.execute_reply":"2021-07-01T20:36:59.967443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that 'A' stores are more frequent than 'B' stores and 'C' stores, while their size generally matches that order. It is possible that the smaller A and B stores have started existing later and for this reason they have had fewer sales.\n\nNext, let's see how many departments exist.","metadata":{}},{"cell_type":"code","source":"depts = sales.Dept.unique()\ndepts.sort()\nprint('There are {0} unique departments.'.format(len(depts)))\nprint(depts)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:36:59.96983Z","iopub.execute_input":"2021-07-01T20:36:59.970376Z","iopub.status.idle":"2021-07-01T20:36:59.980186Z","shell.execute_reply.started":"2021-07-01T20:36:59.970331Z","shell.execute_reply":"2021-07-01T20:36:59.979136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is possible that stores contain different departments according to their type. We will merge two tables to associate stores to store types and store departments.","metadata":{}},{"cell_type":"code","source":"# Merge 'stores' and 'sales'\nsales_stores = pd.merge(datasets['stores'], datasets['sales'], on='Store', how='outer')\nsales_stores","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:36:59.981437Z","iopub.execute_input":"2021-07-01T20:36:59.981819Z","iopub.status.idle":"2021-07-01T20:37:00.063642Z","shell.execute_reply.started":"2021-07-01T20:36:59.981789Z","shell.execute_reply":"2021-07-01T20:37:00.062493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"letters = ['A', 'B', 'C']\ndepts = dict()\nfor l in letters:\n    d = sales_stores[sales_stores['Type'] == l]\n    d = d.Dept.unique()\n    d.sort()\n    print('\\nThere are {0} unique departments in stores of Type {1}.'.format(len(d), l))\n    print(d)\n    depts[l]= d\n\ndef notintersection(list1, list2):\n    list3 = [x for x in list1 if x not in list2]\n    return list3\n    \nprint('\\nDepartments that can exist in A but not in B: {0}'.format(notintersection(depts['A'], depts['B'])))\nprint('\\nDepartments that can exist in B but not in C: {0}'.format(notintersection(depts['B'], depts['C'])))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:00.064767Z","iopub.execute_input":"2021-07-01T20:37:00.065111Z","iopub.status.idle":"2021-07-01T20:37:00.167972Z","shell.execute_reply.started":"2021-07-01T20:37:00.065077Z","shell.execute_reply":"2021-07-01T20:37:00.167082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Stores of types 'A' and 'B' may contain virtually the same departments, while C stores may contain 15 or 14 fewer departments, respectively. \n\nThere could be further analysis, for example by sales, but sales by department are not available. \n\nIt is also possible that not all departments in the same class contain the same number of departments. Then, it would be possible to determine if there is a correlation between the number of departments in a store and the weekly sales there or its size.","metadata":{}},{"cell_type":"markdown","source":"### 2.3. Range of weekly sales by store\nLet's look more carefully at weekly sales.","metadata":{}},{"cell_type":"code","source":"# Basic statistics\nweekly_series = pd.DataFrame(sales_stores['Weekly_Sales'])\nweekly_series.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:00.169068Z","iopub.execute_input":"2021-07-01T20:37:00.169328Z","iopub.status.idle":"2021-07-01T20:37:00.205038Z","shell.execute_reply.started":"2021-07-01T20:37:00.169302Z","shell.execute_reply":"2021-07-01T20:37:00.203956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style='darkgrid')\nfig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False, figsize=(14,5))\ng= sns.histplot(weekly_series, x='Weekly_Sales', bins=50, ax=ax1).set(title='Sales by Week, any store (histogram)')\ng= sns.boxplot(x=weekly_series[\"Weekly_Sales\"], ax=ax2).set(title='Sales by Week, any store (boxplot)')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:00.206199Z","iopub.execute_input":"2021-07-01T20:37:00.206484Z","iopub.status.idle":"2021-07-01T20:37:00.718632Z","shell.execute_reply.started":"2021-07-01T20:37:00.206456Z","shell.execute_reply":"2021-07-01T20:37:00.717564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In addition to the weeks with negative or zero values previously discussed, there is a very small number of stores where sales are unusually high. Let's get a better idea of how many they are.","metadata":{}},{"cell_type":"code","source":"count, division = np.histogram(sales_stores['Weekly_Sales'], bins=50)\nout, bins = pd.cut(sales_stores['Weekly_Sales'], bins=division, include_lowest=True, right=True, retbins=True)\ncounts = out.value_counts().head(35) # There are 15 empty bins at the high end, so we'll exclude them\n\ndef gen_cumulative(counts_df):\n    \"\"\"\n    Generate a 'histogram' of stores according to a count\n    \"\"\"\n    total = sum(counts_df.values)\n    counts_df = pd.DataFrame(data={'Intervals': counts_df.index.to_list(), 'Count': counts_df.values})\n    counts_df['Cumulative Count'] = counts_df['Count'].cumsum()/total\n    counts_df['Remainder'] = 1 - counts_df['Cumulative Count'] # What would be left if highest values were excluded\n    counts_df['Percentage'] = counts_df['Count']/total\n    # Add format\n    counts_df['Count'] = pd.Series([\"{0:,}\".format(round(val, 2)) for val in counts_df['Count']], index = counts_df.index)\n    for c in ['Cumulative Count', 'Remainder', 'Percentage']:\n        counts_df[c] = pd.Series([\"{0:.2f}%\".format(val * 100) for val in counts_df[c]], index = counts_df.index)\n    return counts_df\n    \ncounts = gen_cumulative(counts)\ncounts","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:00.720074Z","iopub.execute_input":"2021-07-01T20:37:00.720474Z","iopub.status.idle":"2021-07-01T20:37:00.778276Z","shell.execute_reply.started":"2021-07-01T20:37:00.720432Z","shell.execute_reply":"2021-07-01T20:37:00.777555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4. Evolution of fuel price, CPI, unemployment, and weekly sales in all stores\n\nLet's now see sale and price variations according to time. For this we will have to get the sum of sales across stores, and the mean price, respectively.","metadata":{}},{"cell_type":"code","source":"def calc_min_max_avg (df, var, sortby='Date', dropped_columns=[]):\n    \"\"\"\n    Add columns with average, min, max of a variable in a dataframe, and remove all unnecessary columns.\n    It is assumed there is a date or datetime column by which rows will be sorted.\n    \"\"\"\n    df_agg = df.copy().drop(columns = dropped_columns)\n    df_agg = df_agg.sort_values(by=sortby).reset_index().drop(columns = ['index'])\n    df_agg[var+'_Min'] = df_agg.groupby(sortby)[var].transform(min)\n    df_agg[var+'_Max'] = df_agg.groupby(sortby)[var].transform(max)\n    q25, q50, q75 = [df_agg.groupby(sortby)[var].quantile([q]).transpose().values.tolist() for q in [.25, .50, .75]]\n    df_agg = df_agg.drop(columns = [var]).drop_duplicates().reset_index().drop(columns = ['index'])\n    df_agg[var+'_q25'], df_agg[var+'_q50'], df_agg[var+'_q75'] = q25, q50, q75\n    return df_agg\n\nfeatures_agg = calc_min_max_avg(features, \n                                'Fuel_Price', \n                                'Date', \n                                ['Temperature', 'Store', 'MarkDown1', 'MarkDown2', \n                                     'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment'])\nfeatures_agg2 = calc_min_max_avg(features, \n                                'CPI', \n                                'Date', \n                                ['Temperature', 'Store', 'MarkDown1', 'MarkDown2', \n                                     'MarkDown3', 'MarkDown4', 'MarkDown5', 'Fuel_Price', 'Unemployment'])\nfeatures_agg3 = calc_min_max_avg(features, \n                                'Unemployment', \n                                'Date', \n                                ['Temperature', 'Store', 'MarkDown1', 'MarkDown2', \n                                     'MarkDown3', 'MarkDown4', 'MarkDown5', 'Fuel_Price', 'CPI'])\nsales_agg = calc_min_max_avg(sales, \n                             'Weekly_Sales', \n                             'Date', \n                             ['Store', 'Dept'])\n","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:00.779192Z","iopub.execute_input":"2021-07-01T20:37:00.779552Z","iopub.status.idle":"2021-07-01T20:37:01.307067Z","shell.execute_reply.started":"2021-07-01T20:37:00.779516Z","shell.execute_reply":"2021-07-01T20:37:01.306385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n    \ndef plot_min_max_avg (df, var, sortby='Date', title='', mode='all'):\n    \"\"\"\n    Using plotly, chart what was calculated with calc_min_max_avg\n    modes: all or mean (will only plot the mean)\n    \"\"\"\n    x = df[sortby]\n    series = list()\n    if mode == 'all':\n        series.append(go.Scatter(x=x, y=df[var+'_Min'], name = \"Min\", line = dict(color = 'green'), opacity = 0.4))\n        series.append(go.Scatter(x=x, y=df[var+'_q25'], name = \"q25\", line = dict(color = 'blue'), opacity = 0.4))\n        series.append(go.Scatter(x=x, y=df[var+'_q50'], name = \"q50\", line = dict(color = 'blue'), opacity = 0.4))\n        series.append(go.Scatter(x=x, y=df[var+'_q75'], name = \"q75\", line = dict(color = 'blue'), opacity = 0.4))\n        series.append(go.Scatter(x=x, y=df[var+'_Max'], name = \"Max\", line = dict(color = 'red'), opacity = 0.4))\n        layout = dict(title='Min, Mean, and Max of '+var+'{0}'.format(title))\n    elif mode == 'mean':\n        series.append(go.Scatter(x=x, y=df[var+'_q50'], name = \"q50\", line = dict(color = 'blue'), opacity = 0.4))\n        layout = dict(title='Mean of '+var+'{0}'.format(title))\n    fig = dict(data=series, layout=layout)\n    iplot(fig)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:01.307983Z","iopub.execute_input":"2021-07-01T20:37:01.308344Z","iopub.status.idle":"2021-07-01T20:37:01.366817Z","shell.execute_reply.started":"2021-07-01T20:37:01.308317Z","shell.execute_reply":"2021-07-01T20:37:01.366195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_min_max_avg (features_agg, 'Fuel_Price')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:01.367685Z","iopub.execute_input":"2021-07-01T20:37:01.36802Z","iopub.status.idle":"2021-07-01T20:37:02.369898Z","shell.execute_reply.started":"2021-07-01T20:37:01.367993Z","shell.execute_reply":"2021-07-01T20:37:02.368857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_min_max_avg (features_agg2, 'CPI')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:02.371407Z","iopub.execute_input":"2021-07-01T20:37:02.371771Z","iopub.status.idle":"2021-07-01T20:37:02.445061Z","shell.execute_reply.started":"2021-07-01T20:37:02.371729Z","shell.execute_reply":"2021-07-01T20:37:02.444012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_min_max_avg (features_agg3, 'Unemployment')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:02.446683Z","iopub.execute_input":"2021-07-01T20:37:02.447028Z","iopub.status.idle":"2021-07-01T20:37:02.521493Z","shell.execute_reply.started":"2021-07-01T20:37:02.446993Z","shell.execute_reply":"2021-07-01T20:37:02.520781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_min_max_avg (sales_agg, 'Weekly_Sales')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:02.522366Z","iopub.execute_input":"2021-07-01T20:37:02.522716Z","iopub.status.idle":"2021-07-01T20:37:02.58272Z","shell.execute_reply.started":"2021-07-01T20:37:02.52268Z","shell.execute_reply":"2021-07-01T20:37:02.581756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the charts we can see the following:\n\n1. The variations in fuel price are consistent across all store locations (while it remains possible that some locations are usually more expensive than others).\n\n2. There are few high performing stores, as the 75th quantile is very near the bottom of the second chart. This is consistent with the histogram and box plot with a few data points far to the right. \n\n3. There appears to be no relation betwen weekly sales and fuel prices.\n\n4. The CPI grows when fuel prices increase because, most likely, fuel price is an input to calculate CPI. The consumer price index, however, seems to have no impact on prices.\n\n5. There also seems to be no relation between unemployment and weekly sales.\n\n6. The high performing stores had sale peaks around Thanksgiving and Christmas 2010 and 2011, but not in 2012.","metadata":{}},{"cell_type":"markdown","source":"## 4. Key findings and insights\n\nTo sum up our previous findings:\n\n1. Stores are classified by size, with 'A' stores (22) generally being larger than 'B' stores (17), and these in turn being usually larger than 'C' stores (6). \n\n2. Some weeks in some stores were very successful (less than 1% of the week-store combinations), especially around some holidays.\n\n3. Sales seem not to depend on fuel price, unemployment, or consumer price index variations.","metadata":{}},{"cell_type":"markdown","source":"## 5. Hypotheses\n\nOur hypothesis were:\n\n1. Are sales larger or smaller in a given week if preceded by a holiday week? Some conclusions about this can be drawn from testing the second hypothesis, and testing it on its own is possibly less interesting.\n\n2. Are sales larger or smaller on normal weeks or holiday weeks? This is the most easily testable hypothesis.\n\n3. Are sales larger or smaller on weekends or on weekdays? The third one cannot be solved with the given data because it is aggregated by weeks.\n\nThe second hypothesis will be tested. We will first try a graphical approach, by simply aggregating the data points by the field that indicates which weeks contain a holiday.","metadata":{}},{"cell_type":"code","source":"sales_agg_holiday = sales_agg[sales_agg['IsHoliday'] == True]\nsales_agg_notholiday = sales_agg[sales_agg['IsHoliday'] == False]","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:02.583716Z","iopub.execute_input":"2021-07-01T20:37:02.583955Z","iopub.status.idle":"2021-07-01T20:37:02.590762Z","shell.execute_reply.started":"2021-07-01T20:37:02.58393Z","shell.execute_reply":"2021-07-01T20:37:02.589735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_min_max_avg (sales_agg_holiday, 'Weekly_Sales')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:02.593522Z","iopub.execute_input":"2021-07-01T20:37:02.593771Z","iopub.status.idle":"2021-07-01T20:37:02.635845Z","shell.execute_reply.started":"2021-07-01T20:37:02.593745Z","shell.execute_reply":"2021-07-01T20:37:02.634729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_min_max_avg (sales_agg_notholiday, 'Weekly_Sales')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:02.637739Z","iopub.execute_input":"2021-07-01T20:37:02.63801Z","iopub.status.idle":"2021-07-01T20:37:02.700178Z","shell.execute_reply.started":"2021-07-01T20:37:02.637983Z","shell.execute_reply":"2021-07-01T20:37:02.699281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are unfortunately only ten weeks marked as containing a holiday, so it will not be statistically possible to conduct an analysis in this manner. Even worse, there are two weeks right before Christmas which are not marked as 'containing a holiday' even though it is very likely that salees spikes have been caused precisely by being right before holidays.\n\nFor this reason, we will try another hypothesis: whether the mean sales of 'A' stores are larger than those of type 'C'.\n\nWe will begin by doing a 'histogram' similar to that in Section 2.3.","metadata":{}},{"cell_type":"code","source":"sales_stores","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:02.701192Z","iopub.execute_input":"2021-07-01T20:37:02.701444Z","iopub.status.idle":"2021-07-01T20:37:02.718425Z","shell.execute_reply.started":"2021-07-01T20:37:02.701419Z","shell.execute_reply":"2021-07-01T20:37:02.717333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:02.719648Z","iopub.execute_input":"2021-07-01T20:37:02.719919Z","iopub.status.idle":"2021-07-01T20:37:02.741117Z","shell.execute_reply.started":"2021-07-01T20:37:02.71989Z","shell.execute_reply":"2021-07-01T20:37:02.740252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales_agg = calc_min_max_avg(sales_stores, \n                             'Weekly_Sales', \n                             'Date', \n                             ['Store', 'Dept', 'Type', 'Size', 'IsHoliday'])\nsales_agg","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:02.742217Z","iopub.execute_input":"2021-07-01T20:37:02.742537Z","iopub.status.idle":"2021-07-01T20:37:03.19071Z","shell.execute_reply.started":"2021-07-01T20:37:02.742507Z","shell.execute_reply":"2021-07-01T20:37:03.189794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split, by store type, sales joined with store type\nletters = ['A', 'B', 'C']\nsales_stores_bytype = dict()\nfor l in letters:\n    t = sales_stores[sales_stores['Type'] == l]\n    t_agg = calc_min_max_avg(t, 'Weekly_Sales', 'Date', ['Store', 'Dept', 'Size', 'IsHoliday'])\n    sales_stores_bytype[l]= t_agg","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:03.191833Z","iopub.execute_input":"2021-07-01T20:37:03.192087Z","iopub.status.idle":"2021-07-01T20:37:03.75171Z","shell.execute_reply.started":"2021-07-01T20:37:03.192056Z","shell.execute_reply":"2021-07-01T20:37:03.750672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_min_max_avg (sales_stores_bytype['A'], 'Weekly_Sales', 'Date', ' (A Stores) ', 'mean')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:03.752872Z","iopub.execute_input":"2021-07-01T20:37:03.753167Z","iopub.status.idle":"2021-07-01T20:37:03.787492Z","shell.execute_reply.started":"2021-07-01T20:37:03.753138Z","shell.execute_reply":"2021-07-01T20:37:03.786443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_min_max_avg (sales_stores_bytype['B'], 'Weekly_Sales', 'Date', ' (B Stores)', 'mean')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:03.788858Z","iopub.execute_input":"2021-07-01T20:37:03.789237Z","iopub.status.idle":"2021-07-01T20:37:03.825245Z","shell.execute_reply.started":"2021-07-01T20:37:03.789197Z","shell.execute_reply":"2021-07-01T20:37:03.824573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_min_max_avg (sales_stores_bytype['C'], 'Weekly_Sales', 'Date', ' (C Stores)', 'mean')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:03.826105Z","iopub.execute_input":"2021-07-01T20:37:03.826373Z","iopub.status.idle":"2021-07-01T20:37:03.861039Z","shell.execute_reply.started":"2021-07-01T20:37:03.826345Z","shell.execute_reply":"2021-07-01T20:37:03.860394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The mean values of all three store types are different, so it should be possible to differentiate between them. However, as the histogram in Section 2.3 showed, these values have an exponential rather than a normal shape, so they should be transformed. We will apply a logarithmic transformation that attempts to prevent getting log(0).","metadata":{}},{"cell_type":"code","source":"# Choose a field\nsales_stores_transformed = sales_stores.copy()\n# Apply a log transformation (numpy syntax) to this column\nsales_stores_transformed['Weekly_Sales'] = sales_stores_transformed['Weekly_Sales'].apply(np.log1p)\n\nsales_stores_bytype_transformed = dict()\nfor l in letters:\n    t = sales_stores_transformed[sales_stores_transformed['Type'] == l]\n    t_agg = calc_min_max_avg(t, 'Weekly_Sales', 'Date', ['Store', 'Dept', 'Size', 'IsHoliday'])\n    sales_stores_bytype_transformed[l]= t_agg\n    display(sales_stores_bytype_transformed[l]) # We still get invalid values","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:03.861993Z","iopub.execute_input":"2021-07-01T20:37:03.862375Z","iopub.status.idle":"2021-07-01T20:37:04.490204Z","shell.execute_reply.started":"2021-07-01T20:37:03.862337Z","shell.execute_reply":"2021-07-01T20:37:04.489556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for l in letters:\n    sales_stores_bytype_transformed[l].replace([np.inf, -np.inf], np.nan, inplace=True)\n    sales_stores_bytype_transformed[l] = sales_stores_bytype_transformed[l].dropna(subset=['Weekly_Sales_Min'])","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:04.491113Z","iopub.execute_input":"2021-07-01T20:37:04.491464Z","iopub.status.idle":"2021-07-01T20:37:04.504866Z","shell.execute_reply.started":"2021-07-01T20:37:04.49143Z","shell.execute_reply":"2021-07-01T20:37:04.503923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_min_max_avg (sales_stores_bytype_transformed['A'], 'Weekly_Sales', 'Date', ' (A Stores) ')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:04.506012Z","iopub.execute_input":"2021-07-01T20:37:04.506311Z","iopub.status.idle":"2021-07-01T20:37:04.570963Z","shell.execute_reply.started":"2021-07-01T20:37:04.506276Z","shell.execute_reply":"2021-07-01T20:37:04.570036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_min_max_avg (sales_stores_bytype_transformed['B'], 'Weekly_Sales', 'Date', ' (B Stores) ')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:04.572179Z","iopub.execute_input":"2021-07-01T20:37:04.572684Z","iopub.status.idle":"2021-07-01T20:37:04.63229Z","shell.execute_reply.started":"2021-07-01T20:37:04.572644Z","shell.execute_reply":"2021-07-01T20:37:04.631424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_min_max_avg (sales_stores_bytype_transformed['C'], 'Weekly_Sales', 'Date', ' (C Stores) ')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:04.633527Z","iopub.execute_input":"2021-07-01T20:37:04.63401Z","iopub.status.idle":"2021-07-01T20:37:04.697427Z","shell.execute_reply.started":"2021-07-01T20:37:04.633971Z","shell.execute_reply":"2021-07-01T20:37:04.69655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have applied a logarithmic transformation and removed invalid values, the differences between store types are more visible. With histograms, the differences will be more visible.","metadata":{}},{"cell_type":"code","source":"sns.set(style='darkgrid')\n\nfig, ax1 = plt.subplots(ncols=1, sharey=False, figsize=(14,5))\nweekly_series = pd.DataFrame(sales_stores_bytype_transformed['A']['Weekly_Sales_q50'])\ng= sns.histplot(weekly_series, x='Weekly_Sales_q50', bins=50, ax=ax1).set(title='Sales by Week, A stores (histogram)')\n\nfig, ax2 = plt.subplots(ncols=1, sharey=False, figsize=(14,5))\nweekly_series = pd.DataFrame(sales_stores_bytype_transformed['B']['Weekly_Sales_q50'])\ng= sns.histplot(weekly_series, x='Weekly_Sales_q50', bins=50, ax=ax2).set(title='Sales by Week, B stores (histogram)')\n\nfig, ax3 = plt.subplots(ncols=1, sharey=False, figsize=(14,5))\nweekly_series = pd.DataFrame(sales_stores_bytype_transformed['C']['Weekly_Sales_q50'])\ng= sns.histplot(weekly_series, x='Weekly_Sales_q50', bins=50, ax=ax3).set(title='Sales by Week, C stores (histogram)')\n","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:04.698676Z","iopub.execute_input":"2021-07-01T20:37:04.69917Z","iopub.status.idle":"2021-07-01T20:37:05.444078Z","shell.execute_reply.started":"2021-07-01T20:37:04.699121Z","shell.execute_reply":"2021-07-01T20:37:05.443198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Statistical test for one of the hypotheses\n\nWe'll see if the store types are actually different populations.\n- Null Hypothesis. No significant difference in the means of the different store types.\n- Alternate Hypothesis. The classes are different.","metadata":{}},{"cell_type":"code","source":"from statsmodels.stats import weightstats as stests\n\nn=10\na = pd.DataFrame(sales_stores_bytype_transformed['A']['Weekly_Sales_q50']).sample(n=n, random_state=1)\nb = pd.DataFrame(sales_stores_bytype_transformed['B']['Weekly_Sales_q50']).sample(n=n, random_state=1)\nc = pd.DataFrame(sales_stores_bytype_transformed['C']['Weekly_Sales_q50']).sample(n=n, random_state=1)\n\nprint('\\nSample of weekly means in A:', len(a))\nprint(a.values.tolist())\nprint('\\nSample of weekly means in B:', len(b))\nprint(b.values.tolist())\nprint('\\nSample of weekly means in C:', len(c))\nprint(c.values.tolist())\n\nttest,pval = stats.ttest_rel(a, b)\nprint('\\npval for A and B:',pval)\nif pval < 0.05: print('Null hypothesis rejected')\nelse: print('Null hypothesis accepted')\n    \nttest,pval = stats.ttest_rel(a, c)\nprint('\\npval for A and B:',pval)\nif pval < 0.05: print('Null hypothesis rejected')\nelse: print('Null hypothesis accepted')\n    \nttest,pval = stats.ttest_rel(b, c)\nprint('\\npval for A and B:',pval)\nif pval < 0.05: print('Null hypothesis rejected')\nelse: print('Null hypothesis accepted')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:37:05.445348Z","iopub.execute_input":"2021-07-01T20:37:05.445699Z","iopub.status.idle":"2021-07-01T20:37:05.488525Z","shell.execute_reply.started":"2021-07-01T20:37:05.445659Z","shell.execute_reply":"2021-07-01T20:37:05.487519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now be sure that all three classes of stores are not the same population when compared by their mean weekly sales. ","metadata":{}},{"cell_type":"markdown","source":"## 7. Next steps\n\nOther analyses that could be performed with the same dataset:\n\n1. Finding which stores have negative sales and if this is a persistent phenomenon. If a store is unprofitable, it might need to be restructured, shrunk down, or even closed.\n\n2. Seeing if the stores with the largest sales are the ones with most departments, i.e., if all stores in the same type (A, B or C) have the same internal structure.\n\n3. Defining other classes by size. Possibly the A-B-C classification is not sufficient to explain the behavior of our stores.","metadata":{}},{"cell_type":"markdown","source":"## 8. Data quality\n\nThe dataset has in general a good quality, but the markdowns are not explained, and the cause of some weeks having negative sales is also unknown. \n\nTo conduct more useful business decisions we would need the following:\n\n1. A more detailed dataset, divided by day, weekday, and hour which would allow to make staffing and pricing decisions. For example, at the times when sales are higher we would need more employees.\n\n2. Data divided by department. This might allow to make decisions of which departments need more sales promotions or discounts when business is slow.\n\n3. More accurate categorization of holidays. It is also possible that stores in different cities or even countries have different holidays throughout the year.","metadata":{}}]}