{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <u> Death Prediction by Heart Failure</u>\n<u> By: Christopher Smith https://github.com/CWSmith022/Learning.git</u>\n\nThe published data is from: <u> Davide Chicco, Giuseppe Jurman: Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone. BMC Medical Informatics and Decision Making 20, 16 (2020). </u>\n\nThe .csv file was obtained from: https://www.kaggle.com/andrewmvd/heart-failure-clinical-data\n\n\nHeart disease is the leading cause of death for people in the United States. The development of models for prediction for potential of heart disease related death could be important for saving lives. Here, an approach using tools in the sci-kit learn library will be used for prediction of deaths by heart attacks. The process starts by feature selection with the KBestFunction by $Chi^{2}$ score, then the data is preprocessed to be used for several supervised Machine Learning Algorithms.\n\nThe Algorithms used are:\n\n- Logistic Regression\n- Support Vector Machines\n- K-Nearest Neighbors\n- Random Forest\n- Gradient Boosting\n- Ridge Classifier\n\n## <u> Logistic Regression </u> \nA model that is used statistically for binary dependent variables based on the probability of an event occuring. This can be further extended for several variables in a classification setting for multi-class prediction.\n    \n## <u> Support Vector Machines (SVM) </u>\nCommonly used for classification tasks, SVM's function by a Kernel which draws points on a hyperplane and uses a set of vectors to separate data points. This separation of data points creates a decision boundary for where a new data point can be predicted for a specific class label. \n\n## <u> K-Nearest Neighbors </u>\nSimply, an algorithm that clusters the data and by a measure of distance to the 'k' nearest points votes for a specific class prediction.\n\n## <u> Random Forest </u> \nAn ensemble method that estimates several weak decision trees and combines the mean to create an uncorrelated forest at the end. The uncorrelated forest should be able to predict more accurately than an individual tree.\n\n## <u> Gradient Boosting </u>\nSimilar to Random Forest, Gradient Boosting builds trees one at a time then ensembles them as each one is built.\n\n## <u> Ridge Classifier </u>\nNormalizes data then treats problem as a multi-output regression task.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Table of Contents\n\n[1.Importing Libraries](#1) <br/>\n[2.Importing Data](#2) <br/>\n[3.Exploring Data](#3) <br/>\n[4.Feature Selection](#3) <br/>\n[5.Splitting the Data](#4) <br/>\n[6.Feature Scaling (Normalization)](#5) <br/>\n[7.Machine Learning](#6) <br/>\n    [7.1.Logistic Regression](#7.1) <br/>\n    [7.2.Support Vector Machine](#7.2) <br/>\n    [7.3.K-Nearest Neighbor](#7.3) <br/>\n    [7.4.Random Forest](#7.4) <br/>\n    [7.5.Gradient Boosting](#7.5) <br/>\n    [7.6.Ridge Classifier](#7.6) <br/>\n[8.Evaluation of Acuracy](#9) <br/>\n[9.Discussion](#10) <br/>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n## Importing Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Simple Data processing\nimport numpy as np #linear algebra\nimport pandas as pd # data processing, .csv load\n\n#Feature Selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n#Data Visualization\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.ticker import NullFormatter\nimport matplotlib.ticker as ticker\nimport itertools #For Confusion Matrix\n%matplotlib inline\nimport seaborn as sns\n\n# Scaling\nfrom sklearn import preprocessing #For data normalization\n\n# Model Selection\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV # For parameterization and splitting data\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics # For Accuracy\n\n#Classification Algorithms\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import RidgeClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n# Importing Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heart=pd.read_csv('/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\nheart","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n# Exploring Data","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"heart.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"heart.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(heart.columns.unique)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a>\n# Feature Selection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Separating the data to asses with feature selection \nX_feat=heart[['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes',\n       'ejection_fraction', 'high_blood_pressure', 'platelets',\n       'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time']]\ny_feat=heart['DEATH_EVENT']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Selection\nbestfeatures = SelectKBest(score_func=chi2, k=5)\nfit = bestfeatures.fit(X_feat,y_feat)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X_feat.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Factors','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(5,'Score'))  #print 5 best features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By using KBest selection with the $Chi^{2}$ scorer that the top 5 Factors that could be related to 'DEATH_EVENT' are shown above and these will be used here on out for prediction of 'DEATH_EVENT'.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a>\n# Splitting The Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_accuracy= []\naccuracy_list = []\nalgorithm = []\n\nX_train,X_test,y_train,y_test = train_test_split(heart[['platelets','time','creatinine_phosphokinase','ejection_fraction','age']]\n                                                 ,heart['DEATH_EVENT'],test_size=0.2, random_state=0)\nprint(\"X_train shape :\",X_train.shape)\nprint(\"Y_train shape :\",y_train.shape)\nprint(\"X_test shape :\",X_test.shape)\nprint(\"Y_test shape :\",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a>\n# Feature Scaling (Normalization)\nTo remove outlier bias the formula $z=(x-u)/s$ is used first on the training set then applied to the testing set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler_ss=preprocessing.StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_scaled=scaler_ss.fit_transform(X_train)\nX_test_scaled=scaler_ss.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Confusion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion Matrix',\n                          cmap=plt.cm.BuGn):\n\n    print(cm)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a>\n# Machine Learning\n<b>Alive is representative of (0) while Death is (1)  </b>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7.1\"></a>\n## Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Log_Reg=LogisticRegression(C=1, class_weight='balanced', dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)\nLog_Reg.fit(X_train_scaled, y_train)\ny_reg=Log_Reg.predict(X_test_scaled)\nprint(\"Train Accuracy {0:.3f}\".format(Log_Reg.score(X_train_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_reg)))\ncm = metrics.confusion_matrix(y_test, y_reg)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Alive', 'Death'],\n                          title='Logistic Regression')\naccuracy_list.append(metrics.accuracy_score(y_test, y_reg)*100)\ntrain_accuracy.append(Log_Reg.score(X_train_scaled, y_train))\nalgorithm.append('Logistic Regression')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7.2\"></a>\n## Support Vector Machine\nBy using GRIDSearchCV the best kernel will be decided for the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SVC_param={'kernel':['sigmoid','rbf','poly'],'C':[1],'decision_function_shape':['ovr'],'random_state':[0]}\nSVC_pol=SVC()\nSVC_parm=GridSearchCV(SVC_pol, SVC_param, cv=5)\nSVC_parm.fit(X_train_scaled, y_train)\ny_pol=SVC_parm.predict(X_test_scaled)\nprint(\"The best parameters are \",SVC_parm.best_params_)\nprint(\"Train Accuracy {0:.3f}\".format(SVC_parm.score(X_train_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_pol)))\ncm = metrics.confusion_matrix(y_test, y_pol)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Alive', 'Death'],\n                          title='SVM')\ntrain_accuracy.append(SVC_parm.score(X_train_scaled, y_train))\naccuracy_list.append(metrics.accuracy_score(y_test, y_pol)*100)\nalgorithm.append('SVM')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7.3\"></a>\n## K-Nearest Neighbor\nFirst we need to select the best value of K for the highest accuracy in the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"error = []\n# Calculating error for K values between 1 and 40\nfor i in range(1, 40):\n    K_NN =KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                     metric_params=None, n_jobs=None, n_neighbors=i, p=2,\n                     weights='distance')\n    K_NN.fit(X_train_scaled, y_train)\n    pred_i = K_NN.predict(X_test_scaled)\n    error.append(np.mean(pred_i != y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nplt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n         markerfacecolor='blue', markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean Error')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the error helps decide the best K-Value given the parameters. The lower the error at K the better accuracy there will be.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"K_NN =KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                     metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n                     weights='distance')\nK_NN.fit(X_train_scaled, y_train)\ny_KNN=K_NN.predict(X_test_scaled)\nprint(\"Train Accuracy {0:.3f}\".format(K_NN.score(X_train_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_KNN)))\ncm = metrics.confusion_matrix(y_test, y_KNN)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Alive', 'Death'],\n                          title='KNN')\ntrain_accuracy.append(K_NN.score(X_train_scaled, y_train))\naccuracy_list.append(metrics.accuracy_score(y_test, y_KNN)*100)\nalgorithm.append('KNN')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7.4\"></a>\n## Random Forest\nBy using GRIDSearchCV the best parameters will be decided for the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"RFC_param={'max_depth':[1,2,3,4,5],'n_estimators':[10,25,50,100,150],'random_state':[None],\n           'criterion':['entropy','gini'],'max_features':[0.5]}\nRFC=RandomForestClassifier()\nRFC_parm=GridSearchCV(RFC, RFC_param, cv=5)\nRFC_parm.fit(X_train_scaled, y_train)\ny_RFC=RFC_parm.predict(X_test_scaled)\nprint(\"The best parameters are \",RFC_parm.best_params_)\nprint(\"Train Accuracy {0:.3f}\".format(RFC_parm.score(X_train_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_RFC)))\ncm = metrics.confusion_matrix(y_test, y_RFC)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Alive', 'Death'],\n                          title='RFC')\ntrain_accuracy.append(RFC_parm.score(X_train_scaled, y_train))\naccuracy_list.append(metrics.accuracy_score(y_test, y_RFC)*100)\nalgorithm.append('Random Forest')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7.5\"></a>\n## Gradient Boosting Classifier\nBy using GRIDSearchCV the best parameters will be decided for the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"GBC_parma={'loss':['deviance','exponential'],'n_estimators':[10,25,50,100,150],'learning_rate':[0.1,0.25, 0.5, 0.75],\n          'criterion':['friedman_mse'], 'max_features':[None],'max_depth':[1,2,3,4,5,10],'random_state':[None]}\nGBC = GradientBoostingClassifier()\nGBC_parm=GridSearchCV(GBC, GBC_parma, cv=5)\nGBC_parm.fit(X_train_scaled, y_train)\ny_GBC=GBC_parm.predict(X_test_scaled)\nprint(\"The best parameters are \",GBC_parm.best_params_)\nprint(\"Train Accuracy {0:.3f}\".format(GBC_parm.score(X_train_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_GBC)))\ncm = metrics.confusion_matrix(y_test, y_GBC)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Alive', 'Death'],\n                          title='GBC')\ntrain_accuracy.append(GBC_parm.score(X_train_scaled, y_train))\naccuracy_list.append(metrics.accuracy_score(y_test, y_GBC)*100)\nalgorithm.append('GBC')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7.6\"></a>\n## Ridge Classifier\nBy using GRIDSearchCV the best parameters will be decided for the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"RC_parma={'solver':['svd','lsqr','cholesky'],'alpha':[0,0.5,0.75,1,1.5,2],'normalize':[True,False]}\nRC=RidgeClassifier()\nRC_parm=GridSearchCV(RC, RC_parma, cv=5)\nRC_parm.fit(X_train_scaled, y_train)\ny_RC=RC_parm.predict(X_test_scaled)\nprint(\"The best parameters are \",RC_parm.best_params_)\nprint(\"Train Accuracy {0:.3f}\".format(RC_parm.score(X_train_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_RC)))\ncm = metrics.confusion_matrix(y_test, y_RC)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Alive', 'Death'],\n                          title='Ridge Classifier')\ntrain_accuracy.append(RC_parm.score(X_train_scaled, y_train))\naccuracy_list.append(metrics.accuracy_score(y_test, y_RC)*100)\nalgorithm.append('Ridge Classifier')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9\"></a>\n# Evaluation of Accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train Accuracy\nf,ax = plt.subplots(figsize = (10,5))\nsns.barplot(x=train_accuracy,y=algorithm,palette = sns.dark_palette(\"blue\",len(accuracy_list)))\nplt.xlabel(\"Accuracy\")\nplt.ylabel(\"Algorithm\")\nplt.title('Algorithm Train Accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Testing Accuracy\nf,ax = plt.subplots(figsize = (10,5))\nsns.barplot(x=accuracy_list,y=algorithm,palette = sns.dark_palette(\"blue\",len(accuracy_list)))\nplt.xlabel(\"Accuracy\")\nplt.ylabel(\"Algorithm\")\nplt.title('Algorithm Test Accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"10\"></a>\n# Discussion\n- Using the KBest approach with $Chi^{2}$ score can be an effective approach for feature selection.\n- However, other methods for this data set in feature selection should be suggested such as a correlation matrix or tree importance based selection method\n- Training accuracy does not mean the model will predict as well and models with lower training accuracy can predict better\n- Lastly, tree ensembles may be a better selection for this type of data set with the given features used.\n- If this notebook is helpful please provide an upvote!\n- Feed back is also really nice!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}