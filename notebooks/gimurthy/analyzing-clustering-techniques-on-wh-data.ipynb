{"metadata":{"language_info":{"nbconvert_exporter":"python","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat_minor":1,"cells":[{"metadata":{"_uuid":"8c020e7f3d476e2651a8a42e45b3f8ec3ae043ac","collapsed":true,"_cell_guid":"40b58dc9-815b-4a85-8538-2566d8212809"},"source":"# **1.0 Call libraries**","cell_type":"markdown"},{"metadata":{},"outputs":[],"source":"import numpy as np                                # Data manipulation\nimport pandas as pd                               # Dataframe manipulation \nimport matplotlib.pyplot as plt                   # For graphics\nimport seaborn as sns                             # For graphics\nfrom sklearn import cluster, mixture, metrics     # For clustering\nfrom sklearn.preprocessing import StandardScaler  # For scaling dataset\n\n# imports required for charting map\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n\nimport os          # For os related operations\n#import sys         # For data size\n#import time        # To time processes\nimport warnings    # To suppress warnings\n\n# Ignore warnings\nwarnings.filterwarnings(\"ignore\")\n\n# To show the graphs inline\n%matplotlib inline","cell_type":"code","execution_count":1},{"metadata":{},"source":"# **2. Read data**","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"source":"whr_data = pd.read_csv(\"../input/2017.csv\", header=0)","cell_type":"code","execution_count":3},{"metadata":{},"source":"# **3. Explore, Prepare, Clean and Scale data**","cell_type":"markdown"},{"metadata":{},"outputs":[],"source":"whr_data.head()","cell_type":"code","execution_count":6},{"metadata":{},"outputs":[],"source":"# 3.1 Prepare Dataset for clustering analysis.\n#   Removing the columns Country, Happiness.Rank from the analysis\n#   Removing Happiness.Score, Whisker.high, Whisker.low as well since they are \n#   deduced by the rest of the columns. \nwhr_data_for_clus = whr_data.iloc[ :, 5:]\nwhr_data_for_clus.head(3)","cell_type":"code","execution_count":7},{"metadata":{},"outputs":[],"source":"# 3.2 Center and scale the dataset\n#   Using StandardScaler function, which Standardize features by removing the \n#   mean and scaling to unit variance..\n# 3.2.2 Instantiate scaler object\nss = StandardScaler()\n# 3.2.3 Use ot now to 'fit' &  'transform'\nss.fit_transform(whr_data_for_clus)","cell_type":"code","execution_count":8},{"metadata":{},"source":"# **4 Define parameters and functions for later use**","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"source":"# 4.1.1 Define the constant parameters\nn_clusters = 2      # No of clusters. To use with techniques which needs this as input\nbandwidth = 0.1     # To use with Mean Shift technique\neps = 0.3           # To use with DbScan technique for incremental area density\ndamping = 0.9       # To use with Affinity Propogation technique\npreference = -200   # To use with Affinity Propogation technique \nmetric='euclidean'  # To use for silhouetter coefficient calculation to determing the right number of clusters\n\n# 4.1.2 Define the Clustering technique Data Frame to use in looping thru the various techniques\ncluster_dist = {'Technique' : ['K-means', 'Mean Shift', 'Mini Batch K-Means', 'Spectral', 'DBSCAN', 'Affinity Propagation', 'Birch', 'Gaussian Mixture Modeling'],\n                'FunctionName' : ['Kmeans_Technique', 'MeanShift_Technique', 'MiniKmean_Technique', 'Spectral_Technique', 'Dbscan_Technique', 'AffProp_Technique', 'Birch_Technique', 'Gmm_Technique']}\ncluster_df = pd.DataFrame(cluster_dist)\n\n\n# 4.2 Define the functions \n#   4.2.1 Define the function for clustering thru K-means\ndef Kmeans_Technique(ds):\n    \"\"\"\n    Ref: http://scikit-learn.org/stable/modules/clustering.html#k-means   \n    \"\"\"\n    km = cluster.KMeans(n_clusters=n_clusters)\n    return km.fit_predict(ds)\n\n#   4.2.2 Define the function for clustering thru Mean Shift\ndef MeanShift_Technique(ds):\n    \"\"\"\n    Ref: http://scikit-learn.org/stable/modules/clustering.html#mean-shift\n    \"\"\"\n    ms = cluster.MeanShift(bandwidth=bandwidth)\n    return ms.fit_predict(ds)\n\n#   4.2.3 Define the function for clustering thru Mini Batch K-Means\ndef MiniKmean_Technique(ds):\n    \"\"\"\n    Ref: http://scikit-learn.org/stable/modules/clustering.html#mini-batch-k-means\n    \"\"\"\n    mkm = cluster.MiniBatchKMeans(n_clusters=n_clusters)\n    return mkm.fit_predict(ds)\n\n#   4.2.4 Define the function for clustering thru Spectral\ndef Spectral_Technique(ds):\n    \"\"\"\n    Ref: http://scikit-learn.org/stable/modules/clustering.html#spectral-clustering\n    \"\"\"\n    spectral = cluster.SpectralClustering(n_clusters=n_clusters)\n    return spectral.fit_predict(ds)\n\n#   4.2.5 Define the function for clustering thru DBSCAN\ndef Dbscan_Technique(ds):\n    \"\"\"\n    Ref: http://scikit-learn.org/stable/modules/clustering.html#dbscan\n    \"\"\"\n    dbscan = cluster.DBSCAN(eps=eps)\n    return dbscan.fit_predict(ds)\n\n#   4.2.6 Define the function for clustering thru Affinity Propagation\ndef AffProp_Technique(ds):\n    \"\"\"\n    Ref: http://scikit-learn.org/stable/modules/clustering.html#affinity-propagation\n    \"\"\"\n    ap = cluster.AffinityPropagation(damping=damping, preference=preference)\n    return ap.fit_predict(ds)\n\n#   4.2.7 Define the function for clustering thru Birch\ndef Birch_Technique(ds):\n    \"\"\"\n    Ref: http://scikit-learn.org/stable/modules/clustering.html#birch\n    \"\"\"\n    birch = cluster.Birch(n_clusters=n_clusters)\n    return birch.fit_predict(ds)\n\n#   4.2.8 Define the function for clustering thru Gaussian Mixture modeling\ndef Gmm_Technique(ds):\n    \"\"\"\n    Ref: http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture\n    \"\"\"\n    gmm = mixture.GaussianMixture(n_components=n_clusters, covariance_type='full')\n    gmm.fit(ds)\n    return gmm.predict(ds)\n\n#   4.2.9 Function to evalauet the cluster performance using Silhouette Coefficient\n#   Closer the value to 1, the better is the clustering.\ndef GetSilhouetteCoeff (ds, result):\n    \"\"\"\n    Ref: http://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient\n    \n    Input - \n        - ds - The dataset for which the clustering was done\n        - result - The labels after the clustering\n    \"\"\"\n    return metrics.silhouette_score(ds, result, metric=metric)\n\n##  Unility method to extract the method name string after extracting it from DataFrame\n#   This is temp method, unless a more elegant or standard solution is found\n#   Need - Kmeans_Technique from DataFrame, instead getting\n#   this - 0    Kmeans_Technique\\nName: FunctionName, dtype: object    \ndef GetMethodName_Temp (method):\n    m = str(method)\n    m = m[1:m.index('\\n')]\n    return m.strip()","cell_type":"code","execution_count":9},{"metadata":{},"source":"# **5 Execute the various clusters & Evaluate Performanee**","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"source":"# Loop thru the various clustering techniques\nfor t in cluster_df.Technique:\n    # Get the name of the method associated with the given technique\n    method = cluster_df[cluster_df.Technique == t].FunctionName\n    m = GetMethodName_Temp(method)    \n    #print(\"Method\", m)\n    \n    # Invoke the method to get the label for each record\n    # Ref: https://stackoverflow.com/questions/3061/calling-a-function-of-a-module-from-a-string-with-the-functions-name\n    result = locals()[m](whr_data_for_clus)\n    \n    # Capture the output label for each of the record\n    whr_data[t] = pd.DataFrame(result)\n    # Execute & capture the Silhouette Coefficient for the each of the clustering response\n    if t != 'Affinity Propagation' :\n       cluster_df.loc[cluster_df.Technique == t, 'Silhouette.Coeff'] =  GetSilhouetteCoeff(whr_data_for_clus, result)\n\ncluster_df.loc[cluster_df.Technique == 'Affinity Propagation', 'SilCoeff'] = 0","cell_type":"code","execution_count":10},{"metadata":{},"source":"# **6 Present the cluster label and performance evaluation**","cell_type":"markdown"},{"metadata":{},"outputs":[],"source":"#   6.1 Cluster Analysis Data\nwhr_data.iloc[:,[0,12,13,14,15,16,17,18,19]]","cell_type":"code","execution_count":11},{"metadata":{},"outputs":[],"source":"#   6.2 Clustering Performance Evaluation\n#   Clustering seems to perform well when the number of clusters are 2 as per Silhouette Coefficient\n#   This is based on the results from testing with multiple cluster values = 2, 3, 5, 8, 10\n#   If time provides, will add the detailed table with the results from other cluster numbers\ncluster_df.iloc[:,[1, 2]]","cell_type":"code","execution_count":12},{"metadata":{},"source":"**Conclusion**: Performance metrics of all the techniques hover around '0' indicating that the clusters are overlapping. ","cell_type":"markdown"},{"metadata":{},"source":"# **7 Plot the Scatter plot for each of the clustering techniques**","cell_type":"markdown"},{"metadata":{},"outputs":[],"source":"rows = 4    # No of rows for the plot\ncols = 2    # No of columns for the plot\n\ncdf = cluster_df['Technique']\n\n# 4 X 2 plot\nfig,ax = plt.subplots(rows,cols, figsize=(15, 10)) \nx = 0\ny = 0\nfor i in cdf:\n   ax[x,y].scatter(whr_data.iloc[:, 6], whr_data.iloc[:, 5],  c=whr_data.iloc[:, 12+(x*y)])\n   # Set the title for each of the plot\n   ax[x,y].set_title(i + \" Cluster Result\")\n    \n   y = y + 1\n   if y == cols:\n       x = x + 1\n       y = 0\n\nplt.subplots_adjust(bottom=-0.5, top=1.5)\nplt.show()\nx = 0\ny = 0","cell_type":"code","execution_count":21},{"metadata":{},"source":"# **8 Plotting Maps for the Countrywise Global Score**","cell_type":"markdown"},{"metadata":{},"source":"***Global Happiness Index Ranking***","cell_type":"markdown"},{"metadata":{},"outputs":[],"source":"data = dict(type = 'choropleth', \n           locations = whr_data['Country'],\n           locationmode = 'country names',\n           z = whr_data['Happiness.Score'], \n           text = whr_data['Country'],\n           colorbar = {'title':'Happiness Score'})\nlayout = dict(title = 'Global Happiness Score', \n             geo = dict(showframe = False, \n                       projection = {'type': 'Mercator'}))\nchoromap3 = go.Figure(data = [data], layout=layout)\niplot(choromap3) ","cell_type":"code","execution_count":14},{"metadata":{},"source":"***Visualization of K-Mean Clustering Output***","cell_type":"markdown"},{"metadata":{},"outputs":[],"source":"data = dict(type = 'choropleth', \n           locations = whr_data['Country'],\n           locationmode = 'country names',\n           z = whr_data['K-means'], \n           text = whr_data['Country'],\n           colorbar = {'title':'Cluster Group'})\nlayout = dict(title = 'K-Means Clustering Visualization', \n             geo = dict(showframe = False, \n                       projection = {'type': 'Mercator'}))\nchoromap3 = go.Figure(data = [data], layout=layout)\niplot(choromap3)","cell_type":"code","execution_count":16},{"metadata":{},"source":"# **Thank You!!**","cell_type":"markdown"}],"nbformat":4}