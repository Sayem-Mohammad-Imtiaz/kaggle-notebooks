{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Import relevant Libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading in Dataset (downloaded from kaggle.com/andrewmvd)\n\ndataset = pd.read_csv('../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\n\n# View first 5 rows\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we begin understanding the data.\n# Using the info() method helps quantify null values and potentially unhelpful catagories.\ndataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the describe() method gives insight into the data's central tendencies.\ndataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split data into a group for numerical values and another group for categorical (in this case, binary 0 or 1) values.\nn_dataset = dataset[['age','creatinine_phosphokinase','ejection_fraction','platelets','serum_creatinine','serum_sodium','time']]\nc_dataset = dataset[['anaemia','diabetes','high_blood_pressure','sex','smoking','DEATH_EVENT']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize distributions for the Numerical Value dataset. \n# This helps us determine which distriutions are generally normalized, skewed, have outliers, etc.\nfor col in n_dataset.columns:\n    plt.hist(n_dataset[col])\n    plt.title(col)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Quantify correlations between the numerical values to understand their relationships.\n# This will help avoid multicolinearity if necessary.\n#print(n_dataset.corr())\n\n# Visualize correlations\nsns.heatmap(n_dataset.corr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comparing mortality rate for each numerical column\npd.pivot_table(dataset, index = 'DEATH_EVENT', values = n_dataset.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now visualize binary distributions for the Categorical Value dataset. \n\nfor col in c_dataset.columns:\n    sns.barplot(c_dataset[col].value_counts().index, c_dataset[col].value_counts()).set_title(col)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now compare mortality rate for each categorical column\nfor col in c_dataset:\n    if col != 'DEATH_EVENT':\n        print(pd.pivot_table(dataset, index = 'DEATH_EVENT', columns = col, values = 'age',aggfunc = 'count'))\n        print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing libraries for Standard scaling and splitting the data into train and test sets\n# Using Standardization and splitting the data\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nscaler = StandardScaler()\n#dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# With the chosen features, define the final dataset that will be split and used in the model\ndf_model = dataset[['DEATH_EVENT','age','anaemia','ejection_fraction','high_blood_pressure','time','serum_creatinine',]]\nX = df_model.drop('DEATH_EVENT', axis = 1)\ny = df_model.DEATH_EVENT.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state =0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Standardization\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First fit Logistic Regression model and show performance metrics\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\n\nlogreg.fit(X_train, y_train)\n\n# Creating a list to save each performance score for each model added\nacc_list = []\nrec_list = []\npre_list = []\nf1_list = []\n# Full lists will each contain [LogRegression, kNearestNeighbors, gNaiveBayes, Randomforest, DecisonTree, SVM]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating confusion matrix for better sense of the model's performance\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n\n# Define a function that prints each element in a given confusion matrix\ndef print_cm(cm):\n    print(\"True Negatives: \"+str(cm[0,0]))\n    print(\"True Positives: \"+str(cm[1,1]))\n    print(\"False Negatives: \"+str(cm[1,0]))\n    print(\"False Positives: \"+str(cm[0,1]))\n\npredictions = logreg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# determine and collect scores of Logistic Regression model\nacc = accuracy_score(y_test,predictions)\nrec = recall_score(y_test,predictions)\npre = precision_score(y_test,predictions)\nf1 = f1_score(y_test,predictions)\n\nprint_cm(confusion_matrix(y_test, predictions))\n\nacc_list.append(acc)\nrec_list.append(rec)\npre_list.append(pre)\nf1_list.append(f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now build a K Nearest Neighbors model\n# First loop to find the number of neighbors that returns the highest accuracy\nfrom sklearn.neighbors import KNeighborsClassifier\n\nn_list = []\nfor n in range(2,10):\n    knn = KNeighborsClassifier(n_neighbors = n)\n    knn.fit(X_train,y_train)\n    predictions = knn.predict(X_test)\n    n_list.append(accuracy_score(y_test,predictions))\nplt.plot(list(range(2,10)), n_list)\nplt.title(\"kNN Accuracy per Number of Neighbors\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since 5 neighbors yields the highest accuracy, use n = 5 in the model\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train,y_train)\npredictions = knn.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN confusion matrix \n\nacc = accuracy_score(y_test,predictions)\nrec = recall_score(y_test,predictions)\npre = precision_score(y_test,predictions)\nf1 = f1_score(y_test,predictions)\n\nprint(\"Accuracy Score: \" +str(acc))\nprint_cm(confusion_matrix(y_test, predictions))\n\nacc_list.append(acc)\nrec_list.append(rec)\npre_list.append(pre)\nf1_list.append(f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Naïve Bayes model\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train,y_train)\npredictions = gnb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# determine and collect scores of Gaussian Naïve Bayes model\nacc = accuracy_score(y_test,predictions)\nrec = recall_score(y_test,predictions)\npre = precision_score(y_test,predictions)\nf1 = f1_score(y_test,predictions)\n\nprint(\"Accuracy Score: \" +str(acc))\nprint_cm(confusion_matrix(y_test, predictions))\n\nacc_list.append(acc)\nrec_list.append(rec)\npre_list.append(pre)\nf1_list.append(f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find best number of nodes\n\nn_list = []\nfor n in range(2,10):\n    dtc = DecisionTreeClassifier(max_leaf_nodes = n, random_state=0, criterion='entropy')\n    dtc.fit(X_train, y_train)\n    predictions = dtc.predict(X_test)\n    n_list.append(accuracy_score(y_test,predictions))\n\nplt.title(\"DTC Accuracy per Number of nodes\")\nplt.plot(list(range(2,10)), n_list)\nplt.show()\n# Based on the plot, n=5 and n=6 yield the highest accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build Decision Tree Classifier\ndtc = DecisionTreeClassifier(max_leaf_nodes = 5, random_state=0, criterion='entropy')\ndtc.fit(X_train, y_train)\npredictions = dtc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine and collect scores of Decision Tree model\n\nacc = accuracy_score(y_test,predictions)\nrec = recall_score(y_test,predictions)\npre = precision_score(y_test,predictions)\nf1 = f1_score(y_test,predictions)\n\nprint(\"Accuracy Score: \" +str(acc))\nprint_cm(confusion_matrix(y_test, predictions))\n\nacc_list.append(acc)\nrec_list.append(rec)\npre_list.append(pre)\nf1_list.append(f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Model\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Find best number of estimators\nn_list = []\nfor n in range(10,100):\n    rfc = RandomForestClassifier(n_estimators = n, random_state=0)\n    rfc.fit(X_train, y_train)\n    predictions = rfc.predict(X_test)\n    n_list.append(accuracy_score(y_test,predictions))\n#print(mylist)\nplt.plot(list(range(10,100)), n_list)\nplt.title(\"Random Forest Classifier Accuracy per Number of Estimators\")\nplt.show()\n# Based on the plot, n=10 yields the highest accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build Random Forest Classifier model\nrfc = RandomForestClassifier(n_estimators = 10, random_state=0)\nrfc.fit(X_train, y_train)\npredictions = rfc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine and collect score of Random Forest model\n\nacc = accuracy_score(y_test,predictions)\nrec = recall_score(y_test,predictions)\npre = precision_score(y_test,predictions)\nf1 = f1_score(y_test,predictions)\n\nprint(\"Accuracy Score: \" +str(acc))\nprint_cm(confusion_matrix(y_test, predictions))\n\nacc_list.append(acc)\nrec_list.append(rec)\npre_list.append(pre)\nf1_list.append(f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Support Vector Machine\nfrom sklearn.svm import SVC\nsvmodel = SVC(random_state=0, kernel = 'rbf')\nsvmodel.fit(X_train, y_train)\npredictions = svmodel.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine and collect scores of Decision Tree model\n\nacc = accuracy_score(y_test,predictions)\nrec = recall_score(y_test,predictions)\npre = precision_score(y_test,predictions)\nf1 = f1_score(y_test,predictions)\n\nprint(\"Accuracy Score: \" +str(acc))\nprint_cm(confusion_matrix(y_test, predictions))\n\nacc_list.append(acc)\nrec_list.append(rec)\npre_list.append(pre)\nf1_list.append(f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Results\n\n# List of Classifier Models used \nmodels = ['Logistic Regression','K-NearestNeighbor','Naive Bayes','Decision Tree','Random Forest', 'Support Vector Machine']\n# List of metrics used\nmets = ['Accuracy', 'Recall','Precision','F1-Score']\ncolors = ['red','purple','blue','black']\n\n# combine into a python Dict\nd = {'Model':models, mets[0]:acc_list, mets[1]:rec_list, mets[2]:pre_list, mets[3]:f1_list}\n\n# create a pandas dataframe from Dict\nstat_df = pd.DataFrame(data=d)\nstat_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rearrange the dataframe for easier plotting\nstat_df = pd.melt(stat_df, id_vars=\"Model\", var_name=\"Metric\", value_name=\"Score\")\nstat_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Contruct bar plot to visualize each classifier's performance\nsns.catplot(x='Model', y='Score', hue='Metric', data=stat_df, kind='bar',palette=colors,height=5,aspect=3)\nplt.title(\"Performance Metrics by Classifier Model\")\nplt.ylabel(\"Score %\")\nplt.xlabel(\"Model\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# K-Nearest Neighbor has the highest Precision, but Recall and F1 have higher importance for this task.\n# Decision Tree and Random Forest tied for highest Accuracy (83%), but Decision Tree has a better Recall and F1 Score.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}