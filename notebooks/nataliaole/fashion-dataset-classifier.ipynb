{"cells":[{"metadata":{},"cell_type":"markdown","source":"\nImporting Basic libraries:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os \n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense\n\nnp.set_printoptions(precision=12, suppress=True, linewidth=150)\npd.options.display.float_format = '{:.6f}'.format\nsns.set()\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"loading data"},{"metadata":{"trusted":true},"cell_type":"code","source":"styles = pd.read_csv('../input/fashion-product-images-small/styles.csv',nrows=4000, error_bad_lines = False)\nstyles['image'] = styles.apply(lambda row: str(row['id']) + \".jpg\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Apparel = styles[styles['masterCategory'].isin(['Apparel'])]\nAccessories = styles[styles['masterCategory'].isin(['Accessories'])]\nFootwear =  styles[styles['masterCategory'].isin(['Footwear'])]\nPersonal_Care =  styles[styles['masterCategory'].isin(['Personal Care'])]\nFree_Items = styles[styles['masterCategory'].isin(['Free Items'])]\nSporting_Goods  =  styles[styles['masterCategory'].isin(['Sporting Goods'])]\n\n\n\nApparel, Accessories , Footwear, Personal_Care, Free_Items, Sporting_Goods= Apparel['id'].to_numpy(), Accessories['id'].to_numpy(), Footwear['id'].to_numpy(),Personal_Care['id'].to_numpy(), Free_Items['id'].to_numpy(), Sporting_Goods['id'].to_numpy()\nApparel.shape, Accessories.shape , Footwear.shape, Personal_Care.shape, Free_Items.shape, Sporting_Goods.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_path = '/kaggle/input/fashion-product-images-small/images/'\n\nIMG_SIZE = 224\nLIMIT_IMAGES = 2000\nNUM_OUTPUTS = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing import image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_imgs(names):\n    from tensorflow.keras.preprocessing import image\n    imgs = []\n    for i, image_name in enumerate(tqdm(names)):\n        try:\n            img = image.load_img(f'{image_path}{image_name}.jpg', target_size=(IMG_SIZE, IMG_SIZE))\n            print(f'{image_path}{image_name}.jpg')\n        except:\n            img = None\n            print(\"Error: Image not found!\")\n        if img is None:\n            continue\n        img = np.array(img)\n        imgs.append(img)\n    return np.array(imgs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Loading Images...\")\nprint(\"Apparel\")\nApparel_images = load_imgs(Apparel[:LIMIT_IMAGES])\ngc.collect()\nprint(\"Accessories\")\nAccessories_images = load_imgs(Accessories[:LIMIT_IMAGES])\ngc.collect()\nprint(\"Footwear\")\nFootwear_images = load_imgs(Footwear[:LIMIT_IMAGES])\ngc.collect()\nprint(\"Done\")\nApparel_images.shape, Accessories_images.shape, Footwear_images.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"View example Image..:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(Apparel_images[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nSet up training and validation generators:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\n#image generator object from keras. reference : Keras Docs\nimage_generator = ImageDataGenerator(\n    validation_split=0.2\n)\n\n#create a flow of images for training the model.\ntraining_generator = image_generator.flow_from_dataframe(\n    dataframe=styles,\n    directory= \"/kaggle/input/fashion-product-images-small/myntradataset/images\",\n    x_col=\"image\",\n    y_col=\"masterCategory\",\n    target_size=(80,60),\n    batch_size=256,\n    subset=\"training\"\n\n)\n\n#create a flow of images for validating(testing) the trained model.\nvalidation_generator = image_generator.flow_from_dataframe(\n    dataframe=styles,\n    directory=\"/kaggle/input/fashion-product-images-small/myntradataset/images\",\n    x_col=\"image\",\n    y_col=\"masterCategory\",\n    target_size=(80,60),\n    batch_size=256,\n    subset=\"validation\"\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compile Convolutional Neural Network "},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import layers,models\nmodel1 = Sequential()\n#model.add(Flatten(input_shape=(28, 28)))\nmodel1.add(layers.Conv2D(32, (4,4), strides = (2,2), activation = 'relu' , input_shape = (80,60,3)))\nmodel1.add(Flatten())\nmodel1.add(Dense(units=128, activation='relu'))\nmodel1.add(Dense(units=5, activation='softmax'))\n\nmodel1.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel1.summary()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train (CNN) Model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model1.fit(training_generator, epochs=10, validation_data=validation_generator, steps_per_epoch=10, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see metrics about model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics = pd.DataFrame(history.history)\nmetrics['epoch'] = history.epoch\nmetrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfig = make_subplots(rows=1, cols=2)\nfig.add_trace(go.Scatter(x=metrics['epoch'], y=metrics['accuracy'], name='accuracy'), row=1, col=1)\nfig.add_trace(go.Scatter(x=metrics['epoch'], y=metrics['loss'], name='loss'), row=1, col=2)\nfig.add_trace(go.Scatter(x=metrics['epoch'], y=metrics['val_accuracy'], name='val_accuracy'), row=1, col=1)\nfig.add_trace(go.Scatter(x=metrics['epoch'], y=metrics['val_loss'], name='val_loss'), row=1, col=2)\n\nfig.update_xaxes(title_text='epochs')\nfig.update_yaxes(title_text='accuracy')\nfig.update_layout(width=1000, title='Accuracy and Loss')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Small differences between accuracy and val_accuracy with accuracy >95% therefore the model looks relatively good."},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}