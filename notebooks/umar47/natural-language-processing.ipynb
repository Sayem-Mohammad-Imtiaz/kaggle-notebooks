{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nimport sklearn\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.preprocessing import OneHotEncoder\nimport os\nimport regex as re\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.backend import eval\nfrom keras.optimizers import Adam\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Dropout\nfrom keras.layers.convolutional import Conv1D,MaxPooling1D\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/nyc-jobs.csv')\npd.set_option('display.max_columns', None)\ndf=df.drop(['Job ID', 'Agency', 'Posting Type', '# Of Positions', 'Business Title',\n       'Civil Service Title', 'Title Code No', 'Level', 'Job Category',\n       'Full-Time/Part-Time indicator' , 'Additional Information', 'To Apply', 'Hours/Shift', 'Work Location 1',\n       'Recruitment Contact', 'Residency Requirement', 'Posting Date',\n       'Post Until', 'Posting Updated', 'Process Date'], axis=1)\ndf=df.reset_index(drop=True)\nprint(df.columns, df.tail(7))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"887c55e1e41ea341c716e20a0d192fe49df11cf2"},"cell_type":"code","source":"print(df.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90dd762cee389f2cd0f3cfb0e858f207c1ee7c32"},"cell_type":"code","source":"#df['Preferred Skills'].dropna(inplace=True)\n#print(len(df['Preferred Skills']))\n#df[['Preferred Skills'].fillna('Unspecified', inplace=True)\nX=df['Job Description']\nohe=OneHotEncoder()\ny=df[['Salary Range From']].astype('str')\nprint(y.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"567fe5dc6c98a6c0c966db9d43b93852e07131cf"},"cell_type":"code","source":"import string\nfrom nltk.corpus import stopwords\nstop=set(stopwords.words('english'))\ndef object_to_list(text):\n    '''\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Return the cleaned text as a list of words\n    '''\n    nopunc = [char for char in text if char not in string.punctuation]#removing puctuations\n    nopunc = ''.join(nopunc)\n    \n    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]#removing non english words\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27562d722b590cbfabc563bc6f88af732383db25"},"cell_type":"markdown","source":"To vectorize our string input we will use sklearn CounterVectorizer. This algorith makes object data like a vector which then help as for applying machine learning algorith on it. for more information about CounterVectorizer visit:   http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"},{"metadata":{"trusted":true,"_uuid":"6e2d5a56af3e1a6cf57c4ad49c9443c2c4bcffb2"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n# next we need to vectorize our input variable (X)\n#we use the count vectoriser function and the analyser we use is the above lines of code\n# this should return a vector array\nX = CountVectorizer(analyzer=object_to_list).fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8eccec6b54a51f663c705284998e1c10e8580b0"},"cell_type":"code","source":"print(X[6].split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75d601d0cc543173c140b09e5b9620fc95e07742"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#from sklearn.preprocessing import OneHotEncoder\n#ohe=OneHotEncoder()\n#ohe.fit_transform(X)\nx_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd2e932a9ef8488780487fe7ef1024759a3a8408"},"cell_type":"code","source":"from sklearn.ensemble  import RandomForestClassifier\nfirst_model=RandomForestClassifier()\nfirst_model.fit(x_train, y_train)\nprint(first_model.score(x_train, y_train))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"233a32508af62042c0bfd90069edcc4b9ae35371"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\npredicted=first_model.predict(x_test)\nprint(confusion_matrix(y_test, predicted))\nprint('\\n')\nprint(classification_report(y_test, predicted))#we see the precision, recall, f1-score and supprt for \n# predicted values here","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea658cb26b79b3d37eb0fc3dc18bfd24d0f9bdef"},"cell_type":"code","source":"def clean_document(doco):\n    punctuation = string.punctuation\n    punc_replace = ''.join([' ' for s in punctuation])\n    doco_link_clean = re.sub(r'http\\S+', '', doco)\n    doco_clean_and = re.sub(r'&\\S+', '', doco_link_clean)\n    doco_clean_at = re.sub(r'@\\S+', '', doco_clean_and)\n    doco_clean = doco_clean_at.replace('-', ' ')\n    doco_alphas = re.sub(r'\\W +', ' ', doco_clean)\n    trans_table = str.maketrans(punctuation, punc_replace)\n    doco_clean = ' '.join([word.translate(trans_table) for word in doco_alphas.split(' ')])\n    doco_clean = doco_clean.split(' ')\n    p = re.compile(r'\\s*\\b(?=[a-z\\d]*([a-z\\d])\\1{3}|\\d+\\b)[a-z\\d]+', re.IGNORECASE)\n    doco_clean = ([p.sub(\"\", x).strip() for x in doco_clean])\n    doco_clean = [word.lower() for word in doco_clean if len(word) > 2]\n    doco_clean = ([i for i in doco_clean if i not in stop])\n#     doco_clean = [spell(word) for word in doco_clean]\n#     p = re.compile(r'\\s*\\b(?=[a-z\\d]*([a-z\\d])\\1{3}|\\d+\\b)[a-z\\d]+', re.IGNORECASE)\n    doco_clean = ([p.sub(\"\", x).strip() for x in doco_clean])\n#     doco_clean = ([spell(k) for k in doco_clean])\n    return doco_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c12cc8880968b5ba345ed41924f96bf5db0fa42"},"cell_type":"code","source":"reviews=df['Job Description']\nreview_cleans = [clean_document(doc) for doc in reviews];\nsentences = [' '.join(r) for r in review_cleans ]\nprint(reviews.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6b567062e21a0da1b37fc9b14eb39711713cfc8"},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(sentences)\ntext_sequences = np.array(tokenizer.texts_to_sequences(sentences))\nsequence_dict = tokenizer.word_index\nword_dict = dict((num, val) for (val, num) in sequence_dict.items())\nprint(text_sequences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e157f979eca1f4d783da4175a142ace227cfbbfb"},"cell_type":"code","source":"#print(sequence_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c18ef757d8c663c0cb5c57dba6071fde6a2e0bc3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a94fdf8f2433fd96ba1f08ba0716f0eefda4532d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}