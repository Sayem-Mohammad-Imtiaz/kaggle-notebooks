{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Motivation\nThis notebook gives ideas about different types of filtering that can be applied to the vast amount of sources that the dataset provides.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Filtering","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\npd.set_option('max_colwidth', None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Metadata-only","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The first big filter is to consider only texts for which there is metadata information.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = '/kaggle/input/hackathon'\nfiles = [f'{path}/task_1-google_search_english_original_metadata.csv',\n         f'{path}/task_1-google_search_translated_to_english_metadata.csv']\n\ndfs = []\nfor file in files:\n    df = pd.read_csv(file, encoding = \"ISO-8859-1\")\n    dfs.append(df)\n    \ndf = pd.concat(dfs, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"Considering only {df.shape[0]} sources\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop some redundant columns:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Is Processed', 'Comments', 'language', 'query'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fix alpha_2_code NaN values for Namibia:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['alpha_2_code'].isna()].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert all(df[df['alpha_2_code'].isna()]['country']=='Namibia')\ndf['alpha_2_code'].fillna('NA', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove empty documents","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(df[df['is_downloaded']==False].index, inplace=True)\ndf['char_number'] = pd.to_numeric(df['char_number'], errors='coerce')\ndf.drop(df[df['char_number']==0].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"Considering only {df.shape[0]} sources\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove duplicated urls","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop_duplicates('url', keep=False, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"Considering only {df.shape[0]} sources\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analyse char number","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['char_number'].value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"80 articles having exactly 895 number of characters is suspicious. Let's see which are the corresponding urls.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['char_number']==895].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['char_number']==895]['url'].str.contains('researchgate.net').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It turns out most of those are from the website *researchgate.net*. Let's see what is the content of one of the files.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"row = df[df['char_number']==895].iloc[0]\ncode = row['alpha_2_code']\nfilename=row['filename']\nfilename = f'/kaggle/input/hackathon/task_1-google_search_txt_files_v2/{code}/{filename}.txt'\n\nwith open(filename, 'r') as file:\n    data = file.read()\n\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This looks like a predefined message that is not useful for us, thus we can ignore all sources coming from *researchgate.net*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(df[(df['url'].str.contains('researchgate.net')) & (df['char_number']==895)].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"Considering only {df.shape[0]} sources\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analyse url domains","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from urllib.parse import urlparse\ndf['url_domain'] = df['url'].apply(lambda x: urlparse(x).netloc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['url_domain'].value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The domain with most entries in the filtered dataset is *www.ncbi.nlm.nih.gov*. This is the website of National Center for Biotechnology Information which provides access to biomedical and genomic information. It's a government site, which suggests that the information on it is reliable. Let's apply another filter and look only at the data from this website.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###Â NCBI sources only","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[df['url_domain']=='www.ncbi.nlm.nih.gov']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"Considering only {df.shape[0]} sources\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's enrich the metadata information for these sources by extracting the title for each article.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install pandarallel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import bs4 as bs\nimport urllib.request\nfrom pandarallel import pandarallel\npandarallel.initialize(progress_bar=True)\n\ndef get_url_title(url):\n    try:\n        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n        source = urllib.request.urlopen(req).read()\n        soup = bs.BeautifulSoup(source,'lxml')\n        if not soup.title:\n            print('No title')\n            print(url)\n            return \"\"\n        return soup.title.text\n    except urllib.error.HTTPError as e:\n        print(e)\n        print(url)\n        return \"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['url_title'] = df['url'].parallel_apply(get_url_title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['country', 'url_title']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking at the titles, we can see that some of the articles are about more than one country, i.e for 'European countries'. This might cause problems, because when extracting the answer for a question we would not know for which country it applies. Let's filter such articles by checking whether the title contains the name of the country.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['title_has_country'] = df.apply(lambda row: row['country'] in row['url_title'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['title_has_country'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's good news that most of the articles are about a single country.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(df[df['title_has_country'] == False].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"Considering only {df.shape[0]} sources\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove articles with the same title","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['url_title'].value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['url_title']=='The Current Status of BCG Vaccination in Young Children in South Korea']['url']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some articles have duplicated titles, because the source url is almost exactly the same, except for a different section tag at the end. This means we can remove those duplicates.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop_duplicates('url_title', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"Considering only {df.shape[0]} sources\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['country'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['char_number'].plot.box()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we end up with just 85 sources for 73 countries with an average of about 20K characters per article.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}