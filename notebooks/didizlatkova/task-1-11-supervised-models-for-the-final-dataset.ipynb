{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Motivation\nSo far using the [Task 1: Label extraction for all 11 questions](https://www.kaggle.com/didizlatkova/task-1-label-extraction-for-all-11-questions) notebook we have sets of snippets that answer each of the 11 questions. In this final notebook we'll expand the BCG atlas data by:\n* Creating 11 supervised datasets by extracting negative examples from manually reviewed data (similarly to how we did it in [Task 1: Supervised dataset from manually reviewed](https://www.kaggle.com/didizlatkova/task-1-supervised-dataset-from-manually-reviewed))\n* Training 11 Logistic Regression models to predict whether a snippet answers each of the 11 questions in the task\n* Filtering the huge number of data sources available\n* Splitting the texts from the filtered data sources into snippets\n* Predicting the probability that each snippet answers each question using the trained models\n* Selecting only the top 2 sources per country that answer the most questions with biggest confidence","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Supervised datasets","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\npd.set_option('max_colwidth', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question_names = ['first_year','last_year','is_mandatory','timing','strain','has_revaccinations','revaccination_timing','location', 'manufacturer', 'supplier', 'groups']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_text(row):\n    code = row['alpha_2_code']\n    filename=row['filename'].replace('.txt', '')\n    filename = f'/kaggle/input/hackathon/task_1-google_search_txt_files_v2/{code}/{filename}.txt'\n    \n    with open(filename, 'r') as file:\n        data = file.read()#.replace('\\n', ' ')\n    return data\n\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\ndef get_snippets(text):\n    '''\n        Returns sentences in the text which contain more than 5 tokens and at least one verb.\n    '''\n    return [sent.text.strip() for sent in nlp(text).sents \n                 if len(sent.text.strip()) < 350 and len(sent.text.strip().split()) > 5 and any([token.pos_ == 'VERB' for token in sent])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport tqdm\n\ndef get_negative_examples(df, q):\n    df = df[df[q].notna()][['alpha_2_code','country','url', 'filename',q]]\n    negative_examples = []\n\n    for _, row in df.iterrows():\n        text = read_text(row)\n        snippets = get_snippets(text)\n\n        tfidf_vectorizer = TfidfVectorizer()\n        tfidf_matrix = tfidf_vectorizer.fit_transform(snippets)\n\n        sim = cosine_similarity(tfidf_vectorizer.transform([row[q]]),tfidf_matrix)\n        res = pd.DataFrame()\n        res['snippet'] = snippets\n        res['sim'] = sim[0]\n        low_sim = res[res['sim']<0.1]['snippet'].values\n        negative_examples.extend(low_sim)\n    return negative_examples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '/kaggle/input/bcg-manually-reviewed-cleaned'\nfile = f'{path}/manually_reviewed_cleaned.csv'\ndf_man = pd.read_csv(file, encoding = \"ISO-8859-1\")\n\ndf_man.columns = ['alpha_2_code', 'country', 'url', 'filename', 'is_pdf','Comments',\n              'Snippet'] + question_names + ['snippet_len', 'text_len']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we combine our positively labeled data with the negative examples we extract from the manually reviewed dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets = []\nfor q in question_names:\n    print(q)\n    file = f'{path}/{q}_labeled.csv'\n    df_labeled = pd.read_csv(file, encoding = \"ISO-8859-1\")\n    neg = get_negative_examples(df_man, q)\n    \n    df_data = pd.DataFrame({'snippet': df_labeled['sentence'], 'label': df_labeled['label']})\n    df_data = df_data.append(pd.DataFrame({'snippet': neg, 'label': 0}), ignore_index=True)\n    \n    print(df_data.shape)\n    display(df_data['label'].value_counts(normalize=True))\n    print()\n    \n    datasets.append(df_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that for some questions the resulting dataset is bigger (i.e first_year, timing with over 1K examples), but for others like revaccination_timing we have only 30 examples.\n\nThe class imbalance is pretty much the same between the 11 datasets - around 2% positive examples.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Evaluate models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We do a cross validation using the whole training sets and compare the results to a 'Dummy' majority class baseline.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\n\n\ndef eval_model(X, y):\n    clfs = [('Dummy', DummyClassifier(strategy='prior')),\n            ('LogReg', LogisticRegression(random_state=0, solver='lbfgs', class_weight='balanced')),]\n    \n    for name, clf in clfs:\n        pipeline = Pipeline([\n        ('tfidf', TfidfVectorizer()),\n        ('clf', clf),\n        ])\n    \n        scores = cross_validate(pipeline, X, y, cv=4, scoring=('accuracy', 'f1', 'roc_auc'), return_train_score=True)\n\n        print(\"{:10s} {:5s} | Train: {:.3f}, Test: {:.3f}\".format(name, 'ACC', np.mean(scores['train_accuracy']), np.mean(scores['test_accuracy'])))\n\n        print(\"{:10s} {:5s} | Train: {:.3f}, Test: {:.3f}\".format(name, 'F1', np.mean(scores['train_f1']), np.mean(scores['test_f1'])))\n\n        print(\"{:10s} {:5s} | Train: {:.3f}, Test: {:.3f}\".format(name, 'AUC', np.mean(scores['train_roc_auc']), np.mean(scores['test_roc_auc'])))\n    \n    \ndef get_model(X, y):\n    pipeline = Pipeline([\n    ('tfidf', TfidfVectorizer()),\n    ('clf', LogisticRegression(random_state=0, solver='lbfgs', class_weight='balanced')),\n    ])\n    \n    return pipeline.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for q, dataset in zip(question_names, datasets):\n    print(q)\n    eval_model(dataset['snippet'], dataset['label'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The evaluation shows that on 6 out of the 11 questions (**last_year**, **is_mandatory**, **strain**, **revaccination_timing**, **location** and **manufacturer**) the model beats the baseline on all 3 metrics","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Train 11 models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [get_model(d['snippet'], d['label']) for d in datasets]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict with models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Load data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '/kaggle/input/hackathon'\nfiles = [f'{path}/task_1-google_search_english_original_metadata.csv',\n         f'{path}/task_1-google_search_translated_to_english_metadata.csv']\ndfs = []\nfor file in files:\n    df = pd.read_csv(file, encoding = \"ISO-8859-1\")\n    dfs.append(df)\ndf = pd.concat(dfs, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Filter sources","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Is Processed', 'Comments', 'language', 'query'], axis=1, inplace=True)\ndf.drop(df[df['is_downloaded']==False].index, inplace=True)\ndf['char_number'] = pd.to_numeric(df['char_number'], errors='coerce')\ndf.drop(df[df['char_number']==0].index, inplace=True)\ndf.drop_duplicates('url', keep=False, inplace=True)\ndf.drop(df[df['url'].str.contains('researchgate.net')].index, inplace=True)\ndf.drop(df[df['is_pdf']].index, inplace=True)\nassert all(df[df['alpha_2_code'].isna()]['country']=='Namibia')\ndf['alpha_2_code'].fillna('NA', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from urllib.parse import urlparse\ndf['url_domain'] = df['url'].apply(lambda x: urlparse(x).netloc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we decided to filter sources so that they include two specific websites, which we manually inspected and contain credible information - www.ncbi.nlm.nih.gov and www.sciencedirect.com.\nThe rest of the sources we're filtering based on the url itself - it has to contain at least one of the substrings *'vaccin'* or *'bcg'*.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_filtered = df[(df['url'].str.contains('vaccin')) |\n                (df['url'].str.contains('bcg')) |\n                 (df['url_domain']=='www.sciencedirect.com') |\n                 (df['url_domain']=='www.ncbi.nlm.nih.gov')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"Working with {df_filtered.shape[0]} sources\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install pandarallel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import bs4 as bs\nimport urllib.request\nfrom pandarallel import pandarallel\npandarallel.initialize(progress_bar=True)\n\ndef get_url_title(url):\n    try:\n        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n        source = urllib.request.urlopen(req).read()\n        soup = bs.BeautifulSoup(source,'lxml')\n        if not soup.title:\n            print('No title')\n            print(url)\n            return \"\"\n        return soup.title.text\n    except Exception as e:\n        print(e)\n        print(url)\n        return \"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_filtered['url_title'] = df_filtered['url'].parallel_apply(get_url_title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We apply one additional filtering at article level - the title has to contain the name of the respective country.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.mode.chained_assignment = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_filtered['title_has_country'] = df_filtered.apply(lambda row: row['country'] in row['url_title'], axis=1)\ndf_filtered.drop(df_filtered[df_filtered['title_has_country'] == False].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_filtered.drop_duplicates('url_title', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"Working with {df_filtered.shape[0]} sources\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tqdm\n\ndfs = []\nfor _, row in tqdm.tqdm(df_filtered.iterrows()):\n    data = read_text(row)\n    \n    snippets = get_snippets(data)\n    \n    result = pd.DataFrame()\n    result['sentence'] = snippets\n    result['len'] = result['sentence'].apply(len)\n    result['country'] = row['country']\n    result['url'] = row['url']\n    result['filename'] = row['filename']\n    result['alpha_2_code'] = row['alpha_2_code']\n    \n    for q, model in zip(question_names, models):\n        result[q] = model.predict_proba(snippets)[:,1]\n    \n    result = result.replace(0, np.nan)\n    result = result.dropna(how='all', axis=0, subset=question_names)\n    \n    dfs.append(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pd.concat(dfs, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"Considering {result.shape[0]} snippets\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.groupby('country')['url'].unique().apply(len).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see that for most countries - 73, there is only a single source of data we consider. For 48 countries there are 2 sources. For the rest of the countries with more than 2 sources we will select only 2 based on a source score calculated below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_source_score(row, min_score=0.65):\n    res = {'score': sum([row[q].max() for q in question_names if row[q].max() > min_score])}\n    for q in question_names:\n        if row[q].max() > min_score:\n            res[q] = row.loc[row[q].idxmax()]['sentence']\n            res[f'{q}_score'] = row[q].max()\n        else:\n            res[q] = np.nan\n            res[f'{q}_score'] = np.nan\n    return res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We ignore all answers with probability from the model < 0.65. Using the rest, the score for a souce is the sum of the probabilities of the most confident answers for all 11 questions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_result = result.groupby(['country', 'alpha_2_code', 'url', 'filename']).apply(lambda x: pd.Series(get_source_score(x))).sort_values('score', ascending=False).groupby(['country']).head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_result = final_result.replace(0, np.nan)\nfinal_result.dropna(how='all', inplace=True)\nfinal_result.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"Final result has {final_result.shape[0]} different sources\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_result.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_result['country'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extracted answers per question:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"total_n = 0\nfor q in question_names:\n    n = final_result[final_result[q].notna()].shape[0]\n    total_n += n\n    print(f'{q}: {n}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"Total number of extracted answers: {total_n}\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extend BCG atlas data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '/kaggle/input/hackathon'\nfile = f'{path}/BCG_world_atlas_data-2020.csv'\ndf_atlas = pd.read_csv(file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_result.insert(3, 'atlas', 'no')\nfinal_result['comments'] = ''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = [c for c in final_result.columns if 'score' not in c]\nassert len(cols) == len(df_atlas.columns)\nres_to_append = final_result[cols]\nres_to_append.columns = df_atlas.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atlas_ext = df_atlas.append(res_to_append.fillna(''), ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atlas_ext.sort_values('Contry Name (Mandatory field)', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"Original dataset has {df_atlas.shape[0]} entries\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"Extended dataset has {df_atlas_ext.shape[0]} entries\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atlas_ext.to_csv(f'/kaggle/working/df_atlas_extended.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the end, we've managed to extend the BCG Atlas dataset with 168 answers from 93 different sources for 75 countries.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}