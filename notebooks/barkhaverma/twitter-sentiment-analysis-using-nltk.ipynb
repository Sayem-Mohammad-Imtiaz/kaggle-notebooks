{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Sentiment Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Sentiment Analysis means analyzing the sentiment of a given text or document and categorizing the text/document into a specific class or category (like positive and negative). In other words, we can say that sentiment analysis classifies any particular text or document as positive or negative. Basically, the classification is done for two classes: positive and negative.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Introduction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this notebook I will show you how to write a python program that predicts that analyze the sentiment of test using a machine learning technique called The Natural Language Toolkit(NLTK).I have drawn word cloud of the text,i have build naive bayes classification for to measure the score.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Importing libraries","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# here i am importing important libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use(\"fivethirtyeight\")\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.classify import SklearnClassifier\nfrom sklearn.model_selection import train_test_split\nfrom textblob import TextBlob\nfrom wordcloud import WordCloud,STOPWORDS\nfrom subprocess import check_output\nimport re","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# here i am reading dataset\ndata = pd.read_csv(\"../input/first-gop-debate-twitter-sentiment/Sentiment.csv\")\n# here i am printing fisrt five line of dataset\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here i am priting shape of dataset\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here i have decided to use only sentiment and text columns for doing sentiment analysis\ndata = data[[\"text\",\"sentiment\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here i am printing first five line of my dataset\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here i am cleaning text column\ndef cleantxt(text):\n    text= re.sub(r'@[A-Za-z0-9]+', '',text)# removed @mentions\n    text= re.sub(r'#', '',text)# removed # symbol\n    text = re.sub(r'RT[\\s]+', '',text)# rmoved RT\n    text = re.sub(r'https?:\\/\\/\\s+', '',text)# removed the hyperlink\n    text = re.sub(r':+', '',text)# removed : symbol\n    text = re.sub(r'--+', '',text)# removed : symbol\n    text = re.sub(r'http', '',text)\n    return text\ndata[\"text\"] = data[\"text\"].apply(cleantxt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we are printing the first five line of cleaned data\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now i am adding two more columns in dataset that is subjectivity and polarity\nsubjectivity:Subjective sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information. Subjectivity is also a float which lies in the range of [0,1]\npolarity: Polarity is float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# here i am creating function to get subjectivity\ndef getSubjectivity(text):\n    return TextBlob(text).sentiment.subjectivity\n# here i am creating function to get polarity\ndef getPolarity(text):\n    return TextBlob(text).sentiment.polarity\n# here i am creating two new column of subjectivity and polarity\ndata[\"subjectivity\"] = data[\"text\"].apply(getSubjectivity)\ndata[\"polarity\"] = data[\"text\"].apply(getPolarity)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we are printing first five line of data after adding two new columns\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First of all, splitting the dataset into a training and a testing set. The test set is the 10% of the original dataset. For this particular analysis I dropped the neutral tweets, as my goal was to only differentiate positive and negative tweets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# here i am spliting dataset in train and test data\ntrain,test = train_test_split(data,test_size=0.1)\n# here i am removing neutral text\ntrain = train[train.sentiment != \"Neutral\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a next step I separated the Positive and Negative tweets of the training set in order to easily visualize their contained words. After that I cleaned the text from hashtags, mentions and links. Now they were ready for a WordCloud visualization which shows only the most emphatic words of the Positive and Negative tweets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# here i am training positive text\ntrain_pos = train[train[\"sentiment\"]==\"positive\"]\ntrain_pos = train_pos[\"text\"]\n# here i am training neagative text\ntrain_neg = train[train[\"sentiment\"]==\"negative\"]\ntrain_neg = train_neg[\"text\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here i am doing WordCloud visualization\nallwords = ' '.join([twts for twts in data[\"text\"]])\nwordcloud = WordCloud(width=2500,\n                      height=2000,stopwords=STOPWORDS,background_color=\"white\",random_state=21\n                     ).generate(allwords)\nplt.figure(1,figsize=(10,10))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here i am removing the hashtags, mentions, links and stopwords from the\n#training set after doing visualisation\ntext = []\nstopwords_set = set(stopwords.words(\"english\"))\n\nfor index, row in train.iterrows():\n    words_filtered = [e.lower() for e in row.text.split() if len(e) >= 3]\n    words_cleaned = [word for word in words_filtered\n        if 'http' not in word\n        and not word.startswith('@')\n        and not word.startswith('#')\n        and word != 'RT']\n    words_without_stopwords = [word for word in words_cleaned if not word in stopwords_set]\n    text.append((words_without_stopwords, row.sentiment))\n\ntest_pos = test[ test['sentiment'] == 'Positive']\ntest_pos = test_pos['text']\ntest_neg = test[ test['sentiment'] == 'Negative']\ntest_neg = test_neg['text']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a next step I extracted the so called features with nltk lib, first by measuring a frequent distribution and by selecting the resulting keys.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting word features\ndef get_words_in_text(text):\n    all = []\n    for (words, sentiment) in text:\n        all.extend(words)\n    return all\n\ndef get_word_features(wordlist):\n    wordlist = nltk.FreqDist(wordlist)\n    features = wordlist.keys()\n    return features\nw_features = get_word_features(get_words_in_text(text))\n\ndef extract_features(document):\n    document_words = set(document)\n    features = {}\n    for word in w_features:\n        features['contains(%s)' % word] = (word in document_words)\n    return features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the nltk NaiveBayes Classifier I classified the extracted tweet word features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#  here i am Training the Naive Bayes classifier\ntraining_set = nltk.classify.apply_features(extract_features,text)\nclassifier = nltk.NaiveBayesClassifier.train(training_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here i have tried to measure how the classifier algorithm scored.\nneg_cnt = 0\npos_cnt = 0\nfor obj in test_neg: \n    res =  classifier.classify(extract_features(obj.split()))\n    if(res == 'Negative'): \n        neg_cnt = neg_cnt + 1\nfor obj in test_pos: \n    res =  classifier.classify(extract_features(obj.split()))\n    if(res == 'Positive'): \n        pos_cnt = pos_cnt + 1\n        \nprint('[Negative]: %s/%s '  % (len(test_neg),neg_cnt))        \nprint('[Positive]: %s/%s '  % (len(test_pos),pos_cnt))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Thanks for reading. I hope you like my sentiment analysis and found it to be helpful. If you have any questions or suggestions, feel free to write them down in the comment section.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}