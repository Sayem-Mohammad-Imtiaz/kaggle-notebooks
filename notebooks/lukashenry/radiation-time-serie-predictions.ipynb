{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"> ## ** Imports and data loading**"},{"metadata":{"trusted":true,"_uuid":"04ceaf254f6a8ad518ae3b00dbc3131b31c20d5e"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom pytz import timezone\nimport pytz\n\n%matplotlib inline\n\nplt.style.use('fivethirtyeight')\ntrain = pd.read_csv('../input/SolarPrediction.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d76d00077e7b4eb48d1a96b42a45fd9dce9f503c"},"cell_type":"markdown","source":"## **Initial observations**"},{"metadata":{"trusted":true,"_uuid":"0cae327a9d89d722577bb7a5d9c1eef17fc369d8"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3309e1edd3b6938b4050adedeb9a590d8eb6deba"},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5de7c0dfca271fea9d2ef47bf2919a0b4ae5c893"},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a607bf28b666460cc19018e4a2f588831ec428fa"},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## **Data preprocessing**"},{"metadata":{"_uuid":"cbba6efb6aadb085a7e76f5a425de838d20b6f5e"},"cell_type":"markdown","source":"### **UNIX Time to Datetime Transformation**"},{"metadata":{"trusted":true,"_uuid":"5ab8360934483dc18632547acd99f1c7aa68e318"},"cell_type":"code","source":"#hawaii = timezone('Pacific/Honolulu')\n\n# Creamos una copia del original\ntrain_origial = train.copy()\ndf = train.copy()\n\ntrain_origial.index = pd.to_datetime(df['UNIXTime'], unit='s')\n#df.index= df.index.tz_localize(pytz.utc).tz_convert(hawaii)\n\ntrain['DateTime'] = train_origial.index\ntrain_origial['DateTime'] = train_origial.index \ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed903c7f09670dd8edf3193056d6d28d00be8fe6"},"cell_type":"markdown","source":"### **Generation DataFrame of Radiation**"},{"metadata":{"trusted":true,"_uuid":"c11646d3e7ab8b168c4a071ae8d717f404b48b6f"},"cell_type":"code","source":"train_radiation = train.drop(['UNIXTime', 'Data', 'Time', 'Temperature','TimeSunRise', 'TimeSunSet',\n                         'Pressure', 'Humidity', 'WindDirection(Degrees)', 'Speed' ], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ddd902efb3f026bdaa18490177b36318f48b76e"},"cell_type":"markdown","source":"### **Time features**"},{"metadata":{"trusted":true,"_uuid":"7fb76738054dd78d0e5067d7a6dca781f755037d"},"cell_type":"code","source":"for i in (train_radiation, train_origial):\n    i['year'] = i.DateTime.dt.year\n    i['month'] = i.DateTime.dt.month\n    i['day'] = i.DateTime.dt.day\n    i['Hour'] = i.DateTime.dt.hour","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1787233176816796ba076dacab91ed16b1cf042c"},"cell_type":"code","source":"train_radiation['Day of week'] = train_radiation['DateTime'].dt.dayofweek\ntemp_rad = train_radiation['DateTime']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8217184c1339aa083bb0dee94a030b6413caa1e7"},"cell_type":"code","source":"# Funcion para saber si es fin de semana o no, poco relevante ...\ndef applyer(row):\n    if row.dayofweek == 5 or row.dayofweek == 6:\n        return 1\n    else:\n        return 0\ntemp2 = train_radiation['DateTime'].apply(applyer)\ntrain_radiation['weekend'] = temp2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8b6cdf2cb3a3f21fbb6b887d5d71394bc315519"},"cell_type":"markdown","source":"### **Change Index for datetime data type**"},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true,"_uuid":"183d5ab074e324e1ee27a57bb04847f731058644"},"cell_type":"code","source":"train_radiation.index = train_radiation['DateTime']","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"_uuid":"093ea81ba964e76255e6ae4d5ef3d6d0f06b2133"},"cell_type":"markdown","source":"### **Preprocessing result**"},{"metadata":{"trusted":true,"_uuid":"75c34f07e34d63dedab87b1db77dd8b1fb707ed3"},"cell_type":"code","source":"train_radiation.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31983ac0dd6a2f14968f6f41c4e53ab74d63627a"},"cell_type":"markdown","source":"## **Radiation Analysis and Data Visualization**"},{"metadata":{"trusted":true,"_uuid":"f20c36bf4b193ef245f4ac9fcd51900aba1b3dca"},"cell_type":"code","source":"df_rad = train_radiation.drop('DateTime', 1)\nts = df_rad['Radiation']\nplt.figure(figsize= (20,5))\nplt.title('Radiation vs Time')\nplt.xlabel('Time (Year-Month-Day))')\nplt.ylabel('Radiation level')\nplt.plot(ts)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a57839bd43915f079dd3b1faa1496a3e419af728"},"cell_type":"markdown","source":"### **Clean index**"},{"metadata":{"trusted":true,"_uuid":"158c12ac400304b874c0f26dc91f9f5fcd57d6ca"},"cell_type":"code","source":"train_radiation['Date']=pd.to_datetime(train_radiation.DateTime).dt.strftime('%Y-%m-%d')\ntrain_radiation.index = train_radiation.Date\ntrain_radiation.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"968685d4a0860de47765ef270e16909c7efa719d"},"cell_type":"code","source":"train_radiation.groupby('month')['Radiation'].mean().plot.bar(figsize = (20,5),\n                                                              title = 'Monthly Average Radiation',\n                                                              fontsize = 14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de5c92372828fbed80788b95c982e45ab0a550ba"},"cell_type":"code","source":"temp = train_radiation.groupby(['day'])['Radiation'].mean()\ntemp.plot(figsize = (20,5), title = \"Average per day radiation Month\", fontsize = 14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76870d6077f813b3c412fdc00c35037d659a0007"},"cell_type":"code","source":"temp = train_radiation.groupby(['day', 'Hour'])['Radiation'].mean()\ntemp.plot(figsize = (20,5), title = \"Average Radiation per Daily, Hour\", fontsize = 14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"193c306130aef1426e33196aa01089513364d272"},"cell_type":"code","source":"temp = train_radiation.groupby(['Hour'])['Radiation'].mean()\ntemp.plot(figsize = (20,5), title = \"Average Radiation per Hour\", fontsize = 14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74db900e6e404e9c551ba70726169e8b5114b777"},"cell_type":"code","source":"train_radiation.groupby('Day of week')['Radiation'].mean().plot.bar(figsize = (20,6),\n                                                                   title = 'Average radiation per day per week')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28e54d89cbb999142c7cd7231742a7773bb2b8b7"},"cell_type":"markdown","source":"## **Visualizacion descompuesta por peridos**"},{"metadata":{"trusted":true,"_uuid":"d4392d2a33dd964f3a5b443a93c7fe5dca539ac9"},"cell_type":"code","source":"train_radiation['Timestamp'] = pd.to_datetime(train_radiation.DateTime, format = '%d-%m-%y %H:%M')\ntrain_radiation.index = train_radiation.Timestamp\n\n#Hourly\nhourly = train_radiation.resample('H').mean()\n\n#Daily\ndaily = train_radiation.resample('D').mean()\n\n#Weekly\nweekly = train_radiation.resample('W').mean()\n    \n#Monthly\nmonthly = train_radiation.resample('M').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"911e900a2cda9981f8740403630d46309cea9c73"},"cell_type":"code","source":"ig,axs = plt.subplots(4,1)\n\nhourly.Radiation.plot(figsize = (15,8), title = \"Hourly\", fontsize = 14, ax = axs[0])\ndaily.Radiation.plot(figsize = (15,8), title = \"Daily\", fontsize = 14, ax = axs[1])\nweekly.Radiation.plot(figsize = (15,8), title = \"Weekly\", fontsize = 14, ax = axs[2])\nmonthly.Radiation.plot(figsize = (15,8), title = \"Monthly\", fontsize = 14, ax = axs[3])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"373efe6ecce2291f7b8cbb4c521980ccb4bbe359"},"cell_type":"markdown","source":"## **Transformation and visualization of regular data**"},{"metadata":{"_uuid":"9fdfb29ad8ef29c6730c60cba60f4306539c6f2c"},"cell_type":"markdown","source":"It can be seen that the most regular data is between October and the end of November beginning of December. Since if we see the graphs above, between December and January there are missing dataframe data, and the same happens in September, so these ranges of analysis are discarded."},{"metadata":{"trusted":true,"_uuid":"724b73c35dbf4b03a804eddf2a6bbbea850fcb8e"},"cell_type":"code","source":"From = '2016-10-01'\nTo   = '2016-12-01'\n\nhourly = hourly.loc[From:To,:]\ndaily = daily.loc[From:To,:]\nweekly = weekly.loc[From:To,:] \nmonthly = monthly.loc[From:To,:] \n\nig,axs = plt.subplots(4,1)\nhourly.Radiation.plot(figsize = (15,8), title = \"Hourly\", fontsize = 14, ax = axs[0])\ndaily.Radiation.plot(figsize = (15,8), title = \"Daily\", fontsize = 14, ax = axs[1])\nweekly.Radiation.plot(figsize = (15,8), title = \"Weekly\", fontsize = 14, ax = axs[2])\nmonthly.Radiation.plot(figsize = (15,8), title = \"Monthly\", fontsize = 14, ax = axs[3])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63bf339baacb27c62164c12ddc413583eda5812c"},"cell_type":"markdown","source":"## **Look at stationarity**\n\nIt is assumed that the data of the underlying time series are stationary. This assumption gives us some ** nice ** statistical properties that allow us to use several models for forecasting.\n\nStationary is a statistical assumption that a time series has:\n\n  - __Media constant__\n  - __Constant balance__\n  - __The autocovariedad does not depend on the time__\n\nIn short, if we use past data to predict future data, we must assume that the data will follow the same general trends and patterns as in the past. This general statement is valid for most training data and modeling tasks.\n\nSometimes we need to transform the data to make it stationary. However, this transformation then questions whether these data are really stationary and can be modeled using these techniques.\n\nSource: https://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-time-series-modeling/"},{"metadata":{"_uuid":"8d590971cd5cd581deb6f45649bfccc8a0dce264"},"cell_type":"markdown","source":"### **Stationary series test function**"},{"metadata":{"trusted":true,"_uuid":"f88dae22628e8fa2a74f9b9a38716b66da760b45"},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\n\ndef test_stationarity(df, ts):\n    # Determining rolling statics\n    rolmean = df[ts].rolling(window = 12, center = False).mean()\n    rolstd = df[ts].rolling(window = 12, center = False).std()\n    \n    # Plot rolling statistics\n    orig = plt.plot(df[ts], color = 'blue', label = 'Original')\n    mean = plt.plot(rolmean, color = 'red' , label = 'Promedio')\n    std = plt.plot(rolstd, color = 'black', label = 'Desviacion Estandar')\n    \n    plt.legend(loc = 'best')\n    plt.title('Promedio y Desviacion Estandar para %s' %(ts))\n    plt.xticks(rotation = 45)\n    plt.show(block = False)\n    plt.close()\n    \n    # Perform Dickey-Fuller test:\n    # Null Hypothesis (H_0): time series is not stationary\n    # Alternate Hypothesis (H_1): time series is stationary\n    \n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(df[ts], autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4],\n                         index = ['Test Statistic',\n                                  'p-value',\n                                  '# Lags Used',\n                                  'Number of Observations Used'])\n    for key, value in dftest[4].items():\n        dfoutput['Critical Value (%s)' %key] = value\n    print(dfoutput)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f835ad8460c2cc0164bf3a677e62c1930152107c"},"cell_type":"code","source":"test_stationarity(df = train_radiation, ts = 'Radiation')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fee4424c1167d99cd875d510ef9053e2ab88d724"},"cell_type":"markdown","source":"## **Hypothesis**"},{"metadata":{"_uuid":"1c9eec65960986b31bd065d860224bada87ad423"},"cell_type":"markdown","source":"### What does the Dickey-Fuller Test tell us?\n\nThis is one of the statistical tests to verify the stationarity. Here the null hypothesis is that the time series is not stationary. The results of the tests include a test statistic (test statistic) and some critical values for the difference confidence levels. If the \"Test Statistic\" is smaller than the \"Critical Value 1%\", the null hypothesis is rejected, therefore the series is stationary.\n\nIn summary,\n\n Critical-Value = cv = -3.43 |\n Test Statistic = ts = -23.77\n\n* H0: It is not stationary; ts> cv\n* H1: It is stationary; ts <cv\n\nTherefore, H0 is rejected, which indicates that the series ** is stationary **.\n \nSource: https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/"},{"metadata":{"_uuid":"2caed810937d68d669aa09055c0634380266da36"},"cell_type":"markdown","source":"## **Predictions**"},{"metadata":{"_uuid":"2ad5579b2a53fc5763060ccae517ea8488ca3b59"},"cell_type":"markdown","source":"### **Imports**"},{"metadata":{"trusted":true,"_uuid":"69e770f170307bd74b09fd2f20bfb06bc133002e"},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport statsmodels.api as sm\nfrom statsmodels.tsa.api import Holt, ExponentialSmoothing, SimpleExpSmoothing\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.arima_model import ARIMA","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"874a5589e1b098020e263d2fd79c26cd0d12eebd"},"cell_type":"markdown","source":"### **Divide data for training and validation**"},{"metadata":{"trusted":true,"_uuid":"290f874a214b9a3c91fd7807d9782de6b8e5cfb2"},"cell_type":"code","source":"_train = hourly.loc['2016-10-02':'2016-11-13',:]\nvalid = hourly.loc['2016-11-14': '2016-11-28',:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3511dbfa855373227c82e85d412b7a6f0b5a1f2c"},"cell_type":"code","source":"_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1232a464db8d07a9b37c0ede3dabe485b5c5e62b"},"cell_type":"code","source":"valid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd283efbf9f72a40c217bcd40fd77bd14875e43d"},"cell_type":"code","source":"_train.Radiation.plot(figsize=(25,5), title = 'Radiacion Diaria', fontsize=14, label='Train')\nvalid.Radiation.plot(figsize=(25,5), title = 'Radiacion Diaria', fontsize=14, label='Valid')\nplt.xlabel('DateTime')\nplt.ylabel('Radiation')\nplt.legend(loc = 'best')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9fb9b837aaf9486275ea558f7d73607b1d1f0801"},"cell_type":"markdown","source":"### **Decomposition by season**"},{"metadata":{"trusted":true,"_uuid":"83764a586aba63b3cfafb4b00644faec0a87669d"},"cell_type":"code","source":"plt.style.use('default')\nplt.figure(figsize = (16,8))\nsm.tsa.seasonal_decompose(_train.Radiation).plot()\nresult = sm.tsa.stattools.adfuller(_train.Radiation)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21fa84a83728dc9056feb13d69305208c414626c"},"cell_type":"markdown","source":"### **Naive Approach**"},{"metadata":{"trusted":true,"_uuid":"545297501c2f43bf695badbec3c0bf7e3b7a99e0"},"cell_type":"code","source":"dd = np.asarray(_train.Radiation)\ny_hat =valid.copy()\ny_hat['naive'] = dd[len(dd)- 1]\nplt.figure(figsize = (25,5))\nplt.plot(_train.index, _train['Radiation'],label = 'Train')\nplt.plot(valid.index, valid['Radiation'], label = 'Validation')\nplt.plot(y_hat.index, y_hat['naive'],  label = 'Naive')\nplt.legend(loc = 'best')\nplt.tick_params(axis = 'x', rotation = 45)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d7121b94c790ad3c6765a8e91532f2c0a7991f6"},"cell_type":"markdown","source":"####  ** Error RMS for Naive Approach**"},{"metadata":{"trusted":true,"_uuid":"c2753575023d3c3e17df4adc4d9ad912c688ad52"},"cell_type":"code","source":"rmse = sqrt(mean_squared_error(valid['Radiation'], y_hat['naive']))\nrmse","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5cb82b813776866246bb0684a7fb526e5e4e814d"},"cell_type":"markdown","source":"### **Holt Linear**"},{"metadata":{"trusted":true,"_uuid":"68f42a8f5e2d6e69cca185694bd7c964b9b526f9"},"cell_type":"code","source":"y_hat_holt = valid.copy()\nfit1 = Holt(np.asarray(_train['Radiation'])).fit(smoothing_level = 0.01, smoothing_slope = 0.1)\ny_hat_holt['Holt_linear'] = fit1.forecast(len(valid))\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(25,5))\nplt.plot(_train.index, _train['Radiation'],label = 'Train')\nplt.plot(valid.index, valid['Radiation'], label = 'Validation')\nplt.plot(y_hat.index, y_hat_holt['Holt_linear'], label = 'Holt Linear')\nplt.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfeac0d4272c34c5bdff1117c6b14c268336a9ec"},"cell_type":"markdown","source":"####  ** Error RMS for Holt Linear**"},{"metadata":{"trusted":true,"_uuid":"f2b39f92092a6c1f867b6ef42d37e0d7956046b7"},"cell_type":"code","source":"rmse = sqrt(mean_squared_error(valid['Radiation'],  y_hat_holt.Holt_linear))\nrmse","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5483b78a66caef676eff688d8325e8d5ea0376b"},"cell_type":"markdown","source":"### **Simple Exponential Smoothing**"},{"metadata":{"trusted":true,"_uuid":"ca4aa7e61b89a176a9984319081e1712880ad8cf"},"cell_type":"code","source":"y_hat_avg2 = valid.copy()\nfit2 = SimpleExpSmoothing(np.asarray(_train['Radiation'])).fit(smoothing_level=0.02,optimized=False)\ny_hat_avg2['SES'] = fit2.forecast(len(valid))\nplt.figure(figsize=(25,5))\nplt.plot(_train['Radiation'], label='Train')\nplt.plot(valid['Radiation'], label='Test')\nplt.plot(y_hat_avg2['SES'], label='SES')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3012f80135608293c44c5e3c63d91f392a32b8e9"},"cell_type":"markdown","source":"####  ** Error RMS for Simple Exponentian Smoothing**"},{"metadata":{"trusted":true,"_uuid":"b2d93d5ca0c1be105b61212ca1f9231d861f0eb0"},"cell_type":"code","source":"rms = sqrt(mean_squared_error(valid.Radiation, y_hat_avg2.SES))\nprint(\"Error: \", rms)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78fb78a48feefed5b8b308052bd710e5deb8b258"},"cell_type":"markdown","source":"### **Holt Winter**\n"},{"metadata":{"trusted":true,"_uuid":"feb8cf8e4592a058d83bf2dda4ef4edc3decb5f2"},"cell_type":"code","source":"y_hat_avg = valid.copy()\nfit1 = ExponentialSmoothing(np.asarray(_train['Radiation']), seasonal_periods=4, trend = 'add', seasonal= 'add').fit()\ny_hat_avg['Holt_Winter'] = fit1.forecast(len(valid))\nplt.figure(figsize = (25,5))\nplt.plot(_train.index, _train['Radiation'],label = 'Train')\nplt.plot(valid.index, valid['Radiation'], label = 'Validation')\nplt.plot(y_hat_avg.index, y_hat_avg['Holt_Winter'], label = 'Holt_Winter')\nplt.legend(loc = 'best')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2519383bddf67eb52ec590385b354f73f874dd82"},"cell_type":"markdown","source":"####  ** Error RMS for Holt Winter**"},{"metadata":{"trusted":true,"_uuid":"5ca4bdd99796e43fbdba6bac9c43ed190fe72466"},"cell_type":"code","source":"rms = sqrt(mean_squared_error(valid.Radiation, y_hat_avg.Holt_Winter))\nprint(\"error: \", rms)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3693f6ab54a7ef95fd0f7c4535357c4f507f5143"},"cell_type":"markdown","source":"### **SARIMAX & ARIMA**"},{"metadata":{"_uuid":"6f63b2227994e2f28063ede0debc4dc69b27c30d"},"cell_type":"markdown","source":"The ARIMA forecast for a stationary time series is no more than a linear equation (like a linear regression). The predictors depend on the parameters (p, d, q) of the ARIMA model:\n\n   - ** Number of AR terms (autoregressive) (p): ** AR terms are only delays of the dependent variable. For example, if p is 5, the predictors for x (t) will be x (t-1) ... .x (t-5).\n   - ** Number of MA terms (moving average) (q): ** MA terms are delayed forecast errors in the prediction equation. For example, if q is 5, the predictors for x (t) will be e (t-1) ... .e (t-5) where e (i) is the difference between the moving average at the instantaneous moment and the real value.\n   - ** Number of differences (d): ** are the number of non-seasonal differences, that is, in this case we take the difference of first order.\n   \n   Source: https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/"},{"metadata":{"_uuid":"1fe52c2973c377665277595f1224c3199812fcbc"},"cell_type":"markdown","source":"#### ACF and PACF Plots\n** How do we determine p, d and q? ** For p and q, we can use ACF and PACF graphs (below).\n\n** Autocorrelation function (ACF) **. Correlation between the time series with a delayed version of itself.\n\n** Partial autocorrelation function (PACF) **. Additional correlation explained by each successive lagged term.\n\n** How do we interpret the ACF and PACF graphs? **\n\n- p - Delay value where the PACF graph crosses the upper confidence interval for the first time.\n- q - Delay value where the ACF graph crosses the upper confidence interval for the first time."},{"metadata":{"trusted":true,"_uuid":"e2f8eabe02f17a7e40507af48a3030036077132e"},"cell_type":"code","source":"def plot_acf_pacf(df, ts):\n  \"\"\"\n  Plot auto-correlation function (ACF) and partial auto-correlation (PACF) plots\n  \"\"\"\n  f, (ax1, ax2) = plt.subplots(2,1, figsize = (10, 5)) \n\n  #Plot ACF: \n\n  ax1.plot(lag_acf)\n  ax1.axhline(y=0,linestyle='--',color='gray')\n  ax1.axhline(y=-1.96/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n  ax1.axhline(y=1.96/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n  ax1.set_title('Autocorrelation Function for %s' %(ts))\n\n  #Plot PACF:\n  ax2.plot(lag_pacf)\n  ax2.axhline(y=0,linestyle='--',color='gray')\n  ax2.axhline(y=-1.96/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n  ax2.axhline(y=1.96/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n  ax2.set_title('Partial Autocorrelation Function for %s' %(ts))\n  \n  plt.tight_layout()\n  plt.show()\n  plt.close()\n  \n  return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77a87ccddac7e46b95b0c39b517322387a4d59dd"},"cell_type":"code","source":"lag_acf = acf(np.array(_train['Radiation']), nlags = 20)\nlag_pacf = pacf(np.array(_train['Radiation']), nlags = 20, method='ols')\n\nplot_acf_pacf(df = _train, ts = 'Radiation')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9ecaa54db254a6b5cb1da649e4020ab99ee3bc5"},"cell_type":"markdown","source":"### **Conclusions**\n\nAs seen in the PACF chart, the largest amount of partial correlation is between 1 and 0, so ** p has a value of 1 **\n\nThe same happens in the ACF chart, so ** has a value of 1 **."},{"metadata":{"_uuid":"3351ac22e5efabc9560da61e76c24d085d636ab8"},"cell_type":"markdown","source":"### **SARIMAX(1,0,1)**"},{"metadata":{"trusted":true,"_uuid":"eb932adc12714298736c3a800ee7a77f0123f735"},"cell_type":"code","source":"fit2 = sm.tsa.statespace.SARIMAX(_train.Radiation, order=(1,0,1),seasonal_order=(1,1,0,12), trend='ct')\nres = fit2.fit()\ny_hat_avg['SARIMA'] = res.predict(start=\"2016-11-14\", end=\"2016-11-29\", dynamic=True)\nplt.figure(figsize=(20,5))\nplt.plot( _train['Radiation'], label='Train')\nplt.plot(valid['Radiation'], label='Test')\nplt.plot(y_hat_avg['SARIMA'], label='SARIMA')\nplt.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89fa689fc1c4cea1be516f671563ccad189f6836"},"cell_type":"code","source":"res.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f724ea7887889a701274667ad9696a6049e4e48"},"cell_type":"code","source":"rms = sqrt(mean_squared_error(valid.Radiation, y_hat_avg['SARIMA']))\nprint('Error:', rms)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"345f1dad5ae58a8cfb864cab934243316a90886c"},"cell_type":"markdown","source":"### **ARIMA(1,0,1)**"},{"metadata":{"trusted":true,"_uuid":"1135d3bf47bfb9709904b6c3572b6c40a1d660a2"},"cell_type":"code","source":"model = ARIMA(_train.Radiation, order=(1, 0, 1))  \nresults_MA = model.fit()  \nplt.plot(_train.Radiation)\nplt.plot(results_MA.fittedvalues, color='red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a815e188a55aefa79698fd044b81229d2b4f9afe"},"cell_type":"code","source":"results_MA.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b54dd81b5336854c1f5b3700bc3fe41c0ab5ca4e"},"cell_type":"markdown","source":"### Finally\nWe include a seasonal effect in an additive way, which means that we add a term that allows the process to depend on the fourth MA delay. It may be that, on the contrary, we want to model a seasonal effect in a multiplicative way. We often write the model then as ARIMA (p, d, q) × (P, D, Q) s, where the letters of low intensity indicate the specification for the non-seasonal component, and uppercase letters indicate the specification of the Season component s is the periodicity of the stations (for example, it is often 4 for quarterly data or 12 for monthly data).\n\nAs it says above, it was used as parameter 12, since it is a monthly data"},{"metadata":{"_uuid":"03afd4d0e00fb3fb94dd50f12fe825364f225055"},"cell_type":"markdown","source":"# EXTRA LSTM\n## Recurrent Neural Networks\n\n  - https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n  - https://blog.statsbot.co/time-series-prediction-using-recurrent-neural-networks-lstms-807fa6ca7f\n  - https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/"},{"metadata":{"trusted":true,"_uuid":"12973e36cf216d11efcbea3120bb54bf41742474"},"cell_type":"code","source":"def do_lstm_model(df, \n                  ts, \n                  look_back, \n                  epochs, \n                  type_ = None, \n                  train_fraction = 0.67):\n  \"\"\"\n   Create LSTM model\n   Source: https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n  \"\"\"\n  # Import packages\n  import numpy\n  import matplotlib.pyplot as plt\n  from pandas import read_csv\n  import math\n  from keras.models import Sequential\n  from keras.layers import Dense\n  from keras.layers import LSTM\n  from sklearn.preprocessing import MinMaxScaler\n  from sklearn.metrics import mean_squared_error\n\n  # Convert an array of values into a dataset matrix\n  def create_dataset(dataset, look_back=1):\n    \"\"\"\n    Create the dataset\n    \"\"\"\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n      a = dataset[i:(i+look_back), 0]\n      dataX.append(a)\n      dataY.append(dataset[i + look_back, 0])\n    return numpy.array(dataX), numpy.array(dataY)\n\n  # Fix random seed for reproducibility\n  numpy.random.seed(7)\n\n  # Get dataset\n  dataset = df[ts].values\n  dataset = dataset.astype('float32')\n\n  # Normalize the dataset\n  scaler = MinMaxScaler(feature_range=(0, 1))\n  dataset = scaler.fit_transform(dataset.reshape(-1, 1))\n  \n  # Split into train and test sets\n  train_size = int(len(dataset) * train_fraction)\n  test_size = len(dataset) - train_size\n  train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n  \n  # Reshape into X=t and Y=t+1\n  look_back = look_back\n  trainX, trainY = create_dataset(train, look_back)\n  testX, testY = create_dataset(test, look_back)\n  \n  # Reshape input to be [samples, time steps, features]\n  if type_ == 'regression with time steps':\n    trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n    testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n  elif type_ == 'stacked with memory between batches':\n    trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n    testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n  else:\n    trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n    testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n  \n  # Create and fit the LSTM network\n  batch_size = 1\n  model = Sequential()\n  \n  if type_ == 'regression with time steps':\n    model.add(LSTM(4, input_shape=(look_back, 1)))\n  elif type_ == 'memory between batches':\n    model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n  elif type_ == 'stacked with memory between batches':\n    model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True, return_sequences=True))\n    model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n  else:\n    model.add(LSTM(4, input_shape=(1, look_back)))\n  \n  model.add(Dense(1))\n  model.compile(loss='mean_squared_error', optimizer='adam')\n\n  if type_ == 'memory between batches' or type_ == 'stacked with memory between batches':\n    for i in range(100):\n      model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n      model.reset_states()\n  else:\n    model.fit(trainX, \n              trainY, \n              epochs = epochs, \n              batch_size = 1, \n              verbose = 2)\n  \n  # Make predictions\n  if type_ == 'memory between batches' or type_ == 'stacked with memory between batches':\n    trainPredict = model.predict(trainX, batch_size=batch_size)\n    testPredict = model.predict(testX, batch_size=batch_size)\n  else:\n    trainPredict = model.predict(trainX)\n    testPredict = model.predict(testX)\n  \n  # Invert predictions\n  trainPredict = scaler.inverse_transform(trainPredict)\n  trainY = scaler.inverse_transform([trainY])\n  testPredict = scaler.inverse_transform(testPredict)\n  testY = scaler.inverse_transform([testY])\n  \n  # Calculate root mean squared error\n  trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n  print('Train Score: %.2f RMSE' % (trainScore))\n  testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n  print('Test Score: %.2f RMSE' % (testScore))\n  \n  # Shift train predictions for plotting\n  trainPredictPlot = numpy.empty_like(dataset)\n  trainPredictPlot[:, :] = numpy.nan\n  trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n  \n  # Shift test predictions for plotting\n  testPredictPlot = numpy.empty_like(dataset)\n  testPredictPlot[:, :] = numpy.nan\n  testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n  \n  # Plot baseline and predictions\n  plt.plot(scaler.inverse_transform(dataset))\n  plt.plot(trainPredictPlot)\n  plt.plot(testPredictPlot)\n  plt.show()\n  plt.close()\n  \n  return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"a2e511b2a8b68868f757830fa7778d2f3f68ad8c"},"cell_type":"code","source":"# LSTM Network for Regression\ndo_lstm_model(df = train_radiation, \n              ts = 'Radiation', \n              look_back = 1, \n              epochs = 5)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}