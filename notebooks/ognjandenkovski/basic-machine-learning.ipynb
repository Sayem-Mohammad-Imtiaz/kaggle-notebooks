{"cells":[{"metadata":{"_uuid":"5ca6cb586cbb0160fd4217772a86560b2cf3f845"},"cell_type":"markdown","source":"***Classifiers and gun violence media coverage between 2013 and 2018 in the United States of America****\n\nThis project analyzes the coverage of gun violence incidents across the five years for which data is available in the Gun Violence Data Set. By training a supervised classifier with the help of the package scikit-learn to recognize whether articles discuss gun violence incidents in Republican or Democrat states, we explore basic machine learning classifiers. \n\nThe primary assumption in the project is that the state in which an incident occured can be used as a proxy for classifying the source outlet as either Democrat or Republican. While this assumption is unverified (and future projects can aim to verify this proxy), when we consider that a majority of the incidents noted in the data set are small-scale, with few individuals murdered or even none murdered, there is no reason to assume that most of these incidents would have received nation-wide coverage. Rather, it is more likely that such incidents get covered by local or state-level outlets. Future more ambitious projects, should aim to conduct the classification by county and to verify that the coverage of the incidents is indeed from local outlets.  \n\n\n**Research Question: **Can a supervised machine learning algorithm correctly classify gun violence coverage articles according to whether the incident occurred in Republican or Democrat states? \n\n**Sub Research Question: **Which characteristics in the corpora allow the classifier to work? "},{"metadata":{"_uuid":"f17efc47655b78146a4a53ef8bc615116000dc0f"},"cell_type":"markdown","source":"* Loading all necessery packages."},{"metadata":{"trusted":true,"_uuid":"00e9d0d8f3334e41c7e9049f327be8344dfa8461","_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\nfrom math import isnan\nfrom itertools import islice\nimport numpy as np\nimport datashader as ds \nimport datashader.transfer_functions as tf\nfrom datashader.utils import export_image\nfrom datashader.colors import colormap_select, Greys9, Hot, inferno\nfrom datashader.bokeh_ext import InteractiveImage\nfrom functools import partial\nfrom bokeh.models import BoxZoomTool\nfrom bokeh.plotting import figure, output_notebook, show\nfrom pandas import ExcelWriter\nfrom pandas import ExcelFile\nimport nltk \nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import metrics \nfrom sklearn.linear_model import LogisticRegression\nimport string \nimport plotly as py \nfrom string import punctuation\nimport pyLDAvis\nimport pyLDAvis.gensim\nfrom gensim import corpora, models\nfrom gensim.utils import simple_preprocess\nfrom gensim.models.ldamodel import CoherenceModel\nfrom nltk.corpus import stopwords \nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import classification_report\nprint(folium.__file__)\nprint(folium.__version__)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed1d5d3bed65879c3eb411a7b1fa404c198953e6"},"cell_type":"markdown","source":"We take a sample of 50k articles and check whether the distribution of incidents per year in our sample is proportional to the original. We create an article_links dict through which we will do the scrapping and finally drop all values which are nan (of type float), i.e. there are no source url's there. The code for the scrapping process is not shown, but scrapping was completed with BeautifulSoup.  Only the final 50klinks file is kept. "},{"metadata":{"trusted":true,"_uuid":"58d8598ca1904cb43fdc58b057d314e192757566","collapsed":true},"cell_type":"code","source":"gunviolence = pd.read_csv(\"../input/gun-violence-data/gun-violence-data_01-2013_03-2018.csv\")#gun violence data set \ngunviolence = gunviolence.dropna(subset = ['longitude', 'latitude'])\ngunviolence = gunviolence[gunviolence.source_url.str.contains(\"youtube\") == False]\ngunviolence['year'] = gunviolence.date.str[:4] # we need this for folium for later \nsample_gun = gunviolence.loc[np.random.permutation(gunviolence.index)[:50000]] #random sample for scrapper\nprint(gunviolence['year'].value_counts())\nprint(sample_gun['year'].value_counts())\narticle_links = sample_gun.set_index('incident_id').to_dict()['source_url'] \nclean_links = {k:v for k,v in article_links.items() if type(v) != float}\ntests = {k: clean_links[k] for k in list(clean_links)[:100]} # had test links for testing out scrapper\ngunviolence.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48349937fbd0a52a1791eb6b1cc51cbc25fdb40c"},"cell_type":"markdown","source":"Below we create a visualization of all incidents using datashader, using the number of individuals killed as a count for the color of the heatmap - the more red it is, the higher the number of incidents and the number of individuals killed. We can observe very few incidents in the mid-West due to the comparatively low population density. Unsurprisingly, locations such as Chicago, New York, Houston, San Diego and Los Angeles clearly stand out. We can zoom into the map using the zoom-toggle icon (third icon on the right of the map). "},{"metadata":{"trusted":true,"_uuid":"b8c0ba3aa8fb124afd7608881aa9ac29e2625d59","collapsed":true},"cell_type":"code","source":"output_notebook()\n\nUS = x_range, y_range = ((-161.75583 ,-68.01197), (19.50139,64.85694))\n\nplot_width  = int(800)\nplot_height = int(plot_width//1.2)\n\ndef base_plot(tools='pan,wheel_zoom,reset',plot_width=plot_width, plot_height=plot_height, **plot_args):\n    p = figure(tools=tools, plot_width=plot_width, plot_height=plot_height,\n        x_range=x_range, y_range=y_range, outline_line_color=None,\n        min_border=0, min_border_left=0, min_border_right=0,\n        min_border_top=0, min_border_bottom=0, **plot_args)\n    p.axis.visible = True\n    p.xgrid.grid_line_color = None\n    p.ygrid.grid_line_color = None\n               \n    p.add_tools(BoxZoomTool(match_aspect=True))\n               \n    return p\n    \noptions = dict(line_color=None, fill_color='blue', size=5)\n\n\nbackground = \"black\"\nexport = partial(export_image, export_path=\"export\", background=background)\ncm = partial(colormap_select, reverse=(background==\"black\"))\n\ndef create_image(x_range, y_range, w=plot_width, h=plot_height):\n    cvs = ds.Canvas(plot_width=w, plot_height=h, x_range=x_range, y_range=y_range)\n    agg = cvs.points(gunviolence, 'longitude', 'latitude',  ds.count('n_killed'))\n    img = tf.shade(agg, cmap=Hot, how='eq_hist')\n    return tf.dynspread(img, threshold=0.5, max_px=4)\n\np = base_plot(background_fill_color=background)\nexport(create_image(*US),\"US_hot\")\nInteractiveImage(p, create_image)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f48b3a2da0d98109cecf8d666fcf5a06e0dadea"},"cell_type":"markdown","source":"The downside of scrapping with BeautifulSoup is that many of our articles also include text regarding updated privacy policies or other unnecessary text. Now we want to check which of the downloaded articles are actually useful. We know that many refuse a connection, many websites simply don't exist anymore, or the new EU GDPR is preventing us from accessing them. A good way of checking is by seeing whether the articles contain \"http\", as if they do not that means that they were overwritten by useful text - this is how we wrote the scrapper. If there was text found in the articles, the text overwrote the source_url in the value column. If there was no text the url stayed. The cleaning process removes 20700 articles, leaving us with 28629 articles that we can use for our analysis. "},{"metadata":{"trusted":true,"_uuid":"932f7e16e4964a077364e150e815c6e542a13d5a","collapsed":true},"cell_type":"code","source":"clean_links = np.load('../input/50klinks/50klinks.npy').item()\n#cleaning \ncleaner_articles = {}\nnot_so_clean_articles = {}\nfor key, value in clean_links.items():\n    if 'http' in value and \\\n        len(value.split()) < 40:\n        not_so_clean_articles[key] = value\n    else:\n        cleaner_articles[key] = value\n#this step removes 3439 articles - if checked they are just links\n#next step of clean-up - specifi terms/privacy policies/etc.\nuseful_articles = {}\ntrash_articles = {}\nfor key,value in cleaner_articles.items():\n    if len(value.split()) < 10 or \\\n        \"GDPR\" in value or \\\n        \"JavaScript\" in value or \\\n        \"page you requested is currently unavailable\" in value or \\\n        \"is no longer available\" in value or \\\n        \"page you requested could not be found\" in value or\\\n        \"CAPTCHA\" in value or\\\n        \"403\" in value:\n            trash_articles[key] = value \n    else:\n        useful_articles[key] = value\n\ngunviolence['texts']=gunviolence.incident_id.map(useful_articles)#add articles to data set ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23b33254bb311c8da457ec6edbdb3b34855c3e90"},"cell_type":"markdown","source":"Now we can start text analysis and for that we import a dataframe containing two columns - the state name and its designation as Red or Blue - Democrat or Republican. Out of the 28629 articles available around 17 thousand are about incidents in Republican states while around 12 thousand are about incidents in Democrat states. "},{"metadata":{"trusted":true,"_uuid":"a84e94cc5cb78721ca4e7d57dcce65cdcef09b60","collapsed":true},"cell_type":"code","source":"#merging extra df\nusefuldf = gunviolence.dropna(subset=['texts'])\nredvsblue = pd.read_excel(\"../input/red-vs-blue-states/Red vs Blue.xlsx\")\nusefuldf = pd.merge(redvsblue, usefuldf, on='state')\ngunviolence = pd.merge(redvsblue,gunviolence, on= 'state')\nusefuldf['Color'].value_counts()\nrepdf = usefuldf.loc[usefuldf['Color'] == 'Red']\ndemdf = usefuldf.loc[usefuldf['Color'] == 'Blue']\nprint(len(repdf))\nprint(len(demdf))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e36ad23a0f351443bf2ac94e97600c13434204e"},"cell_type":"markdown","source":"The following step is an initial exploration of the differences in most freqeuent terms used in the coverage across all articles, relating to incidents in Democrat states and relating to incidents in Republican states. Before we run the frequency count, we convert all words to lowercase, remove dots, and create a function which removes stopwords. "},{"metadata":{"trusted":true,"_uuid":"1c62165aedd706ebbe16991016d8a250066731ee","collapsed":true},"cell_type":"code","source":"#getting lists for frequency analysis \nall_analysis = usefuldf['texts'].tolist()\nrep_analysis = repdf['texts'].tolist()\ndem_analysis = demdf['texts'].tolist()\nall_words = [x.lower().replace('.', ' ') for x in all_analysis]\nrep_analysis = [x.lower().replace('.',' ') for x in rep_analysis]\ndem_analysis = [x.lower().replace('.', ' ') for x in dem_analysis]\n#stopword removal for most frequent word count \nstopwords = stopwords.words('english')# this line sometimes fails. re runing the first cell with packages usually fixes it \nstopwords.append(\"said\") # we append this word due to the amount of times the article say that the Police or whoever \"said\" something.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b959afd2672f9685cab3aa555a60aa7ca753129"},"cell_type":"markdown","source":"This initial observation does not demonstrate any significant differences in the coverage, except showing clear trends of gun violence assosciated to males and females as likely victims."},{"metadata":{"trusted":true,"_uuid":"92dba322a53d7b0e59e4e327cc2f582bb2a114e8","collapsed":true},"cell_type":"code","source":"def stopwording(y):\n    y = [' '.join(w for w in line.split() if w.lower().strip(punctuation) not in stopwords) for line in y]\n    return y  #can i fix punctuation\nall_words = stopwording(all_analysis)\nrep_words = stopwording(rep_analysis)\ndem_words = stopwording(dem_analysis)\n\nall_ready = []\nfor line in all_words:\n    all_ready.extend(line.split())\nrep_ready = []\nfor line in rep_words:\n    rep_ready.extend(line.split())\ndem_ready = []\nfor line in dem_words:\n    dem_ready.extend(line.split())\ndef counting(y):\n    y_counter = Counter(y)\n    y_most_common = y_counter.most_common(50)\n    return y_most_common\n\nall_words_count = counting(all_ready)\nprint(all_words_count)\nrep_words_count = counting(rep_ready)\nprint(rep_words_count)\ndem_words_count = counting(dem_ready)\nprint(dem_words_count) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bef7f1c9815d6b0360f03bcc6d8223e59889d13c"},"cell_type":"markdown","source":"The next step involves training a supervised machine learning model to see whether and how successfully it can recognize the differences between Republican and Democrat state incident coverage. For this process we use the package scikit-learn. First we create three data sets, one for training, one for validating and one for testing, at a ratio of 60:20:20. We pre-process these by making all words lower case, stripping punctuation and removing stopwords. Stemming is not done here but is reccomended.  We then create tuples for each, the first element containing the text, the second element containing the party color - Red or Blue."},{"metadata":{"trusted":true,"_uuid":"39cd140c608e81814c4fd4fda02e4ba5a99f6982","collapsed":true},"cell_type":"code","source":"train, validate, test = np.split(usefuldf.sample(frac=1), [int(.6*len(usefuldf)), int(.8*len(usefuldf))])\ndef cleaning(y):\n    y = y[['Color', 'texts']]\n    y_text_list = y['texts'].tolist()\n    y_text_list = [line.lower().strip(punctuation) for line in y_text_list]       \n    y = [line for line in y_text_list if line not in stopwords]\n    y = [' '.join(w for w in line.split() if w.lower() not in stopwords) for line in y_text_list]\n    return y \ntrain_text_list = cleaning(train)\ntrain_color_list = train['Color'].tolist()\ntrain = list(zip(train_text_list, train_color_list))\nvalidate_text_list = cleaning(validate)\nvalidate_color_list = validate['Color'].tolist()\nvalidate = list(zip(validate_text_list, validate_color_list))\ntest_text_list = cleaning(test)\ntest_color_list = test['Color'].tolist()\ntest = list(zip(test_text_list, test_color_list))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4074c442fe2c2c34dcf16c7cf0afd4e2daa33e0f"},"cell_type":"markdown","source":"The first model we create is a Naive Bayes classifier based on a count vectorizer, based on the frequency of words in documents, which returns an accuracy score of .91, a precision rate of .88 and a recall rate of .93 when examined for Blue. When examined for Red the model returns a precision rate of .95 and a recall rate of .91. The average f1-score is .92. These are rather good results already, showing that our model is capable of accurately classifying whether an article is Red or Blue in a majority of cases. We yet don't know why this works so well. First we run some more models. "},{"metadata":{"trusted":true,"_uuid":"d5d745d703727f03659f11a11b1b8a043c85294f","collapsed":true},"cell_type":"code","source":"# Naive Bayes classic \nvectorizer = CountVectorizer(stop_words = 'english')\ntrain_features = vectorizer.fit_transform(r[0] for r in train)\nvalidate_features = vectorizer.transform(r[0] for r in validate)\nnb = MultinomialNB()\nnb.fit(train_features, [r[1] for r in train])\npredictions = nb.predict(validate_features)\nactual =[r[1] for r in validate]\nprint(metrics.accuracy_score(actual,predictions,normalize = True))\nprint(classification_report(actual, predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f38830f11e1be8ab9aeee2bb9a91bbe296ad0f2"},"cell_type":"markdown","source":"The second model we run is a logistic regression, based on a count vecorizer. This returns an accuracy score of .95, a precision rate of .95 for label Blue and a recall rate of .93. Checking the precision and recall of the same model for label Red returns a precision score of .95 and a recall score of .97. The average f1-score is .95. "},{"metadata":{"trusted":true,"_uuid":"ada756f71b65a439a45424d038ae62731f4a054e","collapsed":true},"cell_type":"code","source":"#Logistic regression classic \nvectorizer = CountVectorizer(stop_words = 'english')\ntrain_features = vectorizer.fit_transform(r[0] for r in train)\nvalidate_features = vectorizer.transform(r[0] for r in validate )\nlogreg = LogisticRegression()\nlogreg.fit(train_features, [r[1] for r in train])\npredictions = logreg.predict(validate_features)\nactual =[r[1] for r in validate]\nprint(metrics.accuracy_score(actual,predictions,normalize = True))\nprint(classification_report(actual, predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e9a0a1ea8a15424d2475dd39513c40d9a003495"},"cell_type":"markdown","source":"Here we train a Naive Bayes classifier using the weight of term frequency as a vectorizer - tfidf in scikit-learn. This model returns an accuracy score of .89, a precision score of .99 for label Blue and a recall score of .75. Running the same model for label Red gives us a precision score of .85 and a recall score of .99. The average f1-score is .89. Our Naive Bayes classifier did better without the term frequency weight."},{"metadata":{"trusted":true,"_uuid":"7f416efd8bab833b854b45f0eb03ed8b8a63e883","collapsed":true},"cell_type":"code","source":"#Naive Bayes tfidf \nvectorizer = TfidfVectorizer(stop_words = 'english')\ntrain_features = vectorizer.fit_transform(r[0] for r in train)\nvalidate_features = vectorizer.transform(r[0] for r in validate )\nnb = MultinomialNB()\nnb.fit(train_features, [r[1] for r in train])\npredictions = nb.predict(validate_features)\nactual =[r[1] for r in validate]\nprint(metrics.accuracy_score(actual,predictions,normalize = True))\nprint(classification_report(actual, predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abc45c496ecfe29d46e3396971eb8b2442de94d0"},"cell_type":"markdown","source":"Finally, we run a logistic regression model which considers the weight of the term frequency - tfidf. This gives us an accuracy score of .93, a precision score of .96 and a recall score of .88 for label Blue. Running the same code with label Red gives us a precision score of .92 and a recall score of .98. The average f1-score is .93."},{"metadata":{"trusted":true,"_uuid":"715e8b31edac258c9d96f8a3051740744f7889b4","collapsed":true},"cell_type":"code","source":"#Logistic tdidf\nvectorizer = TfidfVectorizer(stop_words = 'english')\ntrain_features = vectorizer.fit_transform(r[0] for r in train)\nvalidate_features = vectorizer.transform(r[0] for r in validate )\nlogreg = LogisticRegression()\nlogreg.fit(train_features, [r[1] for r in train])\npredictions = logreg.predict(validate_features)\nactual =[r[1] for r in validate]\nprint(metrics.accuracy_score(actual,predictions,normalize = True))\nprint(classification_report(actual, predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28cd14a0a0316ba79bea97117f7f2c94428d2920"},"cell_type":"markdown","source":"We can see that the best score is provided by the logistic regression model which does not consider the term weight frequency, with an average f-1 score of .95 and recall and precision rates for both labels well above .9, with an accuracy score of .95. We now run this model on the test data and confirm that indeed it does the classification rather well. However, we do not understand why this is the case. The next step would be to examine the topics in coverage about incidents in Democrat and Republican states using LDA to see whether we can determine what makes this classifier work."},{"metadata":{"trusted":true,"_uuid":"5dad031c6429363b4eaf37bb39c3e31c3ceb5ef3","collapsed":true},"cell_type":"code","source":"vectorizer = CountVectorizer(stop_words = 'english')\ntrain_features = vectorizer.fit_transform(r[0] for r in train)\ntest_features = vectorizer.transform(r[0] for r in test)\nlogreg = LogisticRegression()\nlogreg.fit(train_features, [r[1] for r in train])\npredictions = logreg.predict(test_features)\nactual =[r[1] for r in test]\nprint(metrics.accuracy_score(actual,predictions,normalize = True))\nprint(classification_report(actual, predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f2183cef1457cba7959338c821c1d7adaad636a"},"cell_type":"markdown","source":"Before we start our LDA we need to do some preprocessing, like removing stopwords, punctuation etc. For the LDA model we use the package gensim. "},{"metadata":{"trusted":true,"_uuid":"00e05bff31f2266511e7d61f0ab1f4ee6f3492e5","collapsed":true},"cell_type":"code","source":"rep_lda = usefuldf.loc[usefuldf['Color'] == 'Red', 'texts'].tolist()\ndem_lda = usefuldf.loc[usefuldf['Color'] == 'Blue', 'texts'].tolist()\ndem_texts_lda = []\nfor lines in dem_lda:\n    dem_texts_lda.append(lines.split())\nrep_texts_lda = []\nfor lines in rep_lda:\n    rep_texts_lda.append(lines.split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"125353488c609b89b3be052d217364c295a36663","collapsed":true},"cell_type":"code","source":"def processinglda(y):\n    y = [[word.lower().strip(punctuation) for word in lines] for lines in y]       \n    y = [[word for word in lines if word !=''] for lines in y]\n    y = [[word for word in lines if word not in stopwords] for lines in y]\n    return y \nrep_processed = processinglda(rep_texts_lda)\ndem_processed = processinglda(dem_texts_lda)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00c8f57d1082bb0df73db72b76ee8470c6543c8d","collapsed":true},"cell_type":"code","source":"#democrat state topic modeling\nid2word = corpora.Dictionary(dem_processed)\nmm = [id2word.doc2bow(word) for word in dem_processed]\nlda_dem = models.ldamodel.LdaModel(corpus = mm, id2word = id2word, num_topics = 30, alpha = \"auto\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54ab97f8906a9d23ce867287135513e233a97b0d"},"cell_type":"markdown","source":"The optimal number of topics was determined using the u_mass topic coherence measure, which produces a result of -2.61 for 30 topics for the Democrat dataset. While this score is not optimal, an increase and decrease in number of topics results in a lower coherence score. Attempts of topic modeling with the tfidf vectorizer produce coherence scores of around -10, depending on the number of topics. Hence, a normal count vectorizer is recommended for this data set."},{"metadata":{"trusted":true,"_uuid":"d6c78dc7a37f06a7988af62711bfaef0c0b6ad65","collapsed":true},"cell_type":"code","source":"cm1 = models.CoherenceModel(model=lda_dem, corpus= mm, dictionary= id2word, coherence='u_mass')  \nmm_model = cm1.get_coherence()\nprint(mm_model)\nprint(lda_dem.print_topics())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1992784beb60f063e102fc7538cbc4600c680a0"},"cell_type":"markdown","source":"An examination of the topics for the Democrat states incidents gives us some insight into why our classifier may work as well as it does - location mention in the articles. Among the most prominent topics we have frequent mention of Chicago, Bedford-Stuyvesant, Spokane, Seattle, Gresham, Lawndale, Englewood, Harlem, Belmont, and other cities or boroughs in primarily Democrat states. However, the presence of cities like Chesterfield, Austin and several other Republican strongholds puts this assumption into question. Regardless, the presence of Democrat strongholds is much more evident. "},{"metadata":{"trusted":true,"_uuid":"926d40cc0bf3fba121ef7345b10b14b663628350","collapsed":true},"cell_type":"code","source":"#republican state topic modeling \nid2word = corpora.Dictionary(rep_processed)\nmm = [id2word.doc2bow(word) for word in rep_processed]\nlda_rep = models.ldamodel.LdaModel(corpus = mm, id2word = id2word, num_topics = 40, alpha = \"auto\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be0bc9f123f8560b972942e88492d19d4b3c7934"},"cell_type":"markdown","source":"The optimal number of topics was determined using the u_mass topic coherence measure, which produces a result of -2.47 for 30 topics. While this score is not optimal, an increase and decrease in number of topics results in a lower coherence score. Attempts of topic modeling with the tfidf vectorizer produce coherence scores of around -12, depending on the number of topics. Hence, a normal word count vectorizer is recommended for this data set."},{"metadata":{"trusted":true,"_uuid":"ec7f4032966ae888e3d095f7c2db73a29f588131","collapsed":true},"cell_type":"code","source":"cm1 = models.CoherenceModel(model=lda_rep, corpus= mm, dictionary= id2word, coherence='u_mass')  \nmm_model_rep = cm1.get_coherence()\nprint(mm_model_rep)\nprint(lda_rep.print_topics())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a733c90e329c964ef868d6eeaeb1f5ad81f2722"},"cell_type":"markdown","source":"Indeed, an observation of the topics in Republican state coverage, leads to the conclusion that many of the articles include references to  Republican strongholds or Republican leaning (in the 2016 elections) cities and states, such as Dallas, Bexar, Philadelphia which in the last election was Republican, as well as Greenville, Charleston, Milwaukee and Clayton. No reference to Democrat cities or locations is found.  \n\nOverall the analysis using the LDA approach gives some evidence for concluding that the unexpected accuracy of both the Naive Bayes and logistic regression classifiers is largely due to the reference to locations in each corpus. No further substantially distinctive features were found between the topics in the two corpora that could justify the accuracy of the classifier. "},{"metadata":{"_uuid":"9cfc3ed63090a467d06db7d7dd2e9b6ce65e08e5"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}