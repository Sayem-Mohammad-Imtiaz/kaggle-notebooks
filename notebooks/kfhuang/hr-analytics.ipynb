{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\n\ndf_train = pd.read_csv('/kaggle/input/hr-analytics-job-change-of-data-scientists/aug_train.csv')\ndf_test = pd.read_csv('/kaggle/input/hr-analytics-job-change-of-data-scientists/aug_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Cleaning\n\n#### Train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace string with float/int\ndf_train['experience'] = df_train['experience'].replace('>20','25')\ndf_train['experience'] = df_train['experience'].replace('<1','0.5')\ndf_train['experience'] = df_train['experience'].astype('float')\ndf_train['last_new_job'] = df_train['last_new_job'].replace('>4','5')\ndf_train['last_new_job'] = df_train['last_new_job'].replace('never','0')\n\n# Impute/fill NaN\ndf_train['gender'] = df_train['gender'].replace(np.nan, 'unknown')\ndf_train['enrolled_university'] = df_train['enrolled_university'].replace(np.nan, 'unknown')\ndf_train['education_level'] = df_train['education_level'].replace(np.nan, 'unknown')\ndf_train['major_discipline'] = df_train['major_discipline'].replace(np.nan, 'unknown')\ndf_train['education_level'] = df_train['education_level'].replace(np.nan, 'unknown')\ndf_train['experience'] = df_train['experience'].fillna(value = df_train['experience'].median())\ndf_train['company_size'] = df_train['company_size'].fillna(value = df_train['company_size'].value_counts().index[0])\ndf_train['company_type'] = df_train['company_type'].replace(np.nan, 'unknown')\ndf_train['last_new_job'] = df_train['last_new_job'].fillna(value = df_train['last_new_job'].median()).astype('int')\ndf_train['target'] = df_train['target'].astype('int')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Repeat with test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace string with float/int\ndf_test['experience'] = df_test['experience'].replace('>20','25')\ndf_test['experience'] = df_test['experience'].replace('<1','0.5')\ndf_test['experience'] = df_test['experience'].astype('float')\ndf_test['last_new_job'] = df_test['last_new_job'].replace('>4','5')\ndf_test['last_new_job'] = df_test['last_new_job'].replace('never','0')\n# Impute/fill NaN\ndf_test['gender'] = df_test['gender'].replace(np.nan, 'unknown')\ndf_test['enrolled_university'] = df_test['enrolled_university'].replace(np.nan, 'unknown')\ndf_test['education_level'] = df_test['education_level'].replace(np.nan, 'unknown')\ndf_test['major_discipline'] = df_test['major_discipline'].replace(np.nan, 'unknown')\ndf_test['education_level'] = df_test['education_level'].replace(np.nan, 'unknown')\ndf_test['experience'] = df_test['experience'].fillna(value = df_test['experience'].median())\ndf_test['company_size'] = df_test['company_size'].fillna(value = df_test['company_size'].value_counts().index[0])\ndf_test['company_type'] = df_test['company_type'].replace(np.nan, 'unknown')\ndf_test['last_new_job'] = df_test['last_new_job'].fillna(value = df_test['last_new_job'].median()).astype('int')\n# df_test['target'] = df_test['target'].astype('int')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Treat continuous and categorical variables separately and then combine"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Continuous variables\nfeatures = ['city_development_index', 'training_hours', 'experience', 'last_new_job']\nX_train_con = df_train[features]\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n# Since we're using more than one feature, let's scale our features\nscaler = StandardScaler()\nX_train_con_scaled = scaler.fit_transform(df_train[features])\ncont_columns = X_train_con.columns\nX_train_con_df = pd.DataFrame(X_train_con_scaled, columns=cont_columns, index=X_train_con.index)\n\ny_train = df_train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical variables\ncat_variables = ['relevent_experience','enrolled_university','education_level','major_discipline','company_size','company_type']\nX_train_cat = df_train[cat_variables]\n\nohe = OneHotEncoder(sparse=False, drop='first')\nohe.fit(X_train_cat) \ncats = ohe.transform(X_train_cat)\n\ncolumns = ohe.get_feature_names(cat_variables)\nX_train_cat_df = pd.DataFrame(cats, columns=columns, index=X_train_cat.index)\n\n# Combine Con and Cat\nX_train = pd.concat([X_train_con_df,X_train_cat_df], axis='columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Repeat for test set\n\n# y_test = df_test['target']\n\n# Continuous variables\nfeatures = ['city_development_index', 'training_hours', 'experience', 'last_new_job']\nX_test_con = df_test[features]\n\n\n# Since we're using more than one feature, let's scale our features\nscaler = StandardScaler()\nX_test_con_scaled = scaler.fit_transform(df_test[features])\ncont_columns = X_test_con.columns\nX_test_con_df = pd.DataFrame(X_test_con_scaled, columns=cont_columns, index=X_test_con.index)\n\n# y_test = df_test['target']\n\n# Categorical variables\ncat_variables = ['relevent_experience','enrolled_university','education_level','major_discipline','company_size','company_type']\nX_test_cat = df_test[cat_variables]\n\nohe = OneHotEncoder(sparse=False, drop='first')\nohe.fit(X_test_cat) \ncats = ohe.transform(X_test_cat)\n\ncolumns = ohe.get_feature_names(cat_variables)\nX_test_cat_df = pd.DataFrame(cats, columns=columns, index=X_test_cat.index)\n\n# Combine Con and Cat\nX_test = pd.concat([X_test_con_df,X_test_cat_df], axis='columns')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handle imbalanced classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import ADASYN\nX_train, y_train = ADASYN(random_state=42).fit_sample(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model: Decision Trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\ndecisiontree_2 = DecisionTreeClassifier()\ndecisiontree_2.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth = [3,4,5,6]\nmin_samples_leaf = [0.04,0.06,0.08]\nmax_features = [0.2,0.4,0.6,0.8]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define grid search\ngrid = dict(max_depth = max_depth, min_samples_leaf = min_samples_leaf, max_features = max_features)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=decisiontree_2, param_grid=grid, n_jobs=-1, cv=5, scoring='recall',error_score=0)\ngrid_result = grid_search.fit(X_train, y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select the best estimator from grid search results\ndecisiontree_3 = grid_search.best_estimator_\ndecisiontree_3.fit(X_train,y_train)\npred = decisiontree_3.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare for submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission = df_test[['enrollee_id']].copy()\nmy_submission['predict'] = pred\nmy_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}