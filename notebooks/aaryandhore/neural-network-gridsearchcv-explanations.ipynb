{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing Dataset"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv(\"../input/insurance/insurance.csv\")\nprint(dataset.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting dataset into independant variables (age, sex, etc) and dependant variables (insurance costs)."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"X = dataset.iloc[:, 0:6].values\ny = dataset.iloc[:, 6:7].values","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"Age     Gender  BMI     Kids    Smoker  Region    \")\ns = [[str(e) for e in row] for row in X[0:5]]\nlens = [max(map(len, col)) for col in zip(*s)]\nfmt = '\\t'.join('{{:{}}}'.format(x) for x in lens)\ntable = [fmt.format(*row) for row in s]\nprint ('\\n'.join(table))\n\nprint(\"\\n\")\n\nprint(\"Charge\")\ns = [[str(e) for e in row] for row in y[0:5]]\nlens = [max(map(len, col)) for col in zip(*s)]\nfmt = '\\t'.join('{{:{}}}'.format(x) for x in lens)\ntable = [fmt.format(*row) for row in s]\nprint ('\\n'.join(table))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"A neural network (and all other algorithms) require numbers/data, and cannot accept strings and words as it will break the network. Therefore, any of the dependant variables that include words will be replaced with numbers."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlabelencoder_X_1 = LabelEncoder()\nX[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\nlabelencoder_X_4 = LabelEncoder()\nX[:, 4] = labelencoder_X_4.fit_transform(X[:, 4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"shortenedX = X[0:10]\nprint(\"Age     Gender  BMI     Kids    Smoker  Region   \")\ns = [[str(e) for e in row] for row in shortenedX]\nlens = [max(map(len, col)) for col in zip(*s)]\nfmt = '\\t'.join('{{:{}}}'.format(x) for x in lens)\ntable = [fmt.format(*row) for row in s]\nprint ('\\n'.join(table))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, we didn't encode region. That is because we cannot tranform it into a range of values. For example, if we encode it and southwest becomes 0 and southeast becomes 1, to the network it seems that southeast (1) is 'more' than southeast (0). We cannot quantify location; we have to represent it in a way that will not give any bias to the neural network for it to seem that one network is more important than another. This can be done by using dummy variables, and having each location be its own variable. Furthermore, we only need 3 locational variables beause if a person is not in 3 locations, we know it belongs in the last location. We must always remove at least one variable when having dummy variables to avoid the dummy variable trap. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\ncolumnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [5])], remainder='passthrough')\nX = np.array(columnTransformer.fit_transform(X), dtype = np.str)\nX = X[:, 1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"shortenedX = X[0:10]\nprint(\"NW      SE      SW      Age     Gender  BMI     Kids    Smoker     \")\ns = [[str(e) for e in row] for row in shortenedX]\nlens = [max(map(len, col)) for col in zip(*s)]\nfmt = '\\t'.join('{{:{}}}'.format(x) for x in lens)\ntable = [fmt.format(*row) for row in s]\nprint ('\\n'.join(table))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the neural network we require training data and testing data. A standard split is used: 80/20. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In our data we have some datapoints that have a range from 0 to 1 (gender/smoker/region) and other having large ranges of values (age/bmi/chlidren). Because some values in the table are much larger, they will have a much larger impact on the network. To remove this bias, all the values will be scaled to the same range (in this case 0-1). Since this is done to independant variables, it will also be done to the dependant variables. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler((0,1))\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\ny_train = sc.fit_transform(y_train)\ny_test = sc.transform(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"shortenedX_train = X_train[0:5]\nprint(\"NW      SE      SW      Age                     Gender  BMI                     Kids    Smoker     \")\ns = [[str(e) for e in row] for row in shortenedX_train]\nlens = [max(map(len, col)) for col in zip(*s)]\nfmt = '\\t'.join('{{:{}}}'.format(x) for x in lens)\ntable = [fmt.format(*row) for row in s]\nprint ('\\n'.join(table))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"shortenedy_train = y_train[0:5]\nprint(\"Charge\")\ns = [[str(e) for e in row] for row in shortenedy_train]\nlens = [max(map(len, col)) for col in zip(*s)]\nfmt = '\\t'.join('{{:{}}}'.format(x) for x in lens)\ntable = [fmt.format(*row) for row in s]\nprint ('\\n'.join(table))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating the Neural Network"},{"metadata":{},"cell_type":"markdown","source":"We will now be creating our neural network model. The first thing done is import libraries and initializing the classifier/model. Then, each layer of the network is added with the specified number of nuerons (units) and the activation function used. It is standard to use rectified linear unit for all hidden layers. After the architecture is added, the network is compiled. Since this is regression problem, the loss function and metrics (scoring method) will relate to distance from the networks guess to the actual value. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nclassifier = Sequential()\n    \n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 128, activation = 'relu'))\n    \n# Adding the second hidden layer\nclassifier.add(Dense(units = 64, activation = 'relu'))\n    \nclassifier.add(Dense(units = 32, activation = 'relu'))\n    \n# Adding the output layer\nclassifier.add(Dense(units = 1, activation = 'linear'))\n    \n# Compiling the ANN\nclassifier.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluating the Neural Network"},{"metadata":{},"cell_type":"markdown","source":"Since the classifier is built, we can now train the classifier on the data. I have arbitraliy chosen a batch_size and epoch number without any real rhyme or reason, but I will show how to choose the best batch size, epoch, and optimizer later on. The .fit() function returns an object that has many attributes related to the network (loss, scoring, etc). This allows us gain information on how the model performed. "},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"History = classifier.fit(x = X_train, y = y_train, batch_size = 128, epochs = 150, verbose = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the history object, the mean absolute error was graphed over the epochs, and we can see how the model's performance increased as it passed through each epoch. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(History.history['mean_absolute_error'])\nplt.title('Loss Function Over Epochs')\nplt.ylabel('MAE value')\nplt.xlabel('No. epoch')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also use the .predict() method to see how close the predictions of the neural network come to the actual charges. One thing to remember is that y_train and the outputs of the classifier are all scaled using the MinMaxScaler() object. To compare values with this transformation would be hard; therefore, we can inverse the transformation to intuitively see how well our model did.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(X_test)\n\ny_predInverse = sc.inverse_transform(y_pred)\ny_testInverse = sc.inverse_transform(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"combinedArray = np.column_stack((y_testInverse[0:10],y_predInverse[0:10]))\nprint(\"Actual Charge   Predicted Charge\")\ns = [[str(e) for e in row] for row in np.around(combinedArray, 2)]\nlens = [max(map(len, col)) for col in zip(*s)]\nfmt = '\\t'.join('{{:{}}}'.format(x) for x in lens)\ntable = [fmt.format(*row) for row in s]\nprint ('\\n'.join(table))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Improving the Neural Network using GridSearchCV"},{"metadata":{},"cell_type":"markdown","source":"In order to use GridSearchCV easily, we need to create a function that would assemble the achitecture of the neural network. Nothing much is new, except including an optimizer parameter as it is a parameter we might want to tune/change, and it is the only hyperparameter that has to be changed in the architecture of the network. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def buildModel(optimizer):\n    # Initialising the ANN\n    classifier = Sequential()\n    \n    # Adding the input layer and the first hidden layer\n    classifier.add(Dense(units = 128, activation = 'relu'))\n    \n    # Adding the second hidden layer\n    classifier.add(Dense(units = 64, activation = 'relu'))\n    \n    \n    classifier.add(Dense(units = 32, activation = 'relu'))\n    \n    # Adding the output layer\n    classifier.add(Dense(units = 1, activation = 'linear'))\n    \n    # Compiling the ANN\n    classifier.compile(loss='mean_absolute_error', optimizer=optimizer, metrics=['mean_absolute_error'])\n    \n    return classifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now is where we use GridSearchCV. First what we want to do is create a KerasRegressor classifier which will be used in GridSearch. Next, we will create a dictionary or list of parameters we want to tune and what values we want to tune it to. I put 4 options for batch_size, two options for epochs, and two different optimizer functions. Then, we will create an object of the GridSearchCV that is callibrated to look for the best parameters. After that, we are simply having that object run and find the best parameters, which will return itself with some useful information for us to find the best parameters.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV \nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n\nclassifier = KerasRegressor(build_fn = buildModel)\n#What hyperparameter we want to play with\nparameters = {'batch_size': [16, 32, 64, 128],\n              'epochs': [100, 150],\n              'optimizer': ['adam', 'rmsprop']}\ngrid_search = GridSearchCV(estimator = classifier,\n                           param_grid = parameters,\n                           scoring = 'neg_mean_absolute_error',\n                           cv = 5)\ngrid_search = grid_search.fit(X_train, y_train, verbose = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Grid Search has many attributes but the one below shows the best parameters that the object found. This allows us to plug in the best numbers (out of the ones we put in the dictionary) that will result in the best model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"best_parameters = grid_search.best_params_\nbest_score = grid_search.best_score_\n\nprint(\"Best Parameters: \" + str(best_parameters))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"bestClassifier = buildModel('adam')\nHistoryBest = bestClassifier.fit(x = X_train, y = y_train, batch_size = 16, epochs =150 , verbose = 0)\nplt.plot(History.history['mean_absolute_error'], label='Initial Parameters')\nplt.plot(HistoryBest.history['mean_absolute_error'], label='GridSearchCV Best Parameters')\nplt.title('Loss Function Over Epochs')\nplt.ylabel('MAE value')\nplt.xlabel('No. epoch')\nplt.legend(loc=\"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen from the graph above, the loss is lower for the classifier with the ideal parameters. Another to check the performance of the model is to check how far off the model is from its predictions to the actual values, and as we can see the model with the best parameters has a lower error. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error \n\nprint(\"Initial Classifier MAE: \" + str(mean_absolute_error(y_test, y_pred, sample_weight=None, multioutput='uniform_average')))\nprint(\"Best Classifier MAE: \" + str(mean_absolute_error(y_test, bestClassifier.predict(X_test), sample_weight=None, multioutput='uniform_average')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We previously saw the predicted values of the inital neural network (with arbitrarily chosen hyperparameters) and they were decently close. However, the new classifier is much more precise and accurate as you can see from a small sample of the predicted values and the actual charge. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"y_predBestInverse = sc.inverse_transform(bestClassifier.predict(X_test))\n\ncombinedArray = np.column_stack((y_testInverse[0:10],y_predInverse[0:10], y_predBestInverse[0:10]))\nprint(\"Actual Charge   Initial         Best \")\ns = [[str(e) for e in row] for row in np.around(combinedArray, 2)]\nlens = [max(map(len, col)) for col in zip(*s)]\nfmt = '\\t'.join('{{:{}}}'.format(x) for x in lens)\ntable = [fmt.format(*row) for row in s]\nprint ('\\n'.join(table))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}