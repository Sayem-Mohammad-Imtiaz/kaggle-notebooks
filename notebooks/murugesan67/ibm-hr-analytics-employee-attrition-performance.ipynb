{"nbformat":4,"metadata":{"language_info":{"nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.1","pygments_lexer":"ipython3","mimetype":"text/x-python","file_extension":".py","name":"python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat_minor":1,"cells":[{"metadata":{"_cell_guid":"4dc53bf0-20b1-4517-bf42-01557b3fcf59","_uuid":"f1a1a3bbfa47a307b5b97344f419a2dae3e7fc29"},"outputs":[],"execution_count":null,"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the \n#input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input/\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","cell_type":"code"},{"metadata":{"_cell_guid":"1b0b09d6-3328-4ee8-bb2e-a365f1511d29","_uuid":"d0f71facf178f26a864bedcc24a82b9c913fcbb7","collapsed":true},"outputs":[],"execution_count":null,"source":"#Reading the Data set \ndf = pd.read_csv(\"../input/WA_Fn-UseC_-HR-Employee-Attrition.csv\")","cell_type":"code"},{"metadata":{"_cell_guid":"917ad853-69d3-4a22-9cad-64541a81f1cf","_uuid":"2ce08d1321334043e1c954788027e945bf5e5f8e"},"outputs":[],"execution_count":null,"source":"#Quick Data Exploration\n#few top rows can be read by using the function head()\n\ndf.head(10)","cell_type":"code"},{"metadata":{"_cell_guid":"2626272e-2793-488f-8b9f-1c9b7415cfc5","_uuid":"ebf8b02e66a3eeacf63d5c47fe1b36af221b4b24"},"outputs":[],"execution_count":null,"source":"#The summary of numerical fields can be looked by using describe() \n\ndf.describe()","cell_type":"code"},{"metadata":{"_cell_guid":"4e9b2644-b0d0-44d9-8d58-2f5349b556aa","_uuid":"bd6aaf3fe3c4c5acc9901d607118465cb0e757f2"},"outputs":[],"execution_count":null,"source":"#Check missing values in the dataset\ndf.apply(lambda x: sum(x.isnull()),axis=0)","cell_type":"code"},{"metadata":{"_cell_guid":"cd2f4b45-79c8-4254-8918-b0b517f79245","_uuid":"c9f06aab6d5e157253f5434cd44b75f1cc58d3d9","collapsed":true},"outputs":[],"execution_count":null,"source":"#There is no missing value in data","cell_type":"code"},{"metadata":{"_cell_guid":"e46d5900-a3e9-46e5-947b-be9f369859f1","_uuid":"77fdc3b7ec9e84fb6dc58a8b9f641fe19ecd1287"},"outputs":[],"execution_count":null,"source":"#Let’s analyze Age first. Since the extreme values are practically possible, i.e. some people might \n#leave because of Age. So instead of treating them as outliers, let’s try a log transformation to \n#nullify their effect:\ndf['DailyRate'].hist(bins=20)","cell_type":"code"},{"metadata":{"_cell_guid":"be120f5e-1418-4b1d-915d-cf3d85ad5fad","_uuid":"32230096f9736818f17c7ab53459bc6522f957be"},"outputs":[],"execution_count":null,"source":"df['Age'].hist(bins=20)","cell_type":"code"},{"metadata":{"_cell_guid":"9efa8b11-59c7-4c21-b7f8-560e2b38d958","_uuid":"29367e42b4ae59628b7716d67a686f7799c724e3"},"outputs":[],"execution_count":null,"source":"df['Age_log'] = np.log(df['Age'])\ndf['Age_log'].hist(bins=20)","cell_type":"code"},{"metadata":{"_cell_guid":"483ae078-61b5-4ab1-bc65-ba21f35259bc","_uuid":"210a71e87d2a6bbbadea46b0dbdb93b37771ee01"},"outputs":[],"execution_count":null,"source":"df['DailyRate_log'] = np.log(df['DailyRate'])\ndf['DailyRate_log'].hist(bins=20)","cell_type":"code"},{"metadata":{"_cell_guid":"b0682d44-a9b1-4a7f-be89-3498a9d9ae13","_uuid":"5f30f545678a3f95f671b053758a9dcf42c69226"},"outputs":[],"execution_count":null,"source":"#Building a Predictive Model in Python\n#Since, sklearn requires all inputs to be numeric, all the categorical variables to be converted \n#into numeric by encoding the categories. \n\nfrom sklearn.preprocessing import LabelEncoder\n\nvar_mod = ['Attrition', 'BusinessTravel', 'Department', 'Department','EducationField','Gender','JobRole','MaritalStatus','Over18','OverTime']\n\n \nle = LabelEncoder()\n\nfor i in var_mod:\n    df[i] = le.fit_transform(df[i])\ndf.dtypes ","cell_type":"code"},{"metadata":{"_cell_guid":"41311fca-be66-4b47-b5f9-1fdad0e7f991","_uuid":"d370127a5aebb6ba69314137db0041b857c98fdd"},"outputs":[],"execution_count":null,"source":"#The required module is imported, then we will define a generic #classification function, which takes \n#a model #as input and determines the #Accuracy and Cross-Validation scores.\n\n#Import models from scikit learn module:\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.cross_validation import KFold   #For K-fold cross validation\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn import metrics","cell_type":"code"},{"metadata":{"_cell_guid":"1ce7563d-aa50-4cf5-8d0c-da6ccf5ed2b8","_uuid":"41221123f8d156036cda26d6cc72eb67b9ee3946","collapsed":true},"outputs":[],"execution_count":null,"source":"##Generic function for making a classification model and accessing performance:\ndef classification_model(model, data, predictors, outcome):\n  #Fit the model:\n  model.fit(data[predictors],data[outcome])\n  \n  #Make predictions on training set:\n  predictions = model.predict(data[predictors])\n  \n  #Print accuracy\n  accuracy = metrics.accuracy_score(predictions,data[outcome])\n  print (\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n  #Perform k-fold cross-validation with 5 folds\n  kf = KFold(data.shape[0], n_folds=5)\n  error = []\n  for train, test in kf:\n    # Filter training data\n    train_predictors = (data[predictors].iloc[train,:])\n    \n    # The target we're using to train the algorithm.\n    train_target = data[outcome].iloc[train]\n    \n    # Training the algorithm using the predictors and target.\n    model.fit(train_predictors, train_target)\n    \n    #Record error from each cross-validation run\n    error.append(model.score(data[predictors].iloc[test,:], data[outcome].iloc[test]))\n    \n    result = \"{0:.3%}\".format(np.mean(error))\n    print (\"Cross-Validation Score : %s\" % \"{0:.3%}\".format(np.mean(error)))\n\n\n  #Fit the model again so that it can be refered outside the function:\n  model.fit(data[predictors],data[outcome]) \n","cell_type":"code"},{"metadata":{"_cell_guid":"84a18140-e1ea-4201-b9f6-10acb397bb37","_uuid":"89de31c48f110e2638524fd1562664ca4e66d4f3"},"outputs":[],"execution_count":null,"source":"#Logistic Regression\n\n#Let’s make our first Logistic Regression model. One way would be to take #all the variables into the\n#model but this might result in overfitting #(don’t worry if you’re unaware of this terminology yet).\n#In simple words, #taking all variables might result in the model understanding complex #relations \n#specific to the data and will not generalize well\n\noutcome_var = 'Attrition'\nmodel = LogisticRegression()\npredictor_var = ['JobSatisfaction']\nclassification_model(model, df,predictor_var,outcome_var)\n","cell_type":"code"},{"metadata":{"_cell_guid":"aebf4d97-2ae5-40be-a95d-dd675a16673a","_uuid":"620b344d6556d4fd94ffb54c2bc940cf6504ed3a"},"outputs":[],"execution_count":null,"source":"#We can try different combination of variables:\n\nmodel = LogisticRegression()\npredictor_var = ['Age_log','BusinessTravel','DailyRate_log','Department','DistanceFromHome','Education','EducationField','EmployeeCount','EmployeeNumber','EnvironmentSatisfaction','Gender','HourlyRate',\t'JobInvolvement','JobLevel','JobRole','JobSatisfaction','MaritalStatus','MonthlyIncome','MonthlyRate','NumCompaniesWorked','Over18','OverTime','PercentSalaryHike','PerformanceRating','RelationshipSatisfaction','StandardHours','StockOptionLevel','TotalWorkingYears','TrainingTimesLastYear','WorkLifeBalance','YearsAtCompany','YearsInCurrentRole','YearsSinceLastPromotion','YearsWithCurrManager']\nclassification_model(model, df,predictor_var,outcome_var)","cell_type":"code"},{"metadata":{"_cell_guid":"24895fde-7e25-4475-9379-a14c097a0850","_uuid":"b5f57db4e42d2923acc31c23f87385e4cb36a4c1"},"outputs":[],"execution_count":null,"source":"#Decision Tree\n\n#Decision tree is another method for making a predictive model. It is known #to provide higher accuracy\n#than logistic regression model\n\nmodel = DecisionTreeClassifier()\npredictor_var = ['Age_log','BusinessTravel','DailyRate_log','Department','DistanceFromHome','Education','EducationField','EmployeeCount','EmployeeNumber','EnvironmentSatisfaction','Gender','HourlyRate',\t'JobInvolvement','JobLevel','JobRole','JobSatisfaction','MaritalStatus','MonthlyIncome','MonthlyRate','NumCompaniesWorked','Over18','OverTime','PercentSalaryHike','PerformanceRating','RelationshipSatisfaction','StandardHours','StockOptionLevel','TotalWorkingYears','TrainingTimesLastYear','WorkLifeBalance','YearsAtCompany','YearsInCurrentRole','YearsSinceLastPromotion','YearsWithCurrManager']\nclassification_model(model, df,predictor_var,outcome_var)","cell_type":"code"},{"metadata":{"_cell_guid":"f44aeead-25bf-4127-b51f-36dbd09eeadd","_uuid":"5de06710a2595054458834b2c8842409dd18c451"},"outputs":[],"execution_count":null,"source":"#Random Forest\n#Random forest is another algorithm for solving the classification problem.\n\nmodel = RandomForestClassifier(n_estimators=100)\npredictor_var = ['Age_log','BusinessTravel','DailyRate_log','Department','DistanceFromHome','Education','EducationField','EmployeeCount','EmployeeNumber','EnvironmentSatisfaction','Gender','HourlyRate',\t'JobInvolvement','JobLevel','JobRole','JobSatisfaction','MaritalStatus','MonthlyIncome','MonthlyRate','NumCompaniesWorked','Over18','OverTime','PercentSalaryHike','PerformanceRating','RelationshipSatisfaction','StandardHours','StockOptionLevel','TotalWorkingYears','TrainingTimesLastYear','WorkLifeBalance','YearsAtCompany','YearsInCurrentRole','YearsSinceLastPromotion','YearsWithCurrManager']\nclassification_model(model, df,predictor_var,outcome_var)","cell_type":"code"},{"metadata":{"_cell_guid":"e45c1d3e-744b-459c-9b69-d2e934472e74","_uuid":"db187d015fe053999ce830c361e071a89cd37899"},"outputs":[],"execution_count":null,"source":"#Create a series with feature importances:\nfeatimp = pd.Series(model.feature_importances_, index=predictor_var).sort_values(ascending=False)\nprint (featimp)","cell_type":"code"},{"metadata":{"_cell_guid":"f060f8c9-67b6-4e3d-a11d-429768c9fcd2","_uuid":"61283de327d388cb7219355bc1bd8acc2041a611"},"outputs":[],"execution_count":null,"source":"#Let’s use the top 5 variables for creating a model. Also, we will modify the parameters of random\n#forest model a little bit:\n\nmodel = RandomForestClassifier(n_estimators=25, min_samples_split=25, max_depth=7, max_features=1)\npredictor_var = ['MonthlyIncome','HourlyRate','TotalWorkingYears','EmployeeNumber','OverTime']\nclassification_model(model, df,predictor_var,outcome_var)","cell_type":"code"},{"metadata":{"_cell_guid":"bf1695bc-f9e2-43c9-a7d9-5e509aebcccb","_uuid":"3aeacc8a0bfa1e2e2cbfd52b539558fe39511d41","collapsed":true},"outputs":[],"execution_count":null,"source":"#Notice that although accuracy reduced, but the cross-validation score is improving showing that the \n#model is generalizing well. Remember that #random forest models are not exactly repeatable. Different\n#runs will result in slight variations because #of randomization. But the output should stay in the\n#ballpark.You would have noticed that even after some #basic parameter tuning on random forest, we have\n#reached a cross-validation accuracy only slightly better #than the original logistic regression model. ","cell_type":"code"}]}