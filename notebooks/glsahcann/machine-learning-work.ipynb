{"cells":[{"metadata":{"_uuid":"51e0e76ad24a41a0252eb8bf624d4a2b3bdc31c0"},"cell_type":"markdown","source":"**Hi. I am learning machine learning and want to share what i learn. I hope like this kernel.**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"**Linear Regression**\n\nWe are going to start with linear regression.\n\nFirstly i am going to create a dataset and set up a model of basic linear regression.\n\nLets start.."},{"metadata":{"trusted":true,"_uuid":"1729ccbb93bc7b51969c481838be0e6bea687945"},"cell_type":"code","source":"#Create a simple dataset from dictionary.\ndictionary_1 = {\"dogru_soru_sayisi\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\n              \"puan\":[10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200]}\ndata_1 = pd.DataFrame(dictionary_1)\ndata_1.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e4a2e091eace7afbd2f8630b18ed814439a97a4"},"cell_type":"code","source":"#Visualize the data\nplt.scatter(data_1.dogru_soru_sayisi,data_1.puan)\nplt.xlabel(\"dogru_soru_sayisi\")\nplt.ylabel(\"puan\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"123d39bbddcdddd2cba4ba0f6ba257a64a30dda9"},"cell_type":"markdown","source":"Nice...\n\nNow we make linear regression."},{"metadata":{"trusted":true,"_uuid":"f609ff31b89f1bed5df891561020d0a193fb2f3c"},"cell_type":"code","source":"#Lets look a type\ntype(data_1.dogru_soru_sayisi)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce25d0b7ec9611e92f8b13ee8dffada38c7d486c"},"cell_type":"code","source":"#Want a array\nx = data_1.dogru_soru_sayisi.values\ny = data_1.puan.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea68e58cdf25e7024f18c3c70247bb2a5e9bd8a3"},"cell_type":"code","source":"type(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c22c6316a03c072ce8da47190f9155d92e1c0d44"},"cell_type":"code","source":"#How we x of shape?\nx.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a502ef9e0be0e8cdf42f1e78948b69b9e81180a1"},"cell_type":"code","source":"#Want shape (20,1)\nx = data_1.dogru_soru_sayisi.values.reshape(-1,1)\ny = data_1.puan.values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e718e90a62436acb05c632be5080dfbba182c62"},"cell_type":"code","source":"x.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22ff3cd4deb8f3548ae823373ba5d8cf8bee688d"},"cell_type":"markdown","source":"We organized x and y."},{"metadata":{"trusted":true,"_uuid":"6a6e28712fd960673d8a8f87b272ee76858a222a"},"cell_type":"code","source":"#We need sklearn library\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5306e77907add49d8e6630769fe2c0356e566886"},"cell_type":"markdown","source":"A linear regression line has an equation of the form Y = a + bX, where X is the explanatory variable and Y is the dependent variable. The slope of the line is b, and a is the intercept (the value of y when x = 0)."},{"metadata":{"trusted":true,"_uuid":"efc38f77afd8e7b43cd89fb135d66cd54aa7b888"},"cell_type":"code","source":"#We should make fit\nlr.fit(x,y)\n#Visualize\nplt.scatter(x,y)\nplt.xlabel(\"dogru_soru_sayisi\")\nplt.ylabel(\"puan\")\n#We should make predict\ny_head = lr.predict(x)\n#Visualize\nplt.plot(x,y_head, color = \"red\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b90be3cee6e2e6a0e14803964c3c247e9a765b85"},"cell_type":"markdown","source":"**Polynomial Linear Regression**\n\nIn some cases, the relationship between variables may not be linear. In these cases, polynomial regression is used."},{"metadata":{"trusted":true,"_uuid":"ffcded38037b68f69e19690470b255c3a7fb1630"},"cell_type":"code","source":"#Create a simple dataset from dictionary.\ndictionary_2 = {\"enerji\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\n                \"guc\":[10,22,34,46,56,61,70,83,97,100,101,104,109,109,118,120,123,123,123,124]}\ndata_2 = pd.DataFrame(dictionary_2)\ndata_2.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4903aff06be3186620e483ce8dc5b5af20d395b3"},"cell_type":"markdown","source":"Sometimes, a plot of the residuals versus a predictor may suggest there is a nonlinear relationship. One way to try to account for such a relationship is through a polynomial regression model. Such a model for a single predictor.\n\nY=β0+β1*X+β2*X^2+…+βh*X^h\n\nwhere h is called the degree of the polynomial. For lower degrees, the relationship has a specific name (i.e., h = 2 is called quadratic, h = 3 is called cubic, h = 4 is called quartic, and so on). Although this model allows for a nonlinear relationship between Y and X, polynomial regression is still considered linear regression since it is linear in the regression coefficients"},{"metadata":{"trusted":true,"_uuid":"c336669cfcc215f5ed687c17fad104d70e4cd0cb"},"cell_type":"code","source":"#How we make polynomial regression on python?\nx2 = data_2.enerji.values.reshape(-1,1)\ny2 = data_2.guc.values.reshape(-1,1)\n#Library\nfrom sklearn.linear_model import LinearRegression\nlr2 = LinearRegression() \n#Fit\nlr2.fit(x2,y2)\n#Visualize\nplt.scatter(x2,y2, color = \"blue\") #our values\nplt.ylabel(\"guc\")\nplt.xlabel(\"enerji\")\n#Predict\ny_head2 = lr.predict(x2)\n#Visualize\nplt.plot(x2, y_head2 , color=\"red\" )  #that does not represent our values well\n\n#Library\nfrom sklearn.preprocessing import PolynomialFeatures\npolynomial_regression = PolynomialFeatures(degree = 4)  #degree of the polynomial\n#Fit\nx_polynomial = polynomial_regression.fit_transform(x2)\nlr3 = LinearRegression()\nlr3.fit(x_polynomial,y2)\n#Predict\ny_head3 = lr3.predict(x_polynomial)\n#Visualize\nplt.plot(x2,y_head3, color = \"green\")  #this is better\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"321c0123c51e4e1bd00fda46e9bf4ebfda2c908d"},"cell_type":"markdown","source":"**Decision Tree Regression**\n\nDecision Trees are a type of Supervised Machine Learning, where data is constantly divided according to a certain parameter. The tree can be explained by two entities, decision nodes and leaves. Leaves are decisions or final results. And the decision nodes show where the data is divided."},{"metadata":{"trusted":true,"_uuid":"ef4dec85564486dc055b467c16ef5f3b8e42fc37"},"cell_type":"code","source":"#How we use decision tree regression on python?\n#We used the data(data_1) we created above.\nx = data_1.dogru_soru_sayisi.values.reshape(-1,1)\ny = data_1.puan.values.reshape(-1,1)\n#Library\nfrom sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor()\n#Fit\ndtr.fit(x,y)\n#Step\nx_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\n#Predict\ny_head4 = dtr.predict(x_)\n#Visualize\nplt.scatter(x,y,color = \"red\")\nplt.plot(x_,y_head4, color = \"green\")\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78198d1f6408a83475a419c911ed0996ec9024e5"},"cell_type":"markdown","source":"**Logistic Regression Classification**\n\nThe logistic regression predicts the likelihood of a result that can only have two values.The logistic regression produces a logistic curve that is limited to values between 0 and 1. Logistic regression is similar to a linear regression, except that the curve is constructed using the natural logarithm of the probabilities of the target variable instead of probability."},{"metadata":{"trusted":true,"_uuid":"0d812ac5a889e48dc892cb43e86cbeac57376949"},"cell_type":"code","source":"#We build data frames from csv.\ndf = pd.read_csv(\"../input/voice.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41f4e9dca4f0e9f108ddc2744f8c778e9717cc4b"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67c43c013034e7850a67acc5198a3b3811ea4ae4"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89786fef5e0b738bf2335b21cab37b1d98723a12"},"cell_type":"code","source":"#df = pd.read_csv(\"../input/voice.csv\")\ndf.label.unique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1adc7a29a2b9a7952ca6b7669c4105392911f516"},"cell_type":"markdown","source":"We need to transform the output of the labels we have into 0 and 1."},{"metadata":{"trusted":true,"_uuid":"9d8959be98f2721d773cba750b939f29b5c72145"},"cell_type":"code","source":"df.label = [ 1 if each == \"male\" else 0 for each in df.label]\ndf.label.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b797f35fa2e7e3b9ad3d9f3837c559c46b2564f"},"cell_type":"code","source":"y = df.label.values\nx_df = df.drop([\"label\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"894bf61a28ed6355d105f5eb37b269a164e5addd"},"cell_type":"markdown","source":"**Normalization**\n\nRescaling data to have values between 0 and 1. This is usually called feature scaling. One possible formula to achieve this is:\n**(x - min(x))/(max(x)-min(x))** "},{"metadata":{"trusted":true,"_uuid":"4cc9782f51eb9ae33cdcc7b33757cb07a70c191a"},"cell_type":"code","source":"#Normalization on python\nx = (x_df - np.min(x_df))/(np.max(x_df) - np.min(x_df)).values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"825333dee519a4afadc13897eaf960416ff7c345"},"cell_type":"markdown","source":"We build x_train from %80 of our data and x_test from %20 of our data. X_train will learn and arrive y_train. Later x_test will arrive y_test same way."},{"metadata":{"trusted":true,"_uuid":"9ca23d9b9f0468a7fd4f4e5844bc25e5d36c549e"},"cell_type":"code","source":"#Train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x_train:\",x_train.shape)\nprint(\"x_test:\",x_test.shape)\nprint(\"y_train:\",y_train.shape)\nprint(\"y_test:\",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a3ed047210d195590cd666f2058d9aeb186c0eb"},"cell_type":"markdown","source":"We should first determine the values of w and b."},{"metadata":{"trusted":true,"_uuid":"921bd4940aa8222c89d51f597dbe2659b7ba7870"},"cell_type":"code","source":"#Parameter initialize \ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w , b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da0cdc7213b77941b6570de65a8c55a31db0f051"},"cell_type":"markdown","source":"The sigmoid function ensures that y_head values are between 0 and 1."},{"metadata":{"trusted":true,"_uuid":"9cfdd26880791c069eb1a6165ba994a73a60e977"},"cell_type":"code","source":"#Sigmoid function\ndef sigmoid(z):\n    y_head_ = 1/(1 + np.exp(-z))\n    return y_head_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"156e5335333b758b65cf0aed5a10b22babce0744"},"cell_type":"markdown","source":"We arrive cost values from loss values with forward propagation. We want cost values min. Backward propagation allows the updating of cost values by taking derivative of w and b. We will use this function in the update function"},{"metadata":{"trusted":true,"_uuid":"eb3a297223f53ee3face1586350a8e6b3283c8f6"},"cell_type":"code","source":"def forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head_ = sigmoid(z)\n    loss = -y_train*np.log(y_head_)-(1-y_train)*np.log(1-y_head_)\n    cost = (np.sum(loss))/x_train.shape[1]\n    \n    #backward propagation\n    derivative_weight = (np.dot(x_train,((y_head_-y_train).T)))/x_train.shape[1]\n    derivative_bias = np.sum(y_head_-y_train)/x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55f201a8e371cfc159e3b21b67f54175e0bf2dc1"},"cell_type":"markdown","source":"Learning rate is our learning speed. Number of iteration is our step count. We will do forward backward propagation by the number of iterations. We created cost_list2 to see the graph better."},{"metadata":{"trusted":true,"_uuid":"fb8164efdb6a547e1b81e4ac150e1e1dcdc27092"},"cell_type":"code","source":"#Updating parameters\ndef update(w,b,x_train,y_train, learning_rate, num_iterations):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    #Updating parameters is num_iteration times\n    for i in range(num_iterations):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate*gradients[\"derivative_weight\"]\n        b = b - learning_rate*gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"cost after iteration %i: %f\" %(i,cost))\n            \n            \n    #We update parameters weights and bias\n    parameters = {\"weight\": w, \"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters,gradients,cost_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"973097efd92b351110e7dd4dea0762869245bf21"},"cell_type":"markdown","source":"We will guess with x_test data. We will arrive y_predict values."},{"metadata":{"trusted":true,"_uuid":"b7e527b3ba020c306b7268f6674fc6a55e67447b"},"cell_type":"code","source":"#Prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    y_predict = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head_=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head_=0)\n    for i in range (z.shape[1]):\n        if z[0,i] <= 0.5:\n            y_predict[0,i] = 0\n        else:\n            y_predict[0,i] = 1\n            \n    return y_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"949d7e567049c510eb47321e671686920ddface3"},"cell_type":"markdown","source":"How much the y_predict values match the y_test values?"},{"metadata":{"trusted":true,"_uuid":"26666b22ebdc91d76208498fc3f53c4ba9c96642"},"cell_type":"code","source":"#Logistic regression\ndef logistic_regression(x_train,y_train,x_test,y_test,learning_rate,num_iterations):\n    # initialize\n    dimension = x_train.shape[0]  # that is 30\n    w,b = initialize_weights_and_bias(dimension)\n\n    parameters, gradients, cost_list = update(w,b,x_train,y_train, learning_rate, num_iterations)\n    \n    y_predict_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    \n    # print train/test errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_predict_test - y_test))*100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test, learning_rate = 1, num_iterations= 300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12b9652504b70599a2323ae512789b3259a9e3e7"},"cell_type":"markdown","source":"Logistic Regression with sklearn. It is more easy."},{"metadata":{"trusted":true,"_uuid":"eb4b750f19ecf55ded128e900397a13221343c04"},"cell_type":"code","source":"#Library\nfrom sklearn.linear_model import LogisticRegression\nlogistic_regression = LogisticRegression()\nlogistic_regression.fit(x_train.T,y_train.T)\nprint(\"test accuracy {}\".format(logistic_regression.score(x_test.T,y_test.T)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c29c0c6701abe083529040441ad709522223abf3"},"cell_type":"markdown","source":"That's all for now. I will keep to publish by more learning. Thank you."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}