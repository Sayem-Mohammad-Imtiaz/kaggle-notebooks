{"cells":[{"metadata":{"_uuid":"0a94263c2844bda6d5294e23d38285c756a62ed8"},"cell_type":"markdown","source":"**Hi, i make knn algorithm in the kernel. I hope you like it.**"},{"metadata":{"_uuid":"905cc5ed1d6490c82b6a7082d8895ead2ecd2d74"},"cell_type":"markdown","source":"**KNN**\n\nAccording to this algorithm, which is used in classification, feature extraction during classification is used to look at the closeness of the new individual to be categorized to k of the previous individuals.\nFor example, you want to classify a new element for k = 3. in this case the nearest 3 of the old classified elements are taken. If these elements are included in the class, the new element is also included in that class. The euclide distance can be used in the distance calculation."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99bcf7fbfeb6423500c5e6ac257a4243059fabb9"},"cell_type":"markdown","source":"Firstly append dataframe."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# import data\ndata = pd.read_csv(\"../input/column_2C_weka.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"594e3e7d5e6d772c4ea0ff7b86d3007320c300a2"},"cell_type":"markdown","source":"Then examine the data set. We should get information about our data."},{"metadata":{"trusted":true,"_uuid":"2b532a9385e2d2b2132d84ba20fac76bc43cd857"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f0c220e6b4b668addc80d9f9ea018efba05357c"},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81b599d5e850d19c530abf43d489739305ea6636"},"cell_type":"code","source":"data[\"class\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07ab2d8bc1fca9b93365cbb8d842e411bcb7ee5b"},"cell_type":"markdown","source":"Class column has two sample: Abnormal and Normal. We use binary classification."},{"metadata":{"trusted":true,"_uuid":"e0c86f3a7d6bf7dda5a39960ee2676813f685284"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4ae4689eb82fd4ef836c5c71f7f75be6bafb067"},"cell_type":"markdown","source":"We don't need to cleaning data because samples full and types are float."},{"metadata":{"trusted":true,"_uuid":"f54788e331d8861bf4cac871449bc66035f6cf21"},"cell_type":"code","source":"# create new two datas\nA = data[data[\"class\"] == \"Abnormal\"]\nN = data[data[\"class\"] == \"Normal\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c511809d707b89cff7c3771fe7ec06b2081960e"},"cell_type":"code","source":"# visualize\nplt.scatter(A.pelvic_radius,A.sacral_slope,color = \"red\",label = \"Abnormal\")\nplt.scatter(N.pelvic_radius,N.sacral_slope,color = \"blue\",label = \"Normal\")\nplt.xlabel(\"pelvic_radius\")\nplt.ylabel(\"sacral_slope\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a6eabfc8c4cb75f69e811ce1e0c7405c44a5c21"},"cell_type":"markdown","source":"Class column's type should be integer or categorical."},{"metadata":{"trusted":true,"_uuid":"5426066192260cd6ef0de29cb9dfaa5dd49717af"},"cell_type":"code","source":"# class column's type change to integer.\ndata[\"class\"] = [1 if each == \"Abnormal\" else 0 for each in data[\"class\"] ]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b232c9d469373b555c06dc95e35c83c3aee67a4f"},"cell_type":"markdown","source":"Let's crate x and y."},{"metadata":{"trusted":true,"_uuid":"7b8cfc209cfc1c3e3b729f5aabc3d9a9140ba1f3"},"cell_type":"code","source":"y = data[\"class\"].values\nx_data = data.loc[:,data.columns != 'class']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddd7a64816b9822e4e55b814585c849e6acf27e9"},"cell_type":"markdown","source":"Normalization:\nRescaling data to have values between 0 and 1. This is usually called feature scaling. One possible formula to achieve this is: (x - min(x))/(max(x)-min(x))"},{"metadata":{"trusted":true,"_uuid":"c282de7484dd225d07febb242ad77188a9bc4294"},"cell_type":"code","source":"# normalization\nx = (x_data - np.min(x_data))/(np.max(x_data) - np.min(x_data))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92312942664a740d971f8f4aa30f2485ee58826a"},"cell_type":"markdown","source":"We should split dataframe as train and test. Train part will learn and test part will prediction."},{"metadata":{"trusted":true,"_uuid":"bd7d521c88c4c980c5afb6129c5cd0f0fcc66859"},"cell_type":"code","source":"# train test split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82a731c032b33691506467a242e3ceb8a1d29799"},"cell_type":"markdown","source":"Let's create KNN model."},{"metadata":{"trusted":true,"_uuid":"ca03798f7630748c3fab0f5cc7d5d41081e33113"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)  #n_neighbors = k\nknn.fit(x_train,y_train)  # the model is creating\nknn.predict(x_test)  # prediction ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"423046721b3a58a5f440031873ec9c8300c3ce2d"},"cell_type":"markdown","source":"How successful we estimate?"},{"metadata":{"trusted":true,"_uuid":"21f2b49d7a540bd122fc9011dda5c08e15fd083a"},"cell_type":"code","source":"print(\"{} nn score:{}\".format(3,knn.score(x_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22b8536a7e3f2a93454fe9c474afc6369e8480d3"},"cell_type":"markdown","source":"What should be best successfully k value?"},{"metadata":{"trusted":true,"_uuid":"7691fb742c7f6bee3e9d94a3d32a0f05ac1df2e5"},"cell_type":"code","source":"# find k value\nscore_list = []  # we store the values we find in this list.\nfor each in range(1,25):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \n# visualize\nplt.plot(range(1,25),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"950a8c7e8fe9035ee658818458e4bceb9354df23"},"cell_type":"markdown","source":"We can see high accuracy when k=20."},{"metadata":{"trusted":true,"_uuid":"01e35f4a4a674e883f0b20df6c1b0c105df6e39e"},"cell_type":"markdown","source":"Thank you for reviewing my kernel. See you again."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}