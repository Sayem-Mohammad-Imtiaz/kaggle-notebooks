{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-07-02T04:22:20.031571Z","iopub.execute_input":"2021-07-02T04:22:20.031971Z","iopub.status.idle":"2021-07-02T04:22:20.908487Z","shell.execute_reply.started":"2021-07-02T04:22:20.031877Z","shell.execute_reply":"2021-07-02T04:22:20.907037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading the dataset\nfake = pd.read_csv('../input/fake-and-real-news-dataset/Fake.csv')\ntrue = pd.read_csv('../input/fake-and-real-news-dataset/True.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T04:22:30.294705Z","iopub.execute_input":"2021-07-02T04:22:30.295081Z","iopub.status.idle":"2021-07-02T04:22:33.182794Z","shell.execute_reply.started":"2021-07-02T04:22:30.295046Z","shell.execute_reply":"2021-07-02T04:22:33.181863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Looking at top 10 records of fake dataset\nfake.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T04:22:35.07987Z","iopub.execute_input":"2021-07-02T04:22:35.080217Z","iopub.status.idle":"2021-07-02T04:22:35.108223Z","shell.execute_reply.started":"2021-07-02T04:22:35.080189Z","shell.execute_reply":"2021-07-02T04:22:35.107154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Looking at top 10 records of true dataset\ntrue.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T04:22:36.069882Z","iopub.execute_input":"2021-07-02T04:22:36.070234Z","iopub.status.idle":"2021-07-02T04:22:36.082976Z","shell.execute_reply.started":"2021-07-02T04:22:36.070203Z","shell.execute_reply":"2021-07-02T04:22:36.082023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking shape of fake dataset\nfake.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-02T04:22:36.688732Z","iopub.execute_input":"2021-07-02T04:22:36.689077Z","iopub.status.idle":"2021-07-02T04:22:36.694984Z","shell.execute_reply.started":"2021-07-02T04:22:36.689047Z","shell.execute_reply":"2021-07-02T04:22:36.693922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the shape of true dataset\ntrue.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-02T04:22:37.926839Z","iopub.execute_input":"2021-07-02T04:22:37.927354Z","iopub.status.idle":"2021-07-02T04:22:37.933573Z","shell.execute_reply.started":"2021-07-02T04:22:37.92732Z","shell.execute_reply":"2021-07-02T04:22:37.932702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the different subjects and their counts in Fake dataset\nfake['subject'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T04:22:38.959154Z","iopub.execute_input":"2021-07-02T04:22:38.959701Z","iopub.status.idle":"2021-07-02T04:22:38.97609Z","shell.execute_reply.started":"2021-07-02T04:22:38.959658Z","shell.execute_reply":"2021-07-02T04:22:38.975139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can observe that there are total 6 different topics or subjects, among which \"News\" has highest count. ","metadata":{}},{"cell_type":"code","source":"#Checking the different subjects and their value counts in true dataset\ntrue['subject'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T04:22:41.036736Z","iopub.execute_input":"2021-07-02T04:22:41.037081Z","iopub.status.idle":"2021-07-02T04:22:41.049983Z","shell.execute_reply.started":"2021-07-02T04:22:41.037051Z","shell.execute_reply":"2021-07-02T04:22:41.048941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this dataset, we can observe that there are only 2 different topics/subjects, among which \"politicsNews\" has highest count.","metadata":{}},{"cell_type":"markdown","source":"In order to analyse and make model we will combine the two dataset, but before that we need to add an extra column which will help us distinguish between fake news and true news.\nFor this, we will add another feature named 'category' where, all the fake news have 1 as its value and true news will have 0 as its value. ","metadata":{}},{"cell_type":"code","source":"#Creating a category for whether news is fake or not\n\nfake['category']=1\ntrue['category']=0","metadata":{"execution":{"iopub.status.busy":"2021-07-02T04:22:45.190054Z","iopub.execute_input":"2021-07-02T04:22:45.190406Z","iopub.status.idle":"2021-07-02T04:22:45.196767Z","shell.execute_reply.started":"2021-07-02T04:22:45.190363Z","shell.execute_reply":"2021-07-02T04:22:45.19558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets now join both the datasets.","metadata":{}},{"cell_type":"code","source":"#Joining and reseting index\ndf = pd.concat([fake,true]).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T04:22:47.04012Z","iopub.execute_input":"2021-07-02T04:22:47.040475Z","iopub.status.idle":"2021-07-02T04:22:47.05778Z","shell.execute_reply.started":"2021-07-02T04:22:47.04044Z","shell.execute_reply":"2021-07-02T04:22:47.056689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the newly created dataset\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T04:22:47.787149Z","iopub.execute_input":"2021-07-02T04:22:47.787522Z","iopub.status.idle":"2021-07-02T04:22:47.800126Z","shell.execute_reply.started":"2021-07-02T04:22:47.787487Z","shell.execute_reply":"2021-07-02T04:22:47.799135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the shape of the dataset\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-02T04:22:48.89031Z","iopub.execute_input":"2021-07-02T04:22:48.890703Z","iopub.status.idle":"2021-07-02T04:22:48.895842Z","shell.execute_reply.started":"2021-07-02T04:22:48.890671Z","shell.execute_reply":"2021-07-02T04:22:48.895127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets plot the graph of 'category' and 'subject' columns.","metadata":{}},{"cell_type":"code","source":"#Countplot of 'category' attribute\nplt.figure(figsize=(6,7))\nsns.countplot(df['category'])\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T04:22:50.57436Z","iopub.execute_input":"2021-07-02T04:22:50.574891Z","iopub.status.idle":"2021-07-02T04:22:50.736673Z","shell.execute_reply.started":"2021-07-02T04:22:50.574846Z","shell.execute_reply":"2021-07-02T04:22:50.73569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Countplot for 'subject' attribute\nplt.figure(figsize=(10,7))\nsns.countplot(df['subject'])","metadata":{"execution":{"iopub.status.busy":"2021-07-02T04:22:51.793107Z","iopub.execute_input":"2021-07-02T04:22:51.793657Z","iopub.status.idle":"2021-07-02T04:22:52.019723Z","shell.execute_reply.started":"2021-07-02T04:22:51.793615Z","shell.execute_reply":"2021-07-02T04:22:52.018631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Cleaning**","metadata":{}},{"cell_type":"code","source":"#Checking null values explicitly\ndf.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T04:22:55.46537Z","iopub.execute_input":"2021-07-02T04:22:55.465765Z","iopub.status.idle":"2021-07-02T04:22:55.494166Z","shell.execute_reply.started":"2021-07-02T04:22:55.465734Z","shell.execute_reply":"2021-07-02T04:22:55.493147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking if there is empty string in TEXT column\nblanks=[]\n\n#index,label and review of the doc\nfor index,text in df[\"text\"].iteritems(): # it will iter through index,label and review\n    if text.isspace(): # if there is a space\n        blanks.append(index) #it will be noted down in empty list\n\nlen(blanks)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T04:22:57.308325Z","iopub.execute_input":"2021-07-02T04:22:57.30871Z","iopub.status.idle":"2021-07-02T04:22:57.339541Z","shell.execute_reply.started":"2021-07-02T04:22:57.308674Z","shell.execute_reply":"2021-07-02T04:22:57.338426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we have to find out from text whether the news is fake or not, we will just need two attributes :- 'text' and 'category', we will drop other features.","metadata":{}},{"cell_type":"code","source":"#Instead of dropping these values we are going to merge title with text\ndf[\"text\"] =df[\"title\"]+df[\"text\"]\n\n#we only need two columns rest can be ignored\n\ndf=df[[\"text\",\"category\"]]","metadata":{"execution":{"iopub.status.busy":"2021-07-02T04:22:59.272589Z","iopub.execute_input":"2021-07-02T04:22:59.272926Z","iopub.status.idle":"2021-07-02T04:22:59.456507Z","shell.execute_reply.started":"2021-07-02T04:22:59.272897Z","shell.execute_reply":"2021-07-02T04:22:59.455739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing libraries for cleaning purpose\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport spacy\nimport re","metadata":{"execution":{"iopub.status.busy":"2021-07-02T04:25:57.662593Z","iopub.execute_input":"2021-07-02T04:25:57.662982Z","iopub.status.idle":"2021-07-02T04:25:59.383512Z","shell.execute_reply.started":"2021-07-02T04:25:57.662951Z","shell.execute_reply":"2021-07-02T04:25:59.382746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Stopword** \n\nWords like “a” and “the” appear so frequently that they don’t require tagging as thoroughly as nouns, verbs and modifiers.                       \nWe call them stop words, and they can be filtered from the text to be processed.\n\n**Lemmatization**\n\nIt takes into consideration the morphological analysis of the words.The output we will get after lemmatization is called ‘lemma’, which is a root word rather than root stem.","metadata":{}},{"cell_type":"code","source":"#Loading the spacy library\nnlp = spacy.load('en_core_web_sm')\n\n#Creating instance\nlemma = WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T04:32:09.268593Z","iopub.execute_input":"2021-07-02T04:32:09.268941Z","iopub.status.idle":"2021-07-02T04:32:10.619715Z","shell.execute_reply.started":"2021-07-02T04:32:09.268913Z","shell.execute_reply":"2021-07-02T04:32:10.618792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating list of stopwords containing stopwords from spacy and nltk\n\n#Stopwords of spacy\nl1 = nlp.Defaults.stop_words\nprint(len(l1))\n\n#Stopwords of NLTK library\nl2 = stopwords.words('english')\nprint(len(l2))\n\n#Combining both the above lists \nStopwords = set((set(l1)|set(l2)))\nprint(len(Stopwords))","metadata":{"execution":{"iopub.status.busy":"2021-07-02T04:35:38.634764Z","iopub.execute_input":"2021-07-02T04:35:38.635265Z","iopub.status.idle":"2021-07-02T04:35:38.641745Z","shell.execute_reply.started":"2021-07-02T04:35:38.635236Z","shell.execute_reply":"2021-07-02T04:35:38.640636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function for cleaning the text \ndef text_cleaning(text):\n    #Defining empty string\n    string = \"\"\n    \n    #lower casing\n    text=text.lower()\n    \n    #simplifying text\n    text=re.sub(r\"i'm\",\"i am\",text)\n    text=re.sub(r\"he's\",\"he is\",text)\n    text=re.sub(r\"she's\",\"she is\",text)\n    text=re.sub(r\"that's\",\"that is\",text)\n    text=re.sub(r\"what's\",\"what is\",text)\n    text=re.sub(r\"where's\",\"where is\",text)\n    text=re.sub(r\"\\'ll\",\" will\",text)\n    text=re.sub(r\"\\'ve\",\" have\",text)\n    text=re.sub(r\"\\'re\",\" are\",text)\n    text=re.sub(r\"\\'d\",\" would\",text)\n    text=re.sub(r\"won't\",\"will not\",text)\n    text=re.sub(r\"can't\",\"cannot\",text)\n    \n    #removing any special character\n    text=re.sub(r\"[-()\\\"#!@$%^&*{}?.,:]\",\" \",text)\n    text=re.sub(r\"\\s+\",\" \",text)\n    text=re.sub('[^A-Za-z0-9]+',' ', text)\n    \n    for word in text.split():\n        if word not in Stopwords:\n            string+=lemma.lemmatize(word)+\" \"\n    \n    return string","metadata":{"execution":{"iopub.status.busy":"2021-07-02T04:39:42.213811Z","iopub.execute_input":"2021-07-02T04:39:42.214353Z","iopub.status.idle":"2021-07-02T04:39:42.228429Z","shell.execute_reply.started":"2021-07-02T04:39:42.214294Z","shell.execute_reply":"2021-07-02T04:39:42.2273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets clean now 'text' column in our dataframe using above function.","metadata":{}},{"cell_type":"code","source":"#cleaning\ndf['text']=df['text'].apply(text_cleaning)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T04:40:59.356003Z","iopub.execute_input":"2021-07-02T04:40:59.356377Z","iopub.status.idle":"2021-07-02T04:42:30.859232Z","shell.execute_reply.started":"2021-07-02T04:40:59.356344Z","shell.execute_reply":"2021-07-02T04:42:30.858114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Yeah, we have cleaned our text data, lets visualize it via wordcloud.","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud","metadata":{"execution":{"iopub.status.busy":"2021-07-02T05:00:10.737935Z","iopub.execute_input":"2021-07-02T05:00:10.738318Z","iopub.status.idle":"2021-07-02T05:00:10.785025Z","shell.execute_reply.started":"2021-07-02T05:00:10.73828Z","shell.execute_reply":"2021-07-02T05:00:10.783846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nwc = WordCloud(max_words = 500, width = 1600, height = 800).generate(\" \".join(df[df.category==0].text))\nplt.axis('off')\nplt.imshow(wc, interpolation = 'bilinear')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T05:02:06.657779Z","iopub.execute_input":"2021-07-02T05:02:06.658114Z","iopub.status.idle":"2021-07-02T05:02:52.153766Z","shell.execute_reply.started":"2021-07-02T05:02:06.658085Z","shell.execute_reply":"2021-07-02T05:02:52.15264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"X = df['text'] #feature\ny = df['category'] #target","metadata":{"execution":{"iopub.status.busy":"2021-07-02T05:31:45.918168Z","iopub.execute_input":"2021-07-02T05:31:45.918522Z","iopub.status.idle":"2021-07-02T05:31:45.923789Z","shell.execute_reply.started":"2021-07-02T05:31:45.918494Z","shell.execute_reply":"2021-07-02T05:31:45.922783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T05:32:54.667514Z","iopub.execute_input":"2021-07-02T05:32:54.667886Z","iopub.status.idle":"2021-07-02T05:32:54.684702Z","shell.execute_reply.started":"2021-07-02T05:32:54.667854Z","shell.execute_reply":"2021-07-02T05:32:54.68364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing libraries\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC","metadata":{"execution":{"iopub.status.busy":"2021-07-02T05:34:58.121336Z","iopub.execute_input":"2021-07-02T05:34:58.121705Z","iopub.status.idle":"2021-07-02T05:34:58.126164Z","shell.execute_reply.started":"2021-07-02T05:34:58.12167Z","shell.execute_reply":"2021-07-02T05:34:58.125168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tfidf vectorizer\ntext_clf = Pipeline([('tfidf',TfidfVectorizer()),('clf',LinearSVC())])\n\n#Fitting the model\ntext_clf.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T05:39:38.878285Z","iopub.execute_input":"2021-07-02T05:39:38.878665Z","iopub.status.idle":"2021-07-02T05:39:48.133952Z","shell.execute_reply.started":"2021-07-02T05:39:38.878632Z","shell.execute_reply":"2021-07-02T05:39:48.132933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predicting\npredictions =  text_clf.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T05:40:23.143186Z","iopub.execute_input":"2021-07-02T05:40:23.143582Z","iopub.status.idle":"2021-07-02T05:40:27.369218Z","shell.execute_reply.started":"2021-07-02T05:40:23.143524Z","shell.execute_reply":"2021-07-02T05:40:27.36846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nprint(metrics.classification_report(y_test, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-07-02T05:41:01.926876Z","iopub.execute_input":"2021-07-02T05:41:01.927217Z","iopub.status.idle":"2021-07-02T05:41:01.963765Z","shell.execute_reply.started":"2021-07-02T05:41:01.927188Z","shell.execute_reply":"2021-07-02T05:41:01.962734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Accuracy\nprint(metrics.accuracy_score(y_test,predictions))","metadata":{"execution":{"iopub.status.busy":"2021-07-02T05:41:41.096018Z","iopub.execute_input":"2021-07-02T05:41:41.096399Z","iopub.status.idle":"2021-07-02T05:41:41.1048Z","shell.execute_reply.started":"2021-07-02T05:41:41.096364Z","shell.execute_reply":"2021-07-02T05:41:41.103808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#confusion matrix\nprint(metrics.confusion_matrix(y_test,predictions))","metadata":{"execution":{"iopub.status.busy":"2021-07-02T05:42:16.994319Z","iopub.execute_input":"2021-07-02T05:42:16.994878Z","iopub.status.idle":"2021-07-02T05:42:17.023891Z","shell.execute_reply.started":"2021-07-02T05:42:16.994828Z","shell.execute_reply":"2021-07-02T05:42:17.022705Z"},"trusted":true},"execution_count":null,"outputs":[]}]}