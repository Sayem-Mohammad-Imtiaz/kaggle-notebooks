{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"432d566a-c7e9-21a6-10cb-5302b6eb3b85"},"source":"RandomForest vs KNeighbors vs LogisticRegression vs GradientBoosting vs SVC vs XGradient Boosting.\n\n 1. Multiple cross validation tests\n 2. Datasets with feature engineering and without data engineering\n 3. ROC curve for some classifiers\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9452aacf-c764-2e5f-9711-9d1548fe123a"},"outputs":[],"source":"# Importig libs\nfrom xgboost import XGBClassifier\nimport xgboost\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, svm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.svm import SVC\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"244af7b2-ab7b-5e54-12cd-721b5d441907"},"outputs":[],"source":"initial_data = pd.read_csv(\"../input/mushrooms.csv\") \n\n# Replacing chars with numbers, removing  useless records\nnumeric_data = initial_data\nlabel = LabelEncoder()\ndicts = {}\n\nfields_without_class = [\n     'cap-shape', 'cap-surface', \n     'cap-color', 'bruises', 'odor', \n     'gill-attachment', 'gill-spacing', 'gill-size',\n     'gill-color', 'stalk-shape', 'stalk-root', \n     'stalk-surface-above-ring', 'stalk-surface-below-ring',\n     'stalk-color-above-ring', 'stalk-color-below-ring', \n     'veil-type', 'veil-color', 'ring-number', 'ring-type', \n     'spore-print-color', 'population', 'habitat'\n]\n\nfields = fields_without_class\nfields.append('class')\n\n\nfor f in fields:\n    label.fit(initial_data[f].drop_duplicates())\n    dicts[f] = list(label.classes_)\n    numeric_data[f] = label.transform(initial_data[f])    \n\ntarget = numeric_data['class']\nnumeric_data = numeric_data.drop(['class'], axis=1)     \n\n# Looking for most valuable columns in our dataset\nnumeric_data_best = SelectKBest(f_classif, k=6).fit_transform(numeric_data, target)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bb1fbb47-3e0b-a06c-b6a1-914cfd86d793"},"outputs":[],"source":"%matplotlib inline\n\n# Trying to find best model\nmodel_rfc = RandomForestClassifier(n_estimators = 70)\nmodel_knc = KNeighborsClassifier(n_neighbors = 18) \nmodel_lr = LogisticRegression(penalty='l1', tol=0.01) \nmodel_gb = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100)\nmodel_svc = svm.SVC() \nmodel_xgb = XGBClassifier()\nmodel_svc = SVC(kernel='rbf', random_state=0)\n\nROCtrainTRN, ROCtestTRN, ROCtrainTRG, ROCtestTRG = model_selection.train_test_split(numeric_data_best, target, test_size=0.25) \n\nresults = {}\nkfold = 5\n\nresults['RandomForestClassifier_best_params'] = model_selection.cross_val_score(model_rfc, numeric_data_best, target, cv=kfold).mean()\nresults['KNeighborsClassifier_best_params'] = model_selection.cross_val_score(model_knc, numeric_data_best, target, cv=kfold).mean()\nresults['LogisticRegression_best_params'] = model_selection.cross_val_score(model_lr, numeric_data_best, target, cv = kfold).mean()\nresults['GradientBoosting_best_params'] = model_selection.cross_val_score(model_gb, numeric_data_best, target, cv = kfold).mean()\nresults['SVC_best_params'] = model_selection.cross_val_score(model_svc, numeric_data_best, target, cv = kfold).mean()\nresults['XGB_best_params'] = model_selection.cross_val_score(model_xgb, numeric_data_best, target, cv = kfold).mean()\n\nresults['RandomForestClassifier_all_params'] = model_selection.cross_val_score(model_rfc, numeric_data, target, cv=kfold).mean()\nresults['KNeighborsClassifier_all_params'] = model_selection.cross_val_score(model_knc, numeric_data, target, cv=kfold).mean()\nresults['LogisticRegression_all_params'] = model_selection.cross_val_score(model_lr, numeric_data, target, cv = kfold).mean()\nresults['GradientBoosting_all_params'] = model_selection.cross_val_score(model_gb, numeric_data, target, cv = kfold).mean()\nresults['SVC_all_params'] = model_selection.cross_val_score(model_svc, numeric_data, target, cv = kfold).mean()\nresults['XGB_all_params'] = model_selection.cross_val_score(model_xgb, numeric_data, target, cv = kfold).mean()\n    \n\nplt.bar(range(len(results)), results.values(), align='center')\nplt.xticks(range(len(results)), list(results.keys()), rotation='vertical')\nplt.show()\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"92b7724b-5ca6-7066-4082-a7de1cc3a24d"},"outputs":[],"source":"# ROC\nroc_train_all, roc_test_all, roc_train_all_class, roc_test_all_class = model_selection.train_test_split(numeric_data, target, test_size=0.25) \nroc_train_best, roc_test_best, roc_train_best_class, roc_test_best_class = model_selection.train_test_split(numeric_data_best, target, test_size=0.25) \n\nmodels = [\n    {\n        'label' : 'GradientBoosting_best_params',\n        'model': model_gb,\n        'roc_train': roc_train_best,\n        'roc_test': roc_test_best,\n        'roc_train_class': roc_train_best_class,        \n        'roc_test_class': roc_test_best_class,                \n    },\n    {\n        'label' : 'RandomForestClassifier_best_params',\n        'model': model_rfc,\n        'roc_train': roc_train_best,\n        'roc_test': roc_test_best,\n        'roc_train_class': roc_train_best_class,        \n        'roc_test_class': roc_test_best_class,        \n    },\n    {\n        'label' : 'XGB_best_params',\n        'model': model_gb,\n        'roc_train': roc_train_best,\n        'roc_test': roc_test_best,\n        'roc_train_class': roc_train_best_class,        \n        'roc_test_class': roc_test_best_class,        \n    },    \n    {\n        'label' : 'SVC_best_params',\n        'model': model_svc,\n        'roc_train': roc_train_best,\n        'roc_test': roc_test_best,\n        'roc_train_class': roc_train_best_class,        \n        'roc_test_class': roc_test_best_class,        \n    },        \n    {\n        'label' : 'KNeighborsClassifier_all_params',\n        'model': model_knc,\n        'roc_train': roc_train_all,\n        'roc_test': roc_test_all,\n        'roc_train_class': roc_train_all_class,        \n        'roc_test_class': roc_test_all_class,        \n    },\n    {\n        'label' : 'LogisticRegression_all_params',\n        'model': model_knc,\n        'roc_train': roc_train_all,\n        'roc_test': roc_test_all,\n        'roc_train_class': roc_train_all_class,        \n        'roc_test_class': roc_test_all_class,        \n    }        \n]\n\n\nplt.clf()\nplt.figure(figsize=(8,6))\n\nfor m in models:\n    m['model'].probability = True\n    probas = m['model'].fit(m['roc_train'], m['roc_train_class']).predict_proba(m['roc_test'])\n    fpr, tpr, thresholds = roc_curve(m['roc_test_class'], probas[:, 1])\n    roc_auc  = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % (m['label'], roc_auc))\n\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=0, fontsize='small')\nplt.show()"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}