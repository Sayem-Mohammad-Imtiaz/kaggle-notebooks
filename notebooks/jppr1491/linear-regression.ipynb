{"cells":[{"metadata":{"_uuid":"9081e966-7b9b-498a-8fb6-c3e70490a149","_cell_guid":"bbe14352-7f73-456d-a977-ea24ccb17416","trusted":true},"cell_type":"code","source":"#!/usr/bin/env python\n# coding: utf-8\n\n# In[2]:\n\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nget_ipython().run_line_magic('matplotlib', 'inline')\n\n\n# In[3]:\n\n\ndf = pd.read_excel (r'your_path_to_the_file.xlsx') #I save it as EXCEL File\n# we want to create a linear regression model that predicts the \"selling price\" for a car given the data\n\n\n# In[4]:\n\n\ndf.head()\n\n\n# In[5]:\n\n\n# check the if theres any association between the selling price and the other variables\nplt.figure(figsize=(15,10))\nplt.xlabel(\"Present_Price\")\nplt.ylabel(\"Selling_Price\")\nplt.scatter(df.Present_Price, df.Selling_Price,marker='+', color='blue')\n\n\n# In[6]:\n\n\nplt.figure(figsize=(20,10))\nplt.xlabel(\"Kms_Driven\")\nplt.ylabel(\"Selling_Price\")\nplt.scatter(df.Kms_Driven, df.Selling_Price,marker='+', color='blue')\n\n\n# In[7]:\n\n\nplt.figure(figsize=(20,10))\nplt.xlabel(\"Year\")\nplt.ylabel(\"Selling_Price\")\nplt.scatter(df.Year, df.Selling_Price,marker='+', color='blue')\n\n\n# In[8]:\n\n\n# turn \"string\" type data as a factor\n\ndf[\"fact_Car_Name\"] = pd.factorize(df.Car_Name)[0]\ndf[\"fact_Fuel_Type\"] = pd.factorize(df.Fuel_Type)[0]\ndf[\"fact_Seller_Type\"] = pd.factorize(df.Seller_Type)[0]\ndf[\"fact_Transmission\"] = pd.factorize(df.Transmission)[0]\n\ndf.head()\n\n\n# In[9]:\n\n\nplt.scatter(df.fact_Transmission, df.Selling_Price, marker = \"+\")\n\n\n# In[10]:\n\n\nplt.scatter(df.fact_Seller_Type, df.Selling_Price, marker = \"+\")\n\n\n# In[11]:\n\n\nplt.scatter(df.fact_Car_Name, df.Selling_Price, marker = \"+\")\n\n\n# In[76]:\n\n\nplt.scatter(df.fact_Fuel_Type, df.Selling_Price, marker = \"+\")\n\n\n# In[12]:\n\n\nplt.scatter(df.Owner, df.Selling_Price, marker = \"+\")\n\n\n# In[ ]:\n\n\n# These graphs lead me to believe that all variable may have influence on the selling price\n# Some evident conclusions are that a more recent car has a higher selling price and the same can be said about Kms_driven\n\n\n# In[13]:\n\n\n# Modeling\n\nimport numpy as np\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n\n# In[48]:\n\n\n# A backward method is used to select what variables will take part in the model \n\nX = df[[\"fact_Car_Name\", \"Year\", \"Kms_Driven\", \"Present_Price\", \n                   \"fact_Fuel_Type\", \"fact_Seller_Type\", \"fact_Transmission\", \"Owner\"]]\n\n# including a line like Y = df.Selling_Price would make the code cleaner, but this way is more clear to me\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, df.Selling_Price, test_size=0.25, random_state=69)\n\n\n# In[49]:\n\n\nregr1 = linear_model.LinearRegression()\nregr1.fit(X_train,Y_train)\n\n\n# In[50]:\n\n\n# If we are interested on checking the coeficients for the model\n#print('Intercept: \\n', regr1.intercept_)\n#print('Coefficients: \\n', regr1.coef_)\n\n\n# In[51]:\n\n\nregr1.predict(X_test)\n\n\n# In[47]:\n\n\nregr1.score(X_test,Y_test)\n\n\n# In[35]:\n\n\n# 88% of the models variance is explained by the current model\n# but is it good enough? can we make it better? So what is next?\n# from all the variables the name may or may not influenciate the Selling_Price, so lets see if the model gets better without it\n\n\n# In[54]:\n\n\nX2 = df[[\"Year\", \"Kms_Driven\", \"Present_Price\", \"fact_Fuel_Type\", \"fact_Seller_Type\", \"fact_Transmission\", \"Owner\"]]\n\n# including a line like Y = df.Selling_Price would make the code cleaner, but this way is more clear to me\n\nX2_train, X2_test, Y2_train, Y2_test = train_test_split(X2, df.Selling_Price, test_size=0.25, random_state=69)\n\n\n# In[55]:\n\n\nregr2 = linear_model.LinearRegression()\nregr2.fit(X2_train,Y2_train)\n\n\n# In[59]:\n\n\nregr2.predict(X2_test)\n\n\n# In[ ]:\n\n\n# like before checking the coeficients is an option\n\n\n# In[60]:\n\n\nregr2.score(X2_test,Y2_test)\n\n\n# In[ ]:\n\n\n# the model score almost didnt change which means excluding the Car_name just saves sometime on the calculations\n# for the next model we will remove the Owner variable and see if the model gets better\n# of course that a car with zero owners to date, a brand new car, will be more expensive than a car that had 1 or more owners\n\n\n# In[61]:\n\n\nX3 = df[[\"Year\", \"Kms_Driven\", \"Present_Price\", \"fact_Fuel_Type\", \"fact_Seller_Type\", \"fact_Transmission\"]]\n\n# including a line like Y = df.Selling_Price would make the code cleaner, but this way is more clear to me\n\nX3_train, X3_test, Y3_train, Y3_test = train_test_split(X3, df.Selling_Price, test_size=0.25, random_state=69)\n\n\n# In[62]:\n\n\nregr3 = linear_model.LinearRegression()\nregr3.fit(X3_train,Y3_train)\n\n\n# In[63]:\n\n\nregr3.predict(X3_test)\n\n\n# In[64]:\n\n\nregr3.score(X3_test,Y3_test)\n\n\n# In[ ]:\n\n\n# the model improved in quality \n# so at this point I'm thinking that the variables that should be considered for the model are Year, Kms_driven, Fuel_Type\n# will do one with Seller_Type and later another with Transmission as last variable\n\n\n# In[67]:\n\n\nX4 = df[[\"Year\", \"Kms_Driven\", \"Present_Price\", \"fact_Fuel_Type\", \"fact_Seller_Type\"]]\n\n# including a line like Y = df.Selling_Price would make the code cleaner, but this way is more clear to me\n\nX4_train, X4_test, Y4_train, Y4_test = train_test_split(X4, df.Selling_Price, test_size=0.25, random_state=69)\n\n\n# In[69]:\n\n\nregr4 = linear_model.LinearRegression()\nregr4.fit(X4_train,Y4_train)\n\n\n# In[70]:\n\n\nregr4.predict(X4_test)\n\n\n# In[71]:\n\n\nregr4.score(X4_test,Y4_test)\n\n\n# In[ ]:\n\n\n# this model is almost at a score of 90% which leads me to believe that Transmission has less impact on Selling_Price than \n# Seller_Type  \n\n\n# In[68]:\n\n\nX5 = df[[\"Year\", \"Kms_Driven\", \"Present_Price\", \"fact_Fuel_Type\", \"fact_Transmission\"]]\n\n# including a line like Y = df.Selling_Price would make the code cleaner, but this way is more clear to me\n\nX5_train, X5_test, Y5_train, Y5_test = train_test_split(X5, df.Selling_Price, test_size=0.25, random_state=69)\n\n\n# In[72]:\n\n\nregr5 = linear_model.LinearRegression()\nregr5.fit(X5_train,Y5_train)\n\n\n# In[74]:\n\n\nregr5.predict(X5_test)\n\n\n# In[75]:\n\n\nregr5.score(X5_test,Y5_test)\n\n\n# In[ ]:\n\n\n# As stated above we have better predictiong with Seller_Type \n# So at this point if I was to use a linear model to make predictions on Selling_Price I would use model 4 \n# So the challenge now is to work with model 4 variables and decide what to variable to remove\n# This time I think we just have to see what happens to the model when you remove Seller_Type\n# if it gets a better score then Seller_Type isnt needed \n\n\n# In[77]:\n\n\nX6 = df[[\"Year\", \"Kms_Driven\", \"Present_Price\", \"fact_Fuel_Type\"]]\n\n# including a line like Y = df.Selling_Price would make the code cleaner, but this way is more clear to me\n\nX6_train, X6_test, Y6_train, Y6_test = train_test_split(X6, df.Selling_Price, test_size=0.25, random_state=69)\n\n\n# In[78]:\n\n\nregr6 = linear_model.LinearRegression()\nregr6.fit(X6_train,Y6_train)\n\n\n# In[79]:\n\n\nregr6.predict(X6_test)\n\n\n# In[80]:\n\n\nregr6.score(X6_test,Y6_test)\n\n\n# In[ ]:\n\n\n# so it seems with the current data and if we want to use a linear model to make predictions, we have to use model 4 \n\n# X4 = df[[\"Year\", \"Kms_Driven\", \"Present_Price\", \"fact_Fuel_Type\", \"fact_Seller_Type\"]]\n\n# X4_train, X4_test, Y4_train, Y4_test = train_test_split(X4, df.Selling_Price, test_size=0.25, random_state=69)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}