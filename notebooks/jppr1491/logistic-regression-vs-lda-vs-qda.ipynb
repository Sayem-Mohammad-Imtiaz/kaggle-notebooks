{"cells":[{"metadata":{"_uuid":"1dd7077c-5809-4fc8-bc83-155a793556fa","_cell_guid":"8c552cc7-2f9a-41e7-a3a4-70097c837b41","trusted":true},"cell_type":"code","source":"#!/usr/bin/env python\n# coding: utf-8\n\n# In[4]:\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix\n\nget_ipython().run_line_magic('matplotlib', 'inline')\n\n\n# In[5]:\n\n\nfull_data = pd.read_csv(r'\\Users\\Pedro\\Documents\\high_diamond_ranked_10min.csv')\n# lets exclude gameId\n\ndata = full_data.drop(['gameId'],axis=1)\n\n\n# In[6]:\n\n\n# Win-loss count 0 means loss, 1 means win\nblueWins = data['blueWins']\nax = sns.countplot(x = blueWins, data = data)\n\n\n# In[10]:\n\n\nX = data.drop(['blueWins'],axis=1)\nY = blueWins.values.reshape(-1,1)\n\n# if memory doesnt fail, last time we fitted a logistic regression model to this data, the best result came from a train \n# size of 0.9 \nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.9, random_state=69)\n\n\n# In[11]:\n\n\n# logistic regression\nfrom sklearn.linear_model import LogisticRegression\n\n\n# In[13]:\n\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, Y_train.ravel())\n\n\n# In[14]:\n\n\nY_prob = log_reg.predict_proba(X_test)[:,1]\nY_pred = np.where(Y_prob > 0.5, 1, 0) \n\n\n# In[15]:\n\n\nlog_confusion_matrix = confusion_matrix(Y_test, Y_pred)\nlog_confusion_matrix\n# 390 + 358 correct predictions\n# 122 + 119 wrong predictions\n\n\n# In[38]:\n\n\nfalse_positive_rate_log, true_positive_rate_log, thresholds_log = roc_curve(Y_test, Y_prob)\nroc_auc_log = auc(false_positive_rate_log, true_positive_rate_log)\nroc_auc_log\n\n\n# In[19]:\n\n\n# lets take a look at the graph\n\nplt.figure(figsize=(9,9))\nplt.title('ROC')\nplt.plot(false_positive_rate_log, true_positive_rate_log, color='red', label = 'AUC = %2f' % roc_auc_log)\nplt.legend(loc='lower right')\nplt.plot([0,1], [0,1], linestyle = '--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.plot(roc_auc_log)\n\n\n# In[20]:\n\n\n# linear discriminant analysis\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis()\nlda.fit(X_train, Y_train.ravel())\n\n\n# In[21]:\n\n\nY_prob_lda = lda.predict_proba(X_test)[:,1]\nY_pred_lda = np.where(Y_prob_lda > 0.5, 1, 0) \n\n\n# In[23]:\n\n\nlda_confusion_matrix = confusion_matrix(Y_test, Y_pred_lda)\nlda_confusion_matrix\n\n\n# In[24]:\n\n\nfalse_positive_rate_lda, true_positive_rate_lda, thresholds_lda = roc_curve(Y_test, Y_prob_lda)\nroc_auc_lda = auc(false_positive_rate_lda, true_positive_rate_lda)\nroc_auc_lda\n\n\n# In[25]:\n\n\n# the auc for lda is slightly lower than the auc for the logistic regression \n# this means logistic regression performs slightly better, but a very very close to zero\n# roc_auc_log - roc_auc_lda\n\n\n# In[26]:\n\n\nplt.figure(figsize=(9,9))\nplt.title('ROC')\nplt.plot(false_positive_rate_lda, true_positive_rate_lda, color='red', label = 'AUC = %2f' % roc_auc_lda)\nplt.legend(loc='lower right')\nplt.plot([0,1], [0,1], linestyle = '--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.plot(roc_auc_lda)\n\n\n# In[27]:\n\n\n# finally lets see how quadratic discriminant analysis performs\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nqda = QuadraticDiscriminantAnalysis()\nqda.fit(X_train, Y_train.ravel())\n\n\n# In[28]:\n\n\nY_prob_qda = qda.predict_proba(X_test)[:,1]\nY_pred_qda = np.where(Y_prob_qda > 0.5, 1, 0) \n\n\n# In[29]:\n\n\nqda_confusion_matrix = confusion_matrix(Y_test, Y_pred_qda)\nqda_confusion_matrix\n\n\n# In[30]:\n\n\nfalse_positive_rate_qda, true_positive_rate_qda, thresholds_qda = roc_curve(Y_test, Y_prob_qda)\nroc_auc_qda = auc(false_positive_rate_qda, true_positive_rate_qda)\nroc_auc_qda\n\n\n# In[31]:\n\n\nplt.figure(figsize=(9,9))\nplt.title('ROC')\nplt.plot(false_positive_rate_qda, true_positive_rate_qda, color='red', label = 'AUC = %2f' % roc_auc_qda)\nplt.legend(loc='lower right')\nplt.plot([0,1], [0,1], linestyle = '--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.plot(roc_auc_qda)\n\n\n# In[45]:\n\n\n# Conclusions\n# The best perfoming method was logistic regression, followed by lda, followed by qda\nprint('Logistic regression', '\\t' , 'Lda', '\\t', '                Qda')\nprint(roc_auc_log, '\\t', roc_auc_lda, '\\t', roc_auc_qda)\n\n\n# In[ ]:","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}