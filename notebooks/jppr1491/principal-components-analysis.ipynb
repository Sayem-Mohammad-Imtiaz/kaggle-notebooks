{"cells":[{"metadata":{"_uuid":"809fbe26-0263-4d22-a367-b0a79225d8f7","_cell_guid":"de9a44f3-7abf-4dc2-88a5-f321a05e914d","trusted":true},"cell_type":"code","source":"#!/usr/bin/env python\n# coding: utf-8\n\n# In[2]:\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\n\n\n# In[6]:\n\n\nfull_data = pd.read_csv(r'\\Users\\Pedro\\Documents\\high_diamond_ranked_10min.csv')\n\n\n# In[11]:\n\n\n# full_data.head()\n\n\n# In[8]:\n\n\ndf = full_data.drop(['gameId'], axis = 1)\n\n\n# In[10]:\n\n\n# df.head()\n\n\n# In[12]:\n\n\nscaled_data = preprocessing.scale(df.T)\n\n\n# In[14]:\n\n\npca = PCA()\npca.fit(scaled_data)\n\n\n# In[15]:\n\n\npca_data = pca.transform(scaled_data)\n\n\n# In[16]:\n\n\npercent_var = np.round(pca.explained_variance_ratio_*100, decimals=1)\n\n\n# In[18]:\n\n\nlabels = ['PC' + str(x) for x in range(1, len(percent_var)+1)]\nplt.figure(figsize=(20,20))\nplt.bar(x=range(1, len(percent_var)+1), height = percent_var, tick_label = labels)\nplt.ylabel('Percentage of Explained Variance')\nplt.xlabel('Principal Component')\nplt.title('Scree Plot')\nplt.show()\n\n\n# In[19]:\n\n\n# for this data we see that the two fist principal components are enough to explain the variance as shown in the graph\n\n\n# In[21]:\n\n\n# df.columns\n\n\n# In[22]:\n\n\npca_df = pd.DataFrame(pca_data, index = [df.columns], columns = labels)\n\n\n# In[33]:\n\n\nplt.figure(figsize=(20,20))\nplt.scatter(pca_df.PC1, pca_df.PC2)\nplt.title('Principal Components')\nplt.xlabel('PC1 - {0}'.format(percent_var[0]))\nplt.ylabel('PC2 - {0}'.format(percent_var[1]))\nfor sample in pca_df.index:\n    plt.annotate(sample, (pca_df.PC1.loc[sample], pca_df.PC2.loc[sample]))\nplt.show()\n\n\n# In[38]:\n\n\nloading_scores = pd.Series(pca.components_[0], index = full_data['gameId'])\nsorted_loading_scores = loading_scores.abs().sort_values(ascending=False)\n\ntop_10 = sorted_loading_scores[0:20].index.values\nprint(loading_scores[top_10])\n\n\n# In[ ]:\n\n\n# we see 2 clusters that spiit all 40 features and thus suggesting a strong correlation between each other.\n# the score values are similar if not all equal suggesting that every match registered in the dataset contribuited for this\n# separation \n# this pca suggests that if we want to predict the outcome of a match 2 variables are enough, one from each cluster\n\n\n# In[ ]:","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}