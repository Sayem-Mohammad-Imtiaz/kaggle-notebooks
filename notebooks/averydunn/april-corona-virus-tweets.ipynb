{"cells":[{"metadata":{},"cell_type":"markdown","source":"What is a Kaggle notebook? Find out here: https://www.kaggle.com/docs/notebooks","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\n\nimport pandas as pd \nimport numpy as np \nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The dataset contains tweets which include these hashtags:\n        * #coronavirus, #coronavirusoutbreak, #coronavirusPandemic, #covid19, #covid_19\n\n* Below we are **reading in the data** given by each day in April\n* We could combine all of the tweets into one dataframe but it would be far too large and take a lot of time to process, but combining two or three days might be more manageable. Below we are reading in the tweets from **April 1, 2020** that contain the above hashtags.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#uncomment in the tweets for the days in April, and concatenate a few days if necessary\npd.set_option('display.max_columns', None)\npd.set_option('max_colwidth', None)\n\ntweets_april1 = pd.read_csv('/kaggle/input/coronavirus-covid19-tweets-early-april/2020-04-01 Coronavirus Tweets.CSV')\n# tweets_april2 = pd.read_csv('/kaggle/input/coronavirus-covid19-tweets-early-april/2020-04-02 Coronavirus Tweets.CSV')\n# tweets_april3 = pd.read_csv('/kaggle/input/coronavirus-covid19-tweets-early-april/2020-04-03 Coronavirus Tweets.CSV')\n# tweets_april4 = pd.read_csv('/kaggle/input/coronavirus-covid19-tweets-early-april/2020-04-04 Coronavirus Tweets.CSV')\n# tweets_april5 = pd.read_csv('/kaggle/input/coronavirus-covid19-tweets-early-april/2020-04-05 Coronavirus Tweets.CSV')\n\n# tweets = pd.concat([tweets_april1, tweets_april2, tweets_april3, tweets_april4, tweets_april5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Below we can obtain the **size** of the dataframe and the **names of the columns**\n* A **dataframe** is a data structure which contains columns of potentially different types, similar to an excel spreadsheet but rather a **pandas object**. You can learn more about dataframes themselves and how to manipulate them here: https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python\n\n* By using the .head() function we are getting the first few rows of the dataframe; inside the parentheses you can enter exactly how many of the rows you want to view.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Size is: \", tweets_april1.shape)\nprint(\"Columns are: \", tweets_april1.columns)\n\ntweets_april1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Our goal for this project is to do **sentiment analysis** also called **opinion mining** (trying to determine how people feel as they express themselves on social media) on these tweets surrounding the coronavirus. \n* Here is everything you need to know about sentiment analysis: https://monkeylearn.com/sentiment-analysis/\n* Below we are assigning to a new variable a dataframe which only contain the columns which might be useful ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"april1_df = tweets_april1.loc[:,['text', 'retweet_count']]\napril1_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Below we are installing VADER (Valence Aware Dictionary for sEntiment Reasoning)\n* VADER is a sentiment analyzer which classifies the sentiment of tweets into positive, neutral, or negative classes. It also has a compound factor which is a calculation of the sum based on lexicon ratings normalized between -1 and 1\n* We have defined a function below which iterates through the rows of the column of tweets and prints out the sentiment analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install vaderSentiment\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"analyzer = SentimentIntensityAnalyzer()\n\ndef detectSentiment(data_set):\n    for text in data_set.text:\n        analysis = analyzer.polarity_scores(text)\n        print('{} ----- {}'.format(str(text), str(analysis)))\ndetectSentiment(april1_df.head(20))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_count = 0\nneg_count = 0\nneu_count = 0 \n\nfor text in april1_df.text.head(75): \n    if analyzer.polarity_scores(text)['compound'] >= 0.05: \n        pos_count += 1\n    elif analyzer.polarity_scores(text)['compound'] <= -0.05: \n        neg_count += 1 \n    else: \n        neu_count += 1\ntotal = pos_count+neg_count+neu_count\nc = [(pos_count/total)*100, (neg_count/total)*100, (neu_count/total)*100]\nlabels=['Positive', 'Negative', 'Neutral']\nplt.pie(c, labels=labels, autopct='%1.1f%%')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"analyzer = SentimentIntensityAnalyzer()\n\npos_list = []\nneg_list = []\nneu_list = []\n\nfor text in april1_df.text.head(50): \n    if analyzer.polarity_scores(text)['compound'] >= 0.05:\n        pos_list.append(text)\n    elif analyzer.polarity_scores(text)['compound'] <= - 0.05:\n        neg_list.append(text)\n    else:\n        neu_list.append(text)\npos_df = pd.DataFrame(pos_list, columns = ['positive_tweets'])\nneg_df = pd.DataFrame(neg_list, columns=['negative_tweets'])\nneu_df = pd.DataFrame(neu_list, columns=['neutral_tweets'])\n\n\n\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comment_words = ''\nstopwords = set(STOPWORDS)\n\nfor text in pos_df.positive_tweets:\n    a = text.replace('#', '')\n    b = a.replace(\"'\", '')\n    c = b.replace('#', '')\n    d = c.replace('https', '')\n    e = d.replace(':', '')\n    f = e.replace('/', '')\n    final = f.replace('RT', '')\n    final = str(final)\n    tokens = final.split()\n    \n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n    comment_words += ' '.join(tokens)+' '\n\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() \n        \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* What can be done to this data to better visualize it? \n* How can this data be manipulated? \n* Does this data have any value? ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* This dataset may be useful in tracking how people felt in the midst of quarantine, or even those who had the virus \n* Taking a sample of the many tweets and tracking it throughout each day of April could be one place to start, but keep in mind the size of the dataset and the variance of tweets.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can next make predictions with real-time tweets coming straight from the Twitter API, with that you might be able to see how people feel in regards to others current events such as the upcoming election.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}