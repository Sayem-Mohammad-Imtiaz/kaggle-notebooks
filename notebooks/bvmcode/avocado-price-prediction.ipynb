{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Avocado Price Prediction\n\n\n* The data is made up of 2015-2018 retail scan data for national retail volume (units) and price for avacados. \n* Retail scan data comes directly from retailers’ cash registers based on actual retail sales of Hass avocados.\n* Multi-outlet reporting includes an aggregation of the following channels: grocery, mass, club, drug, dollar and military.\n* The Average Price (of avocados) in the table reflects a per unit (per avocado) cost, even when multiple units (avocados) are sold in bags. \n* The Product Lookup codes (PLU’s) in the table are only for Hass avocados.\n\n## The Problem\nHere we want to use some or all of the features in this dataset to predict avocado price.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv('/kaggle/input/avocado-prices/avocado.csv')\n\ndf['Date'] = pd.to_datetime(df['Date'])\ndf.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check out the dataset\nLet's first check if we have any nulls","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like we have a couple of categorical fields. What are their distinct values? We see type is either conventional or organic and its roughly 50/50. And we see that each region has 338 records.","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=\"type\" , data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.type.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.region.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I believe right off the bat we have a couple of fields we don't need. Firstly, the unnamed field seems to be a row id we can discard that. Year is a field that we can always pull from date. Let's just drop it.","metadata":{}},{"cell_type":"code","source":"df = df.drop(['Unnamed: 0', 'year'], axis=1)\ndf.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA\n\nLet's look at the distributions.","metadata":{}},{"cell_type":"code","source":"remove_dates = [col for col in df.columns if col !='Date']\ndf[remove_dates].hist(bins=50, figsize=(15,8))\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that average price is a near normal distribution. The other features look a bit funky. Let's take a deeper look at volume as this might be driving this behavior. Looks like 80% of the volume data doesn't exeed about 6.25K units. We also see from the box plot and the outlier stats that the first outlier is somewhere around 1,067,498 units. This is something to consider going foward.","metadata":{}},{"cell_type":"code","source":"pd.cut(df['Total Volume'],100).value_counts()/df.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.boxplot(df['Total Volume'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib.cbook import boxplot_stats  \nboxplot_stats(df['Total Volume']).pop(0)['fliers'].min()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's see how the average price changes over time. I imagine that season has much to do with the price of avocados and the volume. When in season I imagine more is sold. According to a quick google search, Hass are in season from May through to January. We can take averages for organic and conventional and see how they behave over time. Below I see a little bit of a correlation with month, particularly Spring time. We can also see strongly that when volume jumps price goes down. Seems to be a strong indicator of basic supply and demand principles, at least for the conventional type. Sales for organic is pretty flat.","metadata":{}},{"cell_type":"code","source":"con = df[df['type']=='conventional'].groupby('Date').mean().reset_index()\norg = df[df['type']=='organic'].groupby('Date').mean().reset_index()\n\nfig, ax1 = plt.subplots(figsize=(15,8))\nax2 = ax1.twinx()\nax1.plot(con['Date'], con['AveragePrice'], c='red', label='Conventional Price')\nax1.plot(org['Date'], org['AveragePrice'], c='blue', label='Organic Price', alpha=.4, linestyle=':', lw=3)\nax2.plot(con['Date'], con['Total Volume'], c='green', label='Conventional Volume')\nax2.plot(org['Date'], org['Total Volume'], c='purple', label='Organic Volume', linestyle=':', lw=3, alpha=.4)\n         \nax1.set_xlabel('Date')\nax1.set_ylabel('Avg of Avg Price')\nax2.set_ylabel('Volume')\n\nh1, l1 = ax1.get_legend_handles_labels()\nh2, l2 = ax2.get_legend_handles_labels()\nax1.legend(h1+h2, l1+l2, loc=2)\nplt.title('Mean Avg Price and Volume Over Time')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at just one location and use STL for getting a trend. For example, let's take a look at conventional avocados in Chicago. We can get a trend with seasonal_decompose. Below I don't see much correlation with month however we can see strongly that when volume jumps price goes down. Seems to be a strong indicator of basic supply and demand principles. ","metadata":{}},{"cell_type":"code","source":"from pandas.plotting import register_matplotlib_converters\nfrom statsmodels.tsa.seasonal import STL\n\nchicago_conventional = df[(df['region'] == 'Chicago') & (df['type']=='conventional')].sort_values('Date')\nchicago_conventional.index = chicago_conventional['Date']\n\nstl_price = STL(chicago_conventional['AveragePrice'], period=7)\nres_price = stl_price.fit()\n\nstl_vol = STL(chicago_conventional['Total Volume'], period=7)\nres_vol = stl_vol.fit()\n\n\nfig, ax1 = plt.subplots(figsize=(15,8))\nax2 = ax1.twinx()\nax1.plot(chicago_conventional['Date'], res_price.trend, c='blue', label='Price', alpha=.5, linestyle=':', lw=4)\nax2.plot(chicago_conventional['Date'], res_vol.trend, c='green', label='Volume', alpha=.5, lw=3)\n\nax1.set_xlabel('Date')\nax1.set_ylabel('Avg Price')\nax2.set_ylabel('Volume')\n\nh1, l1 = ax1.get_legend_handles_labels()\nh2, l2 = ax2.get_legend_handles_labels()\nax1.legend(h1+h2, l1+l2, loc=2)\nplt.title('Price and Volume Trend')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that we don't have much volume in 2018 and 2017 is the best year out of the other three years.","metadata":{}},{"cell_type":"code","source":"df.groupby(df.Date.dt.year).sum()[['Total Volume']].plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And as it turns out the last date in 2018 is March 25th.","metadata":{}},{"cell_type":"code","source":"df[df.Date.dt.year == 2018][['Date']].value_counts().sort_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, what does the month distribution look like for each of these years. We see in 2015 and 2016 a spike in Spring. There is also a bit of a spike in Spring for 2017 but the year really started out strong. I think there is a bit of signal here and we can substitute date for month in the dataset.","metadata":{}},{"cell_type":"code","source":"df2 = df.copy()\ndf2['month'] = df2.Date.dt.month\ndf2['year'] = df2.Date.dt.year\ndf2 = df2.groupby(['year', 'month']).sum().reset_index()\n\nplt.figure(figsize=(12,8))\nfor i,year in enumerate(df2['year'].unique()):\n    plt.subplot(2,2,i+1)\n    plt.bar(df2[df2['year']==year]['month'], df2[df2['year']==year]['Total Volume'])\n    plt.title(year)\nfig.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy = df.copy()\ndf_copy['month'] = df_copy['Date'].dt.month\ndf_copy = df_copy.drop(['Date'], axis=1)\ndf_copy.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally let's explore correlation. We see there is very mild negative correlation between price and volume, which we picked up from the time series plot. This seems to be specfically more pronounced with PLU 4046. We see there is very mild positive correlation between price and month.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.heatmap(df_copy.corr(), annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preperation\n\nLet's get our data ready to use in our model selection. We need to do the following\n* create train and test splits\n* scale our numerical features\n* one-hot encode our categorical features\n\nLet's do a simple test/train split. Doesn't appear we need to do a stratified split. So this is very straight forward.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(df_copy, test_size=0.2, random_state=42)\ntrain_set.shape, test_set.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_features = train_set.drop(\"AveragePrice\", axis=1)\ntrain_labels = train_set[\"AveragePrice\"].copy()\n\ntest_features = test_set.drop(\"AveragePrice\", axis=1)\ntest_labels = test_set[\"AveragePrice\"].copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's set up a pipeline for scaling and one-hot encoding and perform it on our train set.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs = ['Total Volume', '4046', '4225', '4770', 'Total Bags','Small Bags', 'Large Bags', 'XLarge Bags','month']\ncat_attribs = ['type', 'region']\n\nnum_pipline = Pipeline([('std_scaler', StandardScaler()),])\npipeline = ColumnTransformer([(\"num\", num_pipline, num_attribs), (\"cat\", OneHotEncoder(), cat_attribs)])\n\ntrain_prepared = pipeline.fit_transform(train_features)\ntrain_prepared.toarray()[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Selection\n\nLet's just start off with simple linear regression, take a look of how it predicts off of some of our training data. Then Let's see with the RMSE value is. We see that our predictions do follow a line but perhaps not the best. The RMSE is within about a 26 cents.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\n\nlin_reg = LinearRegression()\nlin_reg.fit(train_prepared, train_labels)\n\nsome_data = train_features.iloc[:30]\nsome_data_prepared = pipeline.transform(some_data)\nsome_data_predicted = lin_reg.predict(some_data_prepared)\nsome_data_actual = train_labels.iloc[:30]\n\ntest_result = pd.DataFrame({\n    'actual': list(some_data_actual),\n    'predictions': list(some_data_predicted)\n})\n\nplt.scatter(test_result['actual'], test_result['predictions'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\npred = lin_reg.predict(train_prepared)\nlin_mse = mean_squared_error(train_labels, pred)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try Decision Tree Regressor.","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\n# Decision tree model\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(train_prepared, train_labels)\n\nsome_data = train_features.iloc[:30]\nsome_data_prepared = pipeline.transform(some_data)\nsome_data_predicted = tree_reg.predict(some_data_prepared)\nsome_data_actual = train_labels.iloc[:30]\n\ntest_result = pd.DataFrame({\n    'actual': list(some_data_actual),\n    'predictions': list(some_data_predicted)\n})\n\nplt.scatter(test_result['actual'], test_result['predictions'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tree_predictions = tree_reg.predict(train_prepared)\ntree_mse = mean_squared_error(train_labels, tree_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow, awesome results! Nah, we just way overfit the data. We will need some cross-validation.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\nscores_tree = cross_val_score(tree_reg, train_prepared, train_labels, scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores_tree)\ndisplay_scores(tree_rmse_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's try it for linear regression.","metadata":{}},{"cell_type":"code","source":"scores_lin_reg = cross_val_score(lin_reg, train_prepared, train_labels, scoring=\"neg_mean_squared_error\", cv=10)\nlin_reg_rmse_scores = np.sqrt(-scores_lin_reg)\ndisplay_scores(lin_reg_rmse_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's also take a look at Random Forest Regressor. We see that RandomForestRegressor is the best of the three. We will use that going forward.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nforest_reg = RandomForestRegressor()\nforest_reg.fit(train_prepared, train_labels)\nrandom_tree_scores = cross_val_score(forest_reg, train_prepared, train_labels, scoring=\"neg_mean_squared_error\", cv=10)\nrandome_tree_rmse_scores  = np.sqrt(-random_tree_scores)\ndisplay_scores(randome_tree_rmse_scores )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-Tuning\nLet's use GridSearchCV to better our model.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'n_estimators': [3, 10, 20, 30], 'max_features': [2, 4, 6, 8]},\n    {'bootstrap': [False], 'n_estimators': [3, 10, 20, 30], 'max_features': [2, 4, 6, 8]},\n  ]\n\nforest_reg = RandomForestRegressor()\n\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\n\ngrid_search.fit(train_prepared, train_labels)\nprint(grid_search.best_params_)\nprint(grid_search.best_estimator_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's explore the final model with the test set.","metadata":{}},{"cell_type":"code","source":"final_model = grid_search.best_estimator_\n\nX_test_prepared = pipeline.transform(test_features)\n\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(test_labels, final_predictions)\nfinal_rmse = np.sqrt(final_mse) \nprint(final_rmse)\nplt.scatter(test_labels, final_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}