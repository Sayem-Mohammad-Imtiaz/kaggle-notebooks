{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\npd.set_option('display.max_columns',None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First and foremost will check is there any Nan values present in the Dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no Nan values present in Dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.quality.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As there are many labels, we will divide it into 3 labels. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['quality'] = np.where(df['quality']<=4,0,df['quality'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['quality'] = np.where((df['quality']<=6) & (df['quality']!=0 ),1,df['quality'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['quality'] = np.where( df['quality']>=7,2,df['quality'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we have converted quality variable into three labels as 0-poor,1-good,2-best."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.quality.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see here,Dataset is completely imbalanced.\n\nso,we need to fix it. Otherwise your model will baised to single label."},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.combine import SMOTETomek","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smk = SMOTETomek(random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X,y=smk.fit_sample(df.drop('quality',axis=1),df['quality'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([X,y],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.quality.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, it is perfectly balanced dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here,All the variables(features) are of Numerical type."},{"metadata":{},"cell_type":"markdown","source":"will analyse it one by one. "},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [feature for feature in df.columns if feature!='quality']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in features:\n    sns.boxplot(x=feature,data=df)\n    plt.xlabel(feature)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see there are number of Outliers present in each feature.\nso,here will use top encoding and bottom encoding technique to fix this."},{"metadata":{"trusted":true},"cell_type":"code","source":"dic = {}\nfor feature in features:\n    IQR = df[feature].quantile(0.75) - df[feature].quantile(0.25)\n    upper_bond = df[feature].quantile(0.75) + (IQR * 1.5)\n    lower_bond = df[feature].quantile(0.25) - (IQR * 1.5)\n    \n    df[feature] = np.where(df[feature]>upper_bond,upper_bond,df[feature])\n    df[feature] = np.where(df[feature]<lower_bond,lower_bond,df[feature])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in features:\n    sns.boxplot(x=feature,data=df)\n    plt.xlabel(feature)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will move to feature selection part."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selectk = SelectKBest(score_func=chi2,k=7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Best = selectk.fit(df.drop('quality',axis=1),df['quality'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Best.scores_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the scores related to each feature with respect to output variable(quality)."},{"metadata":{"trusted":true},"cell_type":"code","source":"features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfscores = pd.DataFrame(Best.scores_)\ndffeatures = pd.DataFrame(features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we are mapping each score with respect to each feature recpectively."},{"metadata":{"trusted":true},"cell_type":"code","source":"features_scores = pd.concat([dffeatures,dfscores],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_scores.columns = ['feature','scores']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_scores.sort_values(by='scores',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we will take top 7 features"},{"metadata":{"trusted":true},"cell_type":"code","source":"Best_features = features_scores[features_scores['scores']>30]['feature']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature Selection with the help of correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(df.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above we can notice that volatile acidity,citric acid,alcohol and sulphates are correlated more than fifty percent to target variable (quality)."},{"metadata":{},"cell_type":"markdown","source":"Now we split our dataset into train and test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(df[Best_features],df['quality'],test_size=0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score,classification_report","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But Outliers do not impact much on tree based models."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DecisionTreeClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_proba_train = model.predict_proba(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_proba_test = model.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_train,y_predict_proba_train,multi_class='ovo')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test,y_predict_proba_test,multi_class='ovo')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we know Decision tree follows low bias and high variance. Which means for training dataset it gives high accuracy but for testing dataset it gives less accuracy.\n\nThis problem can be easily solved with the help of ensemble techniques. \n\ne.g - RandomForest,XGBoost."},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test,y_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test,y_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above we can see that for class 1 precision and recall is falling behind"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(model,df[Best_features],df['quality'],scoring='accuracy',n_jobs=-1).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"with cross_val_score, Decisison Tree is giving 82% accuracy. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_proba_train = model.predict_proba(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_proba_test = model.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_train,y_predict_proba_train,multi_class='ovo')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test,y_predict_proba_test,multi_class='ovo')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see RandomForest fixed the problem of low bias high variance to low bias low variance."},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test,y_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test,y_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Precision and Recall is improved with Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(model,df[Best_features],df['quality'],scoring='accuracy',n_jobs=-1).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see Random  Forest Classifier is giving 88% with cross_val_score\n\nNow we will check with XGBClassifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_proba_train = model.predict_proba(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_proba_test = model.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_train,y_predict_proba_train,multi_class='ovo')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test,y_predict_proba_test,multi_class='ovo')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test,y_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test,y_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Precision and Recall is further improved with XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(model,df[Best_features],df['quality'],scoring='accuracy',n_jobs=-1).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBClassifier is giving 90% accuracy with cross_val_score"},{"metadata":{},"cell_type":"markdown","source":"we will improve model accuracy by using Hyperparameter Optimization.\n\nHere we are using RandomizedSearchCV."},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'n_estimators' : list(np.arange(5,101,1)) ,\n    'max_depth' : list(np.arange(3,16,1)) ,\n    'min_child_weight' : [1,3,4,5,6,7,8] ,\n    'learning_rate' : list(np.arange(0.05,0.35,0.05)) ,\n    'colsample_bytree' : [0.4,0.5,0.6,0.7],\n    'gamma' : [0.0,0.1,0.2,0.3,0.4]    \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search = RandomizedSearchCV(model,param_distributions=params,n_jobs=-1,scoring='accuracy',verbose=3,cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search.fit(df[Best_features],df['quality'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.4, gamma=0.4, gpu_id=-1,\n              importance_type='gain', interaction_constraints=None,\n              learning_rate=0.3, max_delta_step=0, max_depth=10,\n              min_child_weight=1, monotone_constraints=None,\n              n_estimators=37, n_jobs=0, num_parallel_tree=1,\n              objective='multi:softprob', random_state=0, reg_alpha=0,\n              reg_lambda=1, scale_pos_weight=None, subsample=1,\n              tree_method=None, validate_parameters=False, verbosity=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(model,df[Best_features],df['quality'],scoring='accuracy',n_jobs=-1).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see with the help of Hyperparameter Optimization we have improved 1% accuracy"},{"metadata":{},"cell_type":"markdown","source":"#### I hope you enjoyed a lot."},{"metadata":{},"cell_type":"markdown","source":"#### Thank You"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}