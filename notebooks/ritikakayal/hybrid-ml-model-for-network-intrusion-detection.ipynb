{"cells":[{"metadata":{"trusted":true,"_uuid":"5059b293b34a18afd43e6185dfcba4521bd87878"},"cell_type":"code","source":"# import relevant modules\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport sklearn\nimport imblearn\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Settings\npd.set_option('display.max_columns', None)\nnp.set_printoptions(threshold=np.nan)\nnp.set_printoptions(precision=3)\nsns.set(style=\"darkgrid\")\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d5309ee0cd9b2f807af7a3ad8a783f066a99d0d"},"cell_type":"markdown","source":"# LOAD DATA"},{"metadata":{"trusted":true,"_uuid":"14c15b2699fed6d95c4e3e4de5e4595b4e650920"},"cell_type":"code","source":"train = pd.read_csv(\"../input/Train_data.csv\")\ntest = pd.read_csv(\"../input/Test_data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3ca7cc2b990447e1a401f5b91ba8df21e33c845"},"cell_type":"code","source":"print(train.head(4))\n\nprint(\"Training data has {} rows & {} columns\".format(train.shape[0],train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ed1bc1c00f047f5fed78fb18100b3be7c3b7852"},"cell_type":"code","source":"print(test.head(4))\n\nprint(\"Testing data has {} rows & {} columns\".format(test.shape[0],test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bde409f4fe8da1fcb52751bf8c397ac346a2409f"},"cell_type":"markdown","source":"# EXPLORATORY ANALYSIS"},{"metadata":{"trusted":true,"_uuid":"f231cec911b59adb7de03dd877fb38fbc270e09a"},"cell_type":"code","source":"# Descriptive statistics\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06b94b1d1a0c6ba604a7d69c1495132c3f38710c"},"cell_type":"code","source":"print(train['num_outbound_cmds'].value_counts())\nprint(test['num_outbound_cmds'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e0f8a1adcd11b17c9bda454c8c29e486ac10f64"},"cell_type":"code","source":"#'num_outbound_cmds' is a redundant column so remove it from both train & test datasets\ntrain.drop(['num_outbound_cmds'], axis=1, inplace=True)\ntest.drop(['num_outbound_cmds'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df0fc1cc89e3a82a7b07e1662c99abf42deddbc9"},"cell_type":"code","source":"# Attack Class Distribution\ntrain['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d472d5c2705a8d322b20fa1994139a4c9cdaebce"},"cell_type":"markdown","source":"# SCALING NUMERICAL ATTRIBUTES"},{"metadata":{"trusted":true,"_uuid":"654b1e6dfb83362b836ed892df4deae0d48ce4b0"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# extract numerical attributes and scale it to have zero mean and unit variance  \ncols = train.select_dtypes(include=['float64','int64']).columns\nsc_train = scaler.fit_transform(train.select_dtypes(include=['float64','int64']))\nsc_test = scaler.fit_transform(test.select_dtypes(include=['float64','int64']))\n\n# turn the result back to a dataframe\nsc_traindf = pd.DataFrame(sc_train, columns = cols)\nsc_testdf = pd.DataFrame(sc_test, columns = cols)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd7451b8802bfb2201a978469b0636d082aa3a0a"},"cell_type":"markdown","source":"# ENCODING CATEGORICAL ATTRIBUTES"},{"metadata":{"trusted":true,"_uuid":"e3d637225809dfcc6aa3ea7ef4a9b1d55d2b436c"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\n\n# extract categorical attributes from both training and test sets \ncattrain = train.select_dtypes(include=['object']).copy()\ncattest = test.select_dtypes(include=['object']).copy()\n\n# encode the categorical attributes\ntraincat = cattrain.apply(encoder.fit_transform)\ntestcat = cattest.apply(encoder.fit_transform)\n\n# separate target column from encoded data \nenctrain = traincat.drop(['class'], axis=1)\ncat_Ytrain = traincat[['class']].copy()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"227915d0f6d7a22ec344d4a016049acecb0323f6"},"cell_type":"code","source":"train_x = pd.concat([sc_traindf,enctrain],axis=1)\ntrain_y = train['class']\ntrain_x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca90e6da174743f665a1fc640ff3070e502a3535"},"cell_type":"code","source":"test_df = pd.concat([sc_testdf,testcat],axis=1)\ntest_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58212e80c11969c02f35adbf42d2bba7881dfde0"},"cell_type":"markdown","source":"# FEATURE SELECTION"},{"metadata":{"trusted":true,"_uuid":"6608ad7beebd4cd2ca3cbc0999da5def00d0c4e5"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier();\n\n# fit random forest classifier on the training set\nrfc.fit(train_x, train_y);\n# extract important features\nscore = np.round(rfc.feature_importances_,3)\nimportances = pd.DataFrame({'feature':train_x.columns,'importance':score})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\n# plot importances\nplt.rcParams['figure.figsize'] = (11, 4)\nimportances.plot.bar();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d9cffa96fadfddb552370e614d3e9ccc7a2c420"},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nimport itertools\nrfc = RandomForestClassifier()\n\n# create the RFE model and select 10 attributes\nrfe = RFE(rfc, n_features_to_select=15)\nrfe = rfe.fit(train_x, train_y)\n\n# summarize the selection of the attributes\nfeature_map = [(i, v) for i, v in itertools.zip_longest(rfe.get_support(), train_x.columns)]\nselected_features = [v for i, v in feature_map if i==True]\n\nselected_features","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c20a81752c216722f4fdfb8ce2837956823c1b6d"},"cell_type":"markdown","source":"# DATASET PARTITION"},{"metadata":{"trusted":true,"_uuid":"6a5e5c54d8c5d760085d104bdc8c5eb44b6372b4"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,Y_train,Y_test = train_test_split(train_x,train_y,train_size=0.70, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb80c25403d2e9030d20e37eefa4d54c83dfb056"},"cell_type":"markdown","source":"# FITTING MODELS"},{"metadata":{"trusted":true,"_uuid":"9ab0f6c11f8772d9f1c008ee2b4d1f181af3effb"},"cell_type":"code","source":"from sklearn.svm import SVC \nfrom sklearn.naive_bayes import BernoulliNB \nfrom sklearn import tree\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Train KNeighborsClassifier Model\nKNN_Classifier = KNeighborsClassifier(n_jobs=-1)\nKNN_Classifier.fit(X_train, Y_train); \n\n# Train LogisticRegression Model\nLGR_Classifier = LogisticRegression(n_jobs=-1, random_state=0)\nLGR_Classifier.fit(X_train, Y_train);\n\n# Train Gaussian Naive Baye Model\nBNB_Classifier = BernoulliNB()\nBNB_Classifier.fit(X_train, Y_train)\n            \n# Train Decision Tree Model\nDTC_Classifier = tree.DecisionTreeClassifier(criterion='entropy', random_state=0)\nDTC_Classifier.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9753eed4eab0e0587d016b78efce930aa782f29f"},"cell_type":"markdown","source":"# EVALUATE MODELS"},{"metadata":{"trusted":true,"_uuid":"56a6d972dfc7236bddc02b254416331a45e1b57a"},"cell_type":"code","source":"from sklearn import metrics\n\nmodels = []\nmodels.append(('Naive Baye Classifier', BNB_Classifier))\nmodels.append(('Decision Tree Classifier', DTC_Classifier))\nmodels.append(('KNeighborsClassifier', KNN_Classifier))\nmodels.append(('LogisticRegression', LGR_Classifier))\n\nfor i, v in models:\n    scores = cross_val_score(v, X_train, Y_train, cv=10)\n    accuracy = metrics.accuracy_score(Y_train, v.predict(X_train))\n    confusion_matrix = metrics.confusion_matrix(Y_train, v.predict(X_train))\n    classification = metrics.classification_report(Y_train, v.predict(X_train))\n    print()\n    print('============================== {} Model Evaluation =============================='.format(i))\n    print()\n    print (\"Cross Validation Mean Score:\" \"\\n\", scores.mean())\n    print()\n    print (\"Model Accuracy:\" \"\\n\", accuracy)\n    print()\n    print(\"Confusion matrix:\" \"\\n\", confusion_matrix)\n    print()\n    print(\"Classification report:\" \"\\n\", classification) \n    print()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"304fddb4f81dbe6893caf007ffdde527243fa119"},"cell_type":"markdown","source":"# VALIDATING MODELS"},{"metadata":{"trusted":true,"_uuid":"5ab78f517053e419f6c6667e90a0b352ccb5f531"},"cell_type":"code","source":"for i, v in models:\n    accuracy = metrics.accuracy_score(Y_test, v.predict(X_test))\n    confusion_matrix = metrics.confusion_matrix(Y_test, v.predict(X_test))\n    classification = metrics.classification_report(Y_test, v.predict(X_test))\n    print()\n    print('============================== {} Model Test Results =============================='.format(i))\n    print()\n    print (\"Model Accuracy:\" \"\\n\", accuracy)\n    print()\n    print(\"Confusion matrix:\" \"\\n\", confusion_matrix)\n    print()\n    print(\"Classification report:\" \"\\n\", classification) \n    print()        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecba1871b65bdfd796099ec707855a626d39446c"},"cell_type":"code","source":"# PREDICTING FOR TEST DATA using KNN\npred_knn = KNN_Classifier.predict(test_df)\npred_NB = BNB_Classifier.predict(test_df)\npred_log = LGR_Classifier.predict(test_df)\npred_dt = DTC_Classifier.predict(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extracting TP FP TN FN"},{"metadata":{"trusted":true},"cell_type":"code","source":"def perf_measure(y_actual, y_pred):\n    TP = 0\n    FP = 0\n    TN = 0\n    FN = 0\n\n    for i in range(len(y_pred)): \n        if y_actual.iat[i]==y_pred[i]=='anomaly':\n           TP += 1\n        if y_pred[i]=='anomaly' and y_actual.iat[i]!=y_pred[i]:\n           FP += 1\n        if y_actual.iat[i]==y_pred[i]=='normal':\n           TN += 1\n        if y_pred[i]=='normal' and y_actual.iat[i]!=y_pred[i]:\n           FN += 1\n        \n    return (TP, FP, TN, FN)\n\n\nfor i, v in models:\n    print(\"For model:\", i)\n    TP, FP, TN, FN = perf_measure(Y_test, v.predict(X_test))\n    print (\"TP:\", TP, \"\\tFP:\", FP, \"\\t\\tTN:\", TN, \"\\tFN:\", FN)\n    \n    # Testing for first row\n    #print (\"Expected: \", Y_test.iloc[0], \"Predicted: \", v.predict(X_test).reshape(1, -1)[0][0] )\n    print()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing for second row\nfor i, v in models:\n    print(\"For model: \", i)\n    print (\"Expected: \", Y_test.iloc[2], \"\\tPredicted: \", v.predict(X_test).reshape(1, -1)[0][2] )\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Functions to extract locations of FP, FN as a pandas series"},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_FP(y_actual, y_pred):\n    FP = []\n\n    for i in range(len(y_pred)): \n        if y_pred[i]=='anomaly' and y_actual.iat[i]!=y_pred[i]:\n           FP.append(i)   \n    return (pd.Series(FP))\n    \ndef find_FN(y_actual, y_pred):\n    FN = []\n\n    for i in range(len(y_pred)): \n        if y_pred[i]=='normal' and y_actual.iat[i]!=y_pred[i]:\n           FN.append(i)\n    return (pd.Series(FN))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Combining Naive Bayes and Decision Tree"},{"metadata":{},"cell_type":"markdown","source":"## Getting FP and FN row location from NB output as pd.Series"},{"metadata":{"trusted":true},"cell_type":"code","source":"FP_NB= find_FP(Y_test, models[0][1].predict(X_test))\nprint(\"Size of number of FP:\", FP_NB.size) \nFN_NB= find_FN(Y_test, models[0][1].predict(X_test))\nprint(\"Size of number of FN:\", FN_NB.size) \n\n# Testing \nFP_NB.head(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting FP FN row entry from X_test and Y_test as pd.DataFrame and pd.Series respectively"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_subset=[]\nY_test_subset=[]\nfor i in FP_NB:\n    X_test_subset.append(X_test.iloc[i])\n    Y_test_subset.append(Y_test.iat[i])\nfor i in FN_NB:\n    X_test_subset.append(X_test.iloc[i])\n    Y_test_subset.append(Y_test.iat[i])\n    \nX_test_sub=pd.DataFrame(X_test_subset)\nY_test_sub=pd.Series(Y_test_subset)\nprint(\"Size of X_test_sub:\", X_test_sub.shape[0]) \nprint(\"Size of Y_test_sub:\", Y_test_sub.size) \n\n# To check for each false positive\n#for i in FP_NB:\n#    print (\"Expected: \", Y_test.iloc[i], \"Predicted: \", models[1][1].predict(X_test).reshape(1, -1)[0][i] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(X_test)) \nX_test.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(X_test_sub))\nX_test_sub.head(4)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(Y_test)) \nY_test.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(Y_test_sub))\nY_test_sub.head(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validating the combined model"},{"metadata":{},"cell_type":"markdown","source":"## Validating results for the FP FN subset in the combined model"},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = metrics.accuracy_score(Y_test_sub, models[1][1].predict(X_test_sub))\nconfusion_matrix = metrics.confusion_matrix(Y_test_sub, models[1][1].predict(X_test_sub))\nclassification = metrics.classification_report(Y_test_sub, models[1][1].predict(X_test_sub))\nprint()\nprint('============================== {} Model Test Results =============================='.format(\"NB -> DT\"))\nprint()\nprint (\"Model Accuracy:\" \"\\n\", accuracy)\nprint()\nprint(\"Confusion matrix:\" \"\\n\", confusion_matrix)\nprint()\nprint(\"Classification report:\" \"\\n\", classification) \nprint() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Validating hybrid model, NB + DT"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"For Naive Bayes:\")\nTP_old, FP_old, TN_old, FN_old = perf_measure(Y_test, BNB_Classifier.predict(X_test))\nprint (\"TP:\", TP_old, \"\\tFP:\", FP_old, \"\\t\\tTN:\", TN_old, \"\\tFN:\", FN_old)\n\nprint()\nprint(\"For Naive Bayes -> Decision Tress:\")\nTP_new, FP_new, TN_new, FN_new = perf_measure(Y_test_sub, DTC_Classifier.predict(X_test_sub))\nprint (\"TP:\", TP_new, \"\\tFP:\", FP_new, \"\\t\\tTN:\", TN_new, \"\\tFN:\", FN_new)\n\nprint()\nprint(\"For Naive Bayes + Decision Tress:\")\ntp = TP_old +TP_new\nfp = FP_new\ntn = TN_old +TN_new\nfn = FN_new\nprint (\"TP:\", tp, \"\\tFP:\", fp, \"\\t\\tTN:\", tn, \"\\tFN:\", fn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Accuracy (all correct / all) = TP + TN / TP + TN + FP + FN\n## 2. Misclassification (all incorrect / all) = FP + FN / TP + TN + FP + FN\n## 3. Precision (true positives / predicted positives) = TP / TP + FP\n## 4. Sensitivity aka Recall (true positives / all actual positives) = TP / TP + FN\n## 5. Specificity (true negatives / all actual negatives) =TN / TN + FP"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_old= (TP_old + TN_old) / (TP_old + FP_old + TN_old + FN_old)\nmis_old= (FP_old + FN_old) / (TP_old + FP_old + TN_old + FN_old)\nprec_old= TP_old / (TP_old + FP_old)\nsen_old= TP_old / (TP_old + FN_old)\nspec_old= TN_old / (TN_old + FP_old)\n\nacc= (tp + tn) / (tp + fp + tn + fn)\nmis= (fp + fn) / (tp + fp + tn + fn)\nprec= tp / (tp + fp)\nsen= tp / (tp + fn)\nspec= tn / (tn + fp)\n\nprint (\"Accuracy\")\nprint (\"Old: \", acc_old, \"\\tNew: \", acc)\nprint (\"\\nMisclassification\")\nprint (\"Old: \", mis_old, \"\\tNew: \", mis)\nprint (\"\\nPrecision\")\nprint (\"Old: \", prec_old, \"\\tNew: \", prec)\nprint (\"\\nSensitivity\")\nprint (\"Old: \", sen_old, \"\\tNew: \", sen)\nprint (\"\\nSpecificity\")\nprint (\"Old: \", spec_old, \"\\tNew: \", spec)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"# set width of bar\nbarWidth = 0.25\nfig = plt.subplots(figsize =(12, 8))\n \n# set height of bar\nNB = [TP_old, FP_old, TN_old, FN_old]\nNBandDT = [tp, fp, tn, fn]\n \n# Set position of bar on X axis\nbr1 = np.arange(len(NB))\nbr2 = [x + barWidth for x in br1]\n \n# Make the plot\nplt.bar(br1, NB, color ='b', width = barWidth, edgecolor ='grey', label ='Naive Bayes')\nplt.bar(br2, NBandDT, color ='g', width = barWidth, edgecolor ='grey', label ='Naive Bayes and Decision Tree')\n \n# Adding Xticks\nplt.xlabel('Confusion Matrix Element', fontweight ='bold', fontsize = 15)\nplt.ylabel('Value', fontweight ='bold', fontsize = 15)\nplt.xticks([r + barWidth for r in range(len(NB))], ['TP', 'FP', 'TN', 'FN'])\n \nplt.legend()\nplt.title(\"Confusion Matrix\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set width of bar\nbarWidth = 0.25\nfig = plt.subplots(figsize =(12, 8))\n \n# set height of bar\nOld = [acc_old, mis_old, prec_old, sen_old, spec_old]\nNew = [acc, mis, prec, sen, spec]\n \n# Set position of bar on X axis\nbr1 = np.arange(len(Old))\nbr2 = [x + barWidth for x in br1]\n \n# Make the plot\nplt.bar(br1, Old, color ='b', width = barWidth, edgecolor ='grey', label ='Old')\nplt.bar(br2, New, color ='g', width = barWidth, edgecolor ='grey', label ='New')\n\n\n# Adding Xticks\nplt.xlabel('Performance Metrics', fontweight ='bold', fontsize = 15)\nplt.ylabel('Value', fontweight ='bold', fontsize = 15)\nplt.xticks([r + barWidth for r in range(len(NB))], ['Accuracy', 'Misclassification', 'Precision', 'Sensitivity', 'Specificity'])\n \nplt.legend()\nplt.title(\"Comparison of performance metrics\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accPercent= ((acc- acc_old)/acc_old) *100\nmisPercent= ((mis_old- mis)/mis_old) *100\nprecPercent= ((prec- prec_old)/prec_old) *100\nsenPercent= ((sen- sen_old)/sen_old) *100\nspecPercent= ((spec- spec_old)/spec_old) *100\n\nprint (\"Accuracy increase percentage: \" ,accPercent, \"%\")\nprint (\"Missclasification decrease percentage: \" ,misPercent, \"%\")\nprint (\"Precison increase percentage: \", precPercent, \"%\")\nprint (\"Sensitivity increase percentage: \", senPercent, \"%\")\nprint (\"Specificity increase percentage: \", specPercent, \"%\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}