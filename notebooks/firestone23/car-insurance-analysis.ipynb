{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport math\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = [10,6]\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# EDA\ndata = pd.read_csv(\"/kaggle/input/health-insurance-cross-sell-prediction/train.csv\")\nprint(data.columns)\nprint(data.shape)\nprint(data.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot 1\ndata[\"Gender\"].value_counts().plot.barh(color=\"blue\",\n                                        title=\"Count by Gender\")\nplt.xlabel(\"Count\")\nplt.ylabel(\"Gender\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot 2\ndata[\"Age\"].describe()\n \n# Create bins for age\ndef create_age_bins(x,bins=10):\n    max = int(math.floor((x.max()+10)/10))*10\n    min = int(math.floor((x.min()-10)/10))\n    bin_width = (max-min)/bins\n    intervals = [min+x*bin_width for x in range(bins+1)]\n    labels = [i for i in range(1,len(intervals))]\n    return pd.cut(x,bins=intervals, labels=labels)\n \ndata[\"Age Bins\"] = create_age_bins(data[\"Age\"],bins=6)\ndata[\"Age Bins\"].head()\n \ndata[\"Age Bins\"].value_counts().plot.barh(color=\"purple\",\n                                          title=\"Count by Age Gap\")\nplt.xlabel(\"Count\")\nplt.ylabel(\"Age Gap\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for missing values\npd.isnull(data).sum() # None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dtypes\ndata.dtypes\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# See if can convert some dtypes from object to int or float\ndata[\"Vehicle_Age\"].unique() # Yes\ndata[\"Vehicle_Age\"]=data[\"Vehicle_Age\"].replace({\"> 2 Years\":2, \"1-2 Year\":1,\"< 1 Year\":0})\ncat_type = pd.CategoricalDtype(categories=data[\"Vehicle_Age\"].unique()\n                              ,ordered=True)\ndata[\"Vehicle_Age\"] = data[\"Vehicle_Age\"].astype(cat_type)\ndata[\"Vehicle_Damage\"].unique() #Yes\ndata[\"Vehicle_Damage\"]=data[\"Vehicle_Damage\"].replace({\"Yes\":1, \"No\":0})\n \ndata[\"Gender\"] = data[\"Gender\"].replace({\"Male\":0,\"Female\":0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modeling\n# Predict Response based on other variables\n \n# Drop id, Age\ndata = data.drop(columns=[\"id\",\"Age\"])\n \n# Split into train, validation, and test sets\n# Use a 70/10/20 split\n \ndef train_val_test_split(train_per=.70, val_per=.10,test_per=.20,nrows=0):\n    train_split = math.floor(nrows*train_per)\n    val_split = math.floor(nrows*val_per)\n   \n    train_index = np.random.choice(np.arange(0,nrows),\n                                   train_split,\n                                   replace=False)\n    train_mask = np.isin(np.arange(0,nrows), train_index )\n    val_index=np.random.choice(np.arange(0,nrows)[~train_mask],\n                     val_split,\n                     replace=False)\n    val_mask = np.isin(np.arange(0,nrows),\n                       np.concatenate([train_index,val_index]))\n    test_index = np.arange(0, nrows)[~val_mask]\n   \n    return train_index, val_index,test_index\n \ntrain_index, val_index, test_index = train_val_test_split(nrows=data.shape[0])\n \ntrain_index.shape\nval_index.shape\ntest_index.shape\n \n \n# Split training set into X,Y variables\nX_train = data.iloc[train_index,:].drop(columns=[\"Response\"])\nY_train = data.iloc[train_index,:][\"Response\"]\nX_val = data.iloc[val_index,:].drop(columns=[\"Response\"])\nY_val = data.iloc[val_index,:][\"Response\"]\nX_test = data.iloc[test_index,:].drop(columns=[\"Response\"])\nY_test = data.iloc[test_index,:][\"Response\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Y_train.value_counts())\nprint(Y_val.value_counts())\nprint(Y_test.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit Model\nfrom sklearn.linear_model import LogisticRegression\nLOGI_FIT_1 = LogisticRegression(max_iter=2500)\nLOGI_FIT_1.fit(X_train,Y_train)\n \n# Predict classes using val set\nval_probs=LOGI_FIT_1.predict_proba(X_val)\nfind_class = np.vectorize(lambda x: 0 if x>0.5 else 1)\nval_preds = find_class(val_probs[:,0])\n \n \ndef misclass_rate(preds, real):\n    cross = pd.crosstab(preds,real)\n    cross.columns.name = \"Predictions\"\n    misclass_rate =  (cross.iloc[0,1]+cross.iloc[1,0])/(cross.iloc[0,0]+cross.iloc[1,1])\n    print(\"The misclassification rate is %0.0f percent.\"%(misclass_rate*100))\n \n# Predict classs using test set\ntest_preds = LOGI_FIT_1.predict(X_test)\nmisclass_rate(test_preds, Y_test)\n \ntest_probs=LOGI_FIT_1.predict_proba(X_test)\ntest_preds=find_class(test_probs[:,0])\nmisclass_rate(test_preds, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree\nfrom sklearn import tree\nTREE_FIT_1 = tree.DecisionTreeClassifier()\nTREE_FIT_1.fit(X_train,Y_train)\nval_preds=TREE_FIT_1.predict(X_val)\ntest_preds = TREE_FIT_1.predict(X_test)\n \nmisclass_rate(val_preds,Y_val)\nmisclass_rate(test_preds,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bagging\nfrom sklearn.ensemble import BaggingClassifier\nBAGGER = BaggingClassifier(base_estimator=tree.DecisionTreeClassifier(),\n                           n_estimators=100,random_state=0).fit(X_train,Y_train)\n \nval_preds = BAGGER.predict(X_val)\ntest_preds = BAGGER.predict(X_test)\nmisclass_rate(val_preds,Y_val)\nmisclass_rate(test_preds,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC curve\nfrom sklearn import metrics\nlogi_probs = LOGI_FIT_1.predict_proba(X_test)[:,1]\ntree_probs = TREE_FIT_1.predict_proba(X_test)[:,1]\nbag_probs  = BAGGER.predict_proba(X_test) [:,1]\n \nfpr1, tpr1, threshold1 = metrics.roc_curve(Y_test, logi_probs)\nroc_auc1 = metrics.auc(fpr1, tpr1)\n \nfpr2, tpr2, threshold2 = metrics.roc_curve(Y_test, tree_probs)\nroc_auc2 = metrics.auc(fpr2, tpr2)\n \nfpr3, tpr3, threshold1 = metrics.roc_curve(Y_test, bag_probs)\nroc_auc3 = metrics.auc(fpr1, tpr1)\n \nplt.plot(fpr1,tpr1,\"b-\",label=\"LOGI AUC %0.2f\"%roc_auc1)\nplt.plot(fpr2,tpr2,\"k-\",label=\"Tree AUC %0.2f\"%roc_auc2)\nplt.plot(fpr3,tpr3,\"g-\",label=\"Bag AUC %0.2f\"%roc_auc3)\n \n \nplt.plot([0,1],[0,1] ,\"r\",linestyle=\"--\")\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.legend(loc=\"lower right\")\nplt.title(\"ROC Curve\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot ROC using seaborn\nsns.set_style(\"whitegrid\")\nplt.plot(fpr1,tpr1,\"b-\",label=\"LOGI AUC %0.2f\"%roc_auc1)\nplt.plot(fpr2,tpr2,\"k-\",label=\"Tree AUC %0.2f\"%roc_auc2)\nplt.plot(fpr3,tpr3,\"g-\",label=\"Bag AUC %0.2f\"%roc_auc3)\nplt.plot([0,1],[0,1] ,\"r\",linestyle=\"--\")\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.legend(loc=\"lower right\")\nplt.title(\"ROC Curve\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}