{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from time import time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from string import punctuation\nfrom matplotlib import pyplot as plt\nfrom collections import defaultdict\nfrom nltk.corpus import stopwords\ncachedstopwords = stopwords.words('english')\nfrom torch.utils.data.dataset import Dataset\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport torch\nimport torch.nn as nn\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" print(torch.__version__)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sentiment-analysis-for-financial-news/all-data.csv',engine='python')\ndf.columns = ['polarity', 'review']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['polarity'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nidx = random.randint(0,df['polarity'].count())\nprint(\"IDX :\",idx)\nprint(df.iloc[idx]['review'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review_size=list(map(len,df['review']))\nmax_len = max(review_size)\nmax_len","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#since max size is only 298 so i am not trimming \nsentence_len_dict=defaultdict(lambda:0)\nfor i in review_size:\n    sentence_len_dict[i]+=1\nplt.bar(sentence_len_dict.keys(),sentence_len_dict.values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def removestopwords(x):\n    return [i.lower() for i in x.split() if i.lower() not in cachedstopwords]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['p_review'] = df['review']\ndf['p_review'].replace(to_replace = '[{}]'.format(punctuation),inplace=True,value='',regex=True)\ndf['p_review'].replace(to_replace = '\\s+',inplace=True,value=' ',regex=True)\ndf['p_review'] = df['p_review'].apply(removestopwords)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#converting polarity to numnber\ndf['p_polarity']=df['polarity']\ndf['p_polarity'] = df['p_polarity'].map({'neutral':2,'positive':1,'negative':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['length_of_sentence']=df['p_review'].str.len()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indexNames = df[ df['length_of_sentence'] == 0 ].index\n# Delete these row indexes from dataFrame\ndf.drop(indexNames , inplace=True)\ndf.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get all keywords\nword_cnt = defaultdict(lambda:0)\nword_cnt[' ']=1\nfor i in df['p_review']:\n    for j in i:\n        word_cnt[j]+=1\nvoc=word_cnt.keys()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(list(word_cnt.keys())[:100],list(word_cnt.values())[:100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews = list(df['p_review'])\npolarity=list(df['p_polarity'])\nlength_of_sentence = list(df['length_of_sentence'])\nmy_idxs=list(range(0,len(polarity)))\nnp.random.seed(3)\nnp.random.shuffle(my_idxs)\ndivider = round(len(polarity)*.80)\n\ntrain_dataset = [{'review':reviews[i],'polarity':polarity[i],'length_of_sentence':length_of_sentence[i]} for i in my_idxs[:divider]]\nvalid_dataset = [{'review':reviews[i],'polarity':polarity[i],'length_of_sentence':length_of_sentence[i]} for i in my_idxs[divider:]]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train Set: {},Validation Set: {}\".format(len(train_dataset),len(valid_dataset)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wrd_to_idx=dict()\nidx_to_wrd=dict()\nfor i,j in enumerate(voc):\n    wrd_to_idx[j]=i\n    idx_to_wrd[i]=j","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epoch =50\nbatch_sz=16\ninput_dim=100\nhidden_layers=50\nstack_layer=2\nout_dim=3\nlr=0.1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wrd_to_idx[' ']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import copy\nclass MyLoader(Dataset):\n    def __init__(self,data_set):\n        \n        self.data=data_set\n        \n    def __getitem__(self,index):\n        wrd_idx=[]\n        \n        for idx,word in enumerate(self.data[index]['review']):\n            \n            wrd_idx.append(wrd_to_idx[word])\n        \n        return wrd_idx,self.data[index]['polarity'],self.data[index]['length_of_sentence']\n        \n    def __len__(self):\n        return len(self.data)\ndef my_collate_fn(batch):\n    reviews_raw=[i[0] for i in batch]\n    polarities_raw=[i[1] for i in batch]\n    length_of_sentence_raw=[i[2] for i in batch]\n    reviews=[]\n    polarities=[]\n    length_of_sentence=[]\n    #print(\"Before :\",reviews_raw,\"\\n\\n\",polarities_raw,\"\\n\\n\",length_of_sentence_raw)\n    #sorting in decreasing order\n    for k1 in np.argsort(np.array(length_of_sentence_raw))[::-1]:\n        reviews.append( torch.tensor(reviews_raw[k1], dtype=torch.float))\n        polarities.append(polarities_raw[k1])\n        length_of_sentence.append(length_of_sentence_raw[k1])\n    #print(\"After :\",reviews,\"\\n\\n\",polarities,\"\\n\\n\",length_of_sentence)\n    torch.manual_seed(5)\n    emb = nn.Embedding(len(voc), input_dim)\n    emb_input = torch.nn.utils.rnn.pad_sequence(reviews,batch_first=True)\n    #emb_input = torch.tensor(reviews, dtype=torch.float)\n#     emb_input = pad_sequence(emb_input,batch_first=True)\n    \n#     print(new_reviews)\n    embidd = emb(emb_input)\n    p_embidd = torch.tensor(polarities, dtype=torch.float)\n    length_of_sentence = torch.tensor(length_of_sentence, dtype=torch.float)\n    return embidd,p_embidd,length_of_sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_obj = MyLoader(train_dataset)\ntrain_loader=torch.utils.data.DataLoader(train_obj,batch_size=batch_sz,collate_fn=my_collate_fn)\n\nvalid_obj = MyLoader(valid_dataset)\nvalid_loader=torch.utils.data.DataLoader(valid_obj,batch_size=batch_sz,collate_fn=my_collate_fn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device=\"cpu\"\nif torch.cuda.is_available():\n    device='cuda'\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#torch function to padd seq\n# second seq is of size .after passing to pad_sequence it will become 5 (it will be size of max swq len)\nimport torch \n\nseq_batch = [torch.tensor([[1, 1],\n                           [2, 2],\n                           [3, 3],\n                           [4, 4],\n                           [5, 5]]),\n             torch.tensor([[10, 10],\n                           [20, 20],\n                           [20, 20]]),\n            torch.tensor([[10, 10],\n                           [20, 20]])]\n\nseq_lens = [5,3,2]\npadded_seq_batch = torch.nn.utils.rnn.pad_sequence(seq_batch, batch_first=True)\nprint(type(padded_seq_batch))\nprint(padded_seq_batch,'\\n\\n',padded_seq_batch.shape)\npacked_seq_batch = torch.nn.utils.rnn.pack_padded_sequence(padded_seq_batch, lengths=seq_lens, batch_first=True)\nprint(\"packed_seq_batchpacked_seq_batch:\",packed_seq_batch)\nprint(packed_seq_batch.data.shape)\n\nprint(\"\\n\\n\\n********************Unpacking Sequence********************\\n\")\n# unpack sequence\nunpacked, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(packed_seq_batch, batch_first=True)\nprint(unpacked.shape, unpacked_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SentimentAnlysis(nn.Module):\n    def __init__(self,input_dim,hidden_layers,stack_layer,out_dim):\n        super(SentimentAnlysis, self).__init__()\n        self.input_dim = input_dim\n        self.hidden_layers = hidden_layers\n        self.stack_layer = stack_layer\n        self.out_dim = out_dim\n        #if input shape is (batch_size, seq_len, features(dimension)) then batch_first should br True\n        self.rnn = nn.RNN(self.input_dim,self.hidden_layers,self.stack_layer,batch_first=True)\n        self.fc=nn.Linear(self.hidden_layers,self.out_dim)\n        \n    def forward(self,input_,hidden_,seq_lengths):\n        batch_size = input_.shape[0]\n        \n        #since we are passing batch as first dimension we need to transpose\n        #from (batch*len_of_seq*dim)---> (len_of_seq*batch*dim)\n        \n#         n_input = torch.nn.utils.rnn.pad_sequence(input_, batch_first=True)\n        \n       \n        #Excluding padding during training\n        packed_input = pack_padded_sequence(input_, seq_lengths.cpu().numpy(),batch_first=True)\n#         print(\"packed_input :\",packed_input.shape)\n        \n        #input\n            #n_input = (seq_len, batch, input_size)\n            #hidden_ = (num_layers * num_directions, batch, hidden_size).\n        #output\n#           is the output value at all time-steps of the RNN layer \n            #out  (seq_len, batch, num_directions * hidden_size)\n            \n            #is the hidden value at the last time-step RNN layers\n            #nxt_hidden (num_layers * num_directions, batch, hidden_size)\n            \n        out,nxt_hidden = self.rnn(packed_input,hidden_)\n        \n        \n#         print(\"Input Shape of FC :\",out.shape)\n#         print(out)\n        \n        \n        unpacked, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n        \n        out = self.fc(unpacked)\n#         print(\"\\n\\npopopo\",out)\n#         print(\"Output Shape of FC :\",out.shape)\n#         print(\"-\"*30)\n        \n         # softmax function\n#         sft_max_out = self.multi(out)\n        \n#         # reshape to be batch_size first\n#         sft_max_out = sft_max_out.view(batch_size, -1)\n#         sft_max_out = sft_max_out[:, -1] # get last batch of labels\n        return out,nxt_hidden\n    \n    def initilizeh(self,batch_size):\n        '''\n            (num_layers * num_directions, batch, hidden_size)\n            \n            bidirection is false \n        '''\n        h = np.zeros((self.stack_layer,batch_size,self.hidden_layers))\n        return torch.tensor(h,dtype=torch.float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SentimentAnlysis(input_dim,hidden_layers,stack_layer,out_dim)\nmodel.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Running on \",device)\nsft = nn.Softmax(dim=1)\nfor itr in range(0,epoch):\n    loss_=0\n    correct_cnt=0\n    valid_loss=0\n    start_time = time()\n    ################ Train ###################################################\n    model.train()\n    for i,(records,polarity,sent_lens) in enumerate(train_loader): \n        hitten = model.initilizeh(records.shape[0])\n        optimizer.zero_grad()\n        out_t,hid = model(records.to(device),hitten.to(device),sent_lens)\n        torch.set_printoptions(profile=\"full\")\n\n        \n        out_t = out_t.squeeze()\n         # reshape to be batch_size first        \n        out_t = out_t[:, -1] # get last batch of labels\n#         print(\"Last time step :\",out_t)\n#         print(\"Last time step :\",out_t.shape)\n#         print(\"*\"*30,\"\\n\\n\\n\\n\")\n        d=polarity.unsqueeze(dim=1)\n        #print(\"aaaa\",polarity)\n#         print(out.squeeze(),\"\\n\\n\\n\",polarity)\n#         o=torch.transpose(out,0,1)\n#         loss = criterion(o.to(device),(d.float()).to(device))\n#         print(out_t)\n        out_t = sft(out_t)\n    \n#       oss = criterion(output, target_tensor)\n#       loss.backward()\n#       optimizer.step()\n    \n        loss = criterion(out_t.to(device),polarity.float().to(device))\n        loss_+=loss.item()\n        loss.backward()\n        optimizer.step()\n    ################################# validation ##############################\n    model.eval()\n    for i,(records,polarity,sent_lens) in enumerate(valid_loader): \n        hitten = model.initilizeh(records.shape[0])\n        \n        optimizer.zero_grad()\n        out,hid = model(records.to(device),hitten.to(device),sent_lens)\n#         out_t = torch.transpose(out,0,1)\n        #out_t = out_t.squeeze()\n        out_t = out.squeeze()\n        out_t = out_t[:, -1] # get last batch of labels\n        d=polarity.unsqueeze(dim=1)\n#         print(out_t)\n#         print(\"\\n\\n\",polarity)\n        \n        \n        out_t = sft(out_t)\n        oot = out_t\n        #this will return max from each element and its index(for us undex is noting but label i,e 0,1,2)\n        _,pred_lab = torch.max(oot.data, 1)\n        \n        correct = (pred_lab.to(device) == polarity.to(device)).sum()\n        correct_cnt+=correct.item()\n        \n        v_loss = criterion(out_t.to(device),polarity.float().to(device))\n        valid_loss+=v_loss.item()\n        \n        \n    end_time = time()\n    time_taken = end_time - start_time # time_taken is in seconds\n    hours, rest = divmod(time_taken,3600)\n    minutes, seconds = divmod(rest, 60)\n    \n    print(\"Epoch :\",itr+1 ,\"Train Loss :\",loss_/len(train_dataset),\"Validation Loss :\",valid_loss/len(valid_dataset),\" Accuracy :\",correct_cnt/len(valid_dataset),\" Time Taken :\",hours,\" hr::\",minutes,\" min::\",round(seconds),\" ss\")\n    \n#  n_input : torch.Size([25, 16, 100])\n# packed_input : torch.Size([202, 100])   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running on  cpu\n# Epoch : 1 Train Loss : 0.058593509874036234 Validation Loss : 0.05998079787835986  Accuracy : 0.5758513931888545  Time Taken : 0.0  hr:: 0.0  min:: 11  ss\n# Epoch : 2 Train Loss : 0.057905072250673845 Validation Loss : 0.06012444765575161  Accuracy : 0.5779153766769866  Time Taken : 0.0  hr:: 0.0  min:: 9  ss\n# Epoch : 3 Train Loss : 0.05750968081720414 Validation Loss : 0.060485478279145265  Accuracy : 0.5768833849329206  Time Taken : 0.0  hr:: 0.0  min:: 11  ss\n# Epoch : 4 Train Loss : 0.05733654836685427 Validation Loss : 0.06053810675077763  Accuracy : 0.5717234262125903  Time Taken : 0.0  hr:: 0.0  min:: 7  ss\n# Epoch : 5 Train Loss : 0.056865862569501324 Validation Loss : 0.061418353034984954  Accuracy : 0.5717234262125903  Time Taken : 0.0  hr:: 0.0  min:: 10  ss\n# Epoch : 6 Train Loss : 0.056385914910224176 Validation Loss : 0.062385602724441436  Accuracy : 0.5717234262125903  Time Taken : 0.0  hr:: 0.0  min:: 10  ss\n# Epoch : 7 Train Loss : 0.05605482151431422 Validation Loss : 0.06258670153755645  Accuracy : 0.5737874097007224  Time Taken : 0.0  hr:: 0.0  min:: 10  ss\n# Epoch : 8 Train Loss : 0.05585221382879442 Validation Loss : 0.0627989529394636  Accuracy : 0.5706914344685242  Time Taken : 0.0  hr:: 0.0  min:: 10  ss\n# Epoch : 9 Train Loss : 0.05556135056095739 Validation Loss : 0.0628178871945085  Accuracy : 0.5748194014447885  Time Taken : 0.0  hr:: 0.0  min:: 10  ss\n# Epoch : 10 Train Loss : 0.055119802482666506 Validation Loss : 0.0637615744728053  Accuracy : 0.5717234262125903  Time Taken : 0.0  hr:: 0.0  min:: 7  ss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x =torch.tensor([\n                    [-0.2189, -1.0233,  1.1002],\n                    [-1.6892,  0.8920, -0.5510],\n                    [-1.4843,  0.1082,  0.9838],\n                    [-1.7498, -0.3874,  0.6971],\n                    [-1.9330, -0.5803,  0.3496],\n                    [-1.7892, -0.3349,  0.6219],\n                    [-1.8425, -0.4778,  0.5351],\n                    [-1.8895, -0.5115,  0.4298],\n                    [-1.8252, -0.4108,  0.5649],\n                    [-1.8655, -0.4934,  0.4911],\n                    [-1.8687, -0.4787,  0.4773],\n                    [-1.8434, -0.4486,  0.5291],\n                    [-1.8675, -0.4878,  0.4852],\n                    [-1.8595, -0.4676,  0.4975],\n                    [-1.8537, -0.4655,  0.5100],\n                    [-1.8643, -0.4797,  0.4900]\n                ]\n               )\nprint(x.shape)\nprint([2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2])\nsft = nn.Softmax(dim=1)\noot = sft(x)\noot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#this will return max from each element and its index(for us undex is noting but label i,e 0,1,2)\n_,pred_lab = torch.max(oot.data, 1)\npred_lab\n# [2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correct = (pred_lab == torch.tensor([2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2])).sum()\ncorrect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loss = nn.CrossEntropyLoss()\n# _input = torch.randn(3, 2, requires_grad=True)\n# target = torch.empty(3, dtype=torch.float).random_(2)\n# output = loss(_input, target)\n# output\n\ncriterion = nn.CrossEntropyLoss().cuda()\n_input = torch.autograd.Variable(torch.randn((3,5)))\ntgt = torch.autograd.Variable(torch.LongTensor(3).random_(5))\nprint(_input,_input.shape)\nprint(tgt)\nloss = criterion(_input,tgt)\nloss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#illustrating fc\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hidden_size=3\nbatch=3\nseq_len=2\n\nT_data = [[[1., 2.,1.], [3., 4.,1.]],\n          [[5., 6.,1.], [7., 8.,1.]],\n          [[1., 2.,1.], [3., 4.,1.]],\n         ]\nT = torch.tensor(T_data)\n\nT = T.transpose(0,1)\nprint(\"Output of RNN\\n\\n\")\nprint(T)\nprint(T.shape)\n\nfc=nn.Linear(hidden_size,1)\nz = fc(T)\nprint(\"\\n\\nOutput of FC\\n\\n\")\nprint(z)\nprint(z.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"4/2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# torch.set_printoptions(precision=10)\nzz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array([-4.3963e-01,  2.9392e-01,  2.1258e-01,  6.5708e-01,  2.2840e-01,\n          -9.8323e-02,  2.0006e-01, -7.0602e-01, -1.2906e-01, -3.7235e-01,\n           2.2960e-01,  4.4903e-01, -2.7746e-01,  2.9838e-01,  2.0086e-01,\n          -1.4690e-01,  1.8579e-01,  2.9300e-01, -4.0135e-01,  5.7170e-02])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z=torch.tensor([[[-0.0536],\n         [-0.2593],\n         [-0.1455],\n         [-0.1746],\n         [-0.1541],\n         [-0.2253],\n         [-0.1649],\n         [-0.1411]],\n\n        [[-0.1079],\n         [-0.1342],\n         [ 0.0333],\n         [ 0.0532],\n         [-0.1459],\n         [-0.1847],\n         [-0.1338],\n         [-0.1503]],\n\n        [[-0.0445],\n         [-0.2704],\n         [-0.2591],\n         [-0.1669],\n         [-0.1510],\n         [-0.1350],\n         [-0.2005],\n         [-0.3086]],\n\n        [[-0.0822],\n         [-0.2452],\n         [-0.1207],\n         [-0.0947],\n         [-0.0009],\n         [-0.1569],\n         [-0.2562],\n         [-0.1742]],\n\n        [[-0.2207],\n         [-0.1804],\n         [-0.3316],\n         [-0.1162],\n         [-0.1132],\n         [ 0.0203],\n         [-0.1796],\n         [ 0.1172]],\n\n        [[-0.2966],\n         [-0.0165],\n         [-0.1236],\n         [ 0.0634],\n         [-0.0488],\n         [-0.1326],\n         [-0.2631],\n         [-0.1438]],\n\n        [[-0.1018],\n         [-0.1955],\n         [-0.2225],\n         [-0.2263],\n         [-0.0969],\n         [-0.2181],\n         [-0.3999],\n         [-0.5065]],\n\n        [[-0.0724],\n         [-0.2387],\n         [-0.1735],\n         [-0.1789],\n         [-0.1884],\n         [-0.4258],\n         [-0.5052],\n         [-0.4451]],\n\n        [[-0.0482],\n         [ 0.0920],\n         [-0.2783],\n         [-0.4123],\n         [-0.1829],\n         [-0.4774],\n         [-0.4721],\n         [-0.4481]],\n\n        [[-0.2673],\n         [-0.2185],\n         [-0.2701],\n         [ 0.0193],\n         [-0.0071],\n         [-0.4872],\n         [-0.5006],\n         [-0.4814]],\n\n        [[-0.0207],\n         [-0.1675],\n         [-0.2260],\n         [-0.2594],\n         [-0.4750],\n         [-0.5004],\n         [-0.4424],\n         [-0.4485]],\n\n        [[-0.2907],\n         [-0.0419],\n         [-0.2722],\n         [-0.4477],\n         [-0.4896],\n         [-0.4444],\n         [-0.4431],\n         [-0.4398]],\n\n        [[-0.0739],\n         [-0.1254],\n         [-0.1756],\n         [-0.4678],\n         [-0.5194],\n         [-0.4574],\n         [-0.4483],\n         [-0.4375]],\n\n        [[-0.0207],\n         [-0.2605],\n         [-0.2598],\n         [-0.3797],\n         [-0.4572],\n         [-0.4177],\n         [-0.4525],\n         [-0.4319]],\n\n        [[-0.0666],\n         [ 0.0132],\n         [ 0.1536],\n         [-0.5091],\n         [-0.4919],\n         [-0.4865],\n         [-0.4357],\n         [-0.4427]],\n\n        [[ 0.0191],\n         [ 0.2059],\n         [-0.3197],\n         [-0.4517],\n         [-0.5423],\n         [-0.4545],\n         [-0.4515],\n         [-0.4373]],\n\n        [[-0.0814],\n         [-0.0510],\n         [-0.1913],\n         [-0.4528],\n         [-0.4998],\n         [-0.4540],\n         [-0.4453],\n         [-0.4391]],\n\n        [[-0.1853],\n         [-0.0414],\n         [-0.5791],\n         [-0.4610],\n         [-0.4764],\n         [-0.4346],\n         [-0.4451],\n         [-0.4404]],\n\n        [[-0.2207],\n         [-0.4276],\n         [-0.4988],\n         [-0.4518],\n         [-0.4446],\n         [-0.4384],\n         [-0.4437],\n         [-0.4413]],\n\n        [[-0.2964],\n         [-0.3543],\n         [-0.4941],\n         [-0.4301],\n         [-0.4492],\n         [-0.4347],\n         [-0.4449],\n         [-0.4405]],\n\n        [[-0.0808],\n         [-0.3504],\n         [-0.5283],\n         [-0.4394],\n         [-0.4550],\n         [-0.4322],\n         [-0.4464],\n         [-0.4394]]])\nprint(z.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape to be batch_size first\nsft_max_out = z.view(8, -1)\n# sft_max_out = sft_max_out[:, -1] # get last batch of labels\nsft_max_out.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsft_max_out = nn.Softmax(torch.tensor([10,20,30,40]))\nsft_max_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sft_max_out = sft_max_out[:, -1] # get last batch of labels\nsft_max_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum([-0.0536, -0.2593, -0.1455, -0.1746, -0.1541, -0.2253, -0.1649, -0.1411,\n           -0.1079, -0.1342,  0.0333,  0.0532, -0.1459, -0.1847, -0.1338, -0.1503,\n           -0.0445, -0.2704, -0.2591, -0.1669, -0.1510])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum([-0.2966,\n           -0.0165,\n           -0.1236,\n            0.0634,\n           -0.0488,\n           -0.1326,\n           -0.2631,\n           -0.1438])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn.functional as F\n# Softmax is also in torch.nn.functional\ndata = torch.randn(5)\nprint(data)\nprint(F.softmax([data], dim=0))\nprint(F.softmax(data, dim=0).sum())  # Sums to 1 because it is a distribution!\nprint(F.log_softmax(data, dim=0))  # theres also log_softmax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(_input,\"\\n\\n\\n\",tgt)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}