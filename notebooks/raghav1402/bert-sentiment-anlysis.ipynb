{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from transformers import BertModel,AutoTokenizer,BertTokenizer\nimport torch\nimport pandas as pd\nimport transformers\nfrom torch.utils.data import DataLoader, Dataset\n\nimport torch.nn as nn\nfrom torchvision.transforms import ToTensor\nimport torchvision.transforms.functional as FT\nimport torch.nn.functional as F\nimport datetime\nimport time\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print('Torch version : ', torch.__version__)\nprint('transformers version : ', transformers.__version__)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrained_weights = '/kaggle/input/bert-base-uncased/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Load Tokenizer and Bert Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load pretrained model/tokenizer\ntokenizer = BertTokenizer.from_pretrained(pretrained_weights)\nbert_model = BertModel.from_pretrained(pretrained_weights,max_position_embeddings=512)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get list of sentence and do batch_encode so that all sentences will be padded to the maximum sentence length"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence_list = [\"This is a sample\", \"This is another longer sample text\"]\ntokens = tokenizer.batch_encode_plus(sentence_list,\n    pad_to_max_length=True  # Smallest sentences will be padded to match the longest sentnece\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_dict = tokenizer.encode_plus(\"This is a sample\", \"This is another longer sample text\",max_length =15, pad_to_max_length=True)\nprint(encoded_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Iterate through every sentence to get the embedding '''\nfor i in range(len(sentence_list)):\n    input_ids  = torch.tensor([(tokens['input_ids'][i])])\n    segments_tensors = torch.tensor([[i+1] * len(input_ids[0])])\n    print(\"input_ids : \",input_ids.size())    \n    with torch.no_grad():\n        '''the hidden unit / feature number (768 features) for embedding using bert-base-cased '''\n        '''Pass the input_tokens to the model to get the embeddings'''\n        #Predict hidden states features for each layer\n        ''' BERT outputs two tensors:\n            One with the generated representation for every token in the input (1, NB_TOKENS, REPRESENTATION_SIZE)\n            One with an aggregated representation for the whole input (1, REPRESENTATION_SIZE)'''\n        encoded_layers, pooled = bert_model(input_ids,segments_tensors)#[0]\n    print(\"Embedding size :\", encoded_layers.size()) \n    print(pooled.size())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Load data into dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\ndata_df['sentiment'] = data_df.apply(lambda row: 1 if row['sentiment'] =='positive' else 0, axis=1)\n'''randomly picks 5000 records from the dataframe'''\nsubset_df = data_df.sample(n=3000).reset_index(drop = True)\nsubset_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sentence_list = list(subset_df['review'][:100])\ntrain_sentiment_list = list(subset_df['sentiment'][:100])\n\nvalid_sentence_list = list(subset_df['review'][100: 150])\nvalid_sentiment_list = list(subset_df['sentiment'][100: 150])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_train = len([y for y in train_sentiment_list if y ==1])\npos_valid = len([y for y in valid_sentiment_list if y ==1])\nprint(\"Positive sentiment in train : \",pos_train)\nprint(\"Positive sentiment in valid : \",pos_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### DataLoader"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, sentence_list, score_list, max_len = 512):\n        self.sentence_list = sentence_list\n        self.sentiment = score_list\n        self.tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n        self.max_len = max_len\n        print(\"Total sentences\", len(self.sentence_list))\n                \n    def _get_tokenized_sent_list(self, batch):\n        sent_list = []\n        sentiment_list = []\n        for sent_idx, senti in batch:\n            sent_list.append(self.sentence_list[sent_idx.item()][:self.max_len])\n            sentiment_list.append(senti.item())      \n            \n        tokens_list = self.tokenizer.batch_encode_plus(sent_list, \n            pad_to_max_length=True  # Smallest sentences will be padded to match the longest sentnece\n        )        \n        return torch.tensor(tokens_list['input_ids']), torch.tensor(sentiment_list)\n    \n    def __len__(self):\n        return len(self.sentence_list)    \n    def __getitem__(self, idx):\n        sentiment_score = torch.tensor(self.sentiment[idx])\n        sent_idx = torch.tensor(idx)\n        return sent_idx,sentiment_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_set = TextDataset(train_sentence_list, train_sentiment_list)\nvalid_data_set= TextDataset(valid_sentence_list, valid_sentiment_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Model Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size=8\nembedding_size = 768\nenc_hidd_size = 512\ngru_is_bidirectional = True\nnum_layers =2\nnum_epochs = 2\ndropout = 0.3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = DataLoader(train_data_set, batch_size=batch_size, \\\n                          shuffle=True, num_workers=0, drop_last=True, collate_fn=train_data_set._get_tokenized_sent_list)\n\nvalid_loader = DataLoader(valid_data_set, batch_size=batch_size, \\\n                          shuffle=True, num_workers=0, drop_last=True, collate_fn=valid_data_set._get_tokenized_sent_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Sentiment Analysis using GRU"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SentimentAnalysisGRU(nn.Module):\n    def __init__(self, p_enc_hidd_size, p_embedding_size, num_layers,dropout, p_is_bidirectional):\n        \n        super(SentimentAnalysisGRU, self).__init__()\n        \n        \n        self.hidden_size = p_enc_hidd_size\n        self.num_layers = num_layers\n        self.is_bidirectional = p_is_bidirectional\n        ''' Embedding layer'''    \n        self.bert_embedding = BertModel.from_pretrained(pretrained_weights,max_position_embeddings=512)\n        \n        for name, param in self.bert_embedding.named_parameters():                \n            param.requires_grad = False\n        \n        ''' GRU cell'''\n        self.gru = nn.GRU(input_size = p_embedding_size, hidden_size = p_enc_hidd_size, num_layers=self.num_layers,\\\n                          bidirectional = p_is_bidirectional)\n        \n        self.droupout = nn.Dropout(dropout)\n        self.bi_dir = 1\n        if self.is_bidirectional:\n            self.bi_dir = 2\n        '''#output of linear layer is 1 value (pos/neg) '''   \n        self.fc = nn.Linear(p_enc_hidd_size * self.bi_dir, out_features=1)\n        self.sigmoid = nn.Sigmoid()   \n    \n    def forward(self, sent_batch):\n        #print(batch[0])\n        batch_size = sent_batch.size()[0]\n#         print(batch_size)\n        '''#Embedding size : (batch_size, tokens, 768)'''\n        with torch.no_grad():\n            embedding, pooled = self.bert_embedding(sent_batch)\n#         print(\"Embedding size : \", embedding.size())\n        \n        '''# h_0 shape => (num_layers * num_directions, batch, hidden_size)'''\n        h_0 = torch.zeros((self.num_layers*self.bi_dir, sent_batch.size()[1], self.hidden_size))\n        if self.is_bidirectional:\n            h_0 = torch.zeros((self.num_layers * 2, sent_batch.size()[1], self.hidden_size))\n        \n        '''gru_out : [batch_size,  tokens, gru_hidden*2 (bidirectional)]\n        h_n => last stacked hidden state\n        h_n => (2, batch_size, gru_hidden) (2 cause, bidirectional and num of layers = 1)'''\n        gru_out, h_n = self.gru(embedding, h_0)\n#         print(h_n.size())\n#         print(gru_out.size())\n        \n        '''gru_out :  (batch_size*token_len, n_hidden)'''\n        \n        gru_out = gru_out.contiguous().view(-1, self.hidden_size*self.bi_dir)\n        \n#         print(\"After view : \",gru_out.size())\n        \n        '''out :  (batch_size*seq_length, 1)'''\n        fc_out = self.fc(gru_out)\n#         print(\"Linear output size :\", out.size())\n        \n        sig_out = self.sigmoid(fc_out)\n#         print('Initial sigmoid output : ',sig_out.size())\n        sig_out = sig_out.view(batch_size, -1)\n        \n#         print(\"Sigmoid output shape after view : \", sig_out.size())\n        '''extract the output of ONLY the LAST output of the LAST element of the sequence'''\n        sig_out = sig_out[:, -1]\n#         print(\"Final : \", sig_out.size())\n        #print(sig_out)\n        return sig_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SentimentAnalysisGRU(enc_hidd_size, embedding_size, num_layers,dropout, gru_is_bidirectional)\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(train_loader, model, criterion, optimizer):\n    model.train()\n    losses = []\n    for sentence,label in train_loader:\n        output = model(sentence)\n#         print(\"output in train : \", output)\n        loss = criterion(output, label.float())\n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n        optimizer.step()\n        losses.append(loss.item())\n    return sum(losses)/len(losses)\n\ndef valid(valid_loader, model,criterion):\n    model.eval()\n    losses = []\n    for sentence, label in valid_loader:\n        with torch.no_grad():\n            output = model(sentence)\n#             print(\"output in valid : \",output)\n            loss = criterion(output, label.float())\n            losses.append(loss.item())\n    return sum(losses)/len(losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loss_list = []\nval_loss_list = []\nfor i in range(0,num_epochs):\n    strt = time.time()\n    train_loss = train(train_loader, model, criterion, optimizer)\n    train_loss_list.append(train_loss)\n#     val_loss = valid(valid_loader, model, criterion)\n#     val_loss_list.append(val_loss)\n    print(\"train loss : \", train_loss)\n#     print(\"valid loss : \", val_loss)\n    end = time.time()\n    print(\"Epoch {} Time taken {} \" .format(i+1, round((end-strt),2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(train_loss_list, '-r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"test\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}