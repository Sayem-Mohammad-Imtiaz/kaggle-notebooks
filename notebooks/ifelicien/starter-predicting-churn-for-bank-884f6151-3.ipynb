{"cells":[{"metadata":{"_uuid":"bc08832ce59a88f37245623705451fc4a99a62ad"},"cell_type":"markdown","source":"## Introduction\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing."},{"metadata":{"_uuid":"4c98c09c0e275b6d6490566563a0b81dfa4678ca"},"cell_type":"markdown","source":"## Exploratory Analysis\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using `matplotlib`. Depending on the data, not all plots will be made. (Hey, I'm just a simple kerneling bot, not a Kaggle Competitions Grandmaster!)"},{"metadata":{"_kg_hide-input":false,"_uuid":"53c1cbbb3bba065e24564e5fc5070f2d1405505a","trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split # Import train_test_split from sklearn library\n\n\nfrom sklearn import model_selection #models selection importing\nfrom sklearn import metrics #selection metrics\nfrom sklearn.metrics import confusion_matrix #confusion matrix\nfrom sklearn.linear_model import LogisticRegression #Logistic regression\nfrom sklearn.model_selection import cross_val_score #cross validarion import\nfrom sklearn.metrics import classification_report #classification report import\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import roc_auc_score #import the ROC curve \nfrom sklearn.metrics import roc_curve","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"099ca4ed9608734605a01d5f3c0a473365f028d7"},"cell_type":"markdown","source":"There is 1 csv file in the current version of the dataset:\n"},{"metadata":{"_uuid":"fd9452b776de57f85b5f2c469ae16407a5299297"},"cell_type":"markdown","source":"The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code."},{"metadata":{"_kg_hide-input":true,"_uuid":"43340868311472bb54599bf053b8d655e7ba6ff4","trusted":true},"cell_type":"code","source":"# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"50e18762b32b6c57f5536f71b4ea324d17816f3e","trusted":true},"cell_type":"code","source":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"d2d5d3c2b3b97731aad724f174738460b79c6053","trusted":true},"cell_type":"code","source":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0950662c9360f38fd4efeae8ba2a9dc0a9563b6"},"cell_type":"markdown","source":"Now you're ready to read in the data and use the plotting functions to visualize the data."},{"metadata":{"_uuid":"ae674f8cd1c640f158d79da6faf66865335ccc6d"},"cell_type":"markdown","source":"### Let's check 1st file: ../input/Churn_Modelling.csv"},{"metadata":{"_kg_hide-input":false,"_uuid":"2b40e2e2db81485b81e3c586fc64d4d9a85ded97","trusted":true},"cell_type":"code","source":"nRowsRead = 1000 # specify 'None' if want to read whole file\n# Churn_Modelling.csv has 10000 rows in reality, but we are only loading/previewing the first 1000 rows\ndf1 = pd.read_csv('../input/Churn_Modelling.csv', delimiter=',', nrows = nRowsRead)\ndf1.dataframeName = 'Churn_Modelling.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42b824154d40c179dede4b1ccbe273878a8bcbc1"},"cell_type":"markdown","source":"Let's take a quick look at what the data looks like:"},{"metadata":{"_kg_hide-input":false,"_uuid":"af8461c54d857c8e76d47a5fc2abc7a2223bd259","trusted":true},"cell_type":"code","source":"print(os.listdir('../input'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7fa892b1847602a1668bc972eeb97bdebecddc53"},"cell_type":"markdown","source":"Distribution graphs (histogram/bar graph) of sampled columns:"},{"metadata":{"_kg_hide-input":false,"_uuid":"bddf50e546d48803ef3db67cfd2df09a10519074","trusted":true},"cell_type":"code","source":"plotPerColumnDistribution(df1, 10, 5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dedd1b75edb9b11b9086c58c3861c2aa4a3ac5e4"},"cell_type":"markdown","source":"Correlation matrix:"},{"metadata":{"_kg_hide-input":false,"_uuid":"e7354c0e4a9dbc01d6978605d8fea299296fdd45","trusted":true},"cell_type":"code","source":"plotCorrelationMatrix(df1, 8)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83269dc3e04a0f69971e2cce6201853a53115368"},"cell_type":"markdown","source":"Scatter and density plots:"},{"metadata":{"_kg_hide-input":false,"_uuid":"ca76cbc53b4fe9d37c2852f1b7f184348559047d","trusted":true},"cell_type":"code","source":"plotScatterMatrix(df1, 20, 10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba99452c11ce2911173abb53628260e68f97521a"},"cell_type":"markdown","source":"## Conclusion 1`\nThis concludes your starter analysis! To go forward from here, click the blue \"Fork Notebook\" button at the top of this kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!"},{"metadata":{"_uuid":"12ae5257b0192265420351d4949156cf632955fc","trusted":true},"cell_type":"markdown","source":"# #My work start from here!"},{"metadata":{"_uuid":"311326710499a9c3a82f224e4995476bb8ebca31","trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb695ab7b0ae4084cbfd8ac4c293f7a549c689e7","trusted":true},"cell_type":"code","source":"#As we are predicting the exit status of the customer we need to see how some important features are collerated with the predictive feature\n#plt.bar(df1.Balance,df1.Exited)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"141e3222a143cd7b6ac1fe624e5f5882d7e5d6ff","trusted":true},"cell_type":"code","source":"#checking the correlation between age and balance!\nnp.corrcoef(df1.Balance,df1.Age)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fbd8ba6a1712f17bf998658eeb20927e4ce77ba"},"cell_type":"markdown","source":"Oh! in real life I was expecting these two variables to be more correlated but they are not!!\nThis gives me an idea on how the data might hold some new different amazing relationship! "},{"metadata":{"trusted":true,"_uuid":"18042af95f51e795bdcd5dea698bec0ccd44d3e5"},"cell_type":"code","source":"#copying the original dataset\ndf2=df1.copy()\ndf2['Zero_Balance']=np.where(df2['Balance']==0, 1, 0)\ngender1=np.array(df2.groupby(['Gender'])['HasCrCard','IsActiveMember','Exited','Zero_Balance'].mean().reset_index())\nGeography1=np.array(df2.groupby(['Geography'])['HasCrCard','IsActiveMember','Exited','Zero_Balance'].mean().reset_index())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28b0926583daadd7aaa4e15067c8e7b77f1261f4"},"cell_type":"code","source":"#This method plot the the statistics of given entry\ndef plotData(data,row,label,subplt,fig):\n    ax = fig.add_subplot(subplt)\n    N = 4\n    gen = data[row,1:]\n    ind = np.array(['HasCard','IsActive','Exited','Zero_Balance'])    # the x locations for the groups\n    p2 = ax.bar(ind, gen, 0.6, color=(0.2588,0.4433,1.0))\n    p1 = ax.bar(ind, 1-gen, 0.6,color=(1.0,0.5,0) ,bottom=gen)\n    plt.ylabel(\"Level\")\n    plt.title(\"%s statistics\" %label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c49ef4584fe4d3fc6bfa4af9e264bac65944998"},"cell_type":"code","source":"#plotting the gender statistics\nfig = plt.figure(figsize=(10,6))\nplotData(gender1,0,\"Female\",121,fig)\nplotData(gender1,1,\"Male\",122,fig)\n#plotting the Geography statistics\nfig2 = plt.figure(figsize=(15,6))\nplotData(Geography1,0,\"France\",131,fig2)\nplotData(Geography1,1,\"Germany\",132,fig2)\nplotData(Geography1,2,\"Spain\",133,fig2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd5e97506b7b945b65a6a0e76ae8fe28b056479d"},"cell_type":"markdown","source":"From the Above analysis we can see that the trend is likely to be the same for men and women in a way of being active or having a card or not but as we can see the Female are more likely to exit comparing to the men.\n"},{"metadata":{"_uuid":"6495ae8f8b7221e5ddea2f96328f8056572269dc"},"cell_type":"markdown","source":"From the above figures we can easily get the insight on what is in the data regarding some potential identifiers on account balances and how it goes with being active or Exited accross the countries given. This will help me understand the feature importances in selection. For example for Germany.. No customer has zero balance and yet It has a higher percentage of exited customers..  Thus is more important feature to consider when building model. "},{"metadata":{"_uuid":"96a2ba69462ada6983111e94ddb95f96723738fe"},"cell_type":"markdown","source":"# Feature extractions"},{"metadata":{"_uuid":"41dd6d1a5a307ed36685c4c4508822e1b8b80183"},"cell_type":"markdown","source":"From the dataset we have we can easily throw out the unimportant columns for our prediction..\nFollowing are column that are obvious that has no help on prediction:\n- RowNumber\n- CustomerId\n- Surname\n\nWe will simply ignore them and they might come back when we want to identify what customer we've predicted"},{"metadata":{"trusted":true,"_uuid":"5defad2c29554dfffa3040939d3a0870e3132742"},"cell_type":"code","source":"df=df1.copy()\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9dfe1dd1519e0b19d09c77875b1738742794e306"},"cell_type":"markdown","source":"#Because we will be using Gender and Geography columns which have string values we need to assign some dummie  values to each of the entry of those column.\nThere are two ways of doing this:\n1. By defaults I'm assignning like:\n- Female= 1\n- Male=0\nOr use the dummie function to get the expanded columns of each of the entries \n\nAgain for the Geography case we can use the dummie function as by using the mormal assigning for the case  we have many countries it can bias our prediction.\nThus I will use the dummie function too.\n"},{"metadata":{"trusted":true,"_uuid":"cb73c820a2f5b4bdee557cdab389694e2969850e"},"cell_type":"code","source":"gend=pd.get_dummies(df.Gender)\ngeo=pd.get_dummies(df.Geography)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d71c334f95f4564971b95b8ebb9b0ada482a35dd"},"cell_type":"code","source":"#combining the object so we can concatenate with the bigger dataframe\nobj=[df,gend,geo]\ndf=pd.concat(obj, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"350cae74b71a011575e6009db21bdc3b5a9179fb"},"cell_type":"code","source":"#creating the dataset we will be using \n#this contain only helpfull feature by ignoring the Gender and geography columns\ndf3 = df[['CreditScore','Female','Male','France','Germany','Spain','Age','Tenure','NumOfProducts','HasCrCard','IsActiveMember','Balance','EstimatedSalary','Exited']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95cdb682a03845ca30579e624e3deb944a60c854"},"cell_type":"markdown","source":"**Now on as we are going to build model we need to identify which is a dependent(Predictive) variable and Independent. For our case dependent is \"Exited \" column and  other are predictors**"},{"metadata":{"trusted":true,"_uuid":"881633d158df48829e368e57d80978e131ec1ee1"},"cell_type":"code","source":"X=df3[['CreditScore','Female','Male','France','Germany','Spain','Age','Tenure','NumOfProducts','HasCrCard','IsActiveMember','EstimatedSalary','Balance']]\nY=df3[['Exited']]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"f96973e08c34429934ef119eb791a68409a9671d"},"cell_type":"markdown","source":"# #Splitting data"},{"metadata":{"_uuid":"e5c3bb9d9f3f9538473d3523fbe81186f9862535"},"cell_type":"markdown","source":"As now we have the idea on what is in the data basically, We know that in machine learning, before running any algorithm in our dataset we need to divide our dataset into two sets one called training_set and another test_set. This splitting helps us to prevent Overfiiting or Underfitting of our machine learning model."},{"metadata":{"trusted":true,"_uuid":"ea41fdcc8a045e07fe0c697f766350b7aab8bb1b"},"cell_type":"code","source":"# random_state below is a metric that is used by the function to shuffle datas while splitting. This is chosen randomly.\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state = 2) # 0.2 test_size means 20% for testing\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61222c93db1ea0bd47ad59801059189c7608c720"},"cell_type":"markdown","source":"1. # #MODEL SELECTION\nAs we have the pre-defined label then we will be doing a **Supervised learning**"},{"metadata":{"_uuid":"5f41ce5354adec6c4646c55318d195e22a2fa55c"},"cell_type":"markdown","source":"Supervised learning is the task of inferring a function from labeled training data. By fitting to the labeled training set, we want to find the most optimal model parameters to predict unknown labels on other objects."},{"metadata":{"_uuid":"fad3de3ff02a0fcdce4d11180e30a2932dd0e983"},"cell_type":"markdown","source":"There are several techniques to use for this problem byut we keep in mind that we are predicting a binary classification problem. There are some appropriate model to use for this problem:\n- Logistic regression\n- Support vector  machine\n- Decision tree classification\n- Random forest classification\n- KNN classification.\n\nSome of them are more complex but they don't really improves the performance! I will be starting using Logistic regression"},{"metadata":{"_uuid":"a63c7190837f38ecea539c90f5a83624da3a9e12"},"cell_type":"markdown","source":"# Cross-validation"},{"metadata":{"_uuid":"081f6d163218a78f8d6e09b8fc4e52befc709cc5"},"cell_type":"markdown","source":"Cross-validation is a technique for evaluating ML models by training several ML models on subsets of the available input data and evaluating them on the complementary subset of the data. Use cross-validation to detect overfitting, ie, failing to generalize a pattern.\n\nYou can use the k-fold cross-validation method to perform cross-validation. In k-fold cross-validation, you split the input data into k subsets of data (also known as folds). You train an ML model on all but one (k-1) of the subsets, and then evaluate the model on the subset that was not used for training. This process is repeated k times, with a different subset reserved for evaluation (and excluded from training) each time.  //source: Amazon.com\n\n#This techniques prevent overfitting of the training dataset! //Following is for Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"1f296e2834f321c059e40de7f8eec2930571d63d"},"cell_type":"code","source":"kfold = model_selection.KFold(n_splits=5, random_state=2)\nmodelCV = LogisticRegression()\nscoring = 'accuracy'\nresults = model_selection.cross_val_score(modelCV, X_train, y_train.values.ravel(), cv=kfold, scoring=scoring)\nprint(\"10-fold cross validation average accuracy: %.3f\" % (results.mean()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d1629a18aa3e889994d7442b32484aef5ee65e2"},"cell_type":"markdown","source":"***Score of 78.4% is good for our dataset to build our model.*****"},{"metadata":{"_uuid":"27924bae3ee633108f5ef3cf98ecb4ff6af674b7"},"cell_type":"markdown","source":"# # 1. Logistic regression MODEL "},{"metadata":{"trusted":true,"_uuid":"eec313e39919b7e3bec2253aa3110d845e8ea6d8"},"cell_type":"code","source":"#getting the Logistic Regression model\nlogreg = LogisticRegression()\n#fitting the data\nlogreg.fit(X_train, y_train.values.ravel())\n#predicting \ny_pred = logreg.predict(X_test)\n#printing the accuracy\nprint('Accuracy of logistic regression classifier on test set:'+str(format(logreg.score(X_test, y_test))))\n#confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\nconf_matrix\nprint(conf_matrix)\nfig = plt.figure()\nax = fig.add_subplot(111)\n#printing the confusion matrix graphic\ncax = ax.matshow(conf_matrix)\nplt.title('Confusion matrix of the classifier')\nfig.colorbar(cax)\n\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a7166b88ead2ea0489e8e62a2266f7b2385ea38"},"cell_type":"markdown","source":"# #Precision & recall**"},{"metadata":{"_uuid":"7c5ef87fb23d063649ce5ac8ca2ae55522938eb3"},"cell_type":"markdown","source":"The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative.The recall is intuitively the ability of the classifier to find all the positive samples.\nThe F-beta score weights the recall more than the precision by a factor of beta. beta = 1.0 means recall and precision are equally important. The support is the number of occurrences of each class in y_test."},{"metadata":{"trusted":true,"_uuid":"d1eda39f5451473b3b5822d445cecfe9064f0846"},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c85b56045eb7ec1c6741a69129267698efa9f3ab"},"cell_type":"markdown","source":"# Feature prunning"},{"metadata":{"_uuid":"00ac5a264f8ac9bd85185ffdf85a63f5d4bdb9a4"},"cell_type":"markdown","source":"As we can see some of the features are not so more important like others. I choose to use extra tree classifier. This technique implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting."},{"metadata":{"trusted":true,"_uuid":"5dd13190015783d59e40f667331f3fc6a6ffed02"},"cell_type":"code","source":"#USING THE TREE CLASSIFIER\nclf = ExtraTreesClassifier()\n#fitting the features \nclf = clf.fit(X_train, y_train.values.ravel())\n#\nclf.feature_importances_  \nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\n#extracting the selected features\nfeature_idx=model.get_support()\nfeature_name=X.columns[feature_idx]\n#Getting the new features extracted \nX_test_new=X_test.loc[:,feature_idx]\nX_train_new=X_train.loc[:,feature_idx]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e497d05af04d6660477001e812a11a374a5c11fe"},"cell_type":"code","source":"#Those are more important features then I will reuse them again in the model\nfeature_name","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5096dcd4da7aebb807b11945c0f7c41be1c29976"},"cell_type":"markdown","source":"# Fitting the Logistic Regression again!!"},{"metadata":{"trusted":true,"_uuid":"76532a9bb5dd5e4d982afb78b3f82fadc469f0e8"},"cell_type":"code","source":"logre = LogisticRegression()\nlogre.fit(X_train_new ,y_train.values.ravel())\n\ny_pred = logre.predict(X_test_new)\nprint('Accuracy of logistic regression classifier on test set:'+str(format(logre.score(X_test_new, y_test))))\nconf_matrix = confusion_matrix(y_test, y_pred)\nconf_matrix\nprint(conf_matrix)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_matrix)\nplt.title('Confusion matrix of the classifier IMPROVED')\nfig.colorbar(cax)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\ny= (conf_matrix[0,0]+conf_matrix[1,1])/(conf_matrix[0,0]+conf_matrix[0,1]+conf_matrix[1,0]+conf_matrix[1,1])\nprint('Accurracy= '+str(y))\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25d9172226a95381d5477e29dcc0fc7257b64f8a"},"cell_type":"markdown","source":"The above changes didn't really improves our model performance comparing to when using all the 13 features!\n\nDue to the above results let me try to use The random forest classification methods to improve the performnce."},{"metadata":{"_uuid":"e7152447ae60a9ab3b34f98797d83f5d726fe815"},"cell_type":"markdown","source":"# #Support vector machine"},{"metadata":{"trusted":true,"_uuid":"d3325d830aa2def16c72367fe0072dedfe12648c"},"cell_type":"code","source":"from sklearn.svm import SVC \nmodel = SVC(probability=True)  \n# Fitting the model\nmodel = model.fit(X_train_new, y_train)  \n# Predictions/probs on the test dataset\npredicted = pd.DataFrame(model.predict(X_test_new))  \n# Store metrics\naccuracy = metrics.accuracy_score(y_test, predicted)\nprint(\"Support vector machine acuuracy is \",accuracy)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b5ded3d6cb4d9607a67c21e663ebbdcf7b3e966"},"cell_type":"markdown","source":"Wow! O.82 for this SVM is an improvment. It improved my the model by 0.4%. Which is good somehow. Let me try to use Random forest.\n**The random forest is the powerful one for binary classification predictions.**"},{"metadata":{"trusted":true,"_uuid":"99a1e46141ed3a6428beb1d27be6bee2f5728ad4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f9ab1d772190f9416a09d807221b73a94b17bc9"},"cell_type":"markdown","source":"# # 2. USING RANDOM FOREST"},{"metadata":{"trusted":true,"_uuid":"565573d60323e0275e8d920a0131fd627262248f"},"cell_type":"code","source":"# The  number of correct classifications\nclf_rf_4 = RandomForestClassifier() \nrfecv = RFECV(estimator=clf_rf_4, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\n#fitting the classifier\nrfecv = rfecv.fit(X_train_new, y_train.values.ravel())\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', X_train_new.columns[rfecv.support_])\n\n# Plot number of features VS. cross-validation scores\n\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score of number of selected features\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()\n\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf = RandomForestClassifier(random_state=43)      \nclr_rf = clf_rf.fit(X_train_new,y_train.values.ravel())\n#printing the score \nypred_new=clf_rf.predict(X_test_new)\nac = accuracy_score(y_test,ypred_new)\nprint('Accuracy is: ',ac)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89509a0c60c9b9bb2783c71c5b2d4f93f2223a3c"},"cell_type":"markdown","source":"# !!! Accuracy is:  0.86 WHICH IS AN IMPROVEMENT!!!!"},{"metadata":{"_uuid":"0c4b46203e4f327081d55dc71fe1660c93289304"},"cell_type":"markdown","source":"Considering the size of data this kind of occuracy is good "},{"metadata":{"_uuid":"4965c0d3d223502207ed739810e8dd05aa1b0947"},"cell_type":"markdown","source":"Conclusion:\nConsidering the size of dataset we got and the data structure we had I can confidently say that 86% ACCURACY is good. I am very sure that if I would be having more sufficent data and time to preprocess and to train well my model I would reach up to 98% accuracy!."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}