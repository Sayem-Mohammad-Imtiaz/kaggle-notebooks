{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting Heart Disease using Machine Learning\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Problem Definition\n\n\n> Given clinical parameters about a patient, can we predict whether or not they have heart disease?\n\n\n\nThe following is the data dictionary for the dataset.\n\n1. age - age in years \n2. sex - (1 = male; 0 = female) \n3. cp - chest pain type \n    * 0: Typical angina: chest pain related decrease blood supply to the heart\n    * 1: Atypical angina: chest pain not related to heart\n    * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n    * 3: Asymptomatic: chest pain not showing signs of disease\n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n    * anything above 130-140 is typically cause for concern\n5. chol - serum cholestoral in mg/dl \n    * serum = LDL + HDL + .2 * triglycerides\n    * above 200 is cause for concern\n6. fbs - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) \n    * '>126' mg/dL signals diabetes\n7. restecg - resting electrocardiographic results\n    * 0: Nothing to note\n    * 1: ST-T Wave abnormality\n        - can range from mild symptoms to severe problems\n        - signals non-normal heart beat\n    * 2: Possible or definite left ventricular hypertrophy\n        - Enlarged heart's main pumping chamber\n8. thalach - maximum heart rate achieved \n9. exang - exercise induced angina (1 = yes; 0 = no) \n10. oldpeak - ST depression induced by exercise relative to rest \n    * looks at stress of heart during excercise\n    * unhealthy heart will stress more\n11. slope - the slope of the peak exercise ST segment\n    * 0: Upsloping: better heart rate with excercise (uncommon)\n    * 1: Flatsloping: minimal change (typical healthy heart)\n    * 2: Downslopins: signs of unhealthy heart\n12. ca - number of major vessels (0-3) colored by flourosopy \n    * colored vessel means the doctor can see the blood passing through\n    * the more blood movement the better (no clots)\n13. thal - thalium stress result\n    * 1,3: normal\n    * 6: fixed defect: used to be defect but ok now\n    * 7: reversable defect: no proper blood movement when excercising \n14. target - have disease or not (1=no heart disease, 0=heart disease) (= the predicted attribute)\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline \n\n## Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\n\n\n## Model evaluators\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/heart-disease-uci/heart.csv\") # 'DataFrame' shortened to 'df'\ndf.shape # (rows, columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration (exploratory data analysis or EDA)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# No. of positive and negative patients in our samples\ndf.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since these two values are nearly equal, our `target` column is **balanced**. An **unbalanced** target column, having different number of counts in each label, can be harder to model than a balanced set.\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Normalized value counts\ndf.target.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plot the value counts with a bar graph\ndf.target.value_counts().plot(kind=\"bar\", color=[\"purple\", \"magenta\"]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Age Vs Sex for heart disease and other crosstabs","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.sex.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.crosstab(df.target, df.age)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 207 males and 96 females in our study.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Compare target column with sex column\npd.crosstab(df.target, df.sex)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nSince there are about 100 women and 72 of them have a postive value of heart disease being present, we might infer, based on this one variable if the participant is a woman, there's a 75% chance she does not have heart disease.\n\nAs for males, there's about 200 total with around half indicating a presence of heart disease. So we might predict, if the participant is male, 50% of the time he will have heart disease.\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"\npd.crosstab(df.target, df.sex).plot(kind=\"bar\", figsize=(10,6), color=[\"salmon\", \"lightblue\"])\n\nplt.title(\"Heart Disease Frequency for Sex\")\nplt.xlabel(\"0 = Disease, 1 = No Disease\")\nplt.ylabel(\"Amount\")\nplt.legend([\"Female\", \"Male\"])\nplt.xticks(rotation=0);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above visualization, it is clear that males are at a higher risk of having heart disease with more than 50 percent of the included patients having the disease.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Age vs Max Heart rate for Heart Disease\n\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"\nplt.figure(figsize=(10,6))\n\n# For positve examples\nplt.scatter(df.age[df.target==0], \n            df.thalach[df.target==0], \n            c=\"salmon\") # define it as a scatter figure\n\n# Now for negative examples, \nplt.scatter(df.age[df.target==1], \n            df.thalach[df.target==1], \n            c=\"lightblue\") \n\n\nplt.title(\"Heart Disease in function of Age and Max Heart Rate\")\nplt.xlabel(\"Age\")\nplt.legend([\"Disease\", \"No Disease\"])\nplt.ylabel(\"Max Heart Rate\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nFor patients without heart disease, it seems the younger someone is, the higher their max heart rate (dots are higher on the left of the graph and decreases somewhat linearly), and it decreases with age. There is no such fixed pattern in patients with heart disease.\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Histograms to check age distribution \ndf.age.plot.hist();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is not a perfect normal distribution but sways to the right.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Heart Disease Frequency per Chest Pain Type\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.crosstab(df.cp, df.target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\npd.crosstab(df.cp, df.target).plot(kind=\"bar\", \n                                   figsize=(10,6), \n                                   color=[\"lightblue\", \"salmon\"])\n\n\nplt.title(\"Heart Disease Frequency Per Chest Pain Type\")\nplt.xlabel(\"Chest Pain Type\")\nplt.ylabel(\"Frequency\")\nplt.legend([\"Disease\", \"No disease\"])\nplt.xticks(rotation = 0);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nRemember from our data dictionary what the different levels of chest pain are.\n\n3. cp - chest pain type \n    * 0: Typical angina: chest pain related decrease blood supply to the heart\n    * 1: Atypical angina: chest pain not related to heart\n    * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n    * 3: Asymptomatic: chest pain not showing signs of disease\n\nThe bargraph validates the data dictionary.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Find the correlation between our independent variables\ncorr_matrix = df.corr()\ncorr_matrix ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\ncorr_matrix = df.corr()\nfig, ax=plt.subplots(figsize=(15, 15))\nax=sns.heatmap(corr_matrix, \n            annot=True, \n            linewidths=0.5, \n            fmt= \".2f\", \n            cmap=\"YlGnBu\");\nbottom, top=ax.get_ylim()\nax.set_ylim(bottom+0.5, top-0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Maximum heart rate (thalach) and chest pain type have high positive correlation with having no heart disease. (remember 1= no heart disease). ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Modeling\n\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Everything except target variable\nX = df.drop(\"target\", axis=1)\n\n# Target variable\ny = df.target.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Independent variables (no target column)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Targets\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\nnp.random.seed(42)\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,  \n                                                    y, \n                                                    test_size = 0.2) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train, len(y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Beautiful, we can see we're using 242 samples to train on. Let's look at our test data.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_test, len(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And we've got 61 examples we'll test our model(s) on. Let's build some.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Model choices\n\nNow we've got our data prepared, we can start to fit models. We'll be using the following and comparing their results.\n\n1. Logistic Regression - [`LogisticRegression()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n2. K-Nearest Neighbors - [`KNeighboursClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n3. RandomForest - [`RandomForestClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Put models in a dictionary\nmodels = {\"KNN\": KNeighborsClassifier(),\n          \"Logistic Regression\": LogisticRegression(), \n          \"Random Forest\": RandomForestClassifier(), \"Decision Tree\":DecisionTreeClassifier()}\n\n# Create function to fit and score models\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n\n    \n    np.random.seed(42)\n    \n    model_scores = {}\n    \n    for name, model in models.items():\n        \n        model.fit(X_train, y_train)\n        \n        model_scores[name] = model.score(X_test, y_test)*100\n    return model_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model_scores = fit_and_score(models=models,\n                             X_train=X_train,\n                             X_test=X_test,\n                             y_train=y_train,\n                             y_test=y_test)\nmodel_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Comparison\n\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"model_compare = pd.DataFrame(model_scores, index=['accuracy'])\nmodel_compare.plot.bar();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since KNN, Decision Tree, AdaBoost gives relatively quite low accuracy value I decide to ignore them.\n\n### Tuning models with RandomizedSearchCV\n\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Different LogisticRegression hyperparameters\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# Different RandomForestClassifier hyperparameters\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2)}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We first use `RandomizedSearchCV` to try and tune our `LogisticRegression` model.\n\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"\nnp.random.seed(42)\n\n\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True)\n\n\nrs_log_reg.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rs_log_reg.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rs_log_reg.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'll do the same for `RandomForestClassifier`.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\n# Fit random hyperparameter search model\nrs_rf.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\nrs_rf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\nrs_rf.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because Logistic Regression is returning a better accuracy score, I use GridSearchCV\non it.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"\nlog_reg_grid = {\"penalty\" :['l2'],\n\"C\":np.logspace(-4,4,30),\n\"class_weight\":[{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}],\n\"solver\": ['liblinear', 'saga','sag','newton-cg','lbfgs'],\"max_iter\":[10] }\n\n\n\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                          param_grid=log_reg_grid,\n                          cv=5,\n                          verbose=True)\n\n\ngs_log_reg.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check the best parameters\ngs_log_reg.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Evaluate the model\ngs_log_reg.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using AdaBoost Classifier","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nadaboost=AdaBoostClassifier(base_estimator=LogisticRegression(C= 2.592943797404667,\n class_weight= {1: 0.5, 0: 0.5},\n max_iter= 10,\n penalty= 'l2',\n solver= 'liblinear'),n_estimators=100)\nadaboost.fit(X_train,y_train)\nadaboost.score(X_test,y_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluating other metrics","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\ny_preds = gs_log_reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_probs=gs_log_reg.predict_proba(X_test)\ny_probs_positive=y_probs[:,1]\ny_probs_positive","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\nfrom sklearn.metrics import auc\n\nfpr, tpr, thresholds= roc_curve(y_test, y_probs_positive)\ndef plot_roc_curve(fpr,tpr):\n plt.plot(fpr, tpr, color=\"orange\",label=\"ROC\")\n plt.plot([0,1],[0,1],color=\"darkblue\",linestyle=\"--\",label=\"Guessing\")\n plt.xlabel(\"False positive rate\")\n plt.ylabel(\"True positive rate\")\n plt.title(\"Receiver Operating Characterisitics curve\")\n plt.legend()\n plt.show()\nplot_roc_curve(fpr,tpr)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"roc_auc=auc(fpr,tpr)\nroc_auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Display confusion matrix\nprint(confusion_matrix(y_test, y_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\nimport seaborn as sns\nsns.set(font_scale=1.5) \ndef plot_conf_mat(y_test, y_preds):\n    \n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                     annot=True, # Annotate the boxes\n                     cbar=True)\n    plt.xlabel(\"true label\")\n    plt.ylabel(\"predicted label\")\n    bottom,top=ax.get_ylim()\n    ax.set_ylim(bottom+0.5, top-0.5)\n    \nplot_conf_mat(y_test, y_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"prec=precision_score(y_test,y_preds)\nprec","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rec=recall_score(y_test,y_preds)\nrec","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Show classification report\nprint(classification_report(y_test, y_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**It is important to note that 0 implies heart disease while 1 implies no heart disease. Hence from the confusion matrix, TPR is not 0.87 but 0.89. Similarly FPR is not 0.11 but 0.12. Similarly, ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"rs_rf.best_params_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nrf=RandomForestClassifier(n_estimators=560,\n min_samples_split=12,\n min_samples_leaf=15,\n max_depth=3)\ncv_acc=np.mean(cross_val_score(rf,X,y,cv=5,scoring=\"accuracy\"))\ncv_prec=np.mean(cross_val_score(rf,X,y,cv=5,scoring=\"precision\"))\ncv_recall=np.mean(cross_val_score(rf,X,y,cv=5,scoring=\"recall\"))\ncv_f1=np.mean(cross_val_score(rf,X,y,cv=5,scoring=\"f1\"))\ncv_acc,cv_prec,cv_recall,cv_f1\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n                            \"Precision\": cv_prec,\n                            \"Recall\": cv_recall,\n                            \"F1\": cv_f1},\n                          index=[0])\ncv_metrics.T.plot.bar(title=\"Random Forest Cross-Validated Metrics\", legend=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\ngs_log_reg.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\nfrom sklearn.model_selection import cross_val_score\n\nclf = LogisticRegression(C=0.23357214690901212,\n                         solver=\"liblinear\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\ncv_acc = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5, \n                         scoring=\"accuracy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cv_acc = np.mean(cv_acc)\ncv_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Cross-validated precision score\ncv_precision = np.mean(cross_val_score(clf,\n                                       X,\n                                       y,\n                                       cv=5,\n                                       scoring=\"precision\")) \ncv_precision","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\ncv_recall = np.mean(cross_val_score(clf,\n                                    X,\n                                    y,\n                                    cv=5, \n                                    scoring=\"recall\")) \ncv_recall","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\ncv_f1 = np.mean(cross_val_score(clf,\n                                X,\n                                y,\n                                cv=5, \n                                scoring=\"f1\")) \ncv_f1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Visualizing cross-validated metrics\ncv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n                            \"Precision\": cv_precision,\n                            \"Recall\": cv_recall,\n                            \"F1\": cv_f1},\n                          index=[0])\ncv_metrics.T.plot.bar(title=\"Logistic Regression Cross-Validated Metrics\", legend=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\nclf.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check feature importance\nclf.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Match features to columns\nfeatures_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeatures_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Visualize feature importance\nfeatures_df = pd.DataFrame(features_dict, index=[0])\nfeatures_df.T.plot.bar(title=\"Feature Importance\", legend=False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All our predictions is picked up by the model. As sex variable decreases towards 0 (female), target variable points towards 1 ( no disease). As max heart rate increases, probability of having heart disease decreases.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}