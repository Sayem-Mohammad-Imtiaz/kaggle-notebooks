{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport catboost\nfrom catboost import Pool\nimport xgboost\nimport optuna\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/brasilian-houses-to-rent/houses_to_rent_v2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df.columns = ['city', 'area', 'rooms', 'bathroom', 'parking spaces', 'floor',\n#       'animal', 'furniture', 'hoa', 'rent amount',\n#       'property tax', 'fire insurance', 'total']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def categorise(data,cols):\n    object_cols = []\n    num_cols = []\n    for i in cols:\n        if data[i].dtype == 'object':\n            object_cols.append(i)\n        else:\n            num_cols.append(i)\n    return object_cols, num_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"object_cols, num_cols = categorise(df,df.columns)\nobject_cols, num_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, four columns i.e. 'city', 'floor', 'animal', 'furniture' have values other than a numerical one. Now, let's first check if the data contains any NaN values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly, data not contains any NaN values. Let's check what all different values the object_cols has."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['city'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since, 'city' column contains 5 different values, so we'll use one_hot_encorder."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['floor'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'floor' contains '-', it has to be removed. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['floor'] == '-', 'floor'] = 0\ndf['floor'] = df['floor'].astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['animal'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['furniture'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"For 'animal' and 'furniture', we'll use label  encorder as they have 2 unique values only."},{"metadata":{},"cell_type":"markdown","source":"# Checking for outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data=df['rent amount (R$)'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see that there are some outliers, so we have to remove them**"},{"metadata":{},"cell_type":"markdown","source":"# Removing outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_outlier(df_in, col_name):\n    q1 = df_in[col_name].quantile(0.25)\n    q3 = df_in[col_name].quantile(0.75)\n    iqr = q3-q1 #Interquartile range\n    fence_low  = q1-1.5*iqr\n    fence_high = q3+1.5*iqr\n    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n    return df_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = remove_outlier(df1, 'rent amount (R$)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data=df['rent amount (R$)'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets take a look on how our data was distributed before and after treat outliers\n\nplt.figure(1, figsize=(20, 10))\nplt.subplot(2, 2, 1)\nsns.distplot(df['rent amount (R$)'])\nplt.title('Before Removing Outliers')\nplt.subplot(2, 2, 2)\nsns.distplot(df2['rent amount (R$)'])\nplt.title('After Removing Outliers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols1 = ['rooms', 'bathroom', 'parking spaces']\nplt.figure(figsize=(20, 5))\nsns.set(style = 'whitegrid')\ni = 1\nfor feature in num_cols1:\n    plt.subplot(2, 3, i)\n    sns.barplot(x = feature, y= 'rent amount (R$)', data=df2)\n    i+=1\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now let's see the correlation between features\nplt.figure(figsize=(12,12))\nsns.heatmap(df2.corr(), annot=True, cmap='RdBu_r', linecolor='black',vmin=-1, vmax=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lev_enc = LabelEncoder()\ndf2['animal']=lev_enc.fit_transform(df2['animal'])\ndf2['furniture']=lev_enc.fit_transform(df2['furniture'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hot_enc = pd.get_dummies(df2['city'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final = pd.concat([df2, hot_enc], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.drop(['city'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['Belo Horizonte','Campinas', 'Porto Alegre', 'Rio de Janeiro', 'SÃ£o Paulo', 'rooms', 'bathroom', 'parking spaces', 'fire insurance (R$)',\n        'furniture']\nX = df_final[columns]\ny = df_final['rent amount (R$)']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pool = Pool(\n                data = X_train,\n                label = y_train,\n)\nvalid_pool = Pool(\n                data = X_valid,\n                label = y_valid,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = catboost.CatBoostRegressor(custom_metric= ['R2', 'RMSE'], learning_rate=0.1, n_estimators=264)\nmodel.fit(train_pool, eval_set=valid_pool, verbose=50, plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we create a list to storage all the results for later visualization\nacc = []\n# parameters are the alpha's that we will use to perform the GridSearch\nparameters1= [{'alpha': [0.0001, 0.001, 0.1, 1, 10, 100, 1000, 10000, 100000, 100000]}]\n# on the regressors we define the models that we want use\nregressors = {'Linear Regression': LinearRegression(),\n              'Ridge Model': Ridge(alpha=0.1),\n              'Decision Tree': DecisionTreeRegressor(),\n              'Random Forest': RandomForestRegressor(random_state=1),\n              'SVR': SVR(),\n              'KNN': KNeighborsRegressor(),\n              'Lasso': Lasso(),\n              'GridSearchRidge': GridSearchCV(Ridge(), parameters1, cv=4),\n              'GridSearchLasso': GridSearchCV(Lasso(), parameters1, cv=4)\n             }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we perform a loop with each regressor to perform the model, predict the rent \n# and extract the metrics\nfor i in regressors:\n    model = regressors.get(i)\n    # here we create a condition because for grid we want to perform the model with the best estimator\n    if i == 'GridSearchRidge' or i == 'GridSearchLasso':\n        model.fit(X_train, y_train).best_estimator_ \n    model.fit(X_train, y_train)\n    prediction = model.predict(X_valid)\n    print(i)\n    print('MAE:', mean_absolute_error(y_valid, prediction))\n    print('RMSE:', np.sqrt(mean_squared_error(y_valid, prediction)))\n    print('R2:', r2_score(y_valid, prediction))\n    print('*' * 40)\n    acc.append([i, mean_absolute_error(y_valid, prediction), np.sqrt(mean_squared_error(y_valid, prediction)), r2_score(y_valid, prediction)])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now let's follow the same loop and visualize the plot's for each regressor\nj = 1\nplt.figure(figsize=(20,10))\nfor i in regressors:\n    model = regressors.get(i)\n    model.fit(X_train, y_train)\n    prediction = model.predict(X_valid)\n    plt.subplot(3, 3, j)\n    plt.title(i)\n    ax1 = sns.distplot(y_valid,hist=False,kde =True,color =\"r\",label =\"Actual Value\")\n    sns.distplot(prediction ,color =\"b\",hist = False,kde =True, label = \"Predicted Value\",ax =ax1).set_title(i)\n    j+=1\nplt.tight_layout(pad = 0.5)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}