{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pima Indians Diabetes Database\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. \nThe objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. \nSeveral constraints were placed on the selection of these instances from a larger database. \nIn particular, all patients here are females at least 21 years old of Pima Indian heritage."},{"metadata":{},"cell_type":"markdown","source":"# Visualization of the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Dataset : https://www.kaggle.com/uciml/pima-indians-diabetes-database\ndf = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here the information we need to take in consideration are the length of the dataset (here : 768 elements).\nThen, we will take a look on the missing information. Here, we can see that we have all the data.\nThus, we can see all the type present in our data.\nThat we help us if we do a normalization of our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Permutation importance\n\nHere, we are going to see which column has the most importance in our data.\nIn order to do that, we are going to use a random forest classifier.\nWe select one column that we are going to remove. Then, we see the accuracy of our model.\nA feature is considered \"important\" if the accuracy of our model drops.\nHowever, a feature is considered \"unimportant\" if the accuracy of our model isn't affected."},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n# We get all the column except the last one.\nX = df.iloc[:, :-1]\n# We get the last column (Outcome)\ny = df.iloc[:, -1]\n\n# Create two sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nmy_model = RandomForestClassifier(n_estimators=100, random_state=0).fit(X, y)\n\nperm = PermutationImportance(my_model, random_state=1).fit(X, y)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the \"Glucose\", \"BMI\", \"Age\" and \"DiabetesPedegreeFunciton\" are the 4 features with the most importance in our data.\n\nIn order to see the correlation between the \"Glucose\" and the \"Outcome\", we can create a graph that show us the Glucose value in function of the Outcome (Diabete or no Diabete)."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ng = sns.kdeplot(df[\"Glucose\"][(df[\"Outcome\"] == 0) & (df[\"Glucose\"].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(df[\"Glucose\"][(df[\"Outcome\"] == 1) & (df[\"Glucose\"].notnull())], color=\"Blue\", ax=g, shade = True)\ng.set_xlabel(\"Glucose\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"No Diabete\",\"Diabete\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this graph, we can see if we have a high glucose value, we have more proababilities that the person has Diabete."},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.pairplot(df, hue=\"Outcome\", palette=\"Set2\", diag_kind=\"kde\", height=3, size=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this graph, we can visualize our data by pairs and with the two different value of the Outcome.\nHere, in green, the person have Diabete. And in orange, the person have no Diabete.\nWhen we visualize our data, we can see if some combinaison can be useful in order to describe this problem."},{"metadata":{},"cell_type":"markdown","source":"# Preparation of the pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# A class to select numerical or categorical columns \n# since Scikit-Learn doesn't handle DataFrames yet\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In our pipeline, we select all the features we want to take.\n\nWe can specify an imputer that allow us to select default value for missing one.\nHowever, in our study, we have saw that all the value was there. So, we don't need to use it.\n\n\nThen, we will apply an normalization method, here the \"Standard Scaler\".\nIt standardize features by removing the mean and scaling to unit variance. \n\nThe standard score of a sample x is calculated as:\n\nz = (x - u) / s\n\nWe can also use other approch in order to normalize our data, like MinMaxScaler."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer # Scikit-Learn 0.20+\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# No need of the imputer because all the value are there :)\npreprocess_pipeline = Pipeline([\n    (\"select_numeric\", DataFrameSelector([\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Pregnancies\"])),\n#   (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"Standardization\", StandardScaler())\n])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split our data in two sets (training and test)"},{"metadata":{},"cell_type":"markdown","source":"Here we split our data in two sets. One for the training and another for the test."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create two sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nX_train = preprocess_pipeline.fit_transform(X_train)\nX_test = preprocess_pipeline.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Use some machine learning algorithm"},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machine\n\nFirst of all, we are going to run a SVM with no optimization. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvm_clf = SVC(gamma=\"auto\")\nsvm_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n# Here, we use cross validation\nsvm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10)\nsvm_scores.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Optimization of the SVM\n\nIn order to optimize our SVM, we need to define some parameters.\nHere, we will use a rbf kernel.\n\n#### Parameter : C\n\nC is the penalty parameter, which represents misclassification or error term. The misclassification or error term tells the SVM optimisation how much error is bearable. This is how you can control the trade-off between decision boundary and misclassification term.\n\nBellow, we have an example of different values with the C parameter.\nWe can see that when C is high it will classify all the data points correctly, also there is a chance to overfit.\n\n![image](https://miro.medium.com/max/800/0*08KrYhXpVQdUXWrX)\n\n\n#### Parameter : Gamma\n\nIt defines how far influences the calculation of plausible line of separation.\nWhen gamma is higher, nearby points will have high influence; low gamma means far away points also be considered to get the decision boundary.\n\n![image](https://miro.medium.com/max/1370/1*6HVomcqW7BWuZ2vvGOEptw.png)\n"},{"metadata":{},"cell_type":"markdown","source":"In order to optimize our model, we need to specify some values. The GridSearchCV class is going to test all the possibilities of parameters. This allow us to have the best parameters at the end.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparameters = { \n    'gamma': [0.001, 0.01, 0.1, 1, 10], \n    'kernel': ['rbf'], \n    'C': [0.001, 0.01, 0.1, 1, 10, 15, 20],\n}\n\nsvm_clf = GridSearchCV(SVC(), parameters, cv=10, n_jobs=-1).fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are now going to visualize the best combinaison of parameter."},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_clf.cv_results_['params'][svm_clf.best_index_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n\ny_pred = svm_clf.predict(X_test)\naccuracy_score(y_test, y_pred), f1_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nsvm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10)\nsvm_scores.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nforest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10)\nforest_scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = { \n    'n_estimators': [10,  20,  30,  40,  50,  60,  70,  80,  90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190],\n}\n\nforest_clf = GridSearchCV(RandomForestClassifier(random_state=42), parameters, cv=10, n_jobs=-1).fit(X_train, y_train)\n\nprint(forest_clf.cv_results_['params'][forest_clf.best_index_])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_clf.fit(X_train, y_train)\n\ny_pred = forest_clf.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\n\nprint('Confusion matrix\\n',cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\n\n\nclass_names = [\"No Diabete\", \"Diabet\"]\n\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp = plot_confusion_matrix(forest_clf, X_test, y_test,\n                                 display_labels=class_names,\n                                 cmap=plt.cm.Blues,\n                                 normalize=normalize)\n    disp.ax_.set_title(title)\n\n    print(title)\n    print(disp.confusion_matrix)\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayesian"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\n\ny_pred = gnb.fit(X_train, y_train).predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\n\nprint(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred))\n\nprint('Confusion matrix\\n', cm)\n\n\ngnb_scores = cross_val_score(gnb, X_train, y_train, cv=10)\nprint(gnb_scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nneigh_clf = KNeighborsClassifier(n_neighbors=5)\ny_pred = neigh_clf.fit(X_train, y_train).predict(X_test)\n\nprint(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred))\nprint('Confusion matrix\\n', cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Optimize KNN\n\nWe are going to search the best parameter for the KNN algorithm.\nHere, we are going to focus on the number of neighbors."},{"metadata":{"trusted":true},"cell_type":"code","source":"knn2 = KNeighborsClassifier()\nparam_grid = {\n    'n_neighbors': np.arange(1, 50)\n}\nknn_gscv = GridSearchCV(knn2, param_grid, cv=5).fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(knn_gscv.best_params_)\nprint(knn_gscv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we can use the best parameter for our system."},{"metadata":{"trusted":true},"cell_type":"code","source":"neigh_clf = KNeighborsClassifier(n_neighbors=21)\ny_pred = neigh_clf.fit(X_train, y_train).predict(X_test)\n\nprint(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred))\nprint('Confusion matrix\\n', cm)\n\n\nknn_scores = cross_val_score(neigh_clf, X_train, y_train, cv=10)\nprint(knn_scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classifiers Comparaison"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_roc_curve\nax = plt.gca()\nforest_clf_roc_curve = plot_roc_curve(forest_clf, X_test, y_test, ax=ax, alpha=0.8)\nsvm_clf_roc_curve = plot_roc_curve(svm_clf, X_test, y_test, ax=ax, alpha=0.8)\nbay_clf_curve = plot_roc_curve(gnb, X_test, y_test, ax=ax, alpha=0.8)\nknn_clf_curve = plot_roc_curve(knn_gscv, X_test, y_test, ax=ax, alpha=0.8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the AUC (Aire Under the Curve), we can see thath the SVM is better than the others."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}