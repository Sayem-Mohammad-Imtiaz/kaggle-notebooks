{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#importing necessary libraries\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\n\nimport re\n\nimport pickle\nfrom tqdm import tqdm\nimport os\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading dataset\ndata=pd.read_csv(\"../input/womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#what is the shape of the dta set?\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#which columns are there?\ndata.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets see the first 5 rows of each column.\n\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"!].We found some NaN values. So we will drop that in the next step."},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping nan values\ndata=data.dropna()\ndata.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have unnamed: 0 column which is unnecessary."},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping unnamed column\ndata=data.drop([\"Unnamed: 0\"],axis=1).reset_index()\ndata.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Do you want to see more information about data set?...............","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#here it is........\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't have any null values... great!!!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets describe some statistical data.......","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets check how many different values are there for clothing id\ndata[\"Clothing ID\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets get the value counts of division name\ndata[\"Division Name\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have only 3 Divisions.. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets check the value counts of department name\ndata[\"Department Name\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets check the value counts of class name\ndata[\"Class Name\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Okayy.. Now lets check our class label i.e. Recommended IND, also % of recommendations........***"},{"metadata":{"trusted":true},"cell_type":"code","source":"a=data[\"Recommended IND\"].value_counts()\nprint(a)\nprint(\"The products that are recommended are : \", (a[1]/(a[0]+a[1]))*100,\"%\")\nprint(\"the productys that are not recommended are :\", (a[0]/(a[0]+a[1]))*100,\"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***The dataset is imbalanced. 1 means recommended. 0 means not recommended.***<br>\n1]. We have ***81.8%*** of ***recommended*** data points.... and only ***18.18%*** of ***not recommended*** data points...\n    "},{"metadata":{},"cell_type":"markdown","source":"# Let,s analyse.. (UNIVARIATE ANALYSIS)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets define function for stack plot..........","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stack_plot(data,xtick,col2=\"Recommended IND\",col3=\"total\"):\n    ind=np.arange(data.shape[0])\n    \n    plt.figure(figsize=(25,10))\n    \n    p1=plt.bar(ind,data[col3].values)\n    p2=plt.bar(ind,data[col2].values)\n    \n    plt.ylabel(\"Recommendation\")\n    plt.xticks(ind,list(data[xtick].values))\n    plt.legend((p1[0],p2[0]),(\"total\",\"recommended\"))\n    plt.show()\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def univariate(data,col1,col2=\"Recommended IND\",top=False):\n    temp=pd.DataFrame(data.groupby(col1)[col2].agg(lambda x:x.eq(1).sum())).reset_index()\n    \n    recommend=temp[col2]\n    temp=pd.DataFrame(data.groupby(col1)[col2].agg([(\"Avg\",\"mean\"),(\"total\",\"count\")]).reset_index())\n    temp[col2]=recommend\n    \n    temp.sort_values(by=[\"total\"],inplace=True,ascending=False)\n    \n    if top:\n        temp=temp[0:top]\n        \n    stack_plot(temp,xtick=col1,col2=col2,col3=\"total\")\n    print(temp.head(5))\n    print(\"*\"*50)\n    print(temp.tail(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"univariate(data,\"Clothing ID\",\"Recommended IND\",False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that, clothing_id 1078 is recommended 707 times out of 871 reviews.. <br>similarly Clothing_id 862 is recommended 534 times out of 658 reviews"},{"metadata":{"trusted":true},"cell_type":"code","source":"univariate(data,\"Division Name\",\"Recommended IND\",False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that General division dresses recommended 9490 times out of 11664 of general dress reviews."},{"metadata":{"trusted":true},"cell_type":"code","source":"univariate(data,\"Department Name\",\"Recommended IND\",False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tops are recommended 7047 times out of 8713 tops reviews"},{"metadata":{"trusted":true},"cell_type":"code","source":"univariate(data,\"Class Name\",\"Recommended IND\",False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dresses recommended 4314 times out of 5371 dresses reviews"},{"metadata":{"trusted":true},"cell_type":"code","source":"univariate(data,\"Rating\",\"Recommended IND\",False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that the clothings which are rated 5 are highly recommended.. 10837 clothings which rated 5 are recommended out of 10858..<br>\nratings with 1 are not much recommended. only 7 clothings which are rated 1 are recommended out of 691..<br>***So we can say the higher the rating higher is the recommendation....***"},{"metadata":{"trusted":true},"cell_type":"code","source":"univariate(data,\"Positive Feedback Count\",\"Recommended IND\",False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"univariate(data,\"Age\",\"Recommended IND\",False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this we could say that reviewers of age 39 recommended 962 clothings out of 1103. and they are the people who shopped most."},{"metadata":{"trusted":true},"cell_type":"code","source":"#How to calculate number of words in a string in DataFrame: https://stackoverflow.com/a/37483537/4084039\nword_count = data['Title'].str.split().apply(len).value_counts()\nword_dict = dict(word_count)\nword_dict = dict(sorted(word_dict.items(), key=lambda kv: kv[1]))\n\n\nind = np.arange(len(word_dict))\nplt.figure(figsize=(20,5))\np1 = plt.bar(ind, list(word_dict.values()))\n\nplt.ylabel('total')\nplt.title('Words for each title')\nplt.xticks(ind, list(word_dict.keys()))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the title contain only two words.. second highest is 3 words.."},{"metadata":{},"cell_type":"markdown","source":"# Cleaning the data......"},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert all letters to lower case, replace the space by _ \ndata[\"Class Name\"]=data[\"Class Name\"].str.lower()\ndata[\"Class Name\"]=data[\"Class Name\"].str.replace(\" \",\"_\")\n\n#Want to see the va;ue_counts??\ndata[\"Class Name\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Department Name\"]=data[\"Department Name\"].str.lower()\ndata[\"Department Name\"]=data[\"Department Name\"].str.replace(\" \",\"_\")\ndata[\"Department Name\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Division Name\"]=data[\"Division Name\"].str.lower()\ndata[\"Division Name\"]=data[\"Division Name\"].str.replace(\" \",\"_\")\ndata[\"Division Name\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let us define a function to replace shorthand notations with full....","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/a/47091490/4084039\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://gist.github.com/sebleier/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combining all the above......\nfrom tqdm import tqdm\ndef preprocess_text(text_data):\n    preprocessed_text = []\n    # tqdm is for printing the status bar\n    for sentance in tqdm(text_data):\n        sent = decontracted(sentance)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        # https://gist.github.com/sebleier/554280\n        sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n        preprocessed_text.append(sent.lower().strip())\n    return preprocessed_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessed_title=preprocess_text(data[\"Title\"].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets replace the title column in the dataframe with preprocessed title.\ndata[\"Title\"]=preprocessed_title","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets see how the review text looks like before preprocessing..(here we can see the 2nd review)\ndata[\"Review Text\"][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessed_review=preprocess_text(data[\"Review Text\"].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessed_review[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets replace the review column in the dataframe with preprocessed review.\ndata[\"Review Text\"]=preprocessed_review","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets take out our class label (recommended ind) and keep it in variable y..\n# because this is the class label that we have to predict given all other features.\n#and all other features are kept in x variable..\ny=data[\"Recommended IND\"]\nx=data.drop([\"Recommended IND\"],axis=1)\nx.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NOW, split data into train,cv,test data.......\nfrom sklearn.model_selection import train_test_split\nX_train,x_test,Y_train,y_test=train_test_split(x,y,test_size=0.33,stratify=y, random_state =41)\nx_train,x_cv,y_train,y_cv=train_test_split(X_train,Y_train,test_size=0.33,stratify=Y_train, random_state =41)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PRE_PROCESSING......."},{"metadata":{},"cell_type":"markdown","source":"# Bag Of Words(BOW)"},{"metadata":{"trusted":true},"cell_type":"code","source":"vec=CountVectorizer()\nvec.fit(x_train[\"Title\"].values)\nprint(x_train.shape,y_train.shape)\n\nx_train_title=vec.transform(x_train[\"Title\"].values)\nx_cv_title=vec.transform(x_cv[\"Title\"].values)\nx_test_title=vec.transform(x_test[\"Title\"].values)\n\nprint(\"after vectorization....\")\nprint(x_train_title.shape,y_train.shape)\nprint(x_cv_title.shape,y_cv.shape)\nprint(x_test_title.shape,y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vec=CountVectorizer()\nvec.fit(x_train[\"Review Text\"].values)\n\nx_train_rev=vec.transform(x_train[\"Review Text\"].values)\nx_cv_rev=vec.transform(x_cv[\"Review Text\"].values)\nx_test_rev=vec.transform(x_test[\"Review Text\"].values)\n\nprint(\"after vectorization....\")\nprint(x_train_rev.shape,y_train.shape)\nprint(x_cv_rev.shape,y_cv.shape)\nprint(x_test_rev.shape,y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vec=CountVectorizer()\nvec.fit(x_train[\"Division Name\"].values)\n\nx_train_div=vec.transform(x_train[\"Division Name\"].values)\nx_cv_div=vec.transform(x_cv[\"Division Name\"].values)\nx_test_div=vec.transform(x_test[\"Division Name\"].values)\n\nprint(\"after vectorization....\")\nprint(x_train_div.shape,y_train.shape)\nprint(x_cv_div.shape,y_cv.shape)\nprint(x_test_div.shape,y_test.shape)\n\nprint(vec.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vec=CountVectorizer()\nvec.fit(x_train[\"Department Name\"].values)\n\nx_train_dep=vec.transform(x_train[\"Department Name\"].values)\nx_cv_dep=vec.transform(x_cv[\"Department Name\"].values)\nx_test_dep=vec.transform(x_test[\"Department Name\"].values)\n\nprint(\"after vectorization....\")\nprint(x_train_dep.shape,y_train.shape)\nprint(x_cv_dep.shape,y_cv.shape)\nprint(x_test_dep.shape,y_test.shape)\n\nprint(vec.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vec=CountVectorizer()\nvec.fit(x_train[\"Class Name\"].values)\n\nx_train_cls=vec.transform(x_train[\"Class Name\"].values)\nx_cv_cls=vec.transform(x_cv[\"Class Name\"].values)\nx_test_cls=vec.transform(x_test[\"Class Name\"].values)\n\nprint(\"after vectorization....\")\nprint(x_train_cls.shape,y_train.shape)\nprint(x_cv_cls.shape,y_cv.shape)\nprint(x_test_cls.shape,y_test.shape)\n\nprint(vec.get_feature_names())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We keep our rating data which is categorical data in numerical form as it is.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_rate=(np.array(x_train[\"Rating\"])).reshape((-1,1))\nx_cv_rate=(np.array(x_cv[\"Rating\"])).reshape(-1,1)\nx_test_rate=(np.array(x_test[\"Rating\"])).reshape((-1,1))\n\n\nprint(\"after vectorization.......\")\nprint(x_train_rate.shape,y_train.shape)\nprint(x_cv_rate.shape,y_cv.shape)\nprint(x_test_rate.shape,y_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalize age column.\nfrom sklearn.preprocessing import Normalizer\nnorm=Normalizer()\n\nnorm.fit(x_train[\"Age\"].values.reshape(1,-1))\n\nx_train_age=norm.transform(x_train[\"Age\"].values.reshape(1,-1))\nx_cv_age=norm.transform(x_cv[\"Age\"].values.reshape(1,-1))\nx_test_age=norm.transform(x_test[\"Age\"].values.reshape(1,-1))\n\nx_train_age=x_train_age.reshape(-1,1)\nx_cv_age=x_cv_age.reshape(-1,1)\nx_test_age=x_test_age.reshape(-1,1)\n\n\nprint(\"after vectorization.......\")\nprint(x_train_age.shape,y_train.shape)\nprint(x_cv_age.shape,y_cv.shape)\nprint(x_test_age.shape,y_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Combining all the preprocessed data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import hstack\n\nx_tr=hstack((x_train_rev,x_train_title,x_train_div,x_train_dep,x_train_cls,x_train_rate,x_train_age)).tocsr()\nx_cv=hstack((x_cv_rev,x_cv_title,x_cv_div,x_cv_dep,x_cv_cls,x_cv_rate,x_cv_age)).tocsr()\nx_te=hstack((x_test_rev,x_test_title,x_test_div,x_test_dep,x_test_cls,x_test_rate,x_test_age)).tocsr()\n\nprint(\"FINAL DATA MATRIX SHAPE IS ........\")\nprint(x_tr.shape,y_train.shape)\nprint(x_cv.shape,y_cv.shape)\nprint(x_te.shape,y_test.shape)\nprint(\"*\"*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This function takes classifier and data , and then gives probability_score of class label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def batch_predict(clf, data):\n    \"\"\"This function takes classifier and data , and then gives probability_score of class label\"\"\"\n    y_data_pred = []                                                 #list to store probability of class\n    tr_loop = data.shape[0] - data.shape[0]%1000                     #to loop through batchwise and took 1000 for batch\n    \n    for i in range(0, tr_loop, 1000):\n        y_data_pred.extend(clf.predict_proba(data[i:i+1000])[:,1])     # this loop for upto last 1000 multiplier\n    \n    if data.shape[0]%1000 !=0:                                        #predicting for last data\n        y_data_pred.extend(clf.predict_proba(data[tr_loop:])[:,1])\n    \n    return y_data_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import roc_auc_score\n\ntrain_auc = []    \ncv_auc = []\nalpha = [0.00001,0.0001,0.001,0.01,0.1,1,10,50,100]         #to find the best alpha value\nfor i in tqdm(alpha):\n    neigh = MultinomialNB(alpha=i,class_prior=[0.5,0.5],fit_prior=False)\n    neigh.fit(x_tr, y_train)\n\n    y_train_pred = batch_predict(neigh, x_tr)               #finding probability scores of x_tr using batch_predict function\n    y_cv_pred = batch_predict(neigh, x_cv)                  ##finding probability scores of x_cv using batch_predict function\n\n          \n    train_auc.append(roc_auc_score(y_train,y_train_pred))   #appending auc values\n    cv_auc.append(roc_auc_score(y_cv, y_cv_pred))\n\n\n    \nplt.plot(alpha, train_auc, label='Train AUC')        #plotting hyper_parameter v/s auc value plot....\nplt.plot(alpha, cv_auc, label='CV AUC')\nplt.xscale(\"log\")\nplt.scatter(alpha , train_auc, label='Train AUC points')\nplt.scatter(alpha , cv_auc, label='CV AUC points')\n\nplt.legend()\nplt.xlabel(\"log(alpha): hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"hyperparameter v/s AUC plot to find BEST ALPHA\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#best alpha = 10^0 = 1\nbest_alpha_bow=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\n\n\nclas = MultinomialNB(alpha=best_alpha_bow,class_prior=[0.5,0.5],fit_prior=False)\nclas.fit(x_tr, y_train)\n\n\ny_train_pred = batch_predict(clas, x_tr)    \ny_test_pred = batch_predict(clas, x_te)\n\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\n\ntrain_auc=auc(train_fpr, train_tpr)\ntest_auc=auc(test_fpr, test_tpr)\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(train_auc))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(test_auc))\nplt.legend()\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"FPR v/s TPR plot\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_best_threshold(threshold, fpr, tpr):\n    \"\"\"it will give best threshold value that will give the least fpr\"\"\"\n    t = threshold[np.argmax(tpr*(1-fpr))]\n    \n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    \n    return t\n\ndef predict_with_best_t(proba, threshold):\n    \"\"\"this will give predictions based on best threshold value\"\"\"\n    predictions = []\n    for i in proba:\n        if i>=threshold:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#computing confusion matrix for set_1\nfrom sklearn.metrics import confusion_matrix\nbest_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\nprint(\"Train confusion matrix\")\nm_tr=(confusion_matrix(y_train, predict_with_best_t(y_train_pred, best_t)))\nprint(m_tr)\nprint(\"Test confusion matrix\")\nm_te=(confusion_matrix(y_test, predict_with_best_t(y_test_pred, best_t)))\nprint(m_te)\n#you are concerned that this cell produced fifferent result?\n#yes sir\n#no. if i run this code from the begining then it will be different result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the confusion matrix , we can observe that, TPR, TNR are high. FPR,FNR are significantly lower.<br>***So We got a good result***"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_tr = np.array(m_tr)\ncm_te=np.array(m_te)\n#plotting heatmap\nsns.heatmap(cm_tr, annot=True,fmt=\"d\",cmap='Blues')\nplt.xlabel(\"predict\")\nplt.ylabel(\"actual\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(cm_te,annot=True,fmt=\"d\",cmap=\"BrBG\")\nplt.xlabel(\"predict\")\nplt.ylabel(\"actual\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the result of confusion matrix through ***HEAT MAP*** also."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets calculate precision,recall, and f1 score\n\nprecision= m_te[1][1]/(m_te[1][0]+m_te[1][1])\nprint(precision)\n\nrecall=m_te[1][1]/(m_te[0][1]+m_te[1][1])\nprint(recall)\n\nf1=(2*precision*recall)/(precision+recall)\nprint(f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index=[1]\nsummary_df=pd.DataFrame({\"Vectorizer\":(\"BOW\"),\"Hyper-parameter(alpha)\":(best_alpha_bow),\"Train_AUC\":(train_auc),\"Test_AUC\":(test_auc), \"Precision\":(precision),\"Recall\":(recall),\"F_1_Score\":(f1)},index=index)\nsummary_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1]. When we use BOW vectorizer , we got 0.95 as test AUC.<br>     This means there is 95% chance that our model will be able to classify positive and negative points i.e. ***recommended*** or ***not recommended***<br>2]. We got TPR and TNR high, and FPR,FNR lower, this shows our model predicts pretty well.<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}