{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#burda excel ile yapılınca csv dosyası noktalı virgül ile ayırıyo bu da kaggle da sıkıntı yaratıyo. Onu virgülle ayırmalıyız\ndf = pd.read_csv(\"../input/regressiondata/linear-regression.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4b937e19618f28558cb9f4cc49a8cb5fb3ea6c0"},"cell_type":"code","source":"df\n#görüldüğü gibi noktalı virgül sıkıntı yaratıyo","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b751fc4a3e37a8b992dcdeee81cf81c4fedd25c"},"cell_type":"code","source":"#bunu sep metodu ile engelleyebiliriz.\ndf = pd.read_csv(\"../input/regressiondata/linear-regression.csv\",sep=\";\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa2d1f21aa75b36880f0709daa5783fe12a6143d"},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7ea65a5b8096316e6855e9a769efefb6d7c753d"},"cell_type":"markdown","source":"**Linear Regression**"},{"metadata":{"trusted":true,"_uuid":"df02e3138df39a08cf4550f20f3c563363354b29"},"cell_type":"code","source":"plt.scatter(df.deneyim,df.maas)\nplt.xlabel(\"deneyim\")\nplt.ylabel(\"maas\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1ebcb3979321321f0489a2a70fcb2b91757039b"},"cell_type":"code","source":"#residual(hata) = y - y_head(prediction)  ---> çift taraflı kareler alınması lazım negatiflikten kurtarmak için\n#sum(residual^2) bunun en min değeri the best fitting line\n#MSE (mean squared error) = sum(residual^2) / n(sample)\n#the goal---> min(MSE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0656cd8928b73b4641f1822483b2789fc72245f1"},"cell_type":"code","source":"#bu machine learning algoritmaları için kullanıcalak kütüphane sklearn. Bunun içinde birsürü machine learn algoritması var.\nfrom sklearn.linear_model import LinearRegression  #bu kütüphaneden linear regresyon algoritmasını import ediyoruz.\nlinear_reg = LinearRegression() #böyle bir model kuruyoruz.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b06b7907f167ab4f8e4d50c5f85d40ba9042a7d"},"cell_type":"code","source":"#daha sonra datasetimizden verieleri almalıyız ama pandas olarak almamız sıkıntı yaratabilir onun için numpy olarak almalıyız yani bir array olarak almalıyız\nx = df.deneyim\ntype(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"acbdf17f36a315168f621893c3b0d2484ca02a44"},"cell_type":"code","source":"#görüldüğü gibi type pandas seri olarak gözüküyor ama bunu array olarak almamız gerekiyor.\nx = df.deneyim.values\nx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"312155de60dedf24e91d0c4e4a48b4ffabaecba3"},"cell_type":"code","source":"x.shape  #burda shape 14, gözüküyor. bunu sklearn kütüphanesi kabul etmiyor bunun için reshapelememiz gerekmektedir.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66336bb56f790420cb52f0756ab3ed3bfe613f54"},"cell_type":"code","source":"x = df.deneyim.values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e445dfe408b8c256ac080b952bec5d45a732f97f"},"cell_type":"code","source":"x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d57f18af7681eafc55f8a7abee587b2ae982187a"},"cell_type":"code","source":"#aynı işlemi y için de yapmamız gerekmektedir.\ny = df.maas.values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3e632103f1f06e96f4fa795fdcb7b61f73c21aa"},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82cee18d0fbd939fc4cf67e4515c7b09dc31ff18"},"cell_type":"code","source":"#şimdi fit dogrusunu buldurabiliriz. bunun da kurdugumuz model ve fit() metodu ile gerçekleştirmeliyiz.\nlinear_reg.fit(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b67f44781ba4ca1e603e9f16038579544ea9f300"},"cell_type":"code","source":"#fit ettiğimiz tahmini regresyon dogrumuzun b0(intercept) ve b1(coefficent) değerlerini görebiliriz.\n#b0 = linear_reg.predict(0)   # xin 0 oldugu nokta b0 dır.\nb0 = linear_reg.intercept_  #b0 intercept ile bulunabilir.\nb0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f261769d7a7f5ab455b3b8c47188acb441a22184"},"cell_type":"code","source":"b1 = linear_reg.coef_  #eğim slope\nb1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56cf75d6fd696d43ee43eb8d4a1e6ce9f41883b7"},"cell_type":"code","source":"#regression dogrum --->  maas = 1664 + 1138*deneyim\nprint(linear_reg.predict(11))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9ed2e957cd3c52896fe86e65c191a6bd7e91daf"},"cell_type":"code","source":"#bu hatada sunu diyor tek boyutlu birsey yazıyosun diyo yani 11. ama 2 boyutlu bişi yazman gerekiyor bunu da köşeli parantezle iki boyutlu yapmalısın bir array oldugu için!!\nmaas = linear_reg.predict([[11]])\nmaas","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b27c2332c66641c1cfb0e263de3ec3ea6758560"},"cell_type":"code","source":"#bu problemin diğer çözümü ise sudur\nb0 = 12\nb0 = np.array(b0).reshape(-1,1)\nb0 = linear_reg.predict(b0)\nb0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed91dc1606ca99ee868206534d09a2a090de3d3d"},"cell_type":"code","source":"#şimdi fitting line ı mızı grafikte görelim. onun için bir numpy ile array olusturcam liste olustursam galiba for döngüsü kullanmam gerekecek onun için array.\narray = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]).reshape(-1,1) #bunlar benim x değerlerim deneyim yılları yani hata almayalım diye reshapeliyoruz!\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5794229feb7c26c0104e52f4f7ef6a3728c31c2"},"cell_type":"code","source":"#şimdi tahmini dogru için değerleri bulcam\ny_head = linear_reg.predict(array)   #burdada yine aynı hatayı almamak için 2 boytlu yapmak gerek.\nplt.scatter(array,y_head)  #bu scatter şeklinde noktaları gösterir.\nplt.plot(y_head)  #bu ise noktaları çizgi şeklinde gösteririr!\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f9eae073445130aa6c2e7d6609a6aa510616596"},"cell_type":"code","source":"plt.scatter(x,y)\nplt.plot(array,y_head,color=\"red\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"852341f86978fa24b1a2cc4b5f88aeef1647fc54"},"cell_type":"markdown","source":"**Multiple Regression**"},{"metadata":{"trusted":true,"_uuid":"a7018adf9c3148c30b1871cea30ee1893b57e53c"},"cell_type":"code","source":"dataset = pd.read_csv(\"../input/multipleregression/multiple-linear-regression-dataset.csv\",sep=\";\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2fa5d7543d81d8ea612a2eeb944fe760fb42eaf"},"cell_type":"code","source":"dataset\n#önemli olan maas=dependent variable , yas ve deneyim = independent variable olması lazım! Dikkat et independentlar arasında korelasyon olmasın!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61ee83023af18dd6d98e46b532cadc909c441072"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression  #burda yapcagımız işlemler aynı sadece feature sayısı artıyo","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05b5e7139ecc3d86190fd4976843b34c35a3446e"},"cell_type":"code","source":"#x değişkenimiz yas ve deneyim olacak\nx = dataset.iloc[:,[0,2]].values   #burda diyo ki tüm satırları al ve sadece 0. ve 2. sütunu al! ve tabi onların değerlerini. görüldüğü gibi reshape etmeye gerek kalmadı çünkü 1 için yazmıyo sadece\ny = dataset.maas.values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3af7827ac4e99ed2e14c911d0306674216923cac"},"cell_type":"code","source":"multiple_reg = LinearRegression()   #bu metodu objeye ceviriyoruz\nmultiple_reg.fit(x,y)               #x ve y ye göre bir fit dogrusu olusturuyoruz.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62fcd099dc42d3ceff295550f67edc54d30791d8"},"cell_type":"code","source":"b0 = multiple_reg.intercept_\nprint(b0)\nb0 = multiple_reg.predict([[0,0]])  #burda [[0]] diyemeyiz çünkü multiple reg yaptıgımızdan dolayı b1 ve b2 var onların ikisininde 0 olması gerek\nprint(b0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b09ebbfd7eba6896b18347fcf7d7db80b033a36"},"cell_type":"code","source":"print(\"b1,b2 =\",multiple_reg.coef_)\n#b2 yani yaş katsayısı - çıktı yaş arttıkca maaş azalır sonucunu verir.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"566ea8575a79e26cc848310f0907df58601bc2e8"},"cell_type":"code","source":"array = np.array([[10,35],[6,24]])\ny_head = multiple_reg.predict(array)\ny_head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3209936f23f7c69c1e2d417bac9823ce04be4312"},"cell_type":"code","source":"plt.scatter(x,y)   #aynı boyutta olma hatası verdi!\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8047c417a50454fe661c640a06efba4c549f9521"},"cell_type":"markdown","source":"**POLYNOMIAL REGRESSION**"},{"metadata":{"trusted":true,"_uuid":"4bbf0cddde8215149d6b5d0e06f2093fd13c9a47"},"cell_type":"code","source":"df = pd.read_csv(\"../input/polinomial/polynomial-regression.csv\",sep=\";\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2480672cbb6232f8a04a8ff5ca8ec11295a0f09"},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d8f789aef440197df285ff2247086bd9c0b62bd"},"cell_type":"code","source":"plt.scatter(df.araba_fiyat,df.araba_max_hiz)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"402f299187edd02e774dafb56680f4c623e36320"},"cell_type":"code","source":"#bu datasetini linear regression modeline göre yaparsak \nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\n\nx = df.araba_fiyat.values.reshape(-1,1)\ny = df.araba_max_hiz.values.reshape(-1,1)\n\nlr.fit(x,y)   #best fitting\n\ny_head = lr.predict(x)\n\nplt.scatter(x,y)\nplt.plot(x,y_head,color=\"red\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"827454c49f8667e600a5c79efd017f1d8c7f8df1"},"cell_type":"code","source":"#yukarıdaki grafiktede görüldüğü gibi olusan predict fitting regression dogrusu çok iyi değil mesela örnek verecek olursak\nlr.predict([[10000]])   #arabanın fiyatını 10milyon yaparsak hızını 870km gösteriyo bu da mümkün değildir.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2338d5ac101aa851f8e34ec9e39162137198475"},"cell_type":"markdown","source":"#bundan dolayı bu dataseti linear bir data seti değildir. Yani fiyat arttıgı sürece hız surekli artamaz bi sınırı vardır.\n#ondan dolayı polynomial bir data setidir.\n#linear regression = y = b0 + b1*x1\n#multiple linear regression = y = b0 + b1*x1 + b2*x2 + ... + bn*xn\n#polynomial regression = y = b0 + b1*x + b2* x^2 + bn * x^n ---> yani parabolic denklemlere sahiptir regression dogrusu xkareli xküplü falan"},{"metadata":{"trusted":true,"_uuid":"4a254fd6a54741748c9a0ab34ae2195d4ce315bc"},"cell_type":"code","source":"#arabanın fiyatı arttıkca hızı bi yerde sabit kalıyo daha yükselmiyo ondan parabolic bir denklem ortaya cıkıyor\n#burda data setimnizde bizim xkareli bir featuremız yok bundan dolayı bizim bir xkareli feature olusturmamız gerek. İlk dereceyi 2 alacaz bu dataseti için\n#polynomial bir feature ı sklearn kütüphanesinden olusturabiliyiyoruz!!\nfrom sklearn.preprocessing import PolynomialFeatures\npolynomial_reg = PolynomialFeatures(degree=2) #metodunu tanımlıyoruz ve kacıncı dereceye kadar burda vermemiz gerekiyo\nx_polynomial = polynomial_reg.fit_transform(x)  #burda araba fiyatımız olan feature ın karesi feature nı fit_transform metodu ile yapıyoruz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd5e6d52a17c8263332659e08dab35a30f9a4a39"},"cell_type":"code","source":"#artık xkareli feature mızı olusturduk geri kala işlemler linear regressiondaki işlemlerin aynınısıdır. Önemli olan kac dereceli polinom yarattıgımız \nlr2 = LinearRegression()\nlr2.fit(x_polynomial,y)   #artık x_polynomial feature ını y ye göre fit edebiliriz. burdaki denklem su oluyo=y=b0+b1*x+b2*x^2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8be6aa0272f0c7fdf8b2879ea00245f6b142c1f0"},"cell_type":"code","source":"#şimdi grafik üzerinde çizdirebiliriz\ny_head2 = lr2.predict(x_polynomial)\n\nplt.scatter(x,y)\nplt.plot(x,y_head,color = \"green\",label= \"linear\")  #burda ilk linear egression ile yaptıgımız tahmini dogru\nplt.plot(x,y_head2,color=\"red\",label=\"polynomial\")       #daha sonra polynomial reg ile yapılan\nplt.legend()  #label kısmı lejant için yazılır\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"578cd4b8707a120a0ab8e6345e69bd3a780a4096"},"cell_type":"code","source":"#peki ben bu modeli daha da iyileştirebilirmiyim?\n#Tabiki.Sadece Polinom derecesini artırarak modeli iyileştirebilirim. Yani dereceyi ben 4 yaparsam x^4 e kadar x değerleri olusur\n#bunun sonucunda model daha da iyileşir ! Hemen yapalım\nfrom sklearn.preprocessing import PolynomialFeatures\npolynomial_reg = PolynomialFeatures(degree=4) #metodunu tanımlıyoruz ve kacıncı dereceye kadar burda vermemiz gerekiyo. İYileşme için 4.dereceye kadar x olusur\nx_polynomial = polynomial_reg.fit_transform(x)  #burda araba fiyatımız olan feature(x) ın 4.dereceye kadar feature değerleri olusur. burda fit_transform metodu\n\n#artık x^4 feature mızı olusturduk geri kala işlemler linear regressiondaki işlemlerin aynınısıdır. Önemli olan kac dereceli polinom yarattıgımız \nlr2 = LinearRegression()\nlr2.fit(x_polynomial,y)   #artık x_polynomial feature ını y ye göre fit edebiliriz. burdaki denklem su oluyo=y=b0+b1*x+b2*x^2\n\ny_head3 = lr2.predict(x_polynomial2) #4.dereceye kadar olan x değerlerinin tahmini hız değerleri\nplt.scatter(x,y)\nplt.plot(x,y_head,color = \"green\",label= \"linear\")  #burda ilk linear egression ile yaptıgımız tahmini dogru\nplt.plot(x,y_head2,color=\"red\",label=\"polynomial2\")       #daha sonra polynomial reg ile yapılan 2.dereceli\nplt.plot(x,y_head3,color=\"black\",label=\"polynomial4\")       #daha sonra polynomial reg ile yapılan 4.dereceli\nplt.legend()  #label kısmı lejant için yazılır\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"370059526cab7771eb313bdcd960ab35cb0272f8"},"cell_type":"code","source":"#burda belli bir fiyatın tahmini hız değerini bulabilmek için polinomda şöyle yapıyoruz. biz fit dogrusunu 4.dereceye göre fit ettiysek\n#o zmaan x feature değerleri sırasıyla [x^0,x^1,x^2,x^3,x^4] dizisi olmus oluyo.\nlr2.predict([[1,1000,1000000,1000000000,1000000000000]]) #burda 1milyon tl olan aracın tahmini hızı ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfe5af177d65c944e2b6c13e6a4d5fb9f63da5c7"},"cell_type":"markdown","source":"#Yukarıdaki grafikte gözüktüğü gibi derece ile oynayarak regresyon dogrumuzu olabildiğince iyileştirebiliyoruz! Ama burda ben dereceyi 3,5,6 yaptım hata verdi Büyük ihtimalle dogru fit olmadıgından oldu!\n#Yani en az MSEyi 4.derecede buluyor ondan kaynaklı\n#bu soruya hocanın cevabı==>En uygun degreeyi bulmak için denemek lazım aynı zamandan probleme bakarak önceki tecrübelerine göre yada educated guess yani aldığın eğitim doğrultusunda tahminler yapılabilir. Ama bu konunun derinlerinde aslında over train ve under fit diye iki tane trade off var. Bunları göz önüne alarak seçim yapılır. Bu konuları da anlatacağım ilerleyen kurslarımızda."},{"metadata":{"trusted":true,"_uuid":"a9dcb2b2dccf2034fa76cc4b6a75d31af9370fb1"},"cell_type":"markdown","source":"**Decision Tree**\n#CART : Classification and Regression Tree\n#Burda datamızı belirli bir kurala göre split ederiz(ayırırız.) Mesela x1 ve x2 ye bağlı bir y değişkeni va diyoruz ki;\n#x1<10 için iki secenek var yes or no. sonra bu kosulun altında x2 göre de split ler yapılır ve dataseti  'terminal leaf' lere ayrılır.!\n#Bu splitler information entropy 'ye göre ayrılır. Bu matematiksel bir ifadedir bilmene gerek yok python hallediyo onu!"},{"metadata":{"trusted":true,"_uuid":"4e0aec35913bca0c271c8e8cb3a3cd4e93ceef92"},"cell_type":"code","source":"#dataseti tribündeki kategorilerin fiyatları\ndf = pd.read_csv(\"../input/decison-tree/decision-tree-regression-dataset.csv\",sep=\";\")\ndf\n#burdaki sütun isimlerini none yapabilirz yan index yapabilirz.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d23a7f8ae8041de4b0a671e2eaa679603ae24f83"},"cell_type":"code","source":"df = pd.read_csv(\"../input/decison-tree/decision-tree-regression-dataset.csv\",sep=\";\",header=None) #header ile!!!\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b0e4db18059759efc0eda0cc33c4642de5d01ae"},"cell_type":"code","source":"x = df.iloc[:,0].values.reshape(-1,1)\ny = df.iloc[:,1].values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2af12ce6fbf04db05d612076e780de505020a638"},"cell_type":"code","source":"plt.scatter(x,y)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f58ee9937191e5fbaafb9109ec620ae9d15d40c"},"cell_type":"code","source":"#Simdi regresyon işlemi\nfrom sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()   #random sate = 0\ntree_reg.fit(x,y)  #ağac yapımı olusturdum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2895b6e54bd726fc1f23c993aff58ab20ee3cdb3"},"cell_type":"code","source":"tree_reg.predict([[5]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85275ef5f2bfa26e2595d246767b49a0775654af"},"cell_type":"code","source":"y_head = tree_reg.predict(x)  #burda x bi array zaten\nplt.scatter(x,y)\nplt.plot(x,y_head,color=\"red\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f26b787288027d1de767a7a4091cfa68c19cfee7"},"cell_type":"code","source":"#yeni bi array olusturursak\nx_ = np.arange(min(x),max(x),0.01).reshape(-1,1)  #diyorum ki x dizisini düzenle en küçük ile en büyüğü arasında 0.01 artarak bir dizi olustur.\nx_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e93aee7ea27d6f924fe7b6a414855234cddb5b7"},"cell_type":"code","source":"#tekrardan görselleştirirsek!\ny_head = tree_reg.predict(x_)  #burda x bi array zaten\nplt.scatter(x,y)\nplt.plot(x_,y_head,color=\"red\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55146c7b577a92e26b9ab481967429409e7a21bc"},"cell_type":"markdown","source":" #Decision Tree in MEDIUM = https://medium.com/@rnbrown/creating-and-visualizing-decision-trees-with-python-f8e8fa394176\n #bu makalede sundan bahsedildi."},{"metadata":{"trusted":true,"_uuid":"284f9b2f3f4dc72a25fa551b0c456a951e7bc949"},"cell_type":"code","source":"import sklearn.datasets as datasets\niris=datasets.load_iris() #burda iris datasetini array olarak yüklüyor.\ndf=pd.DataFrame(iris.data, columns=iris.feature_names) #burdada diziyi dataframe ceviriyor!!\niris","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"062a2da947628b9d203c8cbae7e086fa3c8aedb1"},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1dcdfd503059bf8219a0fdb6dd9d19858cb5086"},"cell_type":"code","source":"y = iris.target\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7458cb951515fb84703bb0a0db6746811bd09a95"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtree=DecisionTreeClassifier()\ndtree.fit(df,y)  #fit ediyoruz!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec63af81f495994241db4977368b3235c9d57a84"},"cell_type":"code","source":"#Now that we have a decision tree, we can use the pydotplus package to create a visualization for it.\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\ndot_data = StringIO()\nexport_graphviz(dtree, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())   #burda pydotplus modülü yokmus jupyter notebook ile yapılabilir.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4883393e31e6786faed5e5717d4a90eb4294b15d"},"cell_type":"markdown","source":"**Random Forest Regression**\n#bu konu ensamble learning 'in bir üyesi. Ensamble LEarning ise aynın anda birden fazla machine learning algoritmasını kullanıp bunların ortalamasını alıyor ve benim sonucum bu diyor!\n#Bu random forest mesela film izliyosun hemen ona benzer bir filmi sana reccommedation yapıyor.\n#Başka mesela=> body part classification, stock price prediction,tavsiye sistemleri"},{"metadata":{"trusted":true,"_uuid":"5dc127620d4afe9cf06c115d67b298c84dad6d31"},"cell_type":"code","source":"#Decision Tree deki aynı dataseti kullanılacak. Sahaya yakın yer en pahalı en uzak yer en ucuz\ndf = pd.read_csv(\"../input/decison-tree/decision-tree-regression-dataset.csv\",sep=\";\",header=None) #header ile!!!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb95d3e47d11819d2ea2684a61c3136496fc64b0"},"cell_type":"code","source":"x = df.iloc[:,0].values.reshape(-1,1)\ny = df.iloc[:,1].values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a6d07303172471723c9797c1ac1c415c9e50f99"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf_reg = RandomForestRegressor(n_estimators = 100, random_state = 42)  #burda ben random forest ın içinde kac tane tree kullanacam bunu vermem lazım\n#random state ise ben n sample lara ayırırken datamı her zaman aynı ayırsın diye 42 yazarım cıkan sonuc bi önceki ile aynı cıksın. istatistikte vardıya\nrf_reg.fit(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3f180d50f4a053ce59a64d52d978e9addcf2ba5"},"cell_type":"code","source":"rf_reg.predict([[7.6]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4221a78776a6f7a39ad8373f9ec5984ec065acb"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf_reg = RandomForestRegressor(n_estimators = 100, random_state = 5)  #ramdom_state i değiştirirsem\nrf_reg.fit(x,y)\nrf_reg.predict([[7.6]])\n#görüldüğü gibi verdiği tahmin aynı cıkmadı!!!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18b903e087155b4b452166c0e2f3e35ca1f87473"},"cell_type":"code","source":"#ben bunu şimdi görselleştircem decison tree de yaptıgım gibi\nx_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\ny_head = rf_reg.predict(x_)\n\nplt.scatter(x,y)\nplt.plot(x_,y_head,color=\"red\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6e9217aad4a7c0a8ba7439f6f6c598a1a5d5abd"},"cell_type":"markdown","source":"#Görüldüğü gibi decision tree ye cok benziyor.\n#Decision tree den farkı bir decision tree değilde girdiğimiz gibi 100 tane tree olması!!!\n"},{"metadata":{"trusted":true,"_uuid":"15263586d8d6c35ec65457105b93803667b637d8"},"cell_type":"markdown","source":"**Evaluation Regressıon Models with R-square**\n#Olay screenshot da anlatılmıstır.\n#R-square = 1 - (SSR/SST)     ve R-square 1 e ne kadar yakın olursa o kadar iyi demektir."},{"metadata":{"trusted":true,"_uuid":"05d05746e98452da60331859470222eaa3d617aa"},"cell_type":"code","source":"#Random Forest örneği için Evaulation\ndf = pd.read_csv(\"../input/decison-tree/decision-tree-regression-dataset.csv\",sep=\";\",header=None) #header verinin sütun baslıklarını ataR!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"551dfe96f9eb92ec0be0280d4393567722078507"},"cell_type":"code","source":"df  #index numarası verdi görüldüğü gibi baslıga\n    #kategoriye göre bilet fiyatları","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f25fe2a842f4abbd663776b9e5ddd357bcf0282"},"cell_type":"code","source":"x = df.iloc[:,0].values.reshape(-1,1)\ny = df.iloc[:,1].values.reshape(-1,1)\nplt.scatter(x,y)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"661102f137d4061767446842a04609a88acfc93c"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf=RandomForestRegressor(n_estimators = 100, random_state = 42) #burda sınıf sayısını ve randomluk derecesini(yani bi daha run ettiğimde aynı randomlukta çalıştır) vermeyi unutma!\nrf.fit(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e1dbfc262ef4d5933a53f02f7d1dfcf251538f0"},"cell_type":"code","source":"rf.predict([[3.5]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7697e9f29084c0cc5d71a28677c3f1048fcddb8d"},"cell_type":"code","source":"y_head = rf.predict(x)\ny_head   #elimizde bir x data sı ile fiyat olan y leri tahmin ettik şimdi tahminimiz ne kadar dogru ona bakcaz.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c4c22b36ac94c8ea41b740ea4ac8aa21e0616f7"},"cell_type":"code","source":"#Bunun için sklearn kütüphanesinde baska bir metodu kullanacaz.\n#Bu modelleri değerlendiren metodlar metrics diye gecer.\nfrom sklearn.metrics import r2_score\nprint(\"R_score =\",r2_score(y,y_head))  #kullanımı gayet basit sadece gerçek değerler,tahmini değerler\n\n#1'e yakın olması iyi bir prediction oldugu anlamına gelir. Bunu multiple regresyonda da yapılır. Önemli olan gerçek y, tahmini y değer karsılastırılması\n#zaten bu metod bizim direk formulu uyguluyo! R_square = 1- (SSR/SST)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cec570ec4ae33321f22d2eee97a7af8579141e58"},"cell_type":"code","source":"#Evaulation Regression with Rscore in Linear Regression\ndf = pd.read_csv(\"../input/regressiondata/linear-regression.csv\",sep=\";\") #deneyime göre maas tahmini\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec14c5ec5be79d9667a9742aa357ad4ef99cc1f5"},"cell_type":"code","source":"x = df.deneyim.values.reshape(-1,1)\ny = df.maas.values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"207505f3ef13fe749fa6951f2604ce283accedb4"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db9cce21890d07a899cdb76d0f82647c0937db1a"},"cell_type":"code","source":"lr.predict([[15]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7d45e8303700be3668c2d821b689a59e3b6bc6a"},"cell_type":"code","source":"y_head = lr.predict(x)\ny_head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60956dcd28019d70681487c4853d3520dddf0a01"},"cell_type":"code","source":"plt.scatter(x,y)\nplt.plot(x,y_head,color=\"red\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de6a51abdf99d098d7fde4efd0e6887945bbc279"},"cell_type":"code","source":"#Evaulation\nfrom sklearn.metrics import r2_score\nr_score = r2_score(y_true=y,y_pred=y_head)\nprint(r_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e760bb9888cdf42c70e140697e09b94edbcacc4f"},"cell_type":"markdown","source":"#Soru:Hocam, bizim fazla bağımsız değişkenli datasetimiz olsa biz predict yaptırırsak; mesela hız,motor gücü fiyat'a etki yapsa biz predict(23,42) 'i böyle mi yapacağız, yani  hız 23 güç 42 iken fiyat ne olur diye mi yapacağız? Yoksa kısa yolu var mı? (100 değişken olsa parantez içine 100 değişken için değer mi yazacağız yani)\n#Cevap:Yok tabi ki 100 değişken olsa tek tek yazmayacağız ama daha ilk derslerde insanlar işin mantığını anlasın diye öyle tek tek yazdım. Derslerde biraz daha ilerle sorunun cevabı derslerde var."},{"metadata":{"_uuid":"56c2eb7cc73244ff6fccc515f3d5cf4ba9b4c2ab"},"cell_type":"markdown","source":"**Kendi Deneme Alanım**\n#Linear datasetini random forest ile yapcam bakalım sonucumuz ne olacak?"},{"metadata":{"trusted":true,"_uuid":"9a64d579e53e1055aa6911e1c40aefbde09f5b72"},"cell_type":"code","source":"df = pd.read_csv(\"../input/regressiondata/linear-regression.csv\",sep=\";\") #deneyime göre maas tahmini\nx = df.deneyim.values.reshape(-1,1)\ny = df.maas.values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"daeb4bb3feaf052bf9213d7f9b7f93bea0806d96"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators = 1000,random_state=42)\nrf.fit(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e0202a34d558279e5c5dc0a3704fa1c90662358"},"cell_type":"code","source":"print(rf.predict([[15]]))\ny_head = rf.predict(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec8576118ad2be4cdfb2b831691d9955e6e7c50b"},"cell_type":"code","source":"plt.scatter(x,y)\nplt.plot(x,y_head,color=\"red\")\nplt.show()\n#Görüldüğü gibi daha spesifik bir tahmin doğrusu!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bc2c48727afa6dfdbffd76ac29d5793e05f868a"},"cell_type":"code","source":"from sklearn.metrics import r2_score\nrscore = r2_score(y_true=y,y_pred=y_head)\nprint(rscore)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b8e79d5b521835ce49cb000d8d40f39f1dad20a"},"cell_type":"markdown","source":"#Deneyime göre Random forest algoritması Linear reg 'dan daha fazla R2 ye sahip cıktı   !!!!"},{"metadata":{"trusted":true,"_uuid":"6bec8cfd7fe9bae3b8dd071b9b64119e0e8a585d"},"cell_type":"markdown","source":"***Classification***\n"},{"metadata":{"_uuid":"bdf949c4b80e8705ee7f4477b76f7ebec8036bff"},"cell_type":"markdown","source":"#BU BOLUMUN COK IYI OGRENILMESI GEREK CUNKU NEURAL NETWORK UN TEMELI BUDUR. ANLAMADIGIN YERI AC TEKRAR BAK!!!!\n* https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners\n* **Logistic Regression Classification**\n* #Logistic REgression genelde 0 ya da 1 çıktısı veren data larda kullanılır. Yani 'Binary'\n* #Mesela bi kedi ve köpek var onu tanımlatıyoruz ve yeni bir sey gösterdiğimizde kedi midir köpek midir tahmin ediyor!\n* #Ya da 0 ve 1 resmi var yeni bir resim gösterdiğimizde 0mı 1 mi onu bize söyleyecek.\n* #Ya da mesela kanser hücreleri var onların bize iyi huylu mu kötü huylu mu onu bize söyleyecek!\n* #Bu 'Logistic Regression' deep learning(neural network) ün en basit temelidir!"},{"metadata":{"_uuid":"76b991fa327cce48b1d63ace73036cfc94b57699"},"cell_type":"markdown","source":"**Computation Graph**\n#Bir matematiksel ifadeyi grafiksel ifade etme mesela c=(a^2+b^2)^0,5 böyle bir ifade\n* a->Square->\n                       + ->   Square Root ->  c\n* b->Square->"},{"metadata":{"_uuid":"28b32333cd4122884811adf70a5e3b61b7a34fff"},"cell_type":"markdown","source":"**Logistic Regression with Computation Graph**\n* https://www.udemy.com/machine-learning-ve-python-adan-zye-makine-ogrenmesi-4/learn/v4/t/lecture/11138644?start=0   (burdan bak daha iyi anlarsın)\n* #train etmek bir seyi kendi modelime uyarlamak\n* Burda olay weight ve bias i eğitmek onları en iyi dogru sekilde iyileştirmek (geriye giderken fonksiyonun türevini alıyoruz)"},{"metadata":{"_uuid":"fd2291e473cc5daf19e351f26f4fb5cb36ce6fa7"},"cell_type":"markdown","source":"* Soru:Merhaba hocam.Anlatımınız için çok teşekkürler. En doğru weight ve bias değerlerini loss function a göre sürekli olarak güncelliyoruz ancak anladığım kadarıyla treshold sabit bir değer olarak kalmaya devam ediyor.Treshold değerini neye göre belirliyoruz?\n* Cevap:Ortaya çıkabilecek sonuçlar 0 ve 1 arasında olduğu için bu iki sayının ortalamasını yani 0.5 değerini threshold olarak alıyoruz. Binary classification değilde birden fazla output olsaydı olasılığı en yüksek olanı alacaktık.\n"},{"metadata":{"_uuid":"aca3490e1ac42a80578b91539a3f2258634c67ce"},"cell_type":"markdown","source":"Summary\n* Parameters are weight and bias.\n* Weights: coefficients of each pixels\n* Bias: intercept\n* z = (w.t)x + b => z equals to (transpose of weights times input x) + bias\n* In an other saying => z = b + px1w1 + px2w2 + ... + px4096*w4096\n* y_head = sigmoid(z)\n* Sigmoid function makes z between zero and one so that is probability. You can see sigmoid function in computation graph.\n* Why we use sigmoid function?\n* It gives probabilistic result\n* It is derivative so we can use it in gradient descent algorithm (we will see as soon.)\n* Lets make example:\n* Lets say we find z = 4 and put z into sigmoid function. The result(y_head) is almost 0.9. It means that our classification result is 1 with 90% probability.\n* Now lets start with from beginning and examine each component of computation graph more detailed."},{"metadata":{"_uuid":"a388b2183a3b6008cc0700c13fa7de84a264c078"},"cell_type":"markdown","source":"**Initializing Parameters**\n* Burda weight ve bias ilk basta biz değer atıyoruz daha sonra computihan graphtan sonra geriye dogru güncelleniyor. İlk değeri nasıl atayacaz?\n* Bu değerlerin ilk değerlerini belirleyebilmek için birkaç teknik var ama bu artificial neural network konusunda(deep learningte) gösterilecektir.\n* Şimdilik weight ler 0.01 ve bias = 0 alınarak baslanır!"},{"metadata":{"_uuid":"a294446426ab4f4c95f66e6e72a72b8a28e195c8"},"cell_type":"markdown","source":"**Forward Propagation**\n* İleriye dogru gitme"},{"metadata":{"_uuid":"8cb8c5ad93f9ebd4c22e29a42a9a834c871572b2"},"cell_type":"markdown","source":"Summary\n * The all steps from pixels to cost is called forward propagation\n* z = (w.T)x + b => in this equation we know x that is pixel array, we know w (weights) and b (bias) so the rest is calculation. (T is transpose)\n* Then we put z into sigmoid function that returns y_head(probability). When your mind is confused go and look at computation graph. Also equation of sigmoid function is in computation graph.\n * Then we calculate loss(error) function. *Lost error dogru tahminde 0 yanlıs tahminde ise 1 yakın bir değer olacaktır*\n * Cost function is summation of all loss(error).\n* Lets start with z and the write sigmoid definition(method) that takes z as input parameter and returns y_head(probability)\n* As we write sigmoid method and calculate y_head. Lets learn what is loss(error) function\n* Lets make example, I put one image as input then multiply it with their weights and add bias term so I find z. Then put z into sigmoid method so I find y_head. Up to this point we know what we did. Then e.g y_head became 0.9 that is bigger than 0.5 so our prediction is image is sign one image. Okey every thing looks like fine. But, is our prediction is correct and how do we check whether it is correct or not? The answer is with loss(error) function:\n* Mathematical expression of log loss(error) function is that: \n               (1-y_true)*log(1-y_pred) + y_true*log(y_pred)\n* It says that if you make wrong prediction, loss(error) becomes big.\nExample: our real image is sign one and its label is 1 (y = 1), then we make prediction y_head = 1. When we put y and y_head into loss(error) equation the result is 0. We make correct prediction therefore our loss is 0. However, if we make wrong prediction like y_head = 0, loss(error) is infinity.\n* After that, the *cost function* is summation of loss function. Each image creates loss function. Cost function is summation of loss functions that is created by each input image.\n* Burda for döngüsü ile 348 tane forward yapmaktansa numpy array ile koca bir matris olustururum 4096x348 lik. Tek bir forward yaparım böylece! Sonra 348 tane loss fuction ım olur toplarsam 1 tane cost function ım olur. Sonra güncelleeyrek modeli iyileştirmeye calsırım."},{"metadata":{"_uuid":"7f27f968208568600bac3a29fe26b49a35d21566"},"cell_type":"markdown","source":"**Backward Propagation**\n* Geriye doğrı gitme--> Cost functiondan baslar ve geriye dogru gider.  bu bizim weightleri ve bias ı güncellememizi sağlar böylece modeli optimize ederiz.\n* Bu süreci optimization algorithm olan Gradient Descent metofu ile yaparız. Derece azaltma yani türev!!\n* Cost Function ı en az yapacak optimum parametreleri(weight and bias) bulmak\n* Türev demek eğim demek!!! Bir fonksiyonun bir noktaya göre türevi  o fonksiyonun eğimini verir.\n* Minimum noktadaysam eğim 0 cıkar. Burda min cost tur.\n* Learning Rate iyi ayarlanmalıdır, bir hyperparameter dır. Ne az Ne de cok olmalıdır. Yazicaz deneyecez yazicaz deneyecez öyle belirlenecek learning rate."},{"metadata":{"_uuid":"e684f36a5e3e99b05c179f9e2944f1bbbf3b195e"},"cell_type":"markdown","source":"Summary\n* Well, now we know what is our cost that is error.\n* Therefore, we need to decrease cost because as we know if cost is high it means that we make wrong prediction.\n* Lets think first step, every thing starts with initializing weights and bias. Therefore cost is dependent with them.\n* In order to decrease cost, we need to update weights and bias.\n* In other words, our model needs to learn the parameters weights and bias that minimize cost function. This technique is called gradient descent.\n\n\n*Lets make an example:\n\n* We have w = 5 and bias = 0 (so ignore bias for now). Then we make forward propagation and our cost function is 1.5.\n* It looks like this. (red lines) 7\n* As you can see from graph, we are not at minimum point of cost function. Therefore we need to go through minimum cost. Okey, lets update weight. ( the symbol := is updating)\n* w := w - step. The question is what is this step? Step is slope1. Okey, it looks remarkable. In order to find minimum point, we can use slope1. Then lets say slope1 = 3 and update our weight. w := w - slope1 => w = 2.\n* Now, our weight w is 2. As you remember, we need to find cost function with forward propagation again.\n* Lets say according to forward propagation with w = 2, cost function is 0.4. Hmm, we are at right way because our cost function is decrease. We have new value for cost function that is cost = 0.4. Is that enough? Actually I do not know lets try one more step.\n* Slope2 = 0.7 and w = 2. Lets update weight w : = w - step(slope2) => w = 1.3 that is new weight. So lets find new cost.\n* Make one more forward propagation with w = 1.3 and our cost = 0.3. Okey, our cost even decreased, it looks like fine but is it enough or do we need to make one more step? The answer is again I do not know, lets try.\n* Slope3 = 0.01 and w = 1.3. Updating weight w := w - step(slope3) => w = 1.29 ~ 1.3. So weight does not change because we find minimum point of cost function.\n* Everything looks like good but how we find slope? If you remember from high school or university, in order to find slope of function(cost function) at given point(at given weight) we take derivative of function at given point. Also you can ask that okey well we find slope but how it knows where it go. You can say that it can go more higher cost values instead of going minimum point. The asnwer is that slope(derivative) gives both step and direction of step. Therefore do not worry :)\n* Update equation is this. It says that there is a cost function(takes weight and bias). Take derivative of cost function according to weight and bias. Then multiply it with α learning rate. Then update weight. (In order to explain I ignore bias but these all steps will be applied for bias),\n* Now, I am sure you are asking what is **learning rate** that I mentioned never. It is very simple term that determines learning rate. Hovewer there is tradeoff between learning fast and never learning. For example you are at Paris(current cost) and want to go Madrid(minimum cost). If your speed(learning rate) is small, you can go Madrid very slowly and it takes too long time. On ther other hand, if your speed(learning rate) is big, you can go very fast but maybe you make crash and never go to Madrid. Therefore, we need to choose wisely our speed(learning rate).\n* Learning rate is also called hyperparameter that need to be chosen and tuned. I will explain it more detailed in artificial neural network with other hyperparameters. For now just say learning rate is 1 for our previous example."},{"metadata":{"_uuid":"c18afd4c99d27565f18c50b3c66c27592b32e37e"},"cell_type":"markdown","source":"* Soru: Ayrıca, bir nesne sınıflandırma algoritması geliştiriyorken, kullanacağımız veriler doğal olarak çeşitli resimlerden oluşacak. Peki bu resimlerden oluşan veri setini programımıza her zaman ki gibi \"csv\" doyası olarak mı sunuyoruz, yoksa .jpg .png gibi resim olarak m? Nesne sınıflandırma için kaggle da  veri seti ararken resimler halinde bulmayı beklerken bulduğum veri setleri .csv formatındaydı. Resimler nasıl .csv olarak düzenleniyor?\n\n* Cevap:jpg yada png olarak read ediyoruz daha sonra numpy arraya çevirip ister csv olarak depolarız istersek de depolamadan numpy array olarak kullanırız.Resimleri numpy arraya çevirip .csv formatı ile depolıyorlar. data = np.asarray( img, dtype=\"int32\" ) kısa bir metodu var."},{"metadata":{"trusted":true,"_uuid":"f7bf76663b330df6f22977d2c660633fd828b9b4"},"cell_type":"code","source":"#Şimdi ilk bunları bir tümör dataseti üzerinde herseyi elimizle yazarak kodlayacaz. Daha sonra sklearn kütüphanesini kullanacağız!!\ndf = pd.read_csv(\"../input/ninechapter-breastcancer/breastCancer.csv\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9b894c024f68deead377922583dfbc638378e53"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"659c4b55cd9c340ce8345c10a17559e707450803"},"cell_type":"code","source":"#Bu data da tümörün featurelara göre diagnosis sınıfı verilmiş. İyi huylu mu kötü huylu mu diye\n#id ve unnamed:32 sütunu tümörü sınıflandırma için gerekmez. Bundan dolayı bunları drop edeceğim.\ndf.drop([\"Unnamed: 32\",\"id\"],axis=1,inplace = True)  #axis=1 sütunu komple demekti(axis=0 ise row içindi). inplace ise daraframe eşitle demekti\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25991ff1b7a8718ea66d3f4a305bdbc864089309"},"cell_type":"code","source":"df.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"854a2076e5713e94ec6838568b39ac7e2bce7330"},"cell_type":"code","source":"#Şimdi classify yaparken diagnosis object gözüküyo. Ama ya integer ya da categorical bir değer olması gerekiyor.\n#ilk integer'a göre cevirecez yani 0 ve 1 değişkenine döndürcez.\ndf.diagnosis = [ 1 if i == \"M\" else 0 for i in df.diagnosis ]  #M(kötü huylu ise) 1, B ise 0 yapıyoruz.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18dca6609b1ce43a9160354099c3c462b309b71d"},"cell_type":"code","source":"print(df.diagnosis.values)\ndf.diagnosis.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1c0653c7e824b4ae25fa812fb0979ee4fc04d53"},"cell_type":"code","source":"#burda benim y eksenim yani sınıflarım diagnosis, x eksenim ise diğer bütün feature lar\ny = df.diagnosis\nx_data = df.drop([\"diagnosis\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8af29fab4d6ae6a689db52177f0f547fd93c7a68"},"cell_type":"code","source":"#Diğer en önemli nokta da şu-> featurelar da 2500 değeri olup 0,00032 bilmem değeri olan da var. Bu durum büyük olan diğer feature ı etkisiz bırakabilir.\n#Bundan dolayı bütün değerleri 0 ile 1 arasında bir değer yapacagım ki birbiri arasında etkileşim olmasın modelim bozulmasın. Buna Normalization denir.\n#Normalization(featureların birbirine üstünlük saglamamaları gerek.)\n#Formül => (x-min(x)) / (max(x)-min(x))\n\nx = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data)).values  #values demek numpy array e cevirmektir.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efc1a8e279f9137ad57d396ea07749662a68b770"},"cell_type":"markdown","source":"#Soru:Hocam normalize ederken kullandığımız formül sabit mi yoksa veri seti veya algoritmamıza göre değişkenlik gösterebilir mi? Ayrıca normalize etmemizi sağlayan hazır bir method yok mu?\n#Cevap:Merhaba dostum normalize ederken kullanılan formul sabittir veriye göre değişmez. Amaç 0 ve 1 arasına tüm veriyi sıkıştırmaktır. Ayrıca birde standardize etmek vardır normalize ile karıştırmamak gerekir ben bazen ikisi aynı şey diyebiliyorum. Ve evet sklearn preprocess metotlarından birinden kısa yolu var."},{"metadata":{"trusted":true,"_uuid":"b7898a9b3dec704c3c602117a429aaebd3b90adf"},"cell_type":"code","source":"a = pd.DataFrame(x)\na.describe()  #görüldüğü gibi 0-1 arasında bütün değerler dagılmıstır.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddcd1c93b7024380bc384e95abd8bc724a6d91a7"},"cell_type":"code","source":"#Train Test Split\n#Bu bölüm elimde bir data var ben bu datayı eğitecem ve modelimi kuracam ama aynı zamanda modelim dogru calısıyor mu diye test etmem de gerekecek.\n#Bundan dolayı elimdeki data nın %80 ile train yapacam. Geri kalan %20lik data mı ise test için kullancagım!\n#Bu ayırma işlemini cok güzel yapan sklearn kütüphanemiz var!\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 42) \n#bu metot bize 4 adet çıktı veriyor sırasıyla.(random_state aynı randomlık içindi hani tekrar calıstırdıgımızda aynı değerleri versin diye)\n#test boyutunu %20 aldık.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5e4a838e6bb1acdccd83ff88212a9aa441ff372"},"cell_type":"code","source":"x_test  #görüldüğü gibi 110 tane sample var. Ve bu x_test in sırası y_test in ki ile aynıdır.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8429b96f7f083e4d75d895abddc1d3c227a8fc6a"},"cell_type":"code","source":"#Şimdi hani konu anlatımında pxel ler yukarıdan asagıya idi(yani featurelar) ve resimler soldan saga matriste idi yani(farklı sampler)\n#Burda ters bunun için transpoze alacağız.\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x_train:\",x_train.shape)\nprint(\"x_test:\",x_test.shape)\nprint(\"y_train:\",y_train.shape)\nprint(\"y_test:\",y_test.shape)\n\n#yani benim 30tane pixelim yani feature ım var ve benim 455 tane eğitelecek örneğim ve 114 tane test edilecek sample var.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3ef63ac96c9dc3ab535ef19ca6112c0211f4bb8"},"cell_type":"code","source":"#Initializing Parameters and Sigmoid Function\n#Neydi hani baslangıcta benim her pixelimin(yani featuremın) bir baslangıc weight i vardı.Bunları baslangıcta 0.01 alıyorduk.(ama başka teknikler ilerde deep'te)\n#Sonra bunları toplayım bias ı ekliyorduk baslangıc biası da 0 kabul ediyorduk.\n#Daha sonra cıkan değeri 0 ie 1 arasında bir değer versin diye sigmoid functiona sokuyorduk ve eğer 0.5ten büyükse sonucu 1 alıyorduk.Yani kötü huylu\n\ndef initialize_weight_and_bias(dimension):   \n#bu dimension su: hani her pixel için bir weight vardı ya burda da her feature için bir weight olmalı. 30 tane feature oldugu için 30 tane weight olacak.\n    \n    w = np.full((dimension,1),0.01)  #aynı np.ones veya np.zeros gibi np.full da istediğimiz matrisi yaratmamızı saglar.Yani 30'a 1lik matris ve bütün w'ler 0.01\n    b = 0.0\n    \n    return w,b\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8656de3f0ed8040cc67fe9343c314168d0346f72"},"cell_type":"code","source":"#Sigmoid Function\n#Şimdi sırada sigmoid funtion ın formülasyonunu yazacaz. İnternette hemen bulabilirsin.\n#formül => f(x) = 1 / (1 + e^(-x))\n\ndef sigmoid_func(z):\n    \n    y_head = 1 / (1 + np.exp(-z))   #e üssü demek np.exp(x) demektir!!\n    return y_head\n\n#Zaten bu cıkan y_head değeri ile x_test değerlerini girerek y_test değerleri ile karsılastıracagız.\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16356ac25e386d43cdcab708c3d07c148ee729f7"},"cell_type":"code","source":"#ex\nsigmoid_func(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4a0724ceea34b3a2f97bca53cf1705c3df31c9d"},"cell_type":"code","source":"#Forward and backward propagation\ndef forward_bacward_propagation(w,b,x_train,y_train):\n    #forward propagation\n    z = np.dot(w.T,x_train) + b  #burda klasik matris carpımı yapıyoruz. (a,b)(b,c) matris çarpımı olur. Ondan weight'in transpozesi alınır.\n    y_head = sigmoid_func(z)\n    loss = -(1-y_train)*np.log(1-y_head) - (y_train*np.log(y_head))  #bu da loss function ın formulu idi.\n    cost = np.sum(loss) / x_train.shape[1]  #cok abartı cıkmasın diye ortalama alıyoruz. 'for scaling' yani ölçülendirme için\n    \n    #backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] #türev formülü bu sabit bir matematiksel ifade-türev demek eğim demek\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}  #bu yeni weight ve bias ı sözlük içinde depoluyoruz.\n    return cost,gradients\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"439b645c037955350fbdbfd881128072761df931"},"cell_type":"markdown","source":"#Soru:Şimdi biz min costa erişmeye çalışıyoruz fakat global min ve local min eşit olduğu zaman bir sorun yaşamıyoruz.Global min ve local min farklı olsaydı eğer biz yaptığımız işlemlerle local min'e ulaşıp tamam bizim min costumuz bu diyecektik fakat hata yapmış olacaktık.Bu sorunu nasıl çözebiliriz?\n#Cevap:Çok güzel bir noktaya parmak bastın bu genel herkesin yaşadığı bir sorun. Çözmek için learning rate 'i modifiye etmemiz lazım."},{"metadata":{"trusted":true,"_uuid":"0fd194cdf2b645a647658bf50eff4fc27c6a630c"},"cell_type":"code","source":"#Update\n#Şimdi sırada weight ve bias ları güncelleme yaparak en iyi değerlerini ve en az cost değerini bulma yapacaz.\n\ndef update(w,b,x_train,y_train,learning_rate,number_of_iteration):\n    cost_list = list()\n    cost_list2 = list()\n    index = list()\n    \n    #updating parameters is number_of_iterations times \n    for i in range(number_of_iteration): #biz her bir forward ve backward yapmamız bir iteration dır.\n        cost,gradients = forward_bacward_propagation(w,b,x_train,y_train) #bunun fonksiyonunu yazmıstık\n        cost_list.append(cost)  #bütün iterasyon sonucu cıkan cost u listeme atıyorum.\n        \n        #update kısmı=aslında benim modelimi train etmek demektir!!\n        #her iterasyonda yeni değerleri w,b ye esitliyoruz\n        w = w - learning_rate*gradients[\"derivative_weight\"] #burda gradients yeni w ve bias değerleri sözlükte depoladıgımız\n        b = b - learning_rate*gradients[\"derivative_bias\"]\n        #bu formül vardı hani leraning rate ile.öğrenme hızı yavas olsa uzun sürer fazla versek hiç öğrenemeyebilirz.\n        \n        if i %10 ==0:\n            cost_list2.append(cost)   \n            #bunu yapmamın amacı tamamen görünüş ile alakalı. Çok fazla iterasyonda o kadar cost cıkacak 10da bir yazdırırsam daha güzel gözükür.\n            index.append(i)\n            print(\"Cost after iteration %i: %f\" %(i,cost))  #bu yazımı öğren aynı formatlama gibi\n    \n    parameters = {\"weight\":w,\"bias\":b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation=\"vertical\")\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    return parameters,gradients,cost_list    #gradients=türevlenmis hali w ile b nin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03d044e416bb132d0e6cf4154f87551ebab8fdcc"},"cell_type":"code","source":"#Prediction \ndef prediction(w,b,x_test):\n    z = sigmoid_func(np.dot(w.T,x_test) + b)\n    Y_prediction = np.zeros(1,x_test.shape[1])  #bir matris olusturuyorum (1,114 lük) karsılastırma için\n    \n    #if z is bigger than 0.5, our prediction is 1(y_head=1) kötü\n    #if z is lower than 0.5,our prediction is 0 (y_head=0) iyi\n    \n    for i in range(z.shape[1]):   #daha sonra cıkan z değerleri ile kosullu durumla y_prediction matrisimi dolduruyorum!!\n        if z[0,i] <=0.5:   #treshold kısmı 0.5\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n    \n    return Y_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9847d11424de8c3207737c46116a84510df3f17b"},"cell_type":"code","source":"#Implemeting Logistic Regression\ndef logistic_regression(x_train,y_train,x_test,y_test,learning_rate,number_of_iteration):\n    #initialize\n    dimension = x_train.shape[0] #that is 30\n    w,b = initialize_weight_and_bias(dimension)\n    #sırada forward ve backward var ama ben ayrı ayrı yazmak yerine zaten update metodunun içinde kullandım forwardbackward ı\n    #do not change learning rate\n    parameters,gradients,cost_list = update(w,b,x_train,y_train,learning_rate,number_of_iteration)\n    \n    #prediction\n    y_prediction_test = prediction(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    \n    print(\"Test Accuracy {} %\".format(100-np.mean(np.abs(y_prediction_test - y_test))*100))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb37e96608ce4dc633214aa1628a235fbbb97c25"},"cell_type":"code","source":"#Result\nlogistic_regression(x_train,y_train,x_test,y_test,learning_rate=1,number_of_iteration=100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed9e058a531cd514d62a821ca54caf321a30eadf"},"cell_type":"markdown","source":"**Hocanın Yaptığı**"},{"metadata":{"trusted":true,"_uuid":"7697b80f55f7b98fb6ba36abbcea1e501460b368"},"cell_type":"code","source":"# %% read csv\ndata = pd.read_csv(\"../input/ninechapter-breastcancer/breastCancer.csv\")\ndata.drop([\"Unnamed: 32\",\"id\"],axis=1,inplace = True)\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\nprint(data.info())\n\ny = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"],axis=1)\n\n# %% normalization\nx = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data)).values\n\n# (x - min(x))/(max(x)-min(x))\n\n# %% train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)\n\n# %% parameter initialize and sigmoid function\n# dimension = 30\ndef initialize_weights_and_bias(dimension):\n    \n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\n\n\n# w,b = initialize_weights_and_bias(30)\n\ndef sigmoid(z):\n    \n    y_head = 1/(1+ np.exp(-z))\n    return y_head\n# print(sigmoid(0))\n\n# %%\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients\n\n#%% Updating(learning) parameters\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n\n#%%  # prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n\n# %% logistic_regression\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 30\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 300)    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"216ed2ce6170a3f08f4b771c576f9e753ff7c3f0"},"cell_type":"markdown","source":"**Sklearn with Logistic Regression**"},{"metadata":{"trusted":true,"_uuid":"73909ee9698a7b00ee953ea715f4aa5170e21ea5"},"cell_type":"code","source":"data = pd.read_csv(\"../input/ninechapter-breastcancer/breastCancer.csv\")\ndata.drop([\"Unnamed: 32\",\"id\"],axis=1,inplace = True)\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\nprint(data.info())\n\ny = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bdc8737fe57620dd2db5874fbcedda31273f93f"},"cell_type":"code","source":"# %% normalization\nx = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data)).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee94a5b79f60e13c0fa0e0ae529f790ddaf017f9"},"cell_type":"code","source":"# %% train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66b50a8a0491680c258c02ae4be9faf5f8445a38"},"cell_type":"code","source":"#%% sklearn with LR\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\nprint(\"test accuracy {}\".format(lr.score(x_test,y_test)))  #accuracy aynı R2score gibi tahminin doğrulugunu ölçüyor.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a12dbf3e37126415d8d0c8e380139b2798cfca28"},"cell_type":"markdown","source":"**KNN Algorithm**"},{"metadata":{"trusted":true,"_uuid":"0d84c93464c20ac72165bb0903cb5b1774936f3a"},"cell_type":"code","source":"df = pd.read_csv(\"../input/ninechapter-breastcancer/breastCancer.csv\") #aynı kanser datası üzerinde çalısılcak\ndf.tail()\n#Malignant = M kötü huylu\n#Benign = B iyi huylu tümör","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee362b17566dadc43328fd49a200dac983c2aca8"},"cell_type":"code","source":"#Id ve unnamed featurelarından kurtulcam.çünkü gereksiz featurelar\ndf.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c23db41ead3c2023efad8de2c1a399fd35355255"},"cell_type":"code","source":"#Şimdi datamı iyi huylu ve kötü huylu olarak ikiye ayırcaam\nM = df[df.diagnosis == \"M\"]\nB = df[df.diagnosis == \"B\"]\nM.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d46fdcf06195dab4c5a1c1c0e1509abf67a7996c"},"cell_type":"code","source":"B.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fcccb6b030642c0f831d129f76bdc8f378da98f6"},"cell_type":"code","source":"#M ve B yi 1,0 şeklinde cevirecem. Çünkü biz class label larımızı string istemiyoruz ya integer ya da categorical\ndf.diagnosis = [1 if i==\"M\" else 0 for i in df.diagnosis]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e267fb7373f2a28a5f20fc4dba0df886afeca612"},"cell_type":"code","source":"#Scatter iyi huylu ve kötü huylunun radius_mean ile area_mean karsılastırılması\nplt.scatter(M.radius_mean,M.area_mean,color=\"red\",label=\"Kotu\")\nplt.scatter(B.radius_mean,B.area_mean,color=\"green\",label=\"Iyi\")\nplt.legend() #lejant\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9af36d5b6279613655a99c7cb6d9d9d502aa1741"},"cell_type":"markdown","source":"Görüldüğü gibi gözle görülür bi ayrım var."},{"metadata":{"trusted":true,"_uuid":"987396139dc2b3fc3a008f701ce6678fece6c20b"},"cell_type":"code","source":"#Başka feature ları scatter yapalım\nplt.scatter(M.radius_mean,M.texture_mean,color=\"red\",label=\"Kotu\",alpha = 0.4)  #alpha saydamlık oranı!\nplt.scatter(B.radius_mean,B.texture_mean,color=\"green\",label=\"Iyi\",alpha= 0.4)\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"texture_mean\")\nplt.legend() #lejant\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"967587611f0a4714bbbdeecda6cc6556241d53cf"},"cell_type":"markdown","source":"**Adımlar**\n#KNN = K nearest neighbour(en yakın K komşu)\n#1) K değerini seç\n#2) K değeri kadar en yakın data noktalarını bul\n#3) K en yakın komşu arasına hangi class tan kaç tane var hesapla\n#4) Test ettiğimiz point ya da data hangi class a ait tahmin et"},{"metadata":{"_uuid":"fc042cfa2289c0a3d32a339346380ce652a3aed6"},"cell_type":"markdown","source":"**Açıklama**\n#Mesela yukarıdaki grafikte(radius- texture)  bir test noktası seciyoruz. Mesela x=20, y=30.\n#Daha sonra K değerini sececeğiz. Bu şu demek. Diyelim K =3. Sectiğim test noktasına en yakın 3 tane nokta bul ve onları sec.\n#3(K) tane komsu arasında hangi classtan kac tane var => kotu=3 , iyi=0\n#Kotu class ıma daha yakın oldugu için bu test noktama kötü diyorum."},{"metadata":{"_uuid":"2e4e077f64c503f321732e66ae20e5b75196f8f9"},"cell_type":"markdown","source":"**Önemli**\n#KNN ile en önemli şey normalization yapmaktır. Aksi taktirde featurelar birbirini domine edebilir. Bunu kesin yapmalıyız yoksa data mızın accuracy cok düşük cıkabilir.\n#K değerini çift verdik ve esit sayıda iyi ve kötü cıktı bu durumda algoritma sınıflandırma yapamaz, k değerini bir artırır ve ya azaltır.\n#KNN algoritması kendisi zaten en yüksek score için K değerini kendi içinde kendisi belirler.\n#Hoca cevap:  K değerini deneyerek seçiyoruz. ve Ama gerçek hayatta sadece en iyi score değeri bizim k değerini seçmemiz için yeterli olmuyor. Bu nedenle gerçekten deneyerek buluyoruz :)"},{"metadata":{"trusted":true,"_uuid":"47db668452b92fbc829e58883aaee298475705f7"},"cell_type":"code","source":"#Data mızı x ve y olarak ayırıyoruz. value demek array a cevirir.\ny = df.diagnosis.values  #datamızın iyi huylu mu kötü huylumu sonuc değerleri\nx_data = df.drop([\"diagnosis\"],axis=1)  #diagnosisi cıkarırsam geriye kalan x matrisi olur\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b483387cb9213e876bfa980b91b4e3951f11a84f"},"cell_type":"code","source":"#Normalization\n#En önemli şey= normalization çünkü noktalar arası mesafe bulanacak aynı derecede değerlendirmeliyiz.\nx = (x_data - np.min(x_data)) / (np.max(x_data)-np.min(x_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d074fdf3d3ff2960bbe8efc9b63c9ba62e4866e"},"cell_type":"code","source":"#Train test split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, random_state = 42 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f994b2fa16c1d6b64d75b0f0916dac179a50ca1a"},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6424426c0bb373e784ef7338deafd5a8f4a0e150"},"cell_type":"markdown","source":"**Notlar**\n#1) Data scientists usually choose as an odd number if the number of classes is 2 and another simple approach to select k is set k=sqrt(n).\n#2)K value should be odd\n#3)K value must not be multiples of the number of classes\n#4)Should not be too small or too large"},{"metadata":{"trusted":true,"_uuid":"5817c1f14f94a55b428f4afec9293f7aed11bbcc"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3 )  #n_neighbor = k\nknn.fit(x_train,y_train)  #modeli eğitiyoruz\nprediction = knn.predict(x_test) #x_test değerlerinin y değerlerini tahmin et\nprediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2071abc51f1a03b90e62b616e4bd8e85cfef4944"},"cell_type":"code","source":"#Şimdi K=3 ken accuracy e bakalım\nprint(\"K={} iken accuracy: {}\".format(3,knn.score(x_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6af7afaa9052bd8705aebb76b379852bf8f8277"},"cell_type":"code","source":"#Find k value\nk_value = []\naccuracy = []\nfor i in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors=i)\n    knn2.fit(x_train,y_train)\n    score = knn2.score(x_test,y_test)\n    k_value.append(i)\n    accuracy.append(score)\nfor i,j in zip(k_value,accuracy):\n    print(i,j)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"865ca2838c1c7c2b32d195d9cb578b10d30c3287"},"cell_type":"code","source":"#Find K value for Max accuracy\nplt.plot(range(1,15),accuracy,color = \"blue\")\nplt.xlabel(\"K value\")\nplt.ylabel(\"Accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b9afc36df39faba551c4dd874ea92a21eece851"},"cell_type":"code","source":"max_deger = 0 #accuracy\nfor i in range(1,200):\n    knn = KNeighborsClassifier(n_neighbors=i) \n    knn.fit(x_train, y_train)\n    score = knn.score(x_test,y_test)\n    \n    if score > max_deger:\n        max_deger,k = score,i  \n    else:\n        continue\nprint(max_deger,k)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2261d172bfac76eadb932dd5ee7f6a4e2da446c"},"cell_type":"markdown","source":"**Sonuç**\n#Random_state(yani randomlugu) değiştirince de accuracy değişiyor. Mesela 1 yapınca %95 oluyor.\n#K değeri değişince de accuracy değişiyor.\n#test_size değişince de accuracy değişiyor."},{"metadata":{"trusted":true,"_uuid":"9c054f5b2de5eb1c39ea25fe492a1efc5131a1ce"},"cell_type":"markdown","source":"**Support Vector Machine**\n#İki ayrı sınıf var birbirleri arasında best line çizilecek ve bu iki gruptan best line a en yakın noktalar support vector olur. o noktalar arası uzaklık max margin dir.\n#Yani burda iki sınıfı best line ile ayıracagız."},{"metadata":{"trusted":true,"_uuid":"b9971c25d721b8c6bd439b870e4031c9588141e4"},"cell_type":"code","source":"df = pd.read_csv(\"../input/ninechapter-breastcancer/breastCancer.csv\")\ndf.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\ndf.diagnosis = [1 if i==\"M\" else 0 for i in df.diagnosis]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a76ba38cfa3d659ef2ae9b9c6c549827e2b0f95e"},"cell_type":"code","source":"y = df.diagnosis.values\nx_data = df.drop([\"diagnosis\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4e44f1c62d513dd751e55b757578c6c97ba3ec5"},"cell_type":"code","source":"#normalization\nx = (x_data - np.min(x_data)) / (np.max(x_data)-np.min(x_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf52d27c16a615f4c7e66c56ac162fb914074759"},"cell_type":"code","source":"#train test split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e8386daa8f5c543ba8ddaa59c92c289a5603493"},"cell_type":"code","source":"from sklearn.svm import SVC\nsvm = SVC(random_state=42)\nsvm.fit(x_train,y_train)\nprint(svm.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0412f24717226c4cb9127f82bcfb4e24ac018ec"},"cell_type":"markdown","source":"**Naive Bayes Classification**"},{"metadata":{"trusted":true,"_uuid":"4e23d1cca423ecd20cc66891fde87d2e00ab7d7f"},"cell_type":"code","source":"#konu anlatımındaki o çemberi similatiry_range ile belirliyoruz. Ama hoca anlatmamıs kod da ona bakarsın!\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(nb.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e6370011d7dd73048aacc88eee6925da24c2a4b"},"cell_type":"markdown","source":"6 Easy Steps to Learn Naive Bayes Algorithm (with codes in Python and R) :  https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/"},{"metadata":{"trusted":true,"_uuid":"0430a159c254c38d1f7cac561c050deaa9a224bf"},"cell_type":"markdown","source":"**Decision Tree Classification**"},{"metadata":{"trusted":true,"_uuid":"91f9ddafd14c531bf473467992334a71198413e1"},"cell_type":"code","source":"df = pd.read_csv(\"../input/ninechapter-breastcancer/breastCancer.csv\")\ndf.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\ndf.diagnosis = [1 if i==\"M\" else 0 for i in df.diagnosis]\ny = df.diagnosis.values\nx_data = df.drop([\"diagnosis\"],axis=1)\n\nx = (x_data - np.min(x_data)) / (np.max(x_data)-np.min(x_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0f4cbfecdef53d74223b31e7d8fce316e494412"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.15,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d82e36329d89738f5a1b5985de4fe76e58efdbbc"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(random_state = 42)  #bunun içinde de random_state algoritması var. Her zaman aynı randomluk için bunu da yaz.\ndt.fit(x_train,y_train)\ndt.score(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d8f612fa82b330baedfeacb1eb41f58413c1306"},"cell_type":"markdown","source":"**Random Forest Classification**\n#İçinde sub sample a ayırır ve bu sample lardan bir sürü decision tree elde eder. Bunun sonunda ise test datası sırasıyla decision tree lara girer ve çoğunluk olan karar secilir.\n#Decision tree 'ya göre daha yüksek bir accuracy ve dayanıklılıga sahiptir.\n#Bir ensamble learning modeli dir. Birleştiriyo bir kaç algoritmayı\n#2 sub sample içine aynısı düşebilir. Subsamplelar eşit ve random seçilmiş olabilir. Tüm trainden rastgele seçiliyor."},{"metadata":{"trusted":true,"_uuid":"184eb0d6e080360cc86a1c4409394489907da1cb"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 100,random_state = 42) #n_estimator = tree sayısı\nrf.fit(x_train, y_train)\nrf.score(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af905f3ca7b5672752224dfe4dd166d0252d4e78"},"cell_type":"markdown","source":"#Accuracy karsılastırma sonucudur!! yani x_test i koy modele cıkan tahmini sonucları gerçekleriyle karsılastır."},{"metadata":{"_uuid":"d11373381c7501daaa6824aedc0e6eefa73be015"},"cell_type":"markdown","source":" **Confusion Matrix** = Diğer bir performans ölçütü(Accuracy gibi(score))\n#Bu matris şudur diyelim kedi ve köpek sınıflı bir 100 sample lık datam olsun. Ve accuracy yüzde 80 olsun. Burda kaç tane dogru kedi ve ya köpek bilmişim vee\n#Köpek oldugu halde kac tanesine kedi, kedi oldugu halde kac tanesine köpek demişimin sonucunu verir.\n#Mesela 1000000 sample lık datam olsun ve unbalanced bir data olsun. Yani 990bin tane kedi, 10bin tane de köpek olsun. Ve ben bütün data yı kedi olarak tahmin edersem accuracy %99 oluyor. Ama ben hiç köpek için doğru tahmin yapamamısım. Accuracy bizi yanıltabilir."},{"metadata":{"trusted":true,"_uuid":"0cd554b9a94df92541114c94d2b6f8c9d7332484"},"cell_type":"code","source":"#Random Forest Classfi algoritmasını kullancam bu örnekte\ndf = pd.read_csv(\"../input/ninechapter-breastcancer/breastCancer.csv\")\ndf.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\ndf.diagnosis = [1 if i==\"M\" else 0 for i in df.diagnosis]\ny = df.diagnosis.values\nx_data = df.drop([\"diagnosis\"],axis=1)\nx = (x_data - np.min(x_data)) / (np.max(x_data)-np.min(x_data))\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.15,random_state=42)\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 100,random_state = 42) #n_estimator = tree sayısı\nrf.fit(x_train, y_train)\nprint(\"Accuracy: \",rf.score(x_test,y_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f08719b60f2bf3f20caab76b297b973f15062c3"},"cell_type":"code","source":"#Confusion Matrix i elde edebilmek için y_pred ve t_true değerlerine ihtiyacım var.\ny_pred = rf.predict(x_test)\ny_true = y_test\n\n#Confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae105f4a55d0debb91d4d74b5523073850fecffb"},"cell_type":"markdown","source":"#Yani ben 53 tane iyi huyluyu doğru ve 29 tane kötü huyluyu doğru bilmişim.\n#Veeee 3 tane kötü oldugu halde onlara iyi, 1 tane iyi oldugu halde ona kötü demişim!"},{"metadata":{"trusted":true,"_uuid":"97d0c3fea00e2e0d18ac16d50b7fce2d187bb250"},"cell_type":"code","source":"#Confusion matrix visualization\n#Heat map yapcaz!!!\nimport seaborn as sns\nf, ax = plt.subplots(figsize = (5,5))  # bu plot'umun boyutunu ayarlar!!\nsns.heatmap(cm,annot = True, linewidths = 0.5, linecolor = \"red\", fmt = \".0f\", ax = ax) \n#annot= matrisin sayılarını grafikte yazdırmak için(false ya da annot yazmazsak sayılar yazmaz)\n#fmt = ondalık basamaklar için kullanılır.(.0f demek ondalık basamak yok)\n#eksenim ax tir.\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65760686c0f51fd7bcae32d0588147cd8b0ba093"},"cell_type":"markdown","source":"** K-Means Clustering(Unsupervised Learning)**\n#Label yok\n#Gruplama yapacaz belli bir sınıf ayrımını bilmediğimiz için verileri kendi arasında gruplayacağız.\nSteps:\n#1) K değeri seç. Örneğin K=2 demek datamı iki sınıfa ayıracam demektir.\n#2)Kyı 2 sectiğimiz için data içinden random bir şekilde 2 tane centroid atayacak\n#3)Sonra data pointlerin bu centroide göre olan uzaklıklarına göre bir gruplama yapılacak ve hangisine en kısa mesafede ise o gruba dahil olacak.\n#4)Peki gruplama yapıldı ve dogru mu acaba. Bunun için olusan grupların data poinlerinin ortalaması alır ve yeni centroid o ortalama olur.\n#5)Centroidlerin yeri değişmeyene kadar clustering devam eder."},{"metadata":{"_uuid":"5dbd2af5c0ba25cde785086bf4b37c74735902e4"},"cell_type":"markdown","source":"**K değerini nasıl seçeriz yani kaç seçmeliyiz ve seçtiğimiz değerin doğru olup olmadığını nasıl yapacaz?**\n#Bunun için bir metrics var. Bu şu** WCSS: within cluster sum of squares**\n#WCSS: diyelim K=2. Datamızı 2 gruba ayırdık. ve her grubun merkez noktası secilir ve grubundaki üyelerin kendi merkezine olan uzaklıkların hepsinin toplamı WCSS değerini verir.\n#Biz WCSS değerinin min oldugu değeri isteriz. Ama diyelim 1000 tane data point var ve ben K=1000 alırsam WCSS=0 olacaktır böylece. Ama bu zaten sacma hemde performas acısından baya kötüdür. Onun için burda K ve WCSS arasında trade off vardır.\n#Birde diyelim WCSS ler mesela birbirine yakın değerler fazlasıyla burda **Elbow(dirsek) Kuralı** yapılmalıdır.\n#Elbow Kuralı: Azalan ya da artan bir grafikte birden kırılmanın yaşandıgı noktaya elbow deriz ve o noktayı seçeriz."},{"metadata":{"trusted":true,"_uuid":"8ea03784456209136446d9ac93a6bc7fee3adf8b"},"cell_type":"code","source":"#Burda datasetimizi kendimiz hazırlayacaz! Ben cluster sayımı 3e göre hazırlayacam.Bakalım K-means algoritması çözecekmi dogru!\n#Şimdi 2 feature ı olan ve 3 sınıflı bir data\n\n#class 1\nx1 = np.random.normal(25,5,1000)\ny1 = np.random.normal(25,5,1000)\n\n#class 2\nx2 = np.random.normal(55,5,1000)\ny2 = np.random.normal(60,5,1000)\n\n#class 3\nx3 = np.random.normal(55,5,1000)\ny3 = np.random.normal(15,5,1000)\n                                         #bu concanate methodu sadece yukarıdan asagı birleştirme işlemi yapar.   \nx = np.concatenate((x1,x2,x3),axis = 0)  #aynı pd.concat gibi birleştirme işlemi. axis 0 demek yukarıdan asagı birleştir.\ny = np.concatenate((y1,y2,y3),axis = 0)\n\nsozluk = {\"x\":x,\"y\":y}\ndf = pd.DataFrame(sozluk) #Dataframe olusturma\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d9edb7663ca4e55d895910ebbd8c1b2b84320fe"},"cell_type":"code","source":"#diğer bir data olusturma\nz=np.column_stack((x,y)) #column_stack metodu ise sütüunları yanyana birleştirir.\ndata = pd.DataFrame(z)\ndata.rename(columns={0:\"x\",1:\"y\"},inplace = True)\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3fb8eb43de5ecb4729d7637e9d0901c2ddeaf791"},"cell_type":"code","source":"f,ax=plt.subplots(figsize=(10,6))\nplt.scatter(df.x,df.y,color=\"red\",alpha=0.5)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"935807878d2d837648119bbffa90051b2de88335"},"cell_type":"code","source":"f,ax=plt.subplots(figsize=(10,6))\nplt.plot(x1,y1,label=\"class1\",color=\"red\")\nplt.plot(x2,y2,label=\"class2\",color=\"green\")\nplt.plot(x3,y3,label=\"class3\",color=\"black\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c331081a5cb21e00c1785cfc9824c868782c9dc"},"cell_type":"code","source":"#Aslında K mean için benim böyle bir datam var. Bakalım K means ile nasıl cözecez\nf,ax=plt.subplots(figsize=(10,6))\nplt.scatter(df.x,df.y,color=\"black\",alpha=0.5)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d94f1f54077ea8a578f16fe9d2cecd2d6f383faf"},"cell_type":"code","source":"# Kmeans\n#ilk önce K değeri,bunun için de wcss metriği\nfrom sklearn.cluster import KMeans\nwcss = []\nfor k in range(1,16):\n    kmeans = KMeans(n_clusters = k)\n    kmeans.fit(df)\n    wcss.append(kmeans.inertia_)   #inertia_ methodu her bir k değeri için wcss değerini döndürür.\nplt.plot(range(1,16),wcss,color=\"purple\")\nplt.xlabel(\"K_values\")\nplt.ylabel(\"WCSS\")\nplt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38f9e9a0df18095819ac96bfab8d4cab413c4943"},"cell_type":"markdown","source":"#Grafikte görüldüğü gibi **Elbow Kuralına** göre **K=3** alınır."},{"metadata":{"trusted":true,"_uuid":"02d6dd7ea8ff3e62a326e0f5b0d1dbeb80d916e8"},"cell_type":"code","source":"#K=3 için modelim\nkmeans2 = KMeans(n_clusters = 3)\nclusters = kmeans2.fit_predict(df)  #burda clusterlarımı fit_predict methodu ile. Modeli kur ve kümeleri olustur tahimn et demek\n\npd.DataFrame(clusters) #görüldüğü gibi üç gruba ayırdı ve gruplara 0,1,2 numaralarını verdi.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d762eec0fc5d49b056570345da429478bbedfdd4"},"cell_type":"code","source":"#Şimdi bu grupları datama ekleyeyim\ndf[\"label\"] = clusters\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"383d9fb7d9b256c8e48ccd245cdb0275a60244d8"},"cell_type":"code","source":"#Şimdi de görselleştirelim bakalım doğrumu sınıflandırmış. \n#Datanın saf hali ve tahmin edilen sınıflara göre görselleştirip karsılastıralım.\n#Saf hali\nplt.scatter(df.x,df.y)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f96070b9d0171c586d351e998c14981bf75576f"},"cell_type":"code","source":"#Labellara göre\nplt.scatter(df.x[df.label==0],df.y[df.label==0],color = \"red\")  #x ve y eksenleri label ları 0 olanlar\nplt.scatter(df.x[df.label==1],df.y[df.label==1],color = \"blue\") #x ve y eksenleri label ları 1 olanlar\nplt.scatter(df.x[df.label==2],df.y[df.label==2],color = \"green\")#x ve y eksenleri label ları 2 olanlar\n\n#!!!Clusterların centroidlerini gösterme methodu!!!!!!!!\nplt.scatter(kmeans2.cluster_centers_[:,0],kmeans2.cluster_centers_[:,1],color=\"yellow\")\n#Bu iki boyutlu bişi olduğu için x,y olarak cıktı alcaz. : --> bütün centroidleri al x ve y koordinatları\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97ce62c5cb64664ce2f69f88947192bb454b28ce"},"cell_type":"markdown","source":"**Hiearcical Clustering(HC) -- Dendogram\n**"},{"metadata":{"trusted":true,"_uuid":"48074dcdcc78eaa8fbfc11b2b12bbb5a2596a0a3"},"cell_type":"code","source":"#Kmeans de yaptıgım gibi numpy ile dataset olusturcam.\n#Burda datasetimizi kendimiz hazırlayacaz! Ben cluster sayımı 3e göre hazırlayacam.Bakalım HC algoritması çözecekmi dogru!\n#Şimdi 2 feature ı olan ve 3 sınıflı bir data\n\n#class 1\nx1 = np.random.normal(25,5,1000)\ny1 = np.random.normal(25,5,1000)\n\n#class 2\nx2 = np.random.normal(55,5,1000)\ny2 = np.random.normal(60,5,1000)\n\n#class 3\nx3 = np.random.normal(55,5,1000)\ny3 = np.random.normal(15,5,1000)\n                                         #bu concanate methodu sadece yukarıdan asagı birleştirme işlemi yapar.   \nx = np.concatenate((x1,x2,x3),axis = 0)  #aynı pd.concat gibi birleştirme işlemi. axis 0 demek yukarıdan asagı birleştir.\ny = np.concatenate((y1,y2,y3),axis = 0)\n\nsozluk = {\"x\":x,\"y\":y}\ndf = pd.DataFrame(sozluk) #Dataframe olusturma\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c562635bcc7ecd03db081b22b9cd000bb0201b34"},"cell_type":"code","source":"plt.scatter(x1,y1)\nplt.scatter(x2,y2)\nplt.scatter(x3,y3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff106214b2bb6fdb5c34b539be8ba0aae64930ec"},"cell_type":"code","source":"#Dendogram\n#Dendogram için farklı bir kütüphane kullanacağız. scipy kütüphanesi\nfrom scipy.cluster.hierarchy import linkage, dendrogram  #bu import ettiğimiz linkage, hieararchy algoritması için\n\nmerg = linkage(df,method = \"ward\") #K means ta method olarak wcss(en min uzaklıklar toplamı için) kullanıyorduk.\n                                   #ward ise clusterlarımızın içinde yayılımları(varyansları) minimize et demek.\n\nf,ax = plt.subplots(figsize = (20,10))    \ndendrogram(merg,leaf_rotation = 90) #merg benim scipy kütüphanesinin hieararcical algoritmam olmus oluyo. \n                                    #leaf_roration ise x ekseni değerlerini eksene 90 derece gelecek sekilde yazması.\n#burda dendogramın üzerine cizgi çirdirmeye çalıstım ama tam olmadı\n#burda data point noktaları 2 boyıtlu alıp kendisi ona göre x boyutunda bir değer atıyor.\na = np.linspace(800,800,3000) #600den basla 600e kadar,3000 tane x değeri var ondan\nb = np.arange(1,3001,1)\nplt.scatter(b,a,color = \"black\")    \nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidean distance\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"912e3258a01e6a5568795b89ee047e3f26858828"},"cell_type":"markdown","source":"#Burda trashold değerinine bakacak olursak, distance mesafesi(yatay eksen tarafından kesilmeyen) en yüksek olan yer uzun kırmızı çizgi.\n#Oradan cekersek trasholdu sınıf sayısı 3 şeklinde verecektir."},{"metadata":{"trusted":true,"_uuid":"370f3a53b1b75de8774f94c1c3d72786f2a82a0b"},"cell_type":"code","source":"#HC\nfrom sklearn.cluster import AgglomerativeClustering\nhieartical_clustering = AgglomerativeClustering(n_clusters = 3, affinity = \"euclidean\", linkage = \"ward\")\n#burdaki n_cluster seviyesini dendogramdaki trashol a göre belirliyoruz.\n#affinity ise öklid distance a göre sınıflandırma yap. linkage ise metodumuz ward (minimiza varyans)\ncluster = hieartical_clustering.fit_predict(df)\ncluster","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3851c132f2996255e2c597e5dcb738bdf41b154c"},"cell_type":"code","source":"df[\"label\"] = cluster","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1311ce15432ef817384ec1158e4e46d7fbcafd46"},"cell_type":"code","source":"plt.scatter(df.x[df.label==0],df.y[df.label==0],color = \"red\")  #x ve y eksenleri label ları 0 olanlar\nplt.scatter(df.x[df.label==1],df.y[df.label==1],color = \"blue\") #x ve y eksenleri label ları 1 olanlar\nplt.scatter(df.x[df.label==2],df.y[df.label==2],color = \"green\")#x ve y eksenleri label ları 2 olanlar\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee07b19185506f48a1625e1a9660e80ccbdaa5de"},"cell_type":"code","source":"a = np.linspace(10,20,5)  #10 ile 20 arasına 5 eşit sayı\na\nb = np.arange(1,10,2) #1den basla 10 a kadar 2ser artırarak dizi olustur.\nb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ed88f4f9f2ec6d8cb7547a08c84952b422b2ee7"},"cell_type":"markdown","source":"**Natural Language Process**\n#Siri gibi, ses text e dönüştürülür ve text process edilir.\n#Text Mining\n#Mesela kitapların konusunu tahimn etme de. Korku mu gerilim mi falan diye\n#Ya da bir romanı okutuyo sonra bana 3 4 cümle ile özet cıkart gibi\n#Mesela question -answer gibi\n#Bunun için kullanılabilecek kütüphaneler : nltk,spacy,open nlp,stanford nlp gibi\n"},{"metadata":{"trusted":true,"_uuid":"e74b66d573f091b8b03d6013ee4386674003b63b"},"cell_type":"code","source":"#burda data olarak bir twitter datasetini kullancaz. Twitterdan data üretmek için Twitter API konusuna ait !!!\ndf = pd.read_csv(r\"../input/twitter-user-gender-classification/gender-classifier-DFE-791531.csv\",encoding = \"latin1\") \n#datasetinde latin harfleri oldugundan dolayı. r ise read demek\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59150262f64d7e2bf48e59e00d8e0b85dd2bc2e3"},"cell_type":"code","source":"#ben burda datamda cinsiyetler ve atılan yorumlar var. Amac atılan yorumlara göre atan kişinin cinsiyetini tahmin etme\n#ondan sadece gender ve yorumları alıp yeni bir data olusturuyorum.\ndf = pd.concat([df.gender,df.description],axis = 1)\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2149347c227de793829430781920be1b67bbcca6"},"cell_type":"code","source":"df.info()\n#nan valuelar var görüldüğü gibi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0142cfaa9ee47f07f17e4a3fc36c246a87065d92"},"cell_type":"code","source":"df.dropna(axis = 0 , inplace = True) #satır olarak nan value ları at","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c10813108041c3d934e342b8b3f7e29b6429fc8"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61b8bfebec2aaf490305481cbf2c7cf5618d8011"},"cell_type":"code","source":"#gender lar burda string halde. kadınları 1 erkekleri 0 yapalım\ndf.gender = [1 if i ==\"female\" else 0 for i in df.gender]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ac4b9c09e361bbd8b096a0ee064cca3abedc7c4"},"cell_type":"code","source":"#cleaning data\n#burda datada sacma sapan karakterler gülücükler var. Bunlar için Regular Expression yapacaz. Bunun için bir kütüphane var\nimport re\nfirst_description = df.description[4]\nfirst_description\n#burda görüldüğü gibi :) karakterleri var bunları atacağız.re kütüphanesi ile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44b72b65bc5224a156f635c9fe97bb4d22272d5d"},"cell_type":"code","source":"description = re.sub(\"[^a-zA-Z]\",\" \",first_description) \n#diyoruz ki [^a-zA-Z] bu adan zye olan büyük veya küçük harfleri secme(^). Secmediklerini boşluk ile değiştir.\ndescription","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c130a1394bb06a5062b1466b48b4368733d4739"},"cell_type":"code","source":"description2 = re.sub(\"[a-zA-Z]\",\" \",first_description)\ndescription2 #görüldüğü gibi ^ işareti seçme demek. koymayınca seciyo ve harfleri değiştiriyor.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c63408c42e89c7c25952bc60ef2aa583f202f09f"},"cell_type":"code","source":"#bütün harfleri küçük yapma\ndescription = description.lower()\ndescription","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15503b1ab115c5c744ef004785c93fafad7341eb"},"cell_type":"code","source":"#stopwords (irrelevant words) gereksiz kelimeler mesela bunlar and,the,a gibi kelimeler bunlar grammerle alakalı kadın erkek ayrımı ile alakası yok.\nimport nltk\n#nltk.download(\"stopwords\") #kaggle dısında bunu bilgisayara indirmek için bu kullanılmalı\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d94dfb5a4d912edd1a0213436fc7cb65bc9dcfcd"},"cell_type":"code","source":"#stopwords leri import ettik. Benim bu yorumları önce bir tek tek kelime kelime ayırmam lazım ki stopwordler ile karsılastırayım ve onları atım\ndescription = description.split() #default u zaten boşluğa göre\ndescription","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4d7d7850d049082d9d2b5f1b82b2bfaa4486644"},"cell_type":"code","source":"#split yerine tokenize methodunu kullanabiliriz bunun bir artısı da var örneğin 'shouldn't ve guzel' stringini split ile 3e ayrılacaktır.\n#ama tokenize ile should n't ayrı alıp 4 kelime yapacaktır!!!\ndescription = df.description[4]\ndescription = re.sub(\"[^a-zA-Z]\",\" \",description)\ndescription = description.lower()\n\ndescription = nltk.word_tokenize(description)\ndescription","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e9fae9f876dbcd66cf6ac61a14dc70b775475b6"},"cell_type":"code","source":"#stopwords leri çıkarma\ndescription = [i for i in description if not i in set(stopwords.words(\"english\"))]\n#stopwors un içindeki english kelimeleri çünkü içinde bir sürü dilin kelimelri var. set ise unique ifadeler için.yani hızlı olsun diye","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74fd7338089bd2707a160a49d14af98f7e0e6ee6"},"cell_type":"code","source":"description","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1adf1fdb1c935488b6957fd8904e96c9dbadb09"},"cell_type":"code","source":"#Lemmazation= bu şu demek mesela maça gitmek çok güzel,maç iyiydi,maçını seveyim gibi burda maç kelimesinin çekimi var\n#bunun için bilgisayarda aslında bunlar hepsi ayrı be kelimedir. ama önemli olan asıl kökü olan maç kelimesidir.\n#Köküne inmeyi öğreneceğiz\nimport nltk\nlemma = nltk.WordNetLemmatizer()\ndescription = [ lemma.lemmatize(i) for i in description]\ndescription","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eec60b503249ef0efa0471b09e2340c013187d82"},"cell_type":"code","source":"#join methodu:gerekli işlemlerden sonra artık tekrar metin haline cevirelim listemizi\ndescription = \" \".join(description)\ndescription","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"006702f545bce12a2f0f8e57cb451428a659bf09"},"cell_type":"markdown","source":"#Soru:Hocam mesela yüz kelimesinin 2 ayrı anlamı var bi yüzümüz anlamındaki yüz birde yüzmek anlamındaki mesela . Bu tarz kökleri aynı olan kelimeler için ne yapmamız gerekiyor ?.Yoksa cümleye göre kelimenin hangi anlamda kullanıldıgını anlıyormu ?\n\n#Cevap: anlamıyor çünkü bag of words te her kelimeden sadece 1 tane olabilir. Kendi anlamını öğrenebilmesi için kendinden önceki kelimeye bakması gerekir ve bu iş yükünü 2 katına çıkarır.Evet anlamıyor sadece tek bir kelime olarak görüyor \"yüz\" kelimesini.Ve evet kendinden önceki kelimeye bakması lazım.\n\nBunun ile ilgili recurrent neural network kursunda bir örnek yapacağım."},{"metadata":{"trusted":true,"_uuid":"c8782ace329d2d8ae83144d264fb5d002b856cd4"},"cell_type":"code","source":"#Şimdi bütün data üzerine bu işlemleri ayrı ayrı uygulayacaz\n#Sırasıyla harf olmayanları temizleme,küçük harf,stopwordsleri temizleme,kelimenin köküne inme\ndescription_list = list()\nfor i in df.description:\n    i = re.sub(\"[^a-zA-Z]\",\" \",i) #a'dan z'ye olmayanları seç ve yerine boşluk koy\n    i = i.lower()                #küçük harf\n    i = nltk.word_tokenize(i)    #metni kelime kelime ayır liste yap\n    i = [word for word in i if not word in set(stopwords.words(\"english\"))]  #listedeki kelimelri stopword ile karşılastır\n    \n    lemma = nltk.WordNetLemmatizer()\n    i = [lemma.lemmatize(word) for word in i]   #kelimenin köküne in\n    \n    i = \" \".join(i)   #listeyi boşluk ile birleştir \n   \n    description_list.append(i)\n    \n#cok zaman alırsa stopwords yüzündendir onu kaldırırsın ilerde baska yolla o problemi çözcez.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f11dbd8c986bab414d32e3c2b9bab76299d1859"},"cell_type":"code","source":"description_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"339dd10c64820c4872eaaffbe23a484915a1723d"},"cell_type":"code","source":"#bag of words !!! temel su. her ayrı unique kelime bir feature(sütun) oluyo ve cümleler satır oluyo. varsa cümlede 1 yoksa 0 oluyo\n#Bunun için sklearn kütüphanesinde method var\nfrom sklearn.feature_extraction.text import CountVectorizer\nmaxi = 500 #burda olay su 16 bin tane cümle var.E her cümlede 2 unique kelime olsa 32bin feature eder ve baya uzun sürer\n                  #diyoruz ki max feature mız en cok kullanılan 500 kelime olsun\ncount_vector = CountVectorizer(max_features = maxi,stop_words = \"english\") #burda stop_words u yapabiliyoduk. Hani yukarıda uzun sürerse\n                                                                           #burda küçük harfi lowercase= parametresi ile\n                                                                           #gereksiz karakteri de tokken_pattern= parametresi ile yapabilirdik.\n#bu yazdıgım parametreler ile bi denersin\nsparce_matrix = count_vector.fit_transform(description_list)\nsparce_matrix\n#bunu isim olarak gösterdi array için toarray methodu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"992829d6ea94d2e82ba8c96590d161636afb4322"},"cell_type":"code","source":"sparce_matrix = count_vector.fit_transform(description_list).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0aeba783c17f85f999434a6d0aaa4714aa20a2aa"},"cell_type":"code","source":"sparce_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77c52bf3faf15803de0c66cf0ea679836dc92328"},"cell_type":"code","source":"#peki ben bu 500 tane feature ım ne bakmak istersem\ncount_vector.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a727c1826cd094dc78b6284d1c3f816d629b682"},"cell_type":"markdown","source":"#Soru:hocam ben bi text dosyasının içinde kullanacağım. mesela bi roman, (word yada pdf hali olarak csv değil) en çok kullanılan kelimeleri bulmak istiyorum bunu nasıl yaparım\n#Cevap:Sana gerekli keywordler bunlar nltk (natural language tool kit) count word frequency metodu"},{"metadata":{"trusted":true,"_uuid":"8b99d26b7947ef5030f87418337ef7f0684c0331"},"cell_type":"code","source":"#Text Classification\n#burda yaptıgımız gibi datamızı train test split \n#burda x imiz aslında bizim sparce matrix imiz\ny = df.iloc[:,0].values.reshape(-1,1) #male or female classes\nx = sparce_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c50c7fb08291928966edfb9be8d696d201191ce8"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.1,random_state = 42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"330d57cb7246d041154900a9c9cab67406a9ebca"},"cell_type":"code","source":"#Burda naive baise algoritmasını kullancağız yani tamamen keyfi\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d85b3a85e78e9988af3a97371cfb4bf64ebbee4"},"cell_type":"code","source":"#Prediction\ny_pred = nb.predict(x_test)\nprint(\"Accuracy: \",nb.score(y_pred.reshape(-1,1),y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76a4cb45a9a85b87f0650d6de2997b45883ddcc8"},"cell_type":"markdown","source":"#Çok çok düşük bir accuracy. Kafadan atsak yüzde 50 ihtimal. Bunu nasıl artırabilirz. Şöyle ki max feature ı bi artırmayı deneyelim."},{"metadata":{"trusted":true,"_uuid":"5ab62b5072c440c4df3bdd6401444db4380315b4"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nmaxi = 5000 #burda olay su 16 bin tane cümle var.E her cümlede 2 unique kelime olsa 32bin feature eder ve baya uzun sürer\n                  #diyoruz ki max feature mız en cok kullanılan 500 kelime olsun\ncount_vector = CountVectorizer(max_features = maxi,stop_words = \"english\") #burda stop_words u yapabiliyoduk. Hani yukarıda uzun sürerse\n                                                                           #burda küçük harfi lowercase= parametresi ile\n                                                                           #gereksiz karakteri de tokken_pattern= parametresi ile yapabilirdik.\n#bu yazdıgım parametreler ile bi denersin\nsparce_matrix = count_vector.fit_transform(description_list)\n\nsparce_matrix = count_vector.fit_transform(description_list).toarray()\n\ny = df.iloc[:,0].values.reshape(-1,1) #male or female classes\nx = sparce_matrix\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.1,random_state = 42)\n\n\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n\ny_pred = nb.predict(x_test)\nprint(\"Accuracy: \",nb.score(y_pred.reshape(-1,1),y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"005f715e8caf7cd6423f63970498a30d2eb254b9"},"cell_type":"markdown","source":"**#Bir de bunu farklı ml algoritması ile denersin.!Tek tek tüm algoritmaları uygulamamız lazım ama bunun kolay yolu olarak grid search metodu var sklearn'in.!!!!!**\n**#en optimum algoritma derken? hız mı enerji mi accuracy mi neyi optimize etmek istiyorsun? Gerçi cevap yine evet olacak grid search ile bunların en uygununu seçebilirsin.**\n**#!!!!! https://www.kaggle.com/etatbak/nlp-google-store-reviews/ !!!!!!**\n\n\n#Soru:Hocam burada feature sayisini 10000 falan yaptığımızda yavaşlama olmaması için GPU yu nasıl aktif edebiliriz.\n\nYani tüm işlmeleri CPU üzerinden değil de GPU üzerinden yapmak istiyorum.\n#Cevap:Eğer kaggle için soruyorsan sağ tarafta ki menude gpu seçeneği var. Eğer spyder için soruyorsan sklearn de bildiğim kadarıyla yeni bir güncelleme olmadıysa gpu desteği yok. Bunun içinde zaten keras ve pytorch dersleri çekeceğim advance seviye deep ve machine learning başlığı altında.\n\n#Ben feature'larımız binomial olduğu için Bernoulli metodunu kullandım ve aldığım sonuç 0.6819722650231125 ."},{"metadata":{"_uuid":"93af8ec977f98625e45c829f5754017b494b6a58"},"cell_type":"markdown","source":"#Bir de yapılan ödevlere bak ek bilgiler öğrenirsin!!"},{"metadata":{"trusted":true,"_uuid":"c49d48ee9831cbcb811cdd66b45d9fda4f1c085d"},"cell_type":"markdown","source":"PCA-Principal Component Analysis\n#kulanımları:\n#feature extraction\n#feature reduction:biz en fazla bunu kullncaz cünkü gercek haytta 30 40 tane feature değil yüzbinlerce vardır. Ve bunları eğitmek test etmek cok uzun süre alabilir.\n#stock market pred\n#Mesela 10 boyutlu data var bunu görselleştir diyoruz ama bunu yapamayız max 4 5 boyut bunun için PCA\n#!!!onca feature dan 2 feature a düşüyoruz mesela burda data kaybetmemek için varyansın büyük olması gerek ki data kaybetmeyelim.\n#amac datayı hızlı train ve test etmek ve görselleştirmek\n"},{"metadata":{"trusted":true,"_uuid":"6d81b0b0305f3b10bf0c76fe8e623b8d6e1591d3"},"cell_type":"code","source":"#Bu sefer datasetini sklearn den iris dataseti\nfrom sklearn.datasets import load_iris\niris = load_iris()\niris","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"024a2af9c14dbd8c7f6f5db8a463c6539e51158b"},"cell_type":"code","source":"#dataframe çevirecem. Bu iris arrayin içinde yazıyolar aynı dataframe de featureları . ile yazıyorduk ya\ndata = iris.data\nfeature_names = iris.feature_names\ny = iris.target #çiçeklerin türleri 3 tane vardı bunu 0 1 ve 2 diye yazdılar aynı sekilde target names leri de var\n\ndf = pd.DataFrame(data,columns=feature_names)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"217808a15fef6247f541e5080162ae148e3ebec7"},"cell_type":"code","source":"df[\"labels\"] = y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01595120366a06571b7f978c529a4d5264a676ad"},"cell_type":"code","source":"x = data\ntype(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4246577d10f35ed562b956dbe8120e338d65bc41"},"cell_type":"code","source":"#amacım benim datayı görselleştirmek.Ama 4 boyut var bunu söyle yapabilirdik. 3 boyutlu grafik artı bi de renk kullanırdım\n#ama ben 2 boyutluya indirip öyle yapacam\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2, whiten = True) #n_components=2 demek orjinal datamı 2featurea 2 boyuta indir.\n                                           #whiten ise normalization\npca.fit(x) #normalde x,y yapıyorduk.ama burda bir test eğitim yapmıyoruz.ondan y ile sınıflarla işimiz yok. amac 2boyutlu görselleştirme\nx_pca = pca.transform(x) #burda sadece fit ile modeli kuruyosun. Datama uygulayabilmek için transform demem gerekir.\n\nx_pca","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ff286cb443e99bcc33bf73f33e01e67213c4090"},"cell_type":"code","source":"#burda hangisini principal component hangisinin second component için variance bakcaz\nprint(\"Variance Ratio: \",pca.explained_variance_ratio_)\n#yüzde 92 cok daha büyük oldugundan principal 1.si","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87e36b4d032a7bbf5472f4de58c0821164401e03"},"cell_type":"code","source":"#Peki ben 4ten 2ye düşürdüm bu varyansları. Ama ne nekadarlık datamı koruyabildim\nprint(\"Sum: \",sum(pca.explained_variance_ratio_))\n#yüzde 97lik varyansa yani datamın yüzde 97sine hala sahibim.Yüzde 3lük bilgi kaybı yasamısım","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fca92b0d2b55f7ea437e4d2c6842994a67c048c6"},"cell_type":"code","source":"#2D görselleştirme\nx_pca.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"072b2aa0a9fc068ebd3062f746dabfc8fa8905bb"},"cell_type":"code","source":"#bu componentleri dataframe ekliyorum.\ndf[\"p1\"] = x_pca[:,0]\ndf[\"p2\"] = x_pca[:,1]\ndf\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49f78b7c42181a1dc24cc9fdbdbe348c9027f958"},"cell_type":"code","source":"colors = [\"red\",\"green\",\"blue\"]\nplt.figure(figsize = (15,10))\nfor i in range(3):  #çünkü 3 türüm var. 0,1,2\n    plt.scatter(df.p1[df.labels == i],df.p2[df.labels==i],color=colors[i],label=iris.target_names[i])\nplt.xlabel(\"p1\")\nplt.ylabel(\"p2\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b74d9353f4868781b5e2c36c8bc6ddfaeb7c4c15"},"cell_type":"markdown","source":"4Ten 2ye düşürmeme ragmen gözle görülür bir ayrım var."},{"metadata":{"_uuid":"67bfcfe8217b415aededc575cf1949e56cc78958"},"cell_type":"markdown","source":"**Model Selection**\n#1) öğrenilen parametreler(train edilme biz bunu secemyiz)\n#2)seçilen parametreler(hyperparamaters) mesela knn deki K değeri\n\nK Fold Cross Validation : avoid overfitting(ezber) ezberden kacınmak.eğer ezberlerse datayı yeni gelen test datasını doğru train edemeyecektir.\n#mesela accuracy bi test datasında yüzde 89 diğerinde yüzde 60 cıkıyorsa burda bir problem vardır.\n#Biz normalde tek accuracy buluyorduk ama ben birden fazla datamı train ve test diye ayırıp daha fazla accuracy değerleri bulmalıyım. Bunu da K fold cross validation ile yapcaz"},{"metadata":{"_uuid":"f207da68d4b4da2169e0f2d44bd3a1948717a76a"},"cell_type":"markdown","source":"**K FOLD CROSS VALIDATION**"},{"metadata":{"trusted":true,"_uuid":"ff09d18d3267860cc17776db1a5296e1d55d8cf9"},"cell_type":"code","source":"from sklearn.datasets import load_iris\niris = load_iris()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1255c8004e284e60cb258a529732098692dc31d"},"cell_type":"code","source":"x = iris.data\ny = iris.target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b88388b89691978ddcd3599e6191bbbb06ba5cd7"},"cell_type":"code","source":"#KNN yapcaz önce normalization\nx = (x-np.min(x))/(np.max(x)-np.min(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b6778022bba961b2e35db9ecfa283c0fb45dd3b"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"351f0d6b5fb71cb83a5f9255ac2b6db2d3964650"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3) #K=3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9cef41ce2d2fb0def2780b379602bd1a0baa979"},"cell_type":"code","source":"#K FOLD CV, K=10 sececem. genelde literatürde 10 secilir\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = knn, X= x_train, y=y_train, cv = 10) #estimator=hangi algoritmayı kullancan, cv = K fold un K değeri\naccuracies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4bc721160ccd41d83c3b87eef093e26908f2ab4"},"cell_type":"code","source":"#avarege accuracy\nprint(\"Average accuracy: \",np.mean(accuracies))\nprint(\"Standard Deviation: \",np.std(accuracies)) \n#tutarlı bir data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"879026aa2d084d478291ac96e234ea1d645d9a90"},"cell_type":"code","source":"dir(np)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c74da3c659d4b4b6c17ae133894f09cd8351872"},"cell_type":"code","source":"#daha sonra artık ferçek test asaması, knn deki k değerine karar verdikten sonra \n#Test\n#Ama ondan önce biz crosfold da knn i fit etmiş olsak da burda fit etmemiz gerek\nknn.fit(x_train,y_train)\nprint(\"Test Accuracy: \",knn.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8e58dc1d6196c9413c3fe05e55149e3ae4fcfe1"},"cell_type":"markdown","source":"**Grid Search**\n#Grid Search daha saglam model için!! bu aynı zamanda mesela knn deki en yi K değerini seçerken bunun bize en iyi değerini ve aynı zamanda K fold yapacaktır"},{"metadata":{"trusted":true,"_uuid":"dd01b66176dc3dbcb06a228ac75b0940e9c8162f"},"cell_type":"code","source":"#iris dataset for grid search with knn\nfrom sklearn.model_selection import GridSearchCV\n\ngrid = {\"n_neighbors\":np.arange(1,50)} #knn deki k değerlerim 1den 50 ye kadar olsun\nknn = KNeighborsClassifier()\n\ngrid_knn = GridSearchCV(knn,grid,cv=10) #knn algoritması,knn değerleri,cv için K değeri\ngrid_knn.fit(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef975f95988bc9584186e0889a8fd972f0934952"},"cell_type":"code","source":"#print hyperparamater(knn deki k değeri)\nprint(\"tuned hyperparameters K: \",grid_knn.best_params_) #tuned ayarlanmıs demek.en iyi k değeri\nprint(\"En iyi K değerinin accuracy(best score): \",grid_knn.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57df4ef05f40b7743db8ee5c2f8a5d2e2e836075"},"cell_type":"code","source":"#grid search with logistic regression(output binary!!)\n#bundan dolayı datasetim iris 3tü 2ye düşüyorum. İlk 100 satır 2ye kadar olan ciçek türleri içindi\nfrom sklearn.linear_model import LogisticRegression\nx = x[:100,:]\ny = y[:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbcd9196acf28252a0edb548522631666f26302a"},"cell_type":"code","source":"grid = {\"C\":np.logspace(-3,3,7),\"penalty\":[\"l1\",\"l2\"]} #bu regulazation ama anlatmadı agır olabilir diye\n#l1:lasso l2:ridge.. yani aslında bunlar benim logistic regressionımı hyperparameters\nlogreg  = LogisticRegression()\ngrid_lr = GridSearchCV(logreg,grid,cv=10)\ngrid_lr.fit(x,y)\n\nprint(\"tuned hyperparameters K: \",grid_lr.best_params_) #\nprint(\"En iyi hyperparametrelerine göre accuracy(best score): \",grid_lr.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5f833f3180189f385af64015594a416b1661e9b"},"cell_type":"markdown","source":"#En iyi parametrelerim c=0.1 ve l2 penalty ve buna göre accuracy im 1 çıktı\n#Eğer Future Warning varsa en baştaki kütphane ile hallet!!!"},{"metadata":{"_uuid":"f480d3db96f3d666f7e7804399879c894490a125"},"cell_type":"markdown","source":"**bu k fold ve grid search te biz train test split yapmadık ama yapmamız gerekiyor. onun için öyle yaparsın !!!!!!**\ntekrardan bulunan bu hyperparameter değerlerine göre tekrardan model olustur ve test datasıını incele!"},{"metadata":{"_uuid":"1d94c4dc63e8c645c2b327b6f079d91c6558cbf3"},"cell_type":"markdown","source":"#Normalization\nsklearn üzərindən scaler etmək içn :\n\n#from sklearn.preprocessing import MinMaxScaler\n \n#scaler = MinMaxScaler(feature_range=(0, 1))\n#x= scaler.fit_transform(x)\n \ndatanı scalerdən çıkartmak için:\n \nscaler.inverse_transform(x)"},{"metadata":{"trusted":true,"_uuid":"bd71210afacfaada308521b8460b97ff4d49d03c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}