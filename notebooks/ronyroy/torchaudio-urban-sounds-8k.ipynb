{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"ls /kaggle/input/urbansound8k # the files are already in 10 folders for 10 fold validation etc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# source\n\n# https://everythingtutorial.com/pytorch-audio-tutorial\n# https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/audio_classifier_tutorial.ipynb\n# https://colab.research.google.com/drive/1g_33vGuSy_3SGV7Tc8REOLe8oBz1UA8w#scrollTo=sYyRFlZuOVvI","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install torchaudio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import Dataset\nimport torchaudio\nimport pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"csvData = pd.read_csv('/kaggle/input/urbansound8k/UrbanSound8K.csv')\nprint(csvData.iloc[0, :])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"csvData.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" air_conditioner, car_horn, children_playing, dog_bark, drilling, enginge_idling, gun_shot, jackhammer, siren, and street_music   \n are the classes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import IPython.display as ipd\nipd.Audio('/kaggle/input/urbansound8k/fold1/108041-9-0-5.wav')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ipd.Audio('/kaggle/input/urbansound8k/fold5/100852-0-0-19.wav')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '/kaggle/input/urbansound8k/fold5/100852-0-0-19.wav'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sound = torchaudio.load(path, out = None, normalization = True)\n#load returns a tensor with the sound data and the sampling frequency (44.1kHz for UrbanSound8K)\n# soundData = self.mixer(sound[0]) #Down mix mono doesnt exist anymore\n# https://github.com/pytorch/audio/issues/363","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" sound, sound [0] , sound [1] # an array and freq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"soundData = torch.mean(sound[0], dim=0).unsqueeze(1)\n# was as below\n# soundData = torch.mean(sound[0], dim=0).unsqueeze(0)# add a dim at idx 0 <-unsqueeze? \n# dim 0 is where the second channel comes in\nsoundData, soundData.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"soundData.numel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#downsample the audio to ~8kHz\ntempData = torch.zeros([160000, 1]) #tempData accounts for audio clips that are too short\ntempData.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nif soundData.numel() < 160000:\n    tempData[:soundData.numel()] = soundData[:]\nelse:\n    tempData[:] = soundData[:160000]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"soundData = tempData\nsoundFormatted = torch.zeros([32000, 1])\nsoundFormatted[:32000] = soundData[::5] #take every fifth sample of soundData\nsoundFormatted.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"soundFormatted = soundFormatted.permute(1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"soundFormatted.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.max(soundFormatted),torch.min(soundFormatted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class UrbanSoundDataset(Dataset):\n#rapper for the UrbanSound8K dataset\n    # Argument List\n    #  path to the UrbanSound8K csv file\n    #  path to the UrbanSound8K audio files\n    #  list of folders to use in the dataset\n    \n    def __init__(self, csv_path, file_path, folderList):\n        csvData = pd.read_csv(csv_path)\n        #initialize lists to hold file names, labels, and folder numbers\n        self.file_names = []\n        self.labels = []\n        self.folders = []\n        #loop through the csv entries and only add entries from folders in the folder list\n        for i in range(0,len(csvData)):\n            if csvData.iloc[i, 5] in folderList:\n                self.file_names.append(csvData.iloc[i, 0]) # file name see df.head() above\n                self.labels.append(csvData.iloc[i, 6]) # class id column\n                self.folders.append(csvData.iloc[i, 5]) # folder/fold num\n                \n        self.file_path = file_path\n#         self.mixer = torchaudio.transforms.DownmixMono() #UrbanSound8K uses two channels, this will convert them to one\n#         self.mixer = torch.mean(waveform, dim=0).unsqueeze(0)\n        self.folderList = folderList\n        \n    def __getitem__(self, index):\n        #format the file path and load the file\n        path = self.file_path + \"fold\" + str(self.folders[index]) + \"/\" + self.file_names[index]\n        sound = torchaudio.load(path, out = None, normalization = True)\n        #load returns a tensor with the sound data and the sampling frequency (44.1kHz for UrbanSound8K)\n        # soundData = self.mixer(sound[0]) #Down mix mono doesnt exist anymore\n        # https://github.com/pytorch/audio/issues/363\n        soundData = torch.mean(sound[0], dim=0).unsqueeze(1) # was .unsqueeze(0) \n        #downsample the audio to ~8kHz\n        tempData = torch.zeros([160000, 1]) #tempData accounts for audio clips that are too short\n        if soundData.numel() < 160000:\n            tempData[:soundData.numel()] = soundData[:]\n        else:\n            tempData[:] = soundData[:160000]\n        \n        soundData = tempData\n        soundFormatted = torch.zeros([32000, 1])\n        soundFormatted[:32000] = soundData[::5] #take every fifth sample of soundData\n        soundFormatted = soundFormatted.permute(1, 0)\n        return soundFormatted, self.labels[index]\n    \n    def __len__(self):\n        return len(self.file_names)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    \ncsv_path = '/kaggle/input/urbansound8k/UrbanSound8K.csv'\nfile_path = '/kaggle/input/urbansound8k/'\n\ntrain_set = UrbanSoundDataset(csv_path, file_path, range(1,10))\ntest_set = UrbanSoundDataset(csv_path, file_path, [10])\nprint(\"Train set size: \" + str(len(train_set)))\nprint(\"Test set size: \" + str(len(test_set)))\n\nkwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {} #needed for using datasets on gpu\n\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size = 128, shuffle = True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size = 128, shuffle = True, **kwargs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv1d(1, 128, 80, 4)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.pool1 = nn.MaxPool1d(4)\n        self.conv2 = nn.Conv1d(128, 128, 3)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.pool2 = nn.MaxPool1d(4)\n        self.conv3 = nn.Conv1d(128, 256, 3)\n        self.bn3 = nn.BatchNorm1d(256)\n        self.pool3 = nn.MaxPool1d(4)\n        self.conv4 = nn.Conv1d(256, 512, 3)\n        self.bn4 = nn.BatchNorm1d(512)\n        self.pool4 = nn.MaxPool1d(4)\n        self.avgPool = nn.AvgPool1d(30) #input should be 512x30 so this outputs a 512x1\n        self.fc1 = nn.Linear(512, 10)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(self.bn1(x))\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = F.relu(self.bn2(x))\n        x = self.pool2(x)\n        x = self.conv3(x)\n        x = F.relu(self.bn3(x))\n        x = self.pool3(x)\n        x = self.conv4(x)\n        x = F.relu(self.bn4(x))\n        x = self.pool4(x)\n        x = self.avgPool(x)\n        x = x.permute(0, 2, 1) #change the 512x1 to 1x512\n        x = self.fc1(x)\n        return F.log_softmax(x, dim = 2)\n\nmodel = Net()\nmodel.to(device)\nprint(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n        data = data.to(device)\n        target = target.to(device)\n        data = data.requires_grad_() #set requires_grad to True for training\n        output = model(data)\n        output = output.permute(1, 0, 2) #original output dimensions are batchSizex1x10 \n        loss = F.nll_loss(output[0], target) #the loss functions expects a batchSizex10 input\n        loss.backward()\n        optimizer.step()\n        if batch_idx % log_interval == 0: #print training stats\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(model, epoch):\n    model.eval()\n    correct = 0\n    for data, target in test_loader:\n        data = data.to(device)\n        target = target.to(device)\n        output = model(data)\n        output = output.permute(1, 0, 2)\n        pred = output.max(2)[1] # get the index of the max log-probability\n        correct += pred.eq(target).cpu().sum().item()\n    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_interval = 20\nfor epoch in range(1, 2): # use 41\n    if epoch == 31:\n        print(\"First round of training complete. Setting learn rate to 0.001.\")\n    scheduler.step()\n    train(model, epoch)\n    test(model, epoch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}