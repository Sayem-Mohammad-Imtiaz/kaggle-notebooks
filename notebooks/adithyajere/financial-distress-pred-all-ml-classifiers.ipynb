{"cells":[{"metadata":{"_uuid":"00fab4d969703c8f7946d5281867332ce2a65701"},"cell_type":"markdown","source":"# Intro\n\nThis dataset is from https://www.kaggle.com/shebrahimi/financial-distress.\n\nThe goal is to predict whether a currently healthy company will become distressed, before it becomes distressed.\n\nWe'll use **F-1 score** as our main evaluation metric to deal with the unbalanced set. \n\nWe'll pay particular attention to **recall** (of all companies that truly do become distressed, how often can we predict their distress before they become distressed?). \n\nWe can imagine that if this model were being used to guide investment choices or loans, it would be much more costly to accidentally classify a bad company as a good one (false negative - make a type II error) than to miss out on a good company because we falsely thought it was distress-prone.\n\nNote that this desire to avoid type II errors (with regards to being afraid of failing to identify \"badness\") is characteristic of many processes (companies that hire elite talent, universities with high admissions standards, highly-regarded VC firms).","execution_count":null},{"metadata":{"trusted":true,"_uuid":"ba48861795b54069de45ff856fef1ea0f3de5c85"},"cell_type":"code","source":"# Basics\nimport sys\nimport pandas as pd\nimport numpy as np\n%matplotlib inline\n\n# Imports for data loading\n# import psycopg2\n# import sqlalchemy\n# import imp\n# import os\n\n# Sklearn imports\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import f1_score, recall_score\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.model_selection import TimeSeriesSplit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c0d8e1cae54213863972d539b622421ba0110c8"},"cell_type":"code","source":"# secrets_filepath = '/home/casey/secrets.py'\n# secrets = imp.load_source('secrets', secrets_filepath)\n\n# # Postgres connection info\n# POSTGRES_ADDRESS = secrets.psql_ad\n# POSTGRES_PORT = secrets.psql_port\n# POSTGRES_USERNAME = secrets.psql_username\n# POSTGRES_DBNAME = secrets.psql_db\n# POSTGRES_PASSWORD = secrets.psql_pw\n\n# # Form string\n# postgres_str = ('postgresql://{username}:{password}@{ipaddress}:{port}/{dbname}'\n#                 .format(username=POSTGRES_USERNAME, \n#                         password=POSTGRES_PASSWORD, \n#                         ipaddress=POSTGRES_ADDRESS, \n#                         port=POSTGRES_PORT, \n#                         dbname=POSTGRES_DBNAME)) \n\n# # Make connection\n# cnx = sqlalchemy.create_engine(postgres_str)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b86ae6cacafe258b33406cd614437334ef243717"},"cell_type":"markdown","source":"# Loading Data\n\nI've loaded this into my local PostgreSQL db, but this can easily be replaced with a load from the csv file.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"79ede1cd77b644eb86e231f654606cba0336ab5b"},"cell_type":"code","source":"# companies = pd.read_sql_query('''SELECT * from casey;''', cnx)\n\n### UNCOMMENT BELOW TO LOAD FROM FILE ###\n\ncompanies = pd.read_csv('../input/Financial Distress.csv')\ncompanies.rename(index=str, columns={\"Company\": \"company\", \"Time\": \"time\", \"Financial Distress\": \"financial_distress\"}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c922eb5f62a8eca2ed40743ac45c23b5a5e63b22"},"cell_type":"markdown","source":"The Kaggle description tells us that if the number in the **financal_distress** column < -0.5, the company should be considered distressed. \n\nWe can imagine that this might be a financial ratio of some sort - ratio of income to capital or something.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"5d3e2ea8d9cd228eba67ecd86782c3be9ba9938d"},"cell_type":"code","source":"# Take a look at our loaded data to ensure all is in order\ncompanies.head()\n\n# Print some summaries and checks\n\n # shape\nprint(companies.shape)\n\n# dtypes\nprint(companies.iloc[:5,:5].dtypes)\n\n# check for nulls\nprint(companies.iloc[:5,:5].isnull().any())\n\n# Describe\nprint(companies.describe(percentiles=[0.25,0.5,0.75,0.99]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7174b38ac376bb3130c19d9eb32ce44cdf72bcfd"},"cell_type":"markdown","source":"Hm. Features x1, x7, and x81 look a little funny in terms of their maxes being much higher than their 99th percentile. If we knew what these features were we could have a decent interpretation, but unfortunately we do not.","execution_count":null},{"metadata":{"_uuid":"eaeee10fc0390c0ae72e2c723d8edf117f19bb60"},"cell_type":"markdown","source":"# Quick validity checks\n\nGet number of unique companies.\n\nCheck how many of these companies ever reach a distressed state (should be 136 by Kaggle description).\n\nGet a list of feature names.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"8d2e90daed65042f005ecf380cd4c1fd4fb4675e"},"cell_type":"code","source":"total_n = len(companies.groupby('company')['company'].nunique())\nprint(total_n)\n\ndistress_companies = companies[companies['financial_distress'] < -0.5]\nu_distress = distress_companies['company'].unique()\nprint(u_distress.shape)\n\nfeature_names = list(companies.columns.values)[3:] # ignore first 3: company, time, financial_distress\nprint(feature_names)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c211a8281d492957bc886f51694f3482507358f"},"cell_type":"markdown","source":"# We know feature 80 is categorical...\n...so let's pull it out as a list for use later.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"aab0c875964842f234a5993d42a1f6b105dc2701"},"cell_type":"code","source":"f80 = list(companies.groupby('company')['x80'].agg('mean'))\nf80 = [int(c) for c in f80]\n\n# print(f80)\n# print(len(f80))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b580397ee21557c39005aad58bef24e2dc573bcb"},"cell_type":"markdown","source":"# Temporal cross validation: how to do it?\n\nLet's follow the guidance set out in https://github.com/dssg/hitchhikers-guide/blob/master/curriculum/3_modeling_and_machine_learning/temporal-cross-validation/temporal_cross_validation_slides.pdf.\n\nIn order to pick a good date to separate train/test, we should ideally pick a date that allows most entities to appear in both the train and test data.\n\nUnfortunately not all the companies live for the same amount of time, so if we pick a date that is too early or late, we may cut many of the companies out of the test set.\n\nLet's generate a histogram of counts for each time period so we can pick a reasonable place to cut.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"29d5838596d1e43f1164feebcc1cf7c8cdb3e68f"},"cell_type":"code","source":"companies.hist(column=['time'], bins=14)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce4e0740e53ebb37840aae57b1fa36be03863b5d"},"cell_type":"markdown","source":"We notice a bit of a decline, then uptick in the histogram around time period 10.\n\nDeclines imply that a company dies out of the dataset, so if we set our cut around t=10, we should still get a decent number of distress events in the training data.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"43f6243f87f8d4fd5a2b4da72aab8bd3039f7c9d"},"cell_type":"code","source":"# We can see from this that most companies start at time period 1, \n# but there are some which start their life much later.\n\n# print(companies.groupby(['company'])['time'].agg('min'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ee90ca3406faf08e45317964e3c0f52238c5944"},"cell_type":"markdown","source":"# Does distress occur uniformly over time periods?","execution_count":null},{"metadata":{"trusted":true,"_uuid":"59eb79eed2d009b994db794707964283a925875f"},"cell_type":"code","source":"# What about the histogram of the timestamps when the distress event occurs?\ndistress_companies.hist(column=['time'], bins=14)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8f815d5c69c1b439ee9acaea2f2654237be3da8"},"cell_type":"markdown","source":"Interesting...the frequency of distress definitely does not seem to be uniform across the time periods. \n\nThat indicates that it may be bad science to obtain validation or test sets by simply picking out some companies, as we cannot assume that different companies are independent. The timestamp itself may be a useful signal (i.e. if a certain time period represents a macroeconomic state of decline for a certain industry, or the economy as a whole). Ok then, onto...","execution_count":null},{"metadata":{"_uuid":"57df802fe7aebc28001075275bceffcce42f966c"},"cell_type":"markdown","source":"# ...roll-forward cross validation\n\nWe'll now output a new set of features per training row: sum over each feature during time t, t-1, t-2...t-n. Note that this differs from the average by a constant, so while these features may represent something that shouldn't be summed (like average \"Google maps rating\" - I don't know), it'll just get normalized out later.\n\nThe training targets will be whether or not a distress event occured at the end of the period (t).","execution_count":null},{"metadata":{"trusted":true,"_uuid":"3b19689cfde9028ff9d8358c09b6b5f969c19838"},"cell_type":"code","source":"# Generate new train/val/test sets.\n\n# Populate the entire pandas array into a dict for easier processing\n\ndatadict = {}\ndistress_dict = {}\n\nfor i in range (1, total_n+1):\n    datadict[i] = {}\n    distress_dict[i] = {}\n\nprint(\"Populating dictionary...\")\nfor idx, row in companies.iterrows():\n    company = row['company']\n    time = int(row['time'])\n    \n    datadict[company][time] = {}\n    \n    if row['financial_distress'] < -0.5:\n        distress_dict[company][time] = 1\n    else:\n        distress_dict[company][time] = 0\n        \n    for feat_idx, column in enumerate(row[3:]):\n        feat = feature_names[feat_idx]\n        datadict[company][time][feat] = column\n        \n# print('Dict population complete. Sample below:')\n# print(\"\\nData for company 1, time 1:\")\n# print(datadict[1][1])\n\n# print(\"\\nDistress history for company 1:\")\n# print(distress_dict[1])\n\nprint('We can encode categorical feature 80 as a one-hot vector with this many dimensions:')\nprint(len(list(set(f80))))\n\nlabel_binarizer = LabelBinarizer()\nlabel_binarizer.fit(range(max(f80)))\nf80_oh = label_binarizer.transform(f80)\n\n# print(f80_oh[0:5])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f70c302ac44f5eed766b3208e03c3187e062c15b"},"cell_type":"markdown","source":"# Data generation","execution_count":null},{"metadata":{"trusted":true,"_uuid":"cf0e9662a8b4d913b36e1db107a7b9f1f1cf5f92"},"cell_type":"code","source":"# Make new features as np array. We'll even add x80 back!\n\ndef rolling_operation(time, train_array, datadict, distress_dict, feature_names, total_n,\n                         lookback_periods):\n\n    for company in range(1, total_n+1):\n            \n            all_periods_exist = True\n            for j in range(0, lookback_periods):\n                if not time-j in distress_dict[company]:\n                    all_periods_exist = False\n            if not all_periods_exist:\n                continue\n            \n            distress_at_eop = distress_dict[company][time]\n            new_row = [company]\n\n            for feature in feature_names:\n                if feature == 'x80':\n                    continue\n                feat_sum = 0.0\n                variance_arr = []\n                for j in range(0, lookback_periods):\n                    feat_sum += datadict[company][time-j][feature]\n                    variance_arr.append(datadict[company][time-j][feature])\n                new_row.append(feat_sum)\n                new_row.append(np.var(variance_arr))\n                \n            for j in range(0,len(f80_oh[0])):\n                new_row.append(f80_oh[company-1][j])\n\n            if len(new_row) == ((len(feature_names)-1)*2 + 1 + len(f80_oh[0])) : # we have a complete row\n                new_row.append(distress_at_eop)\n                new_row_np = np.asarray(new_row)\n                train_array.append(new_row_np)\n    \n\ndef custom_timeseries_cv(datadict, distress_dict, feature_names, total_n, val_time, test_time, \n                         lookback_periods, total_periods=14):\n\n    # Train data\n    train_array = []\n    for _t in range(1, val_time+1):\n        time = (val_time+1) -_t # Start from time period 10 and work backwards\n        train_array_np = rolling_operation(time, train_array, datadict, distress_dict, feature_names, total_n,\n                         lookback_periods)\n\n    train_array_np = np.asarray(train_array)\n    print(train_array_np.shape)\n    # print(train_array_np[0])\n    \n    # Val data\n    if val_time != test_time:\n        val_array = []\n        for time in range(val_time+1, test_time+1):\n            val_array_np = rolling_operation(time, val_array, datadict, distress_dict, feature_names, total_n,\n                         lookback_periods)\n\n        val_array_np = np.asarray(val_array)\n        print(val_array_np.shape)\n        # print(val_array_np[0])\n    else:\n        val_array_np = None\n\n    # Test data\n    test_array = []\n    # start from time period 11 and work forwards\n    for time in range(test_time+1,total_periods+1):\n        test_array_np = rolling_operation(time, test_array, datadict, distress_dict, feature_names, total_n,\n                         lookback_periods)\n\n    test_array_np = np.asarray(test_array)\n    print(test_array_np.shape)\n    # print(test_array_np[0])\n    \n    return train_array_np, val_array_np, test_array_np\n\n# Generate our sets\ntrain_array_np, val_array_np, test_array_np = custom_timeseries_cv(datadict, distress_dict, feature_names, total_n,\n                                                     val_time=9, test_time=12, lookback_periods=3, total_periods=14)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb8aa8f9181a740bd5ef727051e821e8669e1e9c"},"cell_type":"markdown","source":"# Pull out last column as labels\n\n","execution_count":null},{"metadata":{"trusted":true,"_uuid":"e1f9cb1890dbccd8356245babfbcc0065c5d38b6"},"cell_type":"code","source":"X_train = train_array_np[:,0:train_array_np.shape[1]-1]\ny_train = train_array_np[:,-1].astype(int)\n\nX_val = val_array_np[:,0:val_array_np.shape[1]-1]\ny_val = val_array_np[:,-1].astype(int)\n\nX_test = test_array_np[:,0:test_array_np.shape[1]-1]\ny_test = test_array_np[:,-1].astype(int)\n\nnp.set_printoptions(threshold=sys.maxsize)\nprint(X_train[0,:])\nprint(y_train)\n\nprint(X_val[0,:])\nprint(y_val)\n\nprint(X_test[0,:])\nprint(y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f6628e3c178481f24454bd8e4141c640a0bde73"},"cell_type":"markdown","source":"# Now try some models! Just the super basic, intro on Udacity stuff. :)","execution_count":null},{"metadata":{"trusted":true,"_uuid":"941348becb664b6c178f9640b2be9b0f6c234d1e"},"cell_type":"code","source":"# Try a couple of different basic classification models\n\nfrom sklearn.linear_model import Ridge, LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef model_trial(model_type, hyperparam):\n    if model_type in ['logistic-regression']:\n        # Logistic Regression. Try 11, l2 penalty, understand one-vs-rest vs multinomial (cross-entropy) \n        model = LogisticRegression(penalty=hyperparam, solver='saga', max_iter=4000)\n    elif model_type in ['decision-tree']:\n        model = DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None)\n    elif model_type in ['random-forest']:\n        model = RandomForestClassifier(n_estimators=hyperparam)\n    else:\n        print(\"Warning: model {} not recognized.\".format(model_type))\n        \n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_val)\n\n    f1 = f1_score(y_val, y_pred)\n    recall = recall_score(y_val, y_pred)\n    print(\"Mean acc: %f\" % model.score(X_val, y_val))\n    print(\"F1: %f\" % f1)\n    print(\"Recall: %f\" % recall)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"5e9cb88673cbeee8f6291fd97d8d48be178ae9f8"},"cell_type":"code","source":"print(\"-\"*20 + \"Logistic regression, l1:\" + \"-\"*20)\nmodel_trial('logistic-regression', 'l1')\n\nprint(\"-\"*20 + \"Logistic regression, l2:\" + \"-\"*20)\nmodel_trial('logistic-regression', 'l2')\n\nprint(\"-\"*20 + \"Decision tree:\" + \"-\"*20)\nmodel_trial('decision-tree', None)\n\nfor i in [2, 4, 10, 50, 100, 1000]:\n    print(\"-\"*20 + \"Random forest, {} estimators:\".format(i) + \"-\"*20)\n    model_trial('random-forest', i)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import linear_model\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import linear_model\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.cluster import KMeans\nfrom lightgbm import LGBMClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=15)\nclf = knn.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nacc_knb_model=roc_auc_score(y_test, y_pred)*100\nacc_knb_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(C = 0.2)\nclf1 = lr.fit(X_train, y_train)\ny_pred1 = clf1.predict(X_test)\nacc_log_reg=roc_auc_score(y_test, y_pred1)*100\nacc_log_reg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf2 = GaussianNB().fit(X_train, y_train)\ny_pred2 = clf2.predict(X_test)\nacc_nb=roc_auc_score(y_test, y_pred2)*100\nacc_nb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf3 = tree.DecisionTreeClassifier().fit(X_train, y_train)\ny_pred3 = clf3.predict(X_test)\nacc_dt=roc_auc_score(y_test, y_pred3)*100\nacc_dt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf4 = RandomForestClassifier(max_depth=5, random_state=0).fit(X_train, y_train)\ny_pred4 = clf4.predict(X_test)\nacc_rmf_model=roc_auc_score(y_test, y_pred4)*100\nacc_rmf_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf5 = SVC(gamma='auto').fit(X_train, y_train)\ny_pred5 = clf5.predict(X_test)\nacc_svm_model=roc_auc_score(y_test, y_pred5)*100\nacc_svm_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd_model=SGDClassifier()\nsgd_model.fit(X_train,y_train)\nsgd_pred=sgd_model.predict(X_test)\nacc_sgd=round(sgd_model.score(X_train,y_train)*100,10)\nacc_sgd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model=XGBClassifier()\nxgb_model.fit(X_train,y_train)\nxgb_pred=xgb_model.predict(X_test)\nacc_xgb=round(xgb_model.score(X_train,y_train)*100,10)\nacc_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm = LGBMClassifier()\nlgbm.fit(X_train,y_train)\nlgbm_pred=lgbm.predict(X_test)\nacc_lgbm=round(lgbm.score(X_train,y_train)*100,10)\nacc_lgbm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regr = linear_model.LinearRegression()\nregr.fit(X_train,y_train)\nregr_pred=regr.predict(X_test)\nacc_regr=round(regr.score(X_train,y_train)*100,10)\nacc_regr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest','Stochastic Gradient Decent','Linear Regression','Naive Bayes','XGBoost','LightGBM','Decision Tree'],\n    'Score': [acc_svm_model, acc_knb_model, acc_log_reg, \n              acc_rmf_model,acc_sgd,acc_regr,acc_nb,acc_xgb,acc_lgbm,acc_dt]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}