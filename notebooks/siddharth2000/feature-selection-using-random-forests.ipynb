{"cells":[{"metadata":{},"cell_type":"markdown","source":"[Help taken from](https://www.kaggle.com/kanncaa1/feature-selection-and-data-visualization)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = ['id', 'Unnamed: 32']\ndf = df.drop(cols_to_drop, axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### First hand observations:\n\n1. The data needs to be standardized or normalised based on our visualizations.\n2. Some of features have very high values as compared to other columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['diagnosis'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's clearly visible from the heatmap that many of the features are highly correlated. We would have to remove these features from our model. We also need to conduct further data visualizations to check for anomalies in our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_coeffs = df.corr()\ncorrelation_stack = correlation_coeffs.unstack()\ncorrelation_stack_sorted = correlation_stack.sort_values(kind=\"quicksort\", ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_stack_sorted[-50:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Boxplots"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['diagnosis']\ndf.drop('diagnosis', axis=1, inplace=True)\ndf_std = (df - df.mean())/df.std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_std.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_data(data, y, plot_type):\n    data = pd.concat([y, data], axis=1)\n    data = pd.melt(data,id_vars=\"diagnosis\",\n                   var_name=\"features\",\n                   value_name='value')\n    plt.figure(figsize=(10,10))\n    if plot_type=='violin':\n        sns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\n    elif plot_type=='box':\n        sns.boxplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\n    elif plot_type=='swarm':\n        sns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\n    plt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data(df_std.iloc[:, 0:10], y, \"violin\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data(df_std.iloc[:, 11:20], y, \"violin\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data(df_std.iloc[:, 21:30], y, \"violin\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observations:\n\n1. There are many features which have different distributions(median) for different diagnosis type ('malignant' or 'benign')\n2. Features like 'radius_mean', 'concavity_mean' have considerably different medians for the different types, thus they will be good for classification.\n3. Features like 'symmetry_mean', 'fractal_dimension_mean', 'texture_se' have the same median thus they won't add much meaning to the classification task."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data(df_std.iloc[:, 0:10], y, 'box')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data(df_std.iloc[:, 11:20], y, 'box')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data(df_std.iloc[:, 0:10], y, 'swarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data(df_std.iloc[:, 10:20], y, 'swarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data(df_std.iloc[:, 21:30], y, 'swarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the above swarm plots we can have the following conclusions:\n1. 'perimeter_mean', 'area_mean', 'concavity_mean' does the perfect job in separating the data.\n2. In the 2nd plot none of the features do a good job in separating the different classes.\n3. In the 3rd plot, 'area_worst', 'perimeter_worst', 'concavity_worst' are also able to separate the data.\n\n#### We now have to run algorithms to select features best for classification."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,15))\nsns.heatmap(df.corr(), cmap='Blues', annot=True,linewidths=.5, fmt= '.1f')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Manually selecting features from observations made above:\n\n1. The features: radius_mean, perimeter_mean, area_mean are highly correlated, so we will choose one feature from among them -> 'perimeter_mean'.\n2. The features: radius_worst, perimeter_worst, area_worst are highly correlated, so we will choose one feature from among them -> 'area_worst'.\n3. 'compactness_mean', 'concavity_mean', 'concave points_mean' are also correlated, we choose -> 'concave points_mean'\n4. 'radius_se', 'perimeter_se', 'area_se' are also correlated, we choose -> 'radius_se'\n5. 'compactness_worst', 'concavity_worst', 'concave points_worst' are also correlated, we choose -> 'concavity_worst'\n6. 'texture_mean' and 'texture_worst' are also correlated, we choose -> 'texture_mean'"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = ['area_mean','radius_mean','compactness_mean','concavity_mean',\n                'area_se','perimeter_se','perimeter_worst', \n                'compactness_worst','concave points_worst','compactness_se',\n                'concave points_se','texture_worst','radius_worst']\n\ndata = df.drop(cols_to_drop, axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.heatmap(data.corr(), cmap='Blues', annot=True,linewidths=.5, fmt= '.1f')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(data, y, test_size=0.3, random_state=42)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf = RandomForestClassifier(random_state=43)      \nclr_rf = clf_rf.fit(x_train,y_train)\n\nac = accuracy_score(y_test,clf_rf.predict(x_test))\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(y_test,clf_rf.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### A simple model with the manually chosen features gave us 97% accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,clf_rf.predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = pd.DataFrame()\nfeatures['Feature'] = x_train.columns\nfeatures['Importance'] = clf_rf.feature_importances_\nfeatures.sort_values(by=['Importance'], ascending=False, inplace=True)\nfeatures.set_index('Feature', inplace=True)\nfeatures.plot(kind='bar', figsize=(20, 10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we shall do the same process but with using all the dataframe features and using a automatic feature selection algorithm"},{"metadata":{},"cell_type":"markdown","source":"## 1. Recursive Feature Elimination with CV\n\nThe Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain. The model uses CV for finding the  optimal number of features and the important features.\n\nIt uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFECV\n\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nrf_clf2 = RandomForestClassifier() \n\nrfecv = RFECV(estimator=rf_clf2, step=1, cv=3, scoring='accuracy')   #5-fold cross-validation\nrfecv = rfecv.fit(data, y)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', x_train.columns[rfecv.support_])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On running a Feature Selection algorithm on the entire dataset we see that we have narrowed down to 14 important features for classification."},{"metadata":{},"cell_type":"markdown","source":"## XG-Boost"},{"metadata":{},"cell_type":"markdown","source":"### Feature Importance in Gradient Boosting\n\nA benefit of using gradient boosting is that after the boosted trees are constructed, it is relatively straightforward to retrieve importance scores for each attribute.\n\nGenerally, importance provides a score that indicates how useful or valuable each feature was in the construction of the boosted decision trees within the model. The more an attribute is used to make key decisions with decision trees, the higher its relative importance."},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import sort\nfrom xgboost import XGBClassifier\nfrom sklearn.feature_selection import SelectFromModel","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### First fit on the entire dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = y.map({'B':0, 'M':1}).astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.25, random_state=7, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model on all training data\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\n# make predictions for test data and evaluate\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n# Fit model using each importance as a threshold\nthresholds = sort(model.feature_importances_)\nfor thresh in thresholds:\n\t# select features using threshold\n\tselection = SelectFromModel(model, threshold=thresh, prefit=True)\n\tselect_X_train = selection.transform(X_train)\n\t# train model\n\tselection_model = XGBClassifier()\n\tselection_model.fit(select_X_train, y_train)\n\t# eval model\n\tselect_X_test = selection.transform(X_test)\n\ty_pred = selection_model.predict(select_X_test)\n\tpredictions = [np.round(value) for value in y_pred]\n\taccuracy = accuracy_score(y_test, predictions)\n\tprint(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see that using a naive xgb classifier and with just 7 features we can achieve around 93% accuracy on the test set. This accuracy can also be increased further using hyperarameter tuning."},{"metadata":{"trusted":true},"cell_type":"code","source":"# How to get back feature_importances_ (gain based) from plot_importance fscore\n# Calculate two types of feature importance:\n# Weight = number of times a feature appears in tree\n# Gain = average gain of splits which use the feature = average all the gain values of the feature if it appears multiple times\n# Normalized gain = Proportion of average gain out of total average gain\n\nk = model.get_booster().trees_to_dataframe()\ngroup = k[k['Feature']!='Leaf'].groupby('Feature').agg(fscore = ('Gain', 'count'),\nfeature_importance_gain = ('Gain', 'mean'))\n\n# Feature importance same as plot_importance(importance_type = ‘weight’), default value\ngroup['fscore'].sort_values(ascending=False)\n# Feature importance same as clf.feature_importance_ default = ‘gain’\ngroup['feature_importance_gain_norm'] = group['feature_importance_gain']/group['feature_importance_gain'].sum()\ngroup.sort_values(by='feature_importance_gain_norm', ascending=False)\nprint('3')\n# Feature importance same as plot_importance(importance_type = ‘gain’)\ngroup[['feature_importance_gain']].sort_values(by='feature_importance_gain', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Link to above comment - [CODE LINK](https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/#comment-540697)"},{"metadata":{},"cell_type":"markdown","source":"Please upvote and leave a comment if the notebook was helpful. Cheers"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}