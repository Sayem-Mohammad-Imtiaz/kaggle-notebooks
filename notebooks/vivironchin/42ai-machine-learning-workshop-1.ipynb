{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Load the Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/california-housing-prices/housing.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['ocean_proximity'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\n\n#load file from image into a numpy.array (read pngs preferably)\ncalifornia_img = mpimg.imread('/kaggle/input/california-housing-feature-engineering/california.png')\ndata.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4, s=data['population']/100, label='population', figsize=(10,7), c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True)\n#kind: scatter = nuage de points\n# alpha: transparence\n# figsize: width, height in inches\n# edgecolors='blue'\n# linewidth=2\n# cmap: A Colormap instance or registered colormap name. cmap is only used if c is an array of floats. If None, defaults to rc image.cmap\nplt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Histograms ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = data.hist(bins=50, figsize=(20,15))\n#bins: In a histogram, the total range of data set (i.e from minimum value to maximum value) is divided into 8 to 15 equal parts. \n#These equal parts are known as bins or class intervals.\n#figsize: figure size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['median_house_value'] >= 500001].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Capped variables ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Delete variables whose median_house_value >= 500001\n\nReason: this data is not meaningful because the rest of houses are at the same point 50 0001","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_capped = data[data['median_house_value'] < 500001]\ndata_capped.shape\n# version sans nouveau dataframe\n#index = data[(data['median_house_value'] < 500001)].index\n# data.drop(index, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vérification qu'on a bien supprimé les capped values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_capped[data_capped['median_house_value'] >= 500001].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Summarizing missing values\n1. Quelles catégories ont le plus de missing values\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean_data = data.dropna()\n# missing_values = [\"n/a\", \"na\", \" \", \"\"]\n# df = pd.read_csv('/kaggle/input/california-housing-prices/housing.csv', na_values=missing_values)\n# df.isnull().sum()\n\ndata_capped.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- on a juste une catégorie 'total_bedrooms' qui possède des missing values: \n    - peut être à cause des missing values qui ne sont pas reconnues dans les autres catégories\n- quelle proportion représentent les missing values sur le total de rows ? réponse 1% -> pas très significatif surtout que ce sont 1% répartis sur plusieurs catégories","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#total le .sum() donne par colomnes, le .sum().sum() donne la somme des colonnes\ndata_capped.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(data_capped.isnull().sum().sum() / 20640, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Mapper les missing values\nOn fait un dataframe qu'avec les 200 missing values et on les map en fonction de plusieurs index pour voir s'ils sont well-spread sur la totalité des valeurs possibles et pas seulement à un endroit précis\nproblème: si les missing values sont toutes au meme endroit géographiquement, alors on perd en fiabilité pour la zone considérée.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_capped.isna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns; sns.set()\n# %matplolib inlineb\ndata_missing = data_capped[data_capped.total_bedrooms.isnull()]\ndata_missing.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nheatmap_missing_data1 = pd.pivot_table(data_missing, values='median_house_value', index=['longitude'], columns='latitude')\nsns.heatmap(heatmap_missing_data1, cmap=\"YlGnBu\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heatmap_missing_data2 = pd.pivot_table(data_missing, values='housing_median_age', index=['longitude'], columns='latitude')\nsns.heatmap(heatmap_missing_data2, cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pas de cluster donc ça devrait être bon.\n\nAprès le total_bedroom, c'est pas un facteur si décisif a priori.\n\nOn peut drop les na","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_capped.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclean_data = data_capped.dropna()\n# on peut utiliser subset['column'] pour ne considérer que certaines catégories\n# how= any ou all si on veut delete tous les rows (axis=0, par défaut)\n# inplace=True pour ne pas créer une copie mais changer la variable considérée directement\n#ici on sait que seul total_bedroom a des na donc inutile de préciser le subset et le how.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical variable: ocean proximity (encode it)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_data['ocean_proximity'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<1H OCEAN\nINLAND       \nNEAR OCEAN   \nNEAR BAY     \nISLAND\n\n- those are nominal categorical vraiables: not ordinal because there is no logical relationship between them or order.\nex: if INLAND = 1, NEAR OCEAN =2 and NEAR BAY =3\nthe model will assume that INLAND < NEAR OCEAN < NEAR BAY, or that  INLAND + NEAR OCEAN = NEAR BAY. does not apply.\n- One hot encoding: create a new column for each categories and put 1 or 0 in it, wether it applies to the category or not.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_data.ocean_proximity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummies = pd.get_dummies(clean_data.ocean_proximity)\ndummies.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"concatenate the two dataframes : clean_data with dummies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"merged = pd.concat([clean_data, dummies], axis='columns')\nmerged","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"+ Drop the column demi-variable trap in order not to have multicollinearity betweeen OCEAN_Proximity and the dummies columns.\nIt could mess up the regression because regression requires with indepedent variables only.\n\n+ need to drop one of the demi-variable among the dummie columns, here we choose ISLAND\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final = merged.drop(['ocean_proximity', 'ISLAND'], axis='columns')\nfinal","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Polynomial features\n- Divide 'total_rooms', 'total_bedrooms' by 'households'\n- Find more!! (look at the kernels..)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final['rooms_p_household'] = final['total_rooms'] / final['households']\nfinal.drop(['total_rooms'], axis='columns', inplace=True)\nfinal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final['bedrooms_p_household'] = final['total_bedrooms'] / final['households']\nfinal.drop(['total_bedrooms'], axis='columns', inplace=True)\nfinal","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Splitting ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"On répartit le dataframe en training et validation data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nhousing_X = final.drop(\"median_house_value\",axis=1)#reste un dataframe\nhousing_y = final['median_house_value']#transformation en series\n\nX_train, X_test, y_train, y_test = train_test_split(housing_X, housing_y, test_size=0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(housing_X))\nprint(housing_X.columns)\nprint()\nprint(type(housing_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Normalization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\n# scaling = MinMaxScaler()\nscaling = StandardScaler()\n# scaled_data = scaling.fit_transform(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = scaling.fit_transform(X_train)\nX_test= scaling.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train)\nprint(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"X_train shape {} and size {} and type {}\".format(X_train.shape,X_train.size, type(X_train)))\nprint(\"X_test shape {} and size {} and type {}\".format(X_test.shape,X_test.size, type(X_test)))\nprint(\"y_train shape {} and size {} and type {}\".format(y_train.shape,y_train.size, type(y_train)))\nprint(\"y_test shape {} and size {} and type {}\".format(y_test.shape,y_test.size, type(y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training a Model: Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = model.fit(X_train, y_train)\nprint(\"Intercept is \"+str(model.intercept_))\nprint(\"coefficients  is \"+str(model.coef_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predictions = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_test[0:5])\nprint(\"y_test shape {} and size {} and type {}\".format(y_test.shape,y_test.size, type(y_test)))\nprint()\nprint(y_predictions[0:5])\nprint(\"y_predictions shape {} and size {} and type {}\".format(y_predictions.shape,y_predictions.size, type(y_predictions)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comparaison entre actual et predicted \n\n- création d'un dataframe qui est la somme d'une Series panda et d'un numpu.ndarray de dimension 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"comparaison = pd.DataFrame({'Predicted':y_predictions,'Actual':y_test})\n\n#reset l'index qui était hérité du pandas.serise\ncomparaison.reset_index(inplace=True)\nd\n#on enleve l'index pour pouvoir mieux comparer\ncomparaison = comparaison.drop(['index'], axis=1)\n\ncomparaison.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,20))\nplt.plot(comparaison[:30])\nplt.legend(['Predicted', 'Actual'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x='Predicted',y='Actual',data=comparaison[:500], kind='reg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(comparaison['Predicted'], color=\"r\")\nsns.distplot(comparaison['Actual'], color=\"b\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Scoring ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nprint(\"MSE= \", mean_squared_error(y_test, y_predictions))\nprint(\"RMSE= \", np.sqrt(mean_squared_error(y_test, y_predictions)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\n\nr2_score(y_test, y_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.score(X_test, y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}