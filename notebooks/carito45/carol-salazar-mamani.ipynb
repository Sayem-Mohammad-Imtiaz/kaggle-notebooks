{"cells":[{"metadata":{"_uuid":"27fb8eb5-bee1-45c9-8202-ef9aafcd5248","_cell_guid":"a5d12778-f097-4a2a-b25d-0ee877fcf986","trusted":true},"cell_type":"markdown","source":"<center>\n<img src=\"https://drive.google.com/uc?export=download&id=1nv5uGKO9BLD9Y19LnZZH35nnQghZsPdD\" />\n\n# Feature Engineering and Feature Selection\n\nPara empezar, vamos a revisar tres tareas similares pero diferentes: \n\n* **feature extraction** and **feature engineering**: Transformación de data(raw) en características adecuadas para modelado;\n* **feature transformation**: Transformación de data para mejorar la precisión de los algoritmos;\n* **feature selection**: Removiendo características innecesarias."},{"metadata":{"_uuid":"3f93f730-a58a-4a5a-885a-d4d0cf6510db","_cell_guid":"3e212596-21b5-4a8c-948e-e73883961155","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"453eafc0-79fa-4284-868e-7063c931e8ea","_cell_guid":"2572af3c-4814-4604-b8c0-94328cb4858a","trusted":true},"cell_type":"markdown","source":"## Temas\n\n1. [Feature Extraction](#1.-Feature-Extraction)\n - [Texts](#Texts)\n - [Geospatial data](#Geospatial-data)\n - [Date and time](#Date-and-time)\n - [Time series, web, etc.](#Time-series,-web,-etc.)"},{"metadata":{"_uuid":"3b0a65ec-6f00-48c8-9df3-6080319aa081","_cell_guid":"cb6ab799-7f47-444f-8c7d-bd16c41d1334","trusted":true},"cell_type":"markdown","source":"## 1. Feature Extraction\n\n\nEn la práctica, los datos rara vez se presentan en forma de matrices listas para usar. Es por eso que cada tarea comienza con la extracción de características. A veces, puede ser suficiente leer el archivo .csv y convertirlo en `numpy.array`, pero esta es una rara excepción. Veamos algunos de los tipos populares de datos de los que se pueden extraer características."},{"metadata":{"_uuid":"44e6b66a-2139-43cf-b1ca-a284169dfda4","_cell_guid":"62d0b9ca-d3b9-4054-b31f-992da4d28dee","trusted":true},"cell_type":"code","source":"import json\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"751108de-d9be-4a8a-84a3-b07d92386d33","_cell_guid":"48a801bb-130a-483f-b870-2bd42d7faad7","trusted":true},"cell_type":"markdown","source":"### Texts\n\nEl texto es un tipo de datos que puede venir en diferentes formatos, revisaremos los más populares.\n\nAntes de trabajar con texto, hay que tokenizarlo. La tokenización implica dividir el texto en unidades (tokens). Los tokens son sólo las palabras. Pero el dividir por palabras nos puede llevar a perder parte del significado-- \"Santa Bárbara\" es un token, no dos, pero \"rock'n'roll\" no debe dividirse en dos token. Hay tokenizadores listos para usar que tienen en cuenta las peculiaridades del lenguaje, pero también cometen errores, especialmente cuando trabajas con fuentes de texto específicas (periódicos, jerga, errores ortográficos, errores tipográficos).\n\nDespués de la tokenización, normalizamos los datos. Para el texto, se trata de la derivación y/o lematización; Estos son procesos similares utilizados para procesar diferentes formas de una palabra. Se puede leer sobre la diferencia entre ellos [aqui](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html).\nEntonces, ahora que hemos convertido el documento en una secuencia de palabras, podemos representarlo con vectores. El enfoque más fácil se llama Bag of Words: creamos un vector con la longitud del diccionario, calculamos el número de ocurrencias de cada palabra en el texto y colocamos ese número de ocurrencias en la posición apropiada en el vector. El proceso descrito parece más simple en el código:"},{"metadata":{"_uuid":"e5e8f511-e226-4f3d-88a5-3cd4db36ee2a","_cell_guid":"9806d74c-5f3c-41a4-8811-10c8f6220443","trusted":true},"cell_type":"code","source":"from functools import reduce \nimport numpy as np\n\n# definicion de corpus\ntexts = [['i', 'have', 'a', 'cat'], \n        ['he', 'have', 'a', 'dog'], \n        ['he', 'and', 'i', 'have', 'a', 'cat', 'and', 'a', 'dog']]\n\ndictionary = list(enumerate(set(list(reduce(lambda x, y: x + y, texts)))))\nprint(dictionary)\ndef vectorize(text): \n    vector = np.zeros(len(dictionary)) \n    for i, word in dictionary: \n        num = 0 \n        for w in text: \n            if w == word: \n                num += 1 \n        if num: \n            vector[i] = num \n    return vector\n\nfor t in texts: \n    print(vectorize(t))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06fc53ff-1a9e-4250-9996-5f5c3c34602e","_cell_guid":"87917974-f2c7-4f49-83bf-935feb0e6806","trusted":true},"cell_type":"markdown","source":"Esta es una ilustración del proceso:\n<img src=\"https://drive.google.com/uc?export=download&id=18dEqfTKT10i5_EJz4MLy_hywpQ_IusUJ\" />\n\nEsta es una implementación extremadamente ingenua. En la práctica, debe considerar palabras de parada, la longitud máxima del diccionario, estructuras de datos más eficientes (generalmente los datos de texto se convierten en un matrices esparsa), etc.\n\nCuando utilizamos algoritmos como Bag of Words, perdemos el orden de las palabras en el texto, lo que significa que los textos \"i have no cows\" y \"no, i have cows\" aparecerán idénticos después de la vectorización cuando, de hecho, tienen el significado opuesto. Para evitar este problema, podemos volver a visitar nuestro paso de tokenización y usar N-grams (la *secuencia* de N tokens consecutivos) en su lugar."},{"metadata":{"_uuid":"7937a75c-6dd1-4a23-a2ac-812a54166908","_cell_guid":"46a51d65-c3be-4c7c-a630-b049d670cf42","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer(ngram_range=(1,1))\nvect.fit_transform(['i have no cows','no, i have cows']).toarray()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3f0c808-3b51-45df-8494-4f9c5474fd6f","_cell_guid":"1d12b0db-e284-4bca-9c28-39ed84c142e5","trusted":true},"cell_type":"code","source":"vect.vocabulary_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9466acfa-6eb4-4178-86fa-216caafbfc6c","_cell_guid":"096b21b1-48d6-43c9-b34a-81cd4fe3f9e1","trusted":true},"cell_type":"code","source":"vect = CountVectorizer(ngram_range=(1,2))\nvect.fit_transform(['i have no cows','no, i have cows']).toarray()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d59cc68-6055-45f1-a441-c7e53c390594","_cell_guid":"95904344-f591-4d45-b684-389cd4f87aa0","trusted":true},"cell_type":"code","source":"vect.vocabulary_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e97ea0c-256a-4a56-8497-30c1ae17c941","_cell_guid":"4eb78e52-1291-49c9-84e4-b85455953a30","trusted":true},"cell_type":"markdown","source":"También tenga en cuenta que uno no tiene que usar sólo palabras. En algunos casos, es posible generar N-gram de caracteres. Este enfoque podría dar cuenta de la similitud de palabras relacionadas o manejar errores tipográficos."},{"metadata":{"_uuid":"fdc73d04-ed9e-4a4e-9351-b3e53c7a77e0","_cell_guid":"2174bc02-1f97-4847-8aa8-f0a65796d29f","trusted":true},"cell_type":"code","source":"from scipy.spatial.distance import euclidean\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer(ngram_range=(3,3), analyzer='char_wb')\n\nn1, n2, n3, n4 = vect.fit_transform(['andersen', 'petersen', 'petrov', 'smith']).toarray()\n\n\neuclidean(n1, n2), euclidean(n2, n3), euclidean(n3, n4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc4def84-cfef-47ca-83aa-d1382c4c12e6","_cell_guid":"5e50a1dd-9687-4fcb-84f5-1385d425ab8a","trusted":true},"cell_type":"markdown","source":"Agregando a la idea de Bag of Words: las palabras que rara vez se encuentran en el corpus (en todos los documentos del dataset) pero que están presentes en un documento en particular podrían ser más importantes. Entonces tiene sentido aumentar el peso de más palabras específicas del dominio para separarlas de las palabras comunes. Este enfoque se llama TF-IDF (term frequency-inverse document frequency), que no se puede escribir en unas pocas líneas, por lo que debe consultar los detalles en referencias como [wiki](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). La opción predeterminada es la siguiente:\n\n<img src=\"https://drive.google.com/uc?export=download&id=1zRnAL7xslzRl3odfsLa3yzbSR9SMw0CM\" />"},{"metadata":{"_uuid":"bcb27310-aacb-4cac-8f82-7b3ab099f032","_cell_guid":"a1788993-2762-4d34-a98c-f0619c831022","trusted":true},"cell_type":"markdown","source":"Usando estos algoritmos, es posible obtener una solución para un problema simple, que puede servir como línea base. Sin embargo, para aquellos a quienes no les gustan los clásicos, hay nuevos enfoques. Un método popular es Word2Vec, pero también hay algunas alternativas (GloVe, Fasttext, etc.).\n\nWord2Vec es un caso especial de los algoritmos word embedding. Usando Word2Vec y modelos similares, no sólo podemos vectorizar palabras en un espacio de alta dimensión (típicamente unos pocos cientos de dimensiones) sino también comparar su similitud semántica. Este es un ejemplo clásico de operaciones que se pueden realizar en conceptos vectorizados: **king - man + woman = queen.**\n\n![image](https://cdn-images-1.medium.com/max/800/1*K5X4N-MJKt8FGFtrTHwidg.gif)"},{"metadata":{"_uuid":"e95c9835-b855-4f20-89aa-982048bb12a5","_cell_guid":"655d9478-6c1f-4d59-a377-bedd7b0a0832","trusted":true},"cell_type":"markdown","source":"Vale la pena señalar que este modelo no comprende el significado de las palabras, simplemente trata de posicionar los vectores de manera que las palabras utilizadas en el contexto común estén cerca unas de otras.\n\nDichos modelos necesitan ser entrenados en data sets muy grandes para que las coordenadas del vector capturen la semántica. Se puede descargar un modelo previamente entrenado para sus propias tareas\n[aquí](https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-models)."},{"metadata":{"_uuid":"68a75bad-c240-4b5d-8a03-52cee8ae957f","_cell_guid":"407b2345-f797-42ee-8d78-fa31a9a07dfd","trusted":true},"cell_type":"markdown","source":"### **Ejercicio: Clasificación de Spam usando Support Vector Machines.**\n#### CONTEXTO:    SMS Spam Collection es un conjunto de mensajes etiquetados SMS que se han recopilado para la investigación de SMS Spam. Contiene un conjunto de mensajes SMS en inglés 5,574 mensajes etiquetados de acuerdo a su contenido 'ham' (legítimo) o 'spam'.\n\nAGRADECIMIENTO:  El dataset original se puede encontrar [aquí](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection). Los creadores desean tener en cuenta que en caso de que encuentre útil el conjunto de datos, haga referencia al documento anterior y la página web: http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/\n\nCONTENIDO: Los archivos contienen un mensaje por línea. Cada línea está compuesta por dos columnas: v1 contiene la etiqueta (ham o spam) y v2 contiene el texto sin formato. Este corpus se ha recopilado de forma gratuita."},{"metadata":{"_uuid":"7136d756-7c0b-470c-9674-72e265f89a25","_cell_guid":"203e379a-f6ce-4d8d-8df0-e78bf9df62d0","trusted":true},"cell_type":"markdown","source":"### **Librerias**"},{"metadata":{"_uuid":"ae18f8b6-a8bf-4d5d-afd4-30c94590c180","_cell_guid":"da83b6af-5971-487d-bff0-a163ec844c43","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn import feature_extraction, model_selection, naive_bayes, metrics, svm\nfrom IPython.display import Image\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5912f403-a313-4b7a-8ea7-13e0ab4a4d64","_cell_guid":"33c3f440-5d36-4980-9d30-29a697433d30","trusted":true},"cell_type":"markdown","source":"### **Explorando el Dataset**"},{"metadata":{"_uuid":"01826adf-3405-425a-b17a-9ee2f8c419de","_cell_guid":"3bb9a2ca-2b02-47da-9bf6-1a82b446d8e6","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/sms-spam-collection-dataset/spam.csv', encoding='latin-1')\ndata.head(n=10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cd3a806-e83a-4df6-a025-8a36f22a1f5a","_cell_guid":"92f11c9b-eda2-4c40-8568-ad09eeb92b54","trusted":true},"cell_type":"markdown","source":"### **Análisis de texto**\n1.  Graficar y encontrar la frecuencia de las palabras en los mensajes spam y no-spam(ham)\n2.  Describir cada gráfico"},{"metadata":{"_uuid":"ff881edd-0adb-419f-9c4d-2fea6573b04f","_cell_guid":"3467f223-bbe8-42b0-a6fd-06aff91a36b1","trusted":true},"cell_type":"code","source":"data= pd.read_csv('../input/sms-spam-collection-dataset/spam.csv',encoding='latin-1')\ndata = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\ndata = data.rename(columns={\"v1\":\"Tipo\", \"v2\":\"Letras\"})\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0b619f2-8876-4c80-bb43-ca9261a3897d","_cell_guid":"369e42b2-422e-48b1-b31d-54eb565e8397","trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count1=Counter(\" \".join(data[data['Tipo']=='ham'][\"Letras\"]).split()).most_common(30)\ndf1=pd.DataFrame.from_dict(count1)\nprint(df1.head())\ndf1 = df1.rename(columns={0:\"palabras non-spam\", 1 :\"count\"})\ncount2 = Counter(\" \".join(data[data['Tipo']=='spam'][\"Letras\"]).split()).most_common(30)\ndf2 = pd.DataFrame.from_dict(count2)\ndf2 = df2.rename(columns={0:\"palabras spam\", 1:\"count_\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.plot.bar(legend=\"False\")\ny_pos=np.arange(len(df1[\"palabras non-spam\"]))\nplt.xticks(y_pos,df1[\"palabras non-spam\"])\nplt.title('Palabras frecuentes en mensajes no-spam')\nplt.xlabel('Palabras')\nplt.ylabel('Numero')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Las palabras con mayor frecuencia que se presentan en los mensajes no spam  son :to,you,I,the,a,and,i,in,u,is,entre otras palabras que son minoria."},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.plot.bar(legend= False , color='red')\ny_pos=np.arange(len(df2[\"palabras spam\"]))\nplt.xticks(y_pos,df2[\"palabras spam\"])\nplt.title('Palabras frecuentes en mensajes spam')\nplt.xlabel('Palabras')\nplt.ylabel('Numero')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Las palabras mas frecuentes en los mensajes spam son:to,a,your,call,or,the,2,for,you,is,Call,entre otras en minoria.\n"},{"metadata":{"_uuid":"fc01137e-c7f7-4483-a934-52d59242f7dc","_cell_guid":"54232bf3-9ffb-4a1f-9e59-e554d2a03515","trusted":true},"cell_type":"code","source":"data['Tipo'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f9f94e8-bd92-49bb-9036-d8ba53f9660f","_cell_guid":"ee151262-323d-45af-8985-d85ecc4d3290","trusted":true},"cell_type":"code","source":"data['Tipo'].value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b606a52b-9e89-4acf-b7da-873f22914bc8","_cell_guid":"6ad4fbec-5ff2-4c13-85cb-dd261efd7582","trusted":true},"cell_type":"markdown","source":"De un total de 5572 mensajes ,4825 son mensajes valioso y 747 son mensajes de publicidad o sin importancia."},{"metadata":{"_uuid":"d7ff585a-448f-4d25-9f50-06c92c15a25d","_cell_guid":"f9dea1e3-4327-474f-8da7-8e56b7a33f60","trusted":true},"cell_type":"code","source":"data[\"Tipo\"].value_counts().plot(kind = 'pie', explode = [0, 0.1], figsize = (6, 6), autopct = '%1.1f%%', shadow = True)\nplt.ylabel(\"Spam vs Ham\")\nplt.legend([\"Ham\", \"Spam\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b56d0ce0-4aa1-4294-90e7-d43e3474dba4","_cell_guid":"d4a24956-44a6-4077-a031-e94c90ac4160","trusted":true},"cell_type":"markdown","source":"El 86.6% de los mensajes que llegan a la bandeja de entrada son mensajes con importancia y  un 13.4% son mensajes sin importancia o de propraganda."},{"metadata":{"_uuid":"89c880a1-b1ad-4a84-af52-bbcd9e13c9a7","_cell_guid":"155f6af4-e680-44f4-81b4-e41cf02c7774","trusted":true},"cell_type":"code","source":"data['Tipo'].replace('spam', 0, inplace = True)\ndata['Tipo'].replace('ham', 1, inplace = True)\n\n# checking the values of the labels now\ndata['Tipo'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c0bc40e-19c1-4f58-a43e-5eadf9131e7d","_cell_guid":"eb2b1015-e684-45b1-81fd-0eab1a7f06f6","trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\n\nwordcloud = WordCloud(background_color = 'gray', width = 1000, height = 1000, max_words = 50).generate(str(data['Letras']))\n\nplt.rcParams['figure.figsize'] = (10, 10)\nplt.title('Most Common words in the dataset', fontsize = 20)\nplt.axis('off')\nplt.imshow(wordcloud)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Las palabras mas populares que podemos encontrar en los mensajes que nos llegan al correo son:name,say,crazy,jurong,go,point,ok,availables, entre otras.\n    "},{"metadata":{"_uuid":"85724e94-74d1-4d06-8fbc-c14666fbcf46","_cell_guid":"a0f761a7-7fd0-45e6-94a8-9a49c1bffa32","trusted":true},"cell_type":"code","source":"spam = ' '.join(text for text in data['Letras'][data['Tipo'] == 0])\n\nwordcloud = WordCloud(background_color ='white', max_words = 50, height = 1000, width = 1000).generate(spam)\n\nplt.rcParams['figure.figsize'] = (10, 10)\nplt.axis('off')\nplt.title('Most Common Words in Spam Messages', fontsize = 20)\nplt.imshow(wordcloud)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Las palabras mas comunes o populares que podemos encontrar en los mensajes spam son:Free,text,now,call,now,txt,messge,150p,reply,entre otras."},{"metadata":{"_uuid":"bd66873e-c24f-494f-a005-785ee2e01a71","_cell_guid":"169ed0f0-87de-4822-bd34-e48e7ffbbd84","trusted":true},"cell_type":"code","source":"ham = ' '.join(text for text in data['Letras'][data['Tipo'] == 1])\n\nwordcloud = WordCloud(background_color = 'gray', max_words = 50, height = 1000, width = 1000).generate(ham)\n\nplt.rcParams['figure.figsize'] = (10, 10)\nplt.axis('off')\nplt.title('Most Common Words in Ham Messages', fontsize = 20)\nplt.imshow(wordcloud)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Las palabras mas comunes o populares que encontramos en los mensajes no spam son:will,It,gt,got,time,need,ok,now,entre otras."},{"metadata":{"_uuid":"f8288de3-5099-4f2f-a85d-b8dfef358400","_cell_guid":"881d6844-eec3-43ca-a704-035b1be55138","trusted":true},"cell_type":"markdown","source":"### **feature extraction and feature engineering **\n\n1. Preprocesamiento de texto \n2. Creación de tokens y el filtrado de palabras clave \n(puede usar un componente de alto nivel como: CountVectorizer que puede crear un diccionario de características y transformar documentos en vectores de características.)"},{"metadata":{"_uuid":"e984a62a-46b0-4398-86ea-972893a1c77a","_cell_guid":"9713010c-9932-4eeb-b045-80e72165fd38","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nwarnings.filterwarnings('ignore')\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba72ab4c-6d01-411b-9727-479c49bcf582","_cell_guid":"b9c6aeaa-32f8-452f-8145-ca95201b235e","trusted":true},"cell_type":"code","source":"feat= feature_extraction.text.CountVectorizer(stop_words = 'english',max_features=100)\nX= feat.fit_transform(data[\"Letras\"])\nnp.shape(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Tipo'].replace('spam', 0, inplace = True)\ndata['Tipo'].replace('ham', 1, inplace = True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = model_selection.train_test_split(X, data[\"Tipo\"], test_size=0.2, random_state=42)\nprint([np.shape(X_train),np.shape(X_test)])\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15988a06-0fc2-46f9-95f1-2d8b992a26a5","_cell_guid":"842234e2-ae84-4dd0-9789-203b4bad4085","trusted":true},"cell_type":"markdown","source":"### **Análisis Predictivo**\n\n1. El objetivo es predecir si un nuevo sms es spam o no-spam. usando SVM\n2. validar: Matriz de confusión"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21d77121-75b0-4178-957e-b19c8f19fb99","_cell_guid":"5d9ec9c4-5648-49d4-86c1-0bd1ff3c19ba","trusted":true},"cell_type":"code","source":"# usamos Support Vector Machine de :https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nstring.punctuation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstopwords.words(\"english\")[100:110]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9aa4a904-57fd-4ec7-996a-447eca1e8966","_cell_guid":"2d2aae49-221d-46cb-8f6b-c857cfdab253","trusted":true},"cell_type":"code","source":"svc = svm.SVC()\nsvc.fit(x_train, y_train)\nscore_train = svc.score(X_train, y_train)\nscore_test = svc.score(X_test, y_test)\nSVC()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"274a57fc-3be0-4608-ac46-109d2238e710","_cell_guid":"676f2b43-a43c-4f9e-a4f6-c63acfe20f25","trusted":true},"cell_type":"code","source":"# para validar debe usar una matriz de confusión usando el siguiente código:\nmatr_confusion_test = metrics.confusion_matrix(y_test, svc.predict(x_test))\npd.DataFrame(data = matr_confusion_test, columns = ['Prediccion spam', 'Prediccion no-spam'],\n            index = ['Real spam', 'Real no-spam'])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}