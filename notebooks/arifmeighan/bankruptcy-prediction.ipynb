{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Detecting Bankruptcy,\n\nA finacial problem that can hold major significance in the world of equities and trading. This sheet will be using information and other tools from other kernals in this dataset. The idea here is to hone my skill of the ***process and approach*** data analytics should take when given a set of data.\n\n## The Goal.\n\nWe will want to provide a method of predicting if a company is at risk of going bankrupt given a set of information. \nI have seen another kernal that states one approach to this is by not only making a model that can:\n* Detect a company going bankrupt\n* Classify the Parameters and variables that contribute the most to this bankruptcy, This can help detect the bankruptcy but also help provide an insight into the ways a company can \"fail\" so-to-speak.\n"},{"metadata":{},"cell_type":"markdown","source":"## Please note, I used [this](https://www.kaggle.com/marto24/bankruptcy-detection) notebook for inspiration, this work below can't be considered my own as I use fairly similar features/techniques used by that notebook, please sent credit to user [Marto93](https://www.kaggle.com/marto24)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\ninput_data = \"/kaggle/input/company-bankruptcy-prediction/data.csv\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(input_data)\n#Input data first","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()\n#Before we work with the data, we want to know more information about it.\n#Are any values missing? Are any catagorical?\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the total number of rows, columns and the amount of distinct datatypes. Here it is only floating points and integers. Therefore we dont need any one-hot encoders."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum().sum()\n# With 6819 values, we have 0 null points of data, 95 Columns of full information","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, I am a computer science graduate with not much background knowledge on how these companies work or how they function. I could lookup the individual 94 features and research how much i think they impact the probability of bankruptcy... or ask someone who does have knowledge to help me find out.\n\nI think a better way is attempting to analyse correlation between data and see if there are emerging patterns that we can take advantage of, In this case maybe even drop some labels that do not help us."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.hist(figsize=(20,20), bins = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below is a correlation heatmap on the values within the dataset, "},{"metadata":{"trusted":true},"cell_type":"code","source":"Correlation = data.corr()[['Bankrupt?']]\nplt.subplots(figsize=(30,30))\nsns.heatmap(Correlation,square=True,fmt=\"\",cmap=\"RdYlGn\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Correlation.info()\nCorrelation.sort_values(by=['Bankrupt?'],ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we can see, Debt ratio, Current liability to assests and the borrowing dependancy etc all have the higher end correlations for a company to go bankrupt.\n\n"},{"metadata":{},"cell_type":"markdown","source":"How many of these companies are bankrupt?\n1 is bankrupt and 0 isn't"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data['Bankrupt?'].value_counts())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = 'Not Bankrupt','Bankrupt'\nsizes = [data['Bankrupt?'].value_counts()[0],data['Bankrupt?'].value_counts()[1]]\n\nfig1,ax1 = plt.subplots()\nplt.figure(figsize=(20,20))\nax1.pie(sizes,labels = labels,autopct=\"%1.1f%%\",startangle=90)\nax1.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# [Imbalanced Classification Problems](https://machinelearningmastery.com/what-is-imbalanced-classification/)\n\nThe above pie chart introduces a severe problem with out analysis. The Dataset is obviously skewed with more **Non-bankrupt** companies than **bankrupt** ones. For binary classification our models need to have a better balance of data to have better performance rankings later on (essentially be better at classifying data). "},{"metadata":{},"cell_type":"markdown","source":">  \"An imbalance occurs when one or more classes have very low proportions in the training data as compared to the other classes.\""},{"metadata":{},"cell_type":"markdown","source":"This imbalance is shown with the minority class being the bankrupt companies and the majority class being the non-bankrupt companies. On a ratio of 220:6599 this shows that without adjustment the learning rate of models may suffer. \n\n## Why is it a problem though, Surely we can still distinguish the companies that are at risk to the ones that aren't?\n\nMany classification algorithms rely on assuming (naively) that the distribution of both classes is near equal. This means with a large majority of one class, the algorithm will learn the characteristics of only that class and nothing about the minority class that is more significant.\n\n> â€¦ the learning process of most classification algorithms is often biased toward the majority class examples, so that minority ones are not well modeled into the final system.\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}