{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport tensorflow as tf\nfrom tensorflow import feature_column\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using the [Classify structured data with feature columns](https://www.tensorflow.org/tutorials/structured_data/feature_columns) Tutorial for this example. Alot of the code was copied but as you can see, anything i dont understand I look up","metadata":{}},{"cell_type":"markdown","source":"**This is going to be a simple structured data classification with Tensorflow. This is my first time using tensorflow (but I already have pre-existing knowledge of neural networks) So please give me feedback in the comments so i can become more versed in this cool library/tool!**","metadata":{}},{"cell_type":"code","source":"HeartAttackFile = '../input/heart-attack-analysis-prediction-dataset/heart.csv'\nO2SaturationFile= '../input/heart-attack-analysis-prediction-dataset/o2Saturation.csv'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HeartAttackData = pd.read_csv(HeartAttackFile)\nO2SaturationData = pd.read_csv(O2SaturationFile)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA - Exploratory Data Analysis Stage\n\nHere we will look at the data and labels and inspect which parts would be more detremental/useful. We will also clean the data up and split it for cross-validation once clean.","metadata":{}},{"cell_type":"code","source":"HeartAttackData.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We check all of the data is present. We have only integer values aside from *oldpeak*","metadata":{}},{"cell_type":"code","source":"HeartAttackData.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HeartAttackData.hist(bins = 25, figsize=(20,20))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the label descriptions, the floating point value is vague to me. The oldpeak column just states \"the previous peak\" as its descrition, We will leave it out for now but test later on if it affects the performance. ","metadata":{}},{"cell_type":"code","source":"#HeartAttackData = HeartAttackData.drop(columns=['oldpeak'])\nCatColumns = ['sex','exng','caa','cp','fbs','restecg']\nNumColumns = ['age','trtbps','chol','thalachh']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = train_test_split(HeartAttackData, test_size=0.2)\ntrain, val = train_test_split(train, test_size=0.2)\nprint(len(train), 'train examples')\nprint(len(val), 'validation examples')\nprint(len(test), 'test examples')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> tf.data is an api that enables complex input pipelines from data. It is designed to handle large amounts of data from different formats and performing complex transformations\n\nIn this case we are just taking the columns of the table, and inputting it into the neural network as different features/inputs","metadata":{}},{"cell_type":"code","source":"# A utility method to create a tf.data dataset from a Pandas Dataframe\n#A tf dataset structure is produced, we can use this with the neural network. \ndef df_to_dataset(dataframe,shuffle=True,batch_size=32):\n    dataframe = dataframe.copy() #Dataframes need to be copied so original isnt affected via actions due to referencing\n    labels = dataframe.pop('output')\n    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe),labels))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(batch_size)\n    return ds\n    \n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We want to use that method above to turn out training, testing and validation **dataframes**** into **datasets****!","metadata":{}},{"cell_type":"code","source":"batch_size = 32 # A small batch sized is used for demonstration purposes\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_columns = []\nfor header in NumColumns:\n  feature_columns.append(feature_column.numeric_column(header))\n\nfor col_name in CatColumns:\n  categorical_column = feature_column.categorical_column_with_vocabulary_list(\n      col_name, HeartAttackData[col_name].unique())\n  indicator_column = feature_column.indicator_column(categorical_column)\n  feature_columns.append(indicator_column)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above code embeds all of the categorical/numerical data into their seperate feature columns. We do this as the categorical data should be represented with whole integers, we dont want a 0.5 in any of the data. I could use the Bucketized cols when doing the age column, but i will pursue that development later.","metadata":{}},{"cell_type":"markdown","source":"DenseFeatures is what converts the feature columns into a tensor. Like how simple perceptrons can be stated as just \"matrix multiplication\" more complex neural networks use tensors like such. ","metadata":{}},{"cell_type":"code","source":"feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will now Create, Compile and train the model:","metadata":{}},{"cell_type":"markdown","source":"The following code has 3 major operations\n* It creates the model, this takes a Keras.input object. You also specify the layers present in the neural network. The feature layer we just specified is the \"input layer, The Layers.Dense(128... is a regularly densely connected (all outs to all inps, essentially fot product). The activation string is the activation function. We dont want just a linear function so we specify relu which is **Rectified Linear Unit**. Finally we will Dropout to reduce the chance of overfitting (large weights overfit more, so dropout just disables a portion of MP neurons randomly). Finally the last Dense layer is the output where we take all the values through a linear function.\n\n* Model.compile() will configure the model for training. It takes the name of an optimizier, an objective function and a list of metrics to measure model performance with. \n\n* Model.fit() will then train the model for a number of epochs (which just amount of time spent/iterations). We validate the dataloss here also, ensuring that the metrics we wanted to measure by are kept.","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n  feature_layer,\n  layers.Dense(128, activation='relu'),\n  layers.Dense(128, activation='relu'),\n  layers.Dropout(.1),\n  layers.Dense(1)\n])\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nmodel.fit(train_ds,\n          validation_data=val_ds,\n          epochs=10)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = model.evaluate(test_ds)\nprint(\"Accuracy\", accuracy)\nmodel.save('HeartAttackClassifier')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, a model that is only 63 - 45% accurate is not good. I would like for this to be higher. Therefore I will perform some more refining of the Model later. For now though this was a good exercise for getting to grips with Tensorflow basics and the capabilities it has.","metadata":{}},{"cell_type":"code","source":"reloaded_model = tf.keras.models.load_model('HeartAttackClassifier')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}