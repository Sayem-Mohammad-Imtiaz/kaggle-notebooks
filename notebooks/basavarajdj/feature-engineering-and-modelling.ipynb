{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Prediction of benign or malignant cancer tumors"},{"metadata":{},"cell_type":"markdown","source":"Let's import required libraries first"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score, classification_report\nfrom sklearn.model_selection import train_test_split, KFold, learning_curve\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#setting max columns display to 35 for more readability\npd.options.display.max_columns=35","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#read data\ndata = pd.read_csv(\"/kaggle/input/breast-cancer-wisconsin-data/data.csv\", sep=',')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. #### Attribute information "},{"metadata":{},"cell_type":"markdown","source":"1) ID number\n2) Diagnosis (M = malignant, B = benign)\n3-32)\n\nTen real-valued features are computed for each cell nucleus:\n\n\ta) radius (mean of distances from center to points on the perimeter)\n\tb) texture (standard deviation of gray-scale values)\n\tc) perimeter\n\td) area\n\te) smoothness (local variation in radius lengths)\n\tf) compactness (perimeter^2 / area - 1.0)\n\tg) concavity (severity of concave portions of the contour)\n\th) concave points (number of concave portions of the contour)\n\ti) symmetry \n\tj) fractal dimension (\"coastline approximation\" - 1)\n\nOkay now that we have information about columns lets construct the columns list. \nwe will have 3 sets of real valued features - mean, std(standard error) and worst"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop('Unnamed: 32', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"data.loc[:, 'radius_mean':].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4>We have 32 columns in total</h4>\n<ul>\n    <li>All the columns have proper datatypes, no conversions needed. </li>\n    <li>There are no missing values which is awesome </li>\n    <li>Just using descibe on data to check few stats </li>\n</ul>"},{"metadata":{},"cell_type":"markdown","source":"Our target variable is **Diagnosis**, let's plot bar graph for each of the features and see how our target classes are distributed"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(3, 10, figsize=(15,6))\ni, j, jt = 0, 0, 0\nfor col in data.columns:\n    if col not in ['id', 'diagnosis']:\n        if j <= 9:\n            data[['diagnosis',col]].groupby('diagnosis').mean().plot.barh(ax=ax[i, j])\n            if j == 0 and i==0:\n                ax[i,j].set_ylabel(\"Mean\")\n            elif j == 0 and i ==1:\n                ax[i,j].set_ylabel(\"Standard Err\")\n            elif j == 0 and i==2:\n                ax[i,j].set_ylabel(\"Worst\")\n            else:\n                ax[i,j].set_ylabel(\"\")\n            \n            if i == 2:\n                ax[i,j].set_xlabel(col[:-6])\n            else:\n                ax[i,j].set_xlabel(\"\")\n            ax[i,j].legend(\"\")\n            if j == 9:\n                j = 0\n                i += 1\n            else:\n                j += 1\nf.suptitle(\"Class distribution\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(15,12))\nsns.heatmap(round(data.loc[:, 'radius_mean':].corr(),2), annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Heatmap is very helpful for understanding the correlation between variables.\n<ul>\n    <li>There is a strong positive correlation between mean and worst set of features</li>\n    <li>There is also strong correlation within mean features and std features</li>\n</ul>"},{"metadata":{"trusted":true},"cell_type":"code","source":"set1 = ['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean','area_mean', 'concave points_mean', 'radius_worst', 'texture_worst','perimeter_worst', 'area_worst', 'concave points_worst']\nset2 = ['diagnosis','radius_se', 'perimeter_se', 'area_se',]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data[set1], hue='diagnosis', corner=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"sns.pairplot(data[set2], hue='diagnosis', corner=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After looking at above plots we can easily identify the columns with high correlation,\nWill take out the features having correlation greater than 0.9\n<ul>\n    <li>For example **mean_radius** and **worst_radius** are positively correlated(0.97)</li>\n    <li>Now we can look at our **class distibution plot** and select one of mean_radius/worst_radius based on how equally the classes are distributed</li>\n    <li>Prepare the list of features to remove and take them out</li>\n</ul>"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_remove = ['radius_worst', 'texture_worst','perimeter_worst', 'area_worst', 'concave points_worst','perimeter_se','area_se','perimeter_mean', 'area_mean', 'concave points_mean']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = data.columns.tolist()\nfor c in cols_to_remove:\n    columns.remove(c)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Data Normalization</h3><br/>\nNormalizing the features before we go ahead with modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_mean = data[columns[2:]].mean()\ndata_std = data[columns[2:]].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_data = (data[columns[2:]] - data_mean) / data_std","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### More feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = norm_data.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_mask = corr_matrix[(corr_matrix >= 0.7) & (corr_matrix < 1)].isna().sum() < 20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_mask[cols_mask.values].index.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we start modelling, let's take look at our heatmap, there are still columns with high correlation. Above code selects features having correlation higher than 0.7, there are 12 such features, can try using PCA and reduce number of features."},{"metadata":{},"cell_type":"markdown","source":"### PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=5)\npca.fit(data[cols_mask[cols_mask.values].index.values])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### That's great first component itself explains 91% of variance in the features, let's take only 1st priciple component and move ahead.\n\n<ul>\n    <li>Eigenvalue and Eigenvectors are main ingredients for constructing principle components. </li>\n    <li>Eigenvalue and Eigenvectors helps to reduces linear operations by compressing related variables. </li>\n</ul>"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=1)\npca.fit(data[cols_mask[cols_mask.values].index.values])\nprint(pca.explained_variance_ratio_)\ncomponents = pca.transform(data[cols_mask[cols_mask.values].index.values])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Prepare target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = data['diagnosis'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict(enumerate(target.cat.categories))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = target.cat.codes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Remove those 12 features and include 1st principle component in dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in cols_mask[cols_mask.values].index.values:\n    columns.remove(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_data['PC1'] = components.reshape(len(components))\ncolumns.append('PC1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = norm_data[columns[2:]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Target variable has 2 classes, which means a binary classification.\nLet's start with classic go-to model for binary classfication - Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression()\nlogreg.fit(X[:400], y[:400])\npred = logreg.predict(X[400:])\nprint(accuracy_score(y[400:], pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y[400:], pred, labels=[0,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y[400:], pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Tried tree based models like Decision tree and Random Forest but it didn't perform well compared to Logistic Regression (may be because tree based models works well with categorical features)\nLet't try Support vector classification model, it works very well with continues variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = LinearSVC(random_state=0, fit_intercept=True)\nsvc.fit(X[:400], y[:400])\npred = svc.predict(X[400:])\nprint(accuracy_score(y[400:], pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y[400:], pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Similar performace, no improvement\n\nIn this perticular use case we cannot afford to have **recall less than 1**, that is we need to minimize **false negatives** for malignant cancer tumors (class 1).<br/>\n**Let's try fine tuning our logistic regression model.**\n<ul>\n    <li>LR Model predicted 2 false negative</li>\n    <li>This may be due to class imbalance</li>\n    <li>Let's try adding class weight to LR model</li>\n</ul>"},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts() #class counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = {0:1.0, 1:1.9}\nlogreg = LogisticRegression(class_weight=weights)\nlogreg.fit(X[:400], y[:400])\npred = logreg.predict(X[400:])\naccuracy_score(y[400:], pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y[400:], pred, labels=[0,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y[400:], pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **Awesome**, now there are no false negatives and we have 100% recall. Yes our precision score is reduced but this trade-off is important."},{"metadata":{},"cell_type":"markdown","source":"## Learning Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_examples, train_score, test_score = learning_curve(logreg, X, y, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(train_examples, train_score.mean(axis=1), marker='d', label=\"Train\")\nplt.plot(train_examples, test_score.mean(axis=1), marker='d', label=\"Test\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our learning shows that model is **good fit** as training/ testing scores are converging, if they are overlapped on the most of the datapoints then it will be a **overfit**, if they are far from each other then its **underfit**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}