{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Mushroom Classification-Is it poisonous or edible?\n\n## PinkDragon1000\n\n#### Date: 8/13/18\n\n![](https://img.webmd.com/dtmcms/live/webmd/consumer_assets/site_images/articles/health_tools/all_about_mushrooms_slideshow/493ss_thinkstock_rf_poisonous_mushroom.jpg)\n---\n\n"},{"metadata":{},"cell_type":"markdown","source":"#### Abstract\n"},{"metadata":{},"cell_type":"markdown","source":"For this project I used the Mushroom Classification dataset from Kaggle.  It was put on Kaggle by the UCI Machine Learning group.  I first started with regular data preparation techniques and went on to exploring the data.  The data is completely categorical and there are 8124 rows and 23 columns.  To prepare the data for use in machine learning models, I had to map these categorical values to numerical values using the labelEncoder function.  Then I split the dataset into test (25%) and train (75%).  The two main classes of mushrooms are poisonous and not poisonous (edible).  The purpose of these models are to predict that.  I used three types of models using scikit learn which are naive bayes, svm (secure vector machines), and logistic regression. The best model was svm and it gave 100% accuracy, the next best model was logistic regression with around 94% accuracy, and finally naive bayes with 81% accuracy.         "},{"metadata":{},"cell_type":"markdown","source":"### 1. Introduction"},{"metadata":{},"cell_type":"markdown","source":"I chose the Mushroom Classification dataset from Kaggle.  The dataset was put on Kaggle by UCI Machine Learning Repository which maintains around 351 datasets.  This dataset is one of those.  The sample set contains around 23 species of mushrooms from the gilled mushroom Agaricus and Lepiota Family.  The mushrooms are either identified as completely poisonous or completely edible.      "},{"metadata":{},"cell_type":"markdown","source":"#### Objectives \n* Preprocessing and exploratory data analysis steps such as: loading the data into the data frame, checking the shape (number of rows/columns), getting the head of data, checking for missing and duplicate values, etc.  \n* Splitting the dataset into test and train.  \n* Finding the best sklearn model that accurately predicts whether a mushroom is poisonous or edible using naive bayes, support vector machines, and logistic regression"},{"metadata":{},"cell_type":"markdown","source":"### 2. Problem Definition\n"},{"metadata":{},"cell_type":"markdown","source":"Poisonous mushrooms can often times confuse people into thinking they are not posionous due to their similar appearance to some non-poisonous mushroom types. Even though most mushrooms seem to be edible, mushroom poisoning can cause discomfort and even in some cases death.  Poisonous mushrooms are found to mostly cause gastrointestinal problems and in the worst case may cause respiratory or kidney failure.  The symptoms appear within twenty minutes to four hours of ingestion. The goal is to find the best model to predict whether a mushroom is poisionous or edible (poisonous or not poisonous)."},{"metadata":{},"cell_type":"markdown","source":"### 3. Data Sets"},{"metadata":{},"cell_type":"markdown","source":"The dataset can be found here: https://www.kaggle.com/uciml/mushroom-classification/home. The format of the data is in a CSV format and is in one single file called mushrooms.csv. To prepare the dataset for classification purposes it needs to be split for the test and train."},{"metadata":{},"cell_type":"markdown","source":"### 4. Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/mushrooms.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 8124 rows and 23 columns in this dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Showing first 5 rows of the dataset.  This dataset is comprised of completely categorical features."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the columns in dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no null values in this dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no duplicate values in this dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Exploration and Visualization\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shows how many unique values there are for each column."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['class'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The two classes of mushrooms are p (poisonous) and e (edible)."},{"metadata":{},"cell_type":"markdown","source":"The Kaggle website provides information on what the column data means:\n    \n**class**: p=poisonous,e=edible  \n**cap-shape**: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s                   \n**cap-surface**: fibrous=f,grooves=g,scaly=y,smooth=s                 \n**cap-color**: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y                   \n**bruises**: bruises=t,no=f                      \n**odor**: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s                         \n**gill-attachment**: attached=a, descending=d, free=f, notched=n              \n**gill-spacing**: close=c,crowded=w,distant=d                \n**gill-size**: broad=b,narrow=n                 \n**gill-color**: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y               \n**stalk-shape**: enlarging=e,tapering=t                  \n**stalk-root**: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?                  \n**stalk-surface-above-ring**: fibrous=f,scaly=y,silky=k,smooth=s    \n**stalk-surface-below-ring**: fibrous=f,scaly=y,silky=k,smooth=s     \n**stalk-color-above-ring**: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y        \n**stalk-color-below-ring**: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y       \n**veil-type**: partial=p,universal=u                    \n**veil-color**: brown=n,orange=o,white=w,yellow=y                \n**ring-number**: none=n,one=o,two=t                 \n**ring-type**: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z                    \n**spore-print-color**: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y            \n**population**: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y                   \n**habitat**: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d "},{"metadata":{},"cell_type":"markdown","source":"Let's look at the descriptive statistics for the data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the number of poisonous and edible mushrooms in this dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are more edible mushrooms than poisonous mushrooms 4,208 versus 3,916 in this dataset."},{"metadata":{},"cell_type":"markdown","source":"Visualizing number of poisonous and edible mushrooms:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nitems=pd.DataFrame(df['class'].value_counts())\nitems.plot(kind='bar', figsize=(4,6), width=0.3, color=[('#63d363', '#d36363')], legend=False)\nplt.title(\"Number of Edible and Poisonous Mushrooms in this Dataset\", fontsize=\"15\")\nplt.xlabel(\"Edible or Poisonous\", fontsize=\"12\")\nplt.ylabel(\"Number of Mushrooms\", fontsize=\"12\")\nplt.xticks(np.arange(2),(\"Edible\", \"Poisonous\"), rotation=0)\nplt.grid()   \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the cap-color distribution in this dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['cap-color'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizing number of each cap color:"},{"metadata":{"trusted":true},"cell_type":"code","source":"caps=pd.DataFrame(df['cap-color'].value_counts())\ncaps.plot(kind='bar', figsize=(8,8), width=0.8, color=[('#bf7050', '#A9A9A9', '#d36363', '#f3f6c3', '#DCDCDC', '#bfa850', '#f9d7f7', '#D2691E', '#63d363', '#7050bf')], legend=False)\nplt.xlabel(\"Cap Color\",fontsize=12)\nplt.ylabel('Number of Mushrooms',fontsize=12)\nplt.title('Mushroom Cap Color Types in the Dataset', fontsize=15)\nplt.xticks(np.arange(10),('Brown', 'Gray','Red','Yellow','White','Buff','Pink','Cinnamon', 'Green','Purple'))\nplt.grid()       \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the number of mushrooms there are for each cap shape in this dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['cap-shape'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizing number of each cap shape:"},{"metadata":{"trusted":true},"cell_type":"code","source":"capsh=pd.DataFrame(df['cap-shape'].value_counts())\ncapsh.plot(kind='bar', figsize=(8,8), width=0.5, color=[('#A9A9A9')], legend=False)\nplt.xlabel(\"Cap Shape\",fontsize=12)\nplt.ylabel('Number of Mushrooms',fontsize=12)\nplt.title('Mushroom Cap Types in the Dataset', fontsize=15)\nplt.xticks(np.arange(6),('Convex', 'Flat','Knobbed','Bell','Sunken','Conical'))\nplt.grid()       \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Modeling and Evaluation\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The sklearn naive bayes algorithm cannot directly operate on categorical features that are non-numeric so we will use sklearn's LabelEncoder to convert the categorical features to numeric values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encodes labels from 0 to n_classes-1\nlabelEncoder = preprocessing.LabelEncoder()\nfor col in df.columns:\n    df[col] = labelEncoder.fit_transform(df[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at how the LabelEncoder transformed our data."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the categorical variables are shown numerically. "},{"metadata":{},"cell_type":"markdown","source":"Seeing what labelEncoder did to the poisonous (p) and edible (e) labels.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Edible is set to 0 and Poisonous is now set to 1."},{"metadata":{},"cell_type":"markdown","source":"To prepare the model the dataset needs to be split into test and train.  The method train_test_split randomly splits the dataset into 75% train and 25% test."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 75% train, 25% test\ntrain, test = train_test_split(df, test_size = 0.25) \ny_train = train['class']\nX_train = train[[x for x in train.columns if 'class' not in x]]\ny_test = test['class']\nX_test = test[[x for x in test.columns if 'class' not in x]]\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# Vectorize the training and test data \nvec = TfidfVectorizer(sublinear_tf=True, max_df=0.5,stop_words='english')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Naive Bayes"},{"metadata":{},"cell_type":"markdown","source":"Scikit-Learn Implementation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n# Creating a MultinomialNB classifier and fit the model\ncl = MultinomialNB()\ncl.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have trained our model. Let us predict our labels using the test portion of the data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=cl.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's evaluate how well the model performs."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n\nprint(\"Accuracy score: \", accuracy_score(y_test, y_pred))\nprint(\"Recall score: \", recall_score(y_test, y_pred, average = 'weighted'))\nprint(\"Precision score: \", precision_score(y_test, y_pred, average = 'weighted'))\nprint(\"F1 score: \", f1_score(y_test, y_pred, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cross validation is a way to check for overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score\nkfold = model_selection.KFold(n_splits=10, random_state=7)\nscoring = 'accuracy'\nresults = model_selection.cross_val_score(cl, X_train, y_train, cv=kfold, scoring=scoring)\nprint(\"Cross validation average accuracy with 10-folds: %f\" % (results.mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given the cross validation average is close to the accuracy of the naive bayes model we can conclude that our model does not really overfit. "},{"metadata":{},"cell_type":"markdown","source":"Making a confusion matrix:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport itertools","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\nplt.figure()\nplot_confusion_matrix(cm, classes=['p','e'], title='Confusion matrix, without normalization')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The matrix below is the same but is normalized.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplot_confusion_matrix(cm, classes=['p','e'], normalize=True, title='Confusion matrix, with normalization')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Support Vector Machines (SVM)"},{"metadata":{},"cell_type":"markdown","source":"Scikit-Learn Implementation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = svm.SVC(gamma='auto')\nclf.fit(X_train, y_train) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's evaluate how well the model performs."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy score: \", accuracy_score(y_test, y_pred))\nprint(\"Recall score: \", recall_score(y_test, y_pred, average = 'weighted'))\nprint(\"Precision score: \", precision_score(y_test, y_pred, average = 'weighted'))\nprint(\"F1 score: \", f1_score(y_test, y_pred, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cross validation is a way to check for overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score\nkfold = model_selection.KFold(n_splits=10, random_state=7)\nscoring = 'accuracy'\nresults = model_selection.cross_val_score(clf, X_train, y_train, cv=kfold, scoring=scoring)\nprint(\"Cross validation average accuracy with 10-folds: %.3f\" % (results.mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given the cross validation average is close to the accuracy of the SVM model we can conclude that our model generalizes well and is not overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The two matrices below show that the SVM model performed with 100% accuracy.  There was nothing inaccurately predicted."},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\nplt.figure()\nplot_confusion_matrix(cm, classes=['p','e'], title='Confusion matrix, without normalization')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normalized matrix:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplot_confusion_matrix(cm, classes=['p','e'], normalize=True, title='Confusion matrix, with normalization')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"Scikit-Learn Implementation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model, datasets\nlogreg = linear_model.LogisticRegression(solver='lbfgs',max_iter=2000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have trained our model. Let us predict our labels using the test portion of the data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=logreg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's evaluate how well the logistic model performs."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n\nprint(\"Accuracy score: \", accuracy_score(y_test, y_pred))\nprint(\"Recall score: \", recall_score(y_test, y_pred, average = 'weighted'))\nprint(\"Precision score: \", precision_score(y_test, y_pred, average = 'weighted'))\nprint(\"F1 score: \", f1_score(y_test, y_pred, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cross validation:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score\nkfold = model_selection.KFold(n_splits=10, random_state=7)\nscoring = 'accuracy'\nresults = model_selection.cross_val_score(logreg, X_train, y_train, cv=kfold, scoring=scoring)\nprint(\"Cross validation average accuracy with 10-folds: %.3f\" % (results.mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the cross validation score is close to the logistic regression model score that means it is not overfitting the data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplot_confusion_matrix(cm, classes=['p','e'], title='Confusion matrix, without normalization')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Same matrix with normalization:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplot_confusion_matrix(cm, classes=['p','e'], normalize=True, title='Confusion matrix, with normalization')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. Conclusion\n"},{"metadata":{},"cell_type":"markdown","source":"Even though all models did well the best model out of naive bayes, SVM, and logistic regression is SVM.  SVM gave about 100% accuracy while logistic gave about 94% accuracy and naive bayes performed the worst which was around 81% accuracy.  The worst situation seems to be if it is predicted edible but it is actually poisonous.  The other situations are if it is predicted poisonous but is edible, poisonous and is poisonous, edible and is edible.     "},{"metadata":{},"cell_type":"markdown","source":"### 8. References"},{"metadata":{},"cell_type":"markdown","source":"https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\n\nhttps://www.namyco.org/mushroom_poisoning_syndromes.php\n\nhttp://scikit-learn.org/stable/index.html"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":1}