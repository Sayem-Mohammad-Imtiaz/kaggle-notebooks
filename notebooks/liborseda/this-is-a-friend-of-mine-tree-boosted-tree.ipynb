{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ***Telco customer churn predictions*** \nrecommended music for exploring this notebook: \n* https://www.youtube.com/watch?v=t3217H8JppI&ab_channel=AnAmericanComposer\n\nWas used while creating.\n\nCultural reference: \n* https://www.youtube.com/watch?v=z3Sj1mXrAoQ&ab_channel=MooCli\n\nWas used while procrastinating.\n\n* Some parts (EDA, hyperparameters tuning) are now commented out, to speed up the execution of notebook. Uncomment with selecting lines and pressing 'Ctrl' + '/'\n\n# To do:\n\n* explore evolutionary hyperparameters search, e.g. from https://github.com/rsteca/sklearn-deap or other that are reference in scikit guide https://scikit-learn.org/0.23//_downloads/scikit-learn-docs.pdf\n\n****"},{"metadata":{},"cell_type":"markdown","source":"# Import libraries, for starters"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport math\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import data and explore basic properties"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import data from kaggle store\ndf=pd.read_csv('/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nice resume table to describe the data\ndef resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n    summary['Fourth Value'] = df.loc[3].values\n    summary['Fifth Value'] = df.loc[4].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=10),4) \n\n    return summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# retyping TotalCharges to numeric\ndf['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TotalCharge vs Tenure x MonthlyCharges - Discounts effect\n* TotalCharge should equal MonthlyCharges x Tenure. If not, it is a sign of a given discount or price inrease, that the customer got. \n* That might be a big factor for churning, lets see further"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate differene between Totalcharge and Tenure*MonthlyCharges\ndf['TotalCharge_diff'] = (df['tenure'] * df['MonthlyCharges']) - df['TotalCharges']\ndf['TotalCharge_diff_abs'] = df['TotalCharge_diff'].abs()\n# leaving both as a possible good features, from logic of the thing, I suppose only TotalCharges_diff will be of any use","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot\nplt.figure(figsize=(14, 4))\nplt.title(\"KDE for {}\".format('TotalCharge_diff'))\nax0 = sns.histplot(df[df['Churn'] == 'No']['TotalCharge_diff'].dropna(), color = \"#22ff57\", label= 'Churn: No')\nax1 = sns.histplot(df[df['Churn'] == 'Yes']['TotalCharge_diff'].dropna(), color= \"#FF5722\", label= 'Churn: Yes')\nplt.legend(prop={'size': 12})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def kde_plot(feature):\n    plt.figure(figsize=(9, 4))\n    plt.title(\"KDE for {}\".format(feature))\n    ax0 = sns.kdeplot(data[data['Churn'] == 'No'][feature].dropna(), color = \"#22ff57\", label= 'Churn: No')\n    ax1 = sns.kdeplot(data[data['Churn'] == 'Yes'][feature].dropna(), color= \"#FF5722\", label= 'Churn: Yes')\n    plt.legend(prop={'size': 12})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#kde_plot('TotalCharge_diff')\n#kde_plot('TotalCharge_diff_abs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# borrowed fcn for plotting nice barplots\ndef barplot_percentages(feature, orient='v', axis_name=\"percentage of customers\"):\n    ratios = pd.DataFrame()\n    g = df.groupby(feature)[\"Churn\"].value_counts().to_frame()\n    g = g.rename({\"Churn\": axis_name}, axis=1).reset_index()\n    g[axis_name] = g[axis_name]/len(df)\n    if orient == 'v':\n        ax = sns.barplot(x=feature, y= axis_name, hue='Churn', data=g, orient=orient)\n        ax.set_yticklabels(['{:,.0%}'.format(y) for y in ax.get_yticks()])\n    else:\n        ax = sns.barplot(x= axis_name, y=feature, hue='Churn', data=g, orient=orient)\n        ax.set_xticklabels(['{:,.0%}'.format(x) for x in ax.get_xticks()])\n    ax.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# borrowed fcn for plotting pie plots with percentages of each category based on rule\ndef plot_var_percentages (df, var_list):\n\n    n_rows = math.ceil(len(var_list)/3)\n    mapper = []\n    count_c = 0\n    count_r = 0\n    for n in range(len(var_list)):\n        if count_c <= 2:\n            mapper.append((count_r,count_c))\n            count_c += 1\n        else:\n            count_r += 1\n            count_c = 0\n            \n    #fig, axes = plt.subplots(nrows = n_rows,ncols = 3,figsize = (15,12))\n    for i,var in enumerate(var_list):\n        \n        labels = list(df[var].value_counts().index)\n        counts = list(df[var].value_counts())\n        \n        plt.figure(i)\n        plt.pie(counts, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)\n        plt.title(var)\n    plt.show ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing: Data preps, feature adding based on EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Churn'] = df['Churn'].replace(\"No\", 0).replace(\"Yes\", 1)\n\ndf['SeniorCitizen'] = df['SeniorCitizen'].replace(0, \"No\").replace(1, \"Yes\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tenure - create two more categories, as the tenure feature does not have linear behaviour\ndf['tenure_short'] = np.where(df['tenure']<18, 1, 0)\ndf['tenure_long'] = np.where(df['tenure']>54, 1, 0)\n\ndf['TCh_diff_positive'] = np.where(df['TotalCharge_diff']>0, 1, 0)\ndf['TCh_diff_negative'] = np.where(df['TotalCharge_diff']<0, 1, 0)\n#df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop NaNs in TotalCharges\ndf = df.dropna()\n\n# drop customerID, as would not be of any help\ndf.drop(['customerID'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"non_dummy_cols = ['tenure','MonthlyCharges','TotalCharges','Churn','churn_rate','TotalCharge_diff','TotalCharge_diff_abs'] \ndummy_cols = list(set(df.columns) - set(non_dummy_cols))\n#df = pd.get_dummies(df, columns=dummy_cols)\ndf = pd.get_dummies(df, columns=dummy_cols,drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show what we get here. Again.\n# resumetable(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Retype to ints and bools"},{"metadata":{"trusted":true},"cell_type":"code","source":"# retype to boolean\nnon_int_cols = ['tenure','MonthlyCharges','TotalCharges','Churn','TotalCharge_diff','TotalCharge_diff_abs'] \nint_cols = list(set(df.columns) - set(non_int_cols))\ndf[int_cols] = df[int_cols].astype(bool)\n\n# retype floats\nfloat_cols = ['MonthlyCharges','TotalCharges','TotalCharge_diff','TotalCharge_diff_abs']\ndf[float_cols] = df[float_cols].astype(np.int64)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking good now. "},{"metadata":{},"cell_type":"markdown","source":"# Features correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import plot_confusion_matrix\n\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop \"No internet service\" items and others with high correlation. It was nto clear to me what is the meaning, from the correlation it is clear there is no information added by multiple columns\n# this was actually added after looking at Correrlation matrix, but I left it here for the sake of simplicity\ndf.drop(['OnlineBackup_No internet service',\n         'TechSupport_No internet service',\n         'StreamingTV_No internet service',\n         'DeviceProtection_No internet service',\n         'OnlineBackup_No internet service',\n         'OnlineSecurity_No internet service', \n         'StreamingMovies_No internet service', \n         'MultipleLines_No phone service',\n#         'PhoneService_No', \n         'TotalCharge_diff_abs',\n         'TotalCharge_diff',],axis=1,inplace=True)\n              # ,'MultipleLines_No',\n              # 'OnlineSecurity_No','OnlineBackup_No','DeviceProtection_No','TechSupport_No','StreamingTV_No','StreamingMovies_No'],\n              # axis=1,inplace=True)\n        \n# leaving out all the rest for now","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# commeted for faster execution\n\ncorrMatrix = df.drop(['Churn'], axis=1).corr()\nfig, ax = plt.subplots(figsize=(30,25))\nsns.heatmap(corrMatrix,annot=True, annot_kws={'size':12},cmap=\"GnBu\")\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The correlation matrix is heavy a lot, but nevertheless we see what features we can drop atm."},{"metadata":{"trusted":true},"cell_type":"code","source":"# df.drop(['OnlineBackup_No internet service',\n#          'TechSupport_No internet service',\n#          'StreamingTV_No internet service',\n#          'DeviceProtection_No internet service',\n#          'OnlineBackup_No internet service',\n#          'OnlineSecurity_No internet service', \n#          'StreamingMovies_No internet service', \n#          'MultipleLines_No phone service',\n#          'PhoneService_No', \n#          'TotalCharge_diff_abs',\n#          'TotalCharge_diff',],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation of \"Churn\" with other variables in 1D:\n\nplt.figure(figsize=(15,8))\n#corrMatrix = df.drop(['Churn'], axis=1).corr()\ndf.corr()['Churn'].sort_values(ascending = False).plot(kind='bar')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Apply scaling and split dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target0 = df['Churn'] # for y\nfeatures0 = df.drop(['Churn'], axis=1) # for X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To preserve the shape of the dataset (no distortion), data will be min max scaled to values between (0, 1) \n# instead of standard scaled. I tried also StandardScaler, but results were worse since the distribution of data is not gaussian. \n# RobustScaler was similar in performance to MinMaxScaker\nscaler0=MinMaxScaler()\n\nf_scale0 = scaler0.fit_transform(features0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # create train and test split on scaled data\nX_train0, X_test0, y_train0, y_test0 = train_test_split (f_scale0,target0,test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Recursive Feature Elimination - Random Forrest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFECV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\n\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nclf_rf = RandomForestClassifier()\ncv = StratifiedKFold(5) #5-fold stratified cross-validation\nrfecv = RFECV(estimator=clf_rf, step=1, cv=cv,scoring='accuracy')\nrfecv = rfecv.fit(X_train0, y_train0)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', features0.columns[rfecv.support_])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot number of features VS. cross-validation scores\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score of number of selected features\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform training and set for new set of features\nX_train1 = rfecv.transform(X_train0)\nX_test1 = rfecv.transform(X_test0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature importance using Random Forrest"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_rfc = RandomForestClassifier()      \nclr_rfc = clf_rfc.fit(X_train0,y_train0)\nimportances = clr_rfc.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf_rfc.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\n# print(\"Feature ranking:\")\n\n#for f in range(X_train0.shape[1]):\n#    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\n\nplt.figure(1, figsize=(14, 13))\nplt.title(\"Feature importances\")\nplt.bar(range(X_train0.shape[1]), importances[indices],\n       color=\"g\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X_train0.shape[1]), features0.columns[indices],rotation=90)\nplt.xlim([-1, X_train0.shape[1]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing different tree-based methods w/o tuned hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier,AdaBoostClassifier, BaggingClassifier, HistGradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline([\n    ('clf', DecisionTreeClassifier()) # classifier to iterate afterwards\n])\npipeline.steps\n\n#random = 2\nclassifiers = []\n\nclassifiers = []\nclassifiers.append(DecisionTreeClassifier())\nclassifiers.append(BaggingClassifier(KNeighborsClassifier()))\nclassifiers.append(ExtraTreesClassifier())\nclassifiers.append(RandomForestClassifier())\nclassifiers.append(AdaBoostClassifier())\nclassifiers.append(CatBoostClassifier(verbose=0))\nclassifiers.append(GradientBoostingClassifier())\nclassifiers.append(HistGradientBoostingClassifier())\n#classifiers.append(LGBMClassifier()) waiting for fix of issue in new build:-(\nclassifiers.append(XGBClassifier(use_label_encoder=False,eval_metric='logloss'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for classifier in classifiers:\n    pipeline.set_params(clf = classifier)\n    scores = cross_validate(pipeline, X_train1, y_train0)\n    print('---------------------------------')\n    print(str(classifier))\n    print('-----------------------------------')\n    for key, values in scores.items():\n            print(key,' mean ', values.mean())\n   #         print(key,' std ', values.std())\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using different weak learner for AdaBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # import Support Vector Classifier\n# from sklearn.svm import SVC\n# from sklearn.metrics import accuracy_score # import scikit-learn metrics module for accuracy calculation\n# svc = SVC(probability=True, kernel='sigmoid')\n# abc = AdaBoostClassifier(base_estimator=svc) # create adaboost classifer object\n# model2 = abc.fit(X_train0, y_train0) # train adaboost classifer\n# y_pred = model2.predict(X_test0) # predict the response for test dataset\n# print(\"Model Accuracy with SVC Base Estimator:\",accuracy_score(y_test0, y_pred)) # calculate and print model accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import Logistic Regression\n# Logistic regression was a good model alltogether in my other notebook, so I wanted to check how it would perform as weak learner in AdaBoost\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import accuracy_score # import scikit-learn metrics module for accuracy calculation\nlogreg0 = LogisticRegression(max_iter=500,C=10, penalty='l2', solver='lbfgs')\nadaBoost = AdaBoostClassifier(base_estimator=logreg0) # create adaboost classifer object\nada_model = adaBoost.fit(X_train1, y_train0) # train adaboost classifer\ny_pred = ada_model.predict(X_test1) # predict the response for test dataset\nprint(\"Model Accuracy on test set with LR Base Estimator:\",accuracy_score(y_test0, y_pred)) # calculate and print model accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sequential search for best parameters of AdaBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import validation_curve\n### 1. Use of validation curves for both datasets.\n#n_estimators\nn_estimators_param_range = [15, 25, 50, 75, 100, 150, 200]\n\n#prepare plot\nplt.figure(figsize=(15, 10))\n\n# Apply model to training data\n\nadaBoost = AdaBoostClassifier()\n\n# Plot validation curve\ntrain_scores, test_scores = validation_curve(estimator=adaBoost\n                                                            ,X=X_train1\n                                                            ,y=y_train0\n                                                            ,param_name='n_estimators'\n                                                            ,param_range=n_estimators_param_range)\n\ntrain_mean = np.mean(train_scores,axis=1)\ntest_mean = np.mean(test_scores,axis=1)\n\nplt.plot(n_estimators_param_range\n            ,train_mean\n            ,color='blue'\n            ,marker='o'\n            ,markersize=5\n            ,label='training accuracy')\n    \nplt.plot(n_estimators_param_range\n            ,test_mean\n            ,color='green'\n            ,marker='x'\n            ,markersize=5\n            ,label='test accuracy') \n    \nplt.xscale('log')\nplt.xlabel('n_estimators')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.79,0.82])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import validation_curve\n### 1. Use of validation curves for both datasets.\n#n_estimators\nlearning_rate_param_range = [0.1, 0.3, 0.4, 0.5, 0.65 ,0.8 ,1, 1.2, 1.3, 1.35, 1.375, 1.4, 1.45]\n\n#prepare plot\nplt.figure(figsize=(15, 10))\n\n# Apply model to training data\n\nadaBoost = AdaBoostClassifier(n_estimators = 50)\n\n# Plot validation curve\ntrain_scores, test_scores = validation_curve(estimator=adaBoost\n                                                            ,X=X_train1\n                                                            ,y=y_train0\n                                                            ,param_name='learning_rate'\n                                                            ,param_range=learning_rate_param_range)\n\ntrain_mean = np.mean(train_scores,axis=1)\ntest_mean = np.mean(test_scores,axis=1)\n\nplt.plot(learning_rate_param_range\n            ,train_mean\n            ,color='blue'\n            ,marker='o'\n            ,markersize=5\n            ,label='training accuracy')\n    \nplt.plot(learning_rate_param_range\n            ,test_mean\n            ,color='green'\n            ,marker='x'\n            ,markersize=5\n            ,label='test accuracy') \n    \nplt.xscale('log')\nplt.xlabel('learning_rate')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.795,0.815])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cross-validated model with AdaBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import mean\nfrom numpy import std\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define the model\nmodel = AdaBoostClassifier(n_estimators = 50, learning_rate = 1.35)\n# evaluate the model\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X_train1, y_train0, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n# report performance\nprint('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"well... that did not help :-( much"},{"metadata":{},"cell_type":"markdown","source":"# Random search for best hyperparameters for 2nd model - GBC"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import uniform, randint\nfrom sklearn.model_selection import cross_val_score, RandomizedSearchCV\n\nparams_gbc = {\n    'max_depth':         randint(2,50),     #\n    'max_leaf_nodes':    randint(1,200),    #\n    'min_samples_leaf':  randint(1,200),   #\n    'n_estimators':      randint(2,200),    #\n    'max_features':      randint(5,25),     #\n    'learning_rate':     uniform(0.01,0.95),#\n    'n_iter_no_change':  randint(5,6),     # \n}\n\ngbc_model = GradientBoostingClassifier()\n\n\nsearch = RandomizedSearchCV(gbc_model, \n                            param_distributions=params_gbc, \n                            random_state=42, \n                            n_iter=500, \n                            cv=3, \n                            verbose=1, \n                            n_jobs=-1, \n                            return_train_score=True)\n\nsearch.fit(X_train1, y_train0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def report_best_scores(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")\nreport_best_scores(search.cv_results_, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Credits\n* https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/ \n* https://towardsdatascience.com/the-dummys-guide-to-creating-dummy-variables-f21faddb1d40 \n* https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/ \n* https://www.analyticsvidhya.com/blog/2020/03/one-hot-encoding-vs-label-encoding-using-scikit-learn/\n* https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n* https://www.kaggle.com/joparga3/2-tuning-parameters-for-logistic-regression\n* ... and various other Kaggle kernels\n\n* https://www.learndatasci.com/tutorials/intro-feature-engineering-machine-learning-python/\n* https://towardsdatascience.com/how-to-avoid-multicollinearity-in-categorical-data-46eb39d9cd0d\n* https://machinelearningmastery.com/rfe-feature-selection-in-python/\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}