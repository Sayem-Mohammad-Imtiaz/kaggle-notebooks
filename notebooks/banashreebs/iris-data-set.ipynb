{"cells":[{"metadata":{},"cell_type":"markdown","source":"****Iris Dataset****"},{"metadata":{},"cell_type":"markdown","source":"Iris Dataset is a classificaton problem where we need to classify the flowers belonging to Iris setosa or Iris virginica or Iris versicolor class based on their petal length, petal width, sepal length and sepal width.\nThis dataset was introduced by British statistician and biologist Ronald Fisher.\n\nThe dataset contains 50 samples from each species.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing useful libraries.\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy.stats import shapiro\nimport seaborn as sns\n\n\n# Load dataset\ndataset = pd.read_csv('/kaggle/input/iris-flower-dataset/IRIS.csv')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for null and categorical variables.\ndataset.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.eq(0).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**EDA**\n\nWe can see that all the features have continuous values and the only categorical data is the dependent variable which has to be encoded. \nBefore that, we can have a glance at the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Univariate analysis*\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data = dataset, orient = 'h')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above, figure we can see that there are not many outliers in the dataset.  \nPetal length is and Petal width features are slightly left skewed."},{"metadata":{},"cell_type":"markdown","source":"*Bivariate Analysis*"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(dataset, hue = 'species')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see in the graph, \n* The distribution of all features is Gaussian.\n* The dimensions of Iris versicolor and Iris virginica are correlated in most of the cases. \n\n"},{"metadata":{},"cell_type":"markdown","source":"*Splitting into Train and Test data*\nFirst, I will endcode the target variable using LabelEnocder."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dataset.iloc[:,:-1]\nY = dataset.iloc[:,-1]\n\n# Converting categorical to num\nfrom sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\nY = lb.fit_transform(Y)\n\n#splitting into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Building classification models.*"},{"metadata":{},"cell_type":"markdown","source":"Since, the data is fairly simple, with no missing values or many outliers, I will choose Logistic regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, Y_train)\nY_pred = classifier.predict(X_test)\n\n#Accuracy\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy score(LR) = ',accuracy_score(Y_test, Y_pred))\n\n#Report\ntarget_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\nfrom sklearn.metrics import classification_report\nprint('Classificaation Report(LR) = \\n',classification_report(Y_test, Y_pred, target_names  = target_names))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the model has done fairly well with precision for the three classes being 1 or almost 1. And the accuracy score is 0.966"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}