{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this kenal we are going to build a Machine Learning model can classify heart disease by flowing this steps :\n\n#        1. Importing required libraries\n#        2. Data exloration\n#        3. Preprocessing training and predicting\n#        4. Conclution \n# 1. Importing required libraries and the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input/heart-disease-uci\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.manifold import TSNE\nfrom sklearn.svm import LinearSVC,NuSVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom time import time\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, cohen_kappa_score, roc_auc_score, roc_curve\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\ndf = pd.read_csv(\"../input/heart-disease-uci/heart.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#        2. Data exloration\n\n**a- SUMMARISING **"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"df.describe()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's important to notethat there is no missing values in the data set that makes our life easier. And we have some categorical columns **(cp, thal, slope',fbs,exang,target)** treated as numericals so we should convert the type of these columns, and rename sex category from 0,1 to Female,Male for visualization purposes."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df[['cp', 'thal', 'slope','fbs','exang','target']] = df[['cp', 'thal', 'slope','fbs','exang','target']].astype('category')\nsexdic = {0:'Female',1:'Male'}\ndf['sex'] = df['sex'].map(sexdic)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**b- DATA VISUALIZATION**\n\n    Which sex is more likly to have Heart Disease HD ?\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axs = plt.subplots(1,2,figsize=(12,5))\nsns.countplot(df['sex'],ax=axs[0]).set_title('SEX CATEGORY ')\nsns.countplot(df['target'],hue=df['sex'],ax=axs[1]).set_title('HD FOR EACH CATEGORY')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Barplot shows male's probability of having heart disease is heigher than female not  they are more likely to have HD but because they are more present in the  dataset.\n\n  **c- AGE DISTRIBUTION**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Male_age_ = df[df['sex']=='Male']['age'] \nFmale_age_ = df[df['sex']=='Female']['age'] \nfig, axs = plt.subplots(2, 2, figsize=(12, 4),sharex=True,gridspec_kw={\"height_ratios\": (0.2, 0.8)})\nsns.boxplot(Male_age_, ax=axs[0,0]).set_title(\"Male's age Distribution\")\nsns.distplot(Male_age_,ax=axs[1,0])\nsns.boxplot(Fmale_age_, ax=axs[0,1]).set_title(\"Female's age Distribution\")\nsns.distplot(Fmale_age_,ax=axs[1,1],color='Orange')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**d- Letter values plot and boxplot**\n\n       Heike Hofmann  et al.(2011)\n**Boxplot** invented in the 20th century and tends to display too many “outliers”, as judged by looking at boxplots of Gaussian data. There the expected number of “outliers” grows approximately linearly with n: the theoretical fourths from a sample of independent Gaussian observations are ±0.6745σ, yielding an interquartile range of 1.35σ, and inner fences at ±(0.675+1.5·1.35)σ = ±2.70σ. Therefore the box and whiskers covers 99.3% of the distribution, leaving about 0.7% of the points to be labeled as “outliers” (cf. Hoaglin (1983)).\n\n**Letter-value plots** are a variation of boxplots that replace the whiskers with a variable number of letter values, selected based on the uncertainty associated with each estimate and hence on the number of observations. Any values outside the most extreme letter value are displayed individually. These two modifications reduce the number of “outliers” displayed for large data sets, and make letter-value plots useful over a much wider range of data sizes\n> if we take resting blood pressure (trestbps) and calculate outliers in Boxplot way we get 9 outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_outliers(data,var):\n    Q1 = data[var].quantile(0.25)\n    Q3 = data[var].quantile(0.75)\n    treshold_height = Q3 + 1.5*(Q3 - Q1)\n    treshold_low = Q1 - 1.5*(Q3 - Q1)\n    return len(data[data[var] > treshold_height]),len(data[data[var] < treshold_low])\nMOH,MOL   = find_outliers(df,'trestbps')\nprint(f\"The size of the data: {df.shape[0]}\")\nprint(f'Number outliers of resting blood pressure height:{MOH},low :{MOL}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, 1, figsize=(12, 4),sharex=True)\nsns.boxplot(df[\"trestbps\"], ax=axs[0]).set_title(\"resting blood pressure  Distribution\")\naxs[0].set_xlabel(\" \")\nsns.boxenplot(df[\"trestbps\"], ax=axs[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DETECTING SIMILARITIES BETWEEN  NUMERICAL FEATURES**\n\nIf two variables  are heightly correlated  they bring the same information  and may increase the variance of the model,  so the idea is to persevere features with low correlation.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_col = df[['age','trestbps','chol','thalach','oldpeak']]\ncorr_m = num_col.corr()\nfig , axs = plt.subplots(1,1,figsize=(12,4))\nsns.heatmap(corr_m,cmap='RdYlGn_r',fmt ='.2f%')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dimentional reduction**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tsne  = TSNE(learning_rate=500,n_components=3).fit_transform(num_col)\nX_pca   = PCA(n_components=3).fit_transform(num_col)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axs = plt.subplots(1,2,figsize = (12,4))\naxs[0].scatter(X_tsne[: ,0],X_tsne[:,1],c=df['target'])\naxs[0].set_title('TENS')\naxs[1].scatter(X_pca[: ,0],X_pca[:,1],c=df['target'])\naxs[1].set_title('PCA')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the correlation matrix plot and dimension reduction methods (TSNE & PCA)\n#        3. Preprocessing training and predicting\n\n>SINCLE FUNCTION TO PREPROCESS TRAIN AND MAKE PREDICTION\n\nCreating  Features and labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"sexdic = {'Female':0,'Male':1}\ndf['sex'] = df['sex'].map(sexdic)\nX = df\ny = df.pop('target')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Process_data_fit_model(classifier,X,y):\n    print(\"Start Prepross...\")\n    start = time()\n    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, shuffle=True)\n    #Normalization\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)    \n    X_test = scaler.transform(X_test)\n    mmscaler = MinMaxScaler()\n    X_train = mmscaler.fit_transform(X_train)    \n    X_test = mmscaler.fit_transform(X_test)\n    end = time()\n    print(\"End Prepross ( {0:.2f} seconds )\".format(end-start))\n    \n    print(\"Start Model fitting...\")\n    start = time()\n    classifier.fit(X_train, y_train)\n    end = time()\n    print(f\"End Model fitting {end-start} seconds...\")\n    \n    print(\"\\nPredicting...\")\n    start = time()\n    y_predicted = classifier.predict(X_test)\n    end = time()\n    print(f\"Model fitting {end-start} seconds...\")\n    print(\"\\nPredicting...\")\n    start = time()\n    y_predicted = classifier.predict(X_test)\n    end = time()\n    print(f\"End Predicting {end-start} seconds...\")\n    \n    print(\"\\nReporting...\\n\")\n    print(classification_report(y_test, y_predicted),\"\\n\")\n    print(\"Confusion matrix:\\n\")\n    print(confusion_matrix(y_test, y_predicted),\"\\n\")\n    print(\"Cohen's Kappa score : \",cohen_kappa_score(y_test, y_predicted),\"\\n\")\n    \n    if len(np.unique(y_test)) == 2:\n        print(\"AUC score : {0:.3f}\".format(roc_auc_score(y_test, y_predicted)))\n        fpr, tpr, thresholds = roc_curve(y_test, y_predicted)\n        plt.plot([0, 1], [0, 1], linestyle='--',color='Red')\n        plt.plot(fpr, tpr, marker='*')\n        plt.title(\"ROC Curve\")\n        plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1-Decision tree****"},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeClassifier(criterion='gini', max_depth=10)   \nProcess_data_fit_model(dt,X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = LogisticRegression()\nProcess_data_fit_model(LR,X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Linear Support Vector Machine**"},{"metadata":{"trusted":true},"cell_type":"code","source":"svclf = LinearSVC()\nProcess_data_fit_model(svclf,X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Multilayer Perceptron**"},{"metadata":{"trusted":true},"cell_type":"code","source":"mlp = MLPClassifier(activation='logistic', alpha=1e-03, batch_size=32,\\\n                    beta_1=0.9, beta_2=0.999, early_stopping=False,\\\n                    epsilon=1e-08, hidden_layer_sizes=(200,200,200),\\\n                    learning_rate='constant', learning_rate_init=0.0001,\\\n                    max_iter=500, momentum=0.9, n_iter_no_change=10,\\\n                    nesterovs_momentum=True, power_t=0.5,\\\n                    shuffle=True, solver='adam', tol=0.00001,\\\n                    validation_fraction=0.1, verbose=True, warm_start=False)\nProcess_data_fit_model(mlp,X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Discussing the result and  Conclution \n\nThere are many differant way to interpret the result but w are going to focus on two measurement: Area Under Curve (AUC) and  Cohen's Kappa score.\n\n** AUC** : means \"area under the  ROC (receiver operating characteristic)  which is a  representation of  the performance of a classification model by plotting the rate of true positives(TPR) vs  the rate of false positives(FPR):\n\n$$ \nTPR = \\frac{TP}{TP +FP} \n$$\n$$ \nFPR = \\frac{FP}{TP +FP} \n$$\n    TP and FP are calculated from Confusion matrix \n    \nAUC represents degree  of separability and shows  how much model is capable of distinguishing between classes. An excellent model has AUC near to the 1 which means it has good measure of separability.\n\n\n**Cohen's Kappa**\n\nCohen's Kappa score  measure of how closely the instances classified by the machine learning classifier matched the data labeled as ground truth. Not only can this kappa statistic shows how the classifier itself performed,it is also used to compare models. Rought speaking statistic is used to test interrater reliability which  represents the extent to which the data collected in the study are correct representations of the variables measured.\n\nKappa value interpretation Landis & Koch (1977):\n\n<0 No agreement\n\n0 — .20 Slight\n\n.21 — .40 Fair\n\n.41 — .60 Moderate\n\n.61 — .80 Substantial\n\n.81–1.0 Perfect\n\nWe've used five Machine Learning Methods [Decision tree,Logistic Regression , Boosted Random Forest with adabost , Linear Support Vector Machine and multilayer perceptron] to classify Heart Disease according to AUC value and Cohen's Kappa score **Logistic Regression** is the best for this case. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}