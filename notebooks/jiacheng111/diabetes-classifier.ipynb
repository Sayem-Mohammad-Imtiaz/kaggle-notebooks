{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Step 1: import libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score,classification_report\nfrom sklearn.preprocessing import MinMaxScaler\nprint('import ok')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Step 2: read in the data and do a preliminary analysis\n\ndf = pd.read_csv(\"../input/pima-indians-diabetes-database/diabetes.csv\", \n                 names=['num_pregnant','glucose_conc','diastolic_bp','triceps_thick',\n                        'serum_insulin','bmi','pedigree','age'\n                        ,'class'],\n                 header=0)\n\n# df = pd.read_csv(\"../input/pima-indians-diabetes-database/diabetes.csv\")\nprint(df.shape)\nprint(df.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Step 3: Split into input_df and target dataframe. axis=0, row, axis=1, column.\ninput_df = df.drop('class', axis=1)\ntarget = df['class']\n\nprint(input_df.shape, target.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#distribution of class - imbalance class with '1' having lesser count\ntarget.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see that there are 268 \"yes\"es - 1\n# and there are 500 \"No\"s - 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters\n# *arrayssequence of indexables with same length / shape[0]\n# Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.\n\n# test_sizefloat or int, default=None\n# If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If train_size is also None, it will be set to 0.25.\n\n# train_sizefloat or int, default=None\n# If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.\n\n# random_stateint, RandomState instance or None, default=None\n# Controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls. See Glossary.\n\n# shufflebool, default=True\n# Whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None.\n\n# stratifyarray-like, default=None\n# If not None, data is split in a stratified fashion, using this as the class labels. Read more in the User Guide.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Step 4: Split feature and label sets to train and data sets - 70-30, random_state is desirable for reproducibility, stratify - same proportion as input data\n\n# random_state = 10\nX_train, X_test, y_train, y_test = train_test_split(input_df, target, test_size = 0.3, random_state = 10, stratify = target)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Question 2 - Normalize using MinMaxScaler to constrain values to between 0 and 1.\n\nscaler = MinMaxScaler(feature_range = (0,1))\n\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Step 5: Create a logistic regression classifier, default c=1\n\nlogreg = LogisticRegression(solver='liblinear')\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nprint('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#look at the value under the '1' class (or 'yes') for the corresponding precision, recall and f1 score\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Question 4 -Rerunning above with resampled data - using oversampling\n\nsm = SMOTE(random_state = 7)\nX_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n\nprint(pd.value_counts(pd.Series(y_train_sm)))\n\nclf = logreg.fit(X_train_sm, y_train_sm)\nprint('Model accuracy is',clf.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf.predict(X_test)\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Question 1,3 - get the descending sorted indices based on coefficient values\nsorted_index = np.argsort(-logreg.coef_)\n\n#get the feature_names\nfeature_names = input_df.columns\n\n#get the names of the important features (largest to smallest)\nprint(feature_names.to_numpy()[sorted_index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}