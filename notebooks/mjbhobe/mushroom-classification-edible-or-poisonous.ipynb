{"nbformat_minor":1,"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"pygments_lexer":"ipython3","version":"3.6.3","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","file_extension":".py"}},"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Problem Statement:**\n\nWe have been given a data file on mushroom species and we need to classify them as **e**dible or **p**oisonous. \n\nData file description:[](http://)\n\n**Target variable**: \n* ****class: e=edible, p=poisonous \n\n**Features:**\n* cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n* cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s\n* cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n* bruises: bruises=t,no=f\n* odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n* gill-attachment: attached=a,descending=d,free=f,notched=n\n* gill-spacing: close=c,crowded=w,distant=d\n* gill-size: broad=b,narrow=n\n* gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n* stalk-shape: enlarging=e,tapering=t\n* stalk-root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?\n* stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\n* stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\n* stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n* stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n* veil-type: partial=p,universal=u\n* veil-color: brown=n,orange=o,white=w,yellow=y\n* ring-number: none=n,one=o,two=t\n* ring-type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n* spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n* population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n* habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d","metadata":{}},{"cell_type":"code","execution_count":null,"source":"# Python 3 environment\n\n# imports and tweaks\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# tweaks for Numpy & Pandas\npd.set_option('display.notebook_repr_html',True)\npd.set_option('display.max_rows',100)\npd.set_option('display.max_columns',100)\npd.set_option('display.width',1024)\n# force all numoy & pandas floating point output to 3 decimal places\nfloat_formatter = lambda x: '%.3f' % x\nnp.set_printoptions(formatter={'float_kind':float_formatter})\npd.set_option('display.float_format', float_formatter)\n# force Numpy to display very small floats using floating point notation\nnp.set_printoptions(threshold=np.inf)\n# force GLOBAL floating point output to 3 decimal places\n%precision 3\n\n# tweaks for plotting libraries (Matplotlib & Seaborn) [recommended]\nplt.style.use('seaborn-muted')\nsns.set_context(context='notebook',font_scale=1.0)\nsns.set_style('whitegrid')\n\nseed = 42 #sum(map(ord, 'Kaggle - Pima Indians Diabetes Analysis'))\nnp.random.seed(seed)","outputs":[],"metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","outputs":[],"metadata":{"_uuid":"37f125d61735956ea1992d4678850a46322f217d","_cell_guid":"9959c94e-5093-4645-bb93-302aab67852a"}},{"cell_type":"markdown","source":"# Loading Data & Preliminary Analysis","metadata":{}},{"cell_type":"code","execution_count":null,"source":"data_file = '../input/mushrooms.csv'\ndata = pd.read_csv(data_file)\ndata.head()","outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":"data.info()","outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":"data.isnull().sum()","outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":"# how many unique output classes?\nnp.unique(data['class'])","outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":"# save names of features & outcome columns of dataset\noutcome = data.columns.values[0]    # name of target (= 'class')\nfeatures = data.columns.values[1:]  # names of 21 features\noutcome, features","outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":"# lets check how many unique values each feature has\nfor feature in features:\n    print('\\'%s\\' feature has %d unique values' % (feature, len(np.unique(data[feature]))))","outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":"# distribution of target \nf, ax = plt.subplots(figsize=(6,4))\n_ = sns.countplot(x='class', data=data, ax=ax)","outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":"# let us view countplot of each feature (we have 22 features)\nnum_features = len(features)\nplots_per_row = 3\nnum_rows = int(num_features/plots_per_row) + 1\ncol_id = 0\n\nwith sns.axes_style('ticks'):\n    for row in range(num_rows):\n        f, ax = plt.subplots(nrows=1, ncols=plots_per_row, sharey=False, figsize=(12,2))\n        for col in range(plots_per_row):\n            sns.countplot(x=data[features[col_id]], data=data, ax=ax[col])\n            col_id += 1\n            if col_id >= num_features:\n                break\nplt.show()\nplt.close()","outputs":[],"metadata":{}},{"cell_type":"markdown","source":"**Observations:**\n* We have dataset with 8124 samples.\n* There are 22 features and the 'class' column is the target value\n    > _Action:_ We have adequate number of records, so we won't have to resort to PCA techniques for feature compression.\n* All features as well as the target value are categorical variables!\n* The target value (class) has 2 possible outcomes 'e' and 'p' - this is a binary classification problem.\n* Each feature has a differnt number of unique values - from 1 (veil-type) to 12 (gill-color). \n    > _Action:_ We will have to use `LabelEncoding` on all the columns to convert from categorical values to numeric values. We **won't** use `One-Hot-Encoding` because all features will then be on 0-1 scale and most classifiers will give accuracy_score = 1. We will use a classifier like `DecisionTreeClassifier()` which can work well with LabelEncoded data.\n* The _target class is not unbalanced_ - we have approximatly equal proportions of each class 'p' and 'e'\n    > _Action:_ We don't have to use starification when splitting data into train/test sets.","metadata":{}},{"cell_type":"markdown","source":"# Data Pre-processing","metadata":{}},{"cell_type":"code","execution_count":null,"source":"from sklearn.preprocessing import LabelEncoder\n# apply label encoding to all the columns \ndata2 = data.copy()\nfor col in data2.columns.values:\n    data2[col] = LabelEncoder().fit_transform(data2[col])\n\ndata2.head() # we can now use data2","outputs":[],"metadata":{}},{"cell_type":"markdown","source":"We have not been provided a _separate_ testing file to do an _independent_ check of the classifier. To _create_ such a file, I will keep aside 10% of the data records as an _independent test set_. This is different from `X_test` and `y_test`. The advantage of this approach is I can 'simulate' me feeding an independent file to the classifier, with the advantage that I have the expected result also available for cross-checking. We will call this 10% of records the *testing_set*","metadata":{}},{"cell_type":"code","execution_count":null,"source":"# select 10% of number of data records\nset_aside_count = int(0.10 * len(data2)) \nset_aside_ids = np.random.choice(data2.index, set_aside_count, replace=False)\ntesting_set = data2.loc[set_aside_ids]   # independent testing data\ndata_set = data2[~data2.index.isin(set_aside_ids)]\nlen(data2), set_aside_count, len(set_aside_ids), len(testing_set), len(data_set)","outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":"# split the data_set into train & test sets\nfrom sklearn.model_selection import train_test_split\n\n# we will use 80:20 split\ntrain_set, test_set = \\\n  train_test_split(data_set, test_size=0.20, random_state=0)\n\nX_train = train_set[features] \nX_test = test_set[features]  \ny_train = train_set[outcome] \ny_test = test_set[outcome] \nX_train.shape, y_train.shape, X_test.shape, y_test.shape","outputs":[],"metadata":{}},{"cell_type":"markdown","source":"## Utility functions","metadata":{}},{"cell_type":"code","execution_count":null,"source":"from sklearn.model_selection import StratifiedKFold, cross_val_score \nfrom sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix","outputs":[],"metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"source":"# some utility functions\ndef do_kfold_cv(classifier, X_train, y_train, n_splits=10, scoring='roc_auc'):\n    \"\"\" do a k-fold cross validation run on classifier & training data\n      and return cross-val scores \"\"\"   \n    kfolds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    cv_scores = cross_val_score(classifier, X_train, y_train, scoring=scoring, cv=kfolds)\n    return cv_scores\n\ndef test_classifier(clf_tuple, X_train, y_train, X_test, y_test, scoring='roc_auc', verbose=2):\n    \"\"\" run a k-fold test, fit model to training data & report\n        scores for training & test data \"\"\"\n    # extract classifier instance & name\n    classifier, classifier_name = clf_tuple\n   \n    if verbose > 0:\n        print('Testing classifier %s...' % classifier_name)\n    \n    classifier.fit(X_train, y_train)\n    \n    # accuracy scores, against test data\n    acc_score = classifier.score(X_test, y_test)\n    \n    # k-fold cross-validation scores\n    cv_scores = do_kfold_cv(classifier, X_train, y_train, scoring=scoring)\n\n    # roc-auc score\n    y_pred_proba_train = classifier.predict_proba(X_test)[:,1]\n    auc_score = roc_auc_score(y_test, y_pred_proba_train)\n\n    if verbose > 1:   \n        print('   - Cross-val score       : Mean - %.3f Std - %.3f Min - %.3f Max - %.3f' % \\\n                  (np.mean(cv_scores), np.std(cv_scores), np.min(cv_scores), np.max(cv_scores)))\n        print('   - Accuracy score (test) : %.3f' % (acc_score))\n        print('   - AUC score (test)      : %.3f' % (auc_score))\n              \n    return cv_scores, acc_score, auc_score","outputs":[],"metadata":{"collapsed":true}},{"cell_type":"markdown","source":"## Create _Decision Tree Classifier_ & run on X_train and check on X_test","metadata":{}},{"cell_type":"code","execution_count":null,"source":"# create & run the DecisionTreeClassifier on the data\nfrom sklearn.tree import DecisionTreeClassifier\nclf_dt = DecisionTreeClassifier(random_state=seed, max_depth=6)\n# following line will print the metrics\n_ = test_classifier((clf_dt,'Decision Tree'), X_train, y_train, X_test, y_test, verbose=2)","outputs":[],"metadata":{}},{"cell_type":"markdown","source":"**NOTE**:\n* We got a _test accuracy_ of 99.5%\n* We got an AUC score on test (X_test, y_test) data of 99.9% (nearly perfect classifer?)","metadata":{}},{"cell_type":"code","execution_count":null,"source":"# lets check the confusion matrix - I am using pandas crosstab function to create a better view\npd.crosstab(y_test.ravel(), clf_dt.predict(X_test), rownames=['Actual'], \n            colnames=['Predicted->'], margins=False)","outputs":[],"metadata":{}},{"cell_type":"markdown","source":"** Results:**\n* We have just 8 incorrect classifications on X_test, y_test - all *false negatives*\n\nNext, let us run our classifier against the *set-aside* data file to check how the classifier behaves.","metadata":{}},{"cell_type":"code","execution_count":null,"source":"X_testing = testing_set[features]\ny_testing = testing_set[outcome]","outputs":[],"metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"source":"pd.crosstab(y_testing.ravel(), clf_dt.predict(X_testing), \n            rownames=['Actual'], colnames=['Predicted->'], margins=False)","outputs":[],"metadata":{}},{"cell_type":"markdown","source":"We have just 2 mis-classifications here.","metadata":{}},{"cell_type":"code","execution_count":null,"source":"# accuracy from this is calculates as\n(433. + 377.) / (433. + 0. + 2. + 377.)","outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":"","outputs":[],"metadata":{"collapsed":true}}]}