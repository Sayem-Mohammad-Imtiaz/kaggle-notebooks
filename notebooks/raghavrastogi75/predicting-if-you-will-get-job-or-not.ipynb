{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Contents of the notebook\n\n[Raghav Rastogi](https://www.kaggle.com/raghavrastogi75)\n\n- [1 - Data Import](#1)\n- [2 - Basic Analysis](#2)\n- [3 - Exploratory Data Analysis](#3)\n- [4 - Prediction](#4)\n- [5 - Explanation](#5)\n- [6 - Conclusion](#6)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### I have referred the book 'Hands-on Machine Learning with scikit-Learn, Keras, and Tensorflow' by AurÃ©lien GÃ©ron and applied on this dataset. I highly recommend it if you are a beginner. If you have a question or feedback, do not hesitate to write and if you find this kernal helpful, please do not forget to **UPVOTE** ðŸ™‚","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data import <a id=\"1\"></a> <br>","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"place = pd.read_csv('/kaggle/input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Basic Analysis <a id=\"2\"></a> <br>\n\nLet's have a look at the Null values and the overall look at the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(place.shape)\nplace.head(20)\nprint(place.isna().any())\n#place = place.set_index('sl_no')\nplace.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis <a id=\"3\"></a> <br>\n\nLooking at the numerical values of the data set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(place.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a visualisation of different aspects of the dataset to get an idea of it and also get an intuition of the important parameters with respect to target variable 'Status'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pv1 = place.pivot_table(index = 'gender', columns = 'status',values = 'ssc_p' )\n\nx = np.arange(len(pv1.index))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(figsize = (5,5))\nrects1 = ax.bar(x - width/2, pv1['Not Placed'] , width, label='Not Placed')\nrects2 = ax.bar(x + width/2, pv1['Placed'] , width, label='Placed')\n\nax.spines[\"top\"].set_color(\"None\")\nax.spines[\"right\"].set_color(\"None\")\nax.set_ylabel('Scores')\nax.set_title('Average Senior secondary percentage by Gender')\nax.set_xticks(x)\nax.set_ylim(0,100)\nax.set_xticklabels(pv1.index)\nax.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion : We get a clear picture that senior secondary percentage is higher for students that got placed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pv2 = place.pivot_table(index = 'gender', columns = 'status',values = 'etest_p' )\n\nx = np.arange(len(pv2.index))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(figsize = (5,5))\nrects1 = ax.bar(x - width/2, pv2['Not Placed'] , width, label='Not Placed')\nrects2 = ax.bar(x + width/2, pv2['Placed'] , width, label='Placed')\n\nax.spines[\"top\"].set_color(\"None\")\nax.spines[\"right\"].set_color(\"None\")\nax.set_ylabel('Scores')\nax.set_title('Average Employability test scores by Gender')\nax.set_xticks(x)\nax.set_ylim(0,100)\nax.set_xticklabels(pv1.index)\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Surprisingly not much clear difference in terms of placement.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pv3 = place.pivot_table(index = 'gender', columns = 'status',values = 'mba_p' )\n\nx = np.arange(len(pv3.index))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(figsize = (5,5))\nrects1 = ax.bar(x - width/2, pv3['Not Placed'] , width, label='Not Placed')\nrects2 = ax.bar(x + width/2, pv3['Placed'] , width, label='Placed')\n\nax.spines[\"top\"].set_color(\"None\")\nax.spines[\"right\"].set_color(\"None\")\nax.set_ylabel('Scores')\nax.set_title('Average MBA percentage by Gender')\nax.set_xticks(x)\nax.set_ylim(0,100)\nax.set_xticklabels(pv1.index)\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not much difference of scores here as well.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pv4 = place.pivot_table(index = 'degree_t', columns = 'status', values = 'gender', aggfunc = 'count')\nprint(pv4)\nx = np.arange(len(pv4.index))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(figsize = (5,5))\nrects1 = ax.bar(x - width/2, pv4['Not Placed'] , width, label='Not Placed')\nrects2 = ax.bar(x + width/2, pv4['Placed'] , width, label='Placed')\n\nax.spines[\"top\"].set_color(\"None\")\nax.spines[\"right\"].set_color(\"None\")\nax.set_ylabel('Count')\nax.set_title('Count of placement status of students by degree')\nax.set_xticks(x)\nax.set_ylim(0,110)\nax.set_xticklabels(pv4.index)\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that students with Commerce and Management degree have more placements although the placements percentage is more for Science & technology","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pv5 = place.pivot_table(index = 'specialisation', columns = 'status', values = 'gender', aggfunc = 'count')\npv5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In terms of 'specialization' students got placed much more in 'Market and Finance'.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Prediction <a id=\"4\"></a> <br>\n\nImporting all libraries and creating the test and train dataset. Also separating the numerical and the categorical attributes to apply different data processing techniques.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.ensemble import RandomForestClassifier\n\nplace_new = place.drop('salary',axis = 1).reset_index()\nplace_new = place_new.replace(['Placed','Not Placed'],[1,0])\n# print(place_new,'place_new')\nplace_cat = place_new[['gender','ssc_b','hsc_b','hsc_s','degree_t','workex','specialisation']]\nplace_num = place_new[['ssc_p','hsc_p','degree_p','etest_p','mba_p']]\n\nX = place_new.drop(['status'],axis = 1).reset_index().drop(['sl_no'],axis = 1)\ny = place_new.iloc[:,-1]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using Pipeline and ColumnTransfer I have combined all the data processing and prediction algorithms for both numerical and categorical data. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\nnum_attribs = list(place_num)\ncat_attribs = list(place_cat)\n# print(num_attribs,\"***num_Attribs***\")\n# print(cat_attribs,\"***cat_attribs***\")\nnum_transformer = Pipeline(steps = [('scaler', StandardScaler())])\n\ncat_transformer = Pipeline(steps = [('onehot',OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(transformers = [('num',num_transformer, num_attribs),\n                                                 ('cat',cat_transformer,cat_attribs)])\n\nparam_grid = [{'n_estimators':[100,500],'max_features': [8, 10, 12]},\n              {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}]\n\nforest_clf = RandomForestClassifier()\n\n\nclf = Pipeline(steps = [('preprocessor',preprocessor),('grid_search',GridSearchCV(forest_clf, param_grid, cv=5,\nscoring='roc_auc',return_train_score=True))])\n\nsplit = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n\nfor train_index, test_index in split.split(X,y):\n    #print(train_index,(test_index))\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\nclf.fit(X_train,y_train)\n\ny_pred = clf.predict(X_test)\n#print(y_pred)\n\n\nfrom sklearn.metrics import confusion_matrix\n\nprint(confusion_matrix(y_test, y_pred))\n\nprint(\"model score: %.3f\" % clf.score(X_test, y_test))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explanation:<a id=\"5\"></a> <br> \n* I created 'num_attribs' and 'cat_attribs' and I created Pipeline for data preprocessing for both Numeric and Categorical data.\n* Then I combined the two using Column transfer and created a combined variable 'preprocessor' which can be used directly in a pipeline later on.\n* Created a 'param_grid' which is the combined list of parameters to be tested for grid search.\n* Using Forest Classifier for prediction.\n* Performing grid search cross validation with Forest Classifier with scoring as 'roc_auc' using pipeline and storing it as clf.\n* Applying stratified shuffle split which will create a test set that is a complete representative of the data for example the gender ratio will remain constant for the test set as well.\n* Finally fitting the data set on the classifier\n* Creating y_pred to create a list of predicted variable\n* Using confusion matrix to find the false negetives and false positives.\n* Finally getting the roc_auc score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(clf.named_steps['grid_search'].best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see the best parameters for Grid Search are max_features = 8 and n_estimators = 500. I might try to tune it even finer in future iterations.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Conclusion<a id=\"6\"></a> <br>\n\nThis is one of my initial attempts on EDA and prediction.\nDo let me know if there are any feedback/comments/suggestions :) \nI would to know how I can improve further and make even better predictions.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}