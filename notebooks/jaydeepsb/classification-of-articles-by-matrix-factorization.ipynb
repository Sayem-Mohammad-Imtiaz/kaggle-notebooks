{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport glob\nimport json\n\n#sys.path.insert(0, \"../\")\n\nroot_path = '/kaggle/input/CORD-19-research-challenge/2020-03-13'\n\njson_filenames = glob.glob(f'{root_path}/**/*.json', recursive=True)\nprint(len(json_filenames))\n\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"all_articles_df = pd.DataFrame(columns=[\"source\", \"title\", \"doc_id\",  \"abstract\", \"text_body\"])\n\n#all_articles_df = pd.DataFrame.from_dict(all_articles_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def return_corona_df(json_filenames, df, source):\n\n    for file_name in json_filenames:\n\n        row = {\"doc_id\": None, \"source\": None, \"title\": None,\n              \"abstract\": None, \"text_body\": None}\n\n        with open(file_name) as json_data:\n            data = json.load(json_data)\n\n            row['doc_id'] = data['paper_id']\n            row['title'] = data['metadata']['title']\n\n            # Now need all of abstract. Put it all in \n            # a list then use str.join() to split it\n            # into paragraphs. \n\n            abstract_list = [data['abstract'][x]['text'] for x in range(len(data['abstract']) - 1)]\n            abstract = \"\\n \".join(abstract_list)\n\n            row['abstract'] = abstract\n\n            # And lastly the body of the text. For some reason I am getting an index error\n            # In one of the Json files, so rather than have it wrapped in a lovely list\n            # comprehension I've had to use a for loop like a neanderthal. \n            \n            # Needless to say this bug will be revisited and conquered. \n            \n            body_list = []\n            for _ in range(len(data['body_text'])):\n                try:\n                    body_list.append(data['body_text'][_]['text'])\n                except:\n                    pass\n\n            body = \"\\n \".join(body_list)\n            \n            row['text_body'] = body\n            \n            # Now just add to the dataframe. \n            \n            if source == 'b':\n                row['source'] = \"BIORXIV\"\n            elif source == \"c\":\n                row['source'] = \"COMMON_USE_SUB\"\n            elif source == \"n\":\n                row['source'] = \"NON_COMMON_USE\"\n            elif source == \"p\":\n                row['source'] = \"PMC_CUSTOM_LICENSE\"\n            \n            df = df.append(row, ignore_index=True)\n    \n    return df\n\nall_articles_df = return_corona_df(json_filenames, all_articles_df, 'b')\nall_articles_df_out = all_articles_df.to_csv('kaggle_covid-19_open_csv_format.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_articles_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titles = all_articles_df['title']\ntitles.fillna(\"\",inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titles.iloc[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(max_features=10000, stop_words='english')\nX_tfidf = tfidf.fit_transform(titles)\ntfidf_feature_names = tfidf.get_feature_names()\n\nvectorizer = CountVectorizer(stop_words='english', max_features=1000)\nX_tf = vectorizer.fit_transform(titles)\ntf_feature_names = vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_feature_names[500:510]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clustered = KMeans(n_clusters=6, random_state=0).fit_predict(X_tfidf)\n\nall_articles_df['cluster_abstract']=clustered\n\ngrouped=all_articles_df.groupby('cluster_abstract')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Factorization **"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pylab as plt\nfrom numpy import arange\nplt.figure()\nfor i in arange(500):\n    plt.plot(i,len(titles.iloc[i]), 'ro')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_topics = 15\n\n# Run NMF\nnmf = NMF(n_components=n_topics).fit(X_tfidf)\n\n# Run LDA\nlda = LatentDirichletAllocation(n_components=n_topics).fit(X_tf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in arange(10):\n    print(\"==============\")\n    for i in nmf.components_[j].argsort()[:-10:-1]:\n        print(tfidf_feature_names[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extract topics\ndef display_topics(model, feature_names, no_top_words):\n    topics=[]\n    for topic_idx, topic in enumerate(model.components_):\n        #rint (\"Topic %d:\" % (topic_idx))\n        topic_words=\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n        #rint(topic_words)\n        topics.append(topic_words)\n    return topics\n\nno_top_words = 5\n#rint(\"NMF: \")\ntopics_nmf=display_topics(nmf, tfidf_feature_names, no_top_words)\n#rint(\"\\nLDA: \")\ntopics_lda=display_topics(lda, tf_feature_names, no_top_words)\n\n#rint(topics_nmf)\n#rint(topics_lda)\n\npred_lda=lda.transform(X_tf)\npred_nmf=nmf.transform(X_tfidf)\n\nres_lda=[topics_lda[np.argmax(r)] for r in pred_lda]\nres_nmf=[topics_nmf[np.argmax(r)] for r in pred_nmf]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_articles_df['topic_lda']=res_lda\nall_articles_df['topic_nmf']=res_nmf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_articles_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped=all_articles_df.groupby('topic_nmf')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}