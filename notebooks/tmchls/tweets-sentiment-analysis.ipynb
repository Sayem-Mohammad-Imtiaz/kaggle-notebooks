{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importing necessary libraries\nimport numpy as np \nimport pandas as pd\nimport nltk\nfrom tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout,Bidirectional\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix,classification_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_train.csv',encoding='ISO-8859-1')\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test=pd.read_csv('/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_test.csv')\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape,test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Since we are only interested in tweets and sentiment removing other columns from train and test sets.*","metadata":{}},{"cell_type":"code","source":"train.drop(columns=['TweetAt','UserName','ScreenName','Location'],axis=1,inplace=True)\ntest.drop(columns=['TweetAt','UserName','ScreenName','Location'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.rename(columns={'OriginalTweet':'Tweet'},inplace=True)\ntest.rename(columns={'OriginalTweet':'Tweet'},inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Sentiment'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['Sentiment'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*In both train and test sets there are 5 classes instead of 3 classes i.e. Positive, Neutral and Negative.Therefore representing Extremely Positive tweets as Positive tweets and  Extremely Negative tweets as Negative tweets.*","metadata":{}},{"cell_type":"code","source":"train.Sentiment.replace({'Extremely Positive': 'Positive','Extremely Negative': 'Negative'},inplace=True)\ntest.Sentiment.replace({'Extremely Positive': 'Positive','Extremely Negative': 'Negative'},inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Sentiment'].value_counts(normalize=True)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Training dataset has 43.84 % of positive tweets,37.41% of the negative tweets and 18.74% of neutral tweets.*","metadata":{}},{"cell_type":"code","source":"test['Sentiment'].value_counts(normalize=True)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Test dataset has 42.99% of negative tweets,40.70% of positive tweets and 16.29% of neutral tweets.*","metadata":{}},{"cell_type":"markdown","source":"***It can be seen that test dataset has more negative tweets then training dataset and less positive tweets than train dataset.However,distribution of neutral tweets is having minor difference in both sets.***","metadata":{}},{"cell_type":"code","source":"#mapping of sentiment values\ntrain.replace({'Negative':0,'Neutral':1,'Positive':2},inplace=True)\ntest.replace({'Negative':0,'Neutral':1,'Positive':2},inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=train.drop_duplicates()\ntest=test.drop_duplicates()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('stopwords')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for lemmatization\nlemma=WordNetLemmatizer()\n#function for preprocessing\ndef preprocessing(review_text):\n    review_text=re.sub(r'http\\S+',' ',review_text) #removing the url\n    review_text=re.sub('[^a-z-A-Z]',' ',review_text) #removing numbers and punctuation\n    review_text=str(review_text).lower()   #converting all characters into lowercase\n    review_text=review_text.split()\n    review_text=\" \".join([lemma.lemmatize(item) for item in review_text \n                 if item not in set(stopwords.words('english'))]) #removing stopwords\n    return review_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Tweet']=train['Tweet'].apply(lambda x : preprocessing(x))\ntest['Tweet']=test['Tweet'].apply(lambda x : preprocessing(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positive=train[train.Sentiment==2]['Tweet']\nnegative=train[train.Sentiment==0]['Tweet']\nneutral=train[train.Sentiment==1]['Tweet']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#displaying top 20 words having highest frequency amongst all three classes\ncolor=['Accent','Paired','Pastel1']\nsplitedData=[positive,negative,neutral]\n\nfor item in range(3):\n    plt.figure(figsize=(10,10))\n    pd.Series(' '.join([i for i in splitedData[item]]).split()).value_counts().head(20).plot(kind='bar',\n                                                                                             colormap=color[item])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Coronavirus is the most common word in amongst all of the three classes.*","metadata":{}},{"cell_type":"code","source":"x_train=train['Tweet']\ny_train=train['Sentiment']\nx_test=test['Tweet']\ny_test=test['Sentiment']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = np.max(x_train.apply(lambda x :len(x)))\nmax_len","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#converting text to numeric using tokenizer with padding sequences to max length\ntokenizer=Tokenizer()\ntokenizer.fit_on_texts(x_train)\nvocab_length = len(tokenizer.word_index) + 1\n\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\n\nx_train = pad_sequences(x_train, maxlen=max_len, padding='post')\nx_test = pad_sequences(x_test, maxlen=max_len, padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Vocab length:\", vocab_length)\nprint(\"Max sequence length:\", max_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#making use of bidirectional LSTM with dropout\nembedding_dim=20\nmodel1 = Sequential()\nmodel1.add(Embedding(vocab_length,embedding_dim,input_length=max_len)) #The embedding layer\nmodel1.add(Bidirectional(LSTM(50,dropout=0.5))) #Our LSTM layer\nmodel1.add(Dense(32,activation='relu'))\nmodel1.add(Dropout(0.5))\nmodel1.add(Dense(3,activation='softmax'))\n\n\nmodel1.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#converting target into categorical array\nfrom keras.utils import to_categorical\n\ny_train = to_categorical(y_train, 3)\ny_test = to_categorical(y_test, 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.fit(x_train, y_train,epochs=5,validation_split=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.evaluate(x_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = np.argmax(model1.predict(x_test),axis=-1)\ncm = confusion_matrix(np.argmax(y_test,1),pred)\nplt.figure(figsize=(10,5))\nsns.heatmap(cm,annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(np.argmax(y_test,1),pred,target_names=['Negative','Neutral','Positive']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*If you like this notebook then upvote and share it.*\n\n*Do provide your valuable feedback.*\n\n*Thank you.*","metadata":{}}]}