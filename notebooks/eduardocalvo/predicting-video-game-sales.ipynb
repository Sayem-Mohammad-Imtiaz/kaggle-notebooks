{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport math\nfrom sklearn.linear_model import BayesianRidge, LinearRegression\n\n# Importing the dataset\ndata = pd.read_csv('/kaggle/input/videogames-sales-dataset/Video_Games_Sales_as_at_22_Dec_2016.csv')\n# Dropping features that are unnecessary for sale prediction\ndata.drop(columns=['Year_of_Release', 'Developer', 'Publisher', 'Platform'], inplace=True)\n\n# Defining X and y\nX = data.iloc[:, :].values\nX = np.delete(X, 6, 1)\ny = data.iloc[:, 6:7].values\n\n# Dropping the column that contains the name of the games as they are not needed for the prediction model\nX = X[:, 1:]\n\n# Replacing all NaN values to 'NA' for both the test and train data\nimp_const = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='NA')\nimp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n\nX[:, [5, 6, 7, 8]] = imp_mean.fit_transform(X[:, [5, 6, 7, 8]])\nX[:, [0, 9]] = imp_const.fit_transform(X[:, [0, 9]])\n\n# Encoding the categories into a string of binary values\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0, 9])], remainder='passthrough')\nX = ct.fit_transform(X)\n\n\n\n# Our models\nclf = tree.DecisionTreeRegressor(max_depth=200)\nregr = svm.SVR(kernel='linear')\nclf_2 = BayesianRidge(compute_score=True)\n\n\n\nr2_dec_list = []\nerror_dec_list = []\nr2_svr_list = []\nerror_svr_list = []\nr2_bay_list = []\nerror_bay_list = []\n\nfor x in range(3):\n    \n    randomize = np.random.uniform(0.05, 1.0)\n    # Splitting the data into train and test\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=randomize, random_state=0)\n\n    \n    # Decision Tree Regression\n    clf = clf.fit(X_train, y_train)\n    y_pred_1 = clf.predict(X_test)\n\n    # R2 score and mean squared error\n    r2_score_1 = r2_score(y_test, y_pred_1)\n    r2_dec_list.append(r2_score_1)\n    rmse = math.sqrt(mean_squared_error(y_test, y_pred_1))\n    error_dec_list.append(rmse)\n   \n    #print(f\"r2 score of the model : {r2_score_1:.3f}\")\n    #print(f\"Root Mean Squared Error of the model : {rmse:.3f}\")\n    \n   \n\n    # Support Vector Regression\n    regr.fit(X_train, y_train.ravel())\n    y_pred_2 = regr.predict(X_test)\n\n    # R2 score and mean squared error\n    r2_score_2 = r2_score(y_test, y_pred_2)\n    r2_svr_list.append(r2_score_2)\n    rmse_2 = math.sqrt(mean_squared_error(y_test, y_pred_2))\n    error_svr_list.append(rmse_2)\n    \n    #print(f\"r2 score of the model : {r2_score_2:.3f}\")\n    #print(f\"Root Mean Squared Error of the model : {rmse_2:.3f}\")\n    \n\n    # Bayesian Ridge Regressor\n    \n    clf_2.fit(X_train, y_train.ravel())\n    y_pred_3 = clf_2.predict(X_test)\n\n    # R2 score and mean squared error\n    r2_score_3 = r2_score(y_test, y_pred_3)\n    r2_bay_list.append(r2_score_3)\n    rmse_3 = math.sqrt(mean_squared_error(y_test, y_pred_3))\n    error_bay_list.append(rmse_3)\n    \n    #print(f\"r2 score of the model : {r2_score_3:.3f}\")\n    #print(f\"Root Mean Squared Error of the model : {rmse_3:.3f}\")\n\n\n\n# Decision Tree\n# Making plot where we are comparing predicted values with the actual ones\n\nfig, ax = plt.subplots()\nplt.title('Comparison of predicted and actual values in Decision Tree Regression', y=1.01, size=16)\nax.scatter(y_test, y_pred_1)\nax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')\nplt.xlim([0, 45])\nplt.ylim([0, 45])\nplt.figtext(.01, .00008, 'Figure 1: Visual representation of comparison between predicted and actual sales when using '\n                  'Decision Tree Regression')\nplt.show()\n\n# Average scores\nr2_dec_avg = np.average(r2_dec_list)\nprint('Average r2 score for Decision Tree Regression over ' + str(x+1) + ' iterations: ', r2_dec_avg)\nerror_dec_avg = np.average(error_dec_list)\nprint('Average Root Mean Square Error score for Decision Tree Regression over ' + str(x+1) + ' iterations: ', error_dec_avg)\n\n\n#SVR\n# The plot again\nfig2, ax = plt.subplots()\nplt.title('Comparison of predicted and actual values in Support Vector Regression', y=1.01, size=16)\nax.scatter(y_test, y_pred_2)\nax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')\nplt.xlim([0, 45])\nplt.ylim([0, 45])\nplt.figtext(.01, .00008, 'Figure 2: Visual representation of comparison between predicted and actual sales when using '\n                      'Support Vector Regression')\nplt.show()\n\n# Averages\nr2_svr_avg = np.average(r2_svr_list)\nprint('Average r2 score for Support Vector Regression over ' + str(x+1) + ' iterations: ', r2_svr_avg)\nerror_svr_avg = np.average(error_svr_list)\nprint('Average Root Mean Square Error score for Support Vector Regression over ' + str(x+1) + ' iterations: ', error_svr_avg)\n\n#Bayesion Ridge\n# The plot again\nfig3, ax = plt.subplots()\nplt.title('Comparison of predicted and actual values in Bayesian Ridge Regression', y=1.01, size=16)\nax.scatter(y_test, y_pred_3)\nax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')\nplt.xlim([0, 45])\nplt.ylim([0, 45])\nplt.figtext(.01, .00008, 'Figure 3: Visual representation of comparison between predicted and actual sales when using '\n                      'Bayesian Ridge Regression')\nplt.show()\n\n# Averages\nr2_bay_avg = np.average(r2_bay_list)\nprint('Average r2 score for Bayesian Ridge Regression over ' + str(x+1) + ' iterations: ', r2_bay_avg)\nerror_bay_avg = np.average(error_bay_list)\nprint('Average Root Mean Square Error score for Bayesian Ridge Regression over ' + str(x+1) + ' iterations: ', error_bay_avg)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}