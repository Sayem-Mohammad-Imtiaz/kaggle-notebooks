{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7eb5c3b2-f270-f800-dd49-1d44a990b319"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"feb48c77-be6b-e4dd-09b0-e3e71ac17963"},"outputs":[],"source":"%matplotlib inline\n\nimport sklearn\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\n\nimport csv\nimport numpy as np\nimport pandas as pd\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, make_scorer, mean_squared_error\n\n# If true, include make and model in Random Forest\n# Shows how much make and model come into play, but when when we calculate prices\n# we should omit to see if the models are overpriced\nincludeMakeAndModel = True\n\n# Number of trees in forest\nnEstimators = 500\n\ndef GetDataMatrix():\n    \n    # Data frame with make and model\n    Xmodelmake = pd.read_csv(\"../input/data.csv\",header=0, usecols=(0,1,2,3,4,5,6,7,8,9,10,11,13,14,));\n    \n    # Excluding make and model\n    if not includeMakeAndModel:\n        X = pd.read_csv(\"../input/data.csv\",header=0, usecols=(2,3,4,5,6,7,8,9,10,11,13,14,));\n    else:\n        X = Xmodelmake\n    Y = pd.read_csv(\"../input/data.csv\",header=0, usecols=(15,));\n\n    X, Y, Xmodelmake = shuffle(X, Y, Xmodelmake)\n    Xmake = Xmodelmake['Make']\n    Xmodel = Xmodelmake['Model']\n    \n    # Turns categorical data into binary values across many columns\n    if not includeMakeAndModel:\n        X = pd.get_dummies(X, dummy_na = False, columns=['Engine Fuel Type', 'Transmission Type', 'Driven_Wheels', 'Market Category', 'Vehicle Size', 'Vehicle Style'] );\n    else:    \n        X = pd.get_dummies(X, dummy_na = False, columns=['Make', 'Model', 'Engine Fuel Type', 'Transmission Type', 'Driven_Wheels', 'Market Category', 'Vehicle Size', 'Vehicle Style'] );\n    \n    X.insert(0, 'ModelRef', Xmodel);\n    X.insert(0, 'MakeRef', Xmake);\n    \n    # Fill the null values with zeros\n    X.fillna(0, inplace=True);\n    return (X, Y, Xmodelmake)\n\n##########\n\n(X, Y, Xmodelmake) = GetDataMatrix() #Gets the X,Y\n\n# Turn into a proper one D arrayY = numpy.ravel(Y);\nY_unraveled = np.ravel(Y);\n\n# Split dataset into training and testing\nprint('Splitting into training and testing...')\nX_train, X_test, Y_train, y_test = train_test_split(X, Y_unraveled, test_size=0.10, random_state=32)\nMSE_Scorer = make_scorer(mean_squared_error);\n\n# Model/Make columns are only used later on to relate indices to Model/Makes\nX_train2 = X_train.drop('MakeRef', axis = 1).drop('ModelRef', axis = 1)\nX_test2 = X_test.drop('MakeRef', axis = 1).drop('ModelRef', axis = 1)\n\n# Train using Random Forest\nprint('Training classifier...')\nclf = RandomForestRegressor(n_estimators=nEstimators, max_features=\"sqrt\");\n# The gradient boosting classifier didnt finish running\n# clf = GradientBoostingClassifier(n_estimators=5)\nclf = clf.fit(X_train2, Y_train);\nprint(\"Done training best classifier.\")\n\nprint('Calculating error...')\ny_pred = clf.predict(X_test2);\nscores = cross_val_score(clf,X_test2,y_test, cv = 5)\nprint()\n\nprint(\"Scores:\")\nprint(scores);\nprint(\"Mean absolute error:\");\nmean_error = sum(abs(y_test-y_pred))/len(y_test);\nprint(mean_error);\nprint(\"Mean percent error: \")\nprint(mean_error/np.mean(y_test))\nprint()\n\nprint(\"ypred:\");\nprint(y_test);\nprint(y_pred);\nnp.savetxt(\"ypred_test.csv\",(y_pred,y_test),delimiter=\",\");\nprint()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a1266613-0802-a769-b84f-65529b025de7"},"outputs":[],"source":"# If we used JSON file, this would've been easier\n# This code is trying to get the data off Kaggle and making the make and popularities unique\n# since the data can list them multiple times\n\n# Make elements in cars unique and return in same order\ncars = np.asarray(Xmodelmake['Make'])\nuniquecarindices = np.unique(cars, return_index=True)[1]\ncars = np.asarray([cars[index] for index in sorted(uniquecarindices)])\n\n# Make elements in popularities unique and return in same order\npopularities = np.asarray(Xmodelmake['Popularity'])\nuniquepopularityindices = np.unique(popularities, return_index=True)[1]\npopularities = np.asarray([popularities[index] for index in sorted(uniquepopularityindices)])\n\n# Get the indices sorted on popularities from highest to lowest\npopindices = np.argsort(popularities)[::-1]\n\n# Data range\ntotalN = popindices.shape[0]\n\nfigsize = (8,6)\nplt.figure(figsize=figsize)\nplt.title(\"Car Popularities\")\nplt.bar(range(totalN), popularities[popindices], color=\"b\", align=\"center\")\nplt.xticks(rotation=90)\nplt.xticks(range(totalN), cars[popindices])\nplt.xlim([-1, totalN])\nplt.xlabel('Car Models')\nplt.ylabel('Popularity')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f917e898-a680-31eb-3e7a-7216cc86719c"},"outputs":[],"source":"# Important questions to answer\n\n# 1. What features most predict price?\n\n# Get the importances and calculate standard deviations for each\nimportances = clf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf.estimators_],axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Get the feature names\nfeatures = X_test2.columns.values\n\n# Want the top 20 features, so limit the indices and labels\ntopLimit = 20 # limit to show up to, ex. top 10\nindices = indices[0: topLimit] # indices for features\ntopLabels = features[indices[0: topLimit]] # actual feature labels, we want to print these\n\n# Plot the feature importances of the forest (top 20)\nfigsize = (8,6)\nplt.figure(figsize=figsize)\nplt.title(\"Top 20 Important Features\")\nax = plt.bar(range(topLimit), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(rotation=90)\nplt.xticks(range(topLimit), topLabels)\nplt.xlim([-1, topLimit])\nplt.xlabel('Features')\nplt.ylabel('Importance')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8104801-bcc3-728f-1747-431e53bd794f"},"outputs":[],"source":"# 2. What cars are the most over-priced for their feature set?\n\n# Get the errors from the prediction and sort from greatest to least\ny_error = y_test-y_pred\nold_indices = np.argsort(y_error)[::-1] # returns the old indices\n\n# Put top 10 overpriced cars into a list\nmodelmakelist = []\nN = 10 # number of top values to extract\nfor i in range(N):\n    modelmakelist.append(X_test['MakeRef'].iloc[old_indices[i]]\n                         + ' ' + X_test['ModelRef'].iloc[old_indices[i]]\n                         + ' ' + str(X['Year'].iloc[old_indices[i]]))\nmodelmakelist = np.asarray(modelmakelist) # don't index into original\n\n# Plot the top 10 overpriced cars against their price\nfigsize = (8,6)\nplt.figure(figsize=figsize)\nplt.title(\"Top 10 Overpriced Cars\")\nplt.bar(range(N), y_error[old_indices[0:N]], color=\"b\", align=\"center\")\nplt.xticks(rotation=90)\nplt.xticks(range(N), modelmakelist)\nplt.xlim([-1, N])\nplt.xlabel('Car Make and Model')\nplt.ylabel('Price')\nplt.show()\n\n# Put top 10 overpriced brands into a list\n# Scan all entries, if maker already exists, go to next entry, else add maker to list\nexistingmakers = []\npricelist = []\nfor i in range(old_indices.shape[0]):\n    currentmaker = X_test['MakeRef'].iloc[old_indices[i]]\n    if currentmaker not in existingmakers:\n        existingmakers.append(currentmaker)\n        pricelist.append(y_error[old_indices[i]])\n        if len(existingmakers) == N:\n            break\n\nexistingmakers = np.asarray(existingmakers)\npricelist = np.asarray(pricelist)\n    \nfigsize = (8,6)\nplt.figure(figsize=figsize)\nplt.title(\"Top 10 Overpriced Car Brands:\")\nplt.bar(range(N), pricelist, color=\"g\", align=\"center\")\nplt.xticks(rotation=90)\nplt.xticks(range(N), existingmakers)\nplt.xlim([-1, N])\nplt.xlabel('Car Brand')\nplt.ylabel('Price')\nplt.show()"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}