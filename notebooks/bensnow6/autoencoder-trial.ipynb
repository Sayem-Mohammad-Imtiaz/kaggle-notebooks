{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"### \\author Ben Snow\n### \\version 1.0\n### \\date Last Revision 18/11/2019 \\n\n\n### \\class ASE Autumn 2019\n### \\brief \n### @brief Simple autoencoder trained to recreate the mnist dataset\n### Modified from :-\n### Francois Chollet (14 May 2016). Building Autoencoders in Keras [online].\n### [Accessed 2019]. Available from: \"https://blog.keras.io/building-autoencoders-in-keras.html\".\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport keras\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"encoding_dim = 32 #number dimensions of encoded representation\n\ninput_img = Input(shape=(784,))  #input image dimensions\n\nencoded = Dense(encoding_dim, activation='relu')(input_img) #encoded representation is stored here\n\ndecoded = Dense(784, activation='sigmoid')(encoded) #decoded representation is stored here\n\nautoencoder = Model(input_img, decoded) #this is the autoencoder! It encodes then decodes the data to reproduce the input image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = Model(input_img, encoded) #maps the input to the reduced representation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_input = Input(shape=(encoding_dim,)) #declaring a variable for holding the reduced representation\n\ndecoded_layer = autoencoder.layers[-1] #isolating the last layer of the autoencoder (to be used as the last layer of the decoder)\n\ndecoder = Model(encoded_input, decoded_layer(encoded_input)) #creates the decoder model with dimensions defined by the size of the reduced representation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder.compile(optimizer = 'adadelta', loss='binary_crossentropy') #configuring the optimiser and the loss function for the model\n#adadelta is an adaptive gradient descent optimiser: https://arxiv.org/abs/1212.5701\n#binary crossentropy loss function is used since the mnist dataset is being used\n#mnist has 2 categories (black + white), hence 'binary': https://bit.ly/2PyiCst (derivation of loss fn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = pd.read_csv(\"../input/mnist-in-csv/mnist_test.csv\") #reading in pre-split testing and training data from the mnist data set\nx_train = pd.read_csv(\"../input/mnist-in-csv/mnist_train.csv\")\nx_test = x_test.drop(['label'], axis=1)\nx_train = x_train.drop(['label'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train.astype('float32')/255. #normalising the training and testing data\nx_test = x_test.astype('float32')/255.\n#x_train.shape\nx_train = pd.DataFrame(x_train).to_numpy()\nx_test = pd.DataFrame(x_test).to_numpy()\n\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1]))) #reshaping the data set to the dimensions of the autoencoder (784 nodes)\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1])))\nprint (x_train.shape)\nprint (x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"autoencoder.fit(x_train, x_train, epochs=5, batch_size=50, shuffle=True, validation_data=(x_test,x_test)) #training the autoencoder for 5 epochs splitting the data into shuffled batches of 50 images\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_imgs = encoder.predict(x_test)  #extracting the encoded and decoded images\ndecoded_imgs = decoder.predict(encoded_imgs)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n#comparing the first 10 input images vs. their corresponding decoded images after being fed through the trained network\n\nn=10\nplt.figure(figsize=(20,4))\nfor i in range(n):\n    ax=plt.subplot(2, n, i+1)\n    plt.imshow(x_test[i].reshape(28,28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    \n    ax=plt.subplot(2, n, i+1+n)\n    plt.imshow(decoded_imgs[i].reshape(28,28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}