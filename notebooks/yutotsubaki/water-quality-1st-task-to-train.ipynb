{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-22T09:14:13.272196Z","iopub.execute_input":"2021-08-22T09:14:13.272716Z","iopub.status.idle":"2021-08-22T09:14:13.282208Z","shell.execute_reply.started":"2021-08-22T09:14:13.272673Z","shell.execute_reply":"2021-08-22T09:14:13.280946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### https://www.kaggle.com/adityakadiwal/water-potability\n\n1. Predict if water is safe for Human consumption\n2. EDA for water potability","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\n\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-08-22T09:14:13.283817Z","iopub.execute_input":"2021-08-22T09:14:13.284196Z","iopub.status.idle":"2021-08-22T09:14:13.294058Z","shell.execute_reply.started":"2021-08-22T09:14:13.28416Z","shell.execute_reply":"2021-08-22T09:14:13.292789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### First, We try 1st task\n\n#### so, we have to make model to predict Potability whether 1 or 0","metadata":{}},{"cell_type":"code","source":"water_potability = pd.read_csv('/kaggle/input/water-potability/water_potability.csv')\nwater_potability.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-22T09:14:13.297286Z","iopub.execute_input":"2021-08-22T09:14:13.297924Z","iopub.status.idle":"2021-08-22T09:14:13.339547Z","shell.execute_reply.started":"2021-08-22T09:14:13.297883Z","shell.execute_reply":"2021-08-22T09:14:13.338491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"water_potability.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-22T09:14:13.341165Z","iopub.execute_input":"2021-08-22T09:14:13.341546Z","iopub.status.idle":"2021-08-22T09:14:13.385396Z","shell.execute_reply.started":"2021-08-22T09:14:13.3415Z","shell.execute_reply":"2021-08-22T09:14:13.384373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's seek information\nwater_potability.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-22T09:14:13.386859Z","iopub.execute_input":"2021-08-22T09:14:13.387219Z","iopub.status.idle":"2021-08-22T09:14:13.401047Z","shell.execute_reply.started":"2021-08-22T09:14:13.387177Z","shell.execute_reply":"2021-08-22T09:14:13.399987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look null percentage\nwater_potability.isnull().sum() / len(water_potability) * 100","metadata":{"execution":{"iopub.status.busy":"2021-08-22T09:14:13.402709Z","iopub.execute_input":"2021-08-22T09:14:13.402971Z","iopub.status.idle":"2021-08-22T09:14:13.41688Z","shell.execute_reply.started":"2021-08-22T09:14:13.402945Z","shell.execute_reply":"2021-08-22T09:14:13.41592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"there are many nulls in some columns\n\nso, we have to fill those null with some representive value(mean? median? predictions??)\n\n\nbecause the each rate of null is higher than 1%...","metadata":{}},{"cell_type":"code","source":"# to decide the value, we have to know how this data look like\n\nvar = 'ph'\nsns.displot(x=var, data=water_potability, kde=True, bins=30)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T09:14:13.418201Z","iopub.execute_input":"2021-08-22T09:14:13.418708Z","iopub.status.idle":"2021-08-22T09:14:13.730983Z","shell.execute_reply.started":"2021-08-22T09:14:13.418668Z","shell.execute_reply":"2021-08-22T09:14:13.7301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PH is great normal distributions!!\n\nkeep seeking and show all in bulk","metadata":{}},{"cell_type":"code","source":"columns = water_potability.columns\n\nfig, axes = plt.subplots(3, 3, figsize=(10, 10))\n\n\nfrom itertools import product\n\nproduct_indexes = product([0, 1, 2], [0, 1, 2])\n\nfor idx, col in zip(product_indexes, columns):\n    sns.histplot(x=col, data=water_potability, kde=True, bins=30, ax=axes[idx[0], idx[1]])","metadata":{"execution":{"iopub.status.busy":"2021-08-22T09:14:13.732375Z","iopub.execute_input":"2021-08-22T09:14:13.732786Z","iopub.status.idle":"2021-08-22T09:14:15.306039Z","shell.execute_reply.started":"2021-08-22T09:14:13.732746Z","shell.execute_reply":"2021-08-22T09:14:15.305005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### all right, every parameters distributes normal distribution\n#### so, I think I can fill the null with each mean value ","metadata":{}},{"cell_type":"code","source":"# fillna\nwater_potability.ph = water_potability.ph.fillna(np.mean(water_potability.ph))\nwater_potability.Sulfate = water_potability.Sulfate.fillna(np.mean(water_potability.Sulfate))\nwater_potability.Trihalomethanes = water_potability.Trihalomethanes.fillna(np.mean(water_potability.Trihalomethanes))\n\nwater_potability.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-22T09:14:15.307459Z","iopub.execute_input":"2021-08-22T09:14:15.30784Z","iopub.status.idle":"2021-08-22T09:14:15.319769Z","shell.execute_reply.started":"2021-08-22T09:14:15.307797Z","shell.execute_reply":"2021-08-22T09:14:15.318971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"all right! we've done!","metadata":{}},{"cell_type":"code","source":"# Potability hist\nsns.countplot(x='Potability', data=water_potability)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T09:14:15.321921Z","iopub.execute_input":"2021-08-22T09:14:15.322313Z","iopub.status.idle":"2021-08-22T09:14:15.439359Z","shell.execute_reply.started":"2021-08-22T09:14:15.322275Z","shell.execute_reply":"2021-08-22T09:14:15.438723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Next, we seek correlation because we have to care about multico","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(10, 10))\ncormat = water_potability.corr()\n\nsns.heatmap(cormat, annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T09:14:15.440668Z","iopub.execute_input":"2021-08-22T09:14:15.441078Z","iopub.status.idle":"2021-08-22T09:14:16.249706Z","shell.execute_reply.started":"2021-08-22T09:14:15.441052Z","shell.execute_reply":"2021-08-22T09:14:16.248859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### OK! We can probably use all columns!\n#### So, we try to predict potability once","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\n\nX = water_potability.drop('Potability', axis=1).values\ny = water_potability['Potability'].values","metadata":{"execution":{"iopub.status.busy":"2021-08-22T09:14:16.250925Z","iopub.execute_input":"2021-08-22T09:14:16.251192Z","iopub.status.idle":"2021-08-22T09:14:16.255614Z","shell.execute_reply.started":"2021-08-22T09:14:16.251168Z","shell.execute_reply":"2021-08-22T09:14:16.255049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_scaled = scaler.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T09:14:16.25649Z","iopub.execute_input":"2021-08-22T09:14:16.256894Z","iopub.status.idle":"2021-08-22T09:14:16.271655Z","shell.execute_reply.started":"2021-08-22T09:14:16.256869Z","shell.execute_reply":"2021-08-22T09:14:16.270936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T09:14:16.272724Z","iopub.execute_input":"2021-08-22T09:14:16.27296Z","iopub.status.idle":"2021-08-22T09:14:16.283538Z","shell.execute_reply.started":"2021-08-22T09:14:16.272937Z","shell.execute_reply":"2021-08-22T09:14:16.283011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = cross_val_score(estimator=LogisticRegression(), \n                        X=X_train, y=y_train, \n                        scoring='accuracy',\n                        cv=10, n_jobs=1)\n\nprint(f'First Accuracy Try is {np.mean(scores):.3f} +/- {np.std(scores)}')","metadata":{"execution":{"iopub.status.busy":"2021-08-22T09:14:16.284444Z","iopub.execute_input":"2021-08-22T09:14:16.284683Z","iopub.status.idle":"2021-08-22T09:14:16.376213Z","shell.execute_reply.started":"2021-08-22T09:14:16.284661Z","shell.execute_reply":"2021-08-22T09:14:16.375257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### So, only LR is not enough to predict\n#### we try lightGBM or some ensemble or PCA action","metadata":{}},{"cell_type":"code","source":"# we have to use only train data so we use kfold\nkfold = StratifiedKFold(n_splits=10).split(X_train, y_train)\n\n\nscores = []\n\nfor (train, test) in kfold:\n    # firstly we set tempolary.\n    params = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'multiclass',\n        'num_class': 2\n    }\n\n    train_data = lgb.Dataset(X_train[train], label=y_train[train])\n    eval_data = lgb.Dataset(X_train[test], label=y_train[test], reference= train_data)\n    \n    gbm = lgb.train(\n        params,\n        train_data,\n        valid_sets=eval_data,\n        num_boost_round=50\n    )\n    \n\n    preds = gbm.predict(X_train[test])\n    y_pred = []\n    for x in preds:\n      y_pred.append(np.argmax(x))\n    \n    score = accuracy_score(y_train[test], y_pred)\n    scores.append(score)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T09:14:16.377907Z","iopub.execute_input":"2021-08-22T09:14:16.378617Z","iopub.status.idle":"2021-08-22T09:14:17.943619Z","shell.execute_reply.started":"2021-08-22T09:14:16.378557Z","shell.execute_reply":"2021-08-22T09:14:17.942802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.mean(scores)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T09:14:17.946362Z","iopub.execute_input":"2021-08-22T09:14:17.946644Z","iopub.status.idle":"2021-08-22T09:14:17.951263Z","shell.execute_reply.started":"2021-08-22T09:14:17.946616Z","shell.execute_reply":"2021-08-22T09:14:17.950657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### well we should keep trying to predict more collectly.","metadata":{}},{"cell_type":"markdown","source":"### We use RandomizeSeach and ensemble to find the best hyperparameters!","metadata":{}},{"cell_type":"markdown","source":"#### still not enough accuracy","metadata":{}},{"cell_type":"code","source":"estimators = Pipeline([\n    ('pca', PCA()),\n    ('lr', LogisticRegression(max_iter=1000))\n])\n\nparam_dist = [\n    {'lr__penalty': ['l1', 'l2']},\n    {'lr__C': [0.01, 0.1, 1.0, 10]},\n#     {'lr__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']},\n    {'pca__n_components': [2, 3, 4, 5, 6]}\n]\n\nrs = RandomizedSearchCV(estimator=estimators, \n                       param_distributions=param_dist,\n                       scoring='accuracy', n_iter=100, \n                       cv=10, refit=True)\n\nrs = rs.fit(X_train, y_train)\n\nprint(f\"best param's are {rs.best_params_}\")\n\nrs.best_score_","metadata":{"execution":{"iopub.status.busy":"2021-08-22T09:14:17.952201Z","iopub.execute_input":"2021-08-22T09:14:17.952438Z","iopub.status.idle":"2021-08-22T09:14:19.603618Z","shell.execute_reply.started":"2021-08-22T09:14:17.952415Z","shell.execute_reply":"2021-08-22T09:14:19.602609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### I'm getting to think about those features can't help the model's accuary growing..\n\n### So, we move to use ensemble","metadata":{}},{"cell_type":"code","source":"\nestimators = [\n   ('rf', RandomForestClassifier(n_estimators=100)),\n   ('pipeline', Pipeline([\n                        ('pca', PCA(n_components=2)),\n                        ('lr', LogisticRegression(max_iter=500))\n                    ])\n   )\n]\n\n\n\nstack = StackingClassifier(estimators=estimators, \n                           final_estimator=LogisticRegression(max_iter=1000))\n\nscores = cross_val_score(estimator=stack,\n                        X=X_train, y=y_train,\n                        cv=10, scoring='accuracy')\n\nnp.mean(scores)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T09:14:19.608Z","iopub.execute_input":"2021-08-22T09:14:19.610323Z","iopub.status.idle":"2021-08-22T09:15:00.502221Z","shell.execute_reply.started":"2021-08-22T09:14:19.61027Z","shell.execute_reply":"2021-08-22T09:15:00.501263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Yes! a little higher..","metadata":{}},{"cell_type":"code","source":"# let's try use test data\n\nscores = cross_val_score(estimator=stack,\n                        X=X_test, y=y_test,\n                        cv=10, scoring='accuracy')\n\nnp.mean(scores)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T09:15:00.503376Z","iopub.execute_input":"2021-08-22T09:15:00.503902Z","iopub.status.idle":"2021-08-22T09:15:14.534139Z","shell.execute_reply.started":"2021-08-22T09:15:00.503862Z","shell.execute_reply":"2021-08-22T09:15:14.53329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### That's not overfitting\n### So, I suspend 1st task.","metadata":{}}]}