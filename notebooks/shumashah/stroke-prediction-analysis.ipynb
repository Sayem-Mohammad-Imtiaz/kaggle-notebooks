{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pylab inline\n%config InlineBackend.figure_formats = ['retina']\n\nimport pandas as pd\nimport seaborn as sns\nsns.set()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sqlite3 as sq3\nimport pandas.io.sql as pds\nimport pandas as pd\n# standard\nimport numpy as np\nimport pandas as pd\nimport time\n\n# plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# statistics tools\nfrom statsmodels.graphics.mosaicplot import mosaic\n\nimport os\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold, cross_val_predict\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncd Users/syedwaqar/Huma # to change directory ","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pwd # to check directory","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = 'healthcare-dataset-stroke-data.csv'\ncon = sq3.Connection(path)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nos.path.isfile('healthcare-dataset-stroke-data.csv')","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"con","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame([[1,0,5],[0,1,4]])\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(path)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.sep # to check the separator used for speciifying the file location\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#feature engineering \ndata.info()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get a Pd.Series consisting of all the string categoricals\none_hot_encode_cols = data.dtypes[data.dtypes == np.object]  # filtering by string categoricals\none_hot_encode_cols = one_hot_encode_cols.index.tolist()  # list of categorical fields\n\ndata[one_hot_encode_cols].head().T","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.get_dummies(data, columns=one_hot_encode_cols, drop_first=True)\ndata.describe().T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.dropna(inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()\n","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nX = data.drop('stroke', axis=1)\ny = data.stroke","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kf = KFold(shuffle=True, random_state=72018, n_splits=3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for train_index, test_index in kf.split(X):\n    print(\"Train index:\", train_index[:10], len(train_index))\n    print(\"Test index:\",test_index[:10], len(test_index))\n    print('')","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.iloc[train_index, :]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.iloc[test_index, :]","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.iloc[train_index]","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.metrics import r2_score, mean_squared_error\n\n\nscores = []\nlr = LinearRegression()\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test, y_train, y_test = (X.iloc[train_index, :], \n                                        X.iloc[test_index, :], \n                                        y.iloc[train_index], # imp to use '.iloc' for speciifying y train index \n                                        y.iloc[test_index])\n    \n    lr.fit(X_train, y_train)\n        \n    y_pred = lr.predict(X_test)\n\n    score = r2_score(y_test.values, y_pred)\n    \n    scores.append(score)\n    \nscores # the closer to 1 is the r2 score, the better are we in explaining the variance","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()# only bmi had null vaules and we deleted them ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_squared_error(y_test.values, y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask = data.dtypes == np.float\nfloat_cols = data.columns[mask]\n\nskew_limit = 0.75 # define a limit above which we will log transform\nskew_vals = X[float_cols].skew()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skew_cols = (skew_vals\n             .sort_values(ascending=False)\n             .to_frame()\n             .rename(columns={0:'Skew'})\n             .query('abs(Skew) > {}'.format(skew_limit)))\n\nskew_cols","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choose a field\nfield = [\"avg_glucose_level\", \"bmi\"]\n\n\n# Create two \"subplots\" and a \"figure\" using matplotlib\nfig, (ax_before, ax_after) = plt.subplots(2, 2, figsize=(10, 5))\n\n# Create a histogram on the \"ax_before\" subplot\nX[field].hist(ax=ax_before)\n\n# Apply a log transformation (numpy syntax) to this column\nX[field].apply(np.log1p).hist(ax=ax_after)\n# Formatting of titles etc. for each subplot\nax_before.set(title='before np.log1p', ylabel='frequency', xlabel='value')\nax_after.set(title='after np.log1p', ylabel='frequency', xlabel='value')\nfig.suptitle('Field \"{}\"'.format(field));","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smaller_df= X.loc[:,field]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we can look at summary statistics of the subset data\nsmaller_df.describe().T","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smaller_df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(smaller_df, plot_kws=dict(alpha=.1, edgecolor='none')) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.metrics import r2_score, mean_squared_error\n\n\nscores = []\nlr = LinearRegression()\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test, y_train, y_test = (X.iloc[train_index, :], \n                                        X.iloc[test_index, :], \n                                        y.iloc[train_index], # imp to use '.iloc' for speciifying y train index \n                                        y.iloc[test_index])\n    \n    lr.fit(X_train, y_train)\n        \n    y_pred = lr.predict(X_test)\n\n    score = r2_score(y_test.values, y_pred)\n    \n    scores.append(score)\n    \nscores # the closer to 1 is the r2 score, the better are we in explaining the variance","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_squared_error(y_test.values, y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = []\n\nlr = LinearRegression()\ns = StandardScaler()\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test, y_train, y_test = (X.iloc[train_index, :], \n                                        X.iloc[test_index, :], \n                                        y.iloc[train_index], \n                                        y.iloc[test_index])\n    \n    X_train_s = s.fit_transform(X_train)\n    \n    lr.fit(X_train_s, y_train)\n    \n    X_test_s = s.transform(X_test)\n    \n    y_pred = lr.predict(X_test_s)\n\n    score = r2_score(y_test.values, y_pred)\n    \n    scores.append(score)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores # these have improved very minutely and have increased little bit from \n# [0.08851187144854655, 0.060816944650728955, 0.06733740589360682] to below values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_squared_error(y_test.values, y_pred) # error has decresead very minutely from '0.039239837981126346' to below ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hyperparameter tuning with scaling, polynomial \n\npf = PolynomialFeatures()\nmse = []\nscores = []\nalphas = np.geomspace(1e-9, 1e0, num=10)\nfor alpha in alphas:\n    las = Lasso(alpha=alpha, max_iter=100000)\n    \n    estimator = Pipeline([\n        (\"scaler\", s),\n        (\"make_higher_degree\", pf),\n        (\"lasso_regression\", las)])\n\n    predictions = cross_val_predict(estimator, X, y, cv = kf)\n    \n    score = r2_score(y, predictions)\n    m_s_e = mean_squared_error(y, predictions)\n    scores.append(score)\n    mse.append(m_s_e)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(zip(alphas,scores)) # here we can see that 0.06 is the best alpha as it gives us the most score ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.semilogx(alphas, scores);","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(zip(alphas,mse))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.semilogx(alphas, mse);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Same estimator as before\nestimator = Pipeline([(\"scaler\", StandardScaler()),\n        (\"polynomial_features\", PolynomialFeatures()),\n        (\"lasso_regression\", Lasso(alpha=0.01))])\n\nparams = {\n    'polynomial_features__degree': [1, 2, 3]\n}\n\ngrid = GridSearchCV(estimator, params, cv=kf)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid.fit(X, y)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid.best_score_, grid.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predict = grid.predict(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r2_score(y, y_predict)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_squared_error(y, y_predict)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid.best_estimator_.named_steps['lasso_regression'].coef_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid.cv_results_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid.best_estimator_.named_steps[\"polynomial_features\"].get_feature_names(input_features=X.columns)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_importances = pd.DataFrame(zip(grid.best_estimator_.named_steps[\"polynomial_features\"].get_feature_names(input_features=X.columns),\n                 grid.best_estimator_.named_steps[\"lasso_regression\"].coef_,\n))","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_names_dict = dict(zip(list(range(len(X.columns.values))), X.columns.values))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_names_dict","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_importances.sort_values(by=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Same estimator as before\nestimator = Pipeline([(\"scaler\", StandardScaler()),\n        (\"polynomial_features\", PolynomialFeatures()),\n        (\"ridge_regression\", Ridge())])\n\nparams = {\n    'polynomial_features__degree': [1, 2, 3],\n    'ridge_regression__alpha': np.geomspace(1e-9, 1e0, num=10)\n}\n\ngrid = GridSearchCV(estimator, params, cv=kf)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid.fit(X, y)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid.best_score_, grid.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predict = grid.predict(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r2_score(y, y_predict)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_squared_error(y, y_predict)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid.best_estimator_.named_steps[\"polynomial_features\"].get_feature_names(input_features=X.columns)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_importances = pd.DataFrame(zip(grid.best_estimator_.named_steps[\"polynomial_features\"].get_feature_names(input_features=X.columns),\n                 grid.best_estimator_.named_steps[\"ridge_regression\"].coef_,\n))","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_names_dict = dict(zip(list(range(len(X.columns.values))), X.columns.values))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_names_dict ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_importances.sort_values(by=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(30,30))\nsns.pairplot(data)\nplt.show()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#After checking the models based on the error values calculated. I found that lasso model was the best suitable model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}