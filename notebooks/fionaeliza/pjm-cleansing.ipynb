{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PJM Data Analysis\nBy Fiona Dean\n\nKaggle has kindly hosted a large data set including 20 years worth of electrical consumption from the PJM company. The data is presented by Business Unit and has a list of hourly consumption for the timeperiod the Business Unite existed. Since electrical consumption often correlates to temperature and working days, we can find further data sets as follows.\n\nThe PJM Data set can be downloaded here: \n    https://www.kaggle.com/robikscube/hourly-energy-consumption\nThe Daily Temperature of major cities can be downloaded here: \n    https://www.kaggle.com/sudalairajkumar/daily-temperature-of-major-cities\nThe major US holidays I have compiled from two sources: \n    https://www.public-holidays.us/mobile_US_KF_1998_Federal%20holidays and https://www.timeanddate.com/holidays/us/\nTo read more about the company, please follow the link: \n    https://www.pjm.com/\n\n# Steps to be completed during import and cleansing:\n\n1. clean up each data set\n    This includes checking each file from file from PJM, ensuring that there are no missing hours, and no duplicated hours. If found, the average of the two lines of data surround the situation will be taken and then averaged.\n    This also includes finding the dates of all major US holidays.\n    \n2. combine the data sets into one large pandas dataframe\n\n\nTo set up the environment, we need to import the following libraries. We will also wish to include the datetime library with timedelta, so that we can quickly and accurately assess any cleansing issues in the data. ","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport datetime\nfrom datetime import timedelta\nimport seaborn as sns\nimport statsmodels.api as sm\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nsns.set_style('darkgrid')\nmpl.rcParams['figure.figsize'] = (20,5)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T19:47:26.829132Z","iopub.execute_input":"2021-09-06T19:47:26.829591Z","iopub.status.idle":"2021-09-06T19:47:26.836964Z","shell.execute_reply.started":"2021-09-06T19:47:26.829558Z","shell.execute_reply":"2021-09-06T19:47:26.835543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To quickly check to see if there are any issues in a PJM file, we can create a function that imports a dataframe, df, and then checks for a duration other than an hour. It can be run with each file after basic processing to make a dataframe and after converting the datetime string into a datetime format. It can also be used to check the final combined dataframe.","metadata":{}},{"cell_type":"code","source":"def check_df(df):\n    print('checking for time related errors...')\n    dup = df.duplicated(subset = 'Datetime')\n    print(df[dup]) #prints a list of duplicated times from the imported dataframe.\n    prev = ''\n    for ind, row in df.iterrows():\n        now = row[0]\n        hour = timedelta(hours = 1)\n        duration = now - prev if prev != '' else ''\n        if duration != '':\n            if duration != hour:\n                print('\\these values have a duration other than 1 hr', ind)\n        prev = now","metadata":{"execution":{"iopub.status.busy":"2021-09-06T19:47:29.92407Z","iopub.execute_input":"2021-09-06T19:47:29.924418Z","iopub.status.idle":"2021-09-06T19:47:29.930779Z","shell.execute_reply.started":"2021-09-06T19:47:29.924389Z","shell.execute_reply":"2021-09-06T19:47:29.929752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This function, process_data(filename) can be used with the raw datafile to return a dataframe without time related error. \n\nFirst it imports the raw data file, converts the Datetime column to a Datetime format using pd.to_datetime, sorts the data by time, and then reindexes.\n\nWe iterrate through each row of data. So that we don't have to do this multiple times, we will check for duplicated timestamps with a gap duration of zero and then check for gaps with a duration greater than 1 hour. Finally we conclude by checking for gaps less than an hour. You'll notice, there are none of these, however we have a print statement just in case. We will not delete any rows or add any rows until we complete the iteration, since we do not want to destroy the row indexing. After completion, we return the dataframe. We will use this process_data(filename) function on each file.\n","metadata":{}},{"cell_type":"code","source":"# use to upload and process csv files in pd.dataframes\ndef process_data(filename):\n    file_string = '../input/hourly-energy-consumption/'+filename\n    df = pd.read_csv(file_string)                                                      #open file\n    df['Datetime'] = pd.to_datetime(df['Datetime'], format='%Y-%m-%d %H:%M:%S')     #convert to timestamp\n    df.sort_values('Datetime', inplace=True)                                        #sort by time\n    df.reset_index(drop=True, inplace=True)                                         #reset the index\n\n    hour = timedelta(hours=1)\n    colname = list(df.columns.values)[1]\n    prev = ''\n    prev_data = ''\n    dates_data_to_add = []\n\n    for ind, row in df.iterrows():\n        now = row[0]\n        now_data = row[1]\n        duration = now - prev if prev != '' else ''\n        if duration != '':\n            if duration != hour:\n                if duration == timedelta(0):               #this is a duplicate\n                    avg = (now_data + prev_data)/2         #find average of the two duplicates\n                    df.iat[ind-1, 1] = avg                 #replace the data in prev\n                    df.iat[ind, 1] = avg                   #replace the data in now\n                    #delete the duplicates at end so that you're not changing indeces during iterration\n                elif duration > hour:\n                    days_to_hours = int(duration.days) * 24\n                    seconds_to_hours = int(duration.seconds/3600)\n                    total_hours = days_to_hours + seconds_to_hours\n                    avg = (now_data + prev_data)/2\n                    for i in range(total_hours-1):\n                        dates_data_to_add.append([prev + hour*(i+1),avg])\n                        #add these later so you're not changing indeces during iterration\n                elif duration < hour:\n                    print('TODO print these and decide what to do with them', now, now_data)\n        prev = now #set prev for the next iterration\n        prev_data = now_data #set prev_data for the next iterration\n\n    df.drop_duplicates(subset =['Datetime'], inplace=True)\n    df_add = pd.DataFrame(dates_data_to_add, columns = ['Datetime', colname])\n    df = df.append(df_add, ignore_index=True)\n    df.sort_values('Datetime', inplace=True) #sort by time\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-06T19:49:05.775649Z","iopub.execute_input":"2021-09-06T19:49:05.776024Z","iopub.status.idle":"2021-09-06T19:49:05.789277Z","shell.execute_reply.started":"2021-09-06T19:49:05.775993Z","shell.execute_reply":"2021-09-06T19:49:05.787966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will go through the current directory to find each file, process it using process_data(filename) and then combine each dataframe together to create a combo dataframe.","metadata":{}},{"cell_type":"code","source":"#list of files to upload\nfilenames = ['AEP_hourly.csv', 'COMED_hourly.csv', 'DAYTON_hourly.csv', 'DEOK_hourly.csv', 'DOM_hourly.csv', 'DUQ_hourly.csv', 'EKPC_hourly.csv', 'FE_hourly.csv', 'NI_hourly.csv', 'PJM_Load_hourly.csv', 'PJME_hourly.csv', 'PJMW_hourly.csv']\n\n#add first csv file as dataframe to combo\ncombo = process_data(filenames[0])\n\n#add remaining csv files as dataframes to combo\nfor ind, file in enumerate(filenames):\n    if ind >0:\n        new_df = process_data(file)\n        combo = combo.merge(new_df, on='Datetime', how='outer')\n        combo = combo.sort_values(by=['Datetime'])","metadata":{"execution":{"iopub.status.busy":"2021-09-06T19:49:10.630241Z","iopub.execute_input":"2021-09-06T19:49:10.630615Z","iopub.status.idle":"2021-09-06T19:51:23.882587Z","shell.execute_reply.started":"2021-09-06T19:49:10.630583Z","shell.execute_reply":"2021-09-06T19:51:23.881592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Often it is a good idea to have a column that sums the other columns. While we can also do this later, with such a large dataset it is faster to complete it using the pandas dataframe.","metadata":{}},{"cell_type":"code","source":"data_filter = list(combo.columns.values)[1:]\ncombo_sum = combo[data_filter].sum(axis=1)\ncombo_sum_df = pd.DataFrame({'Datetime':combo['Datetime'],'Row_Total': combo_sum})\ncombo = combo.merge(combo_sum_df, on='Datetime')","metadata":{"execution":{"iopub.status.busy":"2021-09-06T19:51:47.835625Z","iopub.execute_input":"2021-09-06T19:51:47.835997Z","iopub.status.idle":"2021-09-06T19:51:47.908568Z","shell.execute_reply.started":"2021-09-06T19:51:47.835962Z","shell.execute_reply":"2021-09-06T19:51:47.907576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can import the holiday information and attach the appropriate hour timestamps. As these holidays are listed daily, and not hourly, we will also need to add each hourly timestamp. For days that are not a holiday, we can mark them as \"Not Holiday\".","metadata":{}},{"cell_type":"code","source":"#add holiday information\nhour = timedelta(hours=1)\nholiday_filename = '../input/major-us-holidays/holidays_manual.csv'\nholidays = pd.read_csv(holiday_filename)\nholidays = holidays.rename(columns={'Date':'Datetime','Holiday':'Holiday'})\nholidays['Datetime'] = pd.to_datetime(holidays['Datetime'])\nhol_list = []\nfor ind, row in holidays.iterrows():\n    for i in range(24):\n        hol_list.append([row[0]+hour*i, row[1]])\n\nhol_df = pd.DataFrame(hol_list, columns=['Datetime','Holiday'])\nholidays = holidays.append(hol_df)\nholidays = holidays.drop_duplicates('Datetime')\ncombo = combo.merge(holidays, on='Datetime', how='outer')\ncombo = combo.sort_values('Datetime')\ncombo['Holiday'] = combo['Holiday'].fillna('Not Holiday')","metadata":{"execution":{"iopub.status.busy":"2021-09-06T19:51:52.15147Z","iopub.execute_input":"2021-09-06T19:51:52.151832Z","iopub.status.idle":"2021-09-06T19:51:52.368876Z","shell.execute_reply.started":"2021-09-06T19:51:52.151798Z","shell.execute_reply":"2021-09-06T19:51:52.367694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now the combo datafile also includes a 'Holiday' column, with the name of the holiday. Some clean up is needed, as well as a holiday bivariable column that simply states whether it is a holiday or not a holiday.","metadata":{}},{"cell_type":"code","source":"#clean up holiday column (president's vs presidents')\ncombo['Holiday'] = combo['Holiday'].replace({\"New Years Day\":\"New Year's Day\", \"Martin Luther King, Jr. Day\": \"Martin Luther King Jr. Day\",\"President's Day\":\"Presidents' Day\", \"Day off for New Years Day\":\"Day off for New Year's Day\"})\n\n#add holiday variable column\nbi_conditions = [combo['Holiday'] == 'Not Holiday', combo['Holiday'] != 'Not Holiday']\nbi_var = [0,1]\ncombo['Holiday_Variable'] = np.select(bi_conditions,bi_var)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T19:51:57.984967Z","iopub.execute_input":"2021-09-06T19:51:57.985567Z","iopub.status.idle":"2021-09-06T19:51:58.105473Z","shell.execute_reply.started":"2021-09-06T19:51:57.985532Z","shell.execute_reply":"2021-09-06T19:51:58.104428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will want to breakdown data by weekdays to analyze how work days and non-work days use electricity. We can also include a long weekend column to take into account holidays that are on a Monday or Friday.","metadata":{}},{"cell_type":"code","source":"# add weekday and num\ncombo['Weekday'] = combo['Datetime'].dt.day_name()\n\nweekday = [combo['Weekday'] == 'Monday', combo['Weekday'] == 'Tuesday', combo['Weekday'] == 'Wednesday', combo['Weekday'] == 'Thursday', combo['Weekday'] == 'Friday', combo['Weekday'] == 'Saturday', combo['Weekday'] == 'Sunday']\nweekday_num = [0,1,2,3,4,5,6]\ncombo['Weekday_num'] = np.select(weekday, weekday_num)\n\n#add long weekend variable\ncombo['Long Weekend'] = np.where((combo['Holiday'] != 'Not Holiday') & ((combo['Weekday'] == 'Monday') | (combo['Weekday'] == 'Friday')), True, False)\n\nfor ind, row in combo.iterrows():\n    if row['Long Weekend'] == True and row['Weekday'] == 'Friday' and row['Datetime'].hour == 23:\n        for j in range(1, 48):\n            num = int(ind)+int(j)\n            if num < len(combo):\n                if combo.loc[num]['Weekday'] not in ['Tuesday', 'Wednesday', 'Thursday']:\n                    combo.at[num, 'Long Weekend'] = True\n\nfor ind, row in combo.iterrows():\n    if row['Long Weekend'] == True and row['Weekday'] == 'Monday' and row['Datetime'].hour == 0:\n        for j in range(1, 48):\n            num = int(ind) + int(j)\n            if num < len(combo):\n                if combo.loc[num]['Weekday'] not in ['Tuesday', 'Wednesday', 'Thursday']:\n                    combo.at[num, 'Long Weekend'] = True","metadata":{"execution":{"iopub.status.busy":"2021-09-06T19:52:01.20423Z","iopub.execute_input":"2021-09-06T19:52:01.20462Z","iopub.status.idle":"2021-09-06T19:52:35.546991Z","shell.execute_reply.started":"2021-09-06T19:52:01.204583Z","shell.execute_reply":"2021-09-06T19:52:35.546071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we have completed our basic cleansing of the data, it is a good idea to export as a file so that we may use it as necessary.","metadata":{}},{"cell_type":"code","source":"#write to csv file to use in \ncombo.to_csv('combo.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-06T19:52:41.210324Z","iopub.execute_input":"2021-09-06T19:52:41.21072Z","iopub.status.idle":"2021-09-06T19:52:44.241799Z","shell.execute_reply.started":"2021-09-06T19:52:41.210687Z","shell.execute_reply":"2021-09-06T19:52:44.240686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"During analysis it is often useful to look at the rolling standard deviation.","metadata":{}},{"cell_type":"code","source":"# create rolling standard deviation file\ncolumns = list(combo.columns.values)[1:-5]\nrolling_df = pd.DataFrame({'Datetime':combo['Datetime'], 'Holiday':combo['Holiday'], 'Holiday_Variable':combo['Holiday_Variable'], 'Weekday':combo['Weekday'], 'Weekday_num': combo['Weekday_num'], 'Long Weekend':combo['Long Weekend']})\n\nfor col in columns:\n    col_name = str(col)[:-2] + 'roll'\n    date = pd.DataFrame({'Datetime':combo['Datetime']})\n    new = pd.DataFrame({col_name:combo[col].rolling(len(combo),min_periods=2).std()})\n    new_date = date.merge(new, how='outer', left_index=True, right_index = True)\n    rolling_df = rolling_df.merge(new_date, how = 'outer', on='Datetime')\n\nrolling_df.to_csv('rolling.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-06T19:52:48.686171Z","iopub.execute_input":"2021-09-06T19:52:48.686518Z","iopub.status.idle":"2021-09-06T19:52:53.988932Z","shell.execute_reply.started":"2021-09-06T19:52:48.686489Z","shell.execute_reply":"2021-09-06T19:52:53.988172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have completed and saved our dataframes as csv files.","metadata":{}},{"cell_type":"markdown","source":"# Temperature Data\nWe can now upload and clean our temperature data. First, you'll notice upon using the pd.describe function as well as showing a quick plot, that there is something wrong with the data. It includes -99 as a value in temperature.","metadata":{}},{"cell_type":"code","source":"temp_filename = '../input/daily-temperature-of-major-cities/city_temperature.csv'\ntemp = pd.read_csv(temp_filename)\n\ntemp = temp[temp['Country'] == 'US']\ntemp = temp[temp['State'] != 'Additional Territories']\ntemp = temp[temp['State'] != 'Alaska']\ntemp = temp[temp['State'] != 'Hawaii']\ntemp['Datetime'] = pd.to_datetime(temp[['Year','Month','Day']])\ntemp = temp[temp['Datetime'] >  datetime.datetime(1997, 12, 31) ]\ntemp = temp[['Region', 'Country', 'State', 'City', 'AvgTemperature', 'Datetime']]\n\nprint(temp['AvgTemperature'].describe())","metadata":{"execution":{"iopub.status.busy":"2021-09-06T19:53:41.514201Z","iopub.execute_input":"2021-09-06T19:53:41.514896Z","iopub.status.idle":"2021-09-06T19:53:48.495534Z","shell.execute_reply.started":"2021-09-06T19:53:41.514859Z","shell.execute_reply":"2021-09-06T19:53:48.494603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp.plot(kind='line', x = 'Datetime')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T19:53:54.173695Z","iopub.execute_input":"2021-09-06T19:53:54.174046Z","iopub.status.idle":"2021-09-06T19:54:00.279569Z","shell.execute_reply.started":"2021-09-06T19:53:54.174009Z","shell.execute_reply":"2021-09-06T19:54:00.278586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to see just how often -99 shows up so that we can decide if we can safely remove it. Given that it takes up less than .33% of our data, we can safely remove it. Within the documentation guide for our dataset it is also noted taht -99 is a flag for missing data. Given that this flag doesn't significantly pop up a lot, we can remove these rows and simply continue to plot our data.","metadata":{}},{"cell_type":"code","source":"temp.groupby('AvgTemperature').count()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T19:54:04.117946Z","iopub.execute_input":"2021-09-06T19:54:04.118305Z","iopub.status.idle":"2021-09-06T19:54:04.717798Z","shell.execute_reply.started":"2021-09-06T19:54:04.118275Z","shell.execute_reply":"2021-09-06T19:54:04.716857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(temp))","metadata":{"execution":{"iopub.status.busy":"2021-09-06T19:54:09.273383Z","iopub.execute_input":"2021-09-06T19:54:09.273814Z","iopub.status.idle":"2021-09-06T19:54:09.279317Z","shell.execute_reply.started":"2021-09-06T19:54:09.273778Z","shell.execute_reply":"2021-09-06T19:54:09.278281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(4026/len(temp))","metadata":{"execution":{"iopub.status.busy":"2021-09-06T19:54:13.448247Z","iopub.execute_input":"2021-09-06T19:54:13.448621Z","iopub.status.idle":"2021-09-06T19:54:13.457179Z","shell.execute_reply.started":"2021-09-06T19:54:13.448592Z","shell.execute_reply":"2021-09-06T19:54:13.455934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = temp[temp.AvgTemperature != -99]\nprint(temp['AvgTemperature'].describe())","metadata":{"execution":{"iopub.status.busy":"2021-09-06T19:54:17.993839Z","iopub.execute_input":"2021-09-06T19:54:17.994196Z","iopub.status.idle":"2021-09-06T19:54:18.122174Z","shell.execute_reply.started":"2021-09-06T19:54:17.994161Z","shell.execute_reply":"2021-09-06T19:54:18.121201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp.plot(kind='line', x = 'Datetime')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T19:54:20.57036Z","iopub.execute_input":"2021-09-06T19:54:20.570753Z","iopub.status.idle":"2021-09-06T19:54:24.413392Z","shell.execute_reply.started":"2021-09-06T19:54:20.570722Z","shell.execute_reply":"2021-09-06T19:54:24.412396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_avg = temp.set_index('Datetime').groupby(pd.Grouper(freq='d')).mean()\ntemp_avg.plot(kind='line')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-09-06T19:54:26.605983Z","iopub.execute_input":"2021-09-06T19:54:26.606403Z","iopub.status.idle":"2021-09-06T19:54:27.489619Z","shell.execute_reply.started":"2021-09-06T19:54:26.606365Z","shell.execute_reply":"2021-09-06T19:54:27.488469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before we can add the average temperature to our data on electrical consumption, we need to make sure that it has the correct datetime for each 24 hour period.","metadata":{}},{"cell_type":"code","source":"new_rows = []\nfor item in temp_avg.index:\n    for i in range(23):\n        hour = datetime.timedelta(hours = 1)\n        new = item + hour*(i+1)\n        new_rows.append([new, ''])\ndf = pd.DataFrame(new_rows, columns=['Datetime', 'AvgTemperature'])\ndf = df.set_index('Datetime')\ntemp_avg = temp_avg.append(df)\ntemp_avg = temp_avg.sort_index()\ntemp_avg = temp_avg.replace(r'^\\s*$', np.NaN, regex=True)\ntemp_avg = temp_avg.fillna(method='ffill')\ntemp_avg.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T19:54:32.124792Z","iopub.execute_input":"2021-09-06T19:54:32.125295Z","iopub.status.idle":"2021-09-06T19:54:34.512488Z","shell.execute_reply.started":"2021-09-06T19:54:32.125263Z","shell.execute_reply":"2021-09-06T19:54:34.511498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combo_temp = combo.set_index('Datetime')\ncombo_temp=pd.merge(combo_temp, temp_avg, how='outer', left_index=True, right_index=True)\ncombo_temp.head()\ncombo_temp.to_csv('combo_temp.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-06T19:54:40.338892Z","iopub.execute_input":"2021-09-06T19:54:40.339254Z","iopub.status.idle":"2021-09-06T19:54:43.852852Z","shell.execute_reply.started":"2021-09-06T19:54:40.339223Z","shell.execute_reply":"2021-09-06T19:54:43.851583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combo_temp.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T19:54:59.617852Z","iopub.execute_input":"2021-09-06T19:54:59.618237Z","iopub.status.idle":"2021-09-06T19:54:59.643746Z","shell.execute_reply.started":"2021-09-06T19:54:59.6182Z","shell.execute_reply":"2021-09-06T19:54:59.642852Z"},"trusted":true},"execution_count":null,"outputs":[]}]}