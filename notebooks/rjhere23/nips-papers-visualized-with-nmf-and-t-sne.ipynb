{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"03378873-1dc2-6594-5fac-84f48e9b269b"},"source":"# NIPS 2D: All NIPS papers visualized on a 2D map\n## Primary techniques used: NMF for topic modeling and t-SNE for 2D-embedding\n\nThis is a little test how topic modeling works on visualizing the content of all NIPS papers until 2016. 2D-embeddings are always easy to understand for us, therefore I will try to create a scatterplot that captures both the content similarity of papers as well as their distinct topics. This notebook uses the papers' textual content, not the titles or something else. \n\nThe notebook is rather short and divided into two parts:\n\n- data loading and topic modeling\n- visualization with t-SNE"},{"cell_type":"markdown","metadata":{"_cell_guid":"d4e2fba0-4c0b-0df1-26cf-dbbed7606960"},"source":"### Part 1: data loading and topic modeling"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0842686d-6f58-323d-d0b0-02bdcf29f249"},"outputs":[],"source":"#data wrangling packages\nimport pandas as pd\nimport numpy as np\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\nimport random \nrandom.seed(13)\n\n#visualization packages\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib\n%matplotlib inline\nimport seaborn as sns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"657a1160-7875-34a1-9c61-3baef47680b0"},"outputs":[],"source":"df = pd.read_csv(\"../input/papers.csv\")\ndf.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9c8c37e7-4482-0d6a-4ba1-608d97f15f9b"},"outputs":[],"source":"len(df)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5757eaed-f1d6-42ea-4b80-9b8c4026ef9e"},"outputs":[],"source":"n_features = 1000\nn_topics = 8\nn_top_words = 10\n\n\ndef print_top_words(model, feature_names, n_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([feature_names[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n    print()\n\n\ntfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,max_features=n_features,stop_words='english')\n\ntfidf = tfidf_vectorizer.fit_transform(df['paper_text'])\n\n\nnmf = NMF(n_components=n_topics, random_state=0,alpha=.1, l1_ratio=.5).fit(tfidf)\n\nprint(\"Topics found via NMF:\")\ntfidf_feature_names = tfidf_vectorizer.get_feature_names()\nprint_top_words(nmf, tfidf_feature_names, n_top_words)"},{"cell_type":"markdown","metadata":{"_cell_guid":"4caecab1-39d9-041e-81de-43f7d8329a37"},"source":"In the snippet above you get a first glimpse about the topics found. I honestly don't always find the top words useful when used alone. Another approach I like a lot: looking in which papers the topics are \"activated\" the most. We therefore transform the tfidf-matrix into the nmf-embedding and have a look with np.argsort, which research papers have the strongest link to each topic."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2a10bb18-4a42-3fb7-4985-60f922dcb10e"},"outputs":[],"source":"nmf_embedding = nmf.transform(tfidf)\nnmf_embedding = (nmf_embedding - nmf_embedding.mean(axis=0))/nmf_embedding.std(axis=0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b2a16747-3e84-608e-eec6-b6d3bd84b5ef"},"outputs":[],"source":"top_idx = np.argsort(nmf_embedding,axis=0)[-3:]\n\ncount = 0\nfor idxs in top_idx.T: \n    print(\"\\nTopic {}:\".format(count))\n    for idx in idxs:\n        print(df.iloc[idx]['title'])\n    count += 1"},{"cell_type":"markdown","metadata":{"_cell_guid":"941828f3-d076-15a9-cf27-bfed7faf2e52"},"source":"Looks much better to me. \n\nThe next part is very subjective. But looking at those titles and top-words I decide to assign the following descriptions to the topics: \n\n- **topic 0:** optimization algorithms\n- **topic 1:** neural network application\n- **topic 2:** reinforcement learning\n- **topic 3:** bayesian methods\n- **topic 4:** image recognition\n- **topic 5:** artificial neuron design\n- **topic 6:** graph theory\n- **topic 7:** kernel methods"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1a820451-0abe-e58e-02eb-847f29befe83"},"outputs":[],"source":"topics = ['optimization algorithms',\n          'neural network application',\n          'reinforcement learning',\n          'bayesian methods',\n          'image recognition',\n          'artificial neuron design',\n          'graph theory',\n          'kernel methods'\n         ]"},{"cell_type":"markdown","metadata":{"_cell_guid":"eb2fdd49-f06f-7aaa-ff8a-6c4f7f8d9e44"},"source":"### Part 2: visualize the findings\nLike lots of people in the data science community I'm a huge fan of t-SNE. It's capabilities to visualize complex relationships are stunning. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cfac33c8-91e6-a499-5910-7e07aaa575ab"},"outputs":[],"source":"tsne = TSNE(random_state=3211)\ntsne_embedding = tsne.fit_transform(nmf_embedding)\ntsne_embedding = pd.DataFrame(tsne_embedding,columns=['x','y'])\ntsne_embedding['hue'] = nmf_embedding.argmax(axis=1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"26b3a040-ec04-2193-b666-4745d6064a30"},"source":"The next cell is a little bit hacky. Since I want to create a custom legend afterwards I first plot a dummy output and then obtain the rgb-values used in this plot. Afterwards I have a hard-coded color list I can use for the custom legend. I honestly didn't find a better way. Since I don't want to output the plot in this notebook (it's boring and would be the one published) I didn't find a better way. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0007b7d5-0af3-f296-9111-9dff2151cae9"},"outputs":[],"source":"###code used to create the first plot for getting the colors \n#plt.style.use('ggplot')\n\n#fig, axs = plt.subplots(1,1, figsize=(5, 5), facecolor='w', edgecolor='k')\n#fig.subplots_adjust(hspace = .1, wspace=.001)\n\n#legend_list = []\n\n#data = tsne_embedding\n#scatter = plt.scatter(data=data,x='x',y='y',s=6,c=data['hue'],cmap=\"Set1\")\n#plt.axis('off')\n#plt.show()\n\n#colors = []\n#for i in range(len(topics)):\n#    idx = np.where(data['hue']==i)[0][0]\n#    color = scatter.get_facecolors()[idx]\n#    colors.append(color)\n#    legend_list.append(mpatches.Ellipse((0, 0), 1, 1, fc=color))\n \ncolors = np.array([[ 0.89411765,  0.10196079,  0.10980392,  1. ],\n [ 0.22685121,  0.51898501,  0.66574396,  1. ],\n [ 0.38731259,  0.57588621,  0.39148022,  1. ],\n [ 0.7655671 ,  0.38651289,  0.37099578,  1. ],\n [ 1.        ,  0.78937332,  0.11607843,  1. ],\n [ 0.75226453,  0.52958094,  0.16938101,  1. ],\n [ 0.92752019,  0.48406   ,  0.67238756,  1. ],\n [ 0.60000002,  0.60000002,  0.60000002,  1. ]])\n\nlegend_list = []\n\nfor i in range(len(topics)):   \n    color = colors[i]\n    legend_list.append(mpatches.Ellipse((0, 0), 1, 1, fc=color))"},{"cell_type":"markdown","metadata":{"_cell_guid":"d4377e5c-7bf1-af41-a2d6-806239cb903e"},"source":"Lets try to get some insights out of the data. I think it would be interesting to visualize how the topics at NIPS emerged over time. Let's do this!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a26d896a-26ac-230a-0087-04e8326ade5f"},"outputs":[],"source":"matplotlib.rc('font',family='monospace')\nplt.style.use('ggplot')\n\n\nfig, axs = plt.subplots(3,2, figsize=(10, 15), facecolor='w', edgecolor='k')\nfig.subplots_adjust(hspace = .1, wspace=0)\n\naxs = axs.ravel()\n\ncount = 0\nlegend = []\nfor year, idx in zip([1991,1996,2001,2006,2011,2016], range(6)):\n    data = tsne_embedding[df['year']<=year]\n    scatter = axs[idx].scatter(data=data,x='x',y='y',s=6,c=data['hue'],cmap=\"Set1\")\n    axs[idx].set_title('published until {}'.format(year),**{'fontsize':'10'})\n    axs[idx].axis('off')\n\nplt.suptitle(\"all NIPS proceedings clustered by topic\",**{'fontsize':'14','weight':'bold'})\nplt.figtext(.51,0.95,'unsupervised topic modeling with NMF based on textual content + 2D-embedding with t-SNE:', **{'fontsize':'10','weight':'light'}, ha='center')\n\n\nfig.legend(legend_list,topics,loc=(0.1,0.89),ncol=3)\nplt.subplots_adjust(top=0.85)\n\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"271c0cb4-52e6-ac16-b921-b67c30d0cafc"},"source":"For me plots like this can be very helpful. What can we see here for example? Did you realize that neural networks were the most active topic at the eary times of NIPS? In fact you can cleary see that the blue cluster (neural networks) was strongly present right from the beginning. \n\nAlso interesting for me is how kernel methods took over NIPS in the last years. What else can you see in this plot? \n\nOf course, there is lots of room for improvement of the code. Here are just some examples:\n\n- **Topic count:** I chose 8 by trial and error. It would be worth a look trying out several topic numbers.\n- **Topic description:** in this code I only looked at the top-10 words and top-3 titles (eventough I also looked into the top-10 titles as well, which didn't change anything)\n- **NMF parameters:** the parameters I used where standard ones from sklearn tutorials. Why not try different ones here\n\nIt's also important to note that topic modeling doesn't create one-hot encodings. Eventough we assign each paper to its most activated topic in this notebook, they are of course represented by mixtures of more than one topic. "}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}