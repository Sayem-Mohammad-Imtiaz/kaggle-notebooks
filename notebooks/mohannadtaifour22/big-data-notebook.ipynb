{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting up Spark \n!pip install pyspark\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Entery Point of Spark \nfrom pyspark.sql import SparkSession\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Exploratory Analysis\") \\\n    .getOrCreate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing DataSet\nP_Data_Set = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load('/kaggle/input/nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2015.csv')\nP_Data_Set","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exploring Data_Set, Display summary statistics\nP_Data_set.head(5)\nP_Data_Set.tail(2)\nP_Data_set.describe().show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# datatype of columns\nP_Data_Set.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#PreProcessing of Data such as Droping Duplicates, removing null values ","metadata":{}},{"cell_type":"code","source":"P_Data_Set=P_Data_Set.dropDuplicates()\nP_Data_Set = P_Data_Set.dropna()\nP_Data_Set.count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting the Data set into a data frame\nP_Data_Set = P_Data_Set.toDF(*(c.replace(' ', '_') for c in parking.columns))\nparking.show(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a local temporary view of the table, to avoid changed in the orginal data set \nparking.createOrReplaceTempView(\"Ptable\")\nspark.sql('Select * from Ptable') ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Total number of tickets for each year\nsql_ticket_year = spark.sql(\"select year(Issue_Date) as year, count(Summons_Number) as no_of_tickets from Ptable group by year order by year\")\nsql_ticket_year.show(50)\nsql_ticket_year.count()\n# error while counting the ticket number ","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specfying the scope of the analysis to 2015 data \nparking.createOrReplaceTempView(\"tble_view2015\")\nparking=spark.sql(\"select * from tble_view2015 where year(TO_DATE(CAST(UNIX_TIMESTAMP(Issue_Date,'MM/dd/yyyy') AS TIMESTAMP))) = 2015 \")\nparking.count()\n# error while filting the data  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"P_Data_Set.createOrReplaceTempView(\"tble_view2015\")\n#Showing distribution of tickets over the year \nDistribution_on_years= spark.sql(\"SELECT year(Issue_Date) as year,month(Issue_Date) as month,count(*) as Ticket_Frequency FROM tble_view2015 GROUP BY year(Issue_Date),month(Issue_Date) order by Ticket_Frequency desc\")","metadata":{"execution":{"iopub.status.busy":"2021-06-25T17:05:02.191951Z","iopub.execute_input":"2021-06-25T17:05:02.192409Z","iopub.status.idle":"2021-06-25T17:05:02.228535Z","shell.execute_reply.started":"2021-06-25T17:05:02.19237Z","shell.execute_reply":"2021-06-25T17:05:02.226742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Distribution_on_years.show()\nNumber_of_Violations_by_month = Distribution_on_years.toPandas()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T17:05:06.624385Z","iopub.execute_input":"2021-06-25T17:05:06.624754Z","iopub.status.idle":"2021-06-25T17:06:52.037178Z","shell.execute_reply.started":"2021-06-25T17:05:06.624722Z","shell.execute_reply":"2021-06-25T17:06:52.036168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Number_of_Violations_by_month.plot(x= 'month', y='Ticket_Frequency', kind='line')\nplt.title(\"Violations on the basis of month in 2015\")\nplt.xlabel('month')\nplt.ylabel('Ticket_Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T17:04:25.86428Z","iopub.execute_input":"2021-06-25T17:04:25.864759Z","iopub.status.idle":"2021-06-25T17:04:26.039045Z","shell.execute_reply.started":"2021-06-25T17:04:25.864717Z","shell.execute_reply":"2021-06-25T17:04:26.038137Z"},"trusted":true},"execution_count":null,"outputs":[]}]}