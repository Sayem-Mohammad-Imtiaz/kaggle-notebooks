{"cells":[{"metadata":{"id":"1E_0d5nznEMk"},"cell_type":"markdown","source":"# Data Mining Term Project\n\nAuthor: **Harsh Kishan Daga**\n<br>\nUTA ID: **100 177 1009**\n\n---\n\n<br>\n\n**Purpose:**\n<br>\nThe purpose of this notebook is to classify (predict) the rating of a review for any games.<br>\nI start with cleaning and preprocessing the input data which is plain text.<br>\nThe input data has fractional ratings while we only predict ratings in the integer range $[0,10]$.<br>\nThis is done by rounding off the original ratings to bring them into $[0,10]$ integer range.<br>\nSince the dataset is massive (2.9 million reviews), the models created here are trained on a random sample of 10,000 reviews.<br>\nI start off by using a SVM since it trains faster than most of the alternatives like NBC but after analysing the predictions from a single SVM, there's a significant flaw in the predictions (explained further in the notebook).<br>\nI attempt to resolve that using another SVM that it trained using \"balanced\" train data(explain further) and then using a Decision Tree on the outputs of both SVMs to decide the final prediction.\n\n\n\n"},{"metadata":{"id":"m6P75pTyu8gK"},"cell_type":"markdown","source":"> NOTE: The accuracy stats written below change every run so they might differ from cell outputs over time."},{"metadata":{"id":"bEr76WLBu2Nd"},"cell_type":"markdown","source":"Imports and setting up the notebook environment for plotting."},{"metadata":{"id":"0Q9hz8yqS6_M","trusted":true},"cell_type":"code","source":"%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"fGuK4Pv0U-hX","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport string\nimport nltk\nimport random\nimport matplotlib.mlab as mlab\nimport seaborn as sns\nimport math\nfrom nltk.corpus import stopwords\nfrom sklearn import svm\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"id":"Nx3E5ljiLGB5","trusted":true},"cell_type":"code","source":"sns.set(color_codes=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"B-6iy2WawTrI"},"cell_type":"markdown","source":"*Stopwords* are words that are filtered before processing the text since they don't contribute to the sentiment of the text."},{"metadata":{"id":"4IzJ9SWxaqrz","outputId":"08d1138f-27d2-44d3-b1a2-046edec3e411","trusted":true},"cell_type":"code","source":"nltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"id":"ITnk0CzcHyhN"},"cell_type":"markdown","source":"# Reading Data"},{"metadata":{"id":"B_WUIiw2yJXE"},"cell_type":"markdown","source":"After reading the csv file, I drop the `user` and `name` columns to save some memory."},{"metadata":{"id":"bLKeRPBDVaNS","outputId":"db61854f-d6e9-4c02-ffca-0655865b0c63","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/boardgamegeek-reviews/bgg-15m-reviews.csv', index_col=0)\ndata = data.drop(['user', 'name'], axis=1)\ndata","execution_count":null,"outputs":[]},{"metadata":{"id":"HDYdxbhDH5lz"},"cell_type":"markdown","source":"# Filtering Data"},{"metadata":{"id":"Xc3v4_HZyWAF"},"cell_type":"markdown","source":"Looking at the data above, we can see that a low of the rows are missing a `comment`.<br>\nThese rows are removed from the dataset.<br>\nAdditionally, I also compute the word count of each review since I'll be using this later for further filtering.<br>\nFinally, the fractional ratings are rounded off to integer and store in `rating_int`."},{"metadata":{"id":"SWEOwtAEWLPf","outputId":"eb18eb6b-d687-4cfc-edaf-7229dbf06149","trusted":true},"cell_type":"code","source":"data = data[data.comment.notna()]\ndata['word_count'] = data.comment.str.split().str.len()\ndata['rating_int'] = data.rating.astype('int')\ndata","execution_count":null,"outputs":[]},{"metadata":{"id":"VDE5Fe-O0iD8"},"cell_type":"markdown","source":"The size of the dataset went from **15.8 million** rows to **~3 million rows**!<br>\nOut of these, I discard the reviews that are too short or too long.<br>\nThe short reviews are discarded because they aren't as useful in training the models.<br>\nThe long reviews are discarded because they will slow down the training significantly."},{"metadata":{"id":"mH0NlDma9FuV","trusted":true},"cell_type":"code","source":"usable_data = data[(data.word_count > 10) & (data.word_count < 200)]","execution_count":null,"outputs":[]},{"metadata":{"id":"nxFOpnQ71GWx"},"cell_type":"markdown","source":"The dataset is still too large to train the model on.<br>\nGoing forward, I work with a random sample of the data of size `sample_size`."},{"metadata":{"id":"tXpaAhfT24Gv","trusted":true},"cell_type":"code","source":"sample_size = 10000","execution_count":null,"outputs":[]},{"metadata":{"id":"hENBn1pqWN0K","outputId":"3bb65c4f-1e16-44e8-a014-e2dcbdcd59b0","trusted":true},"cell_type":"code","source":"working_data = usable_data.sample(n=sample_size)\nworking_data","execution_count":null,"outputs":[]},{"metadata":{"id":"okPndato2D-A"},"cell_type":"markdown","source":"Most of the reviews in the random sample above follow the same rating distribution as the original data which leaves us with too few reviews with extreme ratings.<br>\nSo I create a second sample of reviews which is \"balanced\" such that we have the same number of reviews for each prediction class (`rating_int`)."},{"metadata":{"id":"CjVILlAI3Xv2","outputId":"c8d87c5d-bb54-421a-ded3-d99f1937017d","trusted":true},"cell_type":"code","source":"grouped = usable_data[usable_data.rating_int != 0].groupby(['rating_int'])\nworking_data_balanced = grouped.apply(lambda x: x.sample(n=sample_size//10))\nworking_data_balanced","execution_count":null,"outputs":[]},{"metadata":{"id":"PWYEqUtJH-OL"},"cell_type":"markdown","source":"# Cleaning Data"},{"metadata":{"id":"H6jKoiBU2wrK"},"cell_type":"markdown","source":"In addition to the stopwords from `nltk`, I added a few words of my own since I know these reviews are for games and the following words will occur frequently even though they help in conveying the sentiment of the review."},{"metadata":{"id":"ndC81wMeabDL","trusted":true},"cell_type":"code","source":"stopwords_set = set(stopwords.words('english'))\nstopwords_set = stopwords_set.union(('game','play','played','players','player','people','really','board','games','one','plays','cards','would'))","execution_count":null,"outputs":[]},{"metadata":{"id":"8Cdcl5At3FTU"},"cell_type":"markdown","source":"I define a function to clean the reviews in a `Dataframe`.<br>\nAll punctuations are removed from the review text.<br>\nAll stopwords are removed as well."},{"metadata":{"id":"Ou-zj-3TD1iS","trusted":true},"cell_type":"code","source":"def clean_data(df):\n  df['cleaned'] = df['comment'].str.lower().apply(lambda x:''.join([i for i in x if i not in string.punctuation]))\n  df['cleaned'] = df['cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords_set]))\n  df['rating_int'] = df['rating'].astype('int')\n  return df","execution_count":null,"outputs":[]},{"metadata":{"id":"xjD0j-r54f3N"},"cell_type":"markdown","source":"Cleaning the 2 data samples from earlier."},{"metadata":{"id":"ofKefeih4NB9","trusted":true},"cell_type":"code","source":"clean_data(working_data)\nclean_data(working_data_balanced)","execution_count":null,"outputs":[]},{"metadata":{"id":"Q8gRJqDo3ZSt"},"cell_type":"markdown","source":"`Data` class is defined to split and store the `Dataframe` into `train` and `test` datasets."},{"metadata":{"id":"GgXIkpJ4V9jp","trusted":true},"cell_type":"code","source":"class Data:\n  def __init__(self, df, test_size=0.20, feat_cols = 'cleaned', result_col='rating_int'):\n    df = df.copy()\n    clean_data(df)\n    self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(df[feat_cols], df[result_col], test_size=0.20)\n  \n  @property\n  def train(self):\n    return self.X_train, self.y_train\n  \n  @property\n  def test(self):\n    return self.X_test, self.y_test","execution_count":null,"outputs":[]},{"metadata":{"id":"tFOS46FH31VA"},"cell_type":"markdown","source":"Creating `Data` objects from the 2 cleaned random data samples created earlier."},{"metadata":{"id":"nq502wyZbH4W","trusted":true},"cell_type":"code","source":"data_unbalanced = Data(working_data, 0.20)\ndata_balanced = Data(working_data_balanced, 0.20)","execution_count":null,"outputs":[]},{"metadata":{"id":"OZkt3-du4pvR"},"cell_type":"markdown","source":"`eval_model` computes the accuracy of a model on the given data and plots the confusion matrix for the predictions againt the actual ratings."},{"metadata":{"id":"P0nor2LTujlu","trusted":true},"cell_type":"code","source":"def eval_model(name, model, X, y):\n  _y_pred = model.predict(X)\n  acc = accuracy_score(y, _y_pred)\n  conf_mat = confusion_matrix(y, _y_pred, labels=list(range(1, 11)))\n\n  sns.heatmap(conf_mat.T, square=False, annot=True, cbar=False, fmt='d')\n  plt.xlabel('actual')\n  plt.ylabel('predicted');\n  plt.show()\n\n  print(f'Accuracy of {name} = {acc}')","execution_count":null,"outputs":[]},{"metadata":{"id":"-zj8VNqCIEAn"},"cell_type":"markdown","source":"# SVM"},{"metadata":{"id":"c1yMxd1J47DL"},"cell_type":"markdown","source":"This is the first SVM model that I use.<br>\nIt's defined as a pipeline since the text needs to be converted into numerical features before being fed into a SVM.<br>\nThe conversion of text to numerical features is done using $TF-IDF$.\nThe first stage of the pipleline is a `TfidfVectorizer` which also creates [bigrams](https://en.wikipedia.org/wiki/Bigram).<br>\nThe second step is a `SVC` classifier using `rbf` kernel instead of `linear` since `rbf` trades faster training for higher accuracy."},{"metadata":{"id":"2CVbP40pc5AF","trusted":true},"cell_type":"code","source":"model_svm = Pipeline([\n    ('tfidf_vectorizer', TfidfVectorizer(lowercase = True, stop_words = stopwords_set, ngram_range=(1, 2))),\n    ('classifier', svm.SVC(kernel=\"rbf\", probability=True, cache_size=2048))\n    ])","execution_count":null,"outputs":[]},{"metadata":{"id":"WXagl5V17fBn"},"cell_type":"markdown","source":"Training the SVM to unabalanced data sample."},{"metadata":{"id":"DVviEGmKc9LD","outputId":"c16eed54-0232-4e55-9f73-2b7b64e76772","trusted":true},"cell_type":"code","source":"model_svm.fit(*data_unbalanced.train)","execution_count":null,"outputs":[]},{"metadata":{"id":"wXQ1DJ1se02n","outputId":"20082fb7-ebc1-4943-eb42-1bbc5283388c","trusted":true},"cell_type":"code","source":"eval_model('SVM', model_svm, *data_unbalanced.train)","execution_count":null,"outputs":[]},{"metadata":{"id":"xgrhXxo37qmT"},"cell_type":"markdown","source":"That's a really good train accuracy and it's worth noting that <br>\nmost of the wrongly predicted reviews are for `rating == 6`."},{"metadata":{"id":"GWfMLIxyfPwW","outputId":"48345ca7-efd9-453e-99b6-c32b5b127eaf","trusted":true},"cell_type":"code","source":"eval_model('SVM', model_svm, *data_unbalanced.test)","execution_count":null,"outputs":[]},{"metadata":{"id":"sTHJBli579lk"},"cell_type":"markdown","source":"The test accuracy isn't as good, and this was expected. <br>\nBut what's significant here is that the model has only predicted ratings between $4$ and $7$!<br>\nLooks like the model has been overfit around the mean of the ratings."},{"metadata":{"id":"6ZFptCO8IIRq"},"cell_type":"markdown","source":"# SVM Balanced Data"},{"metadata":{"id":"OatvQpGC7p2V"},"cell_type":"markdown","source":"This is another SVM defined in the exact same manner as above but it will be trained on a different data sample."},{"metadata":{"id":"Hr8Z6ZjFgKD6","trusted":true},"cell_type":"code","source":"model_svm2 = Pipeline([\n    ('tfidf_transformer', TfidfVectorizer(lowercase = True, stop_words = stopwords_set, ngram_range=(1, 2))),\n    ('classifier', svm.SVC(kernel=\"rbf\", probability=True, cache_size=2048))\n    ])","execution_count":null,"outputs":[]},{"metadata":{"id":"vBvGSLWT8iLc"},"cell_type":"markdown","source":"This SVM is not trained on balanced data sample. <br>\nIt should be predicting ratings more uniformly now."},{"metadata":{"id":"awGkk88jgfqb","outputId":"f4881f04-c745-47eb-f383-04cac3da61bb","trusted":true},"cell_type":"code","source":"model_svm2.fit(*data_balanced.train)","execution_count":null,"outputs":[]},{"metadata":{"id":"NVFK4VhAglSq","outputId":"353d924f-b4ee-4af7-dd04-a7d23fb32872","trusted":true},"cell_type":"code","source":"eval_model('SVM2', model_svm2, *data_balanced.train)","execution_count":null,"outputs":[]},{"metadata":{"id":"lqKdAyBn83c9"},"cell_type":"markdown","source":"A **very** high train accuracy!"},{"metadata":{"id":"DYCOg5Ugm68X","outputId":"f2cfec3d-4609-4020-e066-8b7f29c35105","trusted":true},"cell_type":"code","source":"eval_model('SVM2', model_svm2, *data_balanced.test)","execution_count":null,"outputs":[]},{"metadata":{"id":"85D41e0f89bN"},"cell_type":"markdown","source":"The test accuracy is worse now **BUT** this model is predicting ratings all across the matrix now.<br>\nAlso, most of the wrongly predicted ratings aren't that far off from the actual rating.<br>\nAn actual rating of $2$ being wrongly predicted as $3$ is not as bad as being predicted as $7$."},{"metadata":{"id":"m0inIbZT9aKo"},"cell_type":"markdown","source":"# Decision Tree\nNow that we have 2 SVMs trained and ready to use, we need a third classifier to get the prediction results of these 2 SVMs and predict a final rating.<br>\nA third random data sample is created for the final classifier and we add the predictions of the above 2 SVMs as features to this new data sample."},{"metadata":{"id":"m66NuvsmmO6y","outputId":"68610ce0-7960-47ad-e63e-69c691b1d59d","trusted":true},"cell_type":"code","source":"all_working_data = usable_data.sample(n=sample_size)\nclean_data(all_working_data)\nall_working_data['unbalanced'] = model_svm.predict(all_working_data['cleaned'])\nall_working_data['balanced'] = model_svm2.predict(all_working_data['cleaned'])\nall_working_data['average'] = np.average(all_working_data[['balanced', 'unbalanced']])\nall_working_data","execution_count":null,"outputs":[]},{"metadata":{"id":"b4qsV6otFxIh","trusted":true},"cell_type":"code","source":"all_working_data['average'] = (all_working_data['balanced'] + all_working_data['unbalanced']) / 2","execution_count":null,"outputs":[]},{"metadata":{"id":"Ws4JddY3nRFt","trusted":true},"cell_type":"code","source":"data_final = Data(all_working_data, 0.2, feat_cols=['balanced', 'unbalanced'])","execution_count":null,"outputs":[]},{"metadata":{"id":"y79fUX1W-AmO"},"cell_type":"markdown","source":"I used a decision tree as the final classifier to combine the results of both SVMs.<br>\nA Decision Tree is a good choice here since we're predicting a single review out of 2 reviews as input.<br>"},{"metadata":{"id":"8Q6RER1eqjaX","outputId":"b53b0653-fc36-44bd-ecfe-53db1723fa4c","trusted":true},"cell_type":"code","source":"model_final = DecisionTreeClassifier()\nmodel_final.fit(*data_final.train)","execution_count":null,"outputs":[]},{"metadata":{"id":"lU1go-0fv6Ok","outputId":"6ecaaee5-a071-4ef3-c5f3-f8ff777e940d","trusted":true},"cell_type":"code","source":"eval_model('final', model_final, *data_final.train)","execution_count":null,"outputs":[]},{"metadata":{"id":"ioSvhE6e-WV8"},"cell_type":"markdown","source":"So the accuracy is about $28.6\\%$ now and the predictions are spread out across the confusion matrix instead of being confined to a tight region around the mean. <br>\n**Note:** The previous 2 SVMs were not retrained to be used in this decision tree."},{"metadata":{"id":"wAnS2iWNwFWS","outputId":"994a5363-90bd-4726-d07d-ac0b2c061441","trusted":true},"cell_type":"code","source":"eval_model('final', model_final, *data_final.test)","execution_count":null,"outputs":[]},{"metadata":{"id":"ByTKjbby_e4G"},"cell_type":"markdown","source":"A final accuracy of $26.1\\%$. <br>\nThe confusion matrix looks concentrated around a rating of $6$ because that's the mean of ratings and the random sample that we chose had more reviews around this rating. <br>\nThe final model still attempted to predict ratings on the extremes but not so accurately."},{"metadata":{"id":"Aao-VYba_VUd"},"cell_type":"markdown","source":"The 3 models that are required to make a prediction are saved to be used outside of this notebook."},{"metadata":{"id":"TUyI-O2UJVYq","outputId":"02ada24b-5d2a-45e9-f993-6695cc5ed32a","trusted":true},"cell_type":"code","source":"import joblib\n\njoblib.dump(model_svm, 'svm.pkl')\njoblib.dump(model_svm2, 'svm_balanced.pkl')\njoblib.dump(model_final, 'decision_tree.pkl')","execution_count":null,"outputs":[]},{"metadata":{"id":"V14cbWMr__CS"},"cell_type":"markdown","source":"# Summary\n\nThe final model is a combination of 2 SVMs and a decision tree.\n\n![](https://drive.google.com/uc?export=view&id=1Aok3WqMVgYStNk0LYQHg8eV3HRzoDZgT)\n\n<br>\n\nHowever, the accuracy scores might be a bit misleading as they compute accuracy as pass/fail instead of measuring a degree of how wrong the prediction is from the actual rating.<br>\nIn most cases, the difference between the actual rating and the predicted one is $2$ or less.<br>\nThis was the motive behind combining 2 SVMs into a single model.<br>\nThe first SVM failed to predict ratings outside of the range $[4,7]$.<br>\nThe second model predicted ratings uniformly but had a low accuracy and since we know that most of the actual ratings have a mean of around $6$, it would be beneficial to combine the results of both."},{"metadata":{"id":"CaokDgpGAjkY"},"cell_type":"markdown","source":"# Challenges\n\nThe most significant challenge in this exercise was to clean and transform the **massive** input data. <br>\nThis is because the reviews are plain text and many of them aren't even in English! <br>\nI handled this by taking random samples of the reviews and training the models on them. The sample size did have an effect on the final accuracy as I tried with several sample sizes.<br>\nA sample size of $10000$ seemed like a good trade off between train time and accuracy in the end.<br>\n<br>\nThe other siginificant which isn't resolved yet is to change the accuracy and loss functions to penalize predictions that are far from the actual rating more.<br>\nUnfortunately, sklearn `SVC` does not allow for custom loss functions.<br>\nPossible solutions for this are to try some other classifiers in sklearn or use a different library altogether like PyTorch or Tensorflow."},{"metadata":{"id":"F2l55h_qAE2k"},"cell_type":"markdown","source":"# References\n\n\n\n1.   https://www.kaggle.com/jvanelteren/exploring-the-13m-reviews-bgg-dataset\n2.   https://stackoverflow.com/questions/34143829/sklearn-how-to-save-a-model-created-from-a-pipeline-and-gridsearchcv-using-jobli\n3.   https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}