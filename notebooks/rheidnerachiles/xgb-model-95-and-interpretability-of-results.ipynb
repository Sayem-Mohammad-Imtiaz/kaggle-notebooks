{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Heart Failure Prediction with and without time - XGB model (95%) and interpretability of results","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Importing libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format='svg'\n%matplotlib inline ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split#, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading and inspecting data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"clinical_data = pd.read_csv('../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\nclinical_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clinical_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data has 299 samples and 13 column. As can be seen there is no null value in the data and all the columns has numerical values, from which `anaemia`, `diabetes`, `high_blood_pressure`, `sex`, `smoking` and `DEATH_EVENT` has binary values. Below some statistics measures for the non-binary columns are given.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clinical_data[['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The objective of the task is to predict the target column `DEATH_EVENT`, which indicates death due to a heart failure, based on the information provided by the other 12 columns. Pearson correlation indicator can be a good start for this purpose.  Below a graph show us the result of this indicator when applied between the target column and all the other variables. As we can see `time` has a strong negative correlation with the target column. Followed by it `serum_creatinine`, `ejection_fraction`, `age` and `serum_sodium` also have considerable correlation with the target, when compared with the other columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\ncorr_death = clinical_data.corr('pearson')['DEATH_EVENT'].drop('DEATH_EVENT')\nsorted_idx_corr_death = corr_death.argsort()\nplt.barh(clinical_data.columns[sorted_idx_corr_death], corr_death[sorted_idx_corr_death])\nplt.title('Pearson correlation with DEATH_EVENT')\nplt.xlabel('corr_value')\nplt.ylabel('variable')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we will see if these correlations are identified and/or useful for the machine learning algorithm.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning Process","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First we separe the target and the feature columns and then split all the data as train and test sets. The test size was setted as 20% of the original data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = clinical_data.drop(['DEATH_EVENT'], axis = 1)\ny = clinical_data['DEATH_EVENT']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2)\n\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we use a GradientBoostClassifier to train with the train set. The hyperparameters of this classifier was found using a GridSearch (the parameter grid used is shown at the end of this document).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = GradientBoostingClassifier(max_depth=2, min_samples_split=0.5, n_estimators=50,random_state=1)\nxgb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy and the confusion matrix of this classifier are showed below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = xgb.predict(X_test)\nprint('Accuracy: ', accuracy_score(y_test, y_pred))\ncm = confusion_matrix(y_pred, y_test)\n\ndef create_confusion_graph(cm, title='Confusion matrix'):\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, cmap='Blues')\n    \n    ax.set_xticks([0,1])\n    ax.set_yticks([0,1])\n    ax.set_xticklabels(['True','False'])\n    ax.set_yticklabels(['True','False'])\n\n    plt.xlabel('Predicted')\n    plt.ylabel('Real')\n\n    for i in range(len(cm)):\n        for j in range(len(cm[0])):\n            text = ax.text(j, i, cm[i, j],\n                           ha=\"center\", va=\"center\", color=\"black\")\n\n    plt.title(title)\n    \n    return fig\n\ncreate_confusion_graph(cm, 'Confusion matrix for the first gradient boost classifier')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interpretability of the first model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Next we will see a analysis of the results of the first model. Gradient boost models in `scikit_learn` has an attribute called `feature_importance`, that tell us \"how much that feature reduced the criterion of a split\", (this is known as the Gini importance). The graph below show us this measure.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_idx = xgb.feature_importances_.argsort()\n\nplt.barh(y=X.columns[sorted_idx], width=xgb.feature_importances_[sorted_idx])\n\nplt.title('Gini importance')\nplt.xlabel('Gini importance')\nplt.ylabel('feature')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can been seen, not coincidentally, the variables with the greater Pearson correlation are the more importants, according with de Gini measure. To confirm this, a permutation importance inspection was done, and the results, which confirms the Gini measure, are seen below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.inspection import permutation_importance\nresult = permutation_importance(xgb, X, y, scoring='accuracy', random_state=1)\nsorted_idx = result.importances_mean.argsort()\n\nfig, ax = plt.subplots()\nax.boxplot(result.importances[sorted_idx].T,\n           vert=False, labels=X_test.columns[sorted_idx])\nax.set_title(\"Permutation Importances (test set)\")\nfig.set_size_inches((7,7))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the results confirmed, we can say that, for this classifier, the most important feature was `time` with a huge advantage if compared with the others features. Both `ejection_faction` and `serum_creatinine` also seems considerable important to this model. Using these three variables a partial dependece plot, which show us the average effect of the features on the target variable, is shown below. From him we can see clearly that the model learn what the Pearson correlation measure told us in the past.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.inspection import plot_partial_dependence\nplot_partial_dependence(xgb, X, features=['time','ejection_fraction','serum_creatinine'], n_cols=3, response_method='predict_proba', method='brute')\nplt.title('Partial dependece plot for the first classifier')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this graph is also shown: \n* Patients with follow-up period (`time`) less than 50, `ejection_fraction < 20` and/or `serum_creatinine > 7.5` has a higher probability of dying.\n* There is an increase in the curve of the `time` variable between the values of 150 and 170. This happens because all of the patients who had a quantity of days of follow-up period at this interval died. This is shown below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clinical_data[(clinical_data['time'] >= 150) & (clinical_data['time'] <= 170)]['DEATH_EVENT'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The only 3 samples that had a wrong prediction in the test set are showed below. All of them have a `DEATH_EVENT` value of 1 and get a predicition value of 0. We can see below that they did not match with what was shown in the partial dependent plot and, because of that, the model could not predict correctly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test[xgb.predict(X_test) != y_test][['time', 'ejection_fraction', 'serum_creatinine']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Is that all?\nThe model created in the last section did a pretty good job (95% accuracy). But, we saw that his predictions was based mostly on the `time` feature. Although this is not necessarily bad, the `time` feature, which tell a patient's follow-up period (in days), is not a biological feature. If we remove the `time` feature, would the model work well? And would it make its predictions based on which variables?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## A new gradient model without `time`","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First we remove the `time` feature from the previous X data. Then we split the new set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_no_time = X.drop(['time'], axis = 1)\nX_train_no_time, X_test_no_time, y_train, y_test = train_test_split(X_no_time, y, test_size = 0.2, random_state = 2)\nprint(X_train_no_time.shape)\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we create a new model. The hyperparameters of this model was found with the second grid search from the final of this document.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_no_time = GradientBoostingClassifier(learning_rate=0.01, max_depth=2,\n                           min_samples_leaf=0.1, min_samples_split=0.5,\n                           random_state=1)\nxgb_no_time.fit(X_train_no_time, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the best hyperparameters found, the maximum accuracy found was 80%. As expected, without `time`, the model has more difficult learning.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = xgb_no_time.predict(X_test_no_time)\nprint('Accuracy: ', accuracy_score(y_test, y_pred))\ncreate_confusion_graph(confusion_matrix(y_pred, y_test))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To answer the second question made before, we going to make the same procedure of analysing the results. Below a Gini importance graph is shown again. Without `time`, the more importants features, according with the Gini measure are `serum_creatinine` and `ejection_fraction`, which were part of the first three previously. Interestingly, now the model seems to consider only three variables to make its predictions, the two already mentioned and `age`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_idx_no_time = xgb_no_time.feature_importances_.argsort()\n\nplt.barh(y=X.columns[sorted_idx_no_time], width=xgb_no_time.feature_importances_[sorted_idx_no_time])\nplt.title('Gini importance')\nplt.xlabel('Gini importance')\nplt.ylabel('feature')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The permutation importance measurement below confirms what was shown with the Gini importance measurement.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"result = permutation_importance(xgb_no_time, X_no_time, y, scoring='accuracy', random_state=1)\nsorted_idx_no_time = result.importances_mean.argsort()\n\nfig, ax = plt.subplots()\nax.boxplot(result.importances[sorted_idx_no_time].T,\n           vert=False, labels=X_test_no_time.columns[sorted_idx_no_time])\nax.set_title(\"Permutation Importances (test set)\")\nfig.set_size_inches((7,7))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The patial dependence plot also is shown below with these three variables. The results of `ejection_fraction` and `serum_creatinine` look like those obtained earlier, but now much more accentuated. With respect to `age`, the partial dependence plot shows that an age greater than 70 increases the probability of death, for this model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_partial_dependence(xgb_no_time, X_no_time, ['ejection_fraction','age', 'serum_creatinine'], grid_resolution=500, response_method='predict_proba', method='brute')\nplt.title('Partial dependence plot for the second classifier')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nWith all that has been presented, we can conclude that, for these samples, the factors of follow-up period, level of serum creatinine in the blood, and ejection fraction (percentage of blood leaving the heart at each contraction) were the most important to predict a death event due to a heart failure.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Grid search parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"'''param_grid = {\n    'learning_rate': [0.01, 0.1, 0.2],\n    'n_estimators': [50, 100, 150],\n    'max_depth': [2,3,5],\n    'min_samples_split': [0.01, 0.1, 0.5],    \n}\n\ngrid_search = GridSearchCV(GradientBoostingClassifier(random_state=1), cv=5, param_grid=param_grid, scoring='roc_auc', verbose = 1, n_jobs = 2)\ngrid_search.fit(X_train, y_train)\n\nprint('Best params found: \\n\\t', grid_search.best_params_)\nprint('Best ROC_AUC score found: \\n\\t', grid_search.best_score_)\nprint('Best estimator: \\n\\t', grid_search.best_estimator_)\n'''\n\nprint('grid search - first model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''param_grid = {\n    'learning_rate': [0.01, 0.1, 0.2],\n    'n_estimators': [50, 100, 150],\n    'max_depth': [2,3,5,10],\n    'min_samples_split': [0.0, 0.1, 0.5],\n    'min_samples_leaf': [0.0, 0.1, 0.5],\n    'min_impurity_decrease': [0.0, 0.1,0.5]\n}\n\ngrid_search = GridSearchCV(GradientBoostingClassifier(random_state=1), cv=5, param_grid=param_grid, scoring='roc_auc', verbose = 1, n_jobs = -1)\ngrid_search.fit(X_train, y_train)\n\nprint('Best params found: \\n\\t', grid_search.best_params_)\nprint('Best ROC_AUC score found: \\n\\t', grid_search.best_score_)\nprint('Best estimator: \\n\\t', grid_search.best_estimator_)'''\n\nprint('grid search - second model')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}