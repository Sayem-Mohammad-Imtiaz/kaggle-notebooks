{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Bruno Viera Ribeiro - 09/2020","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Project Healthy Diet (fighting COVID-19)\n\nHow can eating habits help fight the current COVID-19 pandemic? A healthy diet is very important to prevent and recover from various infections. Keeping a healthy immune system is a **must** in our current situation, and what we eat (along with exercising and clearing our heads every now and then) is key.\n\nWhile it is clear that good nutrition alone will not cure nor prevent the spread of COVID-19, it helps us fight back in the case of infection and prevents several other health issues. A lot of tips can be found in [this](https://www.who.int/campaigns/connecting-the-world-to-combat-coronavirus/healthyathome/healthyathome---healthy-diet) very usefull and clear page kept by WHO (World Health Organization).\n\nIn this project, we will use data *from food intake by countries* along with data associated with the *spread of COVID-19 and other health issues* the help get new insights into the importance of nutrition and eating habits to combat spreading diseases.\n\nData for this project is taken from [this](https://www.kaggle.com/mariaren/covid19-healthy-diet-dataset) very interesting kaggle dataset. From the owner of the dataset:\n\n> In this dataset, I have combined data of different types of food, world population obesity and undernourished rate, and global COVID-19 cases count from around the world in order to learn more about how a healthy eating style could help combat the Corona Virus. And from the dataset, we can gather information regarding diet patterns from countries with lower COVID infection rate, and adjust our own diet accordingly\n\nThere are 5 files in the dataset:\n* Fat_Supply_Quantity_Data.csv: percentage of fat intake from different food groups for 170 different countries.\n* Food_Supply_Quantity_kg_Data.csv: percentage of food intake( in $kg$ ) from different food groups for 170 different countries.\n* Food_Supply_kcal_Data.csv: percentage of energy intake (in $kcal$) from different food groups for 170 different countries.\n* Protein_Supply_Quantity_Data.csv: percentage of protein intake from different food groups for 170 different countries.\n    * All of these files have, also, columns including obesity, undernourishment and COVID-19 cases as percentages of total population.\n* Supply_Food_Data_Descriptions.csv: This dataset is obtained from FAO.org, and is used to show the specific types of food that belongs to each category for the above datasets.\n"},{"metadata":{},"cell_type":"markdown","source":"Now we can dig into the files."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's start by looking into the descriptions"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)\ndesc_df = pd.read_csv('../input/covid19-healthy-diet-dataset/Supply_Food_Data_Descriptions.csv', index_col = 'Categories')\ndesc_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like we might have some redundant categories. Reading `Animal Products` and `Vegetal Products`, it seems they are a summary of other categories. We should be carefull when using these categories for modeling."},{"metadata":{},"cell_type":"markdown","source":"# Food intake (in kg) by food group\n\n## Data cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df_full = pd.read_csv('../input/covid19-healthy-diet-dataset/Food_Supply_Quantity_kg_Data.csv')\nkg_df_full.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df_full.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df_full.columns.size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's drop the last column as it is just a unit information\nkg_df = kg_df_full.drop('Unit (all except Population)', axis = 1)\nkg_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Beyond the columns described in the `Categories` from the data description, we have 7 other columns:\n* Obesity: obesity rate\n* Undernourished: undernourished rate\n* Confirmed: confirmed cases of COVID-19, by population\n* Deaths: confirmed deaths from COVID-19, by population\n* Recovered: recovered cases of COVID-19, by population\n* Active: active cases of COVID-19, by population\n* Population: country population"},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have some missing data from these last columns. We'll start by simply dropping these data."},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df = kg_df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Something is not a number in the `Undernourished` columns. Let's inspect:"},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df['Undernourished'][:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df['Undernourished'][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OK, so we have strings and some of them are of the form '<2.5'. Let's replace these values with '2.0', as a very crude way of dealing with these values. We need to remember, in the analysis, that all values '2.0' represent something below 2.5."},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df.loc[kg_df['Undernourished'] == '<2.5', 'Undernourished'] = '2.0'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df['Undernourished'][:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, to turn data into numeric types:"},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df['Undernourished'] = pd.to_numeric(kg_df['Undernourished'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have no missing values and all data is numeric, except for country names."},{"metadata":{},"cell_type":"markdown","source":"##  General COVID-19 data: analysis and further cleaning\n\nBefore digging into the data from food intake, let's create a simple visualization of COVID-19 cases by country."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(kg_df, x=\"Confirmed\", y = \"Deaths\",size = \"Active\", hover_name='Country', log_x=False,\n                 size_max=30, trendline = \"ols\", marginal_x = \"box\",marginal_y = \"violin\", template=\"simple_white\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, the size of points corresponds to the active cases of COVID-19. As expected, there is a tendency of having more deaths where more confirmed cases are present."},{"metadata":{},"cell_type":"markdown","source":"Now, to understand the dataset a bit more clearly, let's do some sanity checks."},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What is the sum of `Animal Products` and `Vegetal Products`?"},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df['Animal Products'] + kg_df['Vegetal Products']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(kg_df['Animal Products'] + kg_df['Vegetal Products']).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, for all countries this sum appears to be roughly $50 \\%$ of food intake in $kg$. That is strange, as this two are a sum of all other columns.\n\nTo understand the data better, let's sum all food related categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df.iloc[:, 1:24].sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, so it looks like we are counting twice every entry inside `Animal Products` and `Vegetal Products`. From my understanding, `Animal Products` + `Vegetal Products` should sum to $100\\%$ of the food intake. This is easily fixed by multiplying all columns of food categories by 2."},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df.iloc[:,1:24] = kg_df.iloc[:, 1:24] * 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(kg_df['Animal Products'] + kg_df['Vegetal Products']).round(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(kg_df['Animal Products'] + kg_df['Vegetal Products']).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That fixed the issue. Now, let's do some sanity checks with the COVID-19 categories. \n\nColumns related to this are: **'Confirmed', 'Deaths', 'Recovered', 'Active'**.\n\nIf my understanding is correct, the number of confirmed cases should be the sum of deaths, recoverd and active. Let's investigate."},{"metadata":{"trusted":true},"cell_type":"code","source":"(kg_df['Confirmed'] - (kg_df['Deaths'] + kg_df['Recovered'] + kg_df['Active'])).round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! Our understanding is correct.\n\nTo further investigate the impact of deaths by COVID-19, we will create a column `Mortality` which will be calculated as `Deaths` by `Confirmed`."},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df['Mortality'] = kg_df['Deaths']/kg_df['Confirmed']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df['Mortality']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Next, we'll look at some general distributions from the COVID-19 data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distributions\nfig = px.bar(kg_df, x = \"Country\", y =\"Confirmed\").update_xaxes(categoryorder=\"total descending\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distributions\nfig = px.bar(kg_df, x = \"Country\", y =\"Deaths\").update_xaxes(categoryorder=\"total descending\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distributions\nfig = px.bar(kg_df, x = \"Country\", y =\"Active\").update_xaxes(categoryorder=\"total descending\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distributions\nfig = px.bar(kg_df, x = \"Country\", y =\"Mortality\").update_xaxes(categoryorder=\"total descending\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this last figure, we can see that `Yemen` stands out as having a very alarming mortality (almost $30\\%$). However, `Yemen` also appears as one of the lowest death rate countries (death rate of 0.001955)."},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df[kg_df.Country == 'Yemen']['Deaths']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Investigate: does obesity rate affect impact of COVID-19?\n\nThere is a nice report from **Science** ([sciencemag](https://www.sciencemag.org/)) linking obesity to COVID-19 mortalitiy:\n* [Why COVID-19 is more deadly in people with obesity—even if they’re young](https://www.sciencemag.org/news/2020/09/why-covid-19-more-deadly-people-obesity-even-if-theyre-young)\n\nFrom the authors:\n> Since the pandemic began, dozens of studies have reported that many of the sickest COVID-19 patients have been people with obesity. In recent weeks, that link has come into sharper focus as large new population studies have cemented the association and demonstrated that even people who are merely overweight are at higher risk.\n\nOur hypothesis is that we can find a pattern from this datset supporting this report. To do so, we'll start by simply plotting the Obesity rate against our newly defined Mortality."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(kg_df[kg_df.Country != 'Yemen'], x=\"Mortality\", y = \"Obesity\", size = \"Active\", hover_name='Country', log_x=False,\n                 size_max=30, template=\"simple_white\")\n\nfig.add_shape(\n        # Line Horizontal\n            type=\"line\",\n            x0=0,\n            y0=kg_df[kg_df.Country != 'Yemen']['Obesity'].mean(),\n            x1=kg_df[kg_df.Country != 'Yemen']['Mortality'].max(),\n            y1=kg_df[kg_df.Country != 'Yemen']['Obesity'].mean(),\n            line=dict(\n                color=\"crimson\",\n                width=4\n            ),\n    )\n\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The red line represents the avergae obesity rate among countries. In this analysis, we have excluded \"Yemen\", as it was far above the \"main cluster\" of other countries. To clarify, here is the same graph including \"Yemen\":"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(kg_df, x=\"Mortality\", y = \"Obesity\", size = \"Active\", hover_name='Country', log_x=False,\n                 size_max=30, template=\"simple_white\")\n\nfig.add_shape(\n        # Line Horizontal\n            type=\"line\",\n            x0=0,\n            y0=kg_df['Obesity'].mean(),\n            x1=kg_df['Mortality'].max(),\n            y1=kg_df['Obesity'].mean(),\n            line=dict(\n                color=\"crimson\",\n                width=4\n            ),\n    )\n\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(kg_df, x=\"Deaths\", y = \"Obesity\", size = \"Mortality\",\n                 hover_name='Country', log_x=False, size_max=30, template=\"simple_white\")\n\nfig.add_shape(\n        # Line Horizontal\n            type=\"line\",\n            x0=0,\n            y0=kg_df['Obesity'].mean(),\n            x1=kg_df['Deaths'].max(),\n            y1=kg_df['Obesity'].mean(),\n            line=dict(\n                color=\"crimson\",\n                width=4\n            ),\n    )\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this figure, the size of the points correspond to the country's COVID-19 mortality. Here we can see that Yemen indeed stands out as having a big mortality (the huge point just bellow the mean obesity red line)."},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df[kg_df.Obesity < kg_df['Obesity'].mean()].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df[kg_df.Obesity > kg_df['Obesity'].mean()].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our finding: **The \"high mortality\" and \"high death rate\" countries all seem to have an above average obesity rate.**"},{"metadata":{},"cell_type":"markdown","source":"## Distribution of food intake (in kg) - exploring high obesity cases\n\n<mark style=\"background-color: lightblue\">Let's inspect this further. What can we say about the food intake in countries grouped by obesity rate?</mark>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_high_ob = kg_df[kg_df.Obesity > kg_df['Obesity'].mean()]\ndf_low_ob = kg_df[kg_df.Obesity <= kg_df['Obesity'].mean()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df['ObesityAboveAvg'] = (kg_df[\"Obesity\"] > kg_df['Obesity'].mean()).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have created a column `ObesityAboveAvg` that has value **1** if the country has obesity rate above the mean of all other countries, and **0** otherwise."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(kg_df, x = \"Animal Products\", nbins=50, color = \"ObesityAboveAvg\", marginal=\"rug\")\n\nfig.add_shape(\n        # Mean value of Animal Products intake in low obesity countries\n            type=\"line\",\n            x0=df_low_ob['Animal Products'].median(),\n            y0=0,\n            x1=df_low_ob['Animal Products'].median(),\n            y1=12,\n            line=dict(\n                color=\"darkblue\",\n                width=4\n            ),\n    )\n\nfig.add_shape(\n        # Mean value of Animal Products intake in high obesity countries\n            type=\"line\",\n            x0=df_high_ob['Animal Products'].median(),\n            y0=0,\n            x1=df_high_ob['Animal Products'].median(),\n            y1=12,\n            line=dict(\n                color=\"crimson\",\n                width=4\n            ),\n    )\n\n\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(kg_df, x = \"Vegetal Products\", nbins=50, color = \"ObesityAboveAvg\", marginal=\"rug\")\n\nfig.add_shape(\n        # Mean value of Vegetal Products intake in low obesity countries\n            type=\"line\",\n            x0=df_low_ob['Vegetal Products'].median(),\n            y0=0,\n            x1=df_low_ob['Vegetal Products'].median(),\n            y1=12,\n            line=dict(\n                color=\"darkblue\",\n                width=4\n            ),\n    )\n\nfig.add_shape(\n        # Mean value of Vegetal Products intake in high obesity countries\n            type=\"line\",\n            x0=df_high_ob['Vegetal Products'].median(),\n            y0=0,\n            x1=df_high_ob['Vegetal Products'].median(),\n            y1=12,\n            line=dict(\n                color=\"crimson\",\n                width=4\n            ),\n    )\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This might be a naive first analysis, but countries with obesity rates above the mean of all countries have a higher consumption of `Animal Products` and lower consuption of `Vegetal Products`. The vertical lines in both figures represent the **median** value of intake for each group."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(kg_df, x = \"Country\", y =\"Deaths\", facet_col = \"ObesityAboveAvg\")\nfig.update_xaxes(matches=None,categoryorder=\"total descending\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the figure above, we can see **clearly** that the \"high obesity rate\" countries have a worst impact from COVID-19."},{"metadata":{},"cell_type":"markdown","source":"## Distribution of food intake (in kg) by product type\n\nOk, now we can dig into the separate food types (`Animal Products` and `Vegetal Products`) to see their distributions.\n\nFirst, let's define a list for the features in each food type:"},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"animal_features = ['Animal fats', 'Aquatic Products, Other', 'Eggs', 'Fish, Seafood', 'Meat',\n                   'Milk - Excluding Butter', 'Offals']\nvegetal_features = ['Alcoholic Beverages', 'Cereals - Excluding Beer', 'Fruits - Excluding Wine', 'Miscellaneous', 'Oilcrops', 'Pulses',\n                    'Spices', 'Starchy Roots', 'Stimulants', 'Sugar & Sweeteners', 'Sugar Crops', 'Treenuts',\n                    'Vegetable Oils', 'Vegetables']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sanity check\nkg_df[animal_features + vegetal_features].sum(axis=1).round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_high_ob.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have a list with all categories within `Animal Products`, we can check the distribution of intake in our defined \"High Obesity\" and \"Low Obesity\" countries:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.pie(values = df_high_ob[animal_features].mean().tolist(), names = animal_features,\n             title='Mean food intake by Animal products groups - High Obesity Countries')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.pie(values = df_low_ob[animal_features].mean().tolist(), names = animal_features,\n             title='Mean food intake by Animal products groups - Low Obesity Countries')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, the distributions are somewhat similar. The order of highest to lowest intake is the same (except for `Offals` and `Animal fats`). However, two things stand out:\n* The `Milk - Excluding Butter` intake int he first group is huge (almost $60\\%$!)\n* The difference between the `Fish, Seafood` intake in both groups (the first - around $7\\%$, the second - around $20\\%$).\n\nLet's see the vegetal intake:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.pie(values = df_high_ob[vegetal_features].mean().tolist(), names = vegetal_features,\n             title='Mean food intake by Vegetal products groups - High Obesity Countries')\nfig.show()\n\nfig = px.pie(values = df_low_ob[vegetal_features].mean().tolist(), names = vegetal_features,\n             title='Mean food intake by Vegetal products groups - Low Obesity Countries')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's define, for ease of writting, the following: **HOC** - High Obesity Countries and **LOC** - Low Obesity Countries.\nHere, we have some major differences:\n* The intake of `Starchy Roots` in LOC is almost $20\\%$, double that of HOC.\n* The intake of `Alcoholic Beverages` is at $5.8\\%$ in LOC, as in HOC it reaches almost $10\\%$.\n* The intake of `Sugar & Sweeteners` is at $4.78\\%$ in LOC, as in HOC it reaches almost $9.5\\%$.\nSome others can be seen, but these caught my attention."},{"metadata":{},"cell_type":"markdown","source":"We can create a simple graph of `Vegetal Products` versus `Animal Products` to get a new visual on the distribtuion of **HOC** and **LOC**."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(kg_df, x = 'Animal Products', y ='Vegetal Products',\n                 color='ObesityAboveAvg', hover_name = 'Country')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The color corresponds to the `ObesityAboveAvg`. Here, again, we see there is a relation between high consuption of Animal Products (comparing with Vegetal Products) and high obesity rates. Using the hover information you can find the country with highest Animal Products intake (Finland) and the one with highest Vegetal Products intake (Nigeria)."},{"metadata":{},"cell_type":"markdown","source":"# Modeling - Classification\n\n## 1 - KNN for ObesityAboveAvg\n\nWe can do a couple of modeling exercises with this dataset. The first thing we'll try is to check if we can predict the `ObesityAboveAvg` column using food features. Let's start with some exploration:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ob = kg_df[animal_features+vegetal_features+['ObesityAboveAvg']]\ndf_ob.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ob.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ob.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ob_features = df_ob.columns.drop('ObesityAboveAvg')\nob_target = 'ObesityAboveAvg'\n\nprint('Model features: ', ob_features)\nprint('Model target: ', ob_target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training and test datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_data, test_data = train_test_split(df_ob, test_size = 0.2, shuffle = True, random_state = 28)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Target balancing"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training set shape:', train_data.shape)\n\nprint('Class 0 samples in the training set:', sum(train_data[ob_target] == 0))\nprint('Class 1 samples in the training set:', sum(train_data[ob_target] == 1))\n\nprint('Class 0 samples in the test set:', sum(test_data[ob_target] == 0))\nprint('Class 1 samples in the test set:', sum(test_data[ob_target] == 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want to fix any imbalance only in the training set. The test set should keep the original distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle\n\nclass_0_no = train_data[train_data[ob_target] == 0]\nclass_1_no = train_data[train_data[ob_target] == 1]\n\nupsampled_class_0_no = class_0_no.sample(n=len(class_1_no), replace=True, random_state=42)\n\ntrain_data = pd.concat([class_1_no, upsampled_class_0_no])\ntrain_data = shuffle(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training set shape:', train_data.shape)\n\nprint('Class 1 samples in the training set:', sum(train_data[ob_target] == 1))\nprint('Class 0 samples in the training set:', sum(train_data[ob_target] == 0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preprocessing pipeline\n\nFirst, we will do preprocessing on the training set. As there are no missing values, we will build a pipeline to scale features to have similar orders of magnitude by bringing all of them between 0 and 1 using MinMaxScaler and them apply a [KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\n\n## Defining the pipeline\n\nclassifier = Pipeline([\n    ('scaler', MinMaxScaler()),\n    ('estimator', KNeighborsClassifier(n_neighbors = 3))\n])\n\n# Visualize the pipeline\nfrom sklearn import set_config\nset_config(display='diagram')\nclassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training\n\nFirst we train our classifier with the **.fit()** method."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get train data\nX_train = train_data[ob_features]\ny_train = train_data[ob_target]\n\n# Fit the classifier\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n\n# Using the fitted model to make predicitions on the training set\n\ntrain_preds = classifier.predict(X_train)\n\nprint('Model performance on the train set:')\nprint(confusion_matrix(y_train, train_preds))\nprint(classification_report(y_train, train_preds))\nprint(\"Train accuracy:\", accuracy_score(y_train, train_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\n\ndisp = plot_confusion_matrix(classifier, X_train, y_train)\n\ndisp.ax_.set_title('Confusion matrix for train set');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's check performance on the test set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get data to test classifier\nX_test = test_data[ob_features]\ny_test = test_data[ob_target]\n\ntest_preds = classifier.predict(X_test)\n\nprint('Model performance on the test set:')\nprint(confusion_matrix(y_test, test_preds))\nprint(classification_report(y_test, test_preds))\nprint(\"Test accuracy:\", accuracy_score(y_test, test_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disp = plot_confusion_matrix(classifier, X_test, y_test)\n\ndisp.ax_.set_title('Confusion matrix for test set');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tunning the value of n_neighbors"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting k values to try on our validation performance\nk_values = list(range(1,11))\n\n# Creating a validation set within the train set\nsub_train_data, val_data = train_test_split(train_data, test_size = 0.2, shuffle = True, random_state = 28)\n\n# Upsampling to fix imbalance\nclass_0_no = sub_train_data[sub_train_data[ob_target] == 0]\nclass_1_no = sub_train_data[sub_train_data[ob_target] == 1]\n\nupsampled_class_0_no = class_0_no.sample(n=len(class_1_no), replace=True, random_state=42)\n\nsub_train_data = pd.concat([class_1_no, upsampled_class_0_no])\nsub_train_data = shuffle(sub_train_data, random_state = 28)\n\n# Creating training and validation sets\nX_sub_train = sub_train_data[ob_features]\ny_sub_train = sub_train_data[ob_target]\n\nX_val = val_data[ob_features]\ny_val = val_data[ob_target]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Searching for best performing K value\nfor k in k_values:\n    classifier = Pipeline([\n    ('scaler', MinMaxScaler()),\n    ('estimator', KNeighborsClassifier(n_neighbors = k))\n    ])\n    \n    classifier.fit(X_sub_train, y_sub_train)\n    val_preds = classifier.predict(X_val)\n    print(f\"K = {k} -- Test accuracy: {accuracy_score(y_val, val_preds)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**THIS HAS STOCHASTIC NATURE AND WILL CHANGE EACH RUN:** It looks that K = 2 has the best performance. Let's use K = 2 for our classifier, train it on the train set and test it on our test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the classifier\nclassifier = Pipeline([\n    ('scaler', MinMaxScaler()),\n    ('estimator', KNeighborsClassifier(n_neighbors = 2))\n])\n\n# Fit the classifier\nclassifier.fit(X_train, y_train)\n\n# Making predictions on test set\ntest_preds = classifier.predict(X_test)\n\nprint('Model performance on the test set:')\nprint(confusion_matrix(y_test, test_preds))\nprint(classification_report(y_test, test_preds))\nprint(\"Test accuracy:\", accuracy_score(y_test, test_preds))\n\ndisp = plot_confusion_matrix(classifier, X_test, y_test)\n\ndisp.ax_.set_title('Confusion matrix for test set - k = 2');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We gained a slight improvement in accuracy with this quick tunning."},{"metadata":{},"cell_type":"markdown","source":"### Using GridSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nknn = KNeighborsClassifier()\n\n# Creating a dictionary of all values to test\nparam_grid = {'n_neighbors': np.arange(2,10)}\n\n# Use grid search to test all values\nknn_gscv = GridSearchCV(knn, param_grid, cv = 5)\n\n# Fit the model to data\nknn_gscv.fit(X_train, y_train)\n\n# Check for best parameter\nknn_gscv.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy when at best parameters\nknn_gscv.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling - Regression\n\n## 2 - Prediciting mortality\n\nWe'll try to build a model (regressor) to predict the mortality rate based on food inatke information and obesity. Let's start by choosing the right features."},{"metadata":{"trusted":true},"cell_type":"code","source":"kg_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_mort = kg_df[animal_features+vegetal_features+['Obesity','Mortality']]\ndf_mort = kg_df[kg_df.Country != 'Yemen'][animal_features+vegetal_features+['Obesity','Mortality']]\n# df_mort = kg_df[['Animal Products','Vegetal Products','Obesity','Mortality']]\n\ndf_mort = shuffle(df_mort)\n\nmort_features = df_mort.columns.drop('Mortality')\nmort_target = 'Mortality'\n\nprint('Model features: ', mort_features)\nprint('Model target: ', mort_target)\n\nX = df_mort[mort_features]\ny = df_mort[mort_target]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train-test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, test_data = train_test_split(df_mort, test_size = 0.2, shuffle = True, random_state = 28)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the data before building our models.\n\nWe'll start with some visuals and build a scatter matrix for our dataframe. Because we have a large number of features, we will only plot those among the top intake in HOC (check the pie chart for HOC intake)."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_mort[['Meat', 'Milk - Excluding Butter', 'Fish, Seafood',\n                         'Cereals - Excluding Beer', 'Obesity','Mortality']]\ng = sns.PairGrid(df)\ng.map(plt.scatter)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we are interested in the last \"row\" of this matrix. Nothing seems particularly linear, but we'll see what we can tell from building linear models.\n\n**NOTE**: as the data seems very scattered, I am expecting bad values of $R^2$ score (for an interessting explanation on why this is so, I recommend reading this two well written articles: [Interpreting R-squared](https://statisticsbyjim.com/regression/interpret-r-squared-regression/) and [Interpreting low R-squared in regression models](https://statisticsbyjim.com/regression/low-r-squared-regression/))."},{"metadata":{},"cell_type":"markdown","source":"To keep track of what is important later on, let's check what features correlate the most with our target."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mort.corr().tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the correlation matrix we can sort the values of the `Mortality` row to get the info we need."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mort.corr().loc['Mortality'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, the highest correlation with `Mortality` is `Milk - Excluding Butter` (note that this is the highest intake from both HOC and LOC).\n\nLet's actually build some models now."},{"metadata":{},"cell_type":"markdown","source":"### [Ridge Regression](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression)\n\nFirst, we'll split our training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get train data\nX_train = train_data[mort_features]\ny_train = train_data[mort_target]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, to build our regressor with a standardization step in our pipeline (always scale your data for Ridge regression!)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\n\n## Defining the pipeline\n\nregressor = Pipeline([\n    ('scaler', StandardScaler()),\n    ('estimator', Ridge(random_state=28))\n])\n\n# Visualize the pipeline\nfrom sklearn import set_config\nset_config(display='diagram')\nregressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training\nregressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scoring the training set\n\ntrain_preds = regressor.predict(X_train)\nregressor.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Cross validate our score"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross validate\ncv_score = cross_val_score(regressor, X_train, y_train, cv = 10)\nprint(cv_score)\nprint(cv_score.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This looks very bad... Let's see some other metrics for our model.\n\nWe'll use [mean squared error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error), [mean absolute error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error) and [R2](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score) to evaluate the model.\n\nFirst, let's build a simple helper function to return a dictionary with all of our scores for the chosen model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Create function to evaluate model on a few different scores\ndef show_scores(model, X_train, X_test, y_train, y_test):    \n    train_preds = model.predict(X_train)\n    test_preds = model.predict(X_test)\n    scores = {'Training MAE': mean_absolute_error(y_train, train_preds),\n              'Test MAE': mean_absolute_error(y_test, test_preds),\n              'Training MSE': mean_squared_error(y_train, train_preds),\n              'Test MSE': mean_squared_error(y_test, test_preds),\n              'Training R^2': r2_score(y_train, train_preds),\n              'Test R^2': r2_score(y_test, test_preds)}\n    return scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's test the model in the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get data to test model\nX_test = test_data[mort_features]\ny_test = test_data[mort_target]\n\nshow_scores(regressor, X_train, X_test , y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Visualizing\n\nLet's make a simple visualization of our model's predictions using the firts feature entry (`Animal fats`)."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_plot = X_test.copy()\ntest_plot['Mortality'] = y_test\ntest_plot['Mortality_pred'] = regressor.predict(X_test)\n\ntest_plot.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fig = px.scatter(test_plot, x = 'Animal fats', y = ['Mortality','Mortality_pred'],\n#                  trendline = \"ols\")\n\n\n# fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=[10,8])\n\nsns.regplot(x = 'Animal fats', y = 'Mortality', data = test_plot, ax = ax, label='Mortality')\nsns.regplot(x = 'Animal fats', y = 'Mortality_pred', data = test_plot, ax = ax, label='Mortality_pred')\n\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above we see a plot of `Mortality` as a function of the `Animal fats`. Our model fails to make a good prediction but it somehow captures the **tendency** of our target. Let's try to imporve by comparing other models."},{"metadata":{},"cell_type":"markdown","source":"### Training and testing multiple models\n\nNow that we have a general flow of testing our model, let's build a function to test different models.\n\nWe will use, besides our Ridge regressor, three other models:\n* [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR)\n* [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n*  [XGBoost Regressor](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn)\n\nAs tree base models don't require scaling as we have done for Ridge regressor, our function will have to account for scaling as a parameter. The main goal is to print out various metrics for each model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\n\n# First, we create a dict with our desired models\nmodels = {'Ridge':Ridge(random_state=28),\n          'SVR':SVR(),\n          'RandomForest':RandomForestRegressor(),\n          'XGBoost':XGBRegressor(n_estimators = 1000, learning_rate = 0.05)}\n\n# Now to build the function that tests each model\ndef model_build(model, X_train, y_train, X_test, y_test, scale=True):\n    \n    if scale:\n        regressor = Pipeline([\n            ('scaler', StandardScaler()),\n            ('estimator', model)\n        ])\n    \n    else:\n        regressor = Pipeline([\n            ('estimator', model)\n        ])\n\n    # Training\n    regressor.fit(X_train, y_train)\n\n    # Scoring the training set\n\n    train_preds = regressor.predict(X_train)\n    print(f\"R2 on single split: {regressor.score(X_train, y_train)}\")\n\n    # Cross validate\n    cv_score = cross_val_score(regressor, X_train, y_train, cv = 10)\n\n    print(f\"Cross validate R2 score: {cv_score.mean()}\")\n\n    # Scoring the test set\n    for k, v in show_scores(regressor, X_train, X_test , y_train, y_test).items():\n        print(\"     \", k, v)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have our helper function, we loop through our `models` dictionary and score each one of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"for name, model in models.items():\n    print(f\"==== Scoring {name} model====\")\n    \n    if name == 'RandomForest' or name == 'XGBoost':\n        model_build(model, X_train, y_train, X_test, y_test, scale=False)\n    else:\n        model_build(model, X_train, y_train, X_test, y_test,)\n    print()\n    print(40*\"=\")\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameter tunning for XGBoost model\n\nWe can pick our best perfroming model and try some hyperparameter tunning with a simple [GridSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n\nLet's start by defining our parameters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBRegressor()\n\nparameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'objective':['reg:squarederror'],\n              'learning_rate': [.03, 0.05, .07], #so called `eta` value\n              'max_depth': [5, 6, 7],\n              'min_child_weight': [4],\n              'subsample': [0.7],\n              'colsample_bytree': [0.7],\n              'n_estimators': [500, 1000]}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can do the search (note that it can take a long time)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n\n# xgb_grid = GridSearchCV(xgb, parameters, cv = 5, n_jobs = 4, verbose = True)\n\n# xgb_grid.fit(X_train, y_train)\n\n# print(xgb_grid.best_score_)\n# print(xgb_grid.best_params_)\n\n## RAN AND GOT THE PARAMETERS USED BELLOW","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_best = XGBRegressor(colsample_bytree = 0.7,\n                        learning_rate = 0.05,\n                        max_depth = 6,\n                        min_child_weight = 4,\n                        n_estimators = 500,\n                        nthread = 4,\n                        objective = 'reg:squarederror',\n                        subsample = 0.7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_build(xgb_best, X_train, y_train, X_test, y_test, scale=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1 - A simpler model for Mortality\n\nLet's try to reduce the dimensionality by using only two features: **Animal Products** and **Vegetal Products**."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mort2 = kg_df[kg_df.Country != 'Yemen'][['Animal Products','Vegetal Products','Obesity','Mortality']]\n\n\ndf_mort2 = shuffle(df_mort2)\n\nmort2_features = df_mort2.columns.drop('Mortality')\nmort2_target = 'Mortality'\n\nprint('Model features: ', mort2_features)\nprint('Model target: ', mort2_target)\n\nX = df_mort2[mort2_features]\ny = df_mort2[mort2_target]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mort2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's do a simple box visual of our distributions. To better see the `Mortality` distribution, we'll multiply the column by $1000$ just for the boxplot."},{"metadata":{"trusted":true},"cell_type":"code","source":"dummie = df_mort2.copy()\ndummie['Mortality'] = dummie['Mortality']*1000\n\n\nplt.figure(figsize=(10,10))\nsns.boxplot(data = dummie, palette = 'rainbow');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now to split our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, test_data = train_test_split(df_mort2, test_size = 0.2, shuffle = True, random_state = 28)\n\n# Get train data\nX_train = train_data[mort2_features]\ny_train = train_data[mort2_target]\n\n# Get data to test model\nX_test = test_data[mort2_features]\ny_test = test_data[mort2_target]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, we create a dict with our desired models\nmodels = {'Ridge':Ridge(random_state=28),\n          'SVR':SVR(),\n          'RandomForest':RandomForestRegressor(),\n          'XGBoost':XGBRegressor(n_estimators = 1000, learning_rate = 0.05)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for name, model in models.items():\n    print(f\"==== Scoring {name} model====\")\n    \n    if name == 'RandomForest' or name == 'XGBoost':\n        model_build(model, X_train, y_train, X_test, y_test, scale=False)\n    else:\n        model_build(model, X_train, y_train, X_test, y_test,)\n    print()\n    print(40*\"=\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building best performing model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestRegressor()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Making predictions and visualizing"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = model.predict(X_test)\n\ntest_plot = X_test.copy()\ntest_plot['Mortality'] = y_test\ntest_plot['Mortality_pred'] = test_preds\n\ntest_plot.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotTest(col, target, data):\n    fig, ax = plt.subplots(figsize=[10,8])\n\n    sns.regplot(x = col, y = target, data = data, ax = ax, label=target)\n    sns.regplot(x = col, y = target+'_pred', data = data, ax = ax, label=target+'_pred')\n\n    plt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To visualize the resulting model, let's plot target (`Mortality`) dependecy with all features separately. In each plot, let's see both **real** data and predicted data."},{"metadata":{"trusted":true},"cell_type":"code","source":"plotTest('Animal Products', 'Mortality', test_plot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotTest('Vegetal Products', 'Mortality', test_plot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotTest('Obesity', 'Mortality', test_plot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have very few data points and the data is very scattered (bad value for $R^2$). But, this simplified model seems to compare well with the more complex one."},{"metadata":{},"cell_type":"markdown","source":"## 3 - Prediciting obesity\n\nAs obesity has a higher correlation with all \"food features\" let's try to buil our models to predict the actaul obesity rate. We expect these models to have better metrics that the ones build to predict mortality."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_obes = kg_df[animal_features+vegetal_features+['Obesity']]\n\ndf_obes = shuffle(df_obes)\n\nobes_features = df_obes.columns.drop('Obesity')\nobes_target = 'Obesity'\n\nprint('Model features: ', obes_features)\nprint('Model target: ', obes_target)\n\nX = df_obes[obes_features]\ny = df_obes[obes_target]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the correlation of features with target:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_obes.corr().loc['Obesity'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train-test splitting**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, test_data = train_test_split(df_obes, test_size = 0.2, shuffle = True, random_state = 28)\n\n# Get train data\nX_train = train_data[obes_features]\ny_train = train_data[obes_target]\n\n# Get data to test model\nX_test = test_data[obes_features]\ny_test = test_data[obes_target]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can use the same workflow above to test different models. To keep things clear, we'll define, again, the dictionary of all models we'll be using:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, we create a dict with our desired models\nmodels = {'Ridge':Ridge(random_state=28),\n          'SVR':SVR(),\n          'RandomForest':RandomForestRegressor(),\n          'XGBoost':XGBRegressor(n_estimators = 1000, learning_rate = 0.05)}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can loop through these and use our `model_build` function once more:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for name, model in models.items():\n    print(f\"==== Scoring {name} model====\")\n    \n    if name == 'RandomForest' or name == 'XGBoost':\n        model_build(model, X_train, y_train, X_test, y_test, scale=False)\n    else:\n        model_build(model, X_train, y_train, X_test, y_test,)\n    print()\n    print(40*\"=\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building best performing model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBRegressor(n_estimators = 1000, learning_rate = 0.05)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Making predictions and visualizing"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = model.predict(X_test)\n\ntest_plot = X_test.copy()\ntest_plot['Obesity'] = y_test\ntest_plot['Obesity_pred'] = test_preds\n\ntest_plot.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def plotTest(col, target, data):\n#     fig, ax = plt.subplots(figsize=[10,8])\n\n#     sns.regplot(x = col, y = target, data = data, ax = ax, label=target)\n#     sns.regplot(x = col, y = target+'_pred', data = data, ax = ax, label=target+'_pred')\n\n#     plt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's make two plots:\n\n* First, the `Obesity` dependency on `Cereals - Excluding Beer`, as it has the most negative correlation with the target.\n* Second, the `Obesity` dependency on `Meat`, as it has the most positive correlation with the target.\n\nFor both graphs we'll plot the **real** values of `Obesity` and the ones predicted by our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"plotTest('Cereals - Excluding Beer', 'Obesity', test_plot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotTest('Meat', 'Obesity', test_plot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like our model is doing a good job predicting these feature's influence on the obesity rate."},{"metadata":{},"cell_type":"markdown","source":"## Clustering countries by obesity and mortality due to COVID-19\n\nNow that we have seen that there is a relation between `Obesity` rates and `Mortality` we can try to cluster countries together based on these features.\n\nThe first thing we have to do is to filter all other features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = kg_df[kg_df.Country != 'Yemen'][['Obesity', 'Mortality']]\n\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All centroid-based algorithms need a scaling step before modelling. And, as this is a case of unsupervised learning model, we don't need to split the data.\n\nLet's first instantiate our scaler."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\n\n# Fit the scaler\nscaler.fit(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform our data\nX_scaled = scaler.transform(X)\n\n# Sanity checks\nprint(X_scaled.mean(axis = 0))\n\nprint(X_scaled.std(axis=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K-means modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\n# Instantiate the model\nkmeans = KMeans(n_clusters = 3)\n\n# Fit the model\nkmeans.fit(X_scaled)\n\n# Make predictions\npreds = kmeans.predict(X_scaled)\n\nprint(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Amount of countries in each cluster\n\nunique_countries, counts_countries = np.unique(preds, return_counts=True)\nprint(unique_countries)\nprint(counts_countries)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<mark style=\"background-color: lightblue\">We excluded Yemen as it is an outlier in the `Mortality` distirbution.</mark>"},{"metadata":{},"cell_type":"markdown","source":"#### Visualizing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_vis = kg_df[kg_df.Country != 'Yemen'].copy()\ndf_vis['cluster'] = [str(i) for i in preds]\n\ndf_vis.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(df_vis, x = 'Mortality', y = 'Obesity', color = 'cluster', hover_name = 'Country')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can find an optimal value for $k$ (number of clusters) using the \"elbow\" method."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate inertia for a range of clusters number\ninertia = []\n\nfor i in np.arange(1,11):\n    km = KMeans(n_clusters = i)\n    km.fit(X_scaled)\n    inertia.append(km.inertia_)\n    \n# Plotting\nplt.plot(np.arange(1,11), inertia, marker = 'o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.grid()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears $k = 3$ already is a good value for our modelling."},{"metadata":{},"cell_type":"markdown","source":"We can wrap this whole clustering process in a function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cluster_preds(df, feat1, feat2, k):\n    X = df[[feat1, feat2]]\n\n    # Scaling\n    scaler = StandardScaler()\n\n    # Fit the scaler\n    scaler.fit(X)\n\n    # Transform our data\n    X_scaled = scaler.transform(X)\n\n    # Instantiate the model\n    kmeans = KMeans(n_clusters = k)\n\n    # Fit the model\n    kmeans.fit(X_scaled)\n\n    # Make predictions\n    preds = kmeans.predict(X_scaled)\n\n    # Visualizing\n    df_vis = df.copy()\n    df_vis['cluster'] = [str(i) for i in preds]\n\n    fig = px.scatter(df_vis, x = feat1, y = feat2, color = 'cluster', hover_name = 'Country')\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can quickly cluster together countries based on `Animal Products` intake and `Obesity` rate (recalling we used this features in our \"simpler model to predict mortality\")."},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_preds(kg_df, 'Animal Products', 'Obesity', 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a further look into COVID-19 impact, we can cluster countries based on `Deaths` by `Confirmed` cases (referring to `Mortality`)."},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_preds(kg_df, 'Confirmed', 'Deaths', 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusions\n\nThere are MANY factors that are important to fight against the current COVID-19 epidemic. Maintaining good eating habits helps keep our immune system healthy and ready to combat a possible disease.\n\nIn this notebook I tried to explore possible patterns found in data of COVID-19 and food intake in different countries. One major goal was to find the influence of obesity rates in the effect of the disease in each country. Splitting countries into HOC and LOC groups, it was possible to create a classifier, with good accuracy, predicting in which group would a country be based on its food intake data.\n\nHaving this, we created regression models to try to predict the `Mortality` of COVID-19 in countries based on ther eating habits and obesity rate. Two approaches were taken: one with all food related features taken as parameters and a simpler one. Both have issues (mainly of spread and non-linearity), but we could show use of different models and metrics.\n\nNext, we build a model to predict `Obesity` rates based on eating habits in each country. This model was far more succesfull and the overall tendecy of the data was captured and predicted.\n\nFinally, we build a quick helper function to do some clustering based on pairs of features.\n\nFor a more visual data exploration, I have built simple dashboards to do some EDA with Dash:\n- [App1](https://healthycovid19app1.herokuapp.com/)\n- [App2](https://healthycovid19app2.herokuapp.com/)\n- [App3](https://healthycovid19app3.herokuapp.com/)\n\nThe app was split into 3 to avoid long processes erros in Heroku.\n\nPlease comment if you liked the notebook and critic if you found any inaccuracies. I am still very new to the field and this is my second ever notebook, so suggestions are very welcome!\n\nStay safe everyone!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}