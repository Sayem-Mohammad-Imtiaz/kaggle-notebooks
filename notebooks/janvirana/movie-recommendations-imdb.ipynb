{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"_is_fork":false,"language_info":{"pygments_lexer":"ipython3","name":"python","file_extension":".py","mimetype":"text/x-python","version":"3.6.1","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3}},"_change_revision":0},"cells":[{"metadata":{"_execution_state":"idle","_uuid":"8741d5c498cf9d8306dc33460f952edbd99cfe5e","_cell_guid":"3d9e6727-dbbb-82b3-7807-a6b25a7519b0"},"execution_count":null,"cell_type":"markdown","source":"Here's an attempt to create a recommendation engine with this dataset. Our Naive assumption is that a person's taste in film does not evolve with time.","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"6edce1c090a8266c991e28545230886f65b4c28d","_cell_guid":"26da2a2f-dd97-c4cd-6bd8-a7570b95dce9"},"execution_count":null,"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n%pylab inline\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"412e2b4cb3095e34e5d7e97566a10b1d678d341c","_cell_guid":"9a073912-e84f-f2aa-51f1-853a89b8b8e6"},"execution_count":null,"cell_type":"code","source":"df = pd.read_csv('../input/movie_metadata.csv')\ndf.info()","outputs":[]},{"metadata":{"_uuid":"99a6688ad05c85f52b12e74934a9dce1efb4bca3","_cell_guid":"5ec12d92-e36c-f201-50f3-15b3f9333fa0"},"execution_count":null,"cell_type":"markdown","source":"Since cleaning the data is not the focus of this notebook, I'll just dump it all in one cell. That way we can skip over to the nice parts.","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"6cb0fa1af77bced710419280f37411d33b490895","_cell_guid":"71196701-30ef-1c69-2736-7aa8ae69aac0"},"execution_count":null,"cell_type":"code","source":"first_actors = set(df.actor_1_name.unique())\nsecond_actors = set(df.actor_2_name.unique())\nthird_actors = set(df.actor_3_name.unique())\nprint('Those only in first name', len(first_actors - second_actors - third_actors))\nprint('Those only in second name', len(second_actors - first_actors - third_actors))\nprint('Those only in third name', len(third_actors - first_actors - second_actors))\n# ----is it color or not\ndf.color = df.color.map({'Color': 1, ' Black and White':0})\n# ---- Genres as on-off flags instead of strings\nunique_genre_labels = set()\nfor genre_flags in df.genres.str.split('|').values:\n    unique_genre_labels = unique_genre_labels.union(set(genre_flags))\nfor label in unique_genre_labels:\n    df['Genre='+label] = df.genres.str.contains(label).astype(int)\ndf = df.drop('genres', axis=1)\n\n# Titles are supposed to be unique right?\nif len(df.drop_duplicates(subset=['movie_title',\n                                  'title_year',\n                                  'movie_imdb_link'])) < len(df):\n    print('Duplicate Titles Exist')\n    # Let's see these duplicates.\n    duplicates = df[df.movie_title.map(df.movie_title.value_counts() > 1)]\n    duplicates.sort('movie_title')[['movie_title', 'title_year']]\n    # Looks like there are duplicates after all. Let's drop those.\n    df = df.drop_duplicates(subset=['movie_title', 'title_year', 'movie_imdb_link'])\n    # df.info()\ncounts = df.language.value_counts()\ndf.language = df.language.map(counts)\n#df.language\ncount = df.country.value_counts()\ndf.country = df.country.map(count)\n#df.country\ncounts = df.content_rating.value_counts()\ndf.content_rating = df.content_rating.map(counts)\n#df.content_rating\n#df.plot_keywords.head()\nunique_words = set()\nfor wordlist in df.plot_keywords.str.split('|').values:\n    if wordlist is not np.nan:\n        unique_words = unique_words.union(set(wordlist))\nplot_wordbag = list(unique_words)\nfor word in plot_wordbag:\n    df['plot_has_' + word.replace(' ', '-')] = df.plot_keywords.str.contains(word).astype(float)\ndf = df.drop('plot_keywords', axis=1)\n# Is anything left to be done other than imputing?\nprint(df.select_dtypes(include=['O']).columns)\n# We replace director name with counts of movies they've done\ndf.director_name = df.director_name.map(df.director_name.value_counts())\n# We replace actor names with the number of movies they appear in.\ncounts = pd.concat([df.actor_1_name, df.actor_2_name, df.actor_3_name]).value_counts()\n#counts.head()\ndf.actor_1_name = df.actor_1_name.map(counts)\ndf.actor_2_name = df.actor_2_name.map(counts)\ndf.actor_3_name = df.actor_3_name.map(counts)\n# I have no clue what to do with the title. I'll keep it for now in order to search by name\ndf = df.drop(['movie_imdb_link'], axis=1)\n# Let's check if anything is left as object\ndf.select_dtypes(include=['O']).columns","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"db1e0ed95196a6b92e4d0035be8ea26857d37e40","_cell_guid":"b52b8647-e1e6-f60d-ac28-8f61a7d29a36"},"execution_count":null,"cell_type":"code","source":"df.shape","outputs":[]},{"metadata":{"_uuid":"b4054b87074cf6b0e7fa2234adc3a5e5c69084d6","_cell_guid":"e884a5d2-f853-4654-eea4-1aaa3f0c2f9b"},"execution_count":null,"cell_type":"markdown","source":"# Now the data is clean enough. Recommend already!\nIt's filled with holes though. Pun intended. :D\n\nI wanted to try out some fancy imputation (there's a package by that name too) so here goes.","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"448e0cd4f0e423f5150749544000e2aca86ae795","_cell_guid":"40fe4659-32ce-b5e0-7a44-a6b8206ed607"},"execution_count":null,"cell_type":"code","source":"# hold your horses, we still need to fill those missing values.\nnew_style = {'grid': False}\nmatplotlib.rc('axes', **new_style)\nplt.matshow(~df.isnull())\nplt.title('Missing values in the data')","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"c123d958578d5b014998135aac1bab224ab22d09","_cell_guid":"2a879086-6506-4b81-acec-9c090ff7f886"},"execution_count":null,"cell_type":"code","source":"# Let's get those rows which are mostly incomplete. I suspect this was because of our\n# new features being created from old ones which were null.\nnullcount = df.isnull().sum(axis=1)\n# Let's just keep those who have less than a hundred missing values\nndf = df.dropna(thresh=100)\nprint(ndf.shape, df.shape)\n# Let's see those nulls again\nplt.matshow(~ndf.isnull())\nplt.title('Missing values in the data')","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"19626f6c07ec60fef409ddd5b16e970cee31ab61","_cell_guid":"bea5151d-285f-e2f9-f609-b3c17ed8bc70"},"execution_count":null,"cell_type":"code","source":"# We'll treat fillna as a regression / classification problem here.\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\ndef reg_class_fill(df, column, classifier):\n    \"\"\"Treat missing values as a classification / regresion problem\"\"\"\n    ndf = df.dropna(subset=[col for col in df.columns if col != column])\n    nullmask = ndf[column].isnull()\n    train, test  = ndf[~nullmask], ndf[nullmask]\n    train_x, train_y = train.drop(column, axis=1), train[column]\n    classifier.fit(train_x, train_y)\n    if len(test) > 0:\n        test_x, test_y = test.drop(column, axis=1), test[column]\n        values = classifier.predict(test_x)\n        test_y = values\n        new_x, new_y = pd.concat([train_x, test_x]), pd.concat([train_y, test_y])\n        newdf = new_x[column] = new_y\n        return newdf\n    else:\n        return ndf","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"638243314f1c640b1cb1626ac85fcc26936536b6","_cell_guid":"b3bcdbcf-d9b0-a2a4-eba0-874afd2809c5"},"execution_count":null,"cell_type":"code","source":"r, c = KNeighborsRegressor, KNeighborsClassifier  # Regress or classify\ntitle_encoder = LabelEncoder()\ntitle_encoder.fit(ndf.movie_title)\nndf.movie_title = title_encoder.transform(ndf.movie_title)","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"7348b67ab205725ff41fab1960c67c9692d7a02e","_cell_guid":"10652ebc-f97b-f755-9c1d-9d02502b5e03"},"execution_count":null,"cell_type":"code","source":"print(ndf[ndf.columns[:25]].isnull().sum())","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"c6d98393a409e44236ba75761f7a7999f74c51a7","_cell_guid":"5d914191-bc45-552a-06d8-428beab17cba"},"execution_count":null,"cell_type":"code","source":"# Since our imputation will impact other imputations, we specify an order\n# Typically we should do this independently and then combine the results, but meh for now\nimpute_order = [('director_name', c), ('title_year', c),\n                ('actor_1_name', c), ('actor_2_name', c), ('actor_3_name', c),\n                ('gross', r), ('budget', r), ('aspect_ratio', r),\n                ('content_rating', r), ('num_critic_for_reviews', r)]\nfor col, classifier in impute_order:\n    ndf = reg_class_fill(ndf, col, classifier())\n    print(col, 'Done')","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"07a35cd64b04ad28d441bfa0926187081793a179","_cell_guid":"9b2f424a-bf29-3947-9da7-54b5f0ea63fb"},"execution_count":null,"cell_type":"code","source":"# Again we check for what else needs to be imputed.\nndf[ndf.columns[:25]].isnull().sum()","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"fed179ee0778e58f2669a41cefa714eac3fa7a74","_cell_guid":"840b2ac6-3c18-86dd-fbe5-1e585d3469f2"},"execution_count":null,"cell_type":"code","source":"# Did we get everything?\nndf.isnull().sum().sum()","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"32c8a7ed081dca2deaeb0ac5f6e840ae9d997a88","_cell_guid":"231e40ad-7f59-b553-02c7-6b3b987550fd"},"execution_count":null,"cell_type":"code","source":"# YAY! We did indeed get everything, though it may not have been very good.\n# Now we redo the movie title transformation for our searches.\ntitles = title_encoder.inverse_transform(ndf.movie_title)\n#titles = [i.lower().strip() for i in titles]","outputs":[]},{"metadata":{"_uuid":"212e50d23f7596496c4bddd26815960b0c093593","_cell_guid":"04b7b550-410b-75c2-0386-e71968cf3a51"},"execution_count":null,"cell_type":"markdown","source":"# And we are ready to recommend stuff to you love :D\nWe build a simple KD tree recommender.","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"0d61613a21c32a570b6a10e16a4068caf3ea303e","_cell_guid":"430d50a5-e19d-464a-d147-0f63f52628a4"},"execution_count":null,"cell_type":"code","source":"# Give us 5 movies that you liked\ndef get_movies(names):\n    movies = []\n    for name in names:\n        found = [i for i in titles if name.lower() in i.lower()]\n        if len(found) > 0:\n            movies.append(found[0])\n            print(name, ': ', found, 'added', movies[-1], 'to movies')\n        else:\n            print(name, ': ', found)\n    print('-'*10)\n    print(movies)\n    moviecodes = title_encoder.transform(movies)\n    return moviecodes, movies\nnames = ['fight club', 'gump', # This one is Forrest Gump\n                 'usual suspects', 'silence of the lambs']\nmoviecodes, movies = get_movies(names)","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"aeeecc0b28eb295dd208c1541b8b41050f992476","_cell_guid":"1ad0d1dc-6463-34e2-c607-1621db382254"},"execution_count":null,"cell_type":"code","source":"data = ndf.drop('movie_title', axis=1)\ndata = MinMaxScaler().fit_transform(data)","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"989a2a4339547e8db94f6c0aee9d748d1c931d76","_cell_guid":"e5232c5b-a853-d9d3-e347-ccf00b371052"},"execution_count":null,"cell_type":"code","source":"# We assume KNN's assumptions as valid and proceede to compute a distance_matrix\nfrom sklearn.neighbors import KDTree\nfrom collections import Counter","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"ac653558ba999cb561041b407055f66e225c3a7d","_cell_guid":"5525c269-3a8a-bbc4-30fe-80cda7104fab"},"execution_count":null,"cell_type":"code","source":"movies","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"9fd8493d65ef83d16afb1910429f9d4196ba0715","_cell_guid":"cb779843-2087-0693-0037-338f7cf57714"},"execution_count":null,"cell_type":"code","source":"titles","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"7c99fff29cf96c7ed3bd7c464e354983f7aedb35","_cell_guid":"79828a7f-f51a-8596-8a1e-604c35c73ce3"},"execution_count":null,"cell_type":"code","source":"tree = KDTree(data, leaf_size=2)","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"a0d2bf08a03b8f24a2e8a0881a19338e36dc2683","_cell_guid":"052892fb-85b8-e806-f8ee-575cc30b8eb1"},"execution_count":null,"cell_type":"code","source":"def recommend(movies, tree, titles, data):\n    \"\"\"\n    It is assumed that the movies are in order of decreasing like-able-ness\n    Recommend movies on the basis of the KDTree generated.\n    Return them in order of increasing distance form knowns.\n    \"\"\"\n    titles = list(titles)\n    length, recommendations = len(movies) + 1,[]\n    \n    for i, movie in enumerate(movies):\n        weight = length - i\n        dist, index = tree.query([data[titles.index(movie)]], k=3)\n        for d, m in zip(dist[0], index[0]):\n            recommendations.append((d*weight, titles[m]))\n    recommendations.sort()\n    # Stuff is reorganized by frequency.\n    rec = [i[1].strip() for i in recommendations if i[1] not in movies]\n    rec = [i[1] for i in sorted([(v, k) for k, v in Counter(rec).items()],\n                                reverse=True)]\n    return rec","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"070194c5b00a829155979c0db498271589bf7bd9","_cell_guid":"7c63a473-5399-e8f3-8d23-cd24acfdcf12"},"execution_count":null,"cell_type":"code","source":"\nrec = recommend(movies, tree, titles, data)\n\nprint('Rank | Movie')\nprint('-----|------')\nfmt = '{}.   | {}'\nfor index, movie in enumerate(rec[:9]):\n    print(fmt.format(index + 1, movie))","outputs":[]},{"metadata":{"_uuid":"ed27ca4c24c663806e4aafc5dbeddff42e027f1a","_cell_guid":"da79ebde-e74e-7a8f-4b76-0df5bab6d8b6"},"execution_count":null,"cell_type":"markdown","source":"# Tadaa!\nIt's not very neat and awesome! But I did like Untraceable to be honest. \nSome movies are recommended twice! Probably because they are quiet close to multiple choices.\n\n## What else can be done?\n\n- Feature generation: I've done a nasty job of generating features. That could be cleaned up.\n- Imputation: A better way of imputing is welcome. Perhaps even need I say.\n- Some other recommendation method: So far I've only been able to discover KDTrees. If someone could write another one, awesome!\n\n*Upvote* to show your appreciation. :D\n\n# The final product\n\n1. Get movie titles\n2. Recommend","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"979e0795b461d243f1bbcc659a3a74a7d39f131b","_cell_guid":"9fe7a2e8-4792-a6e6-2a6a-6e71fd32691d"},"execution_count":null,"cell_type":"code","source":"names = ['hesher', 'leaving las vegas'] # dedicated to A.S.\nmoviecodes, movies = get_movies(names)\nrec = recommend(movies, tree, titles, data)\nprint('-'*50)\nprint('Recommending on the basis of the above movies')\nprint('-'*50)\nprint()\nprint('+-----|------')\nprint('|Rank | Movie')\nprint('+-----|------')\nfmt = '|{}.   | {}'\nfor index, movie in enumerate(rec[:9]):\n    print(fmt.format(index + 1, movie))\nprint('+-----|------')","outputs":[]}],"nbformat_minor":0,"nbformat":4}