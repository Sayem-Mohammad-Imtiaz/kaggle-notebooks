{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h3>Introduction</h3>\n\nFeature selection is a common phase in feature eningeering process. It is defined as \"selecting a subset of input features that are most relevant to the target variable\". When it comes to categorical data (nominal or ordinal), the main statistical techniques for feature selection are **Chi-squared** and **Mutual Information**.\n\nData scientists usually apply each technique in isolation and compare the results. Now the question is:\n\n> Can an inference based on both techniques *in conjuction*, improve the overall accuracy?\n \nThe present kernel will examine this *mixed inference* hypothesis. Without further ado, let's dive in and encode our categorical dataset:\n\n\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\ndataset = pd.read_csv(\"../input/mushroom-classification/mushrooms.csv\")\n\nX = dataset.iloc[:, 1:]\nX = X.astype(str)\ny = dataset.iloc[:,0]\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\noe = OrdinalEncoder()\noe.fit(X)\nX_enc = oe.transform(X)\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(y)\ny_enc = le.transform(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>A straightforward classifier</h3>\n\nHere for a quick proof-of-concept, we'll use Support Vector Classifier (SVC) with mostly default values, and just a light GridSearchCV on two SVC's main hyper-parameters (C and gamma). That said, for consistency, we'll follow the same footsteps for the rest of the models.\n\nThe models basically predict if a mushroom is poisonous or edible (that is, having the *class* column as target). First off, let's build the model based on *all* the \nfeatures provided by the dataset: ","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nparameters = {'C':(1, 10, 100), 'gamma':(0.1 ,1, 10)}\nclf_full_features = GridSearchCV(svc, parameters)\nclf_full_features.fit(X_enc, y_enc)\nprint(clf_full_features.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model hit the above score with either {C:100, gamma:0.1} or {C:10, gamma:0.1}. With this insight into the hyper-parameters let's adjust the values a bit hoping for a higher accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC()\nparameters = {'C':(5,200,300), 'gamma':(0.01, 0.05,0.1)}\nclf_full_features = GridSearchCV(svc, parameters)\nclf_full_features.fit(X_enc, y_enc)\nprint(clf_full_features.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We improved the accuracy a bit with either {C:300, gamma:0.05} or \n{C:200, gamma:0.05}. So it makes sense perhaps to have another shot with a higher value of C:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC()\nparameters = {'C':(200,300,400), 'gamma':(0.01, 0.05,0.1)}\nclf_full_features = GridSearchCV(svc, parameters)\nclf_full_features.fit(X_enc, y_enc)\nprint(clf_full_features.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No further improvement. So let's take 0.87957696 as the **baseline accuracy** and see if we can improve it through feature selection.\n\n<h3> Feature selection based on statistical measures </h3>\n\nAs the first step, we may plot the Chi-squared and Mutual Information on the dataset with the *class* feature as the target. We expect the plots to provide insight into the statistical importance or relevance of each feature to the  target feature:  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nfs1 = SelectKBest(score_func=chi2, k='all')\nfs1.fit(X_enc, y_enc)\nX_fs1 = fs1.transform(X_enc)\n\nfrom sklearn.feature_selection import mutual_info_classif\n\nfs2 = SelectKBest(score_func=mutual_info_classif, k='all')\nfs2.fit(X_enc, y_enc)\nX_fs2 = fs2.transform(X_enc)\n\nimport matplotlib.pyplot as plt\n\nx = np.arange(len(fs1.scores_))\n\nfig, ax = plt.subplots()\nfig.set_size_inches(18.5, 10.5)\n\nwidth = 0.35  \nrects1 = ax.bar(x - width/2, fs1.scores_, width, label='Chi-squared')\nrects2 = ax.bar(x + width/2, fs2.scores_*10000, width, label='Mutual Information * 10000')\n\nax.set_ylabel('Feature Importance',fontsize=20)\nax.set_title('Feature',fontsize=20)\nax.set_xticks(x)\nax2 = ax.twiny()\nax2.set_xlim(ax.get_xlim())\nax2.set_xticks(x)\nax2.set_xticklabels(x, fontsize=20)\nax.set_xticklabels(dataset.columns[1:], rotation='vertical', fontsize=20)\n\nax.legend(fontsize=20)\nplt.gcf().subplots_adjust(bottom=0.35)  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have plotted both statistical measures in one figure for convenience. For the moment, disregard the Chi-squared measure and focus on Mutual Information. According to this measure, relevancy varies drastically. Let's drop the feature less relevant than *habitat*. Now if we tune the hyper-parameters as before, we'll get:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_enc_mod = X_enc[:,[3,4,7,8,11,12,13,14,18,19,20,21]]\nsvc = SVC()\nparameters = {'C':(200, 300,400), 'gamma':(0.01,0.05,0.1)}\nclf_select_features = GridSearchCV(svc, parameters)\nclf_select_features.fit(X_enc_mod, y_enc)\nprint(clf_select_features.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our select features mark around **4 percent improvement** upon the baseline accuracy. Now lets focus on Chi-squared. Following the same foot-steps, lets disregard any feature less relevant than *habitat*:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_enc_mod = X_enc[:, [3,6,7,8,10,18,21]]\nsvc = SVC()\nparameters = {'C':(100,200,500), 'gamma':(0.1,1,100)}\nclf_select_features = GridSearchCV(svc, parameters)\nclf_select_features.fit(X_enc_mod, y_enc)\nprint(clf_select_features.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An accuracy of 0.8862263 means that our model which is inspired by Chi-squared measure, drops our best result by less than 3 percent. That said, the above 7 features contain a lot of useful information for modeling it seems. So let's see if we can revamp our inference from  Chi-squared, looking at the Mutual Information bars. \n\n<h3> Feature selection based on mixed inference </h3>\n\nBefore we go any further, note that the pitfall here would be comparing the relevancy of each feature based on the actual numbers provided by each measure; which is meaningless of course, as these techniques more-or-less measure different aspects. That said, the relative importance of features within each set side-by-side, may in fact provide valuable insight. For example, looking at the Mutual Information bars, we observe that **odor** of mushrooms are extremely important in deciding whether they are poisonous or edible (and that makes sense too!). Chi-squared measure seems to be blind to this fact. So let's add this feature to our Chi-squared trimmed feature set. Similarly one might add **spore-print-color** and **population** features:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_enc_mod = X_enc[:, [3,6,7,8,10,18,21,4,19,20]]\nsvc = SVC()\nparameters = {'C':(95,100,105), 'gamma':(0.5,0.6,0.7)}\nclf_select_features = GridSearchCV(svc, parameters)\nclf_select_features.fit(X_enc_mod, y_enc)\nprint(clf_select_features.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A 9 percent improvement upon the baseline accuracy, and 5 percent improvement upon the model solely based on Mutual Information measure, proves that feature selection based on mixed inference may in fact be efficacious.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}