{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8df442eb9a44712f28b670c63e28db8b44d84693"},"cell_type":"markdown","source":"### EDA"},{"metadata":{"trusted":true,"_uuid":"5bae1a7ab0632b202b620c3cdb73f07826080ad5"},"cell_type":"code","source":"df = pd.read_csv(\"../input/year_prediction.csv\")\n# Group release years into decades\ndf['label'] = df.label.apply(lambda year : year-(year%10))\n\ntrain = df.iloc[:463715]\ntest = df.iloc[-51630:]\n\nprint( train.shape)\ntrain_labels = train['label']\ntrain_features = train.drop(\"label\", axis=1)\ntest_labels= test['label']\ntest_features = test.drop(\"label\", axis=1)\ntrain_features.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8574427d3f3f59300a4dcd7a95765f3532dff20f"},"cell_type":"code","source":"any(train_features.isna().sum() > 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aac78b2d1df4d7522fb93eea49f60a2dcf6a0006"},"cell_type":"markdown","source":"No Null Values"},{"metadata":{"_uuid":"3330b7c2117ccb5af32cd19cec3d3ee85f7f416b"},"cell_type":"markdown","source":"All features seem to be numeric and on different scales. Might be a good idea to standardize the data before doing any modeling.\n## Scaling"},{"metadata":{"trusted":true,"_uuid":"7699c4b8780c9ff3db20f1cd135e6f879f324dab"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ntrain_features_scaled = scaler.fit_transform(train_features.values)\ntest_features_scaled = scaler.transform(test_features.values)\ntrain_features = pd.DataFrame(train_features_scaled, columns=train_features.columns, index=train_features.index)\ntest_features = pd.DataFrame(test_features_scaled, columns=test_features.columns, index=test_features.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b3bfe8e857089c2d37bdecdf5936f2418581804"},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(16,7))\nsns.countplot(train_labels, ax=ax[0])\nsns.countplot(test_labels, ax=ax[1])\nax[0].set_title(\"Train Labels Dist\")\nax[1].set_title(\"Test Labels Dist\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00eca7750eb88939e307211284600a120d2e3eec"},"cell_type":"markdown","source":"* The counts for 1940 and below is too low. Lets exclude those.\n*  We might want equal representation accross classes for better performance (stratified sampling).\n  - PS - Stratified sampling is turned off in this version. Seems to be harming our scores for some reason."},{"metadata":{"trusted":true,"_uuid":"694fdfe3ffa00b05b2675c0b4eaee900134e367b"},"cell_type":"code","source":"train = train_features\ntrain['label'] = train_labels\ntest = test_features\ntest['label'] = test_labels\nprint (train_features.shape, test_features.shape)\nprint (train_labels.shape, test_labels.shape)\n\ntrain = train[train['label'] > 1940]\ntest = test[test['label'] > 1940]\n# Borrowing Code fr//om https://www.kaggle.com/vinayshanbhag/predict-release-timeframe-from-audio-features for downsampling\nmin_samples = train.label.value_counts().min() \ndecades = train.label.unique()\ndf_sampled = pd.DataFrame(columns=train.columns)\nfor decade in decades:\n    df_sampled = df_sampled.append(train[train.label==decade].sample((min_samples)))\ndf_sampled.label = df_sampled.label.astype(int)\n\ntrain_labels =df_sampled['label']\ntrain_features = df_sampled.drop(\"label\", axis=1)\ntest_labels= test['label']\ntest_features = test.drop(\"label\", axis=1)\nprint (train_features.shape, test_features.shape)\nprint (train_labels.shape, test_labels.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a2bc0c35f35eae0dc1558ee9ecdcb2736775632"},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(16,7))\nsns.countplot(train_labels, ax=ax[0])\nsns.countplot(test_labels, ax=ax[1])\nax[0].set_title(\"Train Labels Dist\")\nax[1].set_title(\"Test Labels Dist\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"705efa31e374b11d1a60c7c158fb7b767ee51f2f"},"cell_type":"markdown","source":"## Baseline - Gradient boosting and RandomForest"},{"metadata":{"trusted":true,"_uuid":"296deab03ed90104e3ab020c0d1d58f306e7da91"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"lgbm = LGBMClassifier()\nrf = RandomForestClassifier()\nscaler = StandardScaler()\npipeline1 = Pipeline([('scaler', scaler), ('lgbm', lgbm)])\npipeline2 = Pipeline([('scaler', scaler), ('rf', rf)])\nprint( cross_val_score(pipeline1, train_features, train_labels, cv=5))\nprint( cross_val_score(pipeline2, train_features, train_labels, cv=5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc8996dbdda515c6248cd09a490f5e37acbecf7a"},"cell_type":"markdown","source":"> **** ~14% for LGBM. That's the baseline."},{"metadata":{"_uuid":"849f3cc719612ac3bc985fa4a59665bbbfaefa4f"},"cell_type":"markdown","source":"## Neural Nets  Sequential Feedforward Networks"},{"metadata":{"trusted":true,"_uuid":"e3af1839be4ceeaedd82888da97fe7d5948e4044"},"cell_type":"code","source":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nimport keras as kr\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.layers.core import Dense\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import StratifiedShuffleSplit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffc39a22a04c07922bf3ec87678dd91a794ff548"},"cell_type":"code","source":"lb = LabelBinarizer()\ntrainY = lb.fit_transform(train_labels)\ntestY = lb.transform(test_labels)\ntrain_features, trainY = shuffle(train_features, trainY)\nscaler = StandardScaler()\ntrain_standardized = scaler.fit_transform(train_features)\nsss = StratifiedShuffleSplit(n_splits=2, test_size=0.5, random_state=0)\nlabel_count = len(test_labels.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff3470fe6211d5ee0623ed3e4b62a84ebfcd96ad"},"cell_type":"code","source":"def get_network():\n    model = kr.models.Sequential()\n    model.add(Dense(20, input_shape=(train_features.shape[1],), activation=\"relu\"))\n    model.add(Dense(20, activation=\"relu\"))\n    model.add(Dense(label_count, activation=\"softmax\"))\n    opt = \"adam\"\n    model.compile(loss= \"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"], )\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"252a998a94300e3c56859d35ae185c54a3f51003","scrolled":true},"cell_type":"code","source":"model = get_network()\nscikit_net= KerasClassifier(build_fn=get_network, epochs=10, batch_size=40)\nprint( cross_val_score(scikit_net, train_features, trainY, cv=5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d5a172228a3ae0d77436650899ab00919015447"},"cell_type":"markdown","source":"Seems like with 1 input layer and 1 hidden layer each with 20 nodes we get close to the baseline. "},{"metadata":{"_uuid":"81b97a8e1f9ca1cc69de6701ca8071939bf39cbf"},"cell_type":"markdown","source":"## Active questions\n- Are those features time dependent/based on slices of time?\n- How do you decide a neural net architecture?\n    - You don't. All trial and error. \n- How do you get things like feature importance in a Neural Net?\n- Standardization seems very important to Neural nets. Why? What effects to stratified samples have?\n  - Ans: Equal representation across classes prevents the classifier from being biased towards the majority class. Might mean lesser raw accuracy but a confusion matrix that looks much better"},{"metadata":{"trusted":true,"_uuid":"b096b3692fb671778fa22fc111c7338e27251431"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint(\"LGB accuracy\")\npipeline1.fit(train_features, train_labels)\nprint(accuracy_score(pipeline1.predict(test_features), test_labels))\nprint(\"RF accuracy\")\npipeline2.fit(train_features, train_labels)\nprint(accuracy_score(pipeline2.predict(test_features), test_labels))\nmodel.fit(train_features, trainY, batch_size=40, epochs=5)\nmodel.evaluate(test_features, testY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99c02ab83f32e99c727ff481df8ba0290be4afef"},"cell_type":"code","source":"preds = model.predict(test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0163011518fcc08f6c028266a9f503e2d9fb6a8e"},"cell_type":"code","source":"pred_single = [i.argmax() for i in preds]\nlabel_single = [i.argmax() for i in testY]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"312c0295cf2167f660c64d257c8326f80678a2a1"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(label_single, pred_single)\ncm_df= pd.DataFrame(cm)\ncm_df = cm_df.apply(lambda x: x/sum(x), axis ='columns')\nsns.heatmap(cm_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3f6b830e1b88dde92b0d300242ee15096ba6065"},"cell_type":"markdown","source":"So it seems like from the 50s ot the 80s we're doing well. What we're failing at is the later stages where we ironically have more data. Maybe try a differnet sampling method to use all the data we have?"},{"metadata":{"trusted":true,"_uuid":"7313d41ec8f642c481537ea92d8bcf05d1dbd1ad"},"cell_type":"code","source":"cm_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96db41ed40efad82aa8c7d0786d1413195821034"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}