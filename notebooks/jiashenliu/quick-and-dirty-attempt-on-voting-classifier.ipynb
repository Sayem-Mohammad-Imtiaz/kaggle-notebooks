{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b5765787-d6e6-d228-b26c-688015673b5b"},"source":"# Quickly trying hard voting classifier in this case "},{"cell_type":"markdown","metadata":{"_cell_guid":"8ae4791c-3e6d-833a-bab1-a5ce887ba9f2"},"source":"## Fire up"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"47510499-1bcb-17cb-2148-e3d316c2b183"},"outputs":[],"source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\n\n## Read Data\ndf=pd.read_csv('../input/prescriber-info.csv')\ndf.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b63c980-9c5f-d76d-2211-a9dbd611c7d1"},"outputs":[],"source":"print(df.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"4fa54f38-8a6d-6025-4ca9-4bf0bace6cd3"},"source":"**We have a high dimensional data set again. How can we create an effective predictive model quickily on such a complicated data-set?**"},{"cell_type":"markdown","metadata":{"_cell_guid":"49a49bb1-2a65-fcea-aef3-cedc3c2e737c"},"source":"## Data Proprecessing"},{"cell_type":"markdown","metadata":{"_cell_guid":"7e4f9f5f-fcd1-3e67-bfae-7a46a5e63cfa"},"source":"**Even a quick and dirty classification model requires the transformation of non-numeric feature, in order to enable Python to identify the variable.**"},{"cell_type":"markdown","metadata":{"_cell_guid":"88519d2c-47bd-d2be-dbe8-6d1e30372274"},"source":"**Thanks for @ Alan (AJ) Pryor, Jr 's feedback! My method was so 'dirty' that I even included misleading variables in my model. Now I will modify my Kernel to a more completed, accurate one.**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b2a3a5b9-dee6-2a25-47bd-9509d3015ce1"},"outputs":[],"source":"opioids=pd.read_csv('../input/opioids.csv')\nname=opioids['Drug Name']\nimport re\nnew_name=name.apply(lambda x:re.sub(\"\\ |-\",\".\",str(x)))\ncolumns=df.columns\nAbandoned_variables = set(columns).intersection(set(new_name))\nKept_variable=[]\nfor each in columns:\n    if each in Abandoned_variables:\n        pass\n    else:\n        Kept_variable.append(each)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"83ba57ec-e28a-6764-45f7-cefabeb1a5c6"},"outputs":[],"source":"df=df[Kept_variable]\nprint(df.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fea5eedb-ea84-a46e-7a30-1c4c8ba56a25"},"outputs":[],"source":"train,test = train_test_split(df,test_size=0.2,random_state=42)\nprint(train.shape)\nprint(test.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3c88ff89-2af0-8e38-332e-0a9086fd35d6"},"outputs":[],"source":"Categorical_columns=['Gender','State','Credentials','Specialty']\nfor col in Categorical_columns:\n    train[col]=pd.factorize(train[col], sort=True)[0]\n    test[col] =pd.factorize(test[col],sort=True)[0]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"444b8d66-20d6-8eb9-fba4-0104a54df4c1"},"outputs":[],"source":"features=train.iloc[:,1:245]\nfeatures.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"cebc7319-c180-dece-0ee5-9f938ac55913"},"source":"## Try Different Classifiers"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a266a2d0-cee0-c5ea-7b6c-8259af0c2b7a"},"outputs":[],"source":"import sklearn\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import VotingClassifier"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"30ffca9a-4654-4575-3021-08247b3652e1"},"outputs":[],"source":"features=train.iloc[:,1:244]\ntarget = train['Opioid.Prescriber']\nName=[]\nAccuracy=[]\nmodel1=LogisticRegression(random_state=22,C=0.000000001,solver='liblinear',max_iter=200)\nmodel2=GaussianNB()\nmodel3=RandomForestClassifier(n_estimators=200,random_state=22)\nmodel4=GradientBoostingClassifier(n_estimators=200)\nmodel5=KNeighborsClassifier()\nmodel6=DecisionTreeClassifier()\nmodel7=LinearDiscriminantAnalysis()\nEnsembled_model=VotingClassifier(estimators=[('lr', model1), ('gn', model2), ('rf', model3),('gb',model4),('kn',model5),('dt',model6),('lda',model7)], voting='hard')\nfor model, label in zip([model1, model2, model3, model4,model5,model6,model7,Ensembled_model], ['Logistic Regression','Naive Bayes','Random Forest', 'Gradient Boosting','KNN','Decision Tree','LDA', 'Ensemble']):\n    scores = cross_val_score(model, features, target, cv=5, scoring='accuracy')\n    Accuracy.append(scores.mean())\n    Name.append(model.__class__.__name__)\n    print(\"Accuracy: %f of model %s\" % (scores.mean(),label))"},{"cell_type":"markdown","metadata":{"_cell_guid":"8aa35582-cfa8-d9bf-d34d-da813cdfb41c"},"source":"**For the model with the accuracy worse than 80%, we drop them and re_ensemble the model.**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8f41232c-34be-262b-9826-570711d39001"},"outputs":[],"source":"Name_2=[]\nAccuracy_2=[]\nEnsembled_model_2=VotingClassifier(estimators=[('rf', model3),('gb',model4)], voting='hard')\nfor model, label in zip([model3, model4,Ensembled_model_2], ['Random Forest', 'Gradient Boosting','Ensemble']):\n    scores = cross_val_score(model, features, target, cv=5, scoring='accuracy')\n    Accuracy_2.append(scores.mean())\n    Name_2.append(model.__class__.__name__)\n    print(\"Accuracy: %f of model %s\" % (scores.mean(),label))"},{"cell_type":"markdown","metadata":{"_cell_guid":"3d0a0e2d-ca08-cb27-8f26-717333c3db29"},"source":"**We can see that the ensembled model putting two models together performed better than the other three models. I will further use the test set to compare the performances between models.**"},{"cell_type":"markdown","metadata":{"_cell_guid":"2c5f1885-b3b9-a75f-9c0e-d3c7af903744"},"source":"## Evaluating with the test set"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4e9099c1-377e-cd36-a839-2454678084b5"},"outputs":[],"source":"from sklearn.metrics import accuracy_score\nclassifers=[model3,model4,Ensembled_model_2]\nout_sample_accuracy=[]\nName_2=[]\nfor each in classifers:\n    fit=each.fit(features,target)\n    pred=fit.predict(test.iloc[:,1:244])\n    accuracy=accuracy_score(test['Opioid.Prescriber'],pred)\n    Name_2.append(each.__class__.__name__)\n    out_sample_accuracy.append(accuracy)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ac12a574-6d84-cd88-2786-337241b32b03"},"source":"## In-sample and out-sample evaluation"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"273e361a-b9a9-ef8f-598b-07d00efe8149"},"outputs":[],"source":"in_sample_accuracy=Accuracy_2"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"18d943a7-adf9-fab6-00d9-81caf4ab0140"},"outputs":[],"source":"Index = [1,2,3]\nplt.bar(Index,in_sample_accuracy)\nplt.xticks(Index, Name_2,rotation=45)\nplt.ylabel('Accuracy')\nplt.xlabel('Model')\nplt.title('In sample accuracy of models')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c67ba9f9-fe5c-ed70-673f-c77b0ca1c6c0"},"outputs":[],"source":"plt.bar(Index,out_sample_accuracy)\nplt.xticks(Index, Name_2,rotation=45)\nplt.ylabel('Accuracy')\nplt.xlabel('Model')\nplt.title('Out sample accuracies of models')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"f52d0d60-792a-7e1d-5153-4e2b5b4f74b2"},"source":"**The hard-voting classifer does not provide a better result in terms of out sample error. Therefore, in this case, the voting classifer cannot reach the satisfied result. A soft voting method can be considered as a substitute.**"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}