{"cells":[{"metadata":{},"cell_type":"markdown","source":"# INTRODUCTION<BR>\n    \nTime-series forcasting is one of most exciting fields of application in the domain of data science. It has endless supply of problems from various topics of interest, such as - in finance, forcasting stock prices and trend behavior overtime, global issues such as forcasting unemployement in each quater, also in medical- number of babies born every minutes, number of new cases under a pandemic, to name a few. <br>\n<br>\n    This is a notebook i am trying to develop  in an effort to learn the tools and methods of time-series data analytics and forcasting. Hence, this is a work in progress kernel. <br>\n    \nI refer the following resources:\n1. An excellent book for understanding the concepts:<br> https://machinelearningmastery.com/introduction-to-time-series-forecasting-with-python/<br>\n2. A number of Kaggle notebooks, i found very helpful:<br>\n    https://www.kaggle.com/jagangupta/time-series-basics-exploring-traditional-ts<br>\n    https://www.kaggle.com/jayitabhattacharyya/facebook-s-neural-prophet<br>\n    https://www.kaggle.com/parulpandey/getting-started-with-time-series-using-pandas<br>\n    https://www.kaggle.com/thebrownviking20/everything-you-can-do-with-a-time-series<BR>\n3. A very informative medium article:<br>\n    https://towardsdatascience.com/time-series-analysis-visualization-forecasting-with-lstm-77a905180eba"},{"metadata":{},"cell_type":"markdown","source":"# Importing tools"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport lightgbm as lgb\nfrom numpy.random import normal, seed\nimport math\nfrom sklearn.metrics import mean_squared_error\n\nfrom pandas.plotting import autocorrelation_plot\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\nfrom matplotlib import pyplot\nfrom pylab import rcParams\nfrom plotly import tools\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport seaborn as sns\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.arima_model import ARMA\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima_process import ArmaProcess\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.distributions.empirical_distribution import ECDF\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nfrom scipy.stats import norm\nimport scipy.stats as scs\n\nfrom fbprophet import Prophet","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset: This includes the stock data of Nifty-50 index from NSE (National Stock Exchange) India over the last 20 years (2000 - 2019). I am going to explore the stock market data of ICICI bank."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/nifty50-stock-market-data/ICICIBANK.csv\", parse_dates=[\"Date\"],index_col=\"Date\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features:\n1. Series: Here EQ stands for equity series of stock market.\n2. Prev Close: The closing price of the stock for the day before.\n3. Open,High, Low, Last, Close: The opening price, highest price, lowest price, last price and closing price of ICICI shares on the current day.\n4. **VWAP**: Volume Weighted Average Price,the **target variable** to predict. VWAP is a trading benchmark used by traders that gives the average price the stock has traded at throughout the day, based on both volume and price.\n5. Volume: Volume of shares traded on the current day.\n6. Turnover: It is a measure of stock liquidity calculated by dividing the total number of shares traded over a period by the average number of shares outstanding for the period. \n7. Trades: total number of trades on the current day.\n8. Deliverable Volume:  is the quantity of shares which actually move from one set of people to another set of people.\n9. Deliverable(%): Deliverable volume in percentage."},{"metadata":{},"cell_type":"markdown","source":"# Exploring the data\nLet's explore the missing values, trend, seasonality, correlation and noise in the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for feature in df.columns:\n#     print(\"{} : {}\".format(feature,df[feature].isna().sum()))\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Trades, Deliverable volume and %Deliverable are the columns with missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Percentage of missing trade values = {:.2f}\".format(100*(df['Trades'].isna().sum())/df.shape[0]))\nprint(\"Percentage of missing Deliverable Volume values = {:.2f}\".format(100*(df['Deliverable Volume'].isna().sum())/df.shape[0]))\nprint(\"Percentage of missing %Deliverble values = {:.2f}\".format(100*(df['%Deliverble'].isna().sum())/df.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So more than 50% trade data is missing, while only 10% each of deliverable volume and deliverable% is missing. We can drop the rows where deliverable volume is missing. For trade data, we will visualize it to understand the best statistic for imputation."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[df['Deliverable Volume'] >0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = df[['Trades']].plot(figsize=(20, 6))\nax.set_title('Trades', fontsize=24);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that trade values show a pulse in growth from the period of 2020 - 2021. To fill the NaN values, perhaps the safest option is to forward fill. But since many consecutive rows have missing values, we can fill with mean value."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Trades'].ffill(axis = 0) \ntrade_data = df.loc[df.index<='2019']\nmean = round(trade_data['Trades'].mean(),1)\ndf[\"Trades\"].fillna(value = mean,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing trends in stock prices"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = df[['Turnover']].plot(figsize=(20, 6))\nax.set_title('Turnover', fontsize=24);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = df[['Volume']].plot(figsize=(20, 6))\nax.set_title('Volume', fontsize=24);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The volume of shares traded showed a sharp growth during the period of 20-21."},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = df[['High', 'Low']].plot(figsize=(20, 6))\nax.set_title('High v/s Low', fontsize=24);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = df[['VWAP']].plot(figsize=(20, 6))\nax.set_title('VWAP', fontsize=24);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even though turnover and volume increased during the year 20-21, the prices dropped significantly. This explains the fact that, due to the pandemic \nCovid-19, many investors took the advantage of dropping prices to buy stocks in bulk, perhaps looking forward to sell when industries regain momentum."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['VWAP'].resample('A').mean().plot(kind='bar')\nplt.title('End of year VWAP for ICICI bank')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['VWAP'].resample('AS').mean().plot(kind='bar')\nplt.title('Yearly Mean VWAP for ICICI bank')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['VWAP']['2010'].resample('M').mean().plot(kind='bar')\nplt.title('Monthly Mean 2010 VWAP for ICICI bank')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['VWAP']['2015'].resample('M').mean().plot(kind='bar')\nplt.title('Monthly Mean 2015 VWAP for ICICI bank')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['VWAP']['2020'].resample('M').mean().plot(kind='bar')\nplt.title('Monthly Mean 2020 VWAP for ICICI bank')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"VWAP has decreased on average in a decade(2010-2020)."},{"metadata":{},"cell_type":"markdown","source":"Box-Cox Transformation - to generate a uniform distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import boxcox\ndf['vwap_boxcox'],lam = boxcox(df['VWAP'])\nprint('Lambda: %f' % lam)\nrcParams['figure.figsize'] = 11, 9\npyplot.figure(1)\n# line plot\npyplot.subplot(211)\npyplot.plot(df['vwap_boxcox'])\n# histogram\npyplot.subplot(212)\npyplot.hist(df['vwap_boxcox'])\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Moving average smoothing\nSmoothing is a technique applied to time series to remove the fine-grained variation between time steps. The hope of smoothing is to remove noise and better expose the signal of the underlying causal processes.\n\nBy plotting rolling"},{"metadata":{"trusted":true},"cell_type":"code","source":"rolling_vwap = df['VWAP'].rolling(window=7)\ndf['VWAP_rolled'] = rolling_vwap.mean()\ndf['VWAP'].plot() \ndf['VWAP_rolled'].plot(color='blue')\nplt.title('Lag in VWAP Annual')\nplt.legend(fontsize=15)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Autocorrelation plot\n\nAutocorrelation - The autocorrelation function (ACF) measures how a series is correlated with itself at different lags.\n \"Correlation values, called correlation coefficients, can be calculated for each observation and different lag values. Once calculated, a plot can be created to help better understand how this relationship changes over the lag. This type of plot is called an autocorrelation plot...\" - *Introduction to Time Series Forecasting With Python*, Jason Brownlee."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_acf(df['VWAP'],title=\"VWAP Autocorrelation\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autocorrelation_plot(df['VWAP'])\nplt.title('VWAP Autocorrelation')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Partial Autocorrelation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_pacf(df['VWAP'],lags=50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stationarity-\nWe'll check with the Augmented Dickey-Fuller test for stationarity in the dataset. A stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time."},{"metadata":{"trusted":true},"cell_type":"code","source":"result = adfuller(df['VWAP'])\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1]) \nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Running the example prints the test statistic value of -2.69. If random, such autocorrelations should be near zero for any and all time-lag separations. If non-random, then one or more of the autocorrelations will be significantly non-zero.The more negative this statistic, the more likely we are to reject the null hypothesis (we have a stationary dataset). Hence the values are relatively not random, but depend on the prior recorded values.\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Decompose Time Series Data\n> \"Time series decomposition involves thinking of a series as a combination of level, trend, seasonality, and noise components. Decomposition provides a useful abstract model for thinking about time series generally and for better understanding problems during time series analysis and forecasting.\"- *Introduction to Time Series Forecasting With Python*, Jason Brownlee.\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose \nseries = df['VWAP']\nresult = seasonal_decompose(series, model='additive', period=1) \nrcParams['figure.figsize'] = 11, 9\nresult.plot()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose \nseries = df['VWAP']\nresult = seasonal_decompose(series, model='multiplicative', period=1) \nrcParams['figure.figsize'] = 11, 9\nresult.plot()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df.copy()\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering:\nAppling sliding window statistic on columns to obtain lag values, to be used in modelling the dataset. The mean and standard lag values of the features will be useful to forcast out of sample or test sample of VWAP. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.reset_index(drop=False,inplace=True)\nlag_features = [\"High\", \"Low\", \"Volume\", \"Turnover\", \"Trades\"]\nwindow1 = 3\nwindow2 = 7\nwindow3 = 30\n\ndf_rolled_3d = df2[lag_features].rolling(window=window1, min_periods=0)\ndf_rolled_7d = df2[lag_features].rolling(window=window2, min_periods=0)\ndf_rolled_30d = df2[lag_features].rolling(window=window3, min_periods=0)\n\ndf_mean_3d = df_rolled_3d.mean().shift(1).reset_index().astype(np.float32)\ndf_mean_7d = df_rolled_7d.mean().shift(1).reset_index().astype(np.float32)\ndf_mean_30d = df_rolled_30d.mean().shift(1).reset_index().astype(np.float32)\n\ndf_std_3d = df_rolled_3d.std().shift(1).reset_index().astype(np.float32)\ndf_std_7d = df_rolled_7d.std().shift(1).reset_index().astype(np.float32)\ndf_std_30d = df_rolled_30d.std().shift(1).reset_index().astype(np.float32)\n\nfor feature in lag_features:\n    df2[f\"{feature}_mean_lag{window1}\"] = df_mean_3d[feature]\n    df2[f\"{feature}_mean_lag{window2}\"] = df_mean_7d[feature]\n    df2[f\"{feature}_mean_lag{window3}\"] = df_mean_30d[feature]\n    \n    df2[f\"{feature}_std_lag{window1}\"] = df_std_3d[feature]\n    df2[f\"{feature}_std_lag{window2}\"] = df_std_7d[feature]\n    df2[f\"{feature}_std_lag{window3}\"] = df_std_30d[feature]\n\ndf2.fillna(df.mean(), inplace=True)\n\ndf2.set_index(\"Date\", drop=False, inplace=True)\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Months and Days are useful features to input in the model for predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.Date = pd.to_datetime(df2.Date, format=\"%Y-%m-%d\")\ndf2[\"month\"] = df2.Date.dt.month\ndf2[\"day\"] = df2.Date.dt.day","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing model \nNote: The additional features supplied to time series problems are called exogenous regressors."},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.dropna(axis =0,inplace=True)\n#Dropping NULL or infinite values to avoid errors.\ndf2 =df2[~df2.isin([np.nan, np.inf, -np.inf]).any(1)]\ndf2=df2.resample('MS').sum()\ndf_train = df2[df2.index < \"2019\"]\ndf_valid = df2[df2.index >= \"2019\"]\n\nexogenous_features = [\"High_mean_lag3\", \"High_std_lag3\", \"Low_mean_lag3\", \"Low_std_lag3\",\n                      \"Volume_mean_lag3\", \"Volume_std_lag3\", \"Turnover_mean_lag3\",\n                      \"Turnover_std_lag3\", \"Trades_mean_lag3\", \"Trades_std_lag3\",\n                      \"High_mean_lag7\", \"High_std_lag7\", \"Low_mean_lag7\", \"Low_std_lag7\",\n                      \"Volume_mean_lag7\", \"Volume_std_lag7\", \"Turnover_mean_lag7\",\n                      \"Turnover_std_lag7\", \"Trades_mean_lag7\", \"Trades_std_lag7\",\n                      \"High_mean_lag30\", \"High_std_lag30\", \"Low_mean_lag30\", \"Low_std_lag30\",\n                      \"Volume_mean_lag30\", \"Volume_std_lag30\", \"Turnover_mean_lag30\",\n                      \"Turnover_std_lag30\", \"Trades_mean_lag30\", \"Trades_std_lag30\",\n                      \"month\", \"day\"]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dataset with target variable and exogeneous features \ndf3 = pd.DataFrame()\ndf3['VWAP'] = df2['VWAP']\nfor i in exogenous_features:\n    df3[i] = df2[i]\nprint(pd.infer_freq(df3.index, warn=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# AutoRegressor:\nA model that uses the dependent relationship between an observation and some number of lagged observations.\n> \"We can use statistical measures to calculate the correlation between the output variable and values at previous time steps at various different lags. The stronger the correlation between the output variable and a specific lagged variable, the more weight that autoregression model can put on that variable when modeling.\"-https://machinelearningmastery.com/autoregression-models-time-series-forecasting-python/<br>\n\nAR(1) model<br>\nRt = μ + ϕRt-1 + εt<br>\n\nAs RHS has only one lagged value(Rt-1)this is called AR model of order 1 where μ is mean and ε is noise at time t\nIf ϕ = 1, it is random walk. Else if ϕ = 0, it is white noise. Else if -1 < ϕ < 1, it is stationary. If ϕ is -ve, there is men reversion. If ϕ is +ve, there is momentum.<br>\n\nAR(2) model<br>\nRt = μ + ϕ1Rt-1 + ϕ2Rt-2 + εt<br>\n\nAR(3) model<br>\nRt = μ + ϕ1Rt-1 + ϕ2Rt-2 + ϕ3Rt-3 + εt<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.ar_model import AutoReg\nmodel = AutoReg(df3.VWAP,lags=3, exog=df3[exogenous_features])\nres = model.fit()\nprint(res.summary())\nprint(\"μ={} ,ϕ={}\".format(res.params[0],res.params[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = model.fit(cov_type=\"HC0\")\nprint(res.summary())\nprint(\"μ={} ,ϕ={}\".format(res.params[0],res.params[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = res.plot_predict('2020-07-01', '2020-11-01')\nax = df3['VWAP'].loc['2020-06-01':].plot(linewidth=4, linestyle=':', label='Actual VWAP')\nplt.title('VWAP Forcast vs Actual: July 2020 - November 2020')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16,9))\nfig = res.plot_diagnostics(fig=fig, lags=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = math.sqrt(mean_squared_error(df3['VWAP'].loc['2014-01-01':'2020-11-01'],res.predict(start='2014-01-01',end='2020-11-01') ))\nprint(\"The root mean squared error is {}.\".format(rmse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AutoRegressor has a good score! Now let's try ARIMA model. <br>\nRef:  https://www.kaggle.com/rohanrao/a-modern-time-series-tutorial"},{"metadata":{},"cell_type":"markdown","source":"# ARIMA Models\nAn ARIMA model is a class of statistical models for analyzing and forecasting time series data. It explicitly caters to a suite of standard structures in time series data, and as such provides a simple yet powerful method for making skillful time series forecasts. ARIMA is an acronym that stands for AutoRegressive Integrated Moving Average."},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install pmdarima","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pmdarima.arima import auto_arima\nmodel = auto_arima(df_train.VWAP, exogenous=df_train[exogenous_features], trace=True, error_action=\"ignore\", suppress_warnings=True)\nmodel.fit(df_train.VWAP, exogenous=df_train[exogenous_features])\n\nforecast = model.predict(n_periods=len(df_valid), exogenous=df_valid[exogenous_features])\ndf_valid[\"Forecast_ARIMAX\"] = forecast\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_valid[[\"VWAP\", \"Forecast_ARIMAX\"]].plot(figsize=(14, 7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error\n\nprint(\"RMSE of Auto ARIMAX:\", np.sqrt(mean_squared_error(df_valid.VWAP, df_valid.Forecast_ARIMAX)))\nprint(\"\\nMAE of Auto ARIMAX:\", mean_absolute_error(df_valid.VWAP, df_valid.Forecast_ARIMAX))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ARIMA models give a fair rmse and mae score. Let's see if deep learning models can do a better job in fitting to our dataset."},{"metadata":{},"cell_type":"markdown","source":"# Deep Learning Models \nLet's explore some deep learning models on the dataset.\nWe'll try LTSM and RNN. RNN, a model designed for allowing information to persist in short term memory to predict subsequent values, should be best suited to our dataset, since we can see that autocorrelations don't show signs of seasonality, it tends to be manipulated by recent previous prices. We'll also apply LTSM, the special RNN model, which might reveal some interesting long term dependencies or relations in the dataset. Let's explore!\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom tensorflow.keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import RNN\nfrom keras.layers import Dropout\nfrom keras.layers import *\nfrom keras.callbacks import EarlyStopping\nfrom math import sqrt\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\ndf4=df3.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (17,25))\nax = fig.gca()\nhist=df4.hist(ax = ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = df4.values\ndataset = dataset.astype('float32')\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(dataset)\ntrain_size = int(len(dataset) * 0.80)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\nX_train = train[:,1:]\ny_train = train[:,0]\nX_test = test[:,1:]\ny_test = test[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 1\ntimesteps = 1\nunits = 100\nnb_epoch = 70","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.reshape(X_train.shape[0],timesteps,X_train.shape[1])\nX_test = X_test.reshape(X_test.shape[0],timesteps,X_test.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(LSTM(units,batch_input_shape=(batch_size, X_train.shape[1], X_train.shape[2]), stateful=True))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nhistory=model.fit(X_train, y_train,epochs=nb_epoch,batch_size=batch_size, validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=21)],verbose=0,shuffle=False)\nmodel.summary()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yhat = model.predict(X_test, batch_size=batch_size)\nrmse = sqrt(mean_squared_error(y_test, yhat))\nmae=mean_absolute_error(y_test, yhat)\nprint('rmse:{} MAE:{}'.format(rmse,mae))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Test Loss')\nplt.title('LSTM model loss')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend(loc='upper right')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nplt.plot(y_test, marker='.', label=\"actual\")\nplt.plot(yhat, label=\"prediction\")\nplt.tick_params(left=False, labelleft=True)\nplt.tight_layout()\nsns.despine(top=True)\nplt.subplots_adjust(left=0.07)\nplt.ylabel('Vwap', size=15)\nplt.xlabel('points', size=15)\nplt.legend(fontsize=15)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LSTM gives an excellent score on our dataset. Let's checkout a simple RNN model!"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(SimpleRNN(units,batch_input_shape=(batch_size, X_train.shape[1], X_train.shape[2]), stateful=True))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nhistory=model.fit(X_train, y_train,epochs=nb_epoch,batch_size=batch_size, validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=21)],verbose=0,shuffle=False)\nmodel.summary()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yhat2 = model.predict(X_test, batch_size=batch_size)\nrmse = sqrt(mean_squared_error(y_test, yhat))\nmae=mean_absolute_error(y_test, yhat2)\nprint('rmse:{} MAE:{}'.format(rmse,mae))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Test Loss')\nplt.title('RNN model loss')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend(loc='upper right')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nplt.plot(y_test, marker='.', label=\"actual\")\nplt.plot(yhat2, label=\"prediction\")\nplt.tick_params(left=False, labelleft=True)\nplt.tight_layout()\nsns.despine(top=True)\nplt.subplots_adjust(left=0.07)\nplt.ylabel('Vwap', size=15)\nplt.xlabel('points', size=15)\nplt.legend(fontsize=15)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RNN performs rather poorly, in comparison to LTSM model, Though the RMSE score turned out to be the same approximately, MAE has increased by 3-4%, as we can see the results from the graphs above. We can say that, when we move from RNN to LSTM, we are introducing more & more controlling knobs, which control the flow and mixing of Inputs as per trained Weights. And thus, bringing in more flexibility in controlling the outputs. Thus, we obtain better results in LTSM."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}