{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#課題1\nndarray = np.array([1,2,3,4,5])\nndarray += 1\nprint(ndarray)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#課題2\nw = np.array([1,2,3,4])\nx = np.array([6,7,8,9])\nb = 4\ny = w.dot(x)+b\nprint(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#課題3\narr = np.array([[1,2,3],[6,50,400],[5,10,100]])\nprint(arr)\narr2 = arr.T\nprint(arr2)\nnp.average(arr2[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#課題4\nindex = [\"Taro\",\"Jiro\",\"Saburo\",\"Hanako\",\"Yoshiko\"]\ndata = [90,100,70,80,100]\nseries = pd.Series(data,index=index)\nprint(series[series != 100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#課題5サンプル\ndata = pd.read_csv(\"../input/pandasdatacsv/pandasdata.csv\",delimiter=\",\")\nprint(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#課題5\ndata = pd.read_csv(\"../input/bostonhoustingmlnd/housing.csv\",delimiter=\",\")\nprint(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#課題6\ndata = pd.read_csv(\"../input/pandasdata2csv/pandasdata2.csv\",delimiter=\",\")\nprint(data)\n\ndata = data.loc[10:13,[\"age\",\"hobby\"]]\nprint(data)\n\ndata.to_csv(\"pandasdata3.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#課題7\ndf = pd.read_csv(\"../input/titanic/train.csv\",delimiter=\",\")\n#print(df.loc[df[\"Embarked\"].isnull()])\n#df = df.dropna(subset=[\"Embarked\"])\n#print(df)\n\ndf = df.drop(\"Name\",axis=1)\ndf = df.drop(\"Ticket\",axis=1)\ndf = df.drop(\"Cabin\",axis=1)\nprint(df)\n\ndf.to_csv(\"train2.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#課題8\na = pd.read_csv(\"./train2.csv\",delimiter=\",\")\n\nby1 = (a[\"Sex\"]==\"male\").astype(int)\nby2 = (a[\"Sex\"]==\"female\").astype(int)\na[\"male\"] = by1\na[\"female\"] = by2\na = a.drop(\"Sex\",axis=1)\n\nemb1 = (a[\"Embarked\"]==\"C\").astype(int)\nemb2 = (a[\"Embarked\"]==\"Q\").astype(int)\nemb3 = (a[\"Embarked\"]==\"S\").astype(int)\na[\"embarked_c\"] = emb1\na[\"embarked_q\"] = emb2\na[\"embarked_s\"] = emb3\na = a.drop(\"Embarked\",axis=1)\n\nprint(a)\na.to_csv(\"train3.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CSV読み込み\ntrain_data = pd.read_csv(\"../input/titanic/train.csv\", delimiter=\",\")\n#先頭5行を表示\ntrain_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib\nfrom statsmodels.graphics.mosaicplot import mosaic\n#ラベル付きトレーニングデータ読み込み\ntrain_data = pd.read_csv(\"../input/titanic/train.csv\",delimiter=\",\")\n#Survived(生存情報)、Sex(性別)のモザイク図\n#print(numpy.c_[train_data['Sex'],train_data['Survived']])\nengineer_data = np.c_[train_data['Sex'],train_data['Survived']]\n#DataFrame作成\nmyDataframe = pd.DataFrame(engineer_data, columns=['Sex','Survived'])\n#モザイク図作成、表示\nmosaic(data=myDataframe, index=['Sex','Survived'],title='Mosaic Plot')\nmatplotlib.pyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ラベル付きトレーニングデータ読み込み\ntrain_data = pd.read_csv(\"../input/titanic/train.csv\",delimiter=\",\")\n#Survived(生存情報)、Pclass(チケットランク)のモザイク図\n#print(np.c_[train_data['Pclass'],train_data['Survived']])\nengineer_data = np.c_[train_data['Pclass'],train_data['Survived']]\n#DataFrame作成\nmyDataframe = pd.DataFrame(engineer_data, columns=['Pclass','Survived'])\n#モザイク図作成、表示\nmosaic(data=myDataframe, index=['Pclass','Survived'], title='Mosaic Plot')\nmatplotlib.pyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/titanic/train.csv\",delimiter=\",\")\n# Survived(生存情報)、Embarked(乗船港)のモザイク図\n# print(numpy.c_[train_data['Embarked'], train_data['Survived']])\nengineer_data = np.c_[train_data['Embarked'], train_data['Survived']]\n# DataFrame作成\nmyDataframe = pd.DataFrame(engineer_data, columns=['Embarked', 'Survived'])\n# モザイク図作成、表示\nmosaic(data=myDataframe, index=['Embarked', 'Survived'], title='Mosaic Plot')\nmatplotlib.pyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 生存した乗客の運賃情報抽出\nsurvived_train_data1 = train_data[train_data.Survived == 1]['Fare']\n# 死亡した乗客の運賃情報抽出\nsurvived_train_data2 = train_data[train_data.Survived == 0]['Fare']\n# リスト化 & 欠損データ（NaN）削除\ndata1 = survived_train_data1.dropna(how='all').values.tolist()\ndata2 = survived_train_data2.dropna(how='all').values.tolist()\n# 0を除外\ndata1 = list(filter(lambda x: x != 0, data1))\ndata2 = list(filter(lambda x: x != 0, data2))\n# 平方根化\n#data1 = numpy.sqrt(data1)\n#data2 = numpy.sqrt(data2)\n# 対数化\ndata1 = np.log(data1)\ndata2 = np.log(data2)\n# 箱ひげ図用にデータを二次元化\nbeard = (data1, data2)\n# 新規ウィンドウ作成\nfig = matplotlib.pyplot.figure()\n# 新規グラフ描画領域作成\nax = fig.subplots()\n# 箱ひげ図作成\nbp = ax.boxplot(beard)\n# ラベル付与\nax.set_xticklabels(['Survived', 'Dead'])\n# グラフの詳細設定\n# タイトル\nmatplotlib.pyplot.title('Box plot')\n# グリッド\nmatplotlib.pyplot.grid()\n# X軸名\nmatplotlib.pyplot.xlabel('Survived')\n# Y軸名\nmatplotlib.pyplot.ylabel('Fare')\n# Y軸の幅\n# matplotlib.pyplot.ylim([0,25]) # 平方根用\nmatplotlib.pyplot.ylim([0,8]) # 対数用\n# グラフ描画\nmatplotlib.pyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 生存した乗客の年齢情報抽出\nsurvived_train_data1 = train_data[train_data.Survived == 1]['SibSp']\n# 死亡した乗客の年齢情報抽出\nsurvived_train_data2 = train_data[train_data.Survived == 0]['SibSp']\n# リスト化 & 欠損データ（NaN）削除\ndata1 = survived_train_data1.dropna(how='all').values.tolist()\ndata2 = survived_train_data2.dropna(how='all').values.tolist()\n# 0を除外\ndata1 = list(filter(lambda x: x != 0, data1))\ndata2 = list(filter(lambda x: x != 0, data2))\n# 箱ひげ図用にデータを二次元化\nhige = (data1, data2)\n# 新規ウィンドウ作成\nfig = matplotlib.pyplot.figure()\n# 新規グラフ描画領域作成\nax = fig.subplots()\n# 箱ひげ図作成\nbp = ax.boxplot(hige)\n# ラベル付与\nax.set_xticklabels(['Survived', 'Dead'])\n# グラフの詳細設定\n# タイトル\nmatplotlib.pyplot.title('Box plot')\n# グリッド\nmatplotlib.pyplot.grid()\n# X軸名\nmatplotlib.pyplot.xlabel('Survived')\n# Y軸名\nmatplotlib.pyplot.ylabel('SibSp')\n# Y軸の幅\n# matplotlib.pyplot.ylim([0,25]) # 平方根用\nmatplotlib.pyplot.ylim([0,10]) # 対数用\n# グラフ描画\nmatplotlib.pyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# データの説明から、以下を理解する\n# 特徴量は12種類\n# PassengerId：旅客ID\n# Survived：生存情報(0=No, 1=Yes)\n# Pclass：チケットクラス(1=1st, 2=2nd, 3=3rd)\n# Name：名前\n# Sex：性別\n# Age：年齢\n# SibSp：同乗した兄弟・配偶者の数\n# Parch：同乗した親子の数\n# Ticket：チケット番号\n# Fare：運賃\n# Cabin：客室\n# Embarked：乗船港(C = Cherbourg, Q = Queenstown, S = Southampton)\n\n\nimport seaborn\n# ラベル付きトレーニングデータ読み込み\ntrain_data = pd.read_csv(\"../input/titanic/train.csv\", delimiter=\",\")\n# データ総数(891)\nprint('【データ総数】\\n', train_data.shape[0])\n# 欠損データ(NaN)有無(Age, Cabin, Embarkedに欠損データあり)\nprint('【欠損データ有無】\\n', train_data.isnull().any())\n# 欠損データ数\nprint('【欠損データ数】\\n', train_data.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('【年齢:平均値】', train_data['Age'].dropna().mean())\nprint('【年齢:中央値】', train_data['Age'].dropna().median())\nprint('【年齢:標準偏差】', train_data['Age'].dropna().std())\nprint('【年齢：範囲】', train_data['Age'].dropna().min(), '~', train_data['Age'].dropna().max())\ntrain_data['Age'].plot(kind='hist', bins=50, subplots=True);\nmatplotlib.pyplot.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 欠損データ対応：Embarked\n# → 'Embarked'列にNaNを持つデータ行を削除\ntrain_data = train_data.dropna(subset=['Embarked'])\n\n# 欠損データ対応：Age\n# → 'Age'列にNaNを持つデータを年齢の中央値に置き換え\ntrain_data_cabin = train_data['Cabin']\ntrain_data = train_data.drop(['Cabin'], axis=1)\ntrain_data = train_data.fillna(train_data.median()['Age'])\ntrain_data['Cabin'] = train_data_cabin\n                              \n# 欠損データ(NaN)有無(Age, Cabin, Embarkedに欠損データあり)\nprint('【欠損データ有無】\\n', train_data.isnull().any())\n\nprint('【データ総数】\\n', train_data.shape[0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.drop(['Name', 'Ticket', 'Cabin'], axis=1)\ntrain_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 性別(Sex)を男性(male)と女性(female)のバイナリ値化\ntrain_data['male'] = (train_data['Sex'] == 'male').astype(int)\ntrain_data['female'] = (train_data['Sex'] == 'female').astype(int)\n# 乗船港(Embarked)を3つの港ごとにバイナリ値化\ntrain_data['embarked_c'] = (train_data['Embarked'] == 'C').astype(int)\ntrain_data['embarked_q'] = (train_data['Embarked'] == 'Q').astype(int)\ntrain_data['embarked_s'] = (train_data['Embarked'] == 'S').astype(int)\n# バイナリ化の元データ(性別(Sex), 乗船港(Embarked))を除去する\ntrain_data = train_data.drop(['Sex', 'Embarked'], axis=1)\n# 加工した学習データをCSVファイルとして出力する\ntrain_data.to_csv('train2.csv')\ntrain_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model, metrics, preprocessing, model_selection\n#k分割交差検証：分割データが必ずテスト用で使われる\nfrom sklearn.model_selection import KFold\n#層状K分割：分布にバラつきがある場合、バラつきを維持して分割\n#from sklearn.model_selection import StratifiedKFold\n#ランダム置換相互検証：ランダム分割\n#from sklearn.model_selection import ShuffleSplit\n#機械学習用トレーニングデータ読込\nsource_train_data = pd.read_csv(\"train2.csv\", delimiter=\",\")\n#numpy.where(条件)は条件に合致した行のみ取得する\n#三項演算子でnumpy.where(条件、真の場合、偽の場合)とすると、条件に応じて取得する値を変更\n#※DataFrame.shapeは(行数、列数)なので、DataFrame.shape[0]は行数\nsource_train_data_x = source_train_data.loc[:,'Pclass':'embarked_s'].values\nsource_train_data_y = source_train_data.loc[:,'Survived'].values\n#データの整形 正規化用のオブジェクト生成\nsc=preprocessing.StandardScaler()\n#正規化の範囲決めるために一旦、値をセット\nsc.fit(source_train_data_x)\n#正規化実施\nsource_train_data_x_std=sc.transform(source_train_data_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#損失関数を全てリスト化\narray_loss=[\"hinge\",\"log\",\"modified_huber\",\"squared_hinge\",\n           \"perceptron\",\"squared_loss\",\"squared_loss\",\"huber\",\"epsilon_insensitive\",\n           \"squared_epsilon_insensitive\"]\n#試行回数をリスト化(5~500)\narray_iter=[5,10,50,100,150,200,250,300,400,500]\n#スコア保持用の行列作成\nscore_k = 10\narray_scores = np.empty((len(array_loss),len(array_iter),score_k))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt #グラフ描画用\nfrom sklearn import linear_model, metrics, preprocessing, model_selection\n#k分岐交差検証：分割データが必ずテスト用で使われる\nfrom sklearn.model_selection import KFold\n#層状K分割：分布にバラつきがある場合、バラつきを維持して分割\n#from sklearn.model_selection import StartifiedKFold\n#ランダム置換相互検証：ランダム分割\n#from sklearn.model_selection import ShuffleSplit\n#機械学習用トレーニングデータ読込\nsource_train_data = pd.read_csv(\"train2.csv\",delimiter=\",\")\n#numpy.where(条件)は条件に合致した行のみ取得する\n#三項演算子でnumpy.where(条件,真の場合,偽の場合)とすると、条件に応じて取得する値を変更\n#※DataFrame.shapeは(行数,列数)なので、DataFrame.shape[0]は行数\nsource_train_data_x = source_train_data.loc[:,'Pclass':'embarked_s'].values\nsource_train_data_y = source_train_data.loc[:,'Survived'].values\n#データの整形正規化用のオブジェクト生成\nsc=preprocessing.StandardScaler()\n#正規化の範囲決めるために一旦、値をセット\nsc.fit(source_train_data_x)\n#正規化実施\nsource_train_data_x_std=sc.transform(source_train_data_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#損失関数を全てリスト化\narray_loss=[\"hinge\",\"log\",\"modified_huber\",\"squared_hinge\",\n           \"perceptron\",\"squared_loss\",\"squared_loss\",\"huber\",\"epsilon_insensitive\",\n           \"squared_epsilon_insensitive\"]\n#試行回数をリスト化(5~500)\narray_iter=[5,10,50,100,150,200,250,300,400,500]\n#スコア保持用の行列作成\nscore_k=10\narray_scores = np.empty((len(array_loss),len(array_iter),score_k))\n#繰り返しに用いるiterableオブジェクトを使用。iにインデックス番号、lossに値(損失関数)を格納してループを行う\nfor i, loss in enumerate(array_loss):\n    #繰り返しに用いるiterableオブジェクトを使用。jにインデックス番号、iter_cntに値(試行回数)を格納してループを行う\n    for j, iter_cnt in enumerate(array_iter):\n        #確率的勾配下降法を使ったロジスティック回帰を行う\n        clf_result=linear_model.SGDClassifier(loss=\"hinge\", max_iter=200, tol=None)\n        #k分割交差検証用\n        kf = KFold(n_splits=score_k, shuffle=False, random_state=None)\n        #k分割交差検証を行う。検証用データをk(cv)分割して、予測値と検証値で検証する。\n        #clf_result:予測値\n        #source_train_data_x_std:正規化を行った特徴量\n        #source_train_data_y_std:正規化を行った正解ラベル\n        #cv:検証データの分割数(k)\n        scores=model_selection.cross_val_score(clf_result,\n            source_train_data_x_std, source_train_data_y, cv=kf)\n        array_scores[i][j] = scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#結果をグラフ化する\nfig, ax = plt.subplots(nrows=1, ncols=len(array_loss), figsize=(20,8))\nfor i, loss in enumerate(array_loss):\n    array_means = np.empty(len(array_iter))\n    for j, iter_cnt in enumerate(array_iter):\n        array_means[j] = array_scores[i][j].mean()\n    ax[i].plot(array_iter, array_means, marker='o')\n    ax[i].set_title(loss)\n    ax[i].set_xlabel(\"max_iter\")\n    ax[i].set_ylabel(\"mean\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#モデルを検証\nscores = model_selection.cross_val_score(clf_result,\nsource_train_data_x_std, source_train_data_y, cv=kf)\nprint(scores)\nprint(\"平均正解率=\",scores.mean())\nprint(\"正解率の標準偏差=\",scores.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#トレーニングデータとテストデータに分けて実行してみる\nX_train, X_test, train_label, test_label=model_selection.train_test_split(source_train_data_x_std,\nsource_train_data_y, test_size=0.3, random_state=0)\nclf_result.fit(X_train,train_label)\n\n#正答率を求める\npre=clf_result.predict(X_test)\nac_score=metrics.accuracy_score(test_label, pre)\nprint(\"正答率=\", ac_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ラベルなしトレーニングデータ読み込み\ntrain_data = pd.read_csv(\"../input/titanic/test.csv\",delimiter=\",\")\n#データ総数\nprint('【データ総数】\\n', train_data.shape[0])\n#欠損データ(NaN)有無(Age,Fare,Cabinに欠損データあり)\nprint('【欠損データ有無】\\n', train_data.isnull().any())\n#欠損データ数\nprint('【欠損データ数】\\n' train_data.isnull().sum())\n#モデル作成時、学習データ(train.csv)にはAge, Cabin, Embarkedの欠損データがあり、以下の対応を行った\n#Age:177/891件と比較的多いので、中央値を使う\n#Embarked:2/891件と少ないので、NaNのあるデータのみ取り除く\n#Name:データ加工が必要なので項目自体を除外\n#Cabin:データ加工が必要なので項目自体を除外\n#Ticket:データ加工が必要なので項目自体を除外\n\n#同様の対応をとるとして、Fareのみ対応を決める必要がある\n#Fareは1/418件だけ欠損しているため、データのばらつきを確認して代替値を設定する\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}