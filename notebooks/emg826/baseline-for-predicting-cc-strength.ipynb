{"cells":[{"metadata":{"_uuid":"3e63bc260bed876ae38754b2c083734ba4419c75"},"cell_type":"markdown","source":"This notebook trys to predict concrete compressive strength. Concrete compressive strength is measured in mega Pascals (MPa), which is a measurement of pressure or stress. Thus, compressive strength is one measurement of how much pressure concrete can withstand. I want to see how low I can get the mean squared error to go using various models.\n\nAccording to Google, 1 mega pascal is equivalent to approximately 145 pound-force per square inch (psi). For reference for these values, 1 atmosphere (atm, which is something like: the pressure of the Earth's atmosphere at sea-level) is equivalent to approximately 14 psi or 0.10 MPa. Also, car tires are often inflated to 37-ish psi or 0.26 MPa. So, 1 MPa (or 145 psi if it's easier to think in terms of psi) is a lot of pressure. This would seem to indicate that the mean squared error of a predictive model has to be REALLY low in order for it to be any good.\n\nAll units are in kilograms per cubic meter (kg / m^3) except for age, which is in days, and of course, concrete compressive strength, which is in mega Pascals (MPa).\n\nOriginal paper: I-Cheng Yeh, \"Modeling of strength of high performance concrete using artificial neural networks,\" Cement and Concrete Research, Vol. 28, No. 12, pp. 1797-1808 (1998). \n\nKaggle data: https://www.kaggle.com/maajdl/yeh-concret-data/data"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7176fb3bb9e8446eacf205c1c921e5c8a3e18a44"},"cell_type":"code","source":"import pandas\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed257a83041aeb77d09334f61c900f713f1325a3"},"cell_type":"code","source":"df = pandas.read_csv('../input/Concrete_Data_Yeh.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30393fb5e2a110565718df83759989190561e542"},"cell_type":"code","source":"# See what variables are tracked\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d436416b3e04e98acdef91d7f6874f85d8d7f9f4"},"cell_type":"code","source":"# Just see what the first 3 rows look like\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d8aca90d96c98a8bbc7556a2fdfe57285e9de38"},"cell_type":"code","source":"print('Number of empty entries by column')\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d5bd456bcdd9b4cc9a8f09ce7c4672102a2d44d6"},"cell_type":"code","source":"# Split into inputs (x) and targets (y)\nx = df.drop('csMPa', axis=1)\n\ny = pandas.DataFrame(df['csMPa'])\ny.columns = ['concrete_compressive_str_MPa']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86546d03a7deca9962e820ad2bab036f749476dd"},"cell_type":"code","source":"# The number of samples with which to work\nx.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a637f2d75ff6ca57135d64af042d6bb14bd7fec4"},"cell_type":"code","source":"# 80/20 split between testing and training\nproportion_of_training = 0.8\n\n# integer intervals [0, t_t_cutoff] and [t_t_cutoff+1, 1030]\ntrain_test_cutoff = int(x.shape[0] * proportion_of_training) \n\n# train (and validation)\nx_train = x.iloc[0:train_test_cutoff]\ny_train = y.iloc[0:train_test_cutoff]\n\n# test\nx_test = x.iloc[train_test_cutoff+1:]\ny_test = y.iloc[train_test_cutoff+1:]\n\n# Now split x_train further into actual training and validation data\n# fit on training; tune on validation\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.3)\n\nx_train = x_train.reset_index(drop=True)\ny_train = y_train.reset_index(drop=True)\n\nx_val = x_val.reset_index(drop=True)\ny_val = y_val.reset_index(drop=True)\n\nx_test = x_test.reset_index(drop=True)\ny_test = y_test.reset_index(drop=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"313fa14e0eef36d6baed6205337c17f6a69bfdfc"},"cell_type":"markdown","source":"Typically, linear regression is a decent place to start for regression problems, and logistic regression is a decent place to start for classification problems. Prediciting concrete compressive strength is a regression problem, so, the linear regression results will serve as the baseline by which all subsequent models are judged. "},{"metadata":{"trusted":true,"_uuid":"245ff086f66b4294092acc42b1dc584a4358b8d9"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nlr = LinearRegression()\n\nlr.fit(x_train, y_train)\n\nlr_predictions = lr.predict(x_val)\nmse = mean_squared_error(y_val, lr_predictions)\nprint('{} had an MSE of {}'.format('linear regression', mse))\nprint('\\t this means the average guess is off by {} mega Pascals'.format(math.sqrt(mse)))\nprint('{} had an R^2 of {}'.format('linear regression', r2_score(y_val, lr_predictions)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f60d2d0f7a9ff05cf1c64179644d2581ab1b435"},"cell_type":"code","source":"for var_name, coeff in zip(x_train.columns.values, lr.coef_[0]):\n    print(var_name, '\\t\\t', coeff)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7527874cb91169f58357d90a43a5d46e03e780da"},"cell_type":"markdown","source":"We have our baseline: a mean squared error of 106.89 or average guesses that are off by 10.34 MPa. \n\nThe signs of those coefficients (negative or positive) shed a bit of light on the relationship (at least in the linear regression) between the givne variable and concrete compressive strength. Also, the coefficients can be interpreted as, for age for instance, 1 additional year of age results in an increase of 0.105 MPa in the concrete compressive strength. What cannot be determined with these coefficients is the relative importance of each variable. Therefore, let's take a look at the coefficients when they are all brought down to the same scale: between 0 and 1, inclusive."},{"metadata":{"trusted":true,"_uuid":"9c87d268061b62dd0a0e0972afc488c497b48266"},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\n\n# try linear regression with each column standard scaled; THEN should be able \n# to see what factors are important (at least relative to each other) in the lin reg\nmms.fit(x_train)\nlr.fit(mms.transform(x_train), y_train)\n\nlr_predictions = lr.predict(mms.transform(x_val))\nmse = mean_squared_error(y_val, lr_predictions)\nprint('{} had an MSE of {}'.format('min-max scaled linear regression',\n                                   mse))\nprint('\\t this means the average guess is off by {} mega Pascals'.format(math.sqrt(mse)))\n\nprint('{} had an R^2 of {}'.format('min-max scaled linear regression',\n                                   r2_score(y_val, lr_predictions)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d561b508c34675f91a2fc63322a70343d1e09aef"},"cell_type":"code","source":"for var_name, coeff in zip(x_train.columns.values, lr.coef_[0]):\n    print(var_name, '\\t\\t', coeff)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"788e31cc10b1b847b41c5017b41ce7ab4fe954df"},"cell_type":"markdown","source":"From the magnitudes of these coefficients (so ignoring the positve or negative sign), the importance of the variables in linear regression is: cement > age > slag > water > superpllesasticizer > flyash > fineaggregate > coarseaggregate."},{"metadata":{"trusted":true,"_uuid":"82c624139a3c44384a4eb31d7a327f5237aad399"},"cell_type":"code","source":"# Let's see the distributions of each of those variables\n\nimport matplotlib.pyplot as plt\nimport matplotlib\n\nmatplotlib.rcParams['font.size'] = 22\n\n\nnum_cols = round(math.sqrt(x_train.shape[1]))\nnum_rows = round(math.sqrt(x_train.shape[1])) + 2\nsorted_cols = sorted(x_train.columns)\n\n\nfig = plt.figure(1, figsize=(26, 24))\nnum_plotted_subplots = 0\nfor col in sorted_cols:\n    num_plotted_subplots += 1\n    ax = fig.add_subplot(num_rows, num_cols, num_plotted_subplots)\n    \n    ax.hist(x_train[col].values, color='skyblue', bins=36)\n    \n    ax.grid(color='lightgray', linestyle='--', axis='y')\n    ax.set_axisbelow(True)\n    ax.set_facecolor(color='gray')\n    ax.set_xlabel(col)\nplt.subplots_adjust(left=0.2, bottom=0.2, right=0.8, top=0.8,\n        wspace=0.25, hspace=0.35)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0b3ea5296f6e7517c3c0c7fc54c393c76e152cc"},"cell_type":"code","source":"# Let's see what all 2-variable combinations' scatter plots look like (if there's anything interesting)\nfig = plt.figure(1, figsize=(30, 55))\nmatplotlib.rcParams['font.size'] = 22\n\n\nnum_cols = 3\nnum_rows = 10\n\nnum_plotted_subplots = 0\n                     \n# reverse because plots with `age' x-axis all look very similar\nrevrese_sorted_cols = [col for col in reversed(sorted_cols)]\n\nfor col_x_idx, col_x in enumerate(revrese_sorted_cols):\n    # this way, plot all combinations, NOT all permutations\n    for col_y in revrese_sorted_cols[col_x_idx:]:\n        if col_x == col_y:\n            continue\n            \n        num_plotted_subplots += 1\n        ax = fig.add_subplot(num_rows, num_cols, num_plotted_subplots)\n\n        ax.scatter(x_train[col_x].values, x_train[col_y].values, color='orange', s=30)\n\n        ax.grid(color='lightgray', linestyle='--', axis='both')\n        ax.set_axisbelow(True)\n        ax.set_facecolor(color='gray')\n        ax.set_xlabel(col_x)\n        ax.set_ylabel(col_y)\nplt.subplots_adjust(left=0.2, bottom=0.2, right=0.8, top=0.8,\n        wspace=0.25, hspace=0.35)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f046f640b0a503a78398b3466f6ed1c0f1aa889"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor(n_estimators=1000, max_depth=None)\n\nrfr.fit(x_train, y_train.values.ravel()) # used ravel() to get rid of a warning message\n\nmse = mean_squared_error(y_val, rfr.predict(x_val))\nprint('{} had an MSE of {}'.format('random forest regressor', mse))\nprint('\\t this means the average guess is off by {} mega Pascals'.format(math.sqrt(mse)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e9d97cf31fb88c08a3c513356db99534194b2f3","scrolled":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor, export_graphviz\ndt = DecisionTreeRegressor(max_depth=3)\n\ndt.fit(x_train, y_train)\n\nmse = mean_squared_error(y_val, dt.predict(x_val))\nprint('{} had an mse of {}'.format('decision tree regressor', mse))\nprint('\\t this means the average guess is off by {} mega Pascals'.format(math.sqrt(mse)))\n\n# to display the decision tree, export to a .dot file and then convert .dot file to .png\n# export_graphviz(dt, out_file='images/concrete_dt.dot', feature_names = x_train.columns.values,\n#                 filled=True, impurity=False, proportion=True, rounded=True,\n#                 leaves_parallel=False,)\n\n# import pydot\n# (graph,) = pydot.graph_from_dot_file('images/concrete_dt.dot')\n# graph.write_png('images/concrete_dt.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ea6d0fca37a0aa28627425e57424830d149ef89"},"cell_type":"markdown","source":"..."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"79ad42ed65fa632129cbe6bd52831d5a62b95717"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler # mlp's like standard scaled data?\nfrom sklearn.neural_network import MLPRegressor\n\nmlpr = MLPRegressor(hidden_layer_sizes=[1820], max_iter=4800, tol=1e-8, alpha=0.07)\n\nsc = StandardScaler()\nmlpr.fit( sc.fit_transform(x_train), y_train.values.ravel())\n\nmse = mean_squared_error(y_val, mlpr.predict(sc.transform(x_val)))\n\n# only printing training performance for this one because it's been the best performing so far\nprint('{} had a training mse of {}'.format('mlp regressor',\n                                           mean_squared_error(y_train, mlpr.predict(sc.transform(x_train)))))\nprint('{} had an mse of {}'.format('multilayer perceptron regressor', mse))\nprint('\\t this means the average guess is off by {} mega Pascals'.format(math.sqrt(mse)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d24aaa980b7b1d54bb6a76567a1c0d97d70abbc"},"cell_type":"markdown","source":"This MSE (28.45) a big improvement over the linear regression model baseline (122.96 on this current run); it's about 76% smaller. Is there a model with an even lower MSE? Conceivably, yes, so let's keep trying. I've been interested in trying this: cluster the rows of data and add a column to data set that has cluster label. I've wondered if this would, perhaps, improve performance and decrease mean squared error."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"492a0154daf1fa94638bdb789f0ace2891d18f56"},"cell_type":"code","source":"# I wonder what happens if I add a cluster column.\n# http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py\n# https://github.com/scikit-learn/scikit-learn/issues/10761\n\nfrom sklearn.cluster import KMeans, MiniBatchKMeans, AgglomerativeClustering\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nimport matplotlib.cm as cm\n\nfor num_clusters in range(2, 11): \n    #clustering_objects = [KMeans(num_clusters), MiniBatchKMeans(num_clusters), AgglomerativeClustering(num_clusters)]\n    #clustering_names = ['KMeans', 'MiniBatchKMeans', 'Agglomerative']\n    \n    clustering_objects = [MiniBatchKMeans(num_clusters)]\n    clustering_names = ['MiniBatchKMeans']\n    \n    for name, model in zip(clustering_names, clustering_objects):\n        sc = StandardScaler() # standard scale so that have meaningful means\n    \n        predicted_clusters = model.fit_predict(sc.fit_transform(x_train))\n\n        # silhouette score for entire training data set\n        # 1 means greatly separable clusters, 0 means a lot of overlap, -1 means a lot misclustering\n        avg_silhouette_score = silhouette_score(sc.fit_transform(x_train), predicted_clusters)\n        print('{} with {} clusters had an average silhouette score of {}'.format(name,\n                                                                                 num_clusters,\n                                                                                 avg_silhouette_score))\n        sample_silhouette_values = silhouette_samples(x_train, predicted_clusters)\n\n        \n        fig, ax1 = plt.subplots(1, 1)\n        fig.set_size_inches(9, 3.5)\n\n        # The 1st subplot is the silhouette plot\n        # The silhouette coefficient can range from -1, 1 but in this example all\n        # lie within [-0.1, 1]\n        ax1.set_xlim([-0.1, 1])\n        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n        # plots of individual clusters, to demarcate them clearly.\n        ax1.set_ylim([0, len(x_train) + (num_clusters + 1) * 10])\n        y_lower = 10\n        for i in range(num_clusters):\n\n\n            # Aggregate the silhouette scores for samples belonging to\n            # cluster i, and sort them\n            ith_cluster_silhouette_values = \\\n                sample_silhouette_values[predicted_clusters == i]\n\n            ith_cluster_silhouette_values.sort()\n\n            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n            y_upper = y_lower + size_cluster_i\n\n            color = cm.nipy_spectral(float(i) / num_clusters)\n            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                              0, ith_cluster_silhouette_values,\n                              facecolor=color, edgecolor=color, alpha=0.7)\n\n            # Label the silhouette plots with their cluster numbers at the middle\n            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n            # Compute the new y_lower for next plot\n            y_lower = y_upper + 10  # 10 for the 0 samples\n\n        ax1.set_title(\"The silhouette plot for {} with {} clusters\".format(name, num_clusters))\n        ax1.set_xlabel(\"silhouette coefficient values\")\n        ax1.set_ylabel(\"Cluster label\")\n\n        # The vertical line for average silhouette score of all the values\n        ax1.axvline(x=avg_silhouette_score, color=\"red\", linestyle=\"--\")\n\n        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n\n        plt.show()\n\n    print()\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6afac4e5dda130fb21a27bb940438839c5bd7df"},"cell_type":"markdown","source":"With silhouette plots there are a few things to know and keep in mind. First is the silhouette coefficient on the x-axis. A silhouette coefficient close to 1 indicates that the clusters are very separable, close to 0 indicates a lot of overlap, and close to -1 indicates misclustering.\n\nTo pick the appropriate number of clusters, you want to look at a few things. First, every cluster's knife edge looking graph should cross that red-dotted line. That red-dotted line is the average silhouette score of all clusters. Second, you want to look at the thickness of the clusters. Do you want one fat cluster and a few skinny clusters? Or would you prefer all clusters that are about the same size? I'm not sure that there's an optimal answer there, so pick what you'd like. \n\n(old values in next paragraph) From these silhouette plots, it seems that 8 clusters is alright. It has the 2nd higest average silhouette score (0.298) after 6 clusters (0.310), all its clusters seem to cross the red-dotted line (unlike the 6 cluster plot), and the clusters are fairly comparable in thickness. \n\nHeavily relied upon: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"da228d5be0cf4836041911610fd5ae262d79d18a"},"cell_type":"code","source":"num_clusters = 8 # based on silhouette plots and average scores above\n\nsc = StandardScaler()\nmini_batch_kmeans = MiniBatchKMeans(num_clusters)\nmini_batch_kmeans.fit(sc.fit_transform(x_train))\n\n# just do random forest regressor since there are few params to tune\nrfr = RandomForestRegressor(n_estimators=10000)\n\n# get cluster labels\ntrain_clusters = mini_batch_kmeans.predict(sc.transform(x_train)).ravel()\nval_clusters = mini_batch_kmeans.predict(sc.transform(x_val)).ravel()\n\n# so as to no mess up x_train, make a temporary set and work with that (O.K. since small size of data set)\ntemp_x = x_train.copy()\ntemp_x['clusters']  = train_clusters\n\nrfr.fit(temp_x, y_train.values.ravel())\n\ntemp_x = x_val.copy()\ntemp_x['clusters'] = val_clusters\n\nmse = mean_squared_error(y_val, rfr.predict(temp_x))\nprint('With clustering labels, RFR had an MSE of {}'.format(mse))\nprint('\\t this means the average guess is off by {} mega Pascals'.format(math.sqrt(mse)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ea795d09edc682729d170e493293e7199c1fef2"},"cell_type":"markdown","source":"Well that was kind of disappointing; average guesses only improved by about 2 kilo Pascals. Still worthwhile since I at least kind of have my answer: clustering and then adding the predicted cluster label column does not really improve the model (reduce mean squared error).\n\nI am curious if the models are, maybe, getting the compressive stregnth at low ages (only a few days in) wrong but for higher ages (many more days in) the models are getting compressive stregth correct. Let's plot the residuals with age on the x-axis."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6d30734f8ce184785e28b353ea32700ab79573e4"},"cell_type":"code","source":"# Plot the residuals of the training data\nsc = StandardScaler()\n\n# I forgot to std scale first time around, and I was getting crazy residuals in the 1000's, whoops!\nmlpr_train_predictions = mlpr.predict(sc.fit_transform(x_train)).ravel()\nmlpr_train_actuals = y_train.values.ravel()\ntrain_ages = x_train['age'].values.ravel()\n\n\nfig, ax = plt.subplots(1,1)\nfig.set_size_inches(15, 9)\n\nprint(mlpr_train_actuals[0:10])\nprint(mlpr_train_predictions[0:10])\n\nax.scatter(train_ages, mlpr_train_actuals - mlpr_train_predictions, color='red', s=1)\nax.axhline(0, color='black', linestyle='--')\n\nax.set_xlabel('Age (days)')\nax.set_ylabel('MLPR Residuals on Training (MPa)')\nax.set_title('Residual Plot for MLP Regressor')\nax.set_xticks( range(0, max(train_ages)+5, 21) )\nax.set\n\nax.grid(color='gray', linestyle='--', axis='both')\nax.set_axisbelow(True)\nax.set_facecolor(color='lightgray')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6832fff4af10c46cd34b8351134ab4624b8288f9"},"cell_type":"code","source":"# Now for the mlp regressor's resdicuals on the validation data\nmlpr_val_predictions = mlpr.predict(sc.transform(x_val)).ravel() # don't forget to std scale!\nmlpr_val_actuals = y_val.values.ravel()\nval_ages = x_val['age'].values.ravel()\n\n\nfig, ax = plt.subplots(1,1)\nfig.set_size_inches(15, 9)\n\n# residuals = actual - predicted\nax.scatter(val_ages, mlpr_val_actuals - mlpr_val_predictions, color='red', s=4, marker='*')\nax.axhline(0, color='black', linestyle='--')\n\nax.set_xlabel('Age (days)')\nax.set_ylabel('MLPR Residuals on Val (MPa)')\nax.set_title('Residual Plot for MLP Regressor')\nax.set_xticks( range(0, max(val_ages)+5, 21) )\n\n# prettify it\nax.grid(color='gray', linestyle='--', axis='both')\nax.set_axisbelow(True)\nax.set_facecolor(color='lightgray')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c0d2a6a6ed3bff241fdff2add06c1f653eb51d4"},"cell_type":"markdown","source":"The MLP regressor's residual plots on the training and the validation data both look pretty similar. I had hoped that maybe I could find a cutoff age. I had hoped that lower ages would have higher residuals than older ages (which seems to be the case, but not a very strong one). Everything below a cutoff age would get its own MLPRegressor while everything at or above the cutoff age goes to the other MLPRegressor. Maybe everything after 30 days should get its own? But is there enough after 30 days? How about other cutoffs? Let's see."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"eeb16c922ca5f943e8c9d4a423db3b01e239cb7a"},"cell_type":"code","source":"# Let's try see the % of data we'd have to work with for different cutoffs\ncutoffs = [7, 21, 30, 50, 100]\nfor cutoff in cutoffs:\n    print('{0:.1f}% with age > {1}'.format(100.0 * x_train[ x_train.age > cutoff ].shape[0] / x_train.shape[0], cutoff)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"40b26c0d1ae6fb3455c2b09d8715f30ea5a4745a"},"cell_type":"code","source":"# Let's try the 30 day cutoff and train 2 MLPRegressors, and see their MSE\ncutoff = 30 # ≤ this and > this\n\n# split the data on the specified cutoff\n\n# do all of the > cutoff stuff first\nx_train_age_gt = x_train[ x_train['age'] > cutoff] # gt means \"greater than\"\ny_train_age_gt = y_train.iloc[[i for i in x_train_age_gt.index]]\n\nmlpr_gt = MLPRegressor(hidden_layer_sizes=[1820], max_iter=4800, tol=1e-8, alpha=40.7)\n\n# std scale and fit\nsc_gt =  StandardScaler()\nmlpr_gt.fit( sc_gt.fit_transform(x_train_age_gt), y_train_age_gt.values.ravel())\n\n# > 30 MSE training\nmse_gt = mean_squared_error(y_train_age_gt, mlpr_gt.predict(sc_gt.transform(x_train_age_gt)) )                 \nprint('MLPRegressor for age greater than {} days had a training MSE of {}'.format(cutoff, mse_gt))\n\n# > 30 validation \nx_val_age_gt = x_val[ x_val['age'] > cutoff]\ny_val_age_gt = y_val.iloc[[i for i in x_val_age_gt.index]]\n\n# > 30 MSE validation\nmse_gt = mean_squared_error(y_val_age_gt, mlpr_gt.predict( sc_gt.transform(x_val_age_gt)) )\nprint('MLPRegressor for age greater than {} days had a validation MSE of {}'.format(cutoff, mse_gt))\nprint('\\t this means the average guess is off by {} mega Pascals'.format(math.sqrt(mse_gt)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e97b4ef23ae2c601b34bc7d781e00d544bf7ac37"},"cell_type":"code","source":"# NOW for the <= cutoff part\n# split the data on the cutoff already given\nx_train_age_lteq = x_train[ x_train['age'] <= cutoff] # lteq means \"less than or equal to\"\ny_train_age_lteq = y_train.iloc[[i for i in x_train_age_lteq.index]]\n\nmlpr_lteq = MLPRegressor(hidden_layer_sizes=[1820], max_iter=4800, tol=1e-8, alpha=20.7)\n\n# std scale and fit\nsc_lteq = StandardScaler()\nmlpr_lteq.fit( sc_lteq.fit_transform(x_train_age_lteq), y_train_age_lteq.values.ravel())\n\n# <= 30 MSE training\nmse_lteq = mean_squared_error(y_train_age_lteq, mlpr_lteq.predict( sc_lteq.transform(x_train_age_lteq)) )\nprint('MLPRegressor for age less than or equal to {} days had a training MSE of {}'.format(cutoff, mse_lteq))\n\n\n# <= cutoff validation\nx_val_age_lteq = x_val[ x_val['age'] <= cutoff]\ny_val_age_lteq = y_val.iloc[[i for i in x_val_age_lteq.index]]\n\n# <= 30 MSE validation\nmse_lteq = mean_squared_error(y_val_age_lteq, mlpr_lteq.predict( sc_lteq.transform(x_val_age_lteq)) )\nprint('MLPRegressor for age greater than {} days had a validation MSE of {}'.format(cutoff, mse_gt))\nprint('\\t this means the average guess is off by {} mega Pascals'.format(math.sqrt(mse_gt)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"650437f5dc01b60fee8b074b65a7d6afeb6e639e"},"cell_type":"markdown","source":"Great! Splitting into >30 and <=30 got the training and validation mean squared errors down to 23.8, i.e., average guesses are off by 4.8 MPa or about 696 psi. That's still not a fantastic estimate, but I'm surprised that splitting into 2 models actually helped that much. Let's plot the residuals again to see if there is, perhaps, some more splitting we could do. It would be identified as before: where the residuals are all over the place less than some cutoff but more stable greater than the cutoff (or vice versa). "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"64803df100c03c7a6318f79855146a1f53481abc"},"cell_type":"code","source":"# Residual plot for MLPR > cutoff\nmlpr_gt_predictions = mlpr_gt.predict(sc_gt.transform(x_train_age_gt)).ravel() # don't forget to std scale!\nmlpr_gt_val_actuals = y_train_age_gt.values.ravel()\ntrain_gt_ages = x_train_age_gt['age'].values.ravel()\n\n\nfig, ax = plt.subplots(1,1)\nfig.set_size_inches(15, 9)\n\n# residuals = actual - predicted\nax.scatter(train_gt_ages, mlpr_gt_val_actuals - mlpr_gt_predictions, color='red', s=8, marker='*')\nax.axhline(0, color='black', linestyle='--')\n\nax.set_xlabel('age (days)')\nax.set_ylabel('MLPR for greater than {} Residuals on Training (MPa)'.format(cutoff))\nax.set_title('Residual Plot for MLP Regressor for greater than {}'.format(cutoff))\nax.set_xticks( range(0, max(train_gt_ages)+5, 28) )\n\n# prettify it\nax.grid(color='gray', linestyle='--', axis='both')\nax.set_axisbelow(True)\nax.set_facecolor(color='lightgray')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6cbe26ec403b408a0bea9ee8ee056a6575fbd6e0"},"cell_type":"code","source":"# Now the residual plot for MLPR <= cutoff\nmlpr_lteq_predictions = mlpr_lteq.predict(sc_lteq.transform(x_train_age_lteq)).ravel() # don't forget to std scale!\nmlpr_lteq_val_actuals = y_train_age_lteq.values.ravel()\ntrain_lteq_ages = x_train_age_lteq['age'].values.ravel()\n\n\nfig, ax = plt.subplots(1,1)\nfig.set_size_inches(15, 9)\n\n# residuals = actual - predicted\nax.scatter(train_lteq_ages, mlpr_lteq_val_actuals - mlpr_lteq_predictions, color='red', s=8, marker='*')\nax.axhline(0, color='black', linestyle='--')\n\nax.set_xlabel('age (days)')\nax.set_ylabel('MLPR for less than or equal to {} Residuals on Training (MPa)'.format(cutoff))\nax.set_title('Residual Plot for MLP Regressor for less than or equal to {}'.format(cutoff))\n\n\nax.set_xticks( range(0, max(train_lteq_ages)+5, 5) )\n# prettify it\nax.grid(color='gray', linestyle='--', axis='both')\nax.set_axisbelow(True)\nax.set_facecolor(color='lightgray')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef59825b98b9db3944aeb6442cc9f5d6b0804ac4"},"cell_type":"markdown","source":"Just comparing these two residual plots to the two previous ones, the dots do seem slightly closer to the dashed-line at residual=0. I don't really see any other splits that would be worth it, so I'm going to leave it at that. \n\nThus, the overall finding is that splitting into 2 models (one for <= 30 days and another for > 30 days) meaningfully improved mean sqaured error. The mean squared error is about 23 MPa^2, which means the average guess is off by about 4.8 MPa. Again, this is not fantastic, but it is a pretty decent improvement over the linear regression, which had MSE of 120 or so, meaning its average guesses were off by about 11 MPa. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}