{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random\nimport sys\n\nrandom.seed(0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data cleaning and formatting\nLet's look at summaries of the training data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_rawDf = pd.read_csv('../input/av-healthcare-analytics-ii/healthcare/train_data.csv')\ntest_rawDf = pd.read_csv('../input/av-healthcare-analytics-ii/healthcare/test_data.csv')\ntrain_rawDf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_rawDf.describe().T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_rawDf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting rid of null values\n'Bed Grade' and 'City_Code_Patient' have null values. \nWe'll replace 'Bed Grade' null values with the integer closest to the mean: 3.0.\nWe'll replace 'City_Code_Patient' null values with the column mode.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"city_code_patient_mode = train_rawDf['City_Code_Patient'].mode()[0]\ntrain_Df = train_rawDf.copy()\ntest_Df = test_rawDf.copy()\ntrain_Df['Bed Grade'] = train_Df['Bed Grade'].fillna(3.0)\ntrain_Df['City_Code_Patient'] = train_Df['City_Code_Patient'].fillna(city_code_patient_mode)\ntest_Df['Bed Grade'] = test_Df['Bed Grade'].fillna(3.0)\ntest_Df['City_Code_Patient'] = test_Df['City_Code_Patient'].fillna(city_code_patient_mode)\ntrain_Df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating maps from category values to integers\nFor the categorical columns, we'll map the values to an integer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_columns = ['Hospital_type_code', 'Hospital_region_code', 'Department', 'Ward_Type', 'Ward_Facility_Code', 'Type of Admission', 'Severity of Illness', 'Age', 'Stay']\ncategorical_column_name_to_value_to_integer_dict = {}\nfor column_name in categorical_columns:\n    value_to_integer_dict = {}\n    values = sorted(train_Df[column_name].unique())\n    for index, value in enumerate(values):\n        value_to_integer_dict[value] = index\n    categorical_column_name_to_value_to_integer_dict[column_name] = value_to_integer_dict\n    \n# Manually set values for 'Severity of Illness' to have an ordinal interpretation: Minor < Moderate < Extreme\ncategorical_column_name_to_value_to_integer_dict['Severity of Illness'] = {'Minor': 0, 'Moderate': 1, 'Extreme': 2}\nprint (categorical_column_name_to_value_to_integer_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Replace the categories expressed as strings with an integer encoding.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def ReplaceCategoricalValues(dataframe, categorical_column_name_to_value_to_integer_dict):\n    for column_name in dataframe.columns:\n        if column_name in categorical_column_name_to_value_to_integer_dict:\n            old_values = list(categorical_column_name_to_value_to_integer_dict[column_name].keys())\n            new_values = list(categorical_column_name_to_value_to_integer_dict[column_name].values())\n            dataframe[column_name] = dataframe[column_name].replace(old_values, new_values)\n\nReplaceCategoricalValues(train_Df, categorical_column_name_to_value_to_integer_dict)\nReplaceCategoricalValues(test_Df, categorical_column_name_to_value_to_integer_dict)\ntrain_Df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get rid of labels containing no useful information: case_id, patientid.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_Df = train_Df.drop(columns=['case_id', 'patientid'])\ntest_Df = test_Df.drop(columns=['patientid']) # We'll need 'case_id' for submission\ntrain_Df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normalize the columns that have values that can be interpreted as continuous values: 'Available Extra Rooms in Hospital', 'Bed Grade', 'Severity of Illness', 'Visitors with Patient', 'Age', 'Admission_Deposit', 'Stay'.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"continuous_values_columns = ['Available Extra Rooms in Hospital', 'Bed Grade', 'Severity of Illness', 'Visitors with Patient', 'Age', 'Admission_Deposit', 'Stay']\ncontinuous_column_to_min_max_dict = {column_name: (train_Df[column_name].min(), train_Df[column_name].max()) for column_name in continuous_values_columns}\nfor column_name in continuous_values_columns:\n    min_value = continuous_column_to_min_max_dict[column_name][0]\n    max_value = continuous_column_to_min_max_dict[column_name][1]\n    if min_value == max_value:\n        raise ValueError(\"min_value == max_value ({})\".format(min_value))\n    train_Df[column_name] = (train_Df[column_name] - min_value)/(max_value - min_value)\n    if column_name is not 'Stay':\n        test_Df[column_name] = (test_Df[column_name] - min_value)/(max_value - min_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"City_Code_Patient should be an integer, not a float.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_Df['City_Code_Patient'] = train_Df['City_Code_Patient'].astype(int)\ntest_Df['City_Code_Patient'] = test_Df['City_Code_Patient'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_Df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Separate the target output column 'Stay' from the other feature columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_target_Df = train_Df['Stay']\ntrain_features_Df = train_Df.drop(columns=['Stay'])\ntrain_features_Df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training and validation split\nWe have the features and the target output in the format that we want. We can now split them into a training set and a validation set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nprint(\"len(train_features_Df) = {}\".format(len(train_features_Df)))\nvalidation_proportion = 0.2\nX_train, X_valid, y_train, y_valid = train_test_split(train_features_Df, train_target_Df, \n                                                     test_size=validation_proportion)\nprint(\"len(X_train) = {}\".format(len(X_train)))\nprint (\"len(X_valid) = {}\".format(len(X_valid)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Classification decision tree\nThis is an easy first trial since decision trees don't require hyperparameters tuning. They can take as input both numerical and categorical values (once converted to one-hot encoding). To make sure the categorical values will be interpreted as such, we'll convert back their values to arbitrary letters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"integer_to_letter_dict = {}\nfor i in range(26):\n    integer_to_letter_dict[i] = chr(i + 97)\nfor i in range(26, 40):\n    integer_to_letter_dict[i] = 'A{}'.format(chr(i - 26 + 97))\nprint (integer_to_letter_dict)\nclassification_tree_categorical_columns = ['Hospital_code', 'Hospital_type_code', 'City_Code_Hospital', 'Hospital_region_code', 'Department', 'Ward_Type', 'Ward_Facility_Code', 'City_Code_Patient', 'Type of Admission']\nclassification_tree_categorical_column_name_to_value_to_integer_dict = {column_name: integer_to_letter_dict for column_name in classification_tree_categorical_columns}\nclassification_tree_X_train = X_train.copy()\nReplaceCategoricalValues(classification_tree_X_train, classification_tree_categorical_column_name_to_value_to_integer_dict)\nclassification_tree_X_valid = X_valid.copy()\nReplaceCategoricalValues(classification_tree_X_valid, classification_tree_categorical_column_name_to_value_to_integer_dict)\nclassification_tree_X_test = test_Df.copy()\nReplaceCategoricalValues(classification_tree_X_test, classification_tree_categorical_column_name_to_value_to_integer_dict)\n\nclassification_tree_X_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"scikit-learn's classification trees cannot deal directly with categorical values (admittedly, the phrasing in the [documentation](https://scikit-learn.org/stable/modules/tree.html) is confusing). We must convert the categorical values to one-hot encoding.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classification_tree_X_train = pd.get_dummies(classification_tree_X_train[classification_tree_X_train.columns], drop_first=True) # Will split the categorical columns (identified by their alphabetic values) into multiple binary columns, encoding them as one-hot\nclassification_tree_X_valid = pd.get_dummies(classification_tree_X_valid[classification_tree_X_valid.columns], drop_first=True)\nclassification_tree_X_test = pd.get_dummies(classification_tree_X_test[classification_tree_X_test.columns], drop_first=True)\n# Make sur both feature matrices have all the column names\ncolumn_names_1 = classification_tree_X_train.columns\ncolumn_names_2 = classification_tree_X_valid.columns\ncolumn_names_test = classification_tree_X_test.columns\ncolumn_names_union = set(column_names_1).union(set(column_names_2))\nfor column_name in column_names_union:\n    if column_name not in column_names_1:\n        classification_tree_X_train[column_name] = [0] * len(classification_tree_X_train.index)\n    if column_name not in column_names_2:\n        classification_tree_X_valid[column_name] = [0] * len(classification_tree_X_valid.index)\n    if column_name not in column_names_test:\n        classification_tree_X_test[column_name] = [0] * len(classification_tree_X_test.index)\n    \n    \n# Sort the column names in alphabetic order, so are identically ordered for both dataframes\nclassification_tree_X_train = classification_tree_X_train.reindex(sorted(classification_tree_X_train.columns), axis=1)\nclassification_tree_X_valid = classification_tree_X_valid.reindex(sorted(classification_tree_X_valid.columns), axis=1)\nclassification_tree_X_test = classification_tree_X_test.reindex(sorted(classification_tree_X_test.columns), axis=1)\nclassification_tree_X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_tree_X_train.columns)\nprint(classification_tree_X_valid.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create and train (with .fit() ) a classification tree.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def ClassFromPrediction(prediction_continuous):\n    if prediction_continuous < 0.05:\n        return 0\n    elif prediction_continuous < 0.15:\n        return 1\n    elif prediction_continuous < 0.25:\n        return 2\n    elif prediction_continuous < 0.35:\n        return 3\n    elif prediction_continuous < 0.45:\n        return 4\n    elif prediction_continuous < 0.55:\n        return 5\n    elif prediction_continuous < 0.65:\n        return 6\n    elif prediction_continuous < 0.75:\n        return 7\n    elif prediction_continuous < 0.85:\n        return 8\n    elif prediction_continuous < 0.95:\n        return 9\n    else:\n        return 10\n    from sklearn import tree\n    \ny_train_classes = [ClassFromPrediction(y) for y in y_train]\n\nfrom sklearn import tree\n\nclassification_tree = tree.DecisionTreeClassifier()\nclassification_tree.fit(classification_tree_X_train, y_train_classes)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how the classification tree performs on the validation set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_valid_classes = [ClassFromPrediction(y) for y in y_valid]\n\nclassification_tree_predicted_validation_y = classification_tree.predict(classification_tree_X_valid)\nprint (\"len(classification_tree_predicted_validation_y) = {}\".format(len(classification_tree_predicted_validation_y)))\nprint(\"len(y_valid_classes) = {}\".format(len(y_valid_classes)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can plot a x-y scatter plot of the predicted value against the target value, to get a visualization of the predictive power.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classification_tree_prediction_mtx = np.zeros((11, 11), dtype=int)\nfor prediction, target in zip(list(classification_tree_predicted_validation_y), list(y_valid_classes)):\n    classification_tree_prediction_mtx[target, prediction] += 1\n\n\nimport seaborn as sns\nax = sns.heatmap(classification_tree_prediction_mtx)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's not very impressive!\nLet's compute the overall accuracy.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"correct = (classification_tree_predicted_validation_y == y_valid_classes)\naccuracy = correct.sum() / correct.size\nprint (\"classification tree accuracy = {}\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay... Is it at least better than always predicting the most common label?!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_class_to_occurrences_dict = {classNdx: 0 for classNdx in range(11)}\nfor target_obs in y_train_classes:\n    y_train_class_to_occurrences_dict[target_obs] += 1\nprint (y_train_class_to_occurrences_dict)\ny_train_mode = 0\nhighest_count = -1\nfor classNdx, count in y_train_class_to_occurrences_dict.items():\n    if count > highest_count:\n        highest_count = count\n        y_train_mode = classNdx\n\nprint (\"y_train_mode = {}\".format(y_train_mode))\ncorrect_dumb_predictions = 0\nfor valid_obs in y_valid_classes:\n    if valid_obs == y_train_mode:\n        correct_dumb_predictions += 1\ndumb_accuracy = correct_dumb_predictions/len(y_valid_classes)\nprint (\"Dumb accuracy obtained when always predicting {}: {}\".format(y_train_mode, dumb_accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I wouldn't say it's better! Depending on your random seed, our classification tree can do better or *worse* than simply predicting the mode value (2) from the training set.\nWe have to do better than that!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## CatBoostClassifier\nFollowing https://www.kaggle.com/abhijeetbhilare/how-long-patient-stay-in-the-hospital, let's try a CatBoostClassifier with the same data as we used for the classification decision tree.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import Pool, CatBoostClassifier\ncat_boost_train_dataset = Pool(data=classification_tree_X_train, label=y_train_classes)\ncat_boost_valid_dataset = Pool(data=classification_tree_X_valid, label=y_valid_classes)\n# Initialising catboost classifier\n\ncat_boost_model = CatBoostClassifier(eval_metric='Accuracy')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_boost_model.fit(cat_boost_train_dataset, eval_set=cat_boost_valid_dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That looks more promising!\nLets'see how it performs on the validation set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_boost_predicted_validation_y = cat_boost_model.predict(classification_tree_X_valid)\ncat_boost_prediction_mtx = np.zeros((11, 11), dtype=int)\nfor prediction, target in zip(list(cat_boost_predicted_validation_y), list(y_valid_classes)):\n    cat_boost_prediction_mtx[target, prediction] += 1\n\n\nax = sns.heatmap(cat_boost_prediction_mtx)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The confusion matrix heatmap looks more concentrated on the main diagonal. What about the validation accuracy?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"correct = [cat_boost_predicted_validation_y[i, 0] == y_valid_classes[i] for i in range(len(y_valid_classes))]\ncorrect_sum = sum([1 for v in correct if v == True ])\naccuracy = correct_sum / len(correct)\nprint (\"cat_boost accuracy = {}\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification with a neural network\nSince we're all set with our one-hot encoded features tensor, let's feed it to a multi-layer perceptron. We first convert our dataframes to torch tensors.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\n\nclassification_X_train_Tsr = torch.tensor(classification_tree_X_train.values)\nclassification_X_valid_Tsr = torch.tensor(classification_tree_X_valid.values)\ny_train_classes_Tsr = torch.tensor(y_train_classes).long()\ny_valid_classes_Tsr = torch.tensor(y_valid_classes).long()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We define a simple multi-layer perceptron:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class MLP(torch.nn.Module):\n    def __init__(self, \n                 number_of_inputs,\n                 number_of_classes,\n                 hidden_layer_width=128,\n                 dropout_proportion=0.5):\n        super(MLP, self).__init__()\n        self.number_of_inputs = number_of_inputs\n        self.number_of_classes = number_of_classes\n        self.hidden_layer_width = hidden_layer_width\n        self.linear1 = torch.nn.Linear(self.number_of_inputs, self.hidden_layer_width)\n        self.linear2 = torch.nn.Linear(self.hidden_layer_width, self.hidden_layer_width)\n        self.linear3 = torch.nn.Linear(self.hidden_layer_width, self.hidden_layer_width)\n        self.linear4 = torch.nn.Linear(self.hidden_layer_width, self.number_of_classes)\n        self.dropout = torch.nn.Dropout(p=dropout_proportion)\n        self.batchnorm = torch.nn.BatchNorm1d(self.hidden_layer_width)\n        \n    def forward(self, inputTsr):\n        # inputTsr.shape = (N, self._number_of_inputs)\n        latent1Tsr = torch.nn.functional.relu(self.linear1(inputTsr)) # latent1Tsr.shape = (N, first_layer_width)\n        latent1Tsr = self.dropout(latent1Tsr)\n        #latent1Tsr = self.batchnorm(latent1Tsr)\n        latent2Tsr = torch.nn.functional.relu( self.linear2(latent1Tsr))\n        #latent2Tsr = self.batchnorm(latent2Tsr)\n        latent3Tsr = torch.nn.functional.relu( self.linear3(latent2Tsr))\n        latent3Tsr = self.batchnorm(latent3Tsr)\n        outputTsr = self.linear4(latent3Tsr)\n        return outputTsr\n\nnumber_of_classes = 11\nmlp = MLP(classification_X_train_Tsr.shape[1], number_of_classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll need a PyTorch Dataset to feed our neural network with minibatches during training.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nclass ClassificationObservationDataset(Dataset):\n    def __init__(self, featuresTsr, target_class_Tsr):\n        if featuresTsr.shape[0] != target_class_Tsr.shape[0]:\n            raise ValueError(\"ClassificationObservationDataset.__init__(): featuresTsr.shape[0] ({}) != target_class_Tsr.shape[0] ({})\".format(featuresTsr.shape[0], target_class_Tsr.shape[0]))\n        self.featuresTsr = featuresTsr\n        self.target_class_Tsr = target_class_Tsr\n        \n    def __len__(self):\n        return self.featuresTsr.shape[0]\n    \n    def __getitem__(self, idx):\n        if idx < 0 or idx >= self.featuresTsr.shape[0]:\n            raise IndexError(\"ClassificationObservationDataset.__getitem__(): idx ({}) is out of [0, {}]\".format(self.featuresTsr.shape[0] - 1))\n        return (self.featuresTsr[idx].float(), self.target_class_Tsr[idx])\n    \ntrain_dataset = ClassificationObservationDataset(classification_X_train_Tsr, y_train_classes_Tsr)\nvalidation_dataset = ClassificationObservationDataset(classification_X_valid_Tsr, y_valid_classes_Tsr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The target class is strongly imbalanced, as we saw earlier, when we could get a 27% - 30% accuracy by always predicting class '21-30'. Let's define a weight matrix that will give a weight to classes inversely proportional to their corresponding frequency.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_train_class_to_occurrences_dict)\n#occurrences_sum = sum([v for k, v in y_train_class_to_occurrences_dict.items()])\nclass_weight_Tsr = torch.zeros(len(y_train_class_to_occurrences_dict))\nfor classNdx, occurrences in y_train_class_to_occurrences_dict.items():\n    class_weight = 1.0/max(occurrences, 1)\n    class_weight_Tsr[classNdx] = class_weight\nclass_weight_sum = class_weight_Tsr.sum().item()\nclass_weight_Tsr = class_weight_Tsr / class_weight_sum\n\nprint (class_weight_Tsr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can launch the training loop.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mlp_parameters = filter(lambda p: p.requires_grad, mlp.parameters())\noptimizer = torch.optim.Adam(mlp_parameters, lr=0.0001)\nlossFcn = torch.nn.CrossEntropyLoss()#weight=class_weight_Tsr)\ntrain_dataLoader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nvalid_dataLoader = DataLoader(validation_dataset, batch_size=len(validation_dataset))\nuseCuda = torch.cuda.is_available()\nif useCuda:\n    mlp = mlp.cuda()\nlowest_validation_loss = sys.float_info.max\nchampion_classification_mlp_filepath = '/kaggle/working/classification_mlp.pth'\nnumber_of_epochs = 20\nfor epoch in range(1, number_of_epochs):\n    mlp.train()\n    loss_sum = 0.0\n    \n    number_of_batches = 0\n    for (featuresTsr, target_class_ndx) in train_dataLoader:\n        if number_of_batches % 20 == 1:\n            print (\".\", end=\"\", flush=True)\n        if useCuda:\n            featuresTsr = featuresTsr.cuda()\n            target_class_ndx = target_class_ndx.cuda()\n        predicted_class_ndx = mlp(featuresTsr)\n        optimizer.zero_grad()\n        loss = lossFcn(predicted_class_ndx, target_class_ndx)\n        loss.backward()\n        optimizer.step()\n        loss_sum += loss.item()\n        number_of_batches += 1\n    train_loss = loss_sum/number_of_batches\n    print (\"\\nepoch {}: train_loss = {}\".format(epoch, train_loss))\n    \n    # Validation\n    mlp.eval()\n    with torch.set_grad_enabled(False):\n        for validation_features_Tsr, validation_target_class_ndx_Tsr in valid_dataLoader: # Will be a single pass since batch_size=len(validation_dataset)\n            if useCuda:\n                validation_features_Tsr = validation_features_Tsr.cuda()\n                validation_target_class_ndx_Tsr = validation_target_class_ndx_Tsr.cuda()\n            validation_predicted_class_ndx_Tsr = mlp(validation_features_Tsr)\n            validation_loss = lossFcn(validation_predicted_class_ndx_Tsr, validation_target_class_ndx_Tsr).item()\n            if validation_loss < lowest_validation_loss:\n                lowest_validation_loss = validation_loss\n                torch.save(mlp.state_dict(), champion_classification_mlp_filepath)\n            # Validation accuracy\n            validation_correct_predictions = (validation_predicted_class_ndx_Tsr.argmax(dim=1) == validation_target_class_ndx_Tsr).sum().item()\n            validation_accuracy = validation_correct_predictions / validation_target_class_ndx_Tsr.shape[0]\n            print (\"validation_loss = {}; validation_accuracy = {}\".format(validation_loss, validation_accuracy))\n            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random forest\nWe'll now try a random forest from scikit-learn.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrandom_forest_classifier = RandomForestClassifier(max_depth=16, random_state=0)\nrandom_forest_classifier.fit(classification_tree_X_train, y_train_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_predicted_validation_y = random_forest_classifier.predict(classification_tree_X_valid)\nrandom_forest_confusion_mtx = np.zeros((11, 11), dtype=int)\nfor prediction, target in zip(list(random_forest_predicted_validation_y), list(y_valid_classes)):\n    random_forest_confusion_mtx[target, prediction] += 1\n\n\nax = sns.heatmap(random_forest_confusion_mtx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correct = (random_forest_predicted_validation_y == y_valid_classes)\naccuracy = correct.sum() / correct.size\nprint (\"random_forest accuracy = {}\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensemble\nWe are now ready to create an ensemble with the models that performed significantly better than simply guessing all the time the most frequent class:\n* The CatBoostClassifier\n* The classification MLP\n* The random forest\n\nOur final prediction will be the one that rallies at least 2 out of three predictors. In case of a draw, let's pick the choice from the CatBoostClassifier, since it is the one that gave a little more accuracy on the validation set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classification_tree_X_test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CatBoost classifier\nprint (classification_tree_X_test.columns)\ncat_boost_prediction = cat_boost_model.predict(classification_tree_X_test.drop(columns=['case_id']))\nprint (\"cat_boost_prediction.shape = {}\".format(cat_boost_prediction.shape))\n\n# Neural network classifier\nclassification_X_test_Tsr = torch.tensor(classification_tree_X_test.drop(columns=['case_id']).values).float()\nmlp.load_state_dict(torch.load(champion_classification_mlp_filepath))\nmlp.eval()\nmlp_prediction_Tsr = torch.argmax(mlp(classification_X_test_Tsr), dim=1)\nprint (\"mlp_prediction_Tsr.shape = {}\".format(mlp_prediction_Tsr.shape))\n\n# Random forest\nrandom_forest_prediction = random_forest_classifier.predict(classification_tree_X_test.drop(columns=['case_id']))\nprint(\"random_forest_prediction.shape = {}\".format(random_forest_prediction.shape))\n\nnumber_of_unanimities = 0\nnumber_of_two_votes = 0\nnumber_of_draws = 0\ncase_id_to_prediction_dict = {}\n\nfor testNdx, row in classification_tree_X_test.iterrows():\n    case_id = int(row['case_id'])\n    cat_boost_predicted_class = cat_boost_prediction[testNdx, 0]\n    neural_network_predicted_class = mlp_prediction_Tsr[testNdx].item()\n    random_forest_predicted_class = random_forest_prediction[testNdx]\n    #print(\"{}: {}, {}, {}\".format(case_id, cat_boost_predicted_class, neural_network_predicted_class, random_forest_predicted_class))\n    \n    if cat_boost_predicted_class == neural_network_predicted_class and cat_boost_predicted_class == random_forest_predicted_class:\n        case_id_to_prediction_dict[case_id] = cat_boost_predicted_class\n        number_of_unanimities += 1\n    elif cat_boost_predicted_class == neural_network_predicted_class:\n        case_id_to_prediction_dict[case_id] = cat_boost_predicted_class\n        number_of_two_votes += 1\n    elif neural_network_predicted_class == random_forest_predicted_class:\n        case_id_to_prediction_dict[case_id] = neural_network_predicted_class\n        number_of_two_votes += 1\n    elif cat_boost_predicted_class == random_forest_predicted_class:\n        case_id_to_prediction_dict[case_id] = cat_boost_predicted_class\n        number_of_two_votes += 1\n    else:\n        case_id_to_prediction_dict[case_id] = cat_boost_predicted_class\n        number_of_draws += 1\n        \nprint(\"number_of_unanimities = {}; number_of_two_votes = {}; number_of_draws = {}\".format(number_of_unanimities, number_of_two_votes, number_of_draws))\n        \nprint(\"case_id_to_prediction_dict = {}\".format(case_id_to_prediction_dict))\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Write a submission file\nwith open('/kaggle/working/submission.csv', 'w+') as submission_file:\n    submission_file.write('case_id,Stay\\n')\n    for case_id, prediction in case_id_to_prediction_dict.items():\n        submission_file.write('{},{}\\n'.format(case_id, prediction))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}