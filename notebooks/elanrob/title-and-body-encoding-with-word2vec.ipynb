{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 60k Stack Overflow Questions with Quality Rating\nWhat makes a great question on Stack Overflow? What makes a question not-so-good, to the point where the administrators feel this discussion should be closed right away? That's what we'll try to assess in this project. Our strategy will be based on the following assumptions:\n* The title structure contains quality predictors\n* The text body structure contains quality predictors\n* The date and time of the posting doesn't contain any quality predictors (although one could argue that posting at 3:00 AM might be correlated with a tired mind, which might be correlated with a low quality posting. We'll ignore this signal).\n* The tags don't contain any quality predictors. They are used for searching purpose, and the evaluators of the question quality probably didn't base their judgment on the tags list.\n\n## Data exploration\nLet's load the dataset and look at it's general features."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\n\nrandom_seed = 0\nrandom.seed(random_seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_filepath = '../input/60k-stack-overflow-questions-with-quality-rate/data.csv'\nrawDf = pd.read_csv(dataset_filepath)\nrawDf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Following our assumptions, we'll drop the columns that do not contain useful information."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"prunedDf = rawDf.drop(columns=['Id', 'Tags', 'CreationDate'])\nprunedDf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## String tokenization\nWe'll convert everything to lowercase, get rid of the digits (which would unnecessarily bloat the vocabulary) and get rid of formatting tokens. We'll keep the punctuation, since questions that have no question marks, or long sentences without comas would probably be low quality indicators."},{"metadata":{"trusted":true},"cell_type":"code","source":"import en_core_web_sm\nimport re\nimport string\nfrom bs4 import BeautifulSoup\n\nnlp = en_core_web_sm.load()\n\ndef Tokenize(text, nlp):\n    text = text.lower()\n    text = re.sub(\"\\d+\", \"\", text) # Remove all digits\n    text = BeautifulSoup(text).get_text() # Remove markups    \n    tokens = [token.text for token in nlp.tokenizer(text) if not token.text.isspace()]\n    return tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title_maximum_length = 0\nbody_maximum_length = 0\n\nfor index, row in prunedDf.iterrows():\n    title = row['Title']\n    body = row['Body']\n    tokenized_title = Tokenize(title, nlp)\n    tokenized_body = Tokenize(body, nlp)\n    \"\"\"if index % 25000 == 0:\n        print (\"body = {}\".format(body))\n        print (\"tokenized_body = {}\".format(tokenized_body))\n    \"\"\"\n    prunedDf.loc[index, 'Title'] = tokenized_title\n    prunedDf.loc[index, 'Body'] = tokenized_body\n    if len(tokenized_title) > title_maximum_length:\n        title_maximum_length = len(tokenized_title)\n    if len(tokenized_body) > body_maximum_length:\n        body_maximum_length = len(tokenized_body)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split between training and validation datasets\nSince we'll build our vocabulary and our word2vec embeddings with the training dataset only, we need to make the split now."},{"metadata":{"trusted":true},"cell_type":"code","source":"featuresDf = prunedDf.drop(columns=['Y'])\ntargetDf = prunedDf['Y']\n\nfrom sklearn.model_selection import train_test_split\ntrain_features_Df, validation_features_Df, train_target_Df, validation_target_Df = train_test_split(featuresDf, targetDf, test_size=0.2, random_state=random_seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build the vocabulary\nWe'll go through the titles and the bodies of the training dataset to list the words that are used. We'll keep a counter of word occurrences, in order to filter the rare words."},{"metadata":{"trusted":true},"cell_type":"code","source":"word_to_occurrences_dict = {}\nfor index, row in train_features_Df.iterrows():\n    title_words = row['Title']\n    body_words = row['Body']\n    for word in title_words:\n        if word in word_to_occurrences_dict:\n            word_to_occurrences_dict[word] += 1\n        else:\n            word_to_occurrences_dict[word] = 1\n    for word in body_words:\n        if word in word_to_occurrences_dict:\n            word_to_occurrences_dict[word] += 1\n        else:\n            word_to_occurrences_dict[word] = 1\n    if index % 1000 == 0:\n        print(\".\", end=\"\", flush=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now get rid of words that appear less than a certain number of times. These could be misspelled or rare words."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Before filtering the single occurrences, len(word_to_occurrences_dict) = {}\".format(len(word_to_occurrences_dict)))\nsingle_occurrence_words = []\nfor word, occurrences in word_to_occurrences_dict.items():\n    if occurrences < 20:\n        single_occurrence_words.append(word)\nfor word in single_occurrence_words:\n    word_to_occurrences_dict.pop(word)\nprint(\"After filtering the single occurrences, len(word_to_occurrences_dict) = {}\".format(len(word_to_occurrences_dict)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got rid of a lot of words! We can sort the remaining words by their relative frequencies. We also want to add three special words that will be used when converting the sentences into sequences of tokens."},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_tokens = sorted(word_to_occurrences_dict.items(),\n                           key=lambda x: x[1], reverse=True) # Cf. https://careerkarma.com/blog/python-sort-a-dictionary-by-value/\nsorted_tokens = [('ENDOFSEQ', 0), ('UNKNOWN', 0), ('NOTSET', 0)] + sorted_tokens\nprint(sorted_tokens[0:100])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the most frequent 'words' are actually punctuation signs such as ')', ',', '\"' etc. There is probably a large amount of those that come from the body of the questions, which often includes code snippets in stack-overflow.\nWe need to convert the words to a corresponding unique index, and to convert from the index to the corresponding word."},{"metadata":{"trusted":true},"cell_type":"code","source":"def WordToIndex(token_occurrence_pairs):\n    word_to_index_dict = {}\n    index_to_word_dict = {}\n    for index, token_occurrence in enumerate(token_occurrence_pairs):\n        word_to_index_dict[token_occurrence[0]] = index\n        index_to_word_dict[index] = token_occurrence[0]\n    return word_to_index_dict, index_to_word_dict\n\nword_to_index_dict, index_to_word_dict = WordToIndex(sorted_tokens)\nprint (\"word_to_index_dict['compile'] = {}\".format(word_to_index_dict['compile']))\nprint (\"index_to_word_dict[391] = {}\".format(index_to_word_dict[391]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Convert the texts to indices\nNow that we have our vocabulary, we can replace the title and body texts with a list of indices. This will be necessary for the next step of word embedding. Let's find the maximum length for the title and the body."},{"metadata":{"trusted":true},"cell_type":"code","source":"title_maximum_length += 1 # To make room for the extra 'ENDOFSEQ' token\nbody_maximum_length += 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ConvertTokensListToIndices(tokens, word_to_index_dict, maximum_length):\n    indices = [word_to_index_dict['NOTSET']] * maximum_length\n    for tokenNdx, token in enumerate(tokens):\n        index = word_to_index_dict.get(token, word_to_index_dict['UNKNOWN']) # If the word is not in the dictionary, fall back to 'UNKOWN'\n        indices[tokenNdx] = index\n    if len(tokens) < maximum_length:\n        indices[len(tokens)] = word_to_index_dict['ENDOFSEQ']\n    return indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feature_indices_Df = pd.DataFrame(columns=['Title', 'Body'])\nvalidation_feature_indices_Df = pd.DataFrame(columns=['Title', 'Body'])\nfor row in train_features_Df.itertuples():\n    titleList = row[1]\n    bodyList = row[2]\n    title_indices = ConvertTokensListToIndices(titleList, word_to_index_dict, title_maximum_length)\n    body_indices = ConvertTokensListToIndices(bodyList, word_to_index_dict, body_maximum_length)\n    train_feature_indices_Df = train_feature_indices_Df.append({'Title': title_indices, 'Body': body_indices}, ignore_index=True)\nfor row in validation_features_Df.itertuples():\n    titleList = row[1]\n    bodyList = row[2]\n    title_indices = ConvertTokensListToIndices(titleList, word_to_index_dict, title_maximum_length)\n    body_indices = ConvertTokensListToIndices(bodyList, word_to_index_dict, body_maximum_length)\n    validation_feature_indices_Df = validation_feature_indices_Df.append({'Title': title_indices, 'Body': body_indices}, ignore_index=True)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feature_indices_Df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word embedding\nNow that all our titles and bodies are converted to lists of indices, we can use this data to create a word embedding. We'll be using the word2vec (CBOW) algorithm. The goal for the embedder will be to predict a central word from the context words (i.e. a few words before and after the central word). In order to train a neural network to do that, we need a PyTorch Dataset that will generate samples from our training dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport torch\nuseCuda = torch.cuda.is_available()\n\nclass ContextToWordDataset(Dataset):\n    def __init__(self,\n                 sentence_indices_dataframe,\n                 context_length,\n                 word_to_index_dict):\n        self.sentence_indices_dataframe = sentence_indices_dataframe\n        self.context_length = context_length\n        self.word_to_index_dict = word_to_index_dict\n        \n    def __len__(self):\n        return len(self.sentence_indices_dataframe)\n    \n    def __getitem__(self, idx):\n        sentence_indices = self.sentence_indices_dataframe.iloc[idx]\n        # Randomly select a target word\n        last_acceptable_center_index = len(sentence_indices) - 1\n        if self.word_to_index_dict['ENDOFSEQ'] in sentence_indices:\n            for position, index in enumerate(sentence_indices):\n                if index == self.word_to_index_dict['ENDOFSEQ']:\n                    last_acceptable_center_index = position        \n        targetNdx = random.choice(range(last_acceptable_center_index + 1))\n        # Create a Long tensor with dim (2 * context_length)\n        context_indicesTsr = torch.ones((2 * self.context_length)).long() * self.word_to_index_dict['NOTSET']\n        runningNdx = targetNdx - int(self.context_length)\n        counter = 0\n        while counter < 2 * self.context_length:\n            if runningNdx != targetNdx:\n                if runningNdx >= 0 and runningNdx < len(sentence_indices):\n                    context_indicesTsr[counter] = sentence_indices[runningNdx]\n                counter += 1\n            runningNdx += 1\n        return (context_indicesTsr, torch.tensor(sentence_indices[targetNdx]).long())\n\ncontext_length = 3\ntitle_context_word_dataset = ContextToWordDataset(train_feature_indices_Df['Title'], context_length, word_to_index_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have to create a neural network that will predict the central word given the context words, using the [word2vec Continuous Bag of Words (word2vec-CBOW)](https://en.wikipedia.org/wiki/Word2vec) algorithm. The predictor neural network is very shallow. It is composed of an embedding layer, an averaging layer and a linear layer for decoding. The embedding layer is a trainable lookup table that will map each word in the vocabulary to a low-dimensional vector space that - hopefully - keeps semantically similar words close together, as opposed to a one-hot encoding that do not preserve any similarities between words. The averaging layer acts as a shuffling operation for the context words. By averaging the context word embeddings, their order will be lost. The context words \\['the', 'red', 'apple', 'from', 'the', 'tree'\\] will produce the same averaged embedding vector as the context words \\['apple', 'from', 'the', 'tree', 'red', 'the'\\]. The last layer is a linear decoder that will predict the central word one-hot encoding from the averaged embedding vector. Once trained, the useful part will be the embedding lookup table, that will be used to encode the the words for the final predictor that we'll build."},{"metadata":{"trusted":true},"cell_type":"code","source":"class CenterWordPredictor(torch.nn.Module):\n    def __init__(self, vocabulary_size, embedding_dimension):\n        super(CenterWordPredictor, self).__init__()\n        self.embedding = torch.nn.Embedding(vocabulary_size, embedding_dimension)\n        self.decoderLinear = torch.nn.Linear(embedding_dimension, vocabulary_size)\n\n    def forward(self, contextTsr):\n        # contextTsr.shape = (N, 2 * context_length); contextTsr.dtype = torch.int64\n        embedding = self.embedding(contextTsr)  # (N, 2 * context_length, embedding_dimension)\n        # Average over context words: (N, 2 * context_length, embedding_dimension) -> (N, embedding_dimension)\n        embedding = torch.mean(embedding, dim=1)\n\n        # Decoding\n        outputTsr = self.decoderLinear(embedding)\n        return outputTsr\n\nembedding_dimension = 128\ntitle_center_word_predictor = CenterWordPredictor(len(word_to_index_dict), embedding_dimension)\nif useCuda:\n    title_center_word_predictor = title_center_word_predictor.cuda()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's train the center word predictor for the titles."},{"metadata":{"trusted":true},"cell_type":"code","source":"def TrainCenterWordPredictor(predictor, optimizer, lossFcn, train_dataLoader, number_of_epochs):\n    for epoch in range(1, number_of_epochs + 1):\n        predictor.train()\n        loss_sum = 0.0\n        number_of_batches = 0\n        for (context_indices_Tsr, target_word_Ndx_Tsr) in train_dataLoader:\n            if number_of_batches % 10 == 1:\n                print (\".\", end=\"\", flush=True)\n            if useCuda:\n                context_indices_Tsr = context_indices_Tsr.cuda()\n                target_word_Ndx_Tsr = target_word_Ndx_Tsr.cuda()\n            predicted_center_word_ndx = predictor(context_indices_Tsr)\n            optimizer.zero_grad()\n            loss = lossFcn(predicted_center_word_ndx, target_word_Ndx_Tsr)\n            loss.backward()\n            optimizer.step()\n            loss_sum += loss.item()\n            number_of_batches += 1\n        train_loss = loss_sum/number_of_batches\n        print (\"\\nepoch {}: train_loss = {}\".format(epoch, train_loss))\n        \nword_predictor_parameters = filter(lambda p: p.requires_grad, title_center_word_predictor.parameters())\noptimizer = torch.optim.Adam(word_predictor_parameters, lr=0.0001)\nlossFcn = torch.nn.CrossEntropyLoss()\ntrain_dataLoader = DataLoader(title_context_word_dataset, batch_size=32, shuffle=True)\n\nTrainCenterWordPredictor(title_center_word_predictor, optimizer, lossFcn, train_dataLoader, 50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can do the same thing with the text bodies."},{"metadata":{"trusted":true},"cell_type":"code","source":"body_context_word_dataset = ContextToWordDataset(train_feature_indices_Df['Body'], context_length, word_to_index_dict)\nbody_center_word_predictor = CenterWordPredictor(len(word_to_index_dict), embedding_dimension)\nif useCuda:\n    body_center_word_predictor = body_center_word_predictor.cuda()\nword_predictor_parameters = filter(lambda p: p.requires_grad, body_center_word_predictor.parameters())\noptimizer = torch.optim.Adam(word_predictor_parameters, lr=0.0001)\nlossFcn = torch.nn.CrossEntropyLoss()\ntrain_dataLoader = DataLoader(body_context_word_dataset, batch_size=32, shuffle=True)\n\nTrainCenterWordPredictor(body_center_word_predictor, optimizer, lossFcn, train_dataLoader, 20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification from the sentence embeddings\nInside the word predictors that we just trained are the word embeddings for the title and for the body that we'll feed as inputs to a multi-layer perceptron (MLP). This MLP will be trained to classify the samples. We first need to define a PyTorch Dataset that will generate samples in the desired format."},{"metadata":{"trusted":true},"cell_type":"code","source":"class EmbeddingsToClassDataset(Dataset):\n    def __init__(self,\n                sentence_indices_dataframe,\n                title_embedding,\n                body_embedding,\n                title_maximum_length,\n                body_maximum_length,\n                end_of_seq_Ndx,\n                not_set_Ndx,\n                target_class_dataframe,\n                class_to_index_dict):\n        self.sentence_indices_dataframe = sentence_indices_dataframe\n        self.title_embedding = title_embedding\n        self.body_embedding = body_embedding\n        self.title_maximum_length = title_maximum_length\n        self.body_maximum_length = body_maximum_length\n        self.end_of_seq_Ndx = end_of_seq_Ndx\n        self.not_set_Ndx = not_set_Ndx\n        self.target_class_dataframe = target_class_dataframe\n        self.class_to_index_dict = class_to_index_dict\n        \n    def __len__(self):\n        return len(self.sentence_indices_dataframe)\n    \n    def __getitem__(self, idx):\n        title_indices = self.sentence_indices_dataframe.iloc[idx]['Title']\n        body_indices = self.sentence_indices_dataframe.iloc[idx]['Body']\n        not_set_embedding = self.title_embedding.weight[self.not_set_Ndx]\n        title_embedding_Tsr = torch.zeros(self.title_maximum_length, self.title_embedding.weight.shape[1])\n        for rowNdx in range(title_embedding_Tsr.shape[0]):\n            title_embedding_Tsr[rowNdx] = not_set_embedding\n        end_of_seq_is_found = False\n        runningNdx = 0\n        while not end_of_seq_is_found and runningNdx < len(title_indices):\n            wordNdx = title_indices[runningNdx]\n            if wordNdx == self.end_of_seq_Ndx:\n                end_of_seq_is_found = True\n            word_embedding_Tsr = self.title_embedding.weight[wordNdx]\n            title_embedding_Tsr[runningNdx] = word_embedding_Tsr\n            runningNdx += 1\n        \n        not_set_embedding = self.body_embedding.weight[self.not_set_Ndx]\n        body_embedding_Tsr = torch.zeros(self.body_maximum_length, self.body_embedding.weight.shape[-1]) \n        for rowNdx in range(body_embedding_Tsr.shape[0]):\n            body_embedding_Tsr[rowNdx] = not_set_embedding\n        end_of_seq_is_found = False\n        runningNdx = 0\n        while not end_of_seq_is_found and runningNdx < self.body_maximum_length:\n            wordNdx = body_indices[runningNdx]\n            if wordNdx == self.end_of_seq_Ndx:\n                end_of_seq_is_found = True\n            word_embedding_Tsr = self.body_embedding.weight[wordNdx]\n            body_embedding_Tsr[runningNdx] = word_embedding_Tsr\n            runningNdx += 1\n            \n        #print (\"self.target_class_dataframe.iloc[idx] = {}\".format(self.target_class_dataframe.iloc[idx]))\n        target_class_index = self.class_to_index_dict[ self.target_class_dataframe.iloc[idx] ]\n        return ((title_embedding_Tsr, body_embedding_Tsr), torch.tensor(target_class_index).long())\n        \n            \n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_to_index_dict = {'HQ': 0, 'LQ_EDIT': 1, 'LQ_CLOSE': 2}\nbody_maximum_length = 100\ntraining_embeddings_to_class_dataset = EmbeddingsToClassDataset(\n                sentence_indices_dataframe=train_feature_indices_Df,\n                title_embedding=title_center_word_predictor.embedding,\n                body_embedding=body_center_word_predictor.embedding,\n                title_maximum_length=title_maximum_length,\n                body_maximum_length=body_maximum_length,\n                end_of_seq_Ndx=word_to_index_dict['ENDOFSEQ'],\n                not_set_Ndx=word_to_index_dict['NOTSET'],\n                target_class_dataframe=train_target_Df,\n                class_to_index_dict=class_to_index_dict)\nvalidation_embeddings_to_class_dataset = EmbeddingsToClassDataset(\n                sentence_indices_dataframe=validation_feature_indices_Df,\n                title_embedding=title_center_word_predictor.embedding,\n                body_embedding=body_center_word_predictor.embedding,\n                title_maximum_length=title_maximum_length,\n                body_maximum_length=body_maximum_length,\n                end_of_seq_Ndx=word_to_index_dict['ENDOFSEQ'],\n                not_set_Ndx=word_to_index_dict['NOTSET'],\n                target_class_dataframe=validation_target_Df,\n                class_to_index_dict=class_to_index_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"((title_embedding_Tsr, body_embeding_Tsr), target_class_Ndx_Tsr) = training_embeddings_to_class_dataset[0]\nprint (\"title_embedding_Tsr.shape = {}\".format(title_embedding_Tsr.shape))\nprint (\"title_embedding_Tsr = {}\".format(title_embedding_Tsr))\nprint (\"body_embeding_Tsr.shape = {}\".format(body_embeding_Tsr.shape))\nprint (\"body_embeding_Tsr = {}\".format(body_embeding_Tsr))\nprint (\"target_class_Ndx_Tsr = {}\".format(target_class_Ndx_Tsr))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Classification with a double LSTM neural network"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Double_LSTM(torch.nn.Module):\n    def __init__(self, embedding_dimension, \n                 lstm_hidden_dimension,\n                 num_lstm_layers, \n                 mlp_hidden_layer_dimension,\n                 number_of_classes,\n                 dropoutProportion=0.5):\n        super(Double_LSTM, self).__init__()\n        self.embedding_dimension = embedding_dimension\n        self.title_lstm = torch.nn.LSTM(embedding_dimension, lstm_hidden_dimension, num_lstm_layers,\n                                  batch_first=True)\n        self.body_lstm = torch.nn.LSTM(embedding_dimension, lstm_hidden_dimension, num_lstm_layers,\n                                  batch_first=True)\n        self.dropout = torch.nn.Dropout(dropoutProportion)\n        self.linear1 = torch.nn.Linear(2 * lstm_hidden_dimension, number_of_classes) #mlp_hidden_layer_dimension)\n        self.linear2 = torch.nn.Linear(mlp_hidden_layer_dimension, number_of_classes)\n        \n    def forward(self, inputTsr):\n        # inputTsr[0].shape = (N, title_maximum_length, embedding_dimension)\n        # inputTsr[1].shape = (N, body_maximum_length, embedding_dimension)\n        title_embedding = inputTsr[0]\n        body_embedding = inputTsr[1]\n        title_aggregated_h, (title_ht, title_ct) = self.title_lstm(title_embedding)\n        # title_ht.shape = (num_lstm_layers, N, lstm_hidden_dimension)\n        # title_ht[-1].shape = (N, lstm_hidden_dimension)\n        body_aggregated_h, (body_ht, body_ct) = self.body_lstm(body_embedding)\n        # body_ht.shape = (num_lstm_layers, N, lstm_hidden_dimension)\n        # body_ht[-1].shape = (N, lstm_hidden_dimension)\n        concatenated_latent_Tsr = torch.cat((title_ht[-1], body_ht[-1]), dim=1)\n        concatenated_latent_Tsr = self.dropout(concatenated_latent_Tsr)\n        outputTsr = self.linear1(concatenated_latent_Tsr)\n        \n        \"\"\"hidden_latent_Tsr = torch.nn.functional.relu(self.linear1(concatenated_latent_Tsr) )\n        hidden_latent_Tsr = self.dropout(hidden_latent_Tsr)\n        outputTsr = self.linear2(hidden_latent_Tsr)\n        \"\"\"\n\n        return outputTsr\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_hidden_dimension = 64\nlstm_number_of_layers = 1\nmlp_hidden_dimension = 1024\n\ndouble_lstm = Double_LSTM(\n    embedding_dimension=embedding_dimension, \n    lstm_hidden_dimension=lstm_hidden_dimension,\n    num_lstm_layers=lstm_number_of_layers, \n    mlp_hidden_layer_dimension=mlp_hidden_dimension,\n    number_of_classes=3,\n    dropoutProportion=0.5\n)\nif useCuda:\n    double_lstm = double_lstm.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n\nparameters = filter(lambda p: p.requires_grad, double_lstm.parameters())\noptimizer = torch.optim.Adam(parameters, lr=0.0003)\nlossFcn = torch.nn.CrossEntropyLoss()\ntrain_dataLoader = DataLoader(training_embeddings_to_class_dataset, batch_size=32, shuffle=True)\nvalidation_dataLoader = DataLoader(validation_embeddings_to_class_dataset, batch_size=32)\nbest_model_filepath = '/kaggle/working/double_lstm.pth'\n\nlowestValidationLoss = sys.float_info.max\nfor epoch in range(0, 15 + 1):\n    double_lstm.train()\n    loss_sum = 0.0\n    numberOfBatches = 0\n    if epoch > 0:\n        for (title_embedding_Tsr, body_embedding_Tsr), target_class_index in train_dataLoader:\n            if numberOfBatches % 4 == 1:\n                print (\".\", end=\"\", flush=True)\n            if useCuda:\n                title_embedding_Tsr = title_embedding_Tsr.cuda()\n                body_embedding_Tsr = body_embedding_Tsr.cuda()\n                target_class_index =  target_class_index.cuda()\n            predicted_index_Tsr = double_lstm((title_embedding_Tsr, body_embedding_Tsr))\n            optimizer.zero_grad()\n            loss = lossFcn(predicted_index_Tsr, target_class_index)\n            loss.backward()\n            optimizer.step()\n            loss_sum += loss.item()\n            numberOfBatches += 1\n        train_loss = loss_sum/numberOfBatches\n        print (\"\\nepoch {}: train_loss = {}\".format(epoch, train_loss))\n\n    # Validation\n    double_lstm.eval()\n    with torch.no_grad():\n        validation_loss_sum = 0\n        validation_correct_predictions = 0\n        number_of_validation_minibatches = 0\n        for (validation_title_embedding_Tsr, validation_body_embedding_Tsr), validation_target_class_index in validation_dataLoader:\n            if useCuda:\n                validation_title_embedding_Tsr = validation_title_embedding_Tsr.cuda()\n                validation_body_embedding_Tsr = validation_body_embedding_Tsr.cuda()\n                validation_target_class_index = validation_target_class_index.cuda()\n            validation_predicted_index_Tsr = double_lstm((validation_title_embedding_Tsr, validation_body_embedding_Tsr))\n            validation_loss = lossFcn(validation_predicted_index_Tsr, validation_target_class_index).item()\n            validation_loss_sum += validation_loss\n            validation_correct_predictions += (validation_predicted_index_Tsr.argmax(dim=1) == validation_target_class_index).sum().item()\n            number_of_validation_minibatches += 1\n        # Validation accuracy      \n        validation_accuracy = validation_correct_predictions / validation_embeddings_to_class_dataset.__len__()\n        validation_loss = validation_loss_sum/number_of_validation_minibatches\n        print (\"validation_loss = {}; validation_accuracy = {}\".format(validation_loss, validation_accuracy))\n\n    if validation_loss < lowestValidationLoss:\n        lowestValidationLoss = validation_loss\n        torch.save(double_lstm.state_dict(), best_model_filepath)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}