{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Práctica 2: Aprendizaje y selección de modelos de clasificación\n\n### Minería de Datos: Curso académico 2020-2021\n\n* Mª Mercedes Guijarro\n"},{"metadata":{},"cell_type":"markdown","source":"En esta práctica estudiaremos los modelos más utilizados en `scikit-learn` para conocer los distintos hiperparámetros que los configuran y estudiar los clasificadores resultantes. Además, veremos métodos de selección de modelos orientados a obtener una configuración óptima de hiperparámetros."},{"metadata":{},"cell_type":"markdown","source":"# 1. Preliminares"},{"metadata":{},"cell_type":"markdown","source":"En primer lugar, importamos todas las librerías necesarias:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Third party\nfrom sklearn.base import clone\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Local application\nimport miner_a_de_datos_aprendizaje_modelos_utilidad as utils\n\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Además, fijamos una semilla para que los experimentos sean reproducibles:"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state = 27912","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"# 2. Pima Indians Diabetes"},{"metadata":{},"cell_type":"markdown","source":"### 2.1. Carga de datos"},{"metadata":{},"cell_type":"markdown","source":"Como siempre, en primer lugar, cargamos el conjunto de datos y comprobamos que se haya realizado correctamente:"},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath = \"../input/pima-indians-diabetes-database/diabetes.csv\"\n\n\ndata = pd.read_csv(filepath, dtype={\"Outcome\": 'category'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comprobando que se ha cargado correctamente:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dividimos el conjunto en variables predictoras y la variable clase:"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = \"Outcome\"\n\n(X, y) = utils.divide_dataset(data, target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vamos a comprobar que se ha separado correctamente. Comenzamos con las variables predictoras:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Y continuamos con la variable clase:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A continuación, dividimos el conjunto de datos en entrenamiento y test, para de esta forma trabajar sobre el conjunto de entrenamiento y la validación sobre el test. Realizamos la división mediante un holdot estratificado."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size = 0.7\n\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=y,\n                                                      train_size=train_size,\n                                                      random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comprobamos que se ha realizado correctamente, tanto para el conjunto de entrenamiento como para el test:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Y una vez realizada la división pasamos a la creación de modelos para nuestro conjunto de datos."},{"metadata":{},"cell_type":"markdown","source":"### 2.2. Obtención de modelos"},{"metadata":{},"cell_type":"markdown","source":"Partiendo del preprocesamiento que realizamos en la práctica anterior, realizaremos la obtención y selección de modelos mediante un proceso de GridSearch, de esta forma obtendremos la mejor configuración de hiperparámetros para cada uno de ellos y así selección el clasificador que mejor se adapte a nuestro problema."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\n# Value Imputer\nimp = SimpleImputer(missing_values=0, strategy='mean')\n\n# Preprocessor\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('missing', imp ,[False, True, True, True, True, True, True, True]) \n    ], remainder=\"passthrough\")\n\n# Fitting the column transformer\npreprocessor = preprocessor.fit(X_train)\n\n#Las columnas a las que no se le aplica la transformación se colocan al final del DataFrame\nX_train = pd.DataFrame(preprocessor.transform(X_train), \n                                columns=X_train.columns[1:].append(X_train.columns[:1]))\n\n\nX_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.2.1. K-Nearest Neighbours (Vecinos más cercanos)"},{"metadata":{},"cell_type":"markdown","source":"Como hemos visto, la configuración de los hiperparámetros de este algoritmo son principalmente dos: `n_neighbours` y `weights`."},{"metadata":{},"cell_type":"markdown","source":"Inicializamos el clasificador:"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_neighbors = 5\n\nk_neighbors_model = KNeighborsClassifier(n_neighbors)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En el caso del parametro `weights` sólo acepta dos opciones `uniform` o `distance`."},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = ['uniform', 'distance']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Por otro lado, la elección del parámetro `n_neighbours`  depende mucho de la distribución de los datos. Una buena manera de seleccionar los valores podría ser fijarlo a $ \\sqrt{N} $ o probar los valores comprendidos entre 1 y $ \\sqrt{N} $ , pero al probar todas esas configuraciones corremos el riesgo de obtener un modelo sobreajustado, por lo que vamos a probar con `[1, 3, 5, 7, 10]`"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_neighbors = [1, 3, 5, 7, 10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Una vez establecidos los valores de los parámetros, utilizamos el algoritmo *GridSearch* para realizar una búsqueda, evaluando mediante validación cruzada todas las posibles combinaciones existentes y seleccionar la mejor."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = k_neighbors_model\n\nk_neighbors_clf = utils.optimize_params(estimator,\n                                        X_train, y_train, cv=5,\n                                        weights=weights,\n                                        n_neighbors=n_neighbors)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La mejor configuración de hiperparámtros que hemos obtenido es para `n_neighbours` 10 y `weights`  = uniform, es decir que todos los vecinos más cercanos tienen la misma importancia o el mismo peso.\nLa tasa de acierto obtenida es de $ 0.741 \\pm 0.031 $."},{"metadata":{},"cell_type":"markdown","source":"#### 2.2.2. Árboles de decisión"},{"metadata":{},"cell_type":"markdown","source":"Continuando con los árboles de decisión,si analizamos un poc sus hiperparámetros podemos ver que hay una gran parte de ellos que nos sirven para controlar el crecimiento del árbol de decisión.\nAhora, `scikit-learn` incluye un nuevo hiperparámetro que nos permite establecer un umbral a partir del cual se puede realizar una poda (`ccp_alpha`). Podemos obtener valores para este hiperparámetro utilizando el método `cost_complexity_pruning_path`, que nos devuelve todos los valores efectivos obtenidos durate el proceso de poda.\n"},{"metadata":{},"cell_type":"markdown","source":"Establecemos por defecto un árbol totalmente profundo:"},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree_model = DecisionTreeClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para los árboles de decisión vamos a tomar los siguientes parámetros:\n\n* Criterion: para medir la calidad de una partición, puede tomar los valores gini o entropy.\n* max_depth: altura máxima del árbol. Para este caso vamos a establecer los valores entre el 1 y el 5."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = clone(decision_tree_model)\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1, 2, 3, 4, 5]\nccp_alpha = [0.0, 0.01, 0.02, 0.03, 0.04]\n\ndecision_tree_clf = utils.optimize_params(estimator,\n                                          X_train, y_train, cv=5,\n                                          criterion=criterion,\n                                          max_depth=max_depth,\n                                          ccp_alpha=ccp_alpha)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como podemos observar los mejores hiperparámetros obtenidos son para ccp_alpha = 0.01, el criterio gini y para max_depth 4.\nLa tasa de acierto obtenida es de $ 0.755 \\pm 0.0485 $, no es mucho mejor que la obtenida con knn."},{"metadata":{},"cell_type":"markdown","source":"#### 2.2.2. AdaBoost (Adaptative Boosting)"},{"metadata":{},"cell_type":"markdown","source":"Para AdaBoost, implementado como `AdaBoostClassifier` en `scikit-learn`, podemos configurar los siguientes hiperparámetros:\n* `base_estimator`: es el estimador base que utilizaremos para la construcción del ensemble.\n* `n_estimators`: número de estimadores del ensemble.\n* `learning_rate`: coeficiente que se aplica a la *importancia* de los clasificadores a la hora de hacer la predicción.\n* `random_state`: semila para la reproducibilidad de los experimentos."},{"metadata":{},"cell_type":"markdown","source":"Primero configuramos un modelo AdaBoost sencillo que utilice los hiperparámetros por defecto:"},{"metadata":{"trusted":true},"cell_type":"code","source":"adaboost_model = AdaBoostClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Y ahora obtenemos la mejor configuración:"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = adaboost_model\n\n# Should not modify the base original model\nbase_estimator = clone(decision_tree_model)\n\nbase_estimator = [base_estimator]\nlearning_rate = [0.95, 1.0]\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1, 2]\nccp_alpha = [0.0, 0.01, 0.02]\n\nadaboost_clf = utils.optimize_params(estimator,\n                                     X_train, y_train, cv=5,\n                                     base_estimator=base_estimator,\n                                     learning_rate=learning_rate,\n                                     base_estimator__criterion=criterion,\n                                     base_estimator__max_depth=max_depth,\n                                     base_estimator__ccp_alpha=ccp_alpha)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La mejor configuración de hiperparámetros seleccionada para AdaBoost es:\n* `base_estimator`: 0.0\n* `criterion` : entropy\n* `learning_rate`: 1.0\n* `base_estimator__max_depth`: 1\n\nLa tasa de acierto obtenida es de $ 0.767 \\pm 0.024 $, mejora muy poco con respecto al árbol de decisión obtenido.\nObtenemos un árbol de profundidad 1, mucho sesgo pero con suficiente número de estimadores es capaz de explorar todo el espacio de búsqueda."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### 2.2.3. Bagging"},{"metadata":{},"cell_type":"markdown","source":"El siguiente algoritmo es *Bagging*, lo podemos encontrar en `scikit-learn` como `BaggingClassifier`.\nEn cuanto a los hiperparámetros algunos son similares a los utilizados en *AdaBoost*, como `base_estimator`, `n_estimators` o `random_state."},{"metadata":{},"cell_type":"markdown","source":"Al igual que el resto, vamos a configurar un ensemble tipo Bagging básico:"},{"metadata":{"trusted":true},"cell_type":"code","source":"bagging_model = BaggingClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Partiedo del árbol de decisión obtenido vamos a optimizar el criterio de partición:"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = bagging_model\n\nbase_estimator = clone(decision_tree_model)\n\nbase_estimator = [base_estimator]\ncriterion = [\"gini\", \"entropy\"]\n\nbagging_clf = utils.optimize_params(estimator,\n                                    X_train, y_train, cv=5,\n                                    base_estimator=base_estimator,\n                                    base_estimator__criterion=criterion)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como podemos ver, la mejor configuración de hiperparámetros para Bagging es usar la ganancia de información (criterion=\"entropy\") en los árboles de decisión.\nLa tasa de acierto obtenida es $ 0.751 \\pm 0.029 $, que no mejora con respecto al anterior."},{"metadata":{},"cell_type":"markdown","source":"#### 2.2.4. Random Forests"},{"metadata":{},"cell_type":"markdown","source":"Este algoritmo se trata de un meta-estimador, parecido a Bagging ya que también se busca reducir el error obtenido mediante varianza. Lo podemos encontrar en `scikit-learn` como `RandomForestClassifier`."},{"metadata":{},"cell_type":"markdown","source":"Al igual que el resto, vamos a configurar un ensemble tipo Random Forest básico:"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_model = RandomForestClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora vamos a optimizar para este algoritmo el criterio de partición (puede ser gini o entropy) y el número de características a considerar en cada nodo de los árboles de decisión (max_features).\n\nBuscando información sobre este algoritmo, se propone para el hiperparámetro max_features usar $ \\log_2{(N + 1)} $ características en cada paso de la construcción de cada árbol. En cualquier caso vamos a optar por las configuraciones que ofrece la librería por defecto: $ \\sqrt{N} $,   $ \\log_2{N} $ y `None`."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = random_forest_model\n\ncriterion = [\"gini\", \"entropy\"]\nmax_features = [\"sqrt\", \"log2\", None]\n\nrandom_forest_clf = utils.optimize_params(estimator,\n                                          X_train, y_train, cv=5,\n                                          criterion=criterion,\n                                          max_features=max_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La mejor configuración de hiperparámetros que obtenemos es para criterion gini y max_features = sqrt, teniendo una tasa de acierto de $ 0.775 \\pm 0.013 $, mejora un poco con respecto a la obtenida anteriormente."},{"metadata":{},"cell_type":"markdown","source":"#### 2.2.5. Gradient Boosting (Gradient Tree Boosting)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Este algoritmo es una generalización de los algoritmos de Boosting con capacidad de optimizar cualquier tipo de función perdida. Lo podemos encontrar en `scikit-learn` como `GradientBoostingClassifier`."},{"metadata":{},"cell_type":"markdown","source":"Vamos a configurar un estimador básico utilizando los hiperparámetros por defecto:"},{"metadata":{"trusted":true},"cell_type":"code","source":"gradient_boosting_model = GradientBoostingClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para este algoritmo vamos a optimizar el parámetro de regularización (learning_rate), el criterio de partición, la altura máxima y el parámetro de complejidad de la poda (ccp_alpha):"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = gradient_boosting_model\n\nlearning_rate = [0.01, 0.05, 0.1]\ncriterion = [\"friedman_mse\", \"mse\"]\nmax_depth = [1, 2, 3]\nccp_alpha = [0.0, 0.01, 0.02]\n\ngradient_boosting_clf = utils.optimize_params(estimator,\n                                              X_train, y_train, cv=5,\n                                              learning_rate=learning_rate,\n                                              criterion=criterion,\n                                              max_depth=max_depth,\n                                              ccp_alpha=ccp_alpha)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La mejor configuración de hiperparámetros que obtenemos es para criterion friedman_mse, ccp_alpha 0.0, learning_rate 0.05 y max_depth 3, teniendo una tasa de acierto de $ 0.77 \\pm 0.007 $, mejora un poco con respecto a la obtenida anteriormente."},{"metadata":{},"cell_type":"markdown","source":"#### 2.2.6. Histogram Gradient Boosting (Histogram-Based Gradient Boosting)"},{"metadata":{},"cell_type":"markdown","source":"Este algoritmo es una optimización de Gradent Boosting que discretiza el conjunto de datos de entrada para reducir el número de puntos de corte a considerar en la construcción de los árboles de decisión.\nEl algoritmo Histogram `Gradient Boosting` se implementa en la clase `HistGradientBoostingClassifier`."},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_gradient_boosting_model = HistGradientBoostingClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para finalizar, vamos a opimizar el parámetro de regularización y el número máximo de nodos hojas de los árboles dedecisión en este algoritmo:"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = hist_gradient_boosting_model\n\nlearning_rate = [0.01, 0.02, 0.03, 0.04, 0.05]\nmax_leaf_nodes = [10, 20, 40, 60, 80, 100]\n\nhist_gradient_boosting_clf = utils.optimize_params(estimator,\n                                                   X_train, y_train, cv=5,\n                                                   learning_rate=learning_rate,\n                                                   max_leaf_nodes=max_leaf_nodes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La mejor configuración de hiperparámetros que obtenemos es para learning_Rate 0.03 y max_leaf_nodes 10, teniendo una tasa de acierto de $ 0.771 \\pm 0.03 $."},{"metadata":{},"cell_type":"markdown","source":"### 2.3. Selección del modelo final"},{"metadata":{},"cell_type":"markdown","source":"Una vez obtenidos todos los modelos y sus mejores hiperparámetros para la partición de *train*, vamos a validar los resultados obtenidos y compararlos entre ellos según los resultados obtenidos por la partición de *test*."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf,\n    \"Decision tree\": decision_tree_clf,\n    \"AdaBoost\": adaboost_clf,\n    \"Bagging\": bagging_clf,\n    \"Random Forests\": random_forest_clf,\n    \"Gradient Boosting\": gradient_boosting_clf,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X_test\ny = y_test\n\nutils.evaluate_estimators(estimators, X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como podemos ver en los resultados obtenidos, los algoritmos no obtienen muy buena score al validar, en concreto para GradientBoosting y para RandomForests obtiene muy malos resultados.\nEl resto de algoritmos obtiene el mismo resultado de un 65%."},{"metadata":{},"cell_type":"markdown","source":"# 3. Wisconsin Breast Cancer"},{"metadata":{},"cell_type":"markdown","source":"### 3.1. Carga de datos"},{"metadata":{},"cell_type":"markdown","source":"Al igual que con el dataset anterior, realizamos la carga de datos."},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath = \"../input/breast-cancer-wisconsin-data/data.csv\"\n\nindexW = \"id\"\ntargetW = \"diagnosis\"\n\ndataW = utils.load_data(filepath, indexW, targetW)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En este dataset el único preprocesamiento que había que realizar era eliminar la última columna, la cual no correspondía a ninguna característica ni tenía valor, por lo que procedemos a eliminarla de todo el conjunto."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataW=dataW.drop(['Unnamed: 32'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comprobamos que ha sido eliminada correctamente:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataW.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Una vez ha sido eliminadaa, procedemos a la división del conjunto de datos:"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_w = \"diagnosis\"\n\n(X_w, y_w) = utils.divide_dataset(dataW, target_w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size = 0.7\n\n(X_train_w, X_test_w, y_train_w, y_test_w) = train_test_split(X_w, y_w,\n                                                      stratify=y_w,\n                                                      train_size=train_size,\n                                                      random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2. Obtención de modelos"},{"metadata":{},"cell_type":"markdown","source":"Al igual que para el anterior dataset, realizaremos los mismos pasos para obtener los mejores hiperparámetros para este problema:"},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.1. K-Nearest Neighbours (Vecinos más cercanos)"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_neighbors_w = 5\n\nk_neighbors_model_w = KNeighborsClassifier(n_neighbors_w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator_w = k_neighbors_model_w\n\nweights_w = [\"uniform\", \"distance\"]\nn_neighbors_w = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\nk_neighbors_clf_w = utils.optimize_params(estimator_w,\n                                        X_train_w, y_train_w, cv=5,\n                                        weights=weights_w,\n                                        n_neighbors=n_neighbors_w)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La mejor configuración de hiperparámetros que obtenemos n_neighbors 10 y weights distance, teniendo una tasa de acierto de $ 0.93 \\pm 0.006 $"},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.2. Árboles de decisión"},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree_model_w = DecisionTreeClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator_w = clone(decision_tree_model_w)\n\ncriterion_w = [\"gini\", \"entropy\"]\nmax_depth_w = [1, 2, 3, 4, 5]\nccp_alpha_w = [0.0, 0.01, 0.02, 0.03, 0.04]\n\ndecision_tree_clf_w = utils.optimize_params(estimator_w,\n                                          X_train_w, y_train_w, cv=5,\n                                          criterion=criterion_w,\n                                          max_depth=max_depth_w,\n                                          ccp_alpha=ccp_alpha_w)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La mejor configuración de hiperparámetros que obtenemos es para ccp_alpha 0.01, criterio entropy y max_depth 5, teniendo una tasa de acierto de $ 0.947 \\pm 0.03 $, que mejora un poco con respecto el algoritmo anterior."},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.2. AdaBoost (Adaptative Boosting)"},{"metadata":{"trusted":true},"cell_type":"code","source":"adaboost_model_w = AdaBoostClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator_w = adaboost_model_w\n\n# Should not modify the base original model\nbase_estimator_w = clone(decision_tree_model_w)\n\nbase_estimator_w = [base_estimator_w]\nlearning_rate_w = [0.95, 1.0]\ncriterion_w = [\"gini\", \"entropy\"]\nmax_depth_w = [1, 2]\nccp_alpha_w = [0.0, 0.01, 0.02]\n\nadaboost_clf_w = utils.optimize_params(estimator_w,\n                                     X_train_w, y_train_w, cv=5,\n                                     base_estimator=base_estimator_w,\n                                     learning_rate=learning_rate_w,\n                                     base_estimator__criterion=criterion_w,\n                                     base_estimator__max_depth=max_depth_w,\n                                     base_estimator__ccp_alpha=ccp_alpha_w)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como podemos ver la mejor configuración de hiperparámetros que obtenemos es para:\n* `base_estimator__ccp_alpha':0.0\n* `base_estimator__criterion`: entropy\n* `base_estimator__max_depth`: 2\n* `learning_rate`: 1.0\n\nteniendo una tasa de acierto de $ 0.98 \\pm 0.017 $, ha mejorado con respecto a los dos primeros."},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.3. Bagging"},{"metadata":{"trusted":true},"cell_type":"code","source":"bagging_model_w = BaggingClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator_w = bagging_model_w\n\nbase_estimator_w = clone(decision_tree_model_w)\n\nbase_estimator_w = [base_estimator_w]\ncriterion_w = [\"gini\", \"entropy\"]\n\nbagging_clf_w = utils.optimize_params(estimator_w,\n                                    X_train_w, y_train_w, cv=5,\n                                    base_estimator=base_estimator_w,\n                                    base_estimator__criterion=criterion_w)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como podemos ver, la mejor configuración de hiperparámetros para Bagging es criterion gini en los árboles de decisión.\nLa tasa de acierto obtenida es $ 0.952 \\pm 0.026 $, que no mejora con respecto al anterior."},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.4. Random Forests"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_model_w = RandomForestClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator_w = random_forest_model_w\n\ncriterion_w = [\"gini\", \"entropy\"]\nmax_features_w = [\"sqrt\", \"log2\", None]\n\nrandom_forest_clf_w = utils.optimize_params(estimator_w,\n                                          X_train_w, y_train_w, cv=5,\n                                          criterion=criterion_w,\n                                          max_features=max_features_w)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La mejor configuración de hiperparámetros es criterion gini y para max_features sqrt.\nLa tasa de acierto obtenida es $ 0.96 \\pm 0.015 $, muy similar a la obtenida con bagging."},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.5. Gradient Boosting (Gradient Tree Boosting)"},{"metadata":{"trusted":true},"cell_type":"code","source":"gradient_boosting_model_w = GradientBoostingClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator_w = gradient_boosting_model_w\n\nlearning_rate_w = [0.01, 0.05, 0.1]\ncriterion_w = [\"friedman_mse\", \"mse\"]\nmax_depth_w = [1, 2, 3]\nccp_alpha_w = [0.0, 0.01, 0.02]\n\ngradient_boosting_clf_w = utils.optimize_params(estimator_w,\n                                              X_train_w, y_train_w, cv=5,\n                                              learning_rate=learning_rate_w,\n                                              criterion=criterion_w,\n                                              max_depth=max_depth_w,\n                                              ccp_alpha=ccp_alpha_w)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La mejor configuración de hiperparámetros es ccp_alpha 0.0, criterion friedman_mse, learning_Rate 0.1 y max_depth 1.\nLa tasa de acierto obtenida es $ 0.96 \\pm 0.02 $, también muy similar a las dos anteriores."},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.6. Histogram Gradient Boosting (Histogram-Based Gradient Boosting)"},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_gradient_boosting_model_w = HistGradientBoostingClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator_w = hist_gradient_boosting_model_w\n\nlearning_rate_w = [0.01, 0.02, 0.03, 0.04, 0.05]\nmax_leaf_nodes_w = [10, 20, 40, 60, 80, 100]\n\nhist_gradient_boosting_clf_w = utils.optimize_params(estimator_w,\n                                                   X_train_w, y_train_w, cv=5,\n                                                   learning_rate=learning_rate_w,\n                                                   max_leaf_nodes=max_leaf_nodes_w)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La mejor configuración de hiperparámetros es para learning_rate 0.05 y max_leaf_nodes 20.\nLa tasa de acierto obtenida es $ 0.965 \\pm 0.025 $."},{"metadata":{},"cell_type":"markdown","source":"### 3.3. Selección del modelo final"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf_w,\n    \"Decision tree\": decision_tree_clf_w,\n    \"AdaBoost\": adaboost_clf_w,\n    \"Bagging\": bagging_clf_w,\n    \"Random Forests\": random_forest_clf_w,\n    \"Gradient Boosting\": gradient_boosting_clf_w,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf_w\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X_test_w\ny = y_test_w\n\nutils.evaluate_estimators(estimators, X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como podemos observar en los resultados obtenidos con los diferentes algoritmos, podemos concluir que en general se obtienen buenos resultado con todos, siendo AdaBoost y Random Forests los que mejores resultados obtienen."},{"metadata":{},"cell_type":"markdown","source":"# 4. Análisis de un kernel de Kaggle"},{"metadata":{},"cell_type":"markdown","source":"Navegando por los distintos conjuntos de datos de la plataforma, he seleccionado este [*kernel*](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python) para estudiarlo ya que está relacionado con lo realizado en la práctica."},{"metadata":{},"cell_type":"markdown","source":"Como se indica en la introducción del kernel es una básica y simple introducción a los ensembles, principalmente la variante que se conoce como Stacking, que se basa en las predicciones de otros clasificadores básicos y luego utiliza otro modelo para predecir los resultados.\nEl conjunto de datos empleado para esto es el del [Titanic ](https://www.kaggle.com/c/titanic).\n"},{"metadata":{},"cell_type":"markdown","source":"Lo primero que realiza es importar las librerías necesarias para llevar a cabo todo el proceso.\nY una vez realizado, procede a explorar el conjunto de datos. Para ello, primero hace la carga de datos, en esste caso en la carga ya separa el conjunto de entrenamiento y de test:\n* train = pd.read_csv('../input/train.csv')\n* test = pd.read_csv('../input/test.csv')\n\nLuego realiza un preprocesamiento del conjunto de datos en el que añade nuevas características y elimina otras, extrayendo la información que necesita."},{"metadata":{},"cell_type":"markdown","source":"El siguiente paso es visualizar el conjunto de datos en diferentes gráficas relacionando variables para extraer información.\nY en el primer gráfico podemos ver que las dos caracteríasticas que más relación tienen son family size y parch."},{"metadata":{},"cell_type":"markdown","source":"El siguiente paso es crear un stacking ensemble para ello se ayuda de la clase *SklearnHelper* que permite extener los métodos incorporados comunes a todos los clasficadores de Sklearn para el caso que quisieramos invocar diferentes clasificadores."},{"metadata":{},"cell_type":"markdown","source":"Después prepara cinco modelos de aprendizaje como clasificación de primer nivel, los utilizados son: Random Forest, Extra Trees, AdaBoost, Gradient Boosting y Support Vector Machine. Algunos han sido vistos en esta practica.\nUna vez preparados, realiza el entramiento y test con los 5 clasificadores.\nAhora que se han aprendido los clasificadores de primer nivel, lo que hace es utilizar una característica de los modelos de Sklear y muestra la importancia de las diferentes características, tanto en el conjunto de entrenamiento como el de test. Esto lo hace utilizando *.featureimportances.* . Y utilizando esto crea un dataframe con los datos de importancia para luego mostrarlos gráficamente y para cada clasificador.\nEsto lo utiliza para calcular la media de todas las características importantes y las almacena como una nueva columna en el dataframe y mostrarlas en un diagrama de barras."},{"metadata":{},"cell_type":"markdown","source":"Una vez obtenidas las predicciones del primer nivel, en el que ha construido como un nuevo conjunto de características que posteriormente se utilizarán como datos de entrenamiento para el siguiente clasificador.\nPara el segundo nivel utiliza XGBoost, es una biblioteca optimizada de Gradient Boosting que ha sido diseñada para ser muy eficiente, flexible y portátil e implemente algoritmos de aprendizaje automático bajo el framework de Gradient Boosting.\nPor lo que, llamad a un XGBClassifier y realiza el fit al primer nivel y a los datos del objetivo.\nFinalmente, después de haber entrenado y ajustado los modelos al primer y segundo nivel, ahora ya se pueden generar las predicciones."},{"metadata":{},"cell_type":"markdown","source":"Como extra, añade que para mejorar se podría implementar una buena estrategia de validación cruzada en el entrenamiento de los modelos para encontrar los valores óptimos, es decir, algo similar a lo que hemos realizado en esta práctica."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}