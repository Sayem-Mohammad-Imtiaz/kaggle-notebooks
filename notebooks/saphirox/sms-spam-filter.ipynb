{"cells":[{"metadata":{"_uuid":"a8668a3184b834d0c14736ea9ac5990d641459bd"},"cell_type":"markdown","source":"<h1>Description</h1>\n\nContext\n\nThe SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.\n\nContent\n\nThe files contain one message per line. Each line is composed by two columns: v1 contains the label (ham or spam) and v2 contains the raw text.\n\nThis corpus has been collected from free or free for research sources at the Internet:\n\n-> A collection of 425 SMS spam messages was manually extracted from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages. The Grumbletext Web site is: [Web Link]. -> A subset of 3,375 SMS randomly chosen ham messages of the NUS SMS Corpus (NSC), which is a dataset of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. The NUS SMS Corpus is avalaible at: [Web Link]. -> A list of 450 SMS ham messages collected from Caroline Tag's PhD Thesis available at [Web Link]. -> Finally, we have incorporated the SMS Spam Corpus v.0.1 Big. It has 1,002 SMS ham messages and 322 spam messages and it is public available at: [Web Link]. This corpus has been used in the following academic researches:\n\nAcknowledgements\n\nThe original dataset can be found here. The creators would like to note that in case you find the dataset useful, please make a reference to previous paper and the web page: http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ in your papers, research, etc.\n\nWe offer a comprehensive study of this corpus in the following paper. This work presents a number of statistics, studies and baseline results for several machine learning methods.\n\nAlmeida, T.A., GÃ³mez Hidalgo, J.M., Yamakami, A. Contributions to the Study of SMS Spam Filtering: New Collection and Results. Proceedings of the 2011 ACM Symposium on Document Engineering (DOCENG'11), Mountain View, CA, USA, 2011.\n\nInspiration\n\nCan you use this dataset to build a prediction model that will accurately classify which texts are spam?\n"},{"metadata":{"_uuid":"470a8e1d4130a0e34e7e76e8e4690831ec153035"},"cell_type":"markdown","source":"<h1>Import data</h1>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport plotly as py\nimport matplotlib.pyplot as plt\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport colorlover as cl\nimport operator\n%matplotlib inline\nimport string\nimport itertools\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.decomposition import PCA\nimport xgboost as xgb\nimport seaborn as sns\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom collections import Counter, OrderedDict\n\nimport os\nprint(os.listdir(\"../input\"))\n\nfrom IPython.display import HTML\n\nRANDOM_STATE = 43","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83ee61deda506d55022c8827ef74f6e2391ec907"},"cell_type":"markdown","source":"<h1>Exploration Data Analysis</h1>\n<h2>Data review</h2>"},{"metadata":{"trusted":true,"_uuid":"5c4060c50d918ebcd75b8920690a88cd2fe74c9b"},"cell_type":"code","source":"df = pd.read_csv(\"../input/spam.csv\",encoding='latin-1')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d393ade626ae41915e041fc09eec3fb0820e358"},"cell_type":"markdown","source":"Let's rename appropriate columns and drop unnessesary columns:"},{"metadata":{"trusted":true,"_uuid":"0ae5e0c1bf930d935a106c05ca05c93fec18c8f0"},"cell_type":"code","source":"df = df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\ndf = df.rename(columns= {\"v1\": \"label\", \"v2\": \"text\"})\ndf.label = df.label.astype('category') \ndf.text = df.text.astype('str')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6914d22a2f39551c6b66dca84d04ef718456025b"},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e058a125e1bd8531d3fe4cb26391b3bcdd9fbcd"},"cell_type":"code","source":"df.label.value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb5f771f0080a3479e880799b51971f8a340d2ac"},"cell_type":"markdown","source":"<h2>2. Visualization</h2>"},{"metadata":{"_uuid":"8993f00435e96946349f13f89d6fc6595b85d65b"},"cell_type":"markdown","source":"Let's visualize counts of objects, According to result we have that ham messages are more than span in 5 times, that's we need to do resampling or use stratified kfold"},{"metadata":{"trusted":true,"_uuid":"11364e59d2512704ce8073f3870b6eeb34f77b24"},"cell_type":"code","source":"_, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20, 5))\ndf.label.value_counts(sort=True).plot(kind='pie', ax=ax2, autopct='%1.0f%%')\ndf.label.value_counts(sort=True).plot(kind='bar', color=['blue', 'red'], ax=ax1)\nax1.set_title('Objects counts')\nax1.set_ylabel('Count')\nax1.set_xlabel('Label')\nax2.set_title('Objects counts')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54a628a1da60343dd907921ff1cb17cf7b540cb2"},"cell_type":"markdown","source":"We have missed phone numbers which was provived by sms message"},{"metadata":{"trusted":true,"_uuid":"0536c14eeb5020a8e18fa25533a18ea61c6fb8be"},"cell_type":"code","source":"def get_number_checker():\n    checker_func = np.vectorize(lambda x: re.search(\"[0-9]{10}\", x) != None or re.search(\"[0-9]{3}-[0-9]{3}-[0-9]{3}\", x) != None)\n    return df[checker_func(df.text)]\n\nget_number_checker().label.value_counts(sort=True).plot(kind=\"bar\")\nchecker_func = np.vectorize(lambda x: re.search(\"[0-9]{10}\", x) != None or re.search(\"[0-9]{3}-[0-9]{3}-[0-9]{3}\", x) != None)\ndf = df.assign(has_phone_number=checker_func(df.text))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a288dec329464f43d7c9596247d0a4b426810857"},"cell_type":"markdown","source":"<h1>3. Text Analitycs</h1>"},{"metadata":{"_uuid":"9679a79be2f0de3120114566c4d1edf23ee2865e"},"cell_type":"markdown","source":"Find most commin 100 word in text. Also we will clean punctuation and stop words using nltk library. Result divide on spam dataset and ham dataset. Next, we will visualize this data."},{"metadata":{"trusted":true,"_uuid":"6b4afeb66fb0a447e2d9a8022b78c89e14af776e"},"cell_type":"code","source":"MAX_COMMON_WORDS = 100\n\ndef clean_from_stop_words_and_punctuation(x):\n    return [word.lower() for word in x.split() if word.lower() not in stopwords.words('english') and  word.lower() not in string.punctuation]\n\ndef sort_dict_by_value(t):\n    return sorted(t, key=lambda x: x[1],reverse=True)\n\ndef get_word_arr(label):\n    clean_and_join = lambda x: \" \".join(clean_from_stop_words_and_punctuation(x))\n    cleaned_arr = df[df.label==label].text.apply(clean_and_join)\n    splitted_strings = cleaned_arr.apply(lambda word: word.split(\" \")).values\n    return list(itertools.chain.from_iterable(splitted_strings))\n\ndef get_counter_dict(label):\n    return sort_dict_by_value( Counter(get_word_arr(label)).most_common(MAX_COMMON_WORDS))\n\ncounter_ham = get_counter_dict('ham')\ncounter_spam = get_counter_dict('spam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a052bd46ef0bb36cc5f42dca707d2c32eada858"},"cell_type":"code","source":"spam_counter_df = pd.DataFrame.from_dict(counter_spam)\nspam_counter_df.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f265251aa68d700cf445ae26a86a4f42fcd7e44"},"cell_type":"code","source":"ham_counter_df = pd.DataFrame.from_dict(counter_ham)\nham_counter_df.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"acf8ebac62c6bd08727cf3d2b5383f099f39d3b6"},"cell_type":"code","source":"ham_plot = go.Bar(\n    x = ham_counter_df.iloc[:, 0],\n    y = ham_counter_df.iloc[:, 1],\n    name = \"Commom spam words\"\n)\n\niplot([ham_plot])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"994644e78605803755299300b234b60077ecfdde"},"cell_type":"code","source":"iplot([go.Bar(\n    x = spam_counter_df.iloc[:, 0],\n    y = spam_counter_df.iloc[:, 1],\n    marker = dict(\n        color=cl.scales['3']['div']['RdYlBu'][0]\n    )\n)])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e75f273dda2eca7e421efd4da3dd416dc914eef"},"cell_type":"markdown","source":"Let's dicover one more feature - length. Try find correlation between length and count of spam"},{"metadata":{"trusted":true,"_uuid":"a4483b8b942f414fc043a4598b6e5b55e3a180c1"},"cell_type":"code","source":"len_df = df.assign(len=df.text.apply(lambda x: len(x)))\nlen_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4fc9b04289d0567a6a227721977342d16a0c69a7"},"cell_type":"markdown","source":"We saw length of the message doesnt depend on spam or ham"},{"metadata":{"trusted":true,"_uuid":"33a55bec988908f6d2261f50f5e2bf78763af37a"},"cell_type":"code","source":"_, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(25, 5))\nax = sns.countplot(data=len_df.sort_values(['len'], ascending=False).sample(200), x='len', hue='label', ax=ax1,dodge=True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43f7b01260baa9a6c58fdfdd44255766be937e4d"},"cell_type":"markdown","source":"<h1>Make predictions</h1>"},{"metadata":{"_uuid":"90f8184e04fbbff8fc144eaf127e2f83e3556fdf"},"cell_type":"markdown","source":"Let's try Null model "},{"metadata":{"trusted":true,"_uuid":"9b027460b8a7296ed72f5c536fe873783f652d8d"},"cell_type":"code","source":"dtc = GridSearchCV(DecisionTreeClassifier(), param_grid = { 'criterion': ['gini', 'entropy'] }, cv=5)\ndtc.fit(np.zeros((df.shape[0],1)), df.label).best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9fdd7c5c5a05cb342f730e320861712d1e5fae1","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"vectorizer = CountVectorizer()\n\nX = vectorizer.fit_transform(df.text)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2aef3183c2823eb9509caa11dcfee7e5ebc9d0c9"},"cell_type":"markdown","source":"Make some transormation to boolean type:"},{"metadata":{"trusted":true,"_uuid":"facc18636dd235a73865220e910572a3e4c2a6ac"},"cell_type":"code","source":"df.label = df.label.map({ 'spam': 1, 'ham': 0 })\ny = df.label","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1202205452699d8b149133333cbe0f7b10d55130"},"cell_type":"markdown","source":"Try visualize our text:"},{"metadata":{"trusted":true,"_uuid":"0a57623eb3e826ea6636057ac2d3671195e0452b"},"cell_type":"code","source":"pca = PCA(n_components=2).fit(X.toarray())\ndata2D = pca.transform(X.toarray())\nfig, ax = plt.subplots(figsize=(20, 15))\nax.set_title('Vectorize plot')\nax.set_ylabel('PC2')\nax.set_xlabel('PC1')\nax.legend(['HAM', 'SPAM'])\nsns.scatterplot(data2D[:,0], data2D[:,1], hue=df.label, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a370c7effe4acca69718d83426f2263699f6488"},"cell_type":"markdown","source":"Let's declare variables clasificators. First clasificator which we are trying to use is Bayesian classifier. Our task is to build binary classifier so we will use BernoulliNB first:\n"},{"metadata":{"trusted":true,"_uuid":"6d58a6784c2c42a8e50d856fa09c4939365c02d3"},"cell_type":"code","source":"bnb = GridSearchCV(BernoulliNB(),{ 'alpha':range(100),}, cv=StratifiedKFold(n_splits=5), refit=True)\ncross_val_score(bnb, X, y, cv=5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"124cc693fe6f0b66fbd2ff315af5deaa7ad52b15"},"cell_type":"markdown","source":"Bernoulli confusion matrix:"},{"metadata":{"trusted":true,"_uuid":"b88965c760e9ad0ea3e6ef0212e907153e4e8cac"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)\nfit = bnb.fit(X_train,y_train)\nprint(classification_report(y_test, fit.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ab379f53c1989fe2e86b42db5dad3b024421429"},"cell_type":"code","source":"mnb = GridSearchCV(MultinomialNB(),{ 'alpha':range(100),}, cv=StratifiedKFold(n_splits=5), refit=True)\nfit = mnb.fit(X_train,y_train)\nprint(classification_report(y_test, fit.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f10e3a3de61c128a163780ce70754c48dd64e254"},"cell_type":"code","source":"gnb = GaussianNB()\nfit = gnb.fit(X_train.toarray(), y_train)\nprint(classification_report(y_test, fit.predict(X_test.toarray())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a46765cae43f0c8e680de876046a4e644e5d82cb"},"cell_type":"markdown","source":"Naive Bayes algorithms give quite good results. "},{"metadata":{"_uuid":"d31fa9a956aa3bbda55f381a006b58417b7f5239"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"8f33d88861721a445fad7770a49d25a3766d25c4"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}