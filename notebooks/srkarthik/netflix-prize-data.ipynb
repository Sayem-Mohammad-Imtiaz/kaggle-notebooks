{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"#importing required libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # graph plots to understand data","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# read the first data file as csv using pandas\ndf1 = pd.read_csv('../input/netflix-prize-data/combined_data_1.txt', header = None, names = ['Customer_Id', 'Rating'], usecols = [0,1])\n# cast rating column to float type\ndf1['Rating'] = df1['Rating'].astype(float)\n\nprint('Dataset 1 shape: {}'.format(df1.shape))\nprint('-Dataset examples-')\nprint(df1.iloc[::5000000, :])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# read other data files as csv and append data together\n\n# commented as my machine lags even for 1 file\n#df2 = pd.read_csv('../input/netflix-prize-data/combined_data_2.txt', header = None, names = ['Customer_Id', 'Rating'], usecols = [0,1])\n#df3 = pd.read_csv('../input/netflix-prize-data/combined_data_3.txt', header = None, names = ['Customer_Id', 'Rating'], usecols = [0,1])\n#df4 = pd.read_csv('../input/netflix-prize-data/combined_data_4.txt', header = None, names = ['Customer_Id', 'Rating'], usecols = [0,1])\n\n\n#df2['Rating'] = df2['Rating'].astype(float)\n#df3['Rating'] = df3['Rating'].astype(float)\n#df4['Rating'] = df4['Rating'].astype(float)\n\n#print('Dataset 2 shape: {}'.format(df2.shape))\n#print('Dataset 3 shape: {}'.format(df3.shape))\n#print('Dataset 4 shape: {}'.format(df4.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# combine all data\ndf = df1\n#df = df1.append(df2)\n#df = df.append(df3)\n#df = df.append(df4)\n\ndf.index = np.arange(0,len(df))\nprint('Full dataset shape: {}'.format(df.shape))\nprint('-Dataset examples-')\nprint(df.iloc[::5000000, :])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# group by rating column with aggregation function count\np = df.groupby('Rating')['Rating'].agg(['count'])\nprint(p.head())\n# get movie count\nmovie_count = df.isnull().sum()[1]\n\n# get customer count\ncust_count = df['Customer_Id'].nunique() - movie_count\n\n# get rating count\nrating_count = df['Customer_Id'].count() - movie_count\n\nax = p.plot(kind = 'barh', legend = False, figsize = (15,10))\nplt.title('Total pool: {:,} Movies, {:,} customers, {:,} ratings given'.format(movie_count, cust_count, rating_count), fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create a data frame with index of movie id rows\ndf_nan = pd.DataFrame(pd.isnull(df.Rating))\ndf_nan = df_nan[df_nan['Rating'] == True]\ndf_nan = df_nan.reset_index()\nprint(df_nan.head())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# scratch line just for understanding creating the required iteration conditions\ntemp = np.full((1,5), 1)\nprint(temp)\nmovie_np = []\nmovie_id =1 \nfor i,j in zip(df_nan['index'][1:],df_nan['index'][:2]):\n    print(i)\n    print(j)\n    # numpy approach\n    temp = np.full((1,i-j-1), movie_id)\n    print(temp)\n    movie_np = np.append(movie_np, temp)\n    print(len(temp[0]))\n    movie_id += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create a new array and fill movie id as new column\nmovie_np = []\nmovie_id = 1\n\nfor i,j in zip(df_nan['index'][1:],df_nan['index'][:-1]):\n    # numpy approach\n    temp = np.full((1,i-j-1), movie_id)\n    movie_np = np.append(movie_np, temp)\n    movie_id += 1\n\n# Account for last record and corresponding length\n# numpy approach\nlast_record = np.full((1,len(df) - df_nan.iloc[-1, 0] - 1),movie_id)\nmovie_np = np.append(movie_np, last_record)\n\nprint('Movie numpy: {}'.format(movie_np))\nprint('Length: {}'.format(len(movie_np)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# remove those Movie ID rows\ndf = df[pd.notnull(df['Rating'])]\n\ndf['Movie_Id'] = movie_np.astype(int)\ndf['Customer_Id'] = df['Customer_Id'].astype(int)\nprint('-Dataset examples-')\nprint(df.iloc[::5000000, :])\nprint(df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\"\"\"\nwe can eliminate movies that have very less number of ratings which is obviously not a good movie and \nalso those customers who vote very less number of times as these accounts can be considered dormant accounts \n\"\"\"\nf = ['count','mean']\n#  setting a benchmark by creating list of  movie id whose rating quantile is 0.7\ndf_movie_summary = df.groupby('Movie_Id')['Rating'].agg(f)\nprint(df_movie_summary.head())\ndf_movie_summary.index = df_movie_summary.index.map(int)\nprint(df_movie_summary.index.map(int))\nmovie_benchmark = round(df_movie_summary['count'].quantile(0.7),0)\ndrop_movie_list = df_movie_summary[df_movie_summary['count'] < movie_benchmark].index\nprint(drop_movie_list)\nprint('Movie minimum times of review: {}'.format(movie_benchmark))\n#  setting a benchmark by creating list of  customer id whose rating quantile is 0.7\ndf_cust_summary = df.groupby('Customer_Id')['Rating'].agg(f)\nprint(df_cust_summary.head())\ndf_cust_summary.index = df_cust_summary.index.map(int)\nprint(df_cust_summary.index.map(int))\ncust_benchmark = round(df_cust_summary['count'].quantile(0.7),0)\ndrop_cust_list = df_cust_summary[df_cust_summary['count'] < cust_benchmark].index\nprint(drop_cust_list)\nprint('Customer minimum times of review: {}'.format(cust_benchmark))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# remove the lower reviews from actual data\nprint('Original Shape: {}'.format(df.shape))\ndf = df[~df['Movie_Id'].isin(drop_movie_list)]\ndf = df[~df['Customer_Id'].isin(drop_cust_list)]\nprint('After Trim Shape: {}'.format(df.shape))\nprint('-Data Examples-')\nprint(df.iloc[::5000000, :])\nprint(df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# read the movie list csv file\ndf_title = pd.read_csv('../input/netflix-prize-data/movie_titles.csv', encoding = \"ISO-8859-1\", header = None, names = ['Movie_Id', 'Year', 'Name'])\ndf_title.set_index('Movie_Id', inplace = True)\nprint (df_title.head(10))\nprint(df_title.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# piovt the data since most algorithms need data as sparse matrix, this will be used in Pearson's R correlation method, and SVD\n# pivoting only first 100000 rows due to system memory constraints\ndf_p = pd.pivot_table(df[:100000],values='Rating',index='Customer_Id',columns='Movie_Id')\n\nprint(df_p.shape)\nprint(df_p.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\"\"\"\nto predict the top 10 movies realted to a particular movie so that we can recommend movies based on movie that has been \nwatched by the customer. for this we are using pearson's R correlation method\n\nin pearson's R  correlation method is to find the streangth of association between any two variables\n\"\"\"\ndef recommend(movie_title, min_count):\n    # get movie name and minimum count of rating\n    print(\"For movie ({})\".format(movie_title))\n    print(\"- Top 10 movies recommended based on Pearson's R correlation - \")\n    # get the movie id from movie list data frame and compare it to the pivot table that we have constructed in previous step\n    i = int(df_title.index[df_title['Name'] == movie_title][0])\n    # get the row from pivot table for the watched movie\n    target = df_p[i]\n    # to find the similar movies correlate the target dataframe with the pivoted data frame\n    similar_to_target = df_p.corrwith(target)\n    print(similar_to_target.head())\n    # create correlated suggestion list as new data from with pearsonR as the column name for the correlation value\n    corr_target = pd.DataFrame(similar_to_target, columns = ['PearsonR'])\n    # remove all null values from the data frame\n    corr_target.dropna(inplace = True)\n    # sort the data frame in descending order to get the most recommended movie to the top of the list \n    corr_target = corr_target.sort_values('PearsonR', ascending = False)\n    corr_target.index = corr_target.index.map(int)\n    \"\"\"\n    join the correlation target data frame wiht movie list and benchmarked movie summary dataframe to get movie name and \n    count and mean of rating for each of them\n    \"\"\"\n    corr_target = corr_target.join(df_title).join(df_movie_summary)[['PearsonR', 'Name', 'count', 'mean']]\n    # filter the top ten rows with count greater than minimum count to get hte top 10 recommended movie list\n    print(corr_target[corr_target['count']>min_count][:10].to_string(index=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# invoke recommend function to get the top 10 recommended movie list\nrecommend(\"Character\", 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# surprise package is not available by default in anaconda so to install execute below \n#conda install -c conda-forge scikit-surprise","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Import required libraries\nimport math\nimport re\nimport matplotlib.pyplot as plt\n\nfrom surprise import Reader, Dataset, SVD\nfrom surprise.model_selection import cross_validate","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# SVD requires dataset as reader object\n# Load Reader library\nreader = Reader()\n\n# get just top 100K rows for faster run time\ndata = Dataset.load_from_df(df[['Customer_Id', 'Movie_Id', 'Rating']][:100000], reader)\n\n# Use the SVD algorithm.\nsvd = SVD()\n\n# Compute the RMSE of the SVD algorithm\n\"\"\"\nmost ML algorithms require RMSE and MAE value which are Root Mean Square Error and Mean Absolute Error. In simple terms the ML \nalgorithms need effective error value and actual error value\n\"\"\"\ncross_validate(svd, data, measures=['RMSE', 'MAE'], cv=10, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#To find all the movies rated as 5 stars by user with userId = 712664\ndataset_712664 = df[(df['Customer_Id'] == 712664) & (df['Rating'] == 5)]\ndataset_712664 = dataset_712664.set_index('Movie_Id')\ndataset_712664 = dataset_712664.join(df_title)['Name']\ndataset_712664.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Train an SVD to predict ratings for user with userId = 1\n# Create a shallow copy for the movies dataset\nuser_712664 = df_title.copy()\n\nuser_712664 = user_712664.reset_index()\n\n#To remove all the movies rated less often \nuser_712664 = user_712664[~user_712664['Movie_Id'].isin(drop_movie_list)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n# getting full dataset\ndata = Dataset.load_from_df(df[['Customer_Id', 'Movie_Id', 'Rating']], reader)\n\n#create a training set for svd\ntrainset = data.build_full_trainset()\nsvd.fit(trainset)\n\n#Predict the ratings for user_712664\nuser_712664['Estimate_Score'] = user_712664['Movie_Id'].apply(lambda x: svd.predict(712664, x).est)\n\n#Drop extra columns from the user_712664 data frame\nuser_712664 = user_712664.drop('Movie_Id', axis = 1)\n\n# Sort predicted ratings for user_712664 in descending order\nuser_712664 = user_712664.sort_values('Estimate_Score', ascending=False)\n\n#Print top 10 recommendations\nprint(user_712664.head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}