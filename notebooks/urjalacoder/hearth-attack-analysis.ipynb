{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Heart attack prediction\n\nThis analysis explores the problem of predicting heart attacks using the provided dataset. The dataset contains various attributes about the person's health and a label attribute whether or not the person has been deamed to have a high risk of heart attack.\n\n\nThe different datapoints for each patient are:\n\n* Age (0=female, 1=male)\n* Gender\n* Chest pain type (categorized in 4 different types, 0=none)\n* Resting blood pressure\n* Serum cholestoral (unit mg/dl)\n* Fasting blood sugar is higher than 120 mg/dl\n* Resting ECG results (categorized in 3 types)\n* Max heart rate\n* Exercise induced angina (present or not)\n* ST depression induced by exercise with respect to rest (present or not)\n* Slope for the peak ST depression during exercise (1=upslope, 2=flat, 3=downslope)\n* Number of major vessels (0-3)\n* Thal (normal, fixed defect, reversible defect)\n* **Target value** high risk for heart attack (0 or 1)\n\n(The original dataset is collected from the [UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/heart+disease).)\n\n\n### Contents\n\n* [Initial exploration](#initial-exp)\n* [Research](#research)\n    - [Sex](#research-gender)\n    - [Max heart rate](#research-mhr)\n    - [Chest pain](#research-cp)\n    - [ST depression slope](#research-slope)\n    - [Major vessels](#research-vessels)\n* [Modelling](#model)\n    - [Clustering & K-Prototypes](#model-cluster)\n    - [Neural network](#model-net)\n* [Conclusion](#conclusion)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"initial-exp\"></a>\n# Initial exploration\n\nThe first thing is to explore the data and to visualize it. We can get more insight by plotting the distributions of various attributes in the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a map for converting abbretiations for the correct string\nABBR_MAP = {\n    'age': 'Age',\n    'sex': 'Gender',\n    'cp': 'Chest pain',\n    'trestbps': 'Resting BP',\n    'chol': 'Cholesterol',\n    'fbs': 'Fasting sugar > 120 mg/dl',\n    'restecg': 'Resting ECG type',\n    'thalach': 'Max heart rate',\n    'exang': 'Exercise induced angina',\n    'oldpeak': 'ST depression during exercise',\n    'slope': 'Slope for ST depression',\n    'ca': 'Number of major vessels',\n    'thal': 'Thal abnormalities'\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"raw_df = pd.read_csv('../input/health-care-data-set-on-heart-attack-possibility/heart.csv')\nattributes = list(raw_df.columns)\ninput_attributes = attributes[:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We first separate the dataset into 2 parts, those with low risk for HA and those with a high risk."},{"metadata":{"trusted":true},"cell_type":"code","source":"low_risk_df, high_risk_df = map(lambda x: x.reset_index().drop('index', axis=1), map(lambda x: x[1], raw_df.groupby(by='target')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We continue by plotting the distribution for the different attributes for the low and high risk datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, nrows=len(input_attributes), figsize=(16, 22))\nfig.tight_layout()\nfor i, attr in enumerate(input_attributes):\n    axes[i, 0].hist(low_risk_df[attr], bins=20, color='b')\n    axes[i, 0].set_title(f\"{ABBR_MAP[attr]}\")\n    axes[i, 1].hist(high_risk_df[attr], bins=20, color='orange')\n    axes[i, 1].set_title(f\"{ABBR_MAP[attr]}\")\n    \n    # Set the same x-axis limits for better comparison\n    x_max = max([axes[i, j].get_xlim() for j  in range(2)])\n    for j in range(2):\n        axes[i, j].set_xlim(x_max)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We note a couple of things from this initial examination:\n\n* The high risk portion has similar amount of men and women, but men are more prominent in the low risk portion. We should investigate the amount of men and women further to draw any conclusions from this observation.\n* Max heart rate is higher among high risk patients. This intuitively seems plausible.\n* Chest pain is more prominent among high risk patients. This also seems intuitively plausible.\n* Slope for ST depression seems to be downsloping on average among high risk patients. On the other hand, the slope is flat on average for low risk patients.\n* Number of major vessels is higher among low risk patients compared to high risk ones.\n\nWe will research these observations further by comparing the amount of people in different groups."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"research\"></a>\n# Research\n\nIn the first section we could draw some preliminary conclusions from the data. Nonetheless, we should confirm our suspicions by a more thorough analysis."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"research-gender\"></a>\n### Sex\n\nTo determine whether or not sex has a factor in determining the risk for heart attack. We can calculate the proportion of men and women in the high risk group when compared to the overall dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total amount of men and women\nfemale_count, male_count = map(len, map(lambda x: x[1], raw_df.groupby(by='sex')))\n# Amount of men and women in high risk group\nfemale_count_hr, male_count_hr = map(len, map(lambda x: x[1], high_risk_df.groupby(by='sex')))\n# Proportions\nfemale_proportion = female_count_hr / female_count\nmale_proportion = male_count_hr / male_count\nprint(f\"Male: {round(male_proportion, 2) * 100}%, female: {round(female_proportion, 2) * 100}%.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using this statistic, we can conclude that females tend to have a higher probability of being in the high risk group. Of course, this conclusion is based on the provided dataset, therefore we cannot draw any definitive conclusions."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"research-mhr\"></a>\n### Maximum heart rate\n\nNext, we'll determine whether the maximum heart rate is higher among high risk patients. We can do this by computing and plotting the quantiles (25%) from heart rate for both groups."},{"metadata":{"trusted":true},"cell_type":"code","source":"# .25 Quartiles\nquantiles = [i * 0.25 for i in range(1, 4)]\nlr_quartile, hr_quartile = (low_risk_df['thalach'].quantile(quantiles), high_risk_df['thalach'].quantile(quantiles))\nprint(lr_quartile)\nprint(hr_quartile)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(16, 9))\ntitles = [\"Low risk\", \"High risk\"]\ndfs = [low_risk_df, high_risk_df]\nfor i in range(2):\n    ax[i].boxplot(dfs[i]['thalach'].values, showfliers=False)\n    ax[i].set_title(titles[i])\n    ax[i].set_xticklabels(['Maximum heart rate'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see that the minimum, maximum and average values for the maximum achieved heart rate is higher among high risk patients.\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"research-cp\"></a>\n### Chest pain\n\nWe continue by determining whether or not chest pain is more prominent among high risk patients. We do this by calculating the prevalence of chest pain types in both groups."},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.colors import Colormap\nlow_risk_groups = low_risk_df['cp'].map({0:0, 1:1, 2:1, 3:1})\nhigh_risk_groups = high_risk_df['cp'].map({0:0, 1:1, 2:1, 3:1})\nlow_risk_neg, low_risk_pos = [low_risk_groups.value_counts(normalize=True)[i] for i in range(2)]\nhigh_risk_neg, high_risk_pos = [high_risk_groups.value_counts(normalize=True)[i] for i in range(2)]\n\nprint(low_risk_pos)\nprint(high_risk_pos)\n\n# Calculate the increase in percentages\npercentage_more = (high_risk_pos - low_risk_pos) / (low_risk_pos)\npercentage_more","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Therefore we conclude that patients in the high risk group do exhibit chest pain significantly more often than low risk patients.\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"research-slope\"></a>\n### ST Depression slope\n\nWe now take a look at the different slopes for ST depression. We start by calculating the prevalence of flat and downwards slopes in each group. Because upwards tending slopes seem to be mostly absent from both groups, we ignore them in this deduction."},{"metadata":{"trusted":true},"cell_type":"code","source":"low_risk_slopes = low_risk_df['slope'].map({0: np.nan, 1:1, 2:2}).value_counts(normalize=True)\nhigh_risk_slopes = high_risk_df['slope'].map({0: np.nan, 1:1, 2:2}).value_counts(normalize=True)\n\n# Different percentages\nprint(low_risk_slopes[2])\nprint(high_risk_slopes[2])\n\n# Calculate increase in proportions\npercentage_more_slope = (high_risk_slopes[2] - low_risk_slopes[2]) / (low_risk_slopes[2])\npercentage_more_slope","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Again, we find our initial observation holds water under more closer examination. The prevalence of downwards slopes in the high risk group is around 1.5x more than in the low risk group."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"research-vessels\"></a>\n### Major vessels\n\nFinally, we'll examine the last observation we made from the distribution plots, namely whether or not the number of major vessels in higher in the low risk group. To do this, we calculate the mean number of major vessels found in both groups and compare them. We also should investigate the standard deviation to get more insight into this datapoint."},{"metadata":{"trusted":true},"cell_type":"code","source":"low_risk_mean_vessels, low_risk_vessel_std = low_risk_df['ca'].mean(), low_risk_df['ca'].std()\nhigh_risk_mean_vessels, high_risk_vessel_std = high_risk_df['ca'].mean(), high_risk_df['ca'].std()\n\nprint(f\"Medians\")\nprint(f\"Low: {round(low_risk_mean_vessels, 2)}\\nHigh: {round(high_risk_mean_vessels, 2)}\")\npercentage_fewer_ca = 1 - high_risk_mean_vessels / low_risk_mean_vessels\nprint(f\"{round(percentage_fewer_ca, 2) * 100} percent fewer major vessels on average in the high risk group.\")\nprint(f\"Standard deviations\")\nprint(f\"Low: {round(low_risk_vessel_std, 2)}\\nHigh: {round(high_risk_vessel_std, 2)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From these values we can clearly see that patients in the low risk group tend to have a larger amount of major vessels. As the standard deviations are very close to each other, we can conclude that the amount of major vessels in each group indicate a difference in the patients' health rather than an anomaly in the dataset."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"model\"></a>\n# Modelling\n\nNow that we have identified some possible characteristics for separating the two groups from each other, we can begin on modelling.\nWe should from here on only include the 5 promising attributes in our analysis in addition to the flag attribute."},{"metadata":{"trusted":true},"cell_type":"code","source":"considered_input_attributes = ['sex', 'cp', 'slope', 'ca', 'thalach']\nraw_input_data = raw_df[considered_input_attributes].values\nraw_input_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"model-cluster\"></a>\n### Clustering & K-Prototype\n\nWe cannot use the traditional K-means clustering as our dataset contains mainly categorical attributes (4 out of 5). Therefore, we have to use something else.\n\nLucklily, there exists a proposed method that is similar to K-means but works with categorical and numerical data, namely **K-Prototype clustering**."},{"metadata":{"trusted":true},"cell_type":"code","source":"from kmodes.kprototypes import KPrototypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"km_model = KPrototypes(n_clusters=2)\n# Remove the only continuous variable:\ncategorical_data = raw_df[considered_input_attributes]\nfit_model = km_model.fit(categorical_data, categorical=[0, 1, 2, 3])\n\nclusters = km_model.predict(categorical_data, categorical=[0, 1, 2, 3])\n\n# Try if the clusters map to the different groups.\npredicted_df = raw_df[considered_input_attributes]\npredicted_df['prediction'] = clusters\npredicted_df['target'] = raw_df['target']\npredicted_df[['prediction', 'target']]\n\n\npredicted_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# As we don't know which group is which, we btry both mappings\n# First the direct map 0 -> 0, 1 -> 1:\ncorrect_count = predicted_df.apply(lambda x: 1 if x['target'] == x['prediction'] else 0, axis=1).value_counts()[1]\nprint(correct_count)\n\n# Try with inverse mapping: 0 -> 1, 1 -> 0\ncorrect_count_inverse = predicted_df.apply(lambda x: 1 if x['target'] == int(not bool(x['prediction'])) else 0, axis=1).value_counts()[1]\nprint(correct_count_inverse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice that the inverse mapping does find some kind of an association between the different groups and the input features. We could also try to use the same K-Prototypes algorithm but use **all** attributes, but still this approach does not seem to be bulletproof."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<a id=\"model-net\"></a>\n### Neural network\n\nWe can solve this problem of classifying the different labels very effectively just by utilizing the power of neural networks. We start by formatting the input data accordingly:\n\n* Map the target value from the range of [0, 1] to [-1, 1]. This can be achieved with a simple map over the dataframe.\n* Scale and center the numerical values to the range [0, 1].\n* Create a function for mapping the rows of the dataframe into tensors."},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_df = raw_df[considered_input_attributes].copy()\nnn_output_series = raw_df['target'].map({0: -1, 1: 1})\nnn_df['target'] = nn_output_series\nmin_t, max_t = nn_df['thalach'].min(), nn_df['thalach'].max()\nnn_df['thalach'] = nn_df['thalach'].apply(lambda x: (x - min_t) / (max_t - min_t))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we just have to create a function for turning a row in the dataframe into a tensor. Before that we calculate how many dimensions the tensor should have when taking the 1-hot encoding into account."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We should calculate the amount of dimensions required for tensors\ntotal_dims = list(map(lambda attr: len(nn_df[attr].unique()), considered_input_attributes[:-1]))\ntotal_dim_count = sum(total_dims) + 1\ntotal_dim_count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Therefore we should map each row into a tensor with 15 dimensions. For better data manipulation, we construct a Dataset class for mapping the rows of the dataframe into tensors."},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_attr = considered_input_attributes[:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class HeartSet(Dataset):\n    def __init__(self, raw_dataframe):\n        self.raw = raw_dataframe\n        self.output_values = self.raw['target'].values\n        self.input_values = self.raw[considered_input_attributes].values\n    \n    def __getitem__(self, idx):\n        running_index = list(map(lambda x: sum(total_dims[0:x]), [i for i in range(len(total_dims))]))\n        def map_to_tens(row):\n            input_tens = [0 for _ in range(14 + 1)]\n            for i, attr in enumerate(categorical_attr):\n                correct_index = row[attr] + running_index[i]\n                input_tens[int(correct_index)] = 1\n            input_tens[14] = row['thalach']\n            return (np.asarray(input_tens), row['target'])\n        values = self.raw.iloc[idx][categorical_attr + ['thalach', 'target']]\n        if isinstance(idx, slice):\n            input_res = values.apply(map_to_tens, axis=1)\n            input_arr_l = list(map(np.array, input_res))\n            input_arr = np.array(input_arr_l)\n        else:\n            input_res = map_to_tens(values)\n            input_arr_l = list(map(np.array, input_res))\n            input_arr = np.array(input_arr_l)\n        \n        # Now convert to tensors\n        return (input_arr[0], input_arr[1])\n    def __len__(self):\n        return self.raw.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = HeartSet(nn_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we'll use utility functions from PyTorch to split the dataset into training and testing sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size = 200\ntest_size = nn_df.shape[0] - train_size\ntrain_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check counts\ntrain_ones = 0\ntest_ones = 0\nfor i in range(train_size):\n    train_ones += 1 if train_dataset[i][1] == 1 else 0\nfor j in range(test_size):\n    test_ones += 1 if test_dataset[j][1] == 1 else 0\nprint(train_ones)\nprint(test_ones)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we'll define our network. It is a very simple network consisting of only 2 layers and it uses the *hyperbolic tangent* activation function."},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nN = total_dim_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ModelNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = nn.Linear(N, 2*N)\n        self.tanh1 = nn.Tanh()\n        self.linear2 = nn.Linear(2*N, 1)\n        self.tanh2 = nn.Tanh()\n        \n    def forward(self, input_tens):\n        x = self.linear1(input_tens)\n        x = self.tanh1(x)\n        x = self.linear2(x)\n        x = self.tanh2(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ModelNN()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will utilize mean squared error loss function and stochastic gradient descent for the optimizer."},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We train the network with 5 loops over the training dataset and with mini batches consisting of max 4 samples each."},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_points = []\nfor epoch in range(5):\n    running_loss = 0.0\n    for i, data in enumerate(train_loader):\n        inputs, labels = data\n        labels = labels.unsqueeze(dim=1).to(torch.float32)\n        inputs = inputs.to(torch.float32)\n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n    \n        with torch.no_grad():\n            loss_i = loss.item()\n            running_loss += loss_i\n    loss_points.append(running_loss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We plot the loss to see how our network learned."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1)\nax.plot(loss_points)\nax.set_ylim(ymin=0)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we'll **evaluate** our network using the test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = model.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correct = 0\nfor input, output in test_loader:\n    input = input.to(torch.float32)\n    prediction = model(input)\n    label_prediction = -1 if prediction < 0 else 1\n    correct = correct + int(output == label_prediction)\ncorrect / (len(test_loader))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see, we get around 80% classification rate when using neural networks. However, this **does depend on the exact partitioning** of the training and testing datasets, during experimentation the network could reach classification accuracies as high as 86% and as low as 76%."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"conclusion\"></a>\n# Conclusion\n\nAs we have seen, from the provided dataset we can find factors which influence whether or not a patient has a high risk of heart attack. We have also seen that a simple K-prototype modelling method does find some clusters in the data which correspond to the two different risk levels.\n\nAlso, we saw how even a very simple neural network can do a reasonably good job in classifying patients into the different groups."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}