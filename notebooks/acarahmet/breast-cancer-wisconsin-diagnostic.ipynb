{"cells":[{"metadata":{},"cell_type":"markdown","source":"# IN THIS NOTEBOOK WE ARE GOING TO PREDICT THE TYPE OF CANCER B/M\n1. [Getting and preparing the data](#1)\n    * [Basic Visualization](#2)\n    * [Outlier Detection](#3)\n    * [Train Test Split](#4)\n1. [BASIC KNN](#5) \n1. [PCA](#6)\n    * [Visualization](#7)\n    * [Find the Wrong Decision](#8)\n1. [Logistic Regression](#9)  \n1. [Hyperparameter Tuning](#10)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LETS FIRST IMPORT LIBRARIES"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split,StratifiedKFold,GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier,NeighborhoodComponentsAnalysis,LocalOutlierFactor\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"1\"></a>\n# Getting and preparing the Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/breast-cancer-wisconsin-data/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"diagnosis\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets change the \"diagnosis\" as \"type\" to understand better."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop([\"id\",\"Unnamed: 32\"],axis = 1,inplace = True)\ndata = data.rename(columns = {\"diagnosis\":\"type\"})\ndata[\"type\"] = [1 if i.strip() == \"M\" else 0 for i in data[\"type\"]]\ny = data[\"type\"]\n#data.drop([\"type\"],axis = 1,inplace = True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"2\"></a>\n> ### Lets visualize and see the numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data[\"type\"])\nprint(data[\"type\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Lets see the correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = data.corr()\nsns.clustermap(corr_matrix,annot = True,fmt = \".2f\",figsize = (15,15))\nplt.title(\"Correlation Between Features\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Above is the each correlation occur between features"},{"metadata":{},"cell_type":"markdown","source":"## Lets look at another map with a threshold(restriction)"},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = 0.5\nfilter1 = np.abs(corr_matrix[\"type\"] > threshold) # Features which have more than 0.75 correlation with 'type'\ncorr_features = corr_matrix.columns[filter1].tolist()\nsns.clustermap(data[corr_features].corr(),annot = True,fmt = \".2f\",figsize = (10,10))\nplt.title(\"Correlation Between Features With Threshold 0.75\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Box Plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"#First melt() the data\ndata_melted = pd.melt(data,id_vars = \"type\",var_name = \"features\",value_name = \"value\")\nplt.figure()\nsns.boxplot(x = \"features\",\n           y = \"value\",\n           hue = \"type\",\n           data = data_melted)\nplt.xticks(rotation = 90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Pair Plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data[corr_features],diag_kind = \"kde\",markers = \"+\",hue = \"type\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"3\"></a>\n# OUTLIER DETECTION"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data[\"type\"]\nx = data.drop([\"type\"],axis = 1)\ncolumns = x.columns.tolist()\n\nclf = LocalOutlierFactor()\ny_pred = clf.fit_predict(x)\nx_score = clf.negative_outlier_factor_\n\noutlier_score = pd.DataFrame()\noutlier_score[\"score\"] = x_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### -1's are the outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_score.sort_values(by = [\"score\"],ascending = True).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold2 = -2.5 \nfilter2 = outlier_score[\"score\"] < threshold2\noutlier_index = outlier_score[filter2].index.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.scatter(x.iloc[outlier_index,0],x.iloc[outlier_index,1],color = \"blue\",s = 50, label = \"Outliers\")\nplt.scatter(x.iloc[:,0],x.iloc[:,1],color = \"k\",s = 3, label = \"Data Points\")\nradius = (x_score.max() - x_score)/(x_score.max() - x_score.min())\nplt.scatter(x.iloc[:,0],x.iloc[:,1], s = 1000*radius, edgecolors = \"r\",facecolors = \"none\", label = \"Outlier Scores\")\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here we observe a one outlier which is not surprising since we set a -2.5 threshold and we already know there is only one point below this."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets drop the outliers\nx = x.drop(outlier_index)\ny = y.drop(outlier_index).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.abs(corr_matrix[\"type\"] > threshold) # Features which have more than 0.5 correlation with 'type'\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"4\"></a>\n## We split the data so that we can test it."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"y_TEST = y[450:]\ny = y[:450]\ndata_test = data.loc[450:,:]\nx = data.loc[:449,:]"},{"metadata":{},"cell_type":"markdown","source":"### We have created the test datas which we will use at the end."},{"metadata":{},"cell_type":"markdown","source":"## Now lets split the data with train_test_split method"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_size = 0.3\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = test_size,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# STANDARDIZATION \nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\nx_train_df = pd.DataFrame(x_train,columns = columns)\nx_train_df[\"type\"] = y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plot\ndata_melted = pd.melt(x_train_df,id_vars = \"type\",var_name = \"features\",value_name = \"value\")\nplt.figure()\nsns.boxplot(x = \"features\",y = \"value\",hue = \"type\",data = data_melted)\nplt.xticks(rotation = 90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pair plot\nsns.pairplot(x_train_df[corr_features].head(100),diag_kind = \"kde\",markers = \"+\",hue = \"type\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"5\"></a>\n## Basic KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 2)\nknn.fit(x_train,y_train)\ny_pred = knn.predict(x_test)\ncm = confusion_matrix(y_test,y_pred)\nacc = accuracy_score(y_test,y_pred)\nscore = knn.score(x_test,y_test)\nprint(\"Score\",score)\nprint(\"CM\",cm)\nprint(\"Basic KNN acc\",acc)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n[[108   1]\n [  7  55]]\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def knn_best_parameters(x_train,x_test,y_train,y_test):\n    k_range = list(range(1,31))\n    weight_options = [\"uniform\",\"distance\"]\n   \n    param_grid = dict(n_neighbors = k_range, weights = weight_options)\n    \n    knn = KNeighborsClassifier()\n    grid = GridSearchCV(knn,param_grid,cv = 10,scoring = \"accuracy\")\n    grid.fit(x_train,y_train)\n    print(\"Best training score: {} with parameters: {}\".format(grid.best_score_,grid.best_params_))\n    \n    \n    knn = KNeighborsClassifier(**grid.best_params_)\n    knn.fit(x_train,y_train)\n    \n    y_pred_test = knn.predict(x_test)\n    y_pred_train = knn.predict(x_train)\n    \n    cm_test = confusion_matrix(y_test, y_pred_test)\n    cm_train = confusion_matrix(y_train, y_pred_train)\n    \n    acc_test = accuracy_score(y_test,y_pred_test)\n    acc_train = accuracy_score(y_train,y_pred_train)\n    \n    print(\"Test Score: {}, Train Score: {}\".format(acc_test,acc_train))\n    print(\"CM Test : {}\".format(cm_test))\n    print(\"CM Train : {}\".format(cm_train))\n    \n    \n    return grid\n\ngrid = knn_best_parameters(x_train,x_test,y_train,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### > We found that the best parameters for training would be n_neighbors = 4, and the weights  = \"uniform\""},{"metadata":{},"cell_type":"markdown","source":"<a id = \"6\"></a>\n## PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nx_scaled = scaler.fit_transform(x)\n\npca = PCA(n_components = 2)\npca.fit(x_scaled)\nx_reduced_pca = pca.transform(x_scaled)\npca_data = pd.DataFrame(x_reduced_pca,columns = [\"p1\",\"p2\"])\npca_data[\"target\"] = y\n\nsns.scatterplot(x = \"p1\",y = \"p2\",hue = \"target\",data = pca_data)\nplt.title(\"PCA: p1 vs p2\")\n\nx_train_pca,x_test_pca,y_train_pca,y_test_pca = train_test_split(x_reduced_pca,y,test_size = test_size,random_state = 42)\ngrid_pca = knn_best_parameters(x_train_pca,x_test_pca,y_train_pca,y_test_pca)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"7\"></a>\n## Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"cmap_light = ListedColormap([\"orange\",\"cornflowerblue\"])\ncmap_bold = ListedColormap([\"darkorange\",\"darkblue\"])\n\nh = .05\nX = x_reduced_pca\nx_min,x_max = X[:,0].min() - 1,X[:,0].max() + 1\ny_min,y_max = X[:,1].min() - 1,X[:,1].max() + 1\nxx,yy = np.meshgrid(np.arange(x_min, y_max, h),\n                  np.arange(y_min, y_max, h))\nZ = grid_pca.predict(np.c_[xx.ravel(),yy.ravel()])\n\n# Put the results into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure()\nplt.pcolormesh(xx,yy,Z,cmap = cmap_light)\n\n# Plot also the training points \nplt.scatter(X[:, 0 ],X[:, 1], c= y,cmap = cmap_bold,\n           edgecolor = \"k\",s = 20)\nplt.xlim(xx.min(),xx.max())\nplt.ylim(yy.min(),yy.max())\nplt.title(\"%i-Class classification (k = %i, weights = '%s')\"\n         % (len(np.unique(y)),grid_pca.best_estimator_.n_neighbors,grid_pca.best_estimator_.weights))\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nca = NeighborhoodComponentsAnalysis(n_components = 2,random_state= 42)\nnca.fit(x_scaled,y) #Supervised learning which needs y \nx_reduced_nca = nca.transform(x_scaled)\nnca_data = pd.DataFrame(x_reduced_nca,columns = [\"p1\",\"p2\"])\nnca_data[\"target\"] = y\nsns.scatterplot(x = \"p1\", y = \"p2\", hue = \"target\", data = nca_data)\nplt.title(\"NCA: p1 vs p2\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_nca, x_test_nca,y_train_nca,y_test_nca = train_test_split(x_reduced_nca,y,test_size = test_size, random_state = 42)\n\ngrid_nca = knn_best_parameters(x_train_nca, x_test_nca,y_train_nca,y_test_nca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cmap_light = ListedColormap(['orange',  'cornflowerblue'])\ncmap_bold = ListedColormap(['darkorange', 'darkblue'])\n\nh = .4 # step size in the mesh\nX = x_reduced_nca\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = grid_nca.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure()\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n            edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"%i-Class classification (k = %i, weights = '%s')\"\n          % (len(np.unique(y)),grid_nca.best_estimator_.n_neighbors, grid_nca.best_estimator_.weights))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"8\"></a>\n## Find the wrong decision"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(**grid_nca.best_params_)\nknn.fit(x_train_nca,y_train_nca)\ny_pred_nca = knn.predict(x_test_nca)\nacc_test_nca = accuracy_score(y_pred_nca,y_test_nca)\nknn.score(x_test_nca,y_test_nca)\n\ntest_data = pd.DataFrame()\ntest_data[\"x_test_nca_p1\"] = x_test_nca[:,0]\ntest_data[\"x_test_nca_p2\"] = x_test_nca[:,1]\ntest_data[\"y_pred_nca\"] = y_pred_nca\ntest_data[\"y_test_nca\"] = y_test_nca\n\nplt.figure()\nsns.scatterplot(x = \"x_test_nca_p1\",y = \"x_test_nca_p2\",hue = \"y_test_nca\",data = test_data)\n\ndiff = np.where(y_pred_nca!=y_test_nca)[0]\nplt.scatter(test_data.iloc[diff,0],test_data.iloc[diff,1],label = \"Wrong Classified\",alpha = 0.2,color = \"red\",s = 1000)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"9\"></a>\n# Logistic Regression "},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(x_train,y_train)\nprint(\"Training Acc\",round(lr.score(x_train,y_train)*100,2))\nprint(\"Test acc\",round(lr.score(x_test,y_test)*100,2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"10\"></a>\n# HYPERPARAMETER TUNING \n* Decisiontree\n* SVM\n* Random Forest\n* KNN\n* Logistic Regression\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state = 42\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n             SVC(random_state = random_state),\n             RandomForestClassifier(random_state = random_state),\n             LogisticRegression(random_state = random_state),\n             KNeighborsClassifier()]\ndt_grid = {\"min_samples_split\":range(10,500,20),\n          \"max_depth\":range(1,20,2)}\n\nsvc_grid = {\"kernel\":[\"rbf\"],\n           \"gamma\":[0.001,0.01,0.1,1],\n           \"C\": [1,10,50,100,200,300,1000]}\n\nrf_grid = {\"max_features\":[1,3,10],\n          \"min_samples_split\":[2,3,10],\n          \"min_samples_leaf\":[1,3,10],\n          \"bootstrap\":[False],\n          \"n_estimators\":[100,300],\n          \"criterion\":[\"gini\"]}\n\nlr_grid = {\"C\":np.logspace(-3,3,7),\n          \"penalty\":[\"l1\",\"l2\"]}\n\nknn_grid = {\"n_neighbors\":np.linspace(1,19,10,dtype = int),\n           \"weights\":[\"uniform\",\"distance\"],\n           \"metric\":[\"euclidean\",\"manhattan\"]}\n\nclassifier_param = [dt_grid,\n                   svc_grid,\n                   rf_grid,\n                   lr_grid,\n                   knn_grid]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_results = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i],param_grid = classifier_param[i],cv = StratifiedKFold(n_splits = 10),\n                       scoring = \"accuracy\", n_jobs = -1,verbose = 1)\n    clf.fit(x_train,y_train)\n    cv_results.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(cv_results[i])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncv_results = pd.DataFrame({\"Cross Validation Means\":cv_results, \"ML Models\":[\"DecisionTreeClassifier\", \"SVM\",\"RandomForestClassifier\",\n             \"LogisticRegression\",\n             \"KNeighborsClassifier\"]})\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.barplot(\"Cross Validation Means\", \"ML Models\", data = cv_results)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}