{"cells":[{"metadata":{},"cell_type":"markdown","source":"___________________\n##### By.\n##### A h M e D _ H e f N a w Y\n___________________\n#### Boston House Price Regression problem!\n___________________","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### UnderStanding Problem Attributes!\n###### 01. CRIM: per capita crime rate by town\n###### 02. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n###### 03. INDUS: proportion of non-retail business acres per town\n###### 04. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n###### 05. NOX: nitric oxides concentration (parts per 10 million)\n###### 06. RM: average number of rooms per dwelling\n###### 07. AGE: proportion of owner-occupied units built prior to 1940\n###### 08. DIS: weighted distances to five Boston employment centers\n###### 09. RAD: index of accessibility to radial highways\n###### 10. TAX: full-value property-tax rate per $10,000\n\n###### 11. PTRATIO : pupil-teacher ratio by town\n\n###### 12. B: 1000(Bk − 0:63)2 where Bk is the proportion of blacks by town\n\n###### 13. LSTAT: % lower status of the population\n\n###### 14. MEDV: Median value of owner-occupied homes in $1000s","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(filename='F:\\Careers\\Machine Learning\\work shop\\Projects\\Boston House Price\\DataSet\\DataEx.JPG',width=800,height=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load libraries\nimport numpy\nfrom numpy import arange\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nfrom pandas import set_option\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Load DataSet","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load dataset\nfilename = '../input/boston-house-prices/housing.csv'\nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n'B', 'LSTAT', 'MEDV']\nDF = pd.read_csv(filename,delim_whitespace=True, names=names)\nDF","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Analyze Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"DF.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DF.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DF.head(21)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# descriptions\nDF.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">correlation between all of the numeric attributes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"DF.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" NOX and INDUS with 0.77.\n\n DIS and INDUS with -0.71.\n\n TAX and INDUS with 0.72.\n\n AGE and NOX with 0.73.\n\n DIS and NOX with -0.78.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Visualizations time!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> Unimodal Data Visualizations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"DF.hist(figsize=(20,20), bins=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CRIM: exponential distribution\n\nZN  : exponential distribution\n\nAGE : exponential distribution\n\nB   : exponential distribution  \n\nRAD : bimodal distribution \n\nTAX : bimodal distribution\n\nNOX :skewed Gaussian distributions\n\nRM :skewed Gaussian distributions\n\nLSTAT :skewed Gaussian distributions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# density chart\nDF.plot(kind='density',subplots=True, layout=(4,4), sharex=False, sharey=False, legend=False, fontsize=10, figsize=(20,20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# box and whisker plots\nDF.plot(kind='box', subplots=True, layout=(4,4), sharex=False, sharey=False,fontsize=10 , figsize=(20,20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Multimodal Data Visualizations!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# scatter plot matrix\nscatter_matrix(DF, figsize=(25,25),ax=None,grid=True,diagonal='hist',marker='*', range_padding=0.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not linear, but nice predictable curved relationships. :) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation matrix\nfig = plt.figure(figsize=(20,15))\nax = fig.add_subplot(111)\ncax = ax.matshow(DF.corr(), vmin=-1, vmax=1, interpolation='none')\nfig.colorbar(cax)\nticks = numpy.arange(0,14,1)\nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(names)\nax.set_yticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"_____________\n### Validation Step!!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There is a lot of structure in this dataset,, I need to transforms that will improve modeling accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split-out validation dataset 80% -- 20%\narray = DF.values\nX = array[:,0:13] # All features \nY = array[:,13] # target\nvalidation_size = 0.20 # validaion precentage to estimate accuracy \nseed = 7 #\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### now is the time for Evaluate Algorithms !!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test options and evaluation metric\nnum_folds = 10\nseed = 7\nscoring = 'neg_mean_squared_error'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Spot-Check Algorithms\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('SVR', SVR()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    print(\"{}:   {} ({}) \\n\".format(name, cv_results.mean(), cv_results.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare Algorithms bu visualizaion boxplt graph\nfig = plt.figure(figsize=(10,7),facecolor='grey')\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The differing scales of the data is probably hurting the skill of all of the algorithms\n\ni will unning the same algorithms using a standardized copy of the data\n### So, Evaluate Algorithms: Standardization TimE !!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardize the dataset\npipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LinearRegression())])))\npipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO',Lasso())])))\npipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN',\nElasticNet())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN',\nKNeighborsRegressor())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART',\nDecisionTreeRegressor())])))\npipelines.append(('ScaledSVR', Pipeline([('Scaler', StandardScaler()),('SVR', SVR())])))\nresults = []\nnames = []\nfor name, model in pipelines:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    print(\"{}:    {} ({}) \\n\".format(name, cv_results.mean(), cv_results.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare updated Algorithms\nfig = plt.figure(figsize=(10,7),facecolor='y')\nfig.suptitle('Scaled Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"KNN has both a tight distribution of error and has the lowest score.\n##### So ,it's time to Improve Results of KNN\n### Tuning Step!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"i will use a grid search to try a set of different numbers of neighbors and see if we can improve the score to try improve the accuracy of KNN algo.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN Algorithm tuning\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nk_values = numpy.array([1,3,5,7,9,11,13,15,17,19,21])\nparam_grid = dict(n_neighbors=k_values)\nmodel = KNeighborsRegressor()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> display the mean and standard deviation scores as well as the best performing value for k below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best: %f using %s\\n\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"i will try to improve the accuracy by Ensembles methods to ,perhaps it make any improvment","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# ensembles\nensembles = []\nensembles.append(('ScaledAB', Pipeline([('Scaler', StandardScaler()),('AB', AdaBoostRegressor())])))\nensembles.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())])))\nensembles.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestRegressor())])))\nensembles.append(('ScaledET', Pipeline([('Scaler', StandardScaler()),('ET', ExtraTreesRegressor())])))\nresults = []\nnames = []\nfor name, model in ensembles:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare Algorithms\nfig = plt.figure(figsize=(10,7),facecolor='grey')\nfig.suptitle('Scaled Ensemble Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> > > Tune Ensemble Methods\n- The default number of boosting stages to perform (n estimators) is 100\n- the larger the number of boosting stages, the better the performance but the longer the training time.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tune scaled GBM\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nparam_grid = dict(n_estimators=numpy.array([50,100,150,200,250,300,350,400]))\nmodel = GradientBoostingRegressor(random_state=seed)\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r \\n\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finalize Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare the model\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nmodel = GradientBoostingRegressor(random_state=seed, n_estimators=400)\nmodel.fit(rescaledX, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform the validation dataset\nrescaledValidationX = scaler.transform(X_validation)\npredictions = model.predict(rescaledValidationX)\nprint(mean_squared_error(Y_validation, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n----------------","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"End OF Boston House Price PROBLEM :) \n    trying to improve the accuracy as much i can\n    \n### AhMeD HefNawY","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}