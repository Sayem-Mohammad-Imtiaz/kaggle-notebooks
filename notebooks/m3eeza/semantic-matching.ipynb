{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Semantic Matching:\n# Word Embedding for Prioritizing Research Papers\n\n![CORD19](https://pages.semanticscholar.org/hs-fs/hubfs/covid-image.png?width=300&name=covid-image.png)\n\n## Introduction\nCOVID-19 Open Research Dataset (CORD-19) is a free dataset of academic papers, aggregated by a coalition of leading research groups, about COVID-19 and the coronavirus family of viruses. The dataset can be found on Semantic Scholar and there is a research challenge on Kaggle. This dataset is being given to the global research community to apply recent developments in natural language processing and other AI techniques to generate new insights in support of the ongoing battle against this infectious disease. These approaches are becoming increasingly urgent due to the rapid acceleration in the new literature on coronavirus, which makes it difficult for medical research community to keep up. In this notebook we try to prioterize research papers based on semantic search instead of classical search. methods\n\n## Approach\nIn this notebook, we use gensim's word2vec in order to generate word embeddings for the research papers' abstracts to use as our corpus. \n\n#### Semantic Matching\nWord embeddings represent words as d-dimensional dense vectors. The similarity or the distance between the vectors of words in the embedding space measure the relatedness between them. \n\n#### IWCS \nThe IDF re-weighted word centroid similarity (IWCS) model used word embeddings to construct a d-dimensional vector representing a passage (abstract in our case). In this model, the word vectors of the given text are aggregated into a single vector using a linear weighted combination of its word vectors.\nThe centroid vector of the query can also be computed in the same manner. Finally, to rank a text according to a query we use the cosine distnace between them.\n\nInspired from: [The Semantic Web: 16th International Conference](https://books.google.com.eg/books?id=PxaaDwAAQBAJ&dq=The+Semantic+Web:+16th+International+Conference,+ESWC&source=gbs_navlinks_s)\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Installing/Loading packagaes","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from IPython.utils import io\nwith io.capture_output() as captured:\n    !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\nimport spacy\nimport string\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nfrom pprint import pprint\nfrom IPython.utils import io\nfrom tqdm.notebook import tqdm\nfrom gensim.models import Word2Vec\nfrom IPython.core.display import HTML, display\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data loading and preprocessing\nWe consider the paper abstract only, but the approach could also be applied to the whole text body.\n\n","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"root_path = '/kaggle/input/CORD-19-research-challenge'\nmetadata_path = f'{root_path}/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str,\n    'abstract': str\n})\nmeta_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cord = pd.DataFrame(columns=['paper_id', 'title','abstract', 'doi'])\ndf_cord['paper_id'] = meta_df.sha\ndf_cord['title'] = meta_df.title\ndf_cord['abstract'] = meta_df.abstract\ndf_cord['doi'] = meta_df.doi\n\ndf_cord.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cord.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping null and duplicate values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cord.drop_duplicates(['abstract'], inplace=True)\ndf_cord.dropna(inplace=True)\ndf_cord.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text Tokenizing\nFor preprocessing we use scispaCy, which is a Python package containing spaCy models for processing biomedical, scientific or clinical text.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import en_core_sci_lg\nnlp = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])# disabling Named Entity Recognition for speed\nnlp.max_length = 3000000\ndef spacy_tokenizer(sentence):\n    return ' '.join([word.lemma_ for word in nlp(sentence) if not (word.like_num or word.is_stop or word.is_punct or word.is_space or len(word)==1)])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"customized_stop_words = [\n    'rights', 'reserved', 'permission', 'use', 'used', 'using', 'biorxiv', 'medrxiv', 'license',\n    'doi', 'preprint', 'copyright', 'org', 'https', 'et', 'al', 'author', 'figure', 'table',\n     'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI',\n    '-PRON-', 'usually'\n]\npunctuations = string.punctuation\nstopwords = list(STOP_WORDS)\n# Mark them as stop words\nstopwords.extend(customized_stop_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying the tokenizer on the abstarcts and creating a new column `tokenized_abstract`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tqdm.pandas()\ndf_cord[\"tokenized_abstract\"] = df_cord[\"abstract\"].progress_apply(spacy_tokenizer)\ndf_cord.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Corpus Sentencization\nWe apply spacy's sentencizer to split all the abstracts into separate sentences so we can use them as our word2vec corpus.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"abstracts = df_cord['abstract'].values\n\nnlp.add_pipe(nlp.create_pipe('sentencizer'), before=\"parser\")\nword2vec_corpus = []\n\nfor i in tqdm(range(0, len(abstracts))):\n    doc = nlp(abstracts[i])\n    word2vec_corpus.extend([spacy_tokenizer(sentence.string.strip()).split(\" \") for sentence in doc.sents])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2vec_corpus[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the Model (word2vec)\nWe will use gensim's word2vec and train it on the processed abstract.\n\n## Important parameters:\n* `min_count` Ignores all words with total absolute frequency lower than this - (2, 100)\n* `window` The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the left of our target - (2, 10)\n* `size` Dimensionality of the feature vectors. - (50, 300)\n* `workers` = Use these many worker threads to train the model (=faster training with multicore machines)\n* `sg`enable skipgram model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the gensim word2vec model with our corpus\nimport multiprocessing\ncores = multiprocessing.cpu_count() # Count the number of cores in a computer\nsize = 50\nmodel = Word2Vec(word2vec_corpus, min_count=5,size= size,workers=cores-1, window =5, sg = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Centroid Calculation\nwe will calculate the centroid for each abstract using the vectors of all the words in the abstract.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cord[\"centroid\"] = [[0.0]*size]*df_cord.shape[0]\nfor index, row in df_cord.iterrows():\n    abstract = row['tokenized_abstract']\n    centroid = np.array([0.0]*size)\n    for word in abstract.split(\" \"):\n        try:\n            word_vector = model[word]\n        except:\n            continue\n        centroid = np.add(centroid, word_vector)\n\n    df_cord.at[index,'centroid'] = centroid.tolist()\n\ndf_cord.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ranking research papers\nGiven a query, we compute its centroid and then determine the top `k` semantically similar papers to the query according to the cosine similarity score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_k_docs(model, query, df_cord, k) :\n    cosine_distance = []\n    \n    vectorized_query = []\n    for word in spacy_tokenizer(query).split(\" \"):\n        try:\n            vectorized_query.append(model[word])\n        except:\n            continue\n    \n    for _, row in df_cord.iterrows():\n        centroid = row['centroid']\n        total_simalirity = 0\n        for word_vec in vectorized_query:\n            word_simalirity = np.dot(word_vec, centroid)/(np.linalg.norm(word_vec)*np.linalg.norm(centroid))\n            total_simalirity += word_simalirity\n        cosine_distance.append((row['title'], row['doi'],row['abstract'], total_simalirity)) \n    \n    \n    cosine_distance.sort(key=lambda x:x[3], reverse=True) #Sort according to cosine simalirity in descending order\n    return cosine_distance[:k]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_top_k_docs(model=model,query='origin of coronavirus',df_cord=df_cord,k=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def search(search_query,n_docs=5):\n    html = \"\"\"\n        <html>\n            <body>\n                <ol>\n            \"\"\"\n    results = get_top_k_docs(model=model,query=search_query,df_cord=df_cord,k=n_docs)\n    for result in results:\n        paper_name = result[0]\n        paper_doi = result[1]\n        paper_abstract = result[2]\n        paper_link = \"https://doi.org/\" + str(paper_doi)\n        html += f\"\"\"            \n                <li id=\"result-1\">\n                    <article>\n                        <header>\n                            <a href=\"{paper_link}\">\n                                <h2>{paper_name}</h2>\n                            </a>\n                        </header>\n                        <p>{paper_abstract}</p>\n                    </article>\n                  </li>\n                \"\"\"\n    html += \"</body></html>\"\n    display(HTML(html))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What is known about transmission, incubation, and environmental stability?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"search('transmission and incubation of coronavirus')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What do we know about COVID-19 risk factors?\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"search('coronavirus risk factors')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What do we know about virus genetics, origin, and evolution?\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"search('genetics origin and evolution of coronavirus')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# What do we know about vaccines and therapeutics?\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"search('coronavirus vaccines and therapeutics')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What do we know about non-pharmaceutical interventions?\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"search('non-pharamceutical interventions of coronavirus')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What has been published about ethical and social science considerations?\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"search('ethical and social science considerations of coronavirus')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What has been published about medical care?\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"search('coronavirus medical care')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What do we know about diagnostics and surveillance?\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"search('coronavirus diagnostics and surveillance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What has been published about information sharing and inter-sectoral collaboration?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"search('coronavirus information sharing and colaboration')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}