{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#importing modules\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading dataset\ndf = pd.read_csv('/kaggle/input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Info about data\n\nID: ID of each client\n\nLIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit\n\nSEX: Gender (1=male, 2=female)\n\nEDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n\nMARRIAGE: Marital status (1=married, 2=single, 3=others)\n\nAGE: Age in years\n\nPAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, â€¦ 8=payment \ndelay for eight months, 9=payment delay for nine months and above)\n\nPAY_2: Repayment status in August, 2005 (scale same as above)\n\nPAY_3: Repayment status in July, 2005 (scale same as above)\n\nPAY_4: Repayment status in June, 2005 (scale same as above)\n\nPAY_5: Repayment status in May, 2005 (scale same as above)\n\nPAY_6: Repayment status in April, 2005 (scale same as above)\n\nBILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n\nBILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n\nBILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n\nBILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n\nBILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n\nBILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n\nPAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n\nPAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n\n\nPAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n\nPAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n\nPAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n\nPAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n\ndefault.payment.next.month: Default payment (1=yes, 0=no)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Renaming columns inorder to avoid confusion\ndf.rename({'PAY_0':'PAY_1','default.payment.next.month':'DEFAULT'},axis = 1,inplace = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since ID is the least inportant in this data.So removing ID column\ndf.drop(['ID'],axis = 1,inplace = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's check the unique values of columns\nprint({'Sex':df['SEX'].unique()},{'Education':df['EDUCATION'].unique()},{'Marriage':df['MARRIAGE'].unique()},sep='\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that ```Education``` column has values other than 1,2,3,4 i.e 5,6,0.<br/>\nAlthough as mentioned in dataset 5,6 are unknown,we will also assume 0 as unknown too.<br/>\nSimilarly ```Marriage``` has also 0.So we take it as unkown too\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's see the number of rows of missing values\nlen(df[(df['EDUCATION'] == 0) | (df['EDUCATION'] == 5) | (df['EDUCATION']== 6) | (df['MARRIAGE'] == 0) ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Percentage of Missing Values is {} %'.format(round(399/len(df) * 100,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Normally there is a rule of thumb is that if the percentage of missing values is less than 5% of total dataset then it can be removed*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Although imputer techniques can be used but removing the rows won't effect the outcome much\ndf_no_missing = df[(df['EDUCATION'] != 0) & (df['EDUCATION'] != 5) & (df['EDUCATION'] != 6) & (df['MARRIAGE'] != 0) ]\nlen(df_no_missing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's check the length of rows having default as 0 and as 1\nprint('No of people defaulted is {0}'.format(len(df[df['DEFAULT'] == 1])))\n\nprint('No of people not defaulted is {0}'.format(len(df[df['DEFAULT'] == 0])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Here we can see that Number of people ``` NOT DEFAULTED``` is almost 4x times the Number of people  ```DEFAULTED``` . So the dataset is skewed\n#### Also SVM performs well in small number of dataset \n#### Hence it is better to *Downsize* the Samples and perform the training\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Downsizing of Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing library\nfrom sklearn.utils import resample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's take 1000 samples from each category of default \n\n#First splitting the dataset into default and not default dataset\ndf_no_default = df_no_missing[df_no_missing['DEFAULT'] == 0]\ndf_default = df_no_missing[df_no_missing['DEFAULT'] == 1]\n\n#Now downsizing the dataset\ndf_no_default_downsampled = resample(df_no_default,\n                                    replace = False,\n                                    n_samples = 1000,\n                                    random_state = 42)\n\ndf_default_downsampled  = resample(df_default,\n                      replace = False,\n                      n_samples = 1000,\n                      random_state = 42)\n\ndf_downsampled = pd.concat([df_no_default_downsampled,df_default_downsampled],axis = 0)\nlen(df_downsampled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dependent and Independent dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting dataset into dependent(X) and independent(y) dataset\n\nX = df_downsampled.drop(['DEFAULT'],axis = 1).copy()\ny = df_downsampled['DEFAULT'].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Since Sklearn only uses numerical values we need to one-hot encode the categorical variables in dataset* <BR/>\n*Categorical variables are - ```SEX```, ```EDUCATION``` , ```MARRIAGE```, ```PAYS```*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## One-Hot Encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_encoded = pd.get_dummies(X,columns = ['SEX','EDUCATION','MARRIAGE','PAY_1','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6'])\n\nX_encoded.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train - Test Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing library\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X_encoded,y,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Scaling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*SVM kernels such as rbf can be sometimes sensitive to datas that are large to datas that are small.*<br/>\n*So normalising the data* ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing library\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training SVM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing library\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#First making a model without any hyperparameter tuning\nsvc = SVC(random_state=42)\nsvc.fit(X_train_scaled,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Confusion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing library\nfrom sklearn.metrics import plot_confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix for training set\n\nplot_confusion_matrix(svc,X_train_scaled,y_train,display_labels = ['Did not Default','Default'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion Matrix for test set\n\nX_test_scaled = scaler.transform(X_test)\nplot_confusion_matrix(svc,X_test_scaled,y_test,display_labels = ['Did not Default','Default'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Optimization of Parameters using Cross Validation and GridSearch ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = [\n    {'C':[0.5,1,10,100,1000],\n     'gamma':[1,0.1,0.001,0.0001],\n     'kernel':['rbf']}]\n\nsvc_optimised = GridSearchCV(\n    SVC(),\n    param_grid,\n    cv = 4,\n    scoring = 'accuracy',\n    verbose = 0)\n\nsvc_optimised.fit(X_train_scaled,y_train)\nprint(svc_optimised.best_params_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training using Optimised Parameter","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_final = SVC(random_state=42,C = 1,gamma=0.001)\nsvc_final.fit(X_train_scaled,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix for training set\n\nplot_confusion_matrix(svc_final,X_train_scaled,y_train,display_labels = ['Did not Default','Default'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion Matrix for test set\n\nplot_confusion_matrix(svc_final,X_test_scaled,y_test,display_labels = ['Did not Default','Default'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inference: Here we see that by using GridSearch the False Positive Rates have been decreased but False negatives have increased since accuracy is used as evaluation metric.<br/>\n\n### Precision is increased while Recall is decreased\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Visualising the prediction using PCA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing library\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA()\n\nX_train_pca = pca.fit_transform(X_train_scaled)\n\nper_var = np.round(pca.explained_variance_ratio_*100,decimals = 1)\nlabels = [str(x) for x in range(1,len(per_var) + 1)]\n\nplt.bar(x = range(1,len(per_var) +1),height = per_var)\nplt.tick_params(\n    axis = 'x',\n    which = 'both',\n    bottom = False,\n    top = False,\n    labelbottom = False)\n\nplt.ylabel('Percentage of Explained Varaince')\nplt.xlabel('Principle Component')\nplt.title('Scree Plot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pc1 = X_train_pca[:,0]\ntrain_pc2 = X_train_pca[:,1]\n\npca_trained_scaled = scaler.fit_transform(np.column_stack((train_pc1,train_pc2)))\n\nparam_grid_pca = [\n    {'C':[0.5,1,10,100,1000],\n    'gamma':[1,0.1,0.01,0.001,0.001],\n    'kernel':['rbf']}]\n\noptimal_params_pca = GridSearchCV(\n    SVC(),\n    param_grid_pca,\n    cv = 4,\n    scoring = 'accuracy',\n    verbose = 0 )\n\noptimal_params_pca.fit(pca_trained_scaled,y_train)\nprint(optimal_params_pca.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_pca = SVC(random_state=42,C = 1000,gamma = 0.1)\nsvc_pca.fit(pca_trained_scaled,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot of PCA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_pca = pca.transform(X_train_scaled)\n\ntest_pc1 = X_test_pca[:,0]\ntest_pc2 = X_test_pca[:,1]\n\n\nx_min = test_pc1.min() - 1\nx_max = test_pc1.max() + 1\n\ny_min = test_pc2.min() - 1\ny_max = test_pc2.max() + 1\n\nxx,yy = np.meshgrid(np.arange(start = x_min,stop = x_max,step = 0.1),\n                   np.arange(start = y_min,stop = y_max,step = 0.1 ))\n\n\nZ = svc_pca.predict(np.column_stack((xx.ravel(),yy.ravel())))\nZ = Z.reshape(xx.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.colors as colors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (10,10))\n\nax.contourf(xx,yy,Z,alpha = 0.1 )\n\ncmap = colors.ListedColormap(['#e41a1c','#4daf4a'])\n\nscatter = ax.scatter(test_pc1,test_pc2,c = y_train,cmap = cmap,s = 100,edgecolors = 'k',alpha = 0.7)\n\nlegend = ax.legend(scatter.legend_elements()[0],\n                  scatter.legend_elements()[1],\n                  loc = 'upper right')\n\nlegend.get_texts()[0].set_text('Not Default')\nlegend.get_texts()[1].set_text('Default')\n\nax.set_ylabel('PC2')\nax.set_xlabel('PC1')\nax.set_title('Classifer Visualisation using transformed featured by PCA')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Thank you","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}