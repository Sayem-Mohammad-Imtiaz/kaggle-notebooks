{"cells":[{"metadata":{},"cell_type":"markdown","source":"## About the Dataset\nA set of 25,000 highly polar movie reviews for training and 25,000 for testing. \n\n## To Do\nPredict wether the review is positive or negative ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# to load, access, process and dump json files\nimport json\n# regular repression\nimport re\n# to parse HTML contents\nfrom bs4 import BeautifulSoup\n\n# for numerical analysis\nimport numpy as np \n# to store and process in a dataframe\nimport pandas as pd \n\n# for ploting graphs\nimport matplotlib.pyplot as plt\n# advancec ploting\nimport seaborn as sns\n# to create word clouds\nfrom wordcloud import WordCloud, STOPWORDS \n\n# To encode values\nfrom sklearn.preprocessing import LabelEncoder\n# Convert a collection of text documents to a matrix of token counts\nfrom sklearn.feature_extraction.text import CountVectorizer\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\n# train test split\nfrom sklearn.model_selection import train_test_split\n\n# for deep learning \nimport tensorflow as tf\n# to tokenize text\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n# to pad sequence \nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Utility Function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_ngram(sentiment, n):\n    \n    temp_df = df[df['sentiment'] == sentiment]\n    \n    word_vectorizer = CountVectorizer(ngram_range=(n, n), analyzer='word')\n    sparse_matrix = word_vectorizer.fit_transform(temp_df['review'])\n    \n    frequencies = sum(sparse_matrix).toarray()[0]\n    \n    return pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\\\n            .sort_values(by='frequency', ascending=False) \\\n            .reset_index() \\\n            .head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_wordcloud(review, cmap):\n    fig, ax = plt.subplots(figsize=(8, 6))\n    wc = WordCloud(max_words = 1000, background_color ='white', stopwords = stopwords, \n                   min_font_size = 10, colormap=cmap)\n    wc = wc.generate(review)\n    plt.axis('off')\n    plt.imshow(wc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to plot model accuracy and loss\n\ndef plot_history(history):\n    \n    plt.figure(figsize=(20, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='Training Accuracy', c='green', lw='2')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', c='orangered', lw='2')\n    plt.title('Accuracy', loc='left', fontsize=16)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Training Loss', c='green', lw='2')\n    plt.plot(history.history['val_loss'], label='Validation Loss', c='orangered', lw='2')\n    plt.title('Loss', loc='left', fontsize=16)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to plot confusion matrix\n# ========================\n\ndef plot_cm(pred, ticklabels, figsize):\n    \n    pred = pred.ravel()\n    pred = np.round(pred)\n      \n    fig, ax = plt.subplots(1, 1, figsize=(figsize, figsize))\n\n    cm = confusion_matrix(validation_labels, pred)\n    sns.heatmap(cm, annot=True, cbar=False, fmt='1d', cmap='Blues', ax=ax)\n\n    ax.set_xlabel('Predicted')\n    ax.set_ylabel('Actual')\n    ax.set_xticklabels(ticklabels)\n    ax.set_yticklabels(ticklabels, rotation=0)\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# read the data\ndf = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n\n# shape\nprint('No. of rows and columns :', df.shape)\n\n# first few rows\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true},"cell_type":"markdown","source":"## Stopwords","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stopwords list from https://github.com/Yoast/YoastSEO.js/blob/develop/src/config/stopwords.js\nstopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n\n# specific stopwords\nspecific_sw = ['br', 'movie', 'film']\n\n# all stopwords\nstopwords = stopwords + specific_sw","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## No. of reviews in with each sentiment","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('darkgrid')\nplt.figure(figsize=(4, 5))\nsns.countplot(df['sentiment'], palette=['teal', 'orangered'])\nplt.title('No. of reviews in with each sentiment')\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plots","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# word cloud on positve reviews\npos_rev = ' '.join(df[df['sentiment']=='positive']['review'].to_list()[:10000])\nplot_wordcloud(pos_rev, 'Blues')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word cloud on positve reviews\nneg_rev = ' '.join(df[df['sentiment']=='negative']['review'].to_list()[:10000])\nplot_wordcloud(neg_rev, 'Reds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_ngram('positive', 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_ngram('negative', 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_ngram('positive', 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_ngram('negative', 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_ngram('positive', 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_ngram('negative', 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# to remove non alphanumeric character\ndef alpha_num(text):\n    return re.sub(r'[^A-Za-z0-9 ]', '', text)\n\n# to remove the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stopwords:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n\n# to remove URLs\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\n# to remove html tags\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply preprocessing steps\n\ndf['review'] = df['review'].apply(remove_URL)\ndf['review'] = df['review'].apply(remove_html)\ndf['review'] = df['review'].str.lower()\ndf['review'] = df['review'].apply(alpha_num)\ndf['review'] = df['review'].apply(remove_stopwords)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get review and labels","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# container for sentences\nreviews = np.array([review for review in df['review']])\n\n# container for labels\nlabels = np.array([label for label in df['sentiment']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Label encode sentiment","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# label encoding labels \n\nenc = LabelEncoder()\nencoded_labels = enc.fit_transform(labels)\n\nprint(enc.classes_)\nprint(labels[:5])\nprint(encoded_labels[:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Test Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# train-test split\ntrain_sentences, validation_sentences, train_labels, validation_labels = train_test_split(reviews, encoded_labels, \n                                                                                          test_size=0.33, \n                                                                                          stratify=labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenize and Sequence text","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenize sentences\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_sentences)\nword_index = tokenizer.word_index\n\n# convert train dataset to sequence and pad sequences\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\ntrain_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)\n\n# convert validation dataset to sequence and pad sequences\nvalidation_sequences = tokenizer.texts_to_sequences(validation_sentences)\nvalidation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# model parameters\n\nvocab_size = len(word_index)\nembedding_dim = 100\nmax_length = 120\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# With Word Embedding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# model initialization\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# compile model\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# model summary\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model\nnum_epochs = 10\nhistory = model.fit(train_padded, train_labels, \n                    epochs=num_epochs, verbose=2, \n                    validation_split=0.3)\n\n# predict values\npred = model.predict(validation_padded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot history\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot confusion matrix\nplot_cm(pred, enc.classes_, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reviews on which we need to predict\nsentence = [\"The movie was very touching and heart whelming\", \n            \"I have never seen a terrible movie like this\"]\n\n# convert to a sequence\nsequences = tokenizer.texts_to_sequences(sentence)\n\n# pad the sequence\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n# preict the label\nprint(model.predict(padded))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# With LSTM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# model initialization\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# compile model\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# model summary\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model\nnum_epochs = 10\nhistory = model.fit(train_padded, train_labels, \n                    epochs=num_epochs, verbose=2, \n                    validation_split=0.33)\n\n# predict values\npred = model.predict(validation_padded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot history\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot confusion matrix\nplot_cm(pred, enc.classes_, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reviews on which we need to predict\nsentence = [\"The movie was very touching and heart whelming\", \n            \"I have never seen a terrible movie like this\"]\n\n# convert to a sequence\nsequences = tokenizer.texts_to_sequences(sentence)\n\n# pad the sequence\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n# preict the label\nprint(model.predict(padded))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LSTM - 2 Layer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# model initialization\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# compile model\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# model summary\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model\nnum_epochs = 10\nhistory = model.fit(train_padded, train_labels, \n                    epochs=num_epochs, verbose=2, \n                    validation_split=0.33)\n\n# predict values\npred = model.predict(validation_padded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot history\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot confusion matrix\nplot_cm(pred, enc.classes_, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reviews on which we need to predict\nsentence = [\"The movie was very touching and heart whelming\", \n            \"I have never seen a terrible movie like this\"]\n\n# convert to a sequence\nsequences = tokenizer.texts_to_sequences(sentence)\n\n# pad the sequence\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n# preict the label\nprint(model.predict(padded))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# With GRU","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# model initialization\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# compile model\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# model summary\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model\nnum_epochs = 10\nhistory = model.fit(train_padded, train_labels, \n                    epochs=num_epochs, verbose=2, \n                    validation_split=0.33)\n\n# predict values\npred = model.predict(validation_padded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot history\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot confusion matrix\nplot_cm(pred, enc.classes_, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reviews on which we need to predict\nsentence = [\"The movie was very touching and heart whelming\", \n            \"I have never seen a terrible movie like this\"]\n\n# convert to a sequence\nsequences = tokenizer.texts_to_sequences(sentence)\n\n# pad the sequence\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n# preict the label\nprint(model.predict(padded))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# With Convolution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# model initialization\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n    tf.keras.layers.GlobalMaxPooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# compile model\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# model summary\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model\nnum_epochs = 10\nhistory = model.fit(train_padded, train_labels, \n                    epochs=num_epochs, verbose=2, \n                    validation_split=0.33)\n\n# predict values\npred = model.predict(validation_padded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot history\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot confusion matrix\nplot_cm(pred, enc.classes_, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reviews on which we need to predict\nsentence = [\"The movie was very touching and heart whelming\", \n            \"I have never seen a terrible movie like this\"]\n\n# convert to a sequence\nsequences = tokenizer.texts_to_sequences(sentence)\n\n# pad the sequence\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n# preict the label\nprint(model.predict(padded))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# With GloVe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note this is the 100 dimension version of GloVe from Stanford\n# laurencemoroney unzipped and hosted it on his site to make this notebook easier\n\n!wget --no-check-certificate \\\n    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt \\\n    -O /tmp/glove.6B.100d.txt\n\nembeddings_index = {}\n\nwith open('/tmp/glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\nembeddings_matrix = np.zeros((vocab_size+1, embedding_dim))\n\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model initialization\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n    tf.keras.layers.MaxPooling1D(pool_size=4),\n    tf.keras.layers.LSTM(64),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# compile model\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# model summary\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model\nnum_epochs = 10\nhistory = model.fit(train_padded, train_labels, \n                    epochs=num_epochs, verbose=2, \n                    validation_split=0.33)\n\n# predict values\npred = model.predict(validation_padded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot history\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot confusion matrix\nplot_cm(pred, enc.classes_, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reviews on which we need to predict\nsentence = [\"The movie was very touching and heart whelming\", \n            \"I have never seen a terrible movie like this\"]\n\n# convert to a sequence\nsequences = tokenizer.texts_to_sequences(sentence)\n\n# pad the sequence\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n# preict the label\nprint(model.predict(padded))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}