{"cells":[{"metadata":{},"cell_type":"markdown","source":"# importing libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# importing csv file"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. \n# It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/spam.csv', encoding='latin-1')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns = ['label', 'msg', 'var1', 'var2', 'var3']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['var1'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['var2'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['var3'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['spam'] = np.where(df['label']=='spam', 1, 0)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# spam and not spam messages"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['spam']==1].head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = set(STOPWORDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = ''.join(list(df[df['spam']==1]['msg']))\nspam_wc = WordCloud(background_color='white',\n                    stopwords=set(STOPWORDS),\n                    max_words=50,).generate(words)\nplt.figure(figsize=(10,8), facecolor='k')\nplt.imshow(spam_wc)\nplt.axis('off')\nplt.tight_layout(pad=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['spam']==0].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = ''.join(list(df[df['spam']==1]['msg']))\nspam_wc = WordCloud(background_color='white',\n                    stopwords=set(STOPWORDS),\n                    max_words=50,).generate(words)\nplt.figure(figsize=(10,8), facecolor='k')\nplt.imshow(spam_wc)\nplt.axis('off')\nplt.tight_layout(pad=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# train test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df['msg'],\n                                                    df['spam'],\n                                                    random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# using word count / count vectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"vect = CountVectorizer().fit(X_train)\nX_train_vectorized = vect.transform(X_train)\n\nprint('every other 700th feature - ',vect.get_feature_names()[::700])\nprint('total number of rows/documents in of the training dataframe/corpus - ', X_train.shape)\nprint('number of features/words - ', len(vect.get_feature_names()))\nprint('shape of the vectorized train sparse matrix - ', X_train_vectorized.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\npred = model.predict(vect.transform(X_test))\n\nscore = roc_auc_score(y_test, pred)\nfeature_names = np.array(vect.get_feature_names())\nsorted_coef_index = model.coef_[0].argsort()\n\nprint(score, '\\n')\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:15]]))\nprint('Largest Coefs: \\n{}\\n'.format(feature_names[sorted_coef_index[:-15:-1]]))\nprint(model.predict(vect.transform(['you won the free call offer. call back to claim.', \n                                    'how are you'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, pred, target_names=['ham', 'spam']))\n\ncm = confusion_matrix(pred, y_test)\ndf_cm = pd.DataFrame(cm, \n                     columns=np.unique(y_test), \n                     index = np.unique(y_test))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\n\nsns.heatmap(df_cm, \n            cmap=\"Blues\", \n            annot=True, \n            fmt='g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# using TF-IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"vect = TfidfVectorizer(min_df=5).fit(X_train)\nX_train_vectorized = vect.transform(X_train)\n\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\npred = model.predict(vect.transform(X_test))\n\nscore = roc_auc_score(y_test, pred)\nfeature_names = np.array(vect.get_feature_names())\nsorted_coef_index = model.coef_[0].argsort()\n\nprint(score, '\\n')\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:15]]))\nprint('Largest Coefs: \\n{}\\n'.format(feature_names[sorted_coef_index[:-15:-1]]))\nprint(model.predict(vect.transform(['you won the free call offer. call back to claim.', \n                                    'how are you'])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# using n-grams"},{"metadata":{"trusted":true},"cell_type":"code","source":"vect = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train)\nX_train_vectorized = vect.transform(X_train)\n\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\npred = model.predict(vect.transform(X_test))\n\nscore = roc_auc_score(y_test, pred)\nfeature_names = np.array(vect.get_feature_names())\nsorted_coef_index = model.coef_[0].argsort()\n\nprint(score, '\\n')\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:15]]))\nprint('Largest Coefs: \\n{}\\n'.format(feature_names[sorted_coef_index[:-15:-1]]))\nprint(model.predict(vect.transform(['you won the free call offer. call back to claim.', \n                                    'how are you'])))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}