{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from itertools import combinations\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import shapiro, t\nimport os\n\nfrom statsmodels.stats.proportion import proportion_confint\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier, LinearRegression, Ridge, Lasso\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree, DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.preprocessing import PolynomialFeatures\n\n%matplotlib inline\nplt.style.use(\"seaborn-darkgrid\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(\"/kaggle/input\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_placement = pd.read_csv(\"factors-affecting-campus-placement/Placement_Data_Full_Class.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_placement.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_placement.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification task for placement","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1 = df_placement.drop(columns = [\"salary\", 'hsc_s', 'degree_t'])\ndf_1['gender'].replace(['M', 'F'], [0, 1], inplace = True)\ndf_1['status'].replace(['Not Placed', 'Placed'], [0, 1], inplace = True)\ndf_1['ssc_b'].replace(['Central', 'Others'], [0, 1], inplace = True)\ndf_1['hsc_b'].replace(['Central', 'Others'], [0, 1], inplace = True)\ndf_1['workex'].replace(['No', 'Yes'], [0, 1], inplace = True)\ndf_1['specialisation'].replace(['Mkt&Fin', 'Mkt&HR'], [0, 1], inplace = True)\ndf_1 = df_1.join([pd.get_dummies(df_placement.hsc_s, prefix = df_placement.hsc_s.name),\n                  pd.get_dummies(df_placement.degree_t, prefix = df_placement.degree_t.name)\n                 ])\n#a loop with if condition can also convert to one hot vectors\n#can be packed into preprocessing data function","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,15))\nsns.heatmap(df_1.corr(), annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare Train Test Sets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_1.drop(columns = [\"status\"])\nY = df_1['status']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = {k : [] for k in range(1, 11)}\n#One could choose to express in the form of 4 lists or 4 dictinaries\n\nfor i in range(1, 11):\n    knncls = KNeighborsClassifier(n_neighbors = i)\n    knncls.fit(X_train, Y_train)\n\n    data[i].append(accuracy_score(Y_train, knncls.predict(X_train)))\n    data[i].append(accuracy_score(Y_test, knncls.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,15))\nplt.plot(list(data.keys()), [i[0] for i in data.values()], marker='o', label=\"Train_accuracy\",\n        drawstyle=\"steps-post\")\nplt.plot(list(data.keys()), [i[1] for i in data.values()], marker='o', label=\"Test_accuracy\",\n        drawstyle=\"steps-post\")\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max([i[-1] for i in data.values()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(data.values())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy of KNN Classifier $\\approx$ 0.8 with n = 8","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 8)\nknn.fit(X_train, Y_train)\n\ncm = pd.DataFrame(confusion_matrix(Y_test, knn.predict(X_test)).T, index=['No', 'Yes'], columns=['No', 'Yes'])\ncm.index.name = 'Predicted'\ncm.columns.name = 'True'\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correct = confusion_matrix(Y_test, knn.predict(X_test))[0][0] + confusion_matrix(Y_test, knn.predict(X_test))[1][1]\nproportion_confint(correct, len(X_test), method = 'wilson')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(max_iter = 200, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_accuracy = accuracy_score(Y_train, lr.predict(X_train))\ntest_accuracy = accuracy_score(Y_test, lr.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\n     \"train_accuracy: \", train_accuracy,\n     \"\\ntest_accuracy: \", test_accuracy,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correct = confusion_matrix(Y_test, lr.predict(X_test))[0][0] + confusion_matrix(Y_test, lr.predict(X_test))[1][1]\nproportion_confint(correct, len(X_test), method = 'wilson')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Suprisingly, Simple Logistic Regression yields higher test accuracy than KNN even with n_neighbors tuned.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Ridge","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = {k : [] for k in np.geomspace(1e-3, 1e2, 10)}\n#Same procedure as tuning KNNClassifier, depending or not will implement function\n\nfor i in data.keys():\n    ridge = RidgeClassifier(alpha = i, normalize = True, random_state = 1)\n    ridge.fit(X_train, Y_train)\n    \n    data[i].append(accuracy_score(Y_train, ridge.predict(X_train)))\n    data[i].append(accuracy_score(Y_test, ridge.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,10))\nplt.plot(list(data.keys()), [i[0] for i in data.values()], marker='o', label=\"Train_accuracy\",\n        drawstyle=\"steps-post\")\nplt.plot(list(data.keys()), [i[1] for i in data.values()], marker='o', label=\"Test_accuracy\",\n        drawstyle=\"steps-post\")\n\nplt.xscale(\"log\")\n\n\nplt.xlabel(\"alpha\")\n\nplt.legend()\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max([i[-1] for i in data.values()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[i[-1] for i in data.values()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_alpha = list(data.keys())[4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = RidgeClassifier(alpha = test_alpha, normalize = True)\nridge.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(Y_test, ridge.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = pd.DataFrame(confusion_matrix(Y_test, ridge.predict(X_test)).T, index=['No', 'Yes'], columns=['No', 'Yes'])\ncm.index.name = 'Predicted'\ncm.columns.name = 'True'\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(Y_test, ridge.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correct = confusion_matrix(Y_test, ridge.predict(X_test))[0][0] + confusion_matrix(Y_test, ridge.predict(X_test))[1][1]\nproportion_confint(correct, len(X_test), method = 'wilson')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ridge with tuned hyperparameter has very high test accuracy","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Test on whether tuned parameter can be robust to random initialization of train_test_split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ac, test_ac = [], []\n\nfor i in range(1,1001):\n    X_train_, X_test_, Y_train_, Y_test_ = train_test_split(X, Y)\n    \n    ridge_ = RidgeClassifier(alpha = test_alpha, normalize = True)\n    ridge_.fit(X_train_, Y_train_)\n    \n    train_ac.append(accuracy_score(Y_train_, ridge_.predict(X_train_)))\n    test_ac.append(accuracy_score(Y_test_, ridge_.predict(X_test_)))\n    \nplt.figure(figsize = (10,6))\n#plt.plot([i for i in range(1,1001)], train_ac, marker='o', label=\"Train_accuracy\",\n#        drawstyle=\"steps-post\")\nplt.plot([i for i in range(1,1001)], test_ac, marker='o', label=\"Test_accuracy\",\n        drawstyle=\"steps-post\")\n\nplt.legend()\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(test_ac, bins= 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shapiro(test_ac)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Trees","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#default tree\ntree = DecisionTreeClassifier(random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(Y_test, tree.predict(X_test)))\nmean_squared_error(Y_test, tree.predict(X_test)), accuracy_score(Y_test, tree.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(100, 100))\nfeatures = X_train.columns\n\nplot_tree(tree, filled = True, ax = ax, feature_names = features, proportion = True, rounded = True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Importance = pd.DataFrame({'Importance':tree.feature_importances_*100}, index=X.columns)\nImportance.sort_values('Importance', axis=0, ascending=True).plot(kind='barh', color='b', )\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pruning Trees","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"path = tree.cost_complexity_pruning_path(X_train, Y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\nax.set_xlabel(\"effective alpha\")\nax.set_ylabel(\"total impurity of leaves\")\nax.set_title(\"Total Impurity vs effective alpha for training set\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, Y_train)\n    clfs.append(clf)\nprint(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n      clfs[-1].tree_.node_count, ccp_alphas[-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\n#we remove last pruned tree with only one terminal node\n\n\nnode_counts = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]\nfig, ax = plt.subplots(2, 1, figsize = (10,10))\nax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle=\"steps-post\")\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[0].set_title(\"Number of nodes vs alpha\")\nax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\")\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[1].set_title(\"Depth vs alpha\")\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_scores = [clf.score(X_train, Y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, Y_test) for clf in clfs]\n\nfig, ax = plt.subplots(figsize=(10,10))\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dic = dict(zip(ccp_alphas, test_scores))\nalpha = max(dic, key = dic.get)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree = DecisionTreeClassifier(random_state=0, ccp_alpha=alpha)\ntree.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(Y_test, tree.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(100, 100))\nfeatures = X_train.columns\n\nplot_tree(tree, filled = True, ax = ax, feature_names = features, proportion = True, rounded = True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forests","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(random_state = 1, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.fit(X_train, np.ravel(Y_train));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Importance = pd.DataFrame({'Importance':rf.feature_importances_*100}, index=X.columns)\nImportance.sort_values('Importance', axis=0, ascending=True).plot(kind='barh' )\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DecisionTreeClassifier, tune $\\alpha$","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = {k : [] for k in np.geomspace(1e-3, 1e2, 20)}\n#Same procedure as tuning KNNClassifier, depending or not will implement function\n\nfor i in list(data.keys()):\n    rf = RandomForestClassifier(random_state = 1, n_jobs = -1, ccp_alpha = i)\n    rf.fit(X_train, Y_train)\n\n    data[i].append(accuracy_score(Y_train, rf.predict(X_train)))\n    data[i].append(accuracy_score(Y_test, rf.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,15))\nplt.plot(list(data.keys()), [i[0] for i in data.values()], marker='o', label=\"Train_accuracy\",\n        drawstyle=\"steps-post\")\nplt.plot(list(data.keys()), [i[1] for i in data.values()], marker='o', label=\"Test_accuracy\",\n        drawstyle=\"steps-post\")\n\nplt.xscale(\"log\")\n\nplt.xlabel(\"alpha\")\n\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient Boosting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"regr = GradientBoostingClassifier(n_estimators = 100, learning_rate = 0.01, random_state = 1)\nregr.fit(X_train, np.ravel(Y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Importance = pd.DataFrame({'Importance':regr.feature_importances_*100}, index=X.columns)\nImportance.sort_values('Importance', axis=0, ascending=True).plot(kind='barh', color='r', )\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_params = {'learning_rate': np.geomspace(1e-4, 100, num = 7),\n               'n_estimators': np.arange(100, 1000, 100),\n               'max_depth': [i for i in range(1,11)]}\n\ngs = GridSearchCV(regr, grid_params, cv = 10, n_jobs = -1, scoring = 'accuracy')\ngs.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs.best_estimator_.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs.best_estimator_.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(Y_test, gs.best_estimator_.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = {k : [] for k in np.geomspace(1e-4, 100, num = 7)}\n#Same procedure as tuning KNNClassifier, depending or not will implement function\n#Different learning rate\n\nfor i in list(data.keys()):\n    regr = GradientBoostingClassifier(n_estimators = 1000, learning_rate = i, random_state = 1)\n    regr.fit(X_train, Y_train)\n\n    data[i].append(accuracy_score(Y_train, regr.predict(X_train)))\n    data[i].append(accuracy_score(Y_test, regr.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,6))\nplt.plot(list(data.keys()), [i[0] for i in data.values()], marker='o', label=\"Train_accuracy\",\n        drawstyle=\"steps-post\")\nplt.plot(list(data.keys()), [i[1] for i in data.values()], marker='o', label=\"Test_accuracy\",\n        drawstyle=\"steps-post\")\n\nplt.xscale(\"log\")\n\nplt.xlabel(\"alpha\")\n\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"depth = [i for i in range(1,11)]\n\n\nfor i in depth:\n    \n    regr = GradientBoostingClassifier(n_estimators = 500, learning_rate = 10, random_state = 1, max_depth = i)\n    #Using prior best test accuracy learning rate\n    regr.fit(X_train, Y_train)\n\n    train_acc.append(accuracy_score(Y_train, regr.predict(X_train)))\n    test_acc.append(accuracy_score(Y_test, regr.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,6))\nplt.plot(depth, train_acc, marker='o', label=\"Train_accuracy\",\n        drawstyle=\"steps-post\")\nplt.plot(depth, test_acc, marker='o', label=\"Test_accuracy\",\n        drawstyle=\"steps-post\")\n\nplt.xlabel(\"depth\")\n\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Salary Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_placement.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2 = df_placement.drop(columns = [\"status\", 'hsc_s', 'degree_t'])\ndf_2 = df_2.dropna()\ndf_2['gender'].replace(['M', 'F'], [0, 1], inplace = True)\ndf_2['ssc_b'].replace(['Central', 'Others'], [0, 1], inplace = True)\ndf_2['hsc_b'].replace(['Central', 'Others'], [0, 1], inplace = True)\ndf_2['workex'].replace(['No', 'Yes'], [0, 1], inplace = True)\ndf_2['specialisation'].replace(['Mkt&Fin', 'Mkt&HR'], [0, 1], inplace = True)\ndf_2 = df_2.join([pd.get_dummies(df_placement.hsc_s, prefix = df_placement.hsc_s.name),\n                  pd.get_dummies(df_placement.degree_t, prefix = df_placement.degree_t.name)\n                 ])\n#a loop with if condition can also convert to one hot vectors\n#can be packed into preprocessing data function","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,15))\nsns.heatmap(df_2.corr(method = \"spearman\"), annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_2.drop(columns = [\"salary\"])\nY = df_2[\"salary\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_X_train, reg_X_test, reg_Y_train, reg_Y_test = train_test_split(X, Y, random_state = 1) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_r = LinearRegression(normalize = True, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_r.fit(reg_X_train, reg_Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scoring(model, Y_test = reg_Y_test, X_test = reg_X_test, Y_train = reg_Y_train, X_train = reg_X_train):\n    R_squared = model.score(X_test, Y_test)\n    R2_adj =1 - (((1 - R_squared) * (len(X_test) - 1)) / (len(X_test) - (len(X_test.columns) - 1)))\n    test_mse = mean_squared_error(Y_test, model.predict(X_test))\n    train_mse = mean_squared_error(Y_train, lin_r.predict(X_train))\n    return [R_squared, R2_adj, train_mse, test_mse] \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring(lin_r)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lasso and Ridge","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_scores, ridge_scores = [], []\n\nfor i in np.geomspace(1e-2, 1e2, 5):\n    lasso = Lasso(alpha = i, normalize = True, random_state = 1, max_iter = 50000)\n    ridge = Ridge(alpha = i, normalize = True, max_iter = 50000)\n    \n    lasso.fit(reg_X_train, reg_Y_train)\n    ridge.fit(reg_X_train, reg_Y_train)\n    \n    lasso_scores.append(scoring(lasso))\n    ridge_scores.append(scoring(ridge))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,6))\n\nplt.plot(np.geomspace(1e-2, 1e2, 5), [i[0] for i in lasso_scores], label = \"Lasso R2\", drawstyle=\"steps-post\")\nplt.plot(np.geomspace(1e-2, 1e2, 5), [i[0] for i in ridge_scores], label = \"Ridge R2\", drawstyle=\"steps-post\")\nplt.xscale(\"log\")\nplt.legend()\n\nplt.show()\n\nplt.figure(figsize = (10,6))\n\nplt.plot(np.geomspace(1e-2, 1e2, 5), [i[1] for i in lasso_scores], label = \"Lasso Adjusted R2\", drawstyle=\"steps-post\")\nplt.plot(np.geomspace(1e-2, 1e2, 5), [i[1] for i in ridge_scores], label = \"Ridge Adjusted R2\", drawstyle=\"steps-post\")\nplt.xscale(\"log\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,6))\n\nplt.plot(np.geomspace(1e-2, 1e2, 5), [i[3] for i in ridge_scores],\n         label = \"Ridge test mse\", drawstyle=\"steps-post\")\nplt.plot(np.geomspace(1e-2, 1e2, 5), [i[3] for i in lasso_scores],\n         label = \"Lasso test mse\", drawstyle=\"steps-post\")\n\nplt.xscale(\"log\")\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Overall, Linear Regression is weak to predict the data, even with distance regularization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Tree Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_tree = DecisionTreeRegressor(random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_tree.fit(reg_X_train, reg_Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring(reg_tree)\n#Very poor values of R2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = reg_tree.cost_complexity_pruning_path(reg_X_train, reg_Y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = [10,6])\nax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\nax.set_xlabel(\"effective alpha\")\nax.set_ylabel(\"total impurity of leaves\")\nax.set_title(\"Total Impurity vs effective alpha for training set\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeRegressor(random_state=1, ccp_alpha=ccp_alpha)\n    clf.fit(reg_X_train, reg_Y_train)\n    clfs.append(clf)\nprint(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n      clfs[-1].tree_.node_count, ccp_alphas[-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"node_counts = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]\nfig, ax = plt.subplots(2, 1, figsize = (10,10))\nax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle=\"steps-post\")\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[0].set_title(\"Number of nodes vs alpha\")\nax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\")\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[1].set_title(\"Depth vs alpha\")\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = [scoring(clf) for clf in clfs]\n\nfig, ax = plt.subplots(2, 1, figsize=(10,6))\nax[0].set_xlabel(\"alpha\")\nax[1].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"mse\")\nax[1].set_ylabel(\"mse\")\nax[0].set_title(\"MSE vs alpha for training and testing sets\")\nax[0].plot(ccp_alphas, [i[2] for i in scores], marker='o', label=\"train mse\",\n        drawstyle=\"steps-post\")\nax[1].plot(ccp_alphas, [i[3] for i in scores], marker='o', label=\"test mse\",\n        drawstyle=\"steps-post\")\nax[0].legend()\nax[1].legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Trees also provide poor regression statistics","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}