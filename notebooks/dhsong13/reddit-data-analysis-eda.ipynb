{"cells":[{"metadata":{},"cell_type":"markdown","source":"<center><h1>Reddit Data Analysis(EDA)-v1</h1></center>\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"## What is about.."},{"metadata":{},"cell_type":"markdown","source":"Explore the Reddit Dataset."},{"metadata":{},"cell_type":"markdown","source":"## Thanks to.. <a id=\"top\"></a>"},{"metadata":{},"cell_type":"markdown","source":"> **Kaggle Data**<br>\n> \n> [Reddit - Data is Beautiful](https://www.kaggle.com/unanimad/dataisbeautiful)<br>\n\n> **Questions**<br>\n> \n> [tqdm: Using progress bar in pandas apply function](https://stackoverflow.com/questions/18603270/progress-indicator-during-pandas-operations)<br>\n> [nltk: pos_tag + lemmatiza](https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python)<br>\n> [wordcloud: how to draw wordcloud](https://lovit.github.io/nlp/2018/04/17/word_cloud/)<br>\n> [sci-kit learn: count vectorizer get feature name](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)<br>"},{"metadata":{},"cell_type":"markdown","source":"## Table of Contents.. <a id=\"top\"></a>"},{"metadata":{},"cell_type":"markdown","source":"1. [Problem Description](#1)\n2. [Data Description](#2)\n3. [Environment Setting](#3)\n    1. [Import Library](#3.1)\n    2. [Load Dataset](#3.2)\n4. [Data Preprocessing](#4)\n    1. [Missing Value](#4.1)\n    2. [Time Management](#4.2)\n    3. [OC(Original Content)](#4.3)\n    4. [Text Cleaning](#4.4)\n5. [Exploratory Data Analysis(EDA)](#5)\n    1. [Distribution of Numerical Value](#5.1)\n    2. [Time Analysis](#5.2)\n    3. [Wordcloud Text Analysis](#5.3)\n6. [Word Embedding](#6)\n    1. [Count Vectorizer](#6.1)\n    2. [TF-IDF Vectorizer](#6.2)\n7. [Data Modeling](#7)\n    1. [Dimension Reduction](#7.1)\n    2. [Classification](#7.2)\n    3. [Topic Modeling](#7.3)\n\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"# 1. Problem Description <a id=\"1\"></a>\n<p style=\"text-align:right;\"><a href=\"#top\">üîù top</a></p>"},{"metadata":{},"cell_type":"markdown","source":"Using Reddit Dataset, do Exploratory Data Analysis. Question and Validation using Data Visualization."},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Description <a id=\"2\"></a>\n<p style=\"text-align:right;\"><a href=\"#top\">üîù top</a></p>"},{"metadata":{},"cell_type":"markdown","source":"Reddit Data is Beautiful - from [Kaggle](https://www.kaggle.com/unanimad/dataisbeautiful)\n> **About**<br>\n> \n> Data is Beautiful, r/dataisbeautiful, is a place for visual representations of data: Graphs, charts, maps, etc. DataIsBeautiful is for visualizations that effectively convey information. Aesthetics are an important part of information visualization, but pretty pictures are not the aim of this subreddit.\n\n> **Content**<br>\n> \n> This dataset contains a couple of fields with the information based on Reddit post submission, such:\n> Fields:\n> * id\n> * title\n> * score\n> * author\n> * authorfalirtext\n> * removed_by\n> * totalawardsreceived\n> * awarders\n> * created_utc\n> * full_link\n> * num_commnets\n> * over_18\n\n> **Method**<br>\n> \n> The data was extracted using the PushShift API for Reddit. Thanks Watchful1 for show me this API."},{"metadata":{},"cell_type":"markdown","source":"# 3. Environment Setting<a id=\"3\"></a>\n<p style=\"text-align:right;\"><a href=\"#top\">üîù top</a></p>"},{"metadata":{},"cell_type":"markdown","source":"## 3.1. Import Library<a id=\"3.1\"></a>\n<p style=\"text-align:right;\"><a href=\"#top\">üîù top</a></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Image\nfrom PIL import Image\n\n# Python Collectino\nfrom collections import Counter\n\n# FOR Loop Verbose\nfrom tqdm import tqdm\n\n# System\nimport os\n\n# String\nimport string\n\n# Natural Language Processing\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\nfrom nltk.stem import WordNetLemmatizer\n\n# Date and Time\nimport datetime\n\n# Dataframe\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\n\n# Numerical Data\nimport numpy as np\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.ensemble import GradientBoostingClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = '/usr/share/nltk_data'\nprint(data_path)\nif not os.path.exists(data_path):\n    nltk.download()\nnltk.data.path.append(data_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_rows = 499\npd.options.display.max_columns = 499\npd.options.mode.chained_assignment = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2. Load Dataset<a id=\"3.2\"></a>\n<p style=\"text-align:right;\"><a href=\"#top\">üîù top</a></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw = pd.read_csv('/kaggle/input/dataisbeautiful/r_dataisbeautiful_posts.csv', encoding='utf-8')\nraw","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Data Preprocessing<a id=\"4\"></a>\n<p style=\"text-align:right;\"><a href=\"#top\">üîù top</a></p>"},{"metadata":{},"cell_type":"markdown","source":"## 4.1. Missing Value<a id=\"4.1\"></a>\n<p style=\"text-align:right;\"><a href=\"#top\">üîù top</a></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned = raw.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**title**<br>\nimpute title to 'null'"},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned.title = cleaned.title.fillna('null')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned[cleaned.title == 'null']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**author_flair_text, removed_by, total_awards_received, awarders**<br>\ndrop"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['author_flair_text', 'removed_by', 'total_awards_received', 'awarders']\ncleaned = cleaned.drop(columns, axis=1)\ncleaned","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2. Time Management<a id=\"4.2\"></a>\n<p style=\"text-align:right;\"><a href=\"#top\">üîù top</a></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def utc_to_datetime(data):\n    data['year'] = data['created_utc'].apply(lambda utc: datetime.datetime.fromtimestamp(utc).year)\n    data['month'] = data['created_utc'].apply(lambda utc: datetime.datetime.fromtimestamp(utc).month)\n    data['day'] = data['created_utc'].apply(lambda utc: datetime.datetime.fromtimestamp(utc).day)    \n    data['hour'] = data['created_utc'].apply(lambda utc: datetime.datetime.fromtimestamp(utc).hour)\n    data['minute'] = data['created_utc'].apply(lambda utc: datetime.datetime.fromtimestamp(utc).minute)\n    data['second'] = data['created_utc'].apply(lambda utc: datetime.datetime.fromtimestamp(utc).second)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"utc_to_datetime(cleaned)\ncleaned","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3. OC(Original Content)<a id=\"4.3\"></a>\n<p style=\"text-align:right;\"><a href=\"#top\">üîù top</a></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned['original_content'] = cleaned['title'].str.contains('[OC]').astype(int)\ncleaned","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.4. Text Cleaning<a id=\"4.4\"></a>\n<p style=\"text-align:right;\"><a href=\"#top\">üîù top</a></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_wordnet_tag(tag):\n    if tag == 'ADJ':\n        return 'j'\n    elif tag == 'VERB':\n        return 'v'\n    elif tag == 'NOUN':\n        return 'n'\n    elif tag == 'ADV':\n        return 'r'\n    else:\n        return 'n'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatize_text(title):\n    stop = stopwords.words('english')\n    lemmatizer = WordNetLemmatizer()\n\n    words = list()\n    title = word_tokenize(title)\n    for word, tag in pos_tag(title):\n        tag = get_wordnet_tag(tag)\n        word = lemmatizer.lemmatize(word, tag)\n        if word not in stop:\n            words.append(word)\n    \n    return ' '.join(words)        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(dataset):\n    \n    tqdm.pandas()\n    \n    dataset['title_cleaned'] = dataset['title'].str.lower()\n    dataset['title_cleaned'] = dataset['title_cleaned'].str.replace(r'\\[oc\\]', ' ')\n    pattern_link = r'https?://[^\\s]+|www\\.[^\\s]+|[^\\s]+\\.com[^\\s]*|[^\\s]+\\.org[^\\s]*|[^\\s]+\\.html[^\\s]*'\n    dataset['title_cleaned'] = dataset['title_cleaned'].str.replace(pattern_link, ' link ')\n    \n    pattern_punctuation = r'[' + string.punctuation + '‚Äô]'\n    dataset['title_cleaned'] = dataset['title_cleaned'].str.replace(pattern_punctuation, '')\n    dataset['title_cleaned'] = dataset['title_cleaned'].str.replace(r' [\\d]+ |^[\\d]+ | [\\d]+$', ' ')\n    dataset['title_cleaned'] = dataset['title_cleaned'].str.replace(r'[^\\w\\d\\s]+', ' ')\n    dataset['title_cleaned'] = dataset['title_cleaned'].progress_apply(lambda title: lemmatize_text(title))\n    \n    dataset['title_cleaned'] = dataset['title_cleaned'].str.replace(r'\\s[\\s]+', ' ')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"clean_text(cleaned)\ncleaned","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.5. Data Type Conversion<a id=\"4.5\"></a>\n<p style=\"text-align:right;\"><a href=\"#top\">üîù top</a></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned['over_18'] = cleaned['over_18'].apply(lambda x: int(x))\ncleaned","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Exploratory Data Analysis(EDA)<a id=\"5\"></a>\n<p style=\"text-align:right;\"><a href=\"#top\">üîù top</a></p>"},{"metadata":{},"cell_type":"markdown","source":"## 5.1. Distribution of Numerical Value<a id=\"5.1\"></a>\n<p style=\"text-align:right;\"><a href=\"#top\">üîù top</a></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def boxplot(data, feature, base):\n    assert base in ['over_18', 'original_content']\n    \n    plt.figure(figsize=(30, 12))\n    sns.boxplot(x=base, y=feature, data=data)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned['score'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot(cleaned, 'score', 'over_18')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot(cleaned, 'score', 'original_content')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned['num_comments'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot(cleaned, 'num_comments', 'over_18')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot(cleaned, 'num_comments', 'original_content')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.2. Time Analysis<a id=\"5.2\"></a>\n<p style=\"text-align:right;\"><a href=\"#top\">üîù top</a></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def countplot(data, by='year'):\n    assert by in ['year', 'month', 'day']\n    data_copy = data.copy()\n    data_copy['year'] = data_copy['year'].astype(str)\n    data_copy['month'] = data_copy['month'].astype(str)\n    data_copy['day'] = data_copy['day'].astype(str)\n\n    plt.figure(figsize=(30, 10))\n    if by == 'year':\n        stat = data_copy['year'].value_counts()        \n        sns.countplot(by, data=data_copy)\n        plt.xlabel(by)\n    elif by == 'month':\n        data_copy['month'] = data_copy['year'] + '/' + data_copy['month']\n        stat = data_copy['month'].value_counts()        \n        sns.countplot(by, data=data_copy)\n        plt.xlabel(by)\n        plt.xticks(rotation=45)\n    elif by == 'day':\n        data_copy['day'] = data_copy['year'] + '/' + data_copy['month'] + '/' + data_copy['day']\n        stat = data_copy['day'].value_counts()        \n        sns.countplot(by, data=data_copy)            \n        \n    plt.ylabel('count')\n    plt.title('Count by Year/Month/Day Recent to Old')\n    plt.show()\n    \n    return stat\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countplot(cleaned, 'year')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countplot(cleaned, 'month')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countplot(cleaned, 'day')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> What happened on April 1st, 2nd?"},{"metadata":{},"cell_type":"markdown","source":"## 5.3. Wordcloud Text Analysis<a id=\"5.3\"></a>\n<p style=\"text-align:right;\"><a href=\"#top\">üîù top</a></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def wordcloud(dataset, min_freq=1):\n    bow = list()\n    for title in tqdm(dataset['title_cleaned']):\n        bow += word_tokenize(title)\n    \n    word_freq = dict()\n    counter = Counter(bow)\n    for word, freq in counter.items():\n        if freq >= min_freq:\n            word_freq[word] = freq\n    \n#     reddit_mask = np.array(Image.open('/kaggle/working/reddit_icon.png'))    \n    \n    wc = WordCloud(width=800, height=800, background_color='white') #, mask=reddit_mask)\n    wc = wc.generate_from_frequencies(word_freq)\n    \n    plt.figure(figsize=(12, 12))\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()\n\n    \n    return counter, word_freq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter, word_freq = wordcloud(cleaned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def wordcloud_by_date(dataset, year=None, month=None, day=None):\n    dataset_cp = dataset.copy()\n    \n    if year:\n        dataset_cp = dataset_cp[dataset_cp['year'] == year]\n    if month:\n        dataset_cp = dataset_cp[dataset_cp['month'] == month]\n    if day:\n        dataset_cp = dataset_cp[dataset_cp['day'] == day]\n    \n    return wordcloud(dataset_cp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter_2020, word_freq_2020 = wordcloud_by_date(cleaned, year=2020)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter_20170401, word_freq_20170401 = wordcloud_by_date(cleaned, year=2017, month=4, day=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter_20170402, word_freq_20170402 = wordcloud_by_date(cleaned, year=2017, month=4, day=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter_201704, word_freq_201704 = wordcloud_by_date(cleaned, year=2017, month=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter_04, word_freq_04 = wordcloud_by_date(cleaned, month=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> What happened on April 1st, 2nd? -> DATAIRL(DATA In Real Life)\n\n> coronavirus is overwhelming"},{"metadata":{},"cell_type":"markdown","source":"# 6. Word Embedding<a id=\"6\"></a>\n<p style=\"text-align:right;\"><a href=\"#top\">üîù top</a></p>"},{"metadata":{},"cell_type":"markdown","source":"## 6.1. Count Vectorizer<a id=\"6.1\"></a>\n<p style=\"text-align:right;\"><a href=\"#top\">üîù top</a></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_vectorize(dataset):\n    \n    vectorizer = CountVectorizer()\n    \n    documents = list()\n    for title in tqdm(dataset['title_cleaned']):\n        documents.append(title)\n    document_vector = vectorizer.fit_transform(documents)\n    return vectorizer, document_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv, cv_encoded = count_vectorize(cleaned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_encoded.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, j in zip(cv_encoded.nonzero()[0][:30], cv_encoded.nonzero()[1][:30]):\n    print('({:4}, {:8}({:15})) -> {}'.format(i, j, cv.get_feature_names()[j], cv_encoded[i, j]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.2. TF-IDF Vectorizer<a id=\"6.2\"></a>\n<p style=\"text-align:right;\"><a href=\"#top\">üîù top</a></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tfidf_vectorize(dataset):\n    vectorizer = TfidfVectorizer()\n    \n    documents = list()\n    for title in tqdm(dataset['title_cleaned']):\n        documents.append(title)\n    document_vector = vectorizer.fit_transform(documents)\n    return vectorizer, document_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf, tfidf_encoded = tfidf_vectorize(cleaned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_encoded.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, j in zip(tfidf_encoded.nonzero()[0][:30], tfidf_encoded.nonzero()[1][:30]):\n    print('({:4}, {:8}({:15})) -> {}'.format(i, j, tfidf.get_feature_names()[j], tfidf_encoded[i, j]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Data Modeling<a id=\"7\"></a>\n<p style=\"text-align:right;\"><a href=\"#top\">üîù top</a></p>"},{"metadata":{},"cell_type":"markdown","source":"## 7.1. Dimension Reduction<a id=\"7.1\"></a>\n<p style=\"text-align:right;\"><a href=\"#top\">üîù top</a></p>"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"oc = cleaned[cleaned['original_content'] == 1]\nnoc = cleaned[cleaned['original_content'] == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scatter(data, x):\n    oc = data[data['original_content'] == 1]\n    noc = data[data['original_content'] == 0]\n\n    plt.figure(figsize=(10, 10))\n    plt.scatter(x[oc.index, 0], x[oc.index, 1], color='red', label='Original Content')\n    plt.scatter(x[noc.index, 0], x[noc.index, 1], color='blue', label='Not Original Content')\n    plt.legend()\n    plt.title('Sample 2-Dimension Features')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Truncated SVD(for Sparse Data)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def svd(encoded, dimension=50):\n    svd = TruncatedSVD(n_components=dimension, n_iter=10, random_state=2020)\n    reduced = svd.fit_transform(encoded)\n    return svd, reduced","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svd50, svd50_reduced = svd(tfidf_encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svd50_reduced.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scatter(cleaned, svd50_reduced)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true},"cell_type":"markdown","source":"**T-SNE**<br>\nTooooooo Many Times needed"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tsne(encoded, dimension=2):\n    tsne = TSNE(n_components=dimension, verbose=5, random_state=2020, n_jobs=4)\n    reduced = tsne.fit_transform(encoded)\n    return tsne, reduced","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tsne2, tsne2_reduced = tsne(svd50_reduced)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# tsne2_reduced.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"# scatter(cleaned, tsne2_reduced)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7.2. Classification<a id=\"7.2\"></a>\n<p style=\"text-align:right;\"><a href=\"#top\">üîù top</a></p>"},{"metadata":{},"cell_type":"markdown","source":"Is it possible to discriminate Original Content vs Non Original Content by title? -> About 60% Accuracy, may need model tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = svd50_reduced\nY = np.array(cleaned['original_content'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(\n    X, Y,\n    test_size=0.2,\n    stratify=Y\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gradient_boosting_model(x_train, y_train, x_test, y_test):\n    model = GradientBoostingClassifier(learning_rate=0.01, n_estimators=100, random_state=2020, verbose=1)\n    scores = cross_validate(model, x_train, y_train, scoring='accuracy', cv=2, return_train_score=True, verbose=1)\n\n    model.fit(x_train, y_train)\n    acc = accuracy_score(model.predict(x_test), y_test)\n    \n    return model, scores, acc","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"gb_model, gb_scores, gb_acc = gradient_boosting_model(x_train, y_train, x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Validation Accuracies: {}'.format(gb_scores['test_score']))\nprint('Test Accuracy: {}'.format(gb_acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7.3. Topic Modeling<a id=\"7.3\"></a>\n<p style=\"text-align:right;\"><a href=\"#top\">üîù top</a></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lda(encoded, n_topic=10):\n    lda = LatentDirichletAllocation(n_components=n_topic, verbose=1, random_state=2020)\n    lda.fit(encoded)\n    return lda","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"lda10 = lda(tfidf_encoded, n_topic=10)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"for idx, topic in enumerate(lda10.components_):\n    words = [tfidf.get_feature_names()[topic_id] for topic_id in topic.argsort()[::-1][:10]]\n    print('Topic {:2d} -> {}'.format(idx, words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_df = cleaned.copy()\nlength = cleaned['title_cleaned'].shape[0]\nfor idx, title in tqdm(enumerate(cleaned['title_cleaned'])):\n    encoded = tfidf_encoded[idx]\n    topics = lda10.transform(encoded)\n    topic = topics.argsort()[0][::-1][0]\n\n    topic_df.loc[idx, 'topic'] = topic\n    topic_df.loc[idx, 'topic_value'] = topics[0][topic]\n\n    if idx % 30000 == 0 or idx == length - 1:\n        print('Topic {:2d}({:.6f}) {:}'.format(topic, topics[0][topic], title))\ntopic_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_df['topic'] = topic_df['topic'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 12))\nsns.countplot(topic_df['topic'])\nplt.xlabel('Topic')\nplt.title('Topic Counter')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svd2, svd2_reduced = svd(tfidf_encoded, dimension=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svd2_reduced.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scatter(cleaned, svd2_reduced)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 12))\nfor topic in sorted(topic_df['topic'].unique()):\n    index = topic_df[topic_df['topic'] == topic].index\n    sns.scatterplot(x=svd2_reduced[index, 0], y=svd2_reduced[index, 1], label=str(topic), s=100)\nplt.legend()\nplt.title('Topic Distribution in 2-d representation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}