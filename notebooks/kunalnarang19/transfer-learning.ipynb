{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This notebook tries to find a good set of hyperparameters and model for each file.\nWe would do 5 experiments in this:\n1. We would train a custom CNN model for 15 epochs\n2. Then, we would train the same CNN for 50 epochs with early stopping and see which one performs better\n3. Then, we would train a deeper CNN for 15 epochs and compare its results with the steps 1 and 2.\n4. We would then use transfer learning with VGG16 to try and improve the accuracy of the RGB images.\n5. Then, we would do on the fly image augmentation on each file with the best model that was found from steps 1 to 4 and compare it with all the previous results. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pickle\nimport matplotlib.pyplot as plt\nfrom math import sqrt, ceil\nfrom timeit import default_timer as timer\n\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, AvgPool2D, BatchNormalization, Reshape\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint\n\nimport os\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change these\nchoice = 0        \n\ndata_filename = f\"/kaggle/input/traffic-signs-preprocessed/data{choice}.pickle\"\npkl_filename = f\"/saved_models/data{choice}_model.pkl\"\n\nplot_acc = f\"/training_plots/data{choice}_acc.png\"\nplot_loss = f\"/training_plots/data{choice}_loss.png\"\n\nfile_training_validation_results = f\"/results/train_validate/data{choice}_train_validate_results.txt\"\nfile_testing_results = f\"/results/test/data{choice}_test_results.txt\"\nbest_model_results = f\"/results/best_models/data{choice}_best_model.txt\"\n# Change these: ends","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 15\nepoch_step_size = 3\n\nactivations = [\"sigmoid\",\"tanh\",\"relu\"]\ndropouts = [0.1,0.3,0.5]\noptimizers = ['adam','sgd']\nneurons = [32, 64, 128]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"annealer = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + epochs))\nes = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=1, restore_best_weights=True)\nmc = tf.keras.callbacks.ModelCheckpoint(f\"model_data{choice}.h5\", monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n\nmodels = {}\nhistory = {}\n\nbest_activation = -1\nbest_dropout = -1\nbest_optimizer = \"\"\nbest_neuron = -1\nbest_acc = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_model(history, activation, dropout, optimizer, neurons):\n    plt.figure(1)\n\n    plt.plot(history.history['acc'], 'b', label='Training Accuracy')\n    plt.plot(history.history['val_acc'], 'r', label='Validation Accuracy')\n    plt.legend(loc='upper right')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epochs')\n    plt.xticks(np.arange(0,epochs,step=epoch_step_size))\n    plt.title('Accuracy Curves for Activation: {0}, Dropout: {1}, Optimizer: {2}, Neurons: {3}'.format(activation,dropout,optimizer,neuron))\n    plt.savefig(plot_acc, bbox_inches='tight')\n\n    plt.figure(2)\n    plt.plot(history.history['loss'], 'b', label='Training Loss')\n    plt.plot(history.history['val_loss'], 'r', label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.ylabel('Loss')\n    plt.xlabel('Epochs')\n    plt.xticks(np.arange(0,epochs,step=epoch_step_size))\n    plt.title('Loss Curves for Activation: {0}, Dropout: {1}, Optimizer: {2}, Neurons: {3}'.format(activation,dropout,optimizer,neuron))\n    plt.savefig(plot_loss, bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(activation=\"relu\", dropout=0.0,optimizer=\"adam\",neurons=128, channel=1):\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Conv2D(neurons, kernel_size=3, padding='same', activation=activation, input_shape=(32, 32, channel)))\n    model.add(tf.keras.layers.MaxPool2D(pool_size=2))\n    model.add(tf.keras.layers.Dropout(dropout))\n\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(256, activation=activation))\n    model.add(tf.keras.layers.Dense(256, activation=activation))\n    model.add(tf.keras.layers.Dense(43, activation='softmax'))\n    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Execution starts here\n\n# with tf.device('/device:GPU:0'):\nwith open(data_filename, 'rb') as f:\n    data = pickle.load(f, encoding='latin1')  # dictionary type\n\ny_train = data['y_train']\ny_val = data['y_validation']\ny_test = data['y_test']\n\n# Making channels come at the end\nX_train = data['x_train'].transpose(0, 2, 3, 1)\nX_val = data['x_validation'].transpose(0, 2, 3, 1)\nX_test = data['x_test'].transpose(0, 2, 3, 1)\n\n\nCHANNEL = X_train.shape[-1]       # 1 for grayscale, 3 for RGB\n\n\n# data[\"x_train\"] = np.concatenate((data[\"x_train\"],data[\"x_validation\"]),axis=0)\n# data[\"y_train\"] = np.concatenate((data[\"y_train\"],data[\"y_validation\"]),axis=0)\n\ntraining_file = open(file_training_validation_results, \"w+\")\n\nfor activation in activations:\n    models[activation] = {}\n    for dropout in dropouts:\n        models[activation][dropout] = {}\n        for optimizer in optimizers:\n            models[activation][dropout][optimizer] = {}\n            for neuron in neurons:\n                train_msg = \"Training with params {0}, {1}, {2}, {3}\".format(activation,dropout,optimizer,neuron)\n                print(train_msg)\n                model = create_model(activation=activation,dropout=dropout,optimizer=optimizer,neurons=neuron, channel=CHANNEL)\n                model.fit(data['x_train'], data['y_train'],batch_size=512, epochs = epochs, validation_split=0.3,callbacks=[annealer, es, mc])\n                models[activation][dropout][optimizer][neuron] = model\n                train_result = \"Training Accuracy = {0}, Validation Accuracy = {1}\".format(model.history.history[\"acc\"][-1],model.history.history[\"val_acc\"][-1])\n                training_file.write(train_msg + \"\\n\" + train_result + \"\\n========\\n\")\n\ntraining_file.close()\n\ntesting_file = open(file_testing_results, \"w+\")\n\n\"\"\"# Calculating accuracy with testing dataset\"\"\"\n\n\nfor activation in activations:\n    for dropout in dropouts:\n        for optimizer in optimizers:\n            for neuron in neurons:\n                temp = models[activation][dropout][optimizer][neuron].predict(data['x_test'])\n                temp = np.argmax(temp, axis=1)\n\n                temp = np.mean(temp == data['y_test'])\n                if temp > best_acc:\n                    best_acc = temp\n                    best_activation = activation\n                    best_dropout = dropout\n                    best_optimizer = optimizer\n                    best_neuron = neuron\n\n                test_result = \"Test Accuracy = {0} for the model: Activation={1}, Dropout={2}, Optimizer={3}, Neurons={4}\".format(temp,activation,dropout,optimizer,neuron)\ntesting_file.write(test_result + \"\\n========\\n\")\ntesting_file.close()\n\nbest_m = open(best_model_results, \"w+\")\nbest_result = \"BEST MODEL\\nTest Accuracy = {0} for the model: Activation={1}, Dropout={2}, Optimizer={3}, Neurons={4}\".format(best_acc,best_activation,best_dropout,best_optimizer,best_neuron)\nbest_m.write(best_result)\nbest_m.close()\n\nbest_model = models[best_activation][best_dropout][best_optimizer][best_neuron]\nplot_model(best_model.history,best_activation, best_dropout, best_optimizer, best_neuron)\n\nwith open(pkl_filename, 'wb') as file:\n  pickle.dump(best_model, file)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}