{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nfrom imblearn.over_sampling import SMOTE\nimport seaborn as sns\nimport scikitplot as skplt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\n\n\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import accuracy_score, recall_score, roc_auc_score, precision_score, f1_score\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\n#Explore data\ndataset = pd.DataFrame(dataset)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 資料查看","metadata":{}},{"cell_type":"code","source":"dataset.info()\ndataset.drop(columns=['id']).describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"從編號第8列可以看出bmi是有缺少資料的狀況","metadata":{}},{"cell_type":"markdown","source":"# 資料缺失","metadata":{}},{"cell_type":"code","source":"#托曼·康斯坦丁（Thoman Konstantin）提供https：//www.kaggle.com/thomaskonstantin/analyzing-and-modeling-stroke-data\nDT_bmi_pipe = Pipeline( steps=[ \n                               ('scale',StandardScaler()),\n                               ('lr',DecisionTreeRegressor(random_state=42))\n                              ])\nX = dataset[['age','gender','bmi']].copy()\nX.gender = X.gender.replace({'Male':0,'Female':1,'Other':-1}).astype(np.uint8)\n\nMissing = X[X.bmi.isna()]\nX = X[~X.bmi.isna()]\nY = X.pop('bmi')\nDT_bmi_pipe.fit(X,Y)\npredicted_bmi = pd.Series(DT_bmi_pipe.predict(Missing[['age','gender']]),index=Missing.index)\ndataset.loc[Missing.index,'bmi'] = predicted_bmi\nprint('Missing values: ',sum(dataset.isnull().sum()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.drop(columns=['id'])\nfor i in range(len(dataset.index)): #bmi\n    if dataset.iloc[i, 8] < 18.5:\n        dataset.iloc[i, 8] = 'Underweight'\n    elif dataset.iloc[i, 8] < 24.0 and dataset.iloc[i, 8] >= 18.5:\n        dataset.iloc[i, 8] = 'Normal weight'\n    elif dataset.iloc[i, 8] < 30.0 and dataset.iloc[i, 8] >= 24.0:\n        dataset.iloc[i, 8] = 'Overweight'\n    else:\n        dataset.iloc[i, 8] = 'Obese'\n\nfor i in range(len(dataset.index)):\n    if dataset.iloc[i, 7] < 100.0:\n        dataset.iloc[i, 7] = 'Normal'\n    elif dataset.iloc[i, 7] >= 100.0 and dataset.iloc[i, 7] < 125.0:\n        dataset.iloc[i, 7] = 'Prediabetes'\n    else:\n        dataset.iloc[i, 7] = 'Diabetes'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\n# dataset_fit = dataset.drop(columns=['age'])\ndataset_fit = dataset.apply(le.fit_transform)\n# dataset_fit.loc[:,'age'] = dataset['age']\ndataset_fit.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\ncorr_num = dataset_fit.corr()\n# print(corr_num)\ncorr_matrix = pd.DataFrame(corr_num)\nstroke_corr = corr_num.loc[\"stroke\"]\nprint(\"stroke_corr \\n\",stroke_corr.sort_values())\npx.imshow(corr_matrix)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(dataset_fit)\ndataset_fit.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"str_only = dataset_fit[dataset_fit['stroke'] == 1]\nno_str_only = dataset_fit[dataset_fit['stroke'] == 0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(20,15))\ngs = fig.add_gridspec(3, 2)\ngs.update(wspace=0.35, hspace=0.35)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\nax2 = fig.add_subplot(gs[1, 0])\nax3 = fig.add_subplot(gs[1, 1])\nax4 = fig.add_subplot(gs[2, 0])\nax5 = fig.add_subplot(gs[2, 1])\n\nbackground_color = \"#ffffff\"\ncolor_y = '#0d7aff' #藍中風\ncolor_n = '#ffa230' #橘無中風\nfig.patch.set_facecolor(background_color) \nax0.set_facecolor(background_color) \nax1.set_facecolor(background_color) \nax2.set_facecolor(background_color)\nax3.set_facecolor(background_color) \nax4.set_facecolor(background_color) \nax5.set_facecolor(background_color) \n\n\nfor s in [\"top\",\"right\",\"left\"]:\n    ax0.spines[s].set_visible(False)\n    ax1.spines[s].set_visible(False)\n    ax2.spines[s].set_visible(False)\n    ax3.spines[s].set_visible(False)\n    ax4.spines[s].set_visible(False)\n    ax5.spines[s].set_visible(False)\n\n## Age\n\ntemp_df = pd.DataFrame(str_only[\"age\"])\nno_temp_df = pd.DataFrame(no_str_only[\"age\"])\nsns.kdeplot(temp_df[\"age\"], ax=ax0,color=color_y, shade=True)\nsns.kdeplot(no_temp_df[\"age\"], ax=ax0, color=color_n, shade=True)\nax0.yaxis.set_major_locator(mtick.MultipleLocator(2))\nax0.set_ylabel('')    \nax0.set_xlabel('')\n\n# heart_disease\n\ntemp_df = pd.DataFrame(str_only[\"heart_disease\"].value_counts())\ntemp_df[\"Percentage\"] = temp_df[\"heart_disease\"].apply(lambda x: x/sum(temp_df[\"heart_disease\"])*100)\nno_temp_df = pd.DataFrame(no_str_only[\"heart_disease\"].value_counts())\nno_temp_df[\"Percentage\"] = no_temp_df[\"heart_disease\"].apply(lambda x: x/sum(no_temp_df[\"heart_disease\"])*100)\n\nx = np.arange(len(temp_df))\nax1.grid(color='gray', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\nax1.bar(x, height=temp_df[\"Percentage\"], zorder=3, color=color_y, width=0.4)\nax1.bar(x+0.4, height=no_temp_df[\"Percentage\"], zorder=3, color=color_n, width=0.4)\nax1.set_xticks(x + 0.4 / 2)\nax1.set_xticklabels(['No History','History'])\nax1.yaxis.set_major_formatter(mtick.PercentFormatter())\nax1.yaxis.set_major_locator(mtick.MultipleLocator(20))\nfor i,j in zip([0, 1], temp_df[\"Percentage\"]):\n    ax1.annotate(f'{j:0.0f}%',xy=(i, j/2), color=background_color, horizontalalignment='center', verticalalignment='center')\nfor i,j in zip([0, 1], no_temp_df[\"Percentage\"]):\n    ax1.annotate(f'{j:0.0f}%',xy=(i+0.4, j/2), color=background_color, horizontalalignment='center', verticalalignment='center')\n    \n# hypertension\n\ntemp_df = pd.DataFrame(str_only[\"hypertension\"].value_counts())\ntemp_df[\"Percentage\"] = temp_df[\"hypertension\"].apply(lambda x: x/sum(temp_df[\"hypertension\"])*100)\n\nno_temp_df = pd.DataFrame(no_str_only[\"hypertension\"].value_counts())\nno_temp_df[\"Percentage\"] = no_temp_df[\"hypertension\"].apply(lambda x: x/sum(no_temp_df[\"hypertension\"])*100)\n\nx = np.arange(len(temp_df))\n\nax2.grid(color='gray', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\nax2.bar(x, height=temp_df[\"Percentage\"], zorder=3, color=color_y, width=0.4)\nax2.bar(x+0.4, height=no_temp_df[\"Percentage\"], zorder=3, color=color_n, width=0.4)\nax2.set_xticks(x + 0.4 / 2)\nax2.set_xticklabels(['No History','History'])\nax2.yaxis.set_major_formatter(mtick.PercentFormatter())\nax2.yaxis.set_major_locator(mtick.MultipleLocator(20))\nfor i,j in zip([0, 1], temp_df[\"Percentage\"]):\n    ax2.annotate(f'{j:0.0f}%',xy=(i, j/2), color=background_color, horizontalalignment='center', verticalalignment='center')\n\nfor i,j in zip([0, 1], no_temp_df[\"Percentage\"]):\n    ax2.annotate(f'{j:0.0f}%',xy=(i+0.4, j/2), color=background_color, horizontalalignment='center', verticalalignment='center')\n\n# ever_married\ntemp_df = pd.DataFrame(str_only[\"ever_married\"].value_counts())\ntemp_df[\"Percentage\"] = temp_df[\"ever_married\"].apply(lambda x: x/sum(temp_df[\"ever_married\"])*100)\n\nno_temp_df = pd.DataFrame(no_str_only[\"ever_married\"].value_counts())\nno_temp_df[\"Percentage\"] = no_temp_df[\"ever_married\"].apply(lambda x: x/sum(no_temp_df[\"ever_married\"])*100)\n\nx = np.arange(len(temp_df))\n\n\nax3.grid(color='gray', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\nax3.bar(x, height=temp_df[\"Percentage\"], zorder=3, color=color_y, width=0.4)\nax3.bar(x+0.4, height=no_temp_df[\"Percentage\"], zorder=3, color=color_n, width=0.4)\nax3.set_xticks(x + 0.4 / 2)\nax3.set_xticklabels(['No Married','Married'])\nax3.yaxis.set_major_formatter(mtick.PercentFormatter())\nax3.yaxis.set_major_locator(mtick.MultipleLocator(10))\nfor i,j in zip([0, 1], temp_df[\"Percentage\"]):\n    ax3.annotate(f'{j:0.0f}%',xy=(i, j/2), color=background_color, horizontalalignment='center', verticalalignment='center')\nfor i,j in zip([0, 1], no_temp_df[\"Percentage\"]):\n    ax3.annotate(f'{j:0.0f}%',xy=(i+0.4, j/2), color=background_color, horizontalalignment='center', verticalalignment='center')\n\n\n# smoking_status\ntemp_df = pd.DataFrame(str_only[\"smoking_status\"].value_counts())\ntemp_df[\"Percentage\"] = temp_df[\"smoking_status\"].apply(lambda x: x/sum(temp_df[\"smoking_status\"])*100)\nno_temp_df = pd.DataFrame(no_str_only[\"smoking_status\"].value_counts())\nno_temp_df[\"Percentage\"] = no_temp_df[\"smoking_status\"].apply(lambda x: x/sum(no_temp_df[\"smoking_status\"])*100)\n\n\nax4.barh(temp_df.index, temp_df['Percentage'], color=color_y, zorder=3, height=0.7)\nax4.barh(no_temp_df.index, no_temp_df['Percentage'], color=color_n, zorder=3, height=0.3)\nax4.xaxis.set_major_formatter(mtick.PercentFormatter())\nax4.xaxis.set_major_locator(mtick.MultipleLocator(10))\n\n\n# bmi\n\ntemp_df = pd.DataFrame(str_only[\"bmi\"].value_counts())\ntemp_df[\"Percentage\"] = temp_df[\"bmi\"].apply(lambda x: x/sum(temp_df[\"bmi\"])*100)\nno_temp_df = pd.DataFrame(no_str_only[\"bmi\"].value_counts())\nno_temp_df[\"Percentage\"] = no_temp_df[\"bmi\"].apply(lambda x: x/sum(no_temp_df[\"bmi\"])*100)\n\n\nax5.barh(temp_df.index, temp_df['Percentage'], color=color_y, zorder=3, height=0.7)\nax5.barh(no_temp_df.index, no_temp_df['Percentage'], color=color_n, zorder=3, height=0.3)\nax5.xaxis.set_major_formatter(mtick.PercentFormatter())\nax5.xaxis.set_major_locator(mtick.MultipleLocator(10))\n\n\n\n#text\n\nfont_size = 16\nfont_weight = 'bold'\nfont_family =  'Arial'\nax0.text(-22, 0.041, 'Age', fontsize=font_size ,fontweight=font_weight,fontfamily=font_family)\n\nax1.text(-0.4, 110, 'Heart Disease', fontsize=font_size ,fontweight=font_weight,fontfamily=font_family)\n\nax2.text(-0.4, 100, 'Hypertension', fontsize=font_size ,fontweight=font_weight,fontfamily=font_family)\n\nax3.text(-0.4, 100, 'Ever Married', fontsize=font_size ,fontweight=font_weight,fontfamily=font_family)\n\n\nax4.text(-3, 3.5, 'Smoking Status', fontsize=font_size ,fontweight=font_weight,fontfamily=font_family)\n\n\nax5.text(-3, 3.5, 'bmi', fontsize=font_size ,fontweight=font_weight,fontfamily=font_family)\n\n\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_fit['stroke'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n# 訓練集分割","metadata":{}},{"cell_type":"code","source":"X  = dataset_fit[['age','heart_disease','hypertension','ever_married','smoking_status','bmi']]\ny = dataset_fit['stroke']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nsm = SMOTE()\nX_train_resh, y_train_resh = sm.fit_resample(X_train, y_train.ravel())\nX_test_resh, y_test_resh = sm.fit_resample(X_test, y_test.ravel())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 訓練","metadata":{}},{"cell_type":"code","source":"svm_pipeline = Pipeline(steps = [('SVM',SVC(random_state=42))])\n\nrf_pipeline = Pipeline(steps = [('RF',RandomForestClassifier(random_state=42))])\n\nDT_pipeline = Pipeline(steps = [('DT',DecisionTreeClassifier(random_state=42))])\n\nXGB_pipeline = Pipeline(steps = [('XGB',XGBClassifier(random_state=42))])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"交叉驗證","metadata":{}},{"cell_type":"code","source":"svm_pipeline_mean = cross_val_score(svm_pipeline,X_train_resh,y_train_resh,scoring='f1').mean()\n\nrf_pipeline_mean = cross_val_score(rf_pipeline,X_train_resh,y_train_resh,scoring='f1').mean()\n\nDT_pipeline_mean = cross_val_score(DT_pipeline,X_train_resh,y_train_resh,scoring='f1').mean()\n\nXGB_pipeline_mean = cross_val_score(XGB_pipeline,X_train_resh,y_train_resh,scoring='f1').mean()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"交叉驗證10次","metadata":{}},{"cell_type":"code","source":"svm_pipeline_mean_cv = cross_val_score(svm_pipeline,X_train_resh,y_train_resh,cv=10,scoring='f1').mean()\n\nrf_pipeline_mean_cv = cross_val_score(rf_pipeline,X_train_resh,y_train_resh,cv=10,scoring='f1').mean()\n\nDT_pipeline_mean_cv = cross_val_score(DT_pipeline,X_train_resh,y_train_resh,cv=10,scoring='f1').mean()\n\nXGB_pipeline_mean_cv = cross_val_score(XGB_pipeline,X_train_resh,y_train_resh,cv=10,scoring='f1').mean()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Mean f1 scores:')\nprint('SVM mean :',svm_pipeline_mean)\nprint('Random Forest mean :',rf_pipeline_mean)\nprint('DecisionTreeClassifier mean :',DT_pipeline_mean)\nprint('XGBoost mean :',XGB_pipeline_mean)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Mean f1 scores(After cv = 10):')\nprint('SVM mean :',svm_pipeline_mean_cv)\nprint('Random Forest mean :',rf_pipeline_mean_cv)\nprint('DecisionTreeClassifier mean :',DT_pipeline_mean_cv)\nprint('XGBoost mean :',XGB_pipeline_mean_cv)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"markdown","source":"找尋最佳參數","metadata":{}},{"cell_type":"code","source":"n_estimators =[64,100,128]\nmax_features = ['auto',2,3,5,7]\nbootstrap = [True,False]\n\nparam_grid = {'n_estimators':n_estimators,\n             'max_features':max_features,\n             'bootstrap':bootstrap}\nrfc = RandomForestClassifier()\ngrid = GridSearchCV(rfc,param_grid)\n\ngrid.fit(X_train_resh, y_train_resh)\ngrid.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf4 = RandomForestClassifier(bootstrap = True, max_features = 'auto', n_estimators = 128)\nrf4.fit(X_train_resh, y_train_resh)\npredprob4 = rf4.predict(X_test)\nrf4_f10  = f1_score(y_test,predprob4)\nprint('RF mean :',rf4_f10)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,predprob4))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}