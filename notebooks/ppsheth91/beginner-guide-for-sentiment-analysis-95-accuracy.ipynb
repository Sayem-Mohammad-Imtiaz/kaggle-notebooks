{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading the data and understanding the problem Statement "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Objective of the problem : To identify the hatespeech in the twitter tweets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the train Data #\n\ndf = pd.read_csv(\"../input/twitter-sentiment-analysis-hatred-speech/train.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Size of the data #\n\ndf.shape\n\n# 31,962 - records and 2 independent features and 1 target variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the train Data #\n\ndf1 = pd.read_csv(\"../input/twitter-sentiment-analysis-hatred-speech/test.csv\")\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Size of the data #\n\ndf1.shape\n\n# 17,197- records and 2 independent features ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Types of sentiments : total classes available for target variable (labels) : 2 classes\n\ndf['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Understanding the sentiments in the data #\n\ndf[df['label']==0].head(20) \n\n# Class 0 : shows the list of positive sentiments\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Understanding the sentiments in the data #\n\ndf[df['label']==1].head(20) \n\n# Class 1  : Negative sentiments of the user ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Problem Statement : Classify the tweets into positive or a negative sentiments (hate speech)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the missing values # - Train\n\ndf.isnull().sum()\n\n# No missing values ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can drop the ID column as it does not have any importance in builiding the model #\n\ndf.drop(['id'],axis=1,inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA - Understanding the twitter sentiment data #"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.heatmap(df.isnull(), yticklabels = False, cbar = False, cmap = 'Blues')\n\n# There no missing values #","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install WordCloud\n\n# Visualize the most frequent words #","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  word cloud library #\nfrom wordcloud import WordCloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combining all tje records togather into a list #\n\nsentences = df['tweet'].tolist()\nlen(sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Joining sentences (combining all the sentences togather into one paragraph through string separation)\nsentences_as_single_string = \" \".join(sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(sentences_as_single_string))\n\n# From this plort, we can understand that positive words are more as compared to the negative words, as we already know that the data is \n# data is imbalanced that we habe more records of positive sentiments (29720) against the negative sentiments (2242)\n\n# Size of words shows the frequency of this word in this document #","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyzing the positive words from this positive sentiments # \n\npositive = df[df['label']==0]['tweet']\npositive","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Understanding positive words from positive sentiments #\n\npos = positive.tolist()\nsentences_as_single_positive = \" \".join(pos)\nplt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(sentences_as_single_positive))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Postive Words # \n\nSmile, good, great, beautiful, happy, love, etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyzing the negative words from this negative sentiments # \n\nnegative = df[df['label']==1]['tweet']\nnegative","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Understanding negative words from negative sentiments #\n\nneg = negative.tolist()\nsentences_as_single_negative = \" \".join(neg)\nplt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(sentences_as_single_negative))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Negative Words - Hate Speech # \n\nbigot, racist,hate, black, condemn, etc."},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing Steps #"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Cleaning : Removing the exclaimation, fullstops, commas, hashtags, symbols, hyphen, semicolon, etc. from this tweets\n\n# String.punctuation : All punctuation marks\nimport string\nstring.punctuation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing Punctuation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean(text):\n    remv_punc = [char for char in text.lower() if char not in string.punctuation]\n    remv_punc_join = ''.join(remv_punc)\n    \n    return remv_punc_join","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean(' @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df_clean = df['tweet'].apply(clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df_clean[6]\n\n# Hence we are able to clean the text data correctly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install nltk","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removal of StopWords fromt this text #\n\nimport nltk\nstopwords = nltk.corpus.stopwords.words('english')\nprint(stopwords[:10])\n# Belw is the list of StopWords in english language ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df = pd.DataFrame(tweets_df_clean)\ntweets_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stopwords treatment and converting the data into lpwer case #\ndef stop(text):\n    remv_stop = [a for a in text.split() if a.lower() not in stopwords]\n    remv_stop_join = ' '.join(remv_stop)\n    return remv_stop_join","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df['tweet'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop(tweets_df['tweet'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df_stopwords = tweets_df['tweet'].apply(stop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df_stopwords[:2]\n\n# all stopwords have been removed from this document#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df_stopwords = pd.DataFrame(tweets_df_stopwords)\ntweets_df_stopwords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying Stemming "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the Stemming to reduce the word to its root form #\n\n#Porter Stemmer : It is used to stem all the words ( inclduing the stopwords as well) so we have removed the stopwords as well\nfrom nltk.stem import PorterStemmer\nst = PorterStemmer()\n\ndef steming(text):\n    ste = [st.stem(word) for word in text.split()]\n    ste_join = ' '.join(ste)\n    return ste_join","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df_stem = tweets_df_stopwords['tweet'].apply(steming)\n\ntweets_df_stem[:2]\n\n# The dataset has been stemmed to its root word","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df_stopwords['tweet'][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying Lemmatization"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom nltk.stem import WordNetLemmatizer\n\nwl = WordNetLemmatizer()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef lematize(text):\n    ste = [wl.lemmatize(word) for word in text.split()]\n    ste_join = ' '.join(ste)\n    return ste_join","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lematize('user father dysfunctional selfish drags kids dysfunction run')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df_stopwords.iloc[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df_stem = pd.DataFrame(tweets_df_stem)\ntweets_df_stem.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Advanced preprocessing techniques : CountVectorizer , TfidfVectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the Count Vectorizer \n\nfrom sklearn.feature_extraction.text import CountVectorizer \n\ncv = CountVectorizer(max_features=5000)\n\nsen = tweets_df_stem['tweet'].tolist()\nlen(sen)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import DataFrame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def document_matrix(text, vectorizer):\n    mat = vectorizer.fit_transform(text)\n    return DataFrame(mat.toarray())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = document_matrix(sen,cv)\nm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tfidf Vectorizer #\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer \n\n\ntfidf_vec = TfidfVectorizer(max_features=2500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = document_matrix(sen,tfidf_vec)\nn.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting the data into dependent and independent variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting the data #\n\n# Independent and dependent vcariables #\n\ny= df['label']\ny.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting the data #\n\n# 1st Model : We will use the Count Vectorizer document\n\n\n# Splitting the data into test and train #\n\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(m,y,test_size=0.33,random_state=25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Optmization of the Model, through different hyper parameter tuning \n# Model 1 : Using the Count Vectorizer and stemming technique\n# Model 2 : Using the TFIDF Vectorizer and stemming technique","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 1 :  Using the Count Vectorizer and stemming technique"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Building #\n# Applying the Naive Bayes Algorithm #\n\nfrom sklearn.naive_bayes import MultinomialNB\nNaiveBclassifier = MultinomialNB()\nNaiveBclassifier.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting train cases\ny_pred_train = NaiveBclassifier.predict(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Accuract Score #\n\nacc = accuracy_score(y_train, y_pred_train)\nacc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting test cases\ny_pred_test = NaiveBclassifier.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred_test)\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix\n\nacc = accuracy_score(y_test, y_pred_test)\nacc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 2 : Using the TFIDF Vectorizer and stemming technique"},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting the data #\n\n# 2nd Model : We will use the TFIDF Vectorizer document\n\n# Splitting the data into test and train #\n\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(n,y,test_size=0.33,random_state=25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Building #\n# 2nd Model : We will use the TFIDF Vectorizer document\n\n# Applying the Naive Bayes Algorithm #\n\nfrom sklearn.naive_bayes import MultinomialNB\nNaiveBclassifier = MultinomialNB()\nNaiveBclassifier.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting train cases\ny_pred_train = NaiveBclassifier.predict(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Accuract Score #\n\nacc = accuracy_score(y_train, y_pred_train)\nacc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting test cases\ny_pred_test = NaiveBclassifier.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = accuracy_score(y_test, y_pred_test)\nacc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model can give an accuracy of 94.92% on any randomly selected data "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}