{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"os.chdir(\"E:\\Data Science\\R Programs\\Csv files\")\nos.getcwd()\ndf = pd.read_csv(\"diabetes.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Pre processing tghe data #\n\n# visualize the data #\n\ndf.info()\n\n#1)  there are no missing values #\n\ndf.Outcome.head() \n# 1 : patient has diabetes and 0 : patieny does not have diabtes #","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# 2) There are no duplicate entrries\n\ndf = df.drop_duplicates()\n\nlen(df)\n\n# ther are no duplocates enteroes in the data\n# 3) Check for data redundancy #, same column or different columns have same data, there is none","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# Data Visualization, to observe the relatiobnship between the data elements #\n\n# # needs to be relationship between independent & dependent variable  - visualization \n# 1. relationship between categorical & mumerical - box plot\n# 2. relationship between categprical & categoruical - bar chart (countplot)\n# 3. relationship between numerical & mumerical - scatter plot \n\n## relationship between independent variables - corr plot or heat map","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# The output variables is Outcome of the patient L: Diabetic or non-diabetic #\n# Visialization #\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.boxplot(x=df['Outcome'],y=df['Pregnancies'])\nplt.show()\n\n# There ia a relation between diabetes and pregancies# ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.boxplot(x=df['Outcome'],y=df['Glucose'])\nplt.show()\n\n# There ia a relation between diabetes and Glucose#  \n# Persons having high glucose are likely to have diabetes ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.boxplot(x=df['Outcome'],y=df['BloodPressure'])\nplt.show()\n\n# There ia a relation between diabetes and Glucose# ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.boxplot(x=df['Outcome'], y=df['SkinThickness'])\nplt.show() \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.boxplot(x=df['Outcome'], y=df['Insulin'])\nplt.show() \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.boxplot(x=df['Outcome'], y=df['BMI'])\nplt.show() \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.boxplot(x=df['Outcome'], y=df['DiabetesPedigreeFunction'])\nplt.show() \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.boxplot(x=df['Outcome'], y=df['Age'])\nplt.show() \n\n# Fronm this it can said all independent variables are significant as there is realatiobship between dependent and independent variables","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Splitting the dependent and independent variables #\n\nY = df['Outcome']\nY.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Independent variables #\nX = df.drop(['Outcome'],axis=1)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Correlation plot #\n\nX.corr()\n\n# All absolute values of correlation coefficents are less than 0.7 yhere is no multo collinearity","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We have multiple locations where there the observation is zero and value is missing #\n\na_Pregnancies = X.loc[X['Pregnancies']==0]\nprint(len(a_Pregnancies))\n\nb_Glucose = X.loc[X['Glucose']==0]\nprint(len(b_Glucose))\n\nc_BloodPressure = X.loc[X['BloodPressure']==0]\nprint(len(c_BloodPressure))\n\nd_SkinThickness = X.loc[X['SkinThickness']==0]\nprint(len(d_SkinThickness))\n\ne_Insulin = X.loc[X['Insulin']==0]\nprint(len(e_Insulin))\n\ne_BMI= X.loc[X['BMI']==0]\nprint(len(e_BMI))\n\ne_DiabetesPedigreeFunction= X.loc[X['DiabetesPedigreeFunction']==0]\nprint(len(e_DiabetesPedigreeFunction))\n\ne_Age= X.loc[X['Age']==0]\nprint(len(e_Age))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif[\"features\"] = X.columns\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We will impute the missing values with mean of that column #\n\nfrom sklearn.impute import SimpleImputer\n\nZero_values = SimpleImputer(missing_values=0, strategy ='mean')  # by default it will impite the missing values column wise#\n\nX = pd.DataFrame(Zero_values.fit_transform(X), columns = X.columns)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"a_Pregnancies = X.loc[X['Pregnancies']==0]\nprint(len(a_Pregnancies))\n\nb_Glucose = X.loc[X['Glucose']==0]\nprint(len(b_Glucose))\n\nc_BloodPressure = X.loc[X['BloodPressure']==0]\nprint(len(c_BloodPressure))\n\nd_SkinThickness = X.loc[X['SkinThickness']==0]\nprint(len(d_SkinThickness))\n\ne_Insulin = X.loc[X['Insulin']==0]\nprint(len(e_Insulin))\n\ne_BMI= X.loc[X['BMI']==0]\nprint(len(e_BMI))\n\ne_DiabetesPedigreeFunction= X.loc[X['DiabetesPedigreeFunction']==0]\nprint(len(e_DiabetesPedigreeFunction))\n\ne_Age= X.loc[X['Age']==0]\nprint(len(e_Age))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# Feature Engineering : \n# 1) Feature scaling : Minmax, standard scaler  : To bring the variabkes to the same scale\n# 2) Feature Extraction : PCA ( we will use PCA to reduce the no. of variabkes and reduce the multi collinearity)\n# 3) Feature transformation : converting catrgorical inyo numerical ( dont have any categorical variables)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Splitting the data frame into test abd train sets #\n\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.20,random_state=30)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_test.shape)\nprint(Y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Applying the Decision tree algorithm #\n\ndtc = DecisionTreeClassifier()\nmo = dtc.fit(X_train,Y_train)\nmo\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Predicting the train data set #\n\ny_train_pred = mo.predict(X_train)\n\nconf_tr = confusion_matrix(y_train_pred,Y_train)\nconf_tr\nAcc = accuracy_score(Y_train,y_train_pred)\nAcc","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# predictions on test data set #\n\ny_test_pred = mo.predict(X_test)\n\nconf_te = confusion_matrix(y_test_pred,Y_test)\nconf_te\n\nAcc = accuracy_score(y_test_pred,Y_test)\nprint(Acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Building the model with changes parameters #\n\ndt = DecisionTreeClassifier(criterion='gini',max_leaf_nodes=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# training #\n\nmo1 = dt.fit(X_train,Y_train)\nmo1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Predicting the train data set # # Acc -0.772\n\ny_train_pred = mo1.predict(X_train)\n\nconf_tr = confusion_matrix(y_train_pred,Y_train)\nconf_tr\nAcc = accuracy_score(Y_train,y_train_pred)\nAcc\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# predictions on test data set # Acc- 0.792\n\ny_test_pred = mo1.predict(X_test)\n\nconf_te = confusion_matrix(y_test_pred,Y_test)\nconf_te\n\nAcc = accuracy_score(y_test_pred,Y_test)\nprint(Acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nscore = roc_auc_score(y_test_pred,Y_test)\nscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Random Forest Model #\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=15,max_features=2, min_samples_leaf=18)\nrf","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"mo1 = rf.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred_train = mo1.predict(X_train)\nconf_tr = confusion_matrix(y_train_pred,Y_train)\nconf_tr\nAcc = accuracy_score(y_pred_train,Y_train)\nprint(Acc)\n\n# train_accuracy score : 0.802","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# test data set #\n\ny_pred_test = mo1.predict(X_test)\n\nacc = accuracy_score(y_pred_test,Y_test)\n\nprint(acc)\n\n# test_accuracy score : 0.766","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Hyper parameter tuning along wiht usung the randome forest alforirhm #\n\n# max_depth = 2,3,4\n#max_leaf_nodes = 4,5,6,7\n# min_sample_leaf = 15,18,20\n# max_features =2,3,4\n# n_estimators = 7,10,15,20\n\nparams = {'min_samples_leaf':[12,15,17,18,20],'n_estimators':[10,12,15,18,20,22],'max_features':[2,3,4]}\n\n# 'max_depth':[2,3,4], 'min_samples_leaf':[15,18], ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nr = RandomForestClassifier()\ngrid = GridSearchCV(r,params,cv=10)\nmo_gr = grid.fit(X_train,Y_train)\n\n# features : 3, n_esto : 10, max_doth =2, min_samp,_leaf :15, max_leaf_noed :5","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(grid.best_score_)  # best score with best parameters #\nprint(grid.best_params_)\nprint(grid.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"estimator = RandomForestClassifier(max_features=2, min_samples_leaf=20, n_estimators=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"score = cross_val_score(estimator,X_train,Y_train,cv=15)\nscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"score.mean() \n# mean score : 0.7586","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cp = mo_gr.best_estimator_\ncp","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# testing model of random forest # \n\ny_pred_test = cp.predict(X_test)\n\ncon = confusion_matrix(y_pred_test,Y_test)\n\n\nacc = accuracy_score(y_pred_test,Y_test)\nprint(acc)\n\n# best test accuracy : 0.7922 with parameters : 'max_features': 2, 'n_estimators': 15, 'min_samples_leaf': 18","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"cl = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features=3, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=12, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=12, n_jobs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# CV : 10, train_accuracy : 0.779, test acc : 0.791, {'max_features': 4, 'n_estimators': 10, 'min_samples_leaf': 12}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"params = {'min_samples_leaf':[9,10,12,15,17,18,20],'n_estimators':[7,10,12,15,18,20,22],'max_features':[2,3,4]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Random Search CV #\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nr = RandomForestClassifier()\ngrid = RandomizedSearchCV(estimator=r,param_distributions=params,cv=10,n_iter=20,n_jobs=-1,scoring='roc_auc')\nmo_rf = grid.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(mo_rf.best_score_)  # best score with best parameters #\nprint(mo_rf.best_params_)\nprint(mo_rf.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cp = mo_rf.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"mo = RandomForestClassifier(max_features=4, min_samples_leaf=12, n_estimators=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# checking the cross validated score for this model #\n\nscore = cross_val_score(mo,X_train,Y_train,cv=10)\nscore\nscore.mean()\n\n\n#score : 0.76\n\n# so if we take any random data, we can get an accuract of ~76%","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# testing model of random forest # \n\ny_pred_test = cp.predict(X_test)\n\ncon = confusion_matrix(y_pred_test,Y_test)\n\nacc = accuracy_score(y_pred_test,Y_test)\nprint(acc)\n\n# test accuracy of 0.75# ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We need to ave the model #\n\nimport pickle\n\npickle.dump(mo_rf,open('diabetes_predict.pkl','wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"a = pickle.load(open('diabetes_predict.pkl','rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test.iloc[0,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Y_test.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Lets test a data,whether it is abloe to predict correctly # for record no: 226\n\nd = a.predict([[8,151,78,32,210,43,0.5,36]])\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Lets test a data,whether it is abloe to predict correctly # for record no: 226\n\nc = a.predict([[3.0,111,56,39,155.5,30.1,0.557,30.0]])\nc","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Lets test a data,whether it is abloe to predict correctly # for record no: 226\n\nm = a.predict([[4.494673,101.0,76.0,29.15,155.54,35.70,0.19,26.0]])\nm","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Lets test a data,whether it is abloe to predict correctly # for record no: 610\n\nn = a.predict([[3,104,54,21,158,30.90,0.292,24.0]])\nn","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"m = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features=4, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=18, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n## Hyper Parameter Optimization\n\nparams={\n \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0,0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n    \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"## Hyperparameter optimization using RandomizedSearchCV\nfrom xgboost import XGBClassifier\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"! pip install xgboost","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import xgboost","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"classifier=xgboost.XGBClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"random_search=RandomizedSearchCV(classifier,param_distributions=params,n_iter=15,scoring='roc_auc',n_jobs=-1,cv=10,verbose=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from datetime import datetime\n# Here we go\nstart_time = timer(None) # timing starts from this point for \"start_time\" variable\nmodel = random_search.fit(X_train,Y_train)\ntimer(start_time) # timing ends here for \"start_time\" variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(model.best_score_)\nprint(model.best_params_)\nprint(model.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"a = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n              max_features=4, min_child_weight=1, min_samples_leaf=18,\n              missing=np.nan, monotone_constraints='()', n_estimators=22, n_jobs=0,\n              num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1,\n              scale_pos_weight=1, subsample=1, tree_method='exact',\n              validate_parameters=1, verbosity=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cp = model.best_estimator_\ncp","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# testing model of random forest #\n\ny_pred_test = cp.predict(X_test)\n\nacc = accuracy_score(y_pred_test,Y_test)\n\nprint(acc)\n\n# test_accuracy : 0.812","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}