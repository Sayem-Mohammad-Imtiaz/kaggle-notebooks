{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.metrics import plot_confusion_matrix\nfrom scipy.stats import norm, boxcox\nfrom collections import Counter\nfrom scipy import stats\nfrom pandas_profiling import ProfileReport\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Dataset","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n","metadata":{}},{"cell_type":"markdown","source":"## 1) Using Manual Methods","metadata":{}},{"cell_type":"code","source":"dataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.isnull().values.any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no Null Values in the Dataset","metadata":{}},{"cell_type":"markdown","source":"## Plotting Count for Qualities","metadata":{}},{"cell_type":"code","source":"sns.set_style('whitegrid')\nplt.figure(figsize=(12, 6))\nsns.countplot(x=\"quality\", data=dataset, palette='husl');","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finding Correlation among the variables","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 17))\nmatrix = np.triu(dataset.corr())\nsns.heatmap(dataset.corr(), annot=True,\n            linewidth=.8, mask=matrix, cmap=\"rocket\");","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualising Numerical Data","metadata":{}},{"cell_type":"code","source":"sns.catplot(x=\"quality\", y=\"fixed acidity\", data=dataset, kind=\"box\")\nsns.catplot(x=\"quality\", y=\"volatile acidity\", data=dataset, kind=\"box\")\nsns.catplot(x=\"quality\", y=\"citric acid\", data=dataset, kind=\"box\")\nsns.catplot(x=\"quality\", y=\"residual sugar\", data=dataset, kind=\"box\")\nsns.catplot(x=\"quality\", y=\"chlorides\", data=dataset, kind=\"box\")\nsns.catplot(x=\"quality\", y=\"density\", data=dataset, kind=\"box\")\nsns.catplot(x=\"quality\", y=\"pH\", data=dataset, kind=\"box\")\nsns.catplot(x=\"quality\", y=\"sulphates\", data=dataset, kind=\"box\")\nsns.catplot(x=\"quality\", y=\"alcohol\", data=dataset, kind=\"box\");","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Acidity Type with Different Qualities of Wine","metadata":{}},{"cell_type":"code","source":"acidity_count = dataset[\"fixed acidity\"].value_counts().reset_index()\nacidity_count","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(30, 10))\nplt.style.use(\"ggplot\")\nsns.barplot(x=acidity_count[\"index\"], y=acidity_count[\"fixed acidity\"])\nplt.title(\"TYPE OF ACIDITY WITH QUALITY\", fontsize=20)\nplt.xlabel(\"ACIDITY\", fontsize=20)\nplt.ylabel(\"COUNT\", fontsize=20)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution of pH with Different Qualities of Wine","metadata":{}},{"cell_type":"code","source":"plt.style.use(\"ggplot\")\nsns.displot(dataset[\"pH\"]);  # using displot here\nplt.title(\"DISTRIBUTION OF pH FOR DIFFERENT QUALITIES\", fontsize=18)\nplt.xlabel(\"pH\", fontsize=20)\nplt.ylabel(\"COUNT\", fontsize=20)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Skewness Correction","metadata":{}},{"cell_type":"markdown","source":"Here we will try to correct Skewness in some independent varaibles of our dataset","metadata":{}},{"cell_type":"code","source":"def skewnessCorrector(columnName):\n    print('''Before Correcting''')\n    (mu, sigma) = norm.fit(dataset[columnName])\n    print(\"Mu before correcting {} : {}, Sigma before correcting {} : {}\".format(\n        columnName.upper(), mu, columnName.upper(), sigma))\n    plt.figure(figsize=(20,10))\n    plt.subplot(1,2,1)\n    sns.distplot(dataset[columnName], fit=norm, color=\"orange\")\n    plt.title(columnName.upper() +\n              \" Distplot before Skewness Correction\", color=\"black\")\n    plt.subplot(1,2,2)\n    stats.probplot(dataset[columnName], plot=plt)\n    plt.show();\n    dataset[columnName], lam_fixed_acidity = boxcox(\n        dataset[columnName])\n    print('''After Correcting''')\n    print(\"Mu after correcting {} : {}, Sigma after correcting {} : {}\".format(\n        columnName.upper(), mu, columnName.upper(), sigma))\n    plt.figure(figsize=(20, 10))\n    plt.subplot(1,2,1)\n    sns.distplot(dataset[columnName], fit=norm, color=\"orange\")\n    plt.title(columnName.upper() +\n              \" Distplot After Skewness Correction\", color=\"black\")\n    plt.subplot(1,2,2)\n    stats.probplot(dataset[columnName], plot = plt)\n    plt.show();\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skewColumnList = [\n    'fixed acidity', 'residual sugar', 'free sulfur dioxide', 'total sulfur dioxide', 'sulphates'\n]\nfor columns in skewColumnList:\n    skewnessCorrector(columns)\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Outlier Correction\n","metadata":{}},{"cell_type":"markdown","source":"We have detected several outliers in our dataset here we will try to correct them.\n","metadata":{}},{"cell_type":"code","source":"def detect_outliers(columns):\n    outlier_indices = []\n\n    for column in columns:\n        # 1st quartile\n        Q1 = np.percentile(dataset[column], 25)\n        # 3st quartile\n        Q3 = np.percentile(dataset[column], 75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier Step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = dataset[(dataset[column] < Q1 - outlier_step)\n                              | (dataset[column] > Q3 + outlier_step)].index\n        # store indeces\n        outlier_indices.extend(outlier_list_col)\n\n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5)\n\n    return multiple_outliers\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"number of outliers detected --> \",\n      len(dataset.loc[detect_outliers(dataset.columns[:-1])]))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.loc[detect_outliers(dataset.columns[:-1])]","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dropping Outliers","metadata":{}},{"cell_type":"code","source":"dataset = dataset.drop(detect_outliers(dataset.columns[:-1]),axis = 0).reset_index(drop = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2) Using Pandas Profiling","metadata":{}},{"cell_type":"code","source":"!pip install pandas_profiling","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ProfileReport(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"- If quality value is less than or eqaul to 6 then it will be in class 0\n- If quality value is greater than 6  then it will be in class 1","metadata":{}},{"cell_type":"code","source":"dataset['quality'] = np.where(dataset['quality'] > 6, 1, 0)\ndataset['quality'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = dataset.iloc[:, 0:-1].values\ny = dataset.iloc[:, -1].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting Dataset into Training Set and Test Set","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Standardizing Independent Variables","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Classifiers on Training Set and drawing Inference","metadata":{}},{"cell_type":"code","source":"accuracy_scores = {}\ndef predictor(predictor, params):\n    global accuracy_scores\n    if predictor == 'lr':\n        print('Training Logistic Regression on Training Set')\n        from sklearn.linear_model import LogisticRegression\n        classifier = LogisticRegression(**params)\n\n    elif predictor == 'svm':\n        print('Training Support Vector Machine on Training Set')\n        from sklearn.svm import SVC\n        classifier = SVC(**params)\n\n    elif predictor == 'knn':\n        print('Training K-Nearest Neighbours on Training Set')\n        from sklearn.neighbors import KNeighborsClassifier\n        classifier = KNeighborsClassifier(**params)\n\n    elif predictor == 'dt':\n        print('Training Decision Tree Classifier on Training Set')\n        from sklearn.tree import DecisionTreeClassifier\n        classifier = DecisionTreeClassifier(**params)\n\n    elif predictor == 'nb':\n        print('Training Naive Bayes Classifier on Training Set')\n        from sklearn.naive_bayes import GaussianNB\n        classifier = GaussianNB(**params)\n\n    elif predictor == 'rfc':\n        print('Training Random Forest Classifier on Training Set')\n        from sklearn.ensemble import RandomForestClassifier\n        classifier = RandomForestClassifier(**params)\n\n    classifier.fit(X_train, y_train)\n\n    print('''Predicting Single Cell Result''')\n    single_predict = classifier.predict(sc.transform([[\n        7.4, 0.7, 0.0, 1.9, 0.076, 11.0, 34.0, 0.9978, 3.51, 0.56, 9.4\n    ]]))\n    if single_predict > 0 :\n        print('High Quality Wine')\n    else:\n        print('Low Quality Wine')\n    print('''Prediciting Test Set Result''')\n    y_pred = classifier.predict(X_test)\n    \n    result = np.concatenate((y_pred.reshape(len(y_pred), 1),\n                             y_test.reshape(len(y_test), 1)),1)\n    print(result, '\\n')\n    print('''Making Confusion Matrix''')\n    from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n    y_pred = classifier.predict(X_test)\n    cm = confusion_matrix(y_test, y_pred)\n    print(cm, '\\n')\n    plot_confusion_matrix(classifier, X_test, y_test, cmap=\"pink\")\n    print('True Positives :', cm[0][0])\n    print('False Positives :', cm[0][1])\n    print('False Negatives :', cm[1][0])\n    print('True Negatives :', cm[0][1], '\\n')\n\n    print('''Classification Report''')\n    print(classification_report(y_test, y_pred,\n          target_names=['0', '1'], zero_division=1))\n\n    print('''Evaluating Model Performance''')\n    accuracy = accuracy_score(y_test, y_pred)\n    print(accuracy, '\\n')\n\n    print('''Applying K-Fold Cross validation''')\n    from sklearn.model_selection import cross_val_score\n    accuracies = cross_val_score(\n        estimator=classifier, X=X_train, y=y_train, cv=10)\n    print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n    accuracy_scores[classifier] = accuracies.mean()*100\n    print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100), '\\n')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Logistic Regression on Training Set","metadata":{}},{"cell_type":"code","source":"predictor('lr', {'penalty': 'l1', 'solver': 'liblinear'})\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training SVM on Training Set","metadata":{}},{"cell_type":"code","source":"predictor('svm', {'C': .5, 'gamma': 0.8,\n          'kernel': 'linear', 'random_state': 0})\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Kernel SVM on Training Set","metadata":{}},{"cell_type":"code","source":"predictor('svm', {'C': .25, 'gamma': 0.1, 'kernel': 'rbf', 'random_state': 0})\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training K-Nearest Neighbours on Training Set","metadata":{}},{"cell_type":"code","source":"predictor('knn', {'algorithm': 'auto', 'n_jobs': 1,\n          'n_neighbors': 8, 'weights': 'distance'})\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Decision Tree on Training Set","metadata":{}},{"cell_type":"code","source":"predictor('dt', {'criterion': 'entropy', 'max_features': 'auto',\n          'splitter': 'best', 'random_state': 0})\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Naive Bayes on Training Set","metadata":{}},{"cell_type":"code","source":"predictor('nb', {})\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Random Forest Classifier on Training Set","metadata":{}},{"cell_type":"code","source":"predictor('rfc', {'criterion': 'gini', 'max_features': 'log2', 'n_estimators': 100,'random_state':0})\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finding which Classifier performed best","metadata":{}},{"cell_type":"code","source":"maxKey = max(accuracy_scores, key=lambda x: accuracy_scores[x])\nprint('The model with highest K-Fold Validation Accuracy score is  {0} with an accuracy of  {1:.2f}'.format(\n    maxKey, accuracy_scores[maxKey]))\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting Bar Chart for Accuracies of different classifiers","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nmodel_accuracies = list(accuracy_scores.values())\nmodel_names = ['LogisticRegression', 'SVC',\n               'K-SVC', 'KNN', 'Decisiontree', 'GaussianNB', 'RandomForest']\nsns.barplot(x=model_accuracies, y=model_names, palette='mako');\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summary\n- Random Forest Classifier performed best on this data set with an accuracy of 90.81%\n- K-Nearest Classifier was just behind with an accuracy of an accuracy of 90.56% \n\n# **Please give your feedback by commenting below.**","metadata":{}}]}