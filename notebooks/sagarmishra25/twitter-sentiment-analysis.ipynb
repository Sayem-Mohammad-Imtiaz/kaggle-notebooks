{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.shape)\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As shown above large number of tweets are not racist/sexist."},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_tweet = train_df['label'].value_counts()[0]\nneg_tweet = train_df['label'].value_counts()[1]\n\ntotal = len(train_df)\n\nprint(\"percentage of positive tweets : \", (pos_tweet/total)*100 )\nprint(\"percentage of negative tweets : \", (neg_tweet/total)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[train_df['label'] == 1].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preprocessing steps :**\n1. As twitter handles(@user) are not giving any useful information therefore we remove it.\n2. Remove html tags.\n3. Decontract text like won't => would not \n4. Removing punctuations, non-alphabetic character.\n5. remove small words like any, all, his, her as this words are not giving any useful information.\n6. lowercase the all the words.\n7. remove stop words.\n8. do stemming "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['tweet'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Decontract():\n    \n    pattern = [(r'won\\'t', ' will not'),\n                 (r'\\'s', ' is'),\n                 (r'don\\'t', ' do not'),\n                 (r'can\\'t', ' can not'),\n                 (r'n\\'t','not'),\n                 (r'\\'re','are'),\n                 (r'\\'d','would'),\n                 (r'\\'ll','will'),\n                 (r'\\'t','not'),\n                 (r'\\'ve','have'),\n                 (r'\\'m','am')\n                ]\n    \n    def __init__(self) :\n        pass\n        \n    def replace(self,text):\n        for (raw,replace) in self.pattern:\n            regex = re.compile(raw)\n            text = regex.sub(replace,text)\n        return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom bs4 import BeautifulSoup \nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\ndecontract = Decontract() \nstopWords = set(stopwords.words('english'))\n\ndef preprocessing(sentence) :\n    \n    #removing tweeter handles\n    sentence = re.sub(r'@[\\w]*','', sentence)\n    \n    #removing html tags\n    sentence = BeautifulSoup(sentence, 'html.parser').get_text()\n    \n    #Decontract words\n    sentence = decontract.replace(sentence)\n    \n    #remove non-alphabetic characters\n    sentence = re.sub('[^A-Za-z]+',\" \", sentence)\n    \n    #remove all words with length less than 3 and lower case the words\n    sentence = ' '.join([word.lower() for word in sentence.split() if len(word)>3])\n\n    #removing stop words\n    sentence = ' '.join([word for word in sentence.split() if word not in stopWords])\n    \n    #stemming the word using porterStemmer\n    sentence = ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(sentence)])\n    \n    return sentence\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['preprocessed_tweet'] = train_df['tweet'].apply(preprocessing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(['id','tweet'],axis = 1, inplace = True)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df['label']\ntrain_df.drop(['label'],axis = 1, inplace = True)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Splitting data into train,cv and test**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_df, y, test_size=0.33, stratify=y)\nX_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.33, stratify=y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Vectorizing text data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10)\nvectorizer.fit(X_train['preprocessed_tweet'].values)\n\nX_train_tfidf = vectorizer.transform(X_train['preprocessed_tweet'].values)\nX_cv_tfidf = vectorizer.transform(X_cv['preprocessed_tweet'].values)\nX_test_tfidf = vectorizer.transform(X_test['preprocessed_tweet'].values)\n\nprint(\"After vectorizations\")\nprint(X_train_tfidf.shape, y_train.shape)\nprint(X_cv_tfidf.shape, y_cv.shape)\nprint(X_test_tfidf.shape, y_test.shape)\nfeature_names = vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sentiments strength of tweet of X_train\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer \nfrom tqdm import tqdm\n\nsid = SentimentIntensityAnalyzer() \n\nneg_sentiment = []\nneu_sentiment = []\npos_sentiment = []\ncom_sentiment = []\n\nfor sentence in tqdm(X_train['preprocessed_tweet']):\n    vector = sid.polarity_scores(sentence)\n    neg_sentiment.append(vector['neg'])\n    neu_sentiment.append(vector['neu'])\n    pos_sentiment.append(vector['pos'])\n    com_sentiment.append(vector['compound'])\n    \nX_train_neg_sentiment = np.array(neg_sentiment).reshape(-1,1)\nX_train_neu_sentiment = np.array(neu_sentiment).reshape(-1,1)\nX_train_pos_sentiment = np.array(pos_sentiment).reshape(-1,1)\nX_train_com_sentiment = np.array(com_sentiment).reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sentiments strength of tweet of X_cv\n\nsid = SentimentIntensityAnalyzer() \n\nneg_sentiment = []\nneu_sentiment = []\npos_sentiment = []\ncom_sentiment = []\n\nfor sentence in tqdm(X_cv['preprocessed_tweet']):\n    vector = sid.polarity_scores(sentence)\n    neg_sentiment.append(vector['neg'])\n    neu_sentiment.append(vector['neu'])\n    pos_sentiment.append(vector['pos'])\n    com_sentiment.append(vector['compound'])\n    \nX_cv_neg_sentiment = np.array(neg_sentiment).reshape(-1,1)\nX_cv_neu_sentiment = np.array(neu_sentiment).reshape(-1,1)\nX_cv_pos_sentiment = np.array(pos_sentiment).reshape(-1,1)\nX_cv_com_sentiment = np.array(com_sentiment).reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sentiments strength of tweet of X_test\n\nsid = SentimentIntensityAnalyzer() \n\nneg_sentiment = []\nneu_sentiment = []\npos_sentiment = []\ncom_sentiment = []\n\nfor sentence in tqdm(X_test['preprocessed_tweet']):\n    vector = sid.polarity_scores(sentence)\n    neg_sentiment.append(vector['neg'])\n    neu_sentiment.append(vector['neu'])\n    pos_sentiment.append(vector['pos'])\n    com_sentiment.append(vector['compound'])\n    \nX_test_neg_sentiment = np.array(neg_sentiment).reshape(-1,1)\nX_test_neu_sentiment = np.array(neu_sentiment).reshape(-1,1)\nX_test_pos_sentiment = np.array(pos_sentiment).reshape(-1,1)\nX_test_com_sentiment = np.array(com_sentiment).reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import hstack\n\nX_train = hstack((X_train_neg_sentiment,X_train_neu_sentiment,X_train_pos_sentiment,X_train_com_sentiment,X_train_tfidf )).tocsr()\nX_cv = hstack((X_cv_neg_sentiment,X_cv_neu_sentiment,X_cv_pos_sentiment,X_cv_com_sentiment,X_cv_tfidf )).tocsr()\nX_test = hstack((X_test_neg_sentiment,X_test_neu_sentiment,X_test_pos_sentiment,X_test_com_sentiment,X_test_tfidf )).tocsr()\n\nprint(X_train.shape, y_train.shape)\nprint(X_cv.shape, y_cv.shape)\nprint(X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Modelling**"},{"metadata":{},"cell_type":"markdown","source":"1) Applying Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import roc_auc_score\n\ntrain_auc = []\ncv_auc = []\nalpha = [0.00001,0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n\nfor a in alpha:\n    clf = SGDClassifier(loss = 'log', penalty = 'l2' , alpha = a,class_weight = \"balanced\")\n    clf.fit(X_train,y_train)\n    y_train_pred = clf.predict_proba(X_train)[:,1]\n    y_cv_pred = clf.predict_proba(X_cv)[:,1]\n    \n    train_auc.append(roc_auc_score(y_train,y_train_pred))\n    cv_auc.append(roc_auc_score(y_cv, y_cv_pred))\n    \nplt.plot(np.log(alpha), train_auc, label = \"Train AUC\")\nplt.plot(np.log(alpha), cv_auc, label = \"CV AUC\")\n\nplt.scatter(np.log(alpha), train_auc, label = \"Train AUC points\")\nplt.scatter(np.log(alpha), cv_auc, label = \"CV AUC points\")\n\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"Error Loss\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_tfidf_best_alpha = 0.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve,auc\n\nlr = SGDClassifier(loss = 'log', alpha = lr_tfidf_best_alpha, penalty = 'l2',class_weight = \"balanced\")\nlr.fit(X_train, y_train)\n\ny_train_pred = lr.predict_proba(X_train)[:,1]\ny_test_pred = lr.predict_proba(X_test)[:,1]\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_cv, y_cv_pred)\n\ntrain_auc_tfidf_lr = auc(train_fpr,train_tpr)\ntest_auc_tfidf_lr = auc(test_fpr, test_tpr)\n\nplt.plot(train_fpr,train_tpr,label = \"Train Auc = \"+str(train_auc_tfidf_lr))\nplt.plot(test_fpr,test_tpr,label = \"Test Auc = \"+str(test_auc_tfidf_lr))\nplt.legend()\nplt.xlabel(\"fpr\")\nplt.ylabel(\"tpr\")\nplt.title(\"ROC\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nprint(\"Train Confusion matrix\")\ntrain_cm = confusion_matrix(y_train,lr.predict(X_train))\nax = sns.heatmap(train_cm, annot=True, fmt=\"d\", linewidths=.5, cbar=False)\nplt.xlabel('Predicted Class')\nplt.ylabel('Original Class')\nplt.title(\"Confusion matrix\")\nax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test Confusion matrix\")\ntrain_cm = confusion_matrix(y_test,lr.predict(X_test))\nax = sns.heatmap(train_cm, annot=True, fmt=\"d\", linewidths=.5, cbar=False)\nplt.xlabel('Predicted Class')\nplt.ylabel('Original Class')\nplt.title(\"Confusion matrix\")\nax","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2) linear SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\ntrain_auc = []\ncv_auc = []\n\nalpha = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\npenalty = ['l1','l2']\n\nparameters = {'alpha' : alpha, 'penalty' : penalty}\n\nsvm = SGDClassifier(loss = 'hinge', class_weight = 'balanced')\nclf = GridSearchCV(svm, parameters, scoring = 'roc_auc', cv=8)\nclf.fit(X_train, y_train) \n\nprint(clf.best_estimator_.alpha)\nprint(clf.best_estimator_.penalty)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(clf.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_svm_tfidf_best_alpha = 0.01\nlr_svm_tfidf_best_penalty = 'l2'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.calibration import CalibratedClassifierCV\n\nlr_svm = SGDClassifier(loss = 'hinge', class_weight = 'balanced', alpha = lr_svm_tfidf_best_alpha, penalty = lr_svm_tfidf_best_penalty )\n\n#configuring calibrated model to obtain output probabilities, because SGDClassifier with hinge los s don't give output probabilities \nlr_svm_cc = CalibratedClassifierCV(lr_svm, cv = 8)\nlr_svm_cc.fit(X_train,y_train)\n\ny_train_pred = lr_svm_cc.predict_proba(X_train)[:,1]\ny_test_pred = lr_svm_cc.predict_proba(X_test)[:,1]\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train,y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\ntrain_auc_tfidf_lr_svm = auc(train_fpr, train_tpr)\ntest_auc_tfidf_lr_svm = auc(test_fpr, test_tpr)\n\nplt.plot(train_fpr, train_tpr, label = \"Train Auc = \"+str(train_auc_tfidf_lr_svm))\nplt.plot(test_fpr, test_tpr, label = \"Test Auc = \"+str(test_auc_tfidf_lr_svm))\n\nplt.legend()\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.title(\"ROC\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3) Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\ntrain_auc = []\ncv_auc = []\n\nmax_depth = [1, 5, 10, 50, 100, 500, 1000]\nn_estimators  = [5, 10, 100, 500, 1000]\n\nfor estimator in n_estimators:\n    train_lst = []\n    cv_lst = []\n    for depth in max_depth:\n        clf = RandomForestClassifier(class_weight = \"balanced_subsample\", max_depth = depth, n_estimators = estimator, n_jobs = -1)\n        clf.fit(X_train, y_train)\n        \n        y_train_pred = clf.predict_proba(X_train)[:,1]\n        y_cv_pred = clf.predict_proba(X_cv)[:,1]\n        \n        train_lst.append(roc_auc_score(y_train,y_train_pred))\n        cv_lst.append(roc_auc_score(y_cv,y_cv_pred))\n        \n    train_auc.append(train_lst)\n    cv_auc.append(cv_lst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating dataframe of train auc for heatmap\ndf = pd.DataFrame(train_auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#heat map of train auc\nax = sns.heatmap(df, annot=True, linewidths=.5, cbar=False,yticklabels=n_estimators, xticklabels=max_depth)\nax\nplt.xlabel(\"max_depth\");\nplt.ylabel(\"n_estimators\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating data frame of cv_auc for heat map\ndf = pd.DataFrame(cv_auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#heat map of cv auc\nax = sns.heatmap(df, annot = True, linewidths=.5, cbar=False,yticklabels=n_estimators, xticklabels=max_depth)\nax\nplt.xlabel(\"max_depth\");\nplt.ylabel(\"n_estimators\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_best_max_depth = 10\nrf_best_n_estimators = 500","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(class_weight = \"balanced_subsample\", max_depth = rf_best_max_depth, n_estimators = rf_best_n_estimators, n_jobs = -1)\nrf.fit(X_train,y_train)\ny_train_pred = rf.predict_proba(X_train)[:,1]\ny_test_pred = rf.predict_proba(X_test)[:,1]\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\ntrain_auc_tfidf_rf = auc(train_fpr, train_tpr)\ntest_auc_tfidf_rf = auc(test_fpr, test_tpr)\n\nplt.plot(train_fpr, train_tpr, label = \"Train AUC \" + str(train_auc_tfidf_rf))\nplt.plot(test_fpr, test_tpr, label = \"Test AUC \" +str(test_auc_tfidf_rf))\nplt.legend()\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.title(\"ROC\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4) XGBOOST"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost.sklearn import XGBClassifier\n\ntrain_auc = []\ncv_auc = []\n\nmax_depth = [3,5,7,9]\nn_estimators = [5,10,100,500,1000]\n\nfor estimator in n_estimators :\n    train_lst = []\n    cv_lst = []\n    for depth in max_depth : \n        xgb_model = XGBClassifier(max_depth = depth, n_estimators = estimator, n_jobs = -1)\n        xgb_model.fit(X_train, y_train)\n        \n        y_train_pred = xgb_model.predict_proba(X_train)[:,1]\n        y_cv_pred = xgb_model.predict_proba(X_cv)[:,1]\n        \n        train_lst.append(roc_auc_score(y_train, y_train_pred))\n        cv_lst.append(roc_auc_score(y_cv, y_cv_pred))\n        \n    train_auc.append(train_lst)\n    cv_auc.append(cv_lst)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(train_auc)\n\nax = sns.heatmap(df, annot=True, linewidths=.5, cbar=False,yticklabels=n_estimators, xticklabels=max_depth)\nax\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"n_estimators\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(cv_auc)\nax = sns.heatmap(df, annot=True, linewidths=.5, cbar=False,yticklabels=n_estimators, xticklabels=max_depth)\nax\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"n_estimators\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_best_max_depth = 3\nrf_best_n_estimators = 500","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model = XGBClassifier(max_depth = rf_best_max_depth, n_estimators = rf_best_n_estimators, n_jobs = -1)\nxgb_model.fit(X_train, y_train)\n\ny_train_pred = xgb_model.predict_proba(X_train)[:,1]\ny_test_pred = xgb_model.predict_proba(X_test)[:,1]\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train,y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\ntrain_auc_tfidf_xgb = auc(train_fpr, train_tpr)\ntest_auc_tfidf_xgb = auc(test_fpr, test_tpr)\n\nplt.plot(train_fpr, train_tpr, label = \"Train AUC = \" + str(train_auc_tfidf_xgb))\nplt.plot(test_fpr, test_tpr, label = \"Test AUC = \" + str(test_auc_tfidf_xgb))\nplt.legend()\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.title(\"ROC\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Summary of each model with their respective AUC Score</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from prettytable import PrettyTable\n\nx = PrettyTable()\nx.field_names = [\"Model\", \"AUC\"]\nx.add_row([\"Logistic Regression\",test_auc_tfidf_lr])\nx.add_row([\"Linear SVM\",test_auc_tfidf_lr_svm])\nx.add_row([\"Random Forest\",test_auc_tfidf_rf])\nx.add_row([\"XGBoost\",test_auc_tfidf_xgb])\nprint(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation :** From above table it seems that XGBoost performs better than other models therefore we use XGBoost with other vectorizer or for prediction of test dataset."},{"metadata":{},"cell_type":"markdown","source":"<h3>Prediction on Test dataset using XGBoost</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['preprocessed_tweet'] = test_df['tweet'].apply(preprocessing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.drop(['id','tweet'],axis = 1, inplace = True)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#applying tfidf vectorizer on preprocessed_tweet\ntest_df_tfidf = vectorizer.transform(test_df['preprocessed_tweet'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sentiments strength of tweet of test dataframe\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer \nfrom tqdm import tqdm\n\nsid = SentimentIntensityAnalyzer() \n\nneg_sentiment = []\nneu_sentiment = []\npos_sentiment = []\ncom_sentiment = []\n\nfor sentence in tqdm(test_df['preprocessed_tweet']):\n    vector = sid.polarity_scores(sentence)\n    neg_sentiment.append(vector['neg'])\n    neu_sentiment.append(vector['neu'])\n    pos_sentiment.append(vector['pos'])\n    com_sentiment.append(vector['compound'])\n    \ntest_df_neg_sentiment = np.array(neg_sentiment).reshape(-1,1)\ntest_df_neu_sentiment = np.array(neu_sentiment).reshape(-1,1)\ntest_df_pos_sentiment = np.array(pos_sentiment).reshape(-1,1)\ntest_df_com_sentiment = np.array(com_sentiment).reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating data matrix \ntest_data_matrix = hstack((test_df_neg_sentiment,test_df_neu_sentiment,test_df_pos_sentiment,test_df_com_sentiment,test_df_tfidf )).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting using XGBoost model\ny_pred = xgb_model.predict(test_data_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}