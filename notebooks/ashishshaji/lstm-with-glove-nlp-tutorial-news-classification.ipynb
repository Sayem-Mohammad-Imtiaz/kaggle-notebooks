{"cells":[{"metadata":{},"cell_type":"markdown","source":"# FAKE NEWS CLASSIFICATION USING DEEP LEARNING WITH GloVe\n\n\nIn this notebook i have tried to classify news into 2 classes real and fake using LSTM nueral network .\n\n![](https://cdn.factcheck.org/UploadedFiles/fakenews.jpg)\nI have used pretrained Glove for vectorization and able to achive an accuracy of 99% by the proposed LSTM model.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Dataset\nThe dataset consists of about 40000 articles consisting around equal number of fake as well as real news Most of the news where collected from U.S newspapers and contian news about american poltics,world news ,news etc.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Loading the dataset","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\"\"\n%matplotlib inline\nsns.set_style(\"darkgrid\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"true = pd.read_csv(\"../input/fake-and-real-news-dataset/True.csv\")\nfalse = pd.read_csv(\"../input/fake-and-real-news-dataset/Fake.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Finding the most used words in fake and real news using Word cloud","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, ImageColorGenerator\ntext = \" \".join(str(each) for each in true.text.unique())\nwordcloud = WordCloud(max_words=200,colormap='Set3', background_color=\"black\").generate(text)\nplt.figure(figsize=(15,10))\nplt.imshow(wordcloud, interpolation='Bilinear')\nplt.axis(\"off\")\nplt.figure(1,figsize=(12, 12))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, ImageColorGenerator\ntext = \" \".join(str(each) for each in false.text.unique())\nwordcloud = WordCloud(max_words=200,colormap='Set3', background_color=\"black\").generate(text)\nplt.figure(figsize=(15,10))\nplt.imshow(wordcloud, interpolation='Bilinear')\nplt.axis(\"off\")\nplt.figure(1,figsize=(12, 12))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* labeling the fake news as 0 and real news as 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"true['label'] = 1\nfalse['label'] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Merging the 2 datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"news = pd.concat([true,false]) \nnews","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news['text'] = news['text'] + \" \" + news['title']\nnews","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=news.drop([\"date\",\"title\",\"subject\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* containing  23481 fake news and 21417 non fake news","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(false.shape)\nprint(true.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"label\", data=news);\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# data Preproccessing\nWe have to convert the raw messages (sequence of characters) into vectors (sequences of numbers).before that we need to do the following:\n1. Remove punctuation\n2. Remove numbers\n3. Remove tags\n4. Remove urls\n5. Remove stepwords\n6. Change the news to lower case\n7. Lemmatisation ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport string\nfrom nltk.corpus import stopwords\nimport re","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following 4 functions will help as to remove punctions (<,.'':, etc),numbers,tags and urls","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def rem_punctuation(text):\n  return text.translate(str.maketrans('','',string.punctuation))\n\ndef rem_numbers(text):\n  return re.sub('[0-9]+','',text)\n\n\ndef rem_urls(text):\n  return re.sub('https?:\\S+','',text)\n\n\ndef rem_tags(text):\n  return re.sub('<.*?>',\" \",text)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'].apply(rem_urls)\ndf['text'].apply(rem_punctuation)\ndf['text'].apply(rem_tags)\ndf['text'].apply(rem_numbers)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"rem_stopwords() is the function for removing stopwords and for converting the words to lower case","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = set(stopwords.words('english'))\n\ndef rem_stopwords(df_news):\n    \n    words = [ch for ch in df_news if ch not in stop]\n    words= \"\".join(words).split()\n    words= [words.lower() for words in df_news.split()]\n    \n    return words    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'].apply(rem_stopwords)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Lemmatization** \nperforms vocabulary and morphological analysis of the word and is normally aimed at removing **inflectional endings** only.That isconvert the words to their base or root form eg in \"plays\" it is converted to \"play\" by removing \"s\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\n#nltk.download('wordnet')\nlemmatizer = WordNetLemmatizer()\n\ndef lemmatize_words(text):\n  lemmas = []\n  for word in text.split():\n    lemmas.append(lemmatizer.lemmatize(word))\n  return \" \".join(lemmas)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'].apply(lemmatize_words)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenizing & Padding\n\n* **Tokenizing**\nis the process of breaking down a text into words. Tokenization can happen on any character, however the most common way of tokenization is to do it on space character.\n\n* **Padding**\nNaturally, some of the sentences are longer or shorter. We need to have the inputs with the same size, for this we use padding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\nfrom keras.utils import to_categorical\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df['text'].values\ny= df['label'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(x)\nword_to_index = tokenizer.word_index\nx = tokenizer.texts_to_sequences(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size =  len(word_to_index)\noov_tok = \"<OOV>\"\nmax_length = 250\nembedding_dim = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\nx = pad_sequences(x, maxlen=max_length)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vectorization\n Word vectorization is a methodology in NLP to map words or phrases from vocabulary to a corresponding vector of real numbers \n There are many  method for doing vectorization including  Bag of words,TFIDF or prettrained method such as Word2Vec ,Glove etc\n \n we are using **GloVe** learning algorithm for obtaining vector representations for words devolped by Stanford\n ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = {};\nwith open('../input/glove6b100dtxt/glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split();\n        word = values[0];\n        coefs = np.asarray(values[1:], dtype='float32');\n        embeddings_index[word] = coefs;\n\nembeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\nfor word, i in word_to_index.items():\n    embedding_vector = embeddings_index.get(word);\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.20,random_state=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n    tf.keras.layers.LSTM(64,return_sequences=True),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.LSTM(32),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n\n   \n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 10\nhistory = model.fit(X_train,y_train,epochs=epochs,validation_data=(X_test,y_test),batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = model.evaluate(X_test, y_test)\n# extract those\nloss = result[0]\naccuracy = result[1]\n\n\nprint(f\"[+] Accuracy: {accuracy*100:.2f}%\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}