{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nimport re\nimport math\n\nimport keras.layers as layers\nfrom keras.models import Model, Sequential\nfrom keras.initializers import glorot_uniform, he_uniform\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.utils import to_categorical, layer_utils, plot_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18800501fa4eabccc39579f0a34a3919988c2aab"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\ndef load_dataset(csv_dir, test_size=None):\n    df = pd.read_csv(csv_dir)\n    # Keeping only the neccessary columns\n    df = df[['text','sentiment']]\n\n#     df = df[df.sentiment != \"Neutral\"]\n    \n    # for idx, row in df.iterrows():\n    #     row[0] = row[0].replace(row[0][row[0].find(\"RT \") : row[0].find(\": \") + 2], '')\n    df['text'] = [x.strip().replace(x[x.find(\"RT \") : x.find(\": \") + 2], '') for x in df['text']]\n    df['text'] = df['text'].apply(lambda x: re.sub('[^A-Za-z0-9 ,\\?\\'\\\"-._\\+\\!/\\`@=;:]+', '', x.lower()))\n    \n#     df[\"sentiment\"] = df.sentiment.astype('category')\n#     classes = df.sentiment.cat.categories\n    classes = [\"Not Negative\", \"Negative\"]\n    \n#     print(\"Negative: {}, Neutral: {}, Positive: {}\".format(\n#         df[df['sentiment'] == 'Negative'].size,\n#         df[df['sentiment'] == 'Neutral'].size, \n#         df[df['sentiment'] == 'Positive'].size))\n\n    print(\"Negative: {}, Not Negative: {}\".format(\n        df[df['sentiment'] == 'Negative'].size,\n        df[df['sentiment'] != 'Negative'].size))\n\n    tokenizer = Tokenizer(num_words = 10000, split = ' ')\n    tokenizer.fit_on_texts(df['text'].values)\n    \n    X = tokenizer.texts_to_sequences(df['text'].values)\n    X = pad_sequences(X, maxlen=32)\n    Y = pd.get_dummies(df['sentiment']).values[:, 0]\n    \n#     for i in range(20):\n#         print('{} - {}'.format(df['sentiment'].iloc[i], Y[i]))\n\n    if test_size is None:\n        return X, Y, tokenizer, classes\n    else:\n        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size , random_state=1)\n        return X_train, X_test, Y_train, Y_test, tokenizer, classes\n    \n    \n\nX_train, X_test, Y_train, Y_test, tokenizer, CLASSES = load_dataset('../input/first-gop-debate-twitter-sentiment/Sentiment.csv', test_size=0.02)\nprint(\"Train shape: {}\".format(X_train.shape))\nprint(\"Test shape: {}\".format(X_test.shape))\nprint(X_test[: 3])\n# print(tokenizer.word_index[\"the\"])\nprint(CLASSES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7277fb5f6d5dfba0d6c0114d7cb7a1360e926381"},"cell_type":"code","source":"def load_vectors(file, total_num=0):\n    with open(file, encoding='utf-8', mode = 'r') as f:\n#         words = set()\n        word_vec = {}\n        i = 0\n        for line in f:\n            values = line.strip().split()\n            curr_word = values[0]\n#             words.add(curr_word)\n\n            try:\n                word_vec[curr_word] = np.array(values[1:], dtype=np.float64)\n\n                i += 1\n                if i % 1000 == 0:\n                    print('Processed {0} of {1}'.format(i, total_num), end='\\r')\n\n            # except Exception as ex:\n            #     template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n            #     message = template.format(type(ex).__name__, ex.args)\n            #     print(message)\n            except ValueError: # For data errors\n                # print(\"ValueError - \", curr_word)\n                pass\n\n        print('Processed {0} of {1}'.format(i, total_num))\n    return word_vec\n\n#         i = 1\n#         words_to_index = {}\n#         index_to_words = {}\n#         for w in sorted(words):\n#             words_to_index[w] = i\n#             index_to_words[i] = w\n#             i = i + 1\n#     return words_to_index, index_to_words, word_to_vec_map\n\n\n# word_vec = load_vectors('../input/glove6b50dtxt/glove.6B.50d.txt', 400000)\nword_vec = load_vectors('../input/glove6b100dtxt/glove.6B.100d.txt', 400000)\nprint(word_vec[\"the\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"580cef87ba3f13808e19e0602594c3142e7ba1dc"},"cell_type":"code","source":"def pretrained_embedding_layer(word_vec, word_index):\n    \"\"\"\n    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n\n    Arguments:\n    word_vec -- dictionary mapping words to their GloVe vector representation.\n    word_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n\n    Returns:\n    embedding_layer -- pretrained layer Keras instance\n    \"\"\"\n\n    vocab_len = len(word_index) + 1  # adding 1 to fit Keras embedding (requirement)\n    emb_dim = word_vec[\"the\"].shape[0]  # define dimensionality of your GloVe word vectors (= 50 or 100)\n\n    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False.\n    embedding_layer = layers.Embedding(vocab_len, emb_dim, trainable=False)\n\n    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n    embedding_layer.build((None,))\n\n    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n    emb_matrix = np.zeros((vocab_len, emb_dim))\n\n    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n    for word, index in word_index.items():\n#         emb_matrix[index, :] = word_vec[word]   # error happens when words are NOT in word_vec\n        vec = word_vec.get(word)\n        if vec is not None:\n            emb_matrix[index, :] = vec\n            \n    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n    embedding_layer.set_weights([emb_matrix])\n\n    return embedding_layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43b9a83ace6fa39ff3eb834eccda15442059f07b"},"cell_type":"code","source":"def MyModel(input_shape, out_shape, word_vec, word_index):\n    \"\"\"\n    Arguments:\n    input_shape -- shape of the input, usually (max_len,)\n    word_vec -- dictionary mapping every word in a vocabulary into its 50/100-dimensional vector representation\n    word_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n\n    Returns:\n    model -- a model instance in Keras\n    \"\"\"\n\n    # Define input sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n    inputs = layers.Input(input_shape, dtype=\"int32\")\n\n    # Create the embedding layer pretrained with GloVe Vectors\n    embedding_layer = pretrained_embedding_layer(word_vec, word_index)\n    # Propagate input sentence_indices through your embedding layer, you get back the embeddings\n    X = embedding_layer(inputs)\n    \n    # Add dropout with a probability\n    X = layers.SpatialDropout1D(0.3)(X)\n    \n    # Propagate the embeddings through an LSTM layer with dimensional hidden state\n    # The returned output is a batch of sequences.\n    X = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode='concat')(X)\n    # Add dropout with a probability\n    X = layers.Dropout(0.3)(X)\n    \n    # Propagate X trough another LSTM layer with dimensional hidden state\n    # The returned output is a single hidden state, not a batch of sequences.\n    X = layers.Bidirectional(layers.LSTM(128, return_sequences=False, dropout=0.2, recurrent_dropout=0.2), merge_mode='concat')(X)\n    \n    # Add dropout with a probability\n    X = layers.Dropout(0.3)(X)\n    \n    # Propagate X through a Dense layer with softmax activation to get back a batch of 1-dimensional vectors.\n#     X = layers.Dense(out_shape)(X)\n    # Add a sigmoid activation\n#     outputs = Activation(\"sigmoid\")(X)\n    outputs = layers.Dense(out_shape, activation='sigmoid')(X)\n    \n#     model.add(Embedding(maxLen, embed_dim, input_length = X_input.shape[1]))\n#     model.add(SpatialDropout1D(0.4))\n#     model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n\n    # Create Model instance which converts input sentence_indices into X.\n    model = Model(inputs = inputs, outputs = outputs)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b5e17e46c3594700d5eb54bd1bfe3a3d084899b"},"cell_type":"code","source":"maxLen = len(max(X_train, key=len))\n\nmodel = MyModel((maxLen,), 1, word_vec, tokenizer.word_index)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66c8ba6679202651e71d27a5b5cac720ab1ad3cd"},"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0003, decay=1e-6, beta_1=0.9, beta_2=0.999), metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46a02363a1beb01b48cf51fd7bf9d51b14ac13c9"},"cell_type":"code","source":"monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto')\ncheckpoint = ModelCheckpoint(filepath='best_weights.hdf5', verbose=1, save_best_only=True)   # Save the best model\nhist = model.fit(X_train, Y_train, batch_size = 64, epochs = 50, verbose=1, callbacks=[monitor, checkpoint], validation_split=0.01, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1005af553b3b5c60da4122f980fdb7af98dbe822"},"cell_type":"code","source":"def plot_train_history(history):\n    # plot the cost and accuracy \n    loss_list = history['loss']\n    val_loss_list = history['val_loss']\n    accuracy_list = history['acc']\n    val_accuracy_list = history['val_acc']\n    # epochs = range(len(loss_list))\n\n    # plot the cost\n    plt.plot(loss_list, 'b', label='Training cost')\n    plt.plot(val_loss_list, 'r', label='Validation cost')\n    plt.ylabel('cost')\n    plt.xlabel('iterations')\n    plt.title('Training and validation cost')\n    plt.legend()\n    \n    plt.figure()\n    \n    # plot the accuracy\n    plt.plot(accuracy_list, 'b', label='Training accuracy')\n    plt.plot(val_accuracy_list, 'r', label='Validation accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('iterations')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n\n\nplot_train_history(hist.history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6556f496a0e56916536ff5f84487c94a8f681127"},"cell_type":"code","source":"score = model.evaluate(X_test, Y_test)\n\nprint (\"Test Loss = \" + str(score[0]))\nprint (\"Test Accuracy = \" + str(score[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a77a0db7b20a38f2d9b43c283e4e0bbeb38f09be"},"cell_type":"code","source":"Y_test_pred = model.predict(X_test, batch_size=32, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7111a06654142189d4248e5436558f488163afc3"},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\ndef calculate_optimal_threshold(Y, Y_pred):\n    # ROC Curve\n    fpr, tpr, thresholds = roc_curve(Y, Y_pred)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC Curve')\n    plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n    plt.xlim([-0.025, 1.025])\n    plt.ylim([-0.025, 1.025])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('RoC Curve')\n    print(\"AUC: \", roc_auc)\n    \n    # Calculate the optimal threshold\n    i = np.arange(len(tpr)) # index for df\n    roc_df = pd.DataFrame({'threshold' : pd.Series(thresholds, index = i), \n                           'fpr': pd.Series(fpr, index=i), \n                           '1-fpr' : pd.Series(1-fpr, index = i), \n                           'tpr': pd.Series(tpr, index = i), \n                           'diff': pd.Series(tpr - (1-fpr), index = i) })\n    opt_threshold = roc_df.iloc[roc_df['diff'].abs().argsort()[:1]]\n    print(opt_threshold)\n    \n    return opt_threshold['threshold'].values[0]\n    \n    \nthreshold = calculate_optimal_threshold(Y_test, Y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93d8ab716e489d8095eb8f1a8d49e4dbbac242c1"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix, classification_report\n\ndef analyze(Y, Y_pred, classes, activation=\"softmax\", threshold=None):\n    if activation == \"sigmoid\":\n        Y_cls = Y\n        Y_pred_cls = (Y_pred > threshold).astype(float)\n    elif activation == \"softmax\":\n        Y_cls = np.argmax(Y, axis=1)\n        Y_pred_cls = np.argmax(Y_pred, axis=1)\n    \n    \n    # Accuracy Score\n    accuracy = accuracy_score(Y_cls, Y_pred_cls)\n    print(\"Accuracy Score: {}\\n\".format(accuracy))\n    \n    \n    # RMSE Score\n    rmse = np.sqrt(mean_squared_error(Y, Y_pred))\n    print(\"RMSE Score: {}\\n\".format(rmse))\n\n    \n    # Confusion Matrix\n    print(\"Confusion Matrix:\")\n    cm = confusion_matrix(Y_cls, Y_pred_cls)\n    print(cm)\n    # Plot the confusion matrix as an image.\n    plt.matshow(cm)\n    # Make various adjustments to the plot.\n    num_classes = len(classes)\n    plt.colorbar()\n    tick_marks = np.arange(num_classes)\n    plt.xticks(tick_marks, range(num_classes))\n    plt.yticks(tick_marks, range(num_classes))\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    \n    \n    # Classification Report\n    print(\"Classification Report:\")\n    print(classification_report(Y_cls, Y_pred_cls, target_names=classes))\n\n\n\nanalyze(Y_test, Y_test_pred, CLASSES, \"sigmoid\", threshold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecb5eec45a68c1dffe5eaff7716ab16b9503bfdd"},"cell_type":"code","source":"def plot_mislabeled(X, Y, Y_pred, classes, activation=\"softmax\", threshold=None, num_plot=0):\n    \"\"\"\n    Plots images where predictions and truth were different.\n    \n    X -- original image data - shape(m, img_rows*img_cols)\n    Y -- true labels - eg. [2,3,4,3,1,1]\n    Y_pred -- predictions - eg. [2,3,4,3,1,2]\n    \"\"\"\n    \n    if activation == \"sigmoid\":\n        Y_pred_cls = (np.squeeze(Y_pred) > threshold).astype(float)\n    elif activation == \"softmax\":\n        Y_pred_cls = np.argmax(Y_pred, axis=1)\n    \n    mislabeled_indices = np.where(Y != Y_pred_cls)[0]\n    if num_plot < 1:\n        num_plot = len(mislabeled_indices)\n        \n    for i, index in enumerate(mislabeled_indices[: num_plot]):\n        sentence = []\n        for id in X_test[index]:\n            if id > 0:\n                sentence += [k for k,v in tokenizer.word_index.items() if v == id]\n#                 print(list(tokenizer.word_index.keys())[list(tokenizer.word_index.values()).index(id)])\n        print(\" \".join(sentence))\n        print(\"Prediction: {} - {}\\nClass: {}\\n\".format(classes[int(Y_pred_cls[index])], Y_pred[index], classes[int(Y[index])]))\n\n\n        \nplot_mislabeled(X_test, Y_test, Y_test_pred, CLASSES, \"sigmoid\", threshold, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de9fb9426edcb5cdeb32fd09889254d84c319b4c"},"cell_type":"code","source":"twt = ['He said Make America Great Again']\n\n#vectorizing the tweet by the pre-fitted tokenizer instance\nX_twt = tokenizer.texts_to_sequences(twt)\nprint(X_twt)\n\n#padding the tweet to have exactly the same shape as 'embedding_2' input\nX_twt = pad_sequences(X_twt, maxlen=32, dtype='int32', value=0)\nprint(X_twt)\n\nY_twt = model.predict(X_twt, verbose=1)\nprint(Y_twt)\n\nif(Y_twt > threshold):\n    print(\"negative\")\nelse:\n    print(\"positive\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}