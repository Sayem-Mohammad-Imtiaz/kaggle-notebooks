{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcf7cadf213ef3736eb53157e76f08a7bce091f1"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nimport re\nimport math\n\nfrom keras.utils import to_categorical, layer_utils, plot_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b1648980a9d06a980bdb3494ae17b6dd9e9773b"},"cell_type":"code","source":"# Use 3 years data in time steps \nNUM_TIME_STEPS = 36","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec2cfe2dc24cc6af60a9ac5e254865c981ec5cef"},"cell_type":"code","source":"df = pd.read_csv(\"../input/candy_production.csv\")\ndf_mean = df['IPG3113N'].mean()\ndf_max = df['IPG3113N'].max()\ndf_min = df['IPG3113N'].min()\ndf_norm = (df['IPG3113N'] - df_mean) / (df_max - df_min)\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0a587b3ebfb504663287e6ee27b2944554e67bc"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndef preprocess_dataset(df, test_size):\n    m = len(df) - NUM_TIME_STEPS - 1\n    X = np.zeros((m, NUM_TIME_STEPS, 12))\n    Y = np.zeros((m))\n    \n    for i in range(m):\n        for j in range(NUM_TIME_STEPS):\n            X[i, j, j%12] = df.iloc[i+j]\n        Y[i] = df.iloc[i+NUM_TIME_STEPS+1]\n    \n#     return train_test_split(X, Y, test_size = test_size, shuffle = False, stratify = None)\n    return train_test_split(X, Y, test_size = test_size, random_state = 1)\n    \n    \n    \nX_train, X_test, Y_train, Y_test = preprocess_dataset(df_norm, test_size=0.02)\nprint(\"X_train\", X_train.shape)\nprint(\"Y_train\", Y_train.shape)\nprint(\"X_test\", X_test.shape)\nprint(\"Y_test\", Y_test.shape)\n# print(X_train[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6fe5fd37457c4d90a9c3752aac6cf8262c0eda2"},"cell_type":"code","source":"from keras.layers import Embedding, Input, Dropout, SpatialDropout1D, LSTM, Dense, Activation\nfrom keras.initializers import glorot_uniform, he_uniform\nfrom keras.models import Model, Sequential\n\ndef MyModel(input_shape):\n    X_input = Input(input_shape, dtype=\"float32\")\n\n    # Add dropout with a probability\n    X = SpatialDropout1D(0.3)(X_input)\n    \n    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state, the returned output should be a batch of sequences.\n    X = LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(X)\n    \n    # Add dropout with a probability\n#     X = Dropout(0.5)(X)\n    \n    # Propagate X trough another LSTM layer with 32-dimensional hidden state, the returned output should be a single hidden state, not a batch of sequences.\n    X = LSTM(32, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)(X)\n    \n    # Add dropout with a probability\n#     X = Dropout(0.5)(X)\n    \n    # Propagate X through a Dense layer without activation to get back a batch of 1-dimensional vectors.\n    X = Dense(1)(X)\n    \n    # Create Model instance.\n    model = Model(inputs = X_input, outputs = X)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20a2d25471aa3913c952bd27144722f350240385"},"cell_type":"code","source":"model = MyModel((NUM_TIME_STEPS, 12))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3bad1cf40880f025f6dcbb8fb458cf6f69b2846"},"cell_type":"code","source":"from keras.optimizers import Adam\n\nmodel.compile(loss='mse', optimizer=Adam(lr=0.00003, decay=1e-6, beta_1=0.9, beta_2=0.999))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a1cb53483f4e57a8c146fc650564a520c109b6f"},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint\n\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto')\ncheckpoint = ModelCheckpoint(filepath='best_weights.hdf5', verbose=1, save_best_only=True)   # Save the best model\nhist = model.fit(X_train, Y_train, batch_size = 32, epochs = 100, verbose=1, callbacks=[monitor, checkpoint], validation_split=0.01, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d98d5010a81dcf3159122b92997b64efaed50924"},"cell_type":"code","source":"model.load_weights('best_weights.hdf5')\nmodel.save('final_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e5172cb6b3cc6dd39bdfd7ade964da5ea0cf9a9"},"cell_type":"code","source":"def plot_train_history(history):\n    # plot the mse cost \n    loss_list = history['loss']\n    val_loss_list = history['val_loss']\n\n    # plot the cost\n    plt.plot(loss_list, 'b', label='Training cost')\n    plt.plot(val_loss_list, 'r', label='Validation cost')\n    plt.ylabel('cost')\n    plt.xlabel('iterations')\n    plt.title('Training and validation cost')\n    plt.legend()\n\n    \n\nplot_train_history(hist.history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b689d2ba40931d85ce93fd8a582ca369c1f0e0c"},"cell_type":"code","source":"score = model.evaluate(X_test, Y_test)\nprint (\"Test Loss = \" + str(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a5dbbf13a60f723490c2891508ce69367a709c3"},"cell_type":"code","source":"Y_test_pred = model.predict(X_test, batch_size=32, verbose=1).squeeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45caa35cb9899d686789c6294241bf492ed46071"},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ndef plot_predictive_interval(Y_test, Y_test_pred, z):\n    rmse = np.sqrt(mean_squared_error(Y_test, Y_test_pred))\n    interval = z * rmse\n    \n    plt.plot(Y_test_pred, 'b', label='labels')\n    plt.plot(Y_test, 'r', label='predictions')\n    plt.plot(Y_test + interval, 'g')\n    plt.plot(Y_test - interval, 'g')\n    plt.ylabel('amounts')\n    plt.xlabel('time steps')\n    plt.title('labels & predictions')\n    plt.legend()\n\n    \n    \nY_test2 = Y_test * (df_max - df_min) + df_mean\nY_test_pred2 = Y_test_pred * (df_max - df_min) + df_mean\n# print(Y_test2)\n# print(Y_test_pred2)\n\n# z = 1.96, by using 95% predictive interval\nplot_predictive_interval(Y_test2, Y_test_pred2, 1.96)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27c358d7553d1ebbfea6e988e750d843fd072f90"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}