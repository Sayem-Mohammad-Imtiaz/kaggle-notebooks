{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport re \nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport warnings \nwarnings.filterwarnings('ignore')\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## Helper Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def df_nan_filter(df, filter_cols):\n    row_nans_mask = df[filter_cols].isnull().any(axis=1)\n    dropped_rows = df.loc[row_nans_mask]\n    filtered_df = df.loc[~row_nans_mask]\n    return filtered_df, dropped_rows\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Info about json schema"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('/kaggle/input/CORD-19-research-challenge/json_schema.txt', 'r') as f:\n    print(f.read())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading the license agreement"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Uncomment if online mode is turned on\n!pip install tika","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tika is useful for reading PDFs in Python\nfrom tika import parser\nraw = parser.from_file('/kaggle/input/CORD-19-research-challenge/COVID.DATA.LIC.AGMT.pdf')\nprint(raw['content'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA) on Meta Data"},{"metadata":{},"cell_type":"markdown","source":"### Reading about Metadata"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('/kaggle/input/CORD-19-research-challenge/metadata.readme', 'r') as f:\n    print(f.read())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sources:**\n* CZI: Chan Zuckerberg Initiative, a company aimed to \"advance human potential and promote equality in areas such as health, education, scientific research and energy\".\n* PMC: PubMed Central, a free full-text archive of biomedical and life sciences journal literature at the U.S. National Institutes of Health's National Library of Medicine (NIH/NLM).\n* bioRxiv: an open access preprint repository for the biological sciences. As preprints, papers hosted on bioRxiv are not peer-reviewed, but undergo basic screening and checked against plagiarism.\n* medRxiv: a preprint service for the medicine and health sciences and provides a free online platform for researchers to share, comment, and receive feedback on their work. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df = pd.read_csv('/kaggle/input/CORD-19-research-challenge/metadata.csv') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df, dropped_rows = df_nan_filter(meta_df, ['title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df['title']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look at the distribution of article titles in terms of total count of each\nmeta_df['title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Functions used to find most commonly used words in article titles taken from https://www.kaggle.com/paultimothymooney/most-common-words-in-the-cord-19-dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_ngrams(df,column,begin_ngram,end_ngram):\n    # adapted from https://stackoverflow.com/questions/36572221/how-to-find-ngram-frequency-of-a-column-in-a-pandas-dataframe\n    word_vectorizer = CountVectorizer(ngram_range=(begin_ngram,end_ngram), analyzer='word')\n    sparse_matrix = word_vectorizer.fit_transform(df['title'].dropna())\n    frequencies = sum(sparse_matrix).toarray()[0]\n    most_common = pd.DataFrame(frequencies, \n                               index=word_vectorizer.get_feature_names(), \n                               columns=['frequency']).sort_values('frequency',ascending=False)\n    most_common['ngram'] = most_common.index\n    most_common.reset_index()\n    return most_common\n\ndef word_cloud_function(df,column,number_of_words):\n    # adapted from https://www.kaggle.com/benhamner/most-common-forum-topic-words\n    topic_words = [ z.lower() for y in\n                       [ x.split() for x in df[column] if isinstance(x, str)]\n                       for z in y]\n    word_count_dict = dict(Counter(topic_words))\n    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words(\"english\")]\n    word_string=str(popular_words_nonstop)\n    wordcloud = WordCloud(stopwords=STOPWORDS,\n                          background_color='white',\n                          max_words=number_of_words,\n                          width=1000,height=1000,\n                         ).generate(word_string)\n    plt.clf()\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\n\ndef word_bar_graph_function(df,column,title):\n    # adapted from https://www.kaggle.com/benhamner/most-common-forum-topic-words\n    topic_words = [ z.lower() for y in\n                       [ x.split() for x in df[column] if isinstance(x, str)]\n                       for z in y]\n    word_count_dict = dict(Counter(topic_words))\n    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words(\"english\")]\n    plt.barh(range(50), [word_count_dict[w] for w in reversed(popular_words_nonstop[0:50])])\n    plt.yticks([x + 0.5 for x in range(50)], reversed(popular_words_nonstop[0:50]))\n    plt.title(title)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Most Common Words in Titles"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nword_bar_graph_function(meta_df,'title','Most common words in titles of papers in CORD-19 dataset')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Most Common Journals"},{"metadata":{"trusted":true},"cell_type":"code","source":"value_counts = meta_df['journal'].value_counts()\nvalue_counts_df = pd.DataFrame(value_counts)\nvalue_counts_df['journal_name'] = value_counts_df.index\nvalue_counts_df['count'] = value_counts_df['journal']\nfig = px.bar(value_counts_df[0:20], \n             x=\"count\", \n             y=\"journal_name\",\n             title='Most Common Journals in CORD-19 Dataset',\n             orientation='h')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Most Common Dates of Publication"},{"metadata":{"trusted":true},"cell_type":"code","source":"value_counts = meta_df['publish_time'].value_counts()\nvalue_counts_df = pd.DataFrame(value_counts)\nvalue_counts_df['which_year'] = value_counts_df.index\nvalue_counts_df['count'] = value_counts_df['publish_time']\nfig = px.bar(value_counts_df[0:5], \n             x=\"count\", \n             y=\"which_year\",\n             title='Most Common Dates of Publication',\n             orientation='h')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Idea is to gather all most popular articles that have most popular words, from the most popular journal from the most popular year, with the most \n# common n-gram phrase and then data mine these articles (and then extend this to top 3 words/journals/years/etc?)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Observe value distribution of publish times\nmeta_df['publish_time'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Observe value distribution of journals\nmeta_df['journal'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's see if PLoS One articles can be found in the set of articles that contain 'virus' in the title and that were published in 2020..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a DataFrame where title and publish_time are the most popular values\nvirus_2020_df = meta_df.loc[(meta_df['title'].str.contains(\"virus\")) & (meta_df['publish_time'] == '2020')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"virus_2020_df['journal'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ok it looks like none of the top journals are PLoS One... do any articles come from this journal, though?"},{"metadata":{"trusted":true},"cell_type":"code","source":"virus_2020_df.loc[virus_2020_df['journal'] == 'PLoS One']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looks like PLoS One is no where to be found in this subset of data..."},{"metadata":{},"cell_type":"markdown","source":"# Data on potential risks factors\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_df = pd.read_csv(\"/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/biorxiv_clean.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_commerical_df = pd.read_csv(\"/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/clean_comm_use.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_commerical_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_noncommerical_df = pd.read_csv(\"/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/clean_noncomm_use.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_noncommerical_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_pmc_df = pd.read_csv(\"/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/clean_pmc.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_pmc_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we don't care about which of these sources the risk factor info comes from... I will combine all of these data sources to analyze (since columns are the same)."},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_df = pd.concat([biorxiv_df, clean_commerical_df, clean_noncommerical_df, clean_pmc_df])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### See which columns have NaNs and then remove the NaNs if the column is of interest"},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's remove all rows with NaNs in the **Abstract** column so that we can search for information on risk factors (i.e. smoking) in the abstracts"},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_abstract_df, abstract_dropped_rows = df_nan_filter(clean_df, ['abstract'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_abstract_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoking_abstracts_df = clean_abstract_df.loc[clean_abstract_df['abstract'].str.contains(\"smoking\")].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoking_abstracts_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoking_abstract_text = ''.join(str(elem) for elem in list(smoking_abstracts_df['abstract']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoking_abstract_text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using RegEx to find all sentences that contain the word 'Smoking'"},{"metadata":{"trusted":true},"cell_type":"code","source":"regExSmokingAbstractResults = re.findall(r\"([^.]*?smoking[^.]*\\.)\",smoking_abstract_text)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regExSmokingAbstractResults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regExSmokingAbstractResultsText = ''.join(str(elem) for elem in regExSmokingAbstractResults)\nregExSmokingAbstractResultsText","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using Pytextrank to summarize text about smoking (taken from https://medium.com/@aneesha/beyond-bag-of-words-using-pytextrank-to-find-phrases-and-summarize-text-f736fa3773c5)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Uncomment if online mode is turned on\n!pip install pytextrank","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pytextrank","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#spaCy is a library for advanced Natural Language Processing in Python and Cython. It's built on the very latest research, and was designed from day one to be used \n# in real products. spaCy comes with pretrained statistical models and word vectors, and currently supports tokenization for 50+ languages.\nimport spacy\n# load a spaCy model, depending on language, scale, etc.\nnlp = spacy.load(\"en_core_web_sm\")\nnlp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add PyTextRank to the spaCy pipeline\ntr = pytextrank.TextRank()\nnlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = nlp(regExSmokingAbstractResultsText)\nsmoking_strings = [x for x in doc._.phrases if 'smoking' in str(x)] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/boudinfl/pke.git","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using PKE (https://github.com/boudinfl/pke) Paper = http://aclweb.org/anthology/C16-2015"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pke\n\n# initialize keyphrase extraction model, here TopicRank\nextractor = pke.unsupervised.TopicRank()\n\n# load the content of the document, here document is expected to be in raw\n# format (i.e. a simple text file) and preprocessing is carried out using spacy\nextractor.load_document(input=regExSmokingAbstractResultsText, language='en')\n\n# keyphrase candidate selection, in the case of TopicRank: sequences of nouns\n# and adjectives (i.e. `(Noun|Adj)*`)\nextractor.candidate_selection()\n\n# candidate weighting, in the case of TopicRank: using a random walk algorithm\nextractor.candidate_weighting()\n\n\n# N-best selection, keyphrases contains the 10 highest scored candidates as\n# (keyphrase, score) tuples\nkeyphrases = extractor.get_n_best(n=10)\n\n# print the n-highest (10) scored candidates\nfor (keyphrase, score) in extractor.get_n_best(n=10):\n    print(keyphrase, score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The topics listed above demonstrate that smoking status (most likely how often someone smokes) is considered an 'important topic' in the article abstracts, which are focused on understanding COVID-19."},{"metadata":{},"cell_type":"markdown","source":"# Trying to use Rake (https://pypi.org/project/rake-nltk/) from the natural language toolkit to find key phrases"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install rake-nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from rake_nltk import Rake\n\nrAbstract = Rake() # Uses stopwords for english from NLTK, and all puntuation characters.\n\nrAbstract.extract_keywords_from_text(regExSmokingAbstractResultsText)\n\nrAbstract.get_ranked_phrases() # To get keyword phrases ranked highest to lowest.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abstractRankedPhrases = rAbstract.get_ranked_phrases()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for phrase in abstractRankedPhrases:\n    if 'smoking' in phrase:\n        print(phrase)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from rake_nltk import Rake\n\nrMainText = Rake() # Uses stopwords for english from NLTK, and all puntuation characters.\n\nrMainText.extract_keywords_from_text(''.join(str(elem) for elem in re.findall(r\"([^.]*?smoking[^.]*\\.)\",''.join(str(elem) for elem in list(clean_abstract_df['text'])))))\n\nrMainText.get_ranked_phrases() # To get keyword phrases ranked highest to lowest.\n\nmaintextRankedPhrases = rMainText.get_ranked_phrases()\n\nfor phrase in maintextRankedPhrases:\n    if 'smoking' in phrase:\n        print(phrase)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, certain risk factors, based on the text mining results, most likely include: \n* tension \n* smoking \n* alcohol \n* salt \n* animal fats \n* body weight \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}