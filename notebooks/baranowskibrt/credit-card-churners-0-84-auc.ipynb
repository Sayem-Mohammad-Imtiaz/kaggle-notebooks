{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Dataset fetching and basic preparation"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import plot_confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import preprocessing\nfrom sklearn.svm import SVC\nfrom sklearn.utils import resample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_path = '/kaggle/input/credit-card-customers/BankChurners.csv'\ncustomers = pd.read_csv(file_path)\ncustomers.drop('Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1', axis=1, inplace=True)\ncustomers.drop('Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We split the dataset into training and test set to avoid data snooping during the visualization.  \nValue that we look for is also mapped from categories to numbers."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_percentage = 0.2\ntest_number = int(test_percentage * len(customers))\n\nattrition_flag = customers['Attrition_Flag'].map({'Attrited Customer': 0, 'Existing Customer': 1})\ny_test, y_train = attrition_flag[test_number:], attrition_flag[:test_number]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"customers.drop('CLIENTNUM', axis=1, inplace=True)\nattrition_flag_names =  customers['Attrition_Flag']\ncustomers.drop('Attrition_Flag', axis=1, inplace=True)\n\nX_test, X_train = customers[test_number:], customers[:test_number]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"attrition_flag.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 6))\nvis_customers = customers.copy()\nvis_customers['Attrition_Flag'] = attrition_flag\nmask = np.triu(np.ones_like(vis_customers.corr(), dtype=np.bool))\n\nheatmap = sns.heatmap(vis_customers.corr(), mask=mask, cmap=\"viridis\", annot=True)\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  It may be reasonable to drop some of the least meaningful ones like Avg_Open_To_Buy (another already overlapping feature, no interesting correlations with other features)."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 12))\nheatmap = sns.heatmap(vis_customers.corr()[['Attrition_Flag']].sort_values(by='Attrition_Flag', ascending=False), vmin=-0.5, vmax=0.5, annot=True, cmap='BrBG')\nheatmap.set_title('Features Correlating with client attrition', fontdict={'fontsize':18}, pad=16);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vis_customers['Attrition_Flag'] = attrition_flag_names\n\nfig, ax = plt.subplots(ncols=3,figsize=(20,5))\nsns.scatterplot(data=vis_customers, x=\"Total_Ct_Chng_Q4_Q1\", y=\"Total_Trans_Ct\", hue=\"Attrition_Flag\", ax=ax[0])\nsns.scatterplot(data=vis_customers, x=\"Total_Revolving_Bal\", y=\"Total_Trans_Ct\", hue=\"Attrition_Flag\", ax=ax[1])\nsns.scatterplot(data=vis_customers, x=\"Customer_Age\", y=\"Total_Trans_Ct\", hue=\"Attrition_Flag\", ax=ax[2])\n\nfig, ax = plt.subplots(ncols=3,figsize=(20,5))\nsns.scatterplot(data=vis_customers, x=\"Months_on_book\", y=\"Total_Trans_Ct\", hue=\"Attrition_Flag\", ax=ax[0]);\nsns.scatterplot(data=vis_customers, x=\"Months_Inactive_12_mon\", y=\"Total_Trans_Ct\", hue=\"Attrition_Flag\", ax=ax[1]);\nsns.scatterplot(data=vis_customers, x=\"Contacts_Count_12_mon\", y=\"Total_Trans_Ct\", hue=\"Attrition_Flag\", ax=ax[2]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can deduce a few things from the diagrams:\n* The more transactions a client makes the higher the chance that he will stay\n* People usually churn after 2-4 months of inactivity\n* More contacts over the 12 months usually means a will to churn"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3,figsize=(30,20))\n\npd.crosstab(attrition_flag_names,vis_customers['Gender']).plot(kind='bar',ax=ax1, rot=0,  ylim=[0,5000])\npd.crosstab(attrition_flag_names,vis_customers['Education_Level']).plot(kind='bar',ax=ax2, rot=0, ylim=[0,3000])\npd.crosstab(attrition_flag_names,vis_customers['Marital_Status']).plot(kind='bar',ax=ax3, rot=0, ylim=[0,4500])\npd.crosstab(attrition_flag_names,vis_customers['Income_Category']).plot(kind='bar',ax=ax4, rot=0, ylim=[0,3200])\npd.crosstab(attrition_flag_names,vis_customers['Card_Category']).plot(kind='bar',ax=ax5, rot=0, ylim=[0,9000])\npd.crosstab(attrition_flag_names,vis_customers['Months_Inactive_12_mon']).plot(kind='bar',ax=ax6, rot=0, ylim=[0,3500]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can discard columns least correlated to our target value."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop(['Credit_Limit', 'Avg_Open_To_Buy', 'Months_on_book', 'Customer_Age', 'Dependent_count'], axis=1, inplace=True)\nX_test.drop(['Credit_Limit', 'Avg_Open_To_Buy', 'Months_on_book', 'Customer_Age', 'Dependent_count'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Correcting the imbalance"},{"metadata":{"trusted":true},"cell_type":"code","source":"attrition_flag_names.value_counts().plot.pie(ylabel='', autopct='%1.1f%%', figsize=(8,8));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Attrited customer data seems to be heavily undersampled. We need to even out this discrepancy to avoid issues with low precision.[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full = X_train.copy()\ndf_full['Attrition_flag'] = y_train\n\nattrited_customer = df_full[df_full['Attrition_flag']==0]\nexisting_customer = df_full[df_full['Attrition_flag']==1]\n\n\nattrited_upsampled = resample(attrited_customer,\n                                 replace=True,\n                                 n_samples=(len(existing_customer.index) - len(attrited_customer.index)),\n                                 random_state=41)\n\nX_train = pd.concat([df_full, attrited_upsampled])\n\nX_train['Attrition_flag'].value_counts()\n\ny_train = X_train.pop('Attrition_flag')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Data preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_attribs = X_train.loc[:,X_train.dtypes==np.object].columns\ncat_attribs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_attribs = np.setxor1d(X_train.columns.values, cat_attribs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nnum_pipeline = Pipeline([\n ('std_scaler', StandardScaler()),\n ])\nfull_pipeline = ColumnTransformer([\n (\"num\", num_pipeline, np.array(num_attribs)),\n (\"cat\", OneHotEncoder(), np.array(cat_attribs)),\n ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_prepared = full_pipeline.fit_transform(X_train)\nX_prepared = pd.DataFrame.from_records(X_train_prepared)\n\nX_test_prepared = full_pipeline.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Implementation"},{"metadata":{},"cell_type":"markdown","source":"We will try out a bunch of different classification alogrithms and choose the most promising ones for further hyperparameter tuning."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_scores(clf, X, y):\n    y_predictions = clf.predict(X)\n    print(\"Precision: \", precision_score(y, y_predictions))\n    print(\"Recall: \", recall_score(y, y_predictions))\n    print(\"F1 score: \", f1_score(y, y_predictions))\n    print(\"AUC: \", roc_auc_score(y, y_predictions))\n    plot_confusion_matrix(clf, X, y);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.1 k-Nearest Neighbors"},{"metadata":{"trusted":true},"cell_type":"code","source":"nbrs = KNeighborsClassifier(n_neighbors=5,metric='minkowski')\nnbrs.fit(X_train_prepared, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_scores(nbrs, X_test_prepared, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.2 Support Vector Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_clf = SVC()\nsvc_clf.fit(X_train_prepared, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_scores(svc_clf, X_test_prepared, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.3 Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"ran_for = RandomForestClassifier()\nran_for.fit(X_train_prepared, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_scores(ran_for, X_test_prepared, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.4 Gradient boosting Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_clf = GradientBoostingClassifier()\ngb_clf.fit(X_train_prepared, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_scores(gb_clf, X_test_prepared, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Hyperparameter tuning"},{"metadata":{},"cell_type":"markdown","source":"Now we should increase the scores of the algorithms. For that Grid Search will be used as the number of parameters will usually not be that high."},{"metadata":{},"cell_type":"markdown","source":"## 6.1 k-Nearest Neighbors"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = [\n {'n_neighbors': [1, 2, 3],\n  'weights': ['uniform', 'distance'],\n }]\ngrid_search = GridSearchCV(nbrs,\n                           param_grid,\n                           cv=5,\n                           return_train_score=True)\n\ngrid_search.fit(X_train_prepared, y_train)\n\nnbrs_params = grid_search.best_params_\n\nprint(nbrs_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nbrs = KNeighborsClassifier(**nbrs_params)\nnbrs.fit(X_train_prepared, y_train)\nget_scores(nbrs, X_test_prepared, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.2 Support Vector Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n    'C':[1,10,100,1000],\n    'gamma':['scale', 'auto'],\n    'kernel':['linear','rbf']\n}\ngrid_search = GridSearchCV(svc_clf,\n                           param_grid,\n                           cv=5,\n                           return_train_score=True,\n                           n_jobs=-1)\n\ngrid_search.fit(X_train_prepared, y_train)\n\nsvc_params = grid_search.best_params_\n\nprint(svc_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_clf = SVC(**svc_params)\nsvc_clf.fit(X_train_prepared, y_train)\nget_scores(svc_clf, X_test_prepared, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.3 Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = { \n    'n_estimators': [50, 75, 100, 125, 150],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [5, 7, 11, 13],\n    'criterion' :['gini', 'entropy']\n}\n\ngrid_search = GridSearchCV(ran_for,\n                           param_grid,\n                           cv=5,\n                           return_train_score=True,\n                           n_jobs=-1)\n\ngrid_search.fit(X_train_prepared, y_train)\n\nforest_params = grid_search.best_params_\n\nprint(forest_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ran_for = RandomForestClassifier(**forest_params)\nran_for.fit(X_train_prepared, y_train)\nget_scores(ran_for, X_test_prepared, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.4 Gradient boosting Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n    \"loss\":[\"deviance\"],\n    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n    \"min_samples_split\": np.linspace(0.1, 0.5, 5),\n    \"min_samples_leaf\": np.linspace(0.1, 0.5, 5),\n    \"max_depth\":[3,5,8],\n    \"max_features\":[\"log2\",\"sqrt\"],\n    \"criterion\": [\"friedman_mse\",  \"mae\"],\n    \"subsample\":[0.5, 0.7, 0.8, 0.9, 1.0],\n    \"n_estimators\":[10]\n}\n\ngrid_search = GridSearchCV(gb_clf,\n                           param_grid,\n                           cv=5,\n                           return_train_score=True,\n                           n_jobs=-1)\n\ngrid_search.fit(X_train_prepared, y_train)\n\ngb_params = grid_search.best_params_\n\nprint(gb_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_clf = GradientBoostingClassifier(**gb_params)\ngb_clf.fit(X_train_prepared, y_train)\nget_scores(gb_clf, X_test_prepared, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7 Conclusion"},{"metadata":{},"cell_type":"markdown","source":"Random forest is the best model after the hyperparameter tuning resulting in Area Under the Curve of 0.84 on the test set."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}