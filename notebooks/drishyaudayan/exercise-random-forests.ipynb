{"cells":[{"metadata":{},"cell_type":"markdown","source":"**[Machine Learning Micro-Course Home Page](https://www.kaggle.com/learn/intro-to-machine-learning)**\n\n---\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recap\nHere's the code you've written so far."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code you have previously used to load data\nimport pandas as pd\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\n\n\n# Path of the file to read\niowa_file_path = '../input/home-data-for-ml-course/train.csv'\n\nhome_data = pd.read_csv(iowa_file_path)\n# Create target object and call it y\ny = home_data.SalePrice\n# Create X\nfeatures = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nX = home_data[features]\n\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\n# Specify Model\niowa_model = DecisionTreeRegressor(random_state=1)\n# Fit Model\niowa_model.fit(train_X, train_y)\n\n# Make validation predictions and calculate mean absolute error\nval_predictions = iowa_model.predict(val_X)\nval_mae = mean_absolute_error(val_predictions, val_y)\nprint(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format(val_mae))\n\n# Using best value for max_leaf_nodes\niowa_model = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)\niowa_model.fit(train_X, train_y)\nval_predictions = iowa_model.predict(val_X)\nval_mae = mean_absolute_error(val_predictions, val_y)\nprint(\"Validation MAE for best value of max_leaf_nodes: {:,.0f}\".format(val_mae))\n\n\n# Set up code checking\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.machine_learning.ex6 import *\nprint(\"\\nSetup complete\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exercises\nData science isn't always this easy. But replacing the decision tree with a Random Forest is going to be an easy win."},{"metadata":{},"cell_type":"markdown","source":"## Step 1: Use a Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# Define the model. Set random_state to 1\nrf_model = RandomForestRegressor(random_state=1)\n\n# fit your model\nrf_model.fit(train_X, train_y)\n\n\n# Calculate the mean absolute error of your Random Forest model on the validation data\nmelb_preds = rf_model.predict(val_X)\nrf_val_mae = mean_absolute_error(val_y, melb_preds)\n\nprint(\"Validation MAE for Random Forest Model: {}\".format(rf_val_mae))\n\nstep_1.check()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The lines below will show you a hint or the solution.\nstep_1.hint() \nstep_1.solution()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So far, you have followed specific instructions at each step of your project. This helped learn key ideas and build your first model, but now you know enough to try things on your own. \n\nMachine Learning competitions are a great way to try your own ideas and learn more as you independently navigate a machine learning project. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom datetime import datetime\nfrom scipy.stats import skew  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport sklearn.linear_model as linear_model\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nimport os\nprint(os.listdir(\"../input\"))\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/home-data-for-ml-course/train.csv')\ntest = pd.read_csv('../input/home-data-for-ml-course/test.csv')\nprint (\"Data is loaded!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train.append(test,sort=False) #Make train set and test set in the same data set\n\ndata #Visualize the DataFrame data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot features with more than 1000 NULL values\n\nfeatures = []\nnullValues = []\nfor i in data:\n    if (data.isna().sum()[i])>1000 and i!='SalePrice':\n        features.append(i)\n        nullValues.append(data.isna().sum()[i])\ny_pos = np.arange(len(features)) \nplt.bar(y_pos, nullValues, align='center', alpha=0.5)\nplt.xticks(y_pos, features)\nplt.ylabel('NULL Values')\nplt.xlabel('Features')\nplt.title('Features with more than 1000 NULL values')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dealing with NULL values\n\ndata = data.dropna(axis=1, how='any', thresh = 1000) #Drop columns that contain more than 1000 NULL values\ndata = data.fillna(data.mean()) #Replace NULL values with mean values\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.get_dummies(data) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop features that are correlated to each other\n\ncovarianceMatrix = data.corr()\nlistOfFeatures = [i for i in covarianceMatrix]\nsetOfDroppedFeatures = set() \nfor i in range(len(listOfFeatures)) :\n    for j in range(i+1,len(listOfFeatures)): #Avoid repetitions \n        feature1=listOfFeatures[i]\n        feature2=listOfFeatures[j]\n        if abs(covarianceMatrix[feature1][feature2]) > 0.8: #If the correlation between the features is > 0.8\n            setOfDroppedFeatures.add(feature1) #Add one of them to the set\n#I tried different values of threshold and 0.8 was the one that gave the best results\n\ndata = data.drop(setOfDroppedFeatures, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop features that are not correlated with output\n\nnonCorrelatedWithOutput = [column for column in data if abs(data[column].corr(data[\"SalePrice\"])) < 0.045]\n#I tried different values of threshold and 0.045 was the one that gave the best results\n\ndata = data.drop(nonCorrelatedWithOutput, axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot one of the features with outliers\n\nplt.plot(data['LotArea'], data['SalePrice'], 'bo')\nplt.axvline(x=75000, color='r')\nplt.ylabel('SalePrice')\nplt.xlabel('LotArea')\nplt.title('SalePrice in function of LotArea')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#First, we need to seperate the data (Because removing outliers â‡” removing rows, and we don't want to remove rows from test set)\n\nnewTrain = data.iloc[:1460]\nnewTest = data.iloc[1460:]\n\n#Second, we will define a function that returns outlier values using percentile() method\n\ndef outliers_iqr(ys):\n    quartile_1, quartile_3 = np.percentile(ys, [25, 75]) #Get 1st and 3rd quartiles (25% -> 75% of data will be kept)\n    iqr = quartile_3 - quartile_1\n    lower_bound = quartile_1 - (iqr * 1.5) #Get lower bound\n    upper_bound = quartile_3 + (iqr * 1.5) #Get upper bound\n    return np.where((ys > upper_bound) | (ys < lower_bound)) #Get outlier values\n\n#Third, we will drop the outlier values from the train set\n\ntrainWithoutOutliers = newTrain #We can't change train while running through it\n\nfor column in newTrain:\n    outlierValuesList = np.ndarray.tolist(outliers_iqr(newTrain[column])[0]) #outliers_iqr() returns an array\n    trainWithoutOutliers = newTrain.drop(outlierValuesList) #Drop outlier rows\n    \ntrainWithoutOutliers = newTrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = trainWithoutOutliers.drop(\"SalePrice\", axis=1) #Remove SalePrice column\nY = np.log1p(trainWithoutOutliers[\"SalePrice\"]) #Get SalePrice column {log1p(x) = log(x+1)}\nreg = LinearRegression().fit(X, Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make prediction\n\nnewTest = newTest.drop(\"SalePrice\", axis=1) #Remove SalePrice column\npred = np.expm1(reg.predict(newTest))\n\n#Submit prediction\n\nsub = pd.DataFrame() #Create a new DataFrame for submission\nsub['Id'] = test['Id']\nsub['SalePrice'] = pred\nsub.to_csv(\"submission.csv\", index=False) #Convert DataFrame to .csv file\n\nsub #Visualize the DataFrame sub\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n**[Machine Learning Micro-Course Home Page](https://www.kaggle.com/learn/intro-to-machine-learning)**\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}