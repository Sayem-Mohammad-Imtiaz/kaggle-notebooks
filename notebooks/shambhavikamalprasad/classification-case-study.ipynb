{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# libraries to read the data and perform mathematical operations\nimport pandas as pd\nimport numpy as np\n\n# libraries to visualise the data\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Importing the Data**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking the data characteristics","metadata":{}},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe(include = object).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing Value Analysis","metadata":{}},{"cell_type":"code","source":"data.isnull().sum().sort_values(ascending = False).plot(kind = 'bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the target column, RainTomorrow is also having missing values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()/len(data) * 100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the amount of missing values in these columns > 20%\n# Evaporation      43.166506\n# Sunshine         48.009762\n# Cloud9am         38.421559\n# Cloud3pm         40.807095\n\n# Hence, due to data insufficiency,dropping these columns\ndata.drop(columns = ['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm'], inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{}},{"cell_type":"code","source":"# extracting the year and month from the date column since not applying times series analysis on this data\ndata['Year'] = data['Date'].apply(lambda x : x.split('-')[0]).astype(int)\ndata['Month'] = data['Date'].apply(lambda x : x.split('-')[1]).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(columns = 'Date', inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Year'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Month'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking the target class","metadata":{}},{"cell_type":"code","source":"data['RainTomorrow'].value_counts().plot(kind = 'bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the target set is imbalanced\n# it would be labelled highly imbalanced if one class is < 10%","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['RainTomorrow'].value_counts()/len(data) *100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Segregating the numerical and categorical columns","metadata":{}},{"cell_type":"code","source":"# segregating the numerical and categorical columns\nnum_data = data.select_dtypes(include = np.number)\ncat_data = data.select_dtypes(include = object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_data.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_data.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing Value Imputation","metadata":{}},{"cell_type":"code","source":"num_data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (20,12))\nnum_data.hist(bins = 100)\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the columns are near-normally distributed, with very less skewness, so not much transformation is required, except for the column Rainfall","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10,5))\nsns.histplot(num_data['Rainfall'], bins = 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standard Scaler is used when the shape of the distribution is near normal. It preserves the shape of the distribution\n# Min-Max scaler is used when we need to preserve the effect of the outliers\n# Standard Scaler is preferred for this case\n# To remove the effect of outliers, need to apply Robust Scaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(data = num_data, orient = 'h')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in num_data.columns:\n    plt.figure(figsize = (10,5))\n    sns.boxplot(x = col, data = num_data)\n    plt.show()\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the classification algorithms which work using Likelihood Estimation, are affected by outliers.\n# Tree-based algorithms are not affected by outliers\n# Outliers make the classification model actually better.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in cat_data:\n    plt.figure(figsize = (20,5))\n    sns.countplot(x = col, data = cat_data, hue = 'RainTomorrow', palette='rainbow')\n    plt.legend(loc = 'best')\n    plt.xticks(rotation  = 90 )\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# when lat-long values are given, multiply them to use as a new column\n# when know from domain expertise that location is neccesary to predict rainfall\n# but the problem is that the number of locations is so long, that it is not possible to label encode these values manually\n# so, we use dummy encoding; so that certain algorithms like ensemble techniques will work well on these encoded locations","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.groupby(['Location']).describe(include = object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# All the categorical columns in this dataset are ordinal but still we could go with dummy variable encoding. \n# It won't affect the performance of a modern ML algorithm.\n# Because, modern day ML algorithms are not entirely affected by the differences in dummy encoding and label encoding, in general.\n# when to choose which, comes from domain expertise","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# segregating the target column\ny = cat_data['RainTomorrow'].values\ncat_data.drop(columns = 'RainTomorrow', inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concatenating the numerical and categorical data to get the feature set\nX = pd.concat([num_data, cat_data], axis = 1)\n\n# splitting into training and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Segregating the training and test sets into numerical and categorical data so as to apply different operations on each\nX_train_num = X_train.select_dtypes(include = np.number)\nX_train_cat = X_train.select_dtypes(include = object)\n\nX_test_num = X_test.select_dtypes(include = np.number)\nX_test_cat = X_test.select_dtypes(include = object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the names of the numerical and categorical columns to be added later\nnum_cols = X_train_num.columns\ncat_cols = X_train_cat.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing Value imputation","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imputing the numerical missing values by the median\nimputer = SimpleImputer(strategy='median')\n\nX_train_num = pd.DataFrame(imputer.fit_transform(X_train_num),columns=num_cols)\nX_test_num = pd.DataFrame(imputer.transform(X_test_num), columns = num_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imputing the categorical missing values by the mode\nimputer = SimpleImputer(strategy='most_frequent')\n\nX_train_cat = pd.DataFrame(imputer.fit_transform(X_train_cat),columns=cat_cols)\nX_test_cat  = pd.DataFrame(imputer.transform(X_test_cat), columns = cat_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling the numerical columns","metadata":{}},{"cell_type":"code","source":"# Scaling the numerical data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train_num = pd.DataFrame(scaler.fit_transform(X_train_num), columns = num_cols)\nX_test_num = pd.DataFrame(scaler.transform(X_test_num), columns = num_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dummy encoding the training categorical data\nX_train_cat  =pd.get_dummies(X_train_cat, drop_first=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the encoded training categorical data\nX_train_cat.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dummy encoding the test categorical data\nX_test_cat  =pd.get_dummies(X_test_cat, drop_first=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Concatenating the categorical and numerical data to get the final train and test sets","metadata":{}},{"cell_type":"code","source":"# Creating the training dataset\nX_train = pd.concat([X_train_num, X_train_cat], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the test dataset\nX_test = pd.concat([X_test_num, X_test_cat], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the shape of the newly created datasets\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How to ascertain that the train-test split is dividing the data correctly. Ans: Check the mean and std for both test and train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.describe().T[['mean', 'std']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.describe().T[['mean', 'std']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = y_train.reshape(-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imputing the missing values in target variable by the mode\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='most_frequent')\ny_train = imputer.fit_transform(y_train.reshape(-1,1))\ny_test = imputer.transform(y_test.reshape(-1,1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label encoding the target variable\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ny_train = encoder.fit_transform(y_train.ravel())\n\ny_test = encoder.transform(y_test.ravel())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating a scorecard to compare different models","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, precision_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scorecard = pd.DataFrame(columns = ['Estimator', 'f1_score', 'Accuracy', 'Precision', 'ROC_AUC_Score'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def update_score (estimator):\n    global scorecard \n    name = estimator.__class__.__name__\n    y_pred = estimator.predict(X_test)\n    y_pred_proba = estimator.predict_proba(X_test)\n    f1 = f1_score(y_test, y_pred)\n    acc = accuracy_score(y_test, y_pred)\n    roc = roc_auc_score(y_test, y_pred_proba[:,1])\n    prec = precision_score(y_test, y_pred)\n    scorecard = scorecard.append({'Estimator':name, 'f1_score': f1, 'Accuracy':acc, 'Precision': prec,'ROC_AUC_Score':roc} ,\n                                       ignore_index=True)\n    return(scorecard)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_roc_curve(estimator):\n    y_pred_proba = estimator.predict_proba(X_test)\n    tpr, fpr, thres = roc_curve(y_test, y_pred_proba[:,1])\n    \n    plt.figure(figsize = (12,7))\n    \n    plt.xlim([0,1])\n    plt.ylim([0,1])\n    plt.plot([0,1], [0,1], '--')\n    \n    plt.plot(tpr, fpr, label = estimator.__class__.__name__ %roc_auc_score(y_test, y_pred_proba[:,1]) )\n    plt.legend()\n    plt.xlabel('False positive rate (1-Specificity)', fontsize = 12)\n    plt.ylabel('True positive rate (Sensitivity)', fontsize = 12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# creating the DecisionTree model","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtf = DecisionTreeClassifier()\n\ndtf.fit(X_train, y_train)\nscorecard = update_score(dtf)\nplot_roc_curve(dtf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_f = X_train.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for train_idx, test_idx in kf.split(X_train_f):\n    X_train_idx, X_test_idx = X_train_f[train_idx], X_train_f[test_idx]\n    y_train_idx, y_test_idx = y_train[train_idx], y_train[test_idx]\n    dtf.fit(X_train_idx, y_train_idx)\n    update_score(dtf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Running different models","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n\nlogreg.fit(X_train, y_train)\n# y_pred_proba = logreg.predict_proba(X_test)\nupdate_score(logreg)\nplot_roc_curve(logreg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}