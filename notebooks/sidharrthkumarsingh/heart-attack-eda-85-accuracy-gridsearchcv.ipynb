{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-05T15:22:35.027201Z","iopub.execute_input":"2021-06-05T15:22:35.027528Z","iopub.status.idle":"2021-06-05T15:22:35.046903Z","shell.execute_reply.started":"2021-06-05T15:22:35.027456Z","shell.execute_reply":"2021-06-05T15:22:35.045849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **GENERAL DESCRIPTION OF THE DATASET**\n\nA heart attack is quite a common ailment for humans with average age of 65 (for men) and 72 (for women). Although new researches in the recent past do put the spotlight on an alarming trend - \"**a rising incidence of heart attacks in younger adults**.\"\n\nThis dataset contains information about people and their chances of succumbing to a **HEART ATTACK**.\n\nThe contents of the dataset range from personal records like age, sex, etc. to more cardiovascular related information like blood pressure levels, types of chest pain, cholestrol levels, etc. \n\n\n#### Given below is a brief explanation of the terminology used in the given dataset :-\n\n1. age : Age of the patient\n2. sex : Sex of the patient\n3. exng : exercise induced angina (1 = yes; 0 = no)\n4. ca : number of major vessels (0-3)\n5. cp : Chest Pain type <br>\n   • Value 1: typical angina <br>\n   • Value 2: atypical angina <br>\n   • Value 3: non-anginal pain <br>\n   • Value 4: asymptomatic <br>\n6. trtbps : resting blood pressure (in mm Hg)\n7. chol : cholestoral in mg/dl fetched via BMI sensor\n8. fbs : (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n9. rest_ecg : resting electrocardiographic results <br>\n   • Value 0: normal <br>\n   • Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) <br>\n   • Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria <br>\n10. thalach : maximum heart rate achieved\n11. target : <br>\n    • 0 = less chance of heart attack <br>\n    • 1 = more chance of heart attack","metadata":{}},{"cell_type":"markdown","source":"# **OBJECTIVES OF THE NOTEBOOK**\n\n1. Perform the EDA (Exploratory Data Analysis) followed by the VDA (Visual Data Analysis) to better understand the given dataset. \n\n2. Correctly predict (max accuracy) the probability of a person succumbing to a heart attack with the help of the various features present in the dataset.\n\n3. Apply and compare the various ML classification algorithms we have at our disposal and select the one with the highest accuracy.\n\n4. Boost the ML classification models using processing techniques to examine any substantial increase in the acuracy scores.  ","metadata":{}},{"cell_type":"markdown","source":"# **IMPORTING THE LIBRRIES AND DATASTET**","metadata":{}},{"cell_type":"markdown","source":"### **LIBRARIES**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:35.048387Z","iopub.execute_input":"2021-06-05T15:22:35.048672Z","iopub.status.idle":"2021-06-05T15:22:37.486063Z","shell.execute_reply.started":"2021-06-05T15:22:35.048644Z","shell.execute_reply":"2021-06-05T15:22:37.48506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **DATASET**","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv (\"../input/heart-attack-analysis-prediction-dataset/heart.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:37.489906Z","iopub.execute_input":"2021-06-05T15:22:37.490285Z","iopub.status.idle":"2021-06-05T15:22:37.507227Z","shell.execute_reply.started":"2021-06-05T15:22:37.490257Z","shell.execute_reply":"2021-06-05T15:22:37.506052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **EXPLORATORY DATA ANALYSIS (EDA)**\n\nIn this section, we will be performing some basic operations on the dataset in order to break down our data to the rudimentary level. For example, checking for missing values, what are the input types of the features, dropping unvalued features etc. to name a few.","metadata":{}},{"cell_type":"code","source":"dataset.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:37.508566Z","iopub.execute_input":"2021-06-05T15:22:37.508823Z","iopub.status.idle":"2021-06-05T15:22:37.516436Z","shell.execute_reply.started":"2021-06-05T15:22:37.508797Z","shell.execute_reply":"2021-06-05T15:22:37.515665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.columns","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:37.517669Z","iopub.execute_input":"2021-06-05T15:22:37.517935Z","iopub.status.idle":"2021-06-05T15:22:37.532247Z","shell.execute_reply.started":"2021-06-05T15:22:37.517909Z","shell.execute_reply":"2021-06-05T15:22:37.531404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.info ()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:37.533472Z","iopub.execute_input":"2021-06-05T15:22:37.533869Z","iopub.status.idle":"2021-06-05T15:22:37.566458Z","shell.execute_reply.started":"2021-06-05T15:22:37.533839Z","shell.execute_reply":"2021-06-05T15:22:37.565723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head ()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:37.568911Z","iopub.execute_input":"2021-06-05T15:22:37.5694Z","iopub.status.idle":"2021-06-05T15:22:37.600046Z","shell.execute_reply.started":"2021-06-05T15:22:37.569372Z","shell.execute_reply":"2021-06-05T15:22:37.599095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To the naked eye, all of the features that we have present in our dataset appear to contribute to the final predictions of the heart attck analysis in some form or the other. Hence, in our intial stages of EDA we wont be dropping any feature/column. ","metadata":{}},{"cell_type":"markdown","source":"### **CHECKING FOR MISSING DATA**","metadata":{}},{"cell_type":"code","source":"dataset.isnull ()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:37.601516Z","iopub.execute_input":"2021-06-05T15:22:37.601758Z","iopub.status.idle":"2021-06-05T15:22:37.628257Z","shell.execute_reply.started":"2021-06-05T15:22:37.601732Z","shell.execute_reply":"2021-06-05T15:22:37.627478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the output of the **isna ()** function only the first 5 and the last 5 data entries are visible and we dont get any actual idea whether any data point is mising or not in the middle of the table. We will rectify that in the next code cell.","metadata":{}},{"cell_type":"code","source":"dataset.isna (). sum ()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:37.629374Z","iopub.execute_input":"2021-06-05T15:22:37.629646Z","iopub.status.idle":"2021-06-05T15:22:37.650798Z","shell.execute_reply.started":"2021-06-05T15:22:37.629617Z","shell.execute_reply":"2021-06-05T15:22:37.64988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The **isna (). sum ()** function gives the sum of all the missing or uncorrectly entered data points for each and every column in the dataset. Since, none of the features in our dataset is capable of having negative values it is safe to state that the dataset doestn't contain any missing values. ","metadata":{}},{"cell_type":"markdown","source":"### **CHECKING THE NUMBER OF UNIQUE VALUES**\n","metadata":{}},{"cell_type":"code","source":"dict = {}\nfor i in list(dataset.columns):\n    dict[i] = dataset[i].value_counts().shape[0]\n\npd.DataFrame(dict,index=[\"unique count\"]).transpose()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:37.652175Z","iopub.execute_input":"2021-06-05T15:22:37.65243Z","iopub.status.idle":"2021-06-05T15:22:37.677775Z","shell.execute_reply.started":"2021-06-05T15:22:37.652403Z","shell.execute_reply":"2021-06-05T15:22:37.67708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DataFrame.describe()** method generates descriptive statistics that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values. This method tells us a lot of things about a dataset. One important thing is that the describe() method deals only with numeric values. It doesn't work with any categorical values. So if there are any categorical values in a column the describe() method will ignore it and display summary for the other columns unless parameter include=\"all\" is passed.\n\nNow, let's understand the statistics that are generated by the describe() method:\n\n• count tells us the number of NoN-empty rows in a feature. <br>\n• mean tells us the mean value of that feature. <br>\n• std tells us the Standard Deviation Value of that feature. <br>\n• min tells us the minimum value of that feature. <br>\n• 25%, 50%, and 75% are the percentile/quartile of each features. This quartile information helps us to detect Outliers. <br>\n• max tells us the maximum value of that feature.","metadata":{}},{"cell_type":"code","source":"dataset.describe ()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:37.678799Z","iopub.execute_input":"2021-06-05T15:22:37.679089Z","iopub.status.idle":"2021-06-05T15:22:37.726613Z","shell.execute_reply.started":"2021-06-05T15:22:37.67906Z","shell.execute_reply":"2021-06-05T15:22:37.726149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **CHECKING FOR DUPLICATED DATA**","metadata":{}},{"cell_type":"code","source":"dataset.duplicated (). sum ()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:37.727349Z","iopub.execute_input":"2021-06-05T15:22:37.727618Z","iopub.status.idle":"2021-06-05T15:22:37.738641Z","shell.execute_reply.started":"2021-06-05T15:22:37.727596Z","shell.execute_reply":"2021-06-05T15:22:37.737757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A duplicate row has been found and will have to be removed to establish chances of best results.","metadata":{}},{"cell_type":"code","source":"dataset.drop_duplicates (inplace = True)\ndataset.duplicated (). sum ()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:37.739916Z","iopub.execute_input":"2021-06-05T15:22:37.740231Z","iopub.status.idle":"2021-06-05T15:22:37.761511Z","shell.execute_reply.started":"2021-06-05T15:22:37.740202Z","shell.execute_reply":"2021-06-05T15:22:37.760698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **VISUAL DATA ANALYSIS (VDA)**\n\nAfter doing a theoretical analysis in the previous section, we will be moving on to visual analysis of the dataset. This will include quite a number of scatter plots, bar plots, etc. between the different features and how they affect one other and how they will affect the working algorithm. We will also be getting a general idea about which features will play a more active role while determining the accuracy of the model.\n\nFor this section we will be using a new library **plotly.express** as well.","metadata":{}},{"cell_type":"markdown","source":"### **CORRELATION MATRIX AND HEATMAP**","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame (dataset, columns = ['age', 'sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg', 'thalachh', 'exng', 'oldpeak', 'slp', 'caa', 'thall', 'output'])\ndf.corr ()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:37.762331Z","iopub.execute_input":"2021-06-05T15:22:37.762619Z","iopub.status.idle":"2021-06-05T15:22:37.794443Z","shell.execute_reply.started":"2021-06-05T15:22:37.762597Z","shell.execute_reply":"2021-06-05T15:22:37.793552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_Matrix = df.corr ()\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap (corr_Matrix, linewidths = 0.5, annot = True, fmt= '.1f',ax=ax)\nplt.show ()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:37.795534Z","iopub.execute_input":"2021-06-05T15:22:37.795803Z","iopub.status.idle":"2021-06-05T15:22:38.80626Z","shell.execute_reply.started":"2021-06-05T15:22:37.795773Z","shell.execute_reply":"2021-06-05T15:22:38.805487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be seen from the heatmap that we don't have much strong correlation neither between any two features nor between a feature and the response variable (output column). Therefore, it won't be advantageous for us to make choice of features on the basis of such low correlation scores.\n\nFor this dataset we will make use of the medical knowledge available pertaining to heart attacks and what are the main factors that influence it.\n\nFor example, from our features and consulting the internet it becomes fairly obvious that features like age, cp, trtbps, chol, etc will be accounted for.\n\nThe list of relevant features is as follows :- <br>\n• age <br>\n• cp <br>\n• trtbps <br>\n• chol <br>\n• fbs <br>\n• rest_ecg <br>\n• thalachh","metadata":{}},{"cell_type":"markdown","source":"### **HEART ATTACK vs AGE** ","metadata":{}},{"cell_type":"code","source":"fig = px.histogram (dataset, x = \"age\", nbins = 6, facet_row = \"output\", title = \"Heart Attacks Per Age Group\", template = 'plotly_dark')\nfig.show ()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:38.807478Z","iopub.execute_input":"2021-06-05T15:22:38.807932Z","iopub.status.idle":"2021-06-05T15:22:39.909664Z","shell.execute_reply.started":"2021-06-05T15:22:38.807891Z","shell.execute_reply":"2021-06-05T15:22:39.908685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**output = 1** corresponds to people who had a heart attack. <br>\n**output = 0** corresponds to people who didn't have a heart attack.\n\n**INFERENCES** <br>\n⁫• people in the age bracket of 50-60 (65 cases) have the maximum tendency to have a heart attack closely followed by the     age group of 40-50 (50 cases). <br>\n⁫• 10 cases in the age range of 30-40 is an alarming figure because people in this age group weren't known to suffer from     heart attacks. <br>\n⁫• the age group of 50-60 (60 cases) also leads the chart in the maximum number of people not succumbing to heart attacks     followed by people in the range of 60-70 (48 cases).","metadata":{}},{"cell_type":"markdown","source":"### **HEART ATTACK vs CHEST PAIN**","metadata":{}},{"cell_type":"code","source":"fig =  px.pie (dataset, names = \"cp\", hole = 0.4, template = \"gridon\", title = \" Types Of Chest Pain\")\nfig.show ()\nsns.countplot (x = 'cp', data = dataset)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:39.910936Z","iopub.execute_input":"2021-06-05T15:22:39.911176Z","iopub.status.idle":"2021-06-05T15:22:40.09325Z","shell.execute_reply.started":"2021-06-05T15:22:39.911155Z","shell.execute_reply":"2021-06-05T15:22:40.092265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Maximum number of people suffer from type 0/typical angina (47.4%) chest pain followed by type 2/non-anginal pain        (28.5%), type 1/atypical angina (16,6%) chest pain and lastly type 3/asymptomatic (7.62%). <br>","metadata":{}},{"cell_type":"code","source":"fig = px.sunburst(dataset, names = \"cp\", path = [\"output\",\"cp\"], template = \"gridon\", title = \"Heart Attack Chances Based On Chest Pain \")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:40.094747Z","iopub.execute_input":"2021-06-05T15:22:40.095177Z","iopub.status.idle":"2021-06-05T15:22:40.211931Z","shell.execute_reply.started":"2021-06-05T15:22:40.095139Z","shell.execute_reply":"2021-06-05T15:22:40.211064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SUNBURST CHART TERMINOLOGY** <br>\n\n**lighter color corresponds to chest pain types** <br>\ntype 0 - typical angina <br>\ntype 1 - atypical angina <br>\ntype 2 - non-anginal pain <br>\ntype 3 - asymptomatic <br>\n**darker color corresponds to heart attack response** <br>\n1 - heart attack <br>\n0 - non heart attack <br>\n\n**INFERENCES** <br>\n• patients with higher chances of heart attack or have suffered a heart attack tend to experience non-anginal/type 2 pain   (41.4%) the most and asymptomatic/type 3 pain (9.7%) the least. <br>\n• 75.3% of the patients with a lesser chance of suffering from a heart attack experience typical angina (type 0) pain.","metadata":{}},{"cell_type":"markdown","source":"### **HEART ATTACK vs RESTING BLOOD SUGAR**","metadata":{}},{"cell_type":"code","source":"fig = px.scatter (dataset, x = \"age\", y = \"trtbps\", color = \"output\", template = \"plotly\", size = \"trtbps\", title = \"Age vs Resting Blood Sugar Level and Impact On Heart Attack\")\nfig.show ()\nfig = px.histogram (dataset, x = \"trtbps\", nbins = 12, facet_row = \"output\", title = \"Heart Attacks per Resting Blood Sugar Levels\", template = 'plotly_dark')\nfig.show ()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:40.216129Z","iopub.execute_input":"2021-06-05T15:22:40.216405Z","iopub.status.idle":"2021-06-05T15:22:40.60957Z","shell.execute_reply.started":"2021-06-05T15:22:40.216377Z","shell.execute_reply":"2021-06-05T15:22:40.608565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**output = 1** corresponds to people who had a heart attack. <br>\n**output = 0** corresponds to people who didn't have a heart attack.\n\n**INFERENCES** <br>\n• patients prone to a heart attack have a resting blood sugar level in the range of 120-140 mm Hg. <br>\n• patients not prone to a heart attack have a resting blood sugar distributed more or less evenly in the range of 110-150   mm Hg. ","metadata":{}},{"cell_type":"markdown","source":"### **HEART ATTACK vs CHOLESTROL**","metadata":{}},{"cell_type":"code","source":"fig = px.scatter (dataset, x = \"age\", y = \"chol\", color = \"output\", template = \"plotly\", size = \"chol\", title = \"Age vs Cholestrol Level and Impact On Heart Attack\")\nfig.show ()\nfig = px.histogram (dataset, x = \"chol\", nbins = 10, facet_row = \"output\", title = \"Heart Attacks per Cholestrol Levels\", template = 'plotly_dark')\nfig.show ()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:40.611688Z","iopub.execute_input":"2021-06-05T15:22:40.611977Z","iopub.status.idle":"2021-06-05T15:22:40.795022Z","shell.execute_reply.started":"2021-06-05T15:22:40.611936Z","shell.execute_reply":"2021-06-05T15:22:40.793878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**output = 1** corresponds to people who had a heart attack. <br>\n**output = 0** corresponds to people who didn't have a heart attack.\n\n**INFERENCES** <br>\n• maximum number of patients prone to a heart attack have cholestrol level in the range of 200-250 mg/dL. <br>\n• maximum cholestrol level reached for a heart attack prone victim was 564 mg/dL. <br>\n• patients not prone to a heart attack have a cholestrol level distributed evenly in the range of 200-300 mg/dL. ","metadata":{}},{"cell_type":"markdown","source":"### **HEART ATTACK vs FASTING BLOOD SUGAR**","metadata":{}},{"cell_type":"code","source":"fig =  px.pie (dataset, names = \"fbs\", hole = 0.4, template = \"gridon\", title = \"Fasting Blood Sugar Levels\")\nfig.show ()\nsns.countplot (x = 'fbs', data = dataset)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:40.796244Z","iopub.execute_input":"2021-06-05T15:22:40.796499Z","iopub.status.idle":"2021-06-05T15:22:40.937396Z","shell.execute_reply.started":"2021-06-05T15:22:40.796473Z","shell.execute_reply":"2021-06-05T15:22:40.936772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1 - fbs > 120mg/dL** <br>\n**0 - fbs < 120mg/dL** <br>\n\nMaximum people have a fasting blood sugar lower than 120mg/dL","metadata":{}},{"cell_type":"code","source":"fig = px.sunburst(dataset, names = \"fbs\", path = [\"output\",\"fbs\"], template = \"gridon\", title = \"Heart Attack Chances Based On Fasting Blood Sugar Levels\")\nfig.show ()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:40.938459Z","iopub.execute_input":"2021-06-05T15:22:40.938706Z","iopub.status.idle":"2021-06-05T15:22:41.009427Z","shell.execute_reply.started":"2021-06-05T15:22:40.93868Z","shell.execute_reply":"2021-06-05T15:22:41.008828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SUNBURST CHART TERMINOLOGY** <br>\n\n**lighter color corresponds to fasting blood sugar level** <br>\ntype 0 - fbs<120mg/dL <br>\ntype 1 - fbs>120mg/dL <br>\n**darker color corresponds to heart attack response** <br>\n1 - heart attack <br>\n0 - non heart attack <br>\n\n**INFERENCES** <br>\n• 85.9% patients prone to a heart attack have a fasting blood sugar level greater than 120 mg/dL. <br>\n• 84% patients not prone to a heart attack have a fasting blood sugar level less than 120 mg/dL.","metadata":{}},{"cell_type":"markdown","source":"### **HEART ATTACK vs RESTING ELECTROCARDIOGRAPHIC RESULTS**","metadata":{}},{"cell_type":"code","source":"fig =  px.pie (dataset, names = \"restecg\", hole = 0.4, template = \"gridon\", title = \"Resting Electrocardiographic Results\")\nfig.show ()\nsns.countplot (x = 'restecg', data = dataset)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:41.010479Z","iopub.execute_input":"2021-06-05T15:22:41.010878Z","iopub.status.idle":"2021-06-05T15:22:41.154397Z","shell.execute_reply.started":"2021-06-05T15:22:41.010851Z","shell.execute_reply":"2021-06-05T15:22:41.153799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The percentage of patients with a restecg of type 2 (1.32%) is more or less negligible in comparison to types 0 and 1 (48.7% and 50% respectively). ","metadata":{}},{"cell_type":"code","source":"fig = px.sunburst(dataset, names = \"restecg\", path = [\"output\",\"restecg\"], template = \"gridon\", title = \"Heart Attack Chances Based On Resting Electrocardiographic Results\")\nfig.show ()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:41.155453Z","iopub.execute_input":"2021-06-05T15:22:41.155841Z","iopub.status.idle":"2021-06-05T15:22:41.227427Z","shell.execute_reply.started":"2021-06-05T15:22:41.155813Z","shell.execute_reply":"2021-06-05T15:22:41.226686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SUNBURST CHART TERMINOLOGY** <br>\n\n**lighter color corresponds to resting electrocardiographic results** <br>\ntype 0: normal <br>\ntype 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) <br>\ntype 2: showing probable or definite left ventricular hypertrophy by Estes' criteria <br>\n**darker color corresponds to heart attack response** <br>\n1 - heart attack <br>\n0 - non heart attack <br>\n\n**INFERENCES** <br>\n• heart attack prone patients predominantly have ST-T wave abnormality/type 1 restecg result (57.9%) followed by             normal/type 0 restecg result (41.4%). <br>\n• patients not prone to a heart attack have an opposite scenario with normal/type 0 restecg result being the major case     (57.2%) and ST-T wave abnormality/type 1 restecg result being secondary (40.5%). <br>\n• both the categories have a marginal number of patients who have a type 2 restecg result (0.006% and 0.02% respectively).","metadata":{}},{"cell_type":"markdown","source":"### **HEART ATTACK vs MAXIMUM HEART RATE**","metadata":{}},{"cell_type":"code","source":"fig = px.scatter (dataset, x = \"age\", y = \"thalachh\", color = \"output\", size = \"thalachh\", template = \"plotly\", title = \"Age vs Maxmum Heart Rate and Impact on Heart Attack\")\nfig.show ()\nfig = px.histogram (dataset, x = \"thalachh\", nbins = 7, facet_row = \"output\", title = \"Heart Attacks per Maximum Heart Rate\", template = 'plotly_dark')\nfig.show ()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:41.228673Z","iopub.execute_input":"2021-06-05T15:22:41.229208Z","iopub.status.idle":"2021-06-05T15:22:41.40063Z","shell.execute_reply.started":"2021-06-05T15:22:41.229169Z","shell.execute_reply":"2021-06-05T15:22:41.400024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**output = 1** corresponds to people who had a heart attack. <br>\n**output = 0** corresponds to people who didn't have a heart attack.\n\n**INFERENCES** <br>\n• patients with the maximum heart rate in the range of 160-180 are more prone to a heart attack. <br>\n• at the same time patients with the maximum heart rate in the range of 140-160 are less prone to suffer a heart attack.","metadata":{}},{"cell_type":"markdown","source":"### **SOME 3D PLOTS AND PAIRPLOTS**","metadata":{}},{"cell_type":"code","source":"fig = px.scatter_3d (dataset, x = \"age\", y = \"trtbps\", z = \"chol\", color = \"output\", template = \"plotly_dark\", title = \"Age vs Resting Blood Sugar vs Cholestrol\")\nfig.show ()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:41.401698Z","iopub.execute_input":"2021-06-05T15:22:41.402099Z","iopub.status.idle":"2021-06-05T15:22:41.518587Z","shell.execute_reply.started":"2021-06-05T15:22:41.402071Z","shell.execute_reply":"2021-06-05T15:22:41.517719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter_3d (dataset, x = \"age\", y = \"trtbps\", z = \"fbs\", color = \"output\", template = \"plotly_dark\", title = \"Age vs Resting Blood Sugar vs Fasting Blood Sugar\")\nfig.show ()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:41.519675Z","iopub.execute_input":"2021-06-05T15:22:41.519972Z","iopub.status.idle":"2021-06-05T15:22:41.600933Z","shell.execute_reply.started":"2021-06-05T15:22:41.519931Z","shell.execute_reply":"2021-06-05T15:22:41.600245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter_3d (dataset, x = \"age\", y = \"thalachh\", z = \"cp\", color = \"output\", template = \"plotly_dark\", title = \"Age vs Maximum Heart Rate vs Chest Pain\")\nfig.show ()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:41.601889Z","iopub.execute_input":"2021-06-05T15:22:41.602166Z","iopub.status.idle":"2021-06-05T15:22:41.681838Z","shell.execute_reply.started":"2021-06-05T15:22:41.602139Z","shell.execute_reply":"2021-06-05T15:22:41.681254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot (dataset, hue = \"output\")\nplt.show ()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:22:41.682753Z","iopub.execute_input":"2021-06-05T15:22:41.683108Z","iopub.status.idle":"2021-06-05T15:23:29.007398Z","shell.execute_reply.started":"2021-06-05T15:22:41.683082Z","shell.execute_reply":"2021-06-05T15:23:29.006127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **DATA PREPROCESSING**\n\n1. Splitting dataset into matrix of features and response variables. <br>\n2. Splitting dataset into training and test sets. <br>\n3. Feature scaling.","metadata":{}},{"cell_type":"markdown","source":"### **SPLITTING DATASET INTO MATRIX OF FEATURES AND RESPONSE VARIABLES**","metadata":{}},{"cell_type":"code","source":"X = dataset.iloc [ : , : -1].values\nY = dataset.iloc [ :, -1].values\nX.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:23:29.008933Z","iopub.execute_input":"2021-06-05T15:23:29.009305Z","iopub.status.idle":"2021-06-05T15:23:29.016773Z","shell.execute_reply.started":"2021-06-05T15:23:29.009272Z","shell.execute_reply":"2021-06-05T15:23:29.016202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **SPLITTING DATASET INTO TRAINING AND TEST SETS**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split (X, Y, test_size = 0.25, random_state = 1)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:23:29.017842Z","iopub.execute_input":"2021-06-05T15:23:29.018242Z","iopub.status.idle":"2021-06-05T15:23:29.223248Z","shell.execute_reply.started":"2021-06-05T15:23:29.018207Z","shell.execute_reply":"2021-06-05T15:23:29.222448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape ","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:23:29.224706Z","iopub.execute_input":"2021-06-05T15:23:29.225187Z","iopub.status.idle":"2021-06-05T15:23:29.229762Z","shell.execute_reply.started":"2021-06-05T15:23:29.225153Z","shell.execute_reply":"2021-06-05T15:23:29.22892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train.shape ","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:23:29.230937Z","iopub.execute_input":"2021-06-05T15:23:29.231309Z","iopub.status.idle":"2021-06-05T15:23:29.245479Z","shell.execute_reply.started":"2021-06-05T15:23:29.231272Z","shell.execute_reply":"2021-06-05T15:23:29.244893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:23:29.246215Z","iopub.execute_input":"2021-06-05T15:23:29.24651Z","iopub.status.idle":"2021-06-05T15:23:29.260563Z","shell.execute_reply.started":"2021-06-05T15:23:29.246481Z","shell.execute_reply":"2021-06-05T15:23:29.259829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:23:29.261377Z","iopub.execute_input":"2021-06-05T15:23:29.261571Z","iopub.status.idle":"2021-06-05T15:23:29.274187Z","shell.execute_reply.started":"2021-06-05T15:23:29.26155Z","shell.execute_reply":"2021-06-05T15:23:29.273697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **FEATURE SCALING**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler ()\nX_train = sc.fit_transform (X_train)\nX_test = sc.transform (X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:23:29.275007Z","iopub.execute_input":"2021-06-05T15:23:29.275294Z","iopub.status.idle":"2021-06-05T15:23:29.287998Z","shell.execute_reply.started":"2021-06-05T15:23:29.275272Z","shell.execute_reply":"2021-06-05T15:23:29.287457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (X_train [ : 5, : ])","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:23:29.288755Z","iopub.execute_input":"2021-06-05T15:23:29.289044Z","iopub.status.idle":"2021-06-05T15:23:29.301749Z","shell.execute_reply.started":"2021-06-05T15:23:29.289022Z","shell.execute_reply":"2021-06-05T15:23:29.300866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (X_test [ : 5, : ])","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:23:29.302811Z","iopub.execute_input":"2021-06-05T15:23:29.303212Z","iopub.status.idle":"2021-06-05T15:23:29.319139Z","shell.execute_reply.started":"2021-06-05T15:23:29.303177Z","shell.execute_reply":"2021-06-05T15:23:29.318186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **PRINCIPLE COMPONENT ANALYSIS (PCA)**\n\nPCA algorithm helps in optimizing the data to its best analytical form. It removes any unneeded features which have zero or play next to negligible part in determining the accuracy of our models and reducing the dimentions of our data. It is usually helpful in datasets which have a strong correlation between features (>30%). \n\nAfter analysing the correlation heatmap of our dataset, most of the features have a correlation of less than 30% with some even showing negative correlation. Due to this, it would be redundant to apply PCA in this dataset.","metadata":{}},{"cell_type":"markdown","source":"# **MODEL IMPLEMENTATIONS WITH HYPER PARAMETER TUNING**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:23:29.457283Z","iopub.execute_input":"2021-06-05T15:23:29.457744Z","iopub.status.idle":"2021-06-05T15:23:29.469713Z","shell.execute_reply.started":"2021-06-05T15:23:29.457708Z","shell.execute_reply":"2021-06-05T15:23:29.469024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **LOGISTIC REGRESSION**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression \nclassifier_log = LogisticRegression ()\nclassifier_log.fit (X_train, Y_train)\nY_pred_log = classifier_log.predict (X_test)\nacc_log = accuracy_score (Y_test, Y_pred_log)\nparameters = [{'penalty': ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n                'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}]\ngrid_search = GridSearchCV(estimator = classifier_log,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)\nbest_accuracy_log = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(best_parameters)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:23:29.4709Z","iopub.execute_input":"2021-06-05T15:23:29.471483Z","iopub.status.idle":"2021-06-05T15:23:31.622789Z","shell.execute_reply.started":"2021-06-05T15:23:29.471447Z","shell.execute_reply":"2021-06-05T15:23:31.6221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **K-NN MODEL**","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nclassifier_knn = KNeighborsClassifier ()\nclassifier_knn.fit (X_train, Y_train)\nY_pred_knn = classifier_knn.predict (X_test)\nacc_knn = accuracy_score (Y_test, Y_pred_knn)\nparameters = [{'n_neighbors': [3,5,7,10,13,15], 'weights': ['uniform', 'distance'],\n                'p': [1,2]}] \ngrid_search = GridSearchCV(estimator = classifier_knn,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)\nbest_accuracy_knn = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(best_parameters)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T16:44:34.323809Z","iopub.execute_input":"2021-06-05T16:44:34.324234Z","iopub.status.idle":"2021-06-05T16:44:34.802948Z","shell.execute_reply.started":"2021-06-05T16:44:34.324204Z","shell.execute_reply":"2021-06-05T16:44:34.801933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **NAIVE BAYES MODEL**","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nclassifier_nb = GaussianNB ()\nclassifier_nb.fit (X_train, Y_train)\nY_pred_nb = classifier_nb.predict (X_test)\nacc_nb = accuracy_score (Y_test, Y_pred_nb)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:23:32.556017Z","iopub.execute_input":"2021-06-05T15:23:32.556375Z","iopub.status.idle":"2021-06-05T15:23:32.565835Z","shell.execute_reply.started":"2021-06-05T15:23:32.55634Z","shell.execute_reply":"2021-06-05T15:23:32.564748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **SVM MODEL**","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nclassifier_svm = SVC (kernel = 'rbf', random_state = 0)\nclassifier_svm.fit (X_train, Y_train)\nY_pred_svm = classifier_svm.predict (X_test)\nacc_svm = accuracy_score (Y_test, Y_pred_svm)\nparameters = [{'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'kernel': ['linear', 'rbf'],\n                'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\ngrid_search = GridSearchCV(estimator = classifier_svm,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)\nbest_accuracy_svm = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(best_parameters)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:23:32.567064Z","iopub.execute_input":"2021-06-05T15:23:32.567308Z","iopub.status.idle":"2021-06-05T15:24:26.576362Z","shell.execute_reply.started":"2021-06-05T15:23:32.567282Z","shell.execute_reply":"2021-06-05T15:24:26.574951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **DECISION TREE MODEL**","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nclassifier_dtc = DecisionTreeClassifier (criterion = 'entropy', random_state = 0)\nclassifier_dtc.fit (X_train, Y_train)\nY_pred_dtc = classifier_dtc.predict (X_test)\nacc_dtc = accuracy_score (Y_test, Y_pred_dtc)\nparameters = [{'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150], \n                'max_leaf_nodes': [2,4,6,10,15,30,40,50,100], 'min_samples_split': [2, 3, 4]}]\ngrid_search = GridSearchCV(estimator = classifier_dtc,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)\nbest_accuracy_dtc = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(best_parameters)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:24:26.577569Z","iopub.execute_input":"2021-06-05T15:24:26.577854Z","iopub.status.idle":"2021-06-05T15:24:34.504753Z","shell.execute_reply.started":"2021-06-05T15:24:26.577826Z","shell.execute_reply":"2021-06-05T15:24:34.503944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **RANDOM FOREST MODEL**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclassifier_rfc = RandomForestClassifier (n_estimators = 100, criterion = 'entropy', random_state = 1)\nclassifier_rfc.fit (X_train, Y_train)\nY_pred_rfc = classifier_rfc.predict (X_test)\nacc_rfc = accuracy_score (Y_test, Y_pred_rfc)\nparameters = [{'n_estimators': [100,200,300],\n               'max_features': ['auto', 'sqrt'],\n               'max_depth': [10,25,50,'none'],\n               'min_samples_leaf': [1, 2], \n               'min_samples_split': [2, 5]}]\ngrid_search = GridSearchCV(estimator = classifier_rfc,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)\nbest_accuracy_rfc = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(best_parameters)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:24:34.505852Z","iopub.execute_input":"2021-06-05T15:24:34.50616Z","iopub.status.idle":"2021-06-05T15:26:06.780846Z","shell.execute_reply.started":"2021-06-05T15:24:34.506139Z","shell.execute_reply":"2021-06-05T15:26:06.779699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **XGBOOST MODEL**","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nclassifier_xgb = XGBClassifier (use_label_encoder = False)\nclassifier_xgb.fit (X_train, Y_train, eval_metric = \"logloss\")\nY_pred_xgb = classifier_xgb.predict (X_test)\nacc_xgb = accuracy_score (Y_test, Y_pred_xgb)\nparameters = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2],\n        'max_depth': [3, 4, 5]}\ngrid_search = GridSearchCV(estimator = classifier_xgb,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)\nbest_accuracy_xgb = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(best_parameters)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:28:39.969569Z","iopub.execute_input":"2021-06-05T15:28:39.969857Z","iopub.status.idle":"2021-06-05T16:04:45.946446Z","shell.execute_reply.started":"2021-06-05T15:28:39.969833Z","shell.execute_reply":"2021-06-05T16:04:45.945774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **ACCURACY COMPARISON**","metadata":{}},{"cell_type":"code","source":"prediction_columns = [\"NAME OF MODEL\", \"ACCURACY SCORE\"]\ndf_pred = {\"NAME OF MODEL\" : [\"LOGISTIC REGRESSION\", \"K-NN\", \"NAIVE BAYES\", \"SVM\", \"DECISION TREE\", \"RANDOM FOREST\", \"XGBOOST\"],\n           \"ACCURACY SCORE \" : [acc_log, acc_knn, acc_nb, acc_svm, acc_dtc, acc_rfc, acc_xgb],\n           \"BEST ACCURACY (AFTER HYPER-PARAMETER TUNING)\" : [best_accuracy_log, best_accuracy_knn, \"-\", best_accuracy_svm, best_accuracy_dtc, best_accuracy_rfc, best_accuracy_xgb]}\ndf_predictions = pd.DataFrame (df_pred)\ndf_predictions","metadata":{"execution":{"iopub.status.busy":"2021-06-05T16:44:37.548603Z","iopub.execute_input":"2021-06-05T16:44:37.548921Z","iopub.status.idle":"2021-06-05T16:44:37.56102Z","shell.execute_reply.started":"2021-06-05T16:44:37.548895Z","shell.execute_reply":"2021-06-05T16:44:37.560505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **CONCLUSION**\n\nAfter applying EDA, VDA and numerous algorithms, finally it came down to the **K-NN Model** with the highest accuracy score of **0.850395 (85%)** after hyper-parameter tuning.","metadata":{}}]}