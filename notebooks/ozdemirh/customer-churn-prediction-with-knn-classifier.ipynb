{"cells":[{"metadata":{},"cell_type":"markdown","source":"K Nearest Neighbor (KNN) is a supervised machine learning algorithm which can be used for both classification and regression. It does not use a model that generalizes on training data, that's why it's described as a **lazy learning method**. On the other hand, the methods using models to generalize on training data are called **eager learning methods**, e.g. neural network, SVM, tree based methods. On inference, lazy learning methods are slow and computationally expensive whereas eager learning methods do the hardwork during training. Lazy learning is especially suitable if training data is updated very often.\n\nKNN is also denoted as a nonparametric method which means it does not make any assumptions on data. On the other hand, a parametric method makes some strong assumptions. For example, if you want to fit a probability distribution to your data and assume Gaussian distribution, this is a parametric method. You only need to compute mean and standard deviation. If your assumption is consistent with your data, then your method gives good results, otherwise your method may fail. As a nonparametric method, KNN is suitable for both linear and nonlinear cases.\n\nNow, think about classification. How does KNN do classification without training? When the class of a test sample is queried, KNN inspects the similarity of all training samples with test sample. Degree of similarity is measured with a distance metric. It is assumed that if two samples are close to each other in feature space, they probably belong to same class. Searching for the closest point in a set is named as nearest neighbor search. KNN takes K closest samples. Final decision is made with majority voting. The mode of the classes of K nearest neighbors is the class of the queried test sample.\n\nK is a hyperparameter that determines the sensitivity of KNN. As K increases, the number of voting samples increases, decreasing the sensitivity. Large K results in low variance, high bias and small K results in high variance, low bias.\n\nOutline of the work is as follows:\n\n* Load Data\n* Feature Engineering\n* Split Data\n* Outlier Check with IQR\n* Visualization\n* Standardization\n* Correlation Analysis\n* KNN with Brute NN Search\n* KNN with KDTree"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sea\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sea.set_style(\"darkgrid\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Data\n\n**Telco Customer Churn** dataset is used. Following information is included:\n\n* Customers who left within the last month – target column **Churn**\n* Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support and streaming TV and movies\n* Customer account information – how long they’ve been a customer, contract, payment method, paperless billing, monthly charges and total charges\n* Demographic info about customers – gender, age range, and if they have partners and dependents\n\nDataset is loaded from input csv file. **Pandas** extracts the data and stores in a dataframe. Pandas styling is used to customize the look of the table."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/telco-customer-churn/\"\n                   \"WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n\ndata.head(10).style.set_precision(2). \\\n                    set_properties(**{\"min-width\": \"80px\"}). \\\n                    set_properties(**{\"color\": \"#111111\"}). \\\n                    set_properties(**{\"text-align\": \"center\"}). \\\n                    set_table_styles([\n                          {\"selector\": \"th\",\n                           \"props\": [(\"font-weight\", \"bold\"),\n                                     (\"font-size\", \"12px\"),\n                                     (\"text-align\", \"center\")]},\n                          {\"selector\": \"tr:nth-child(even)\",\n                           \"props\": [(\"background-color\", \"#f2f2f2\")]},\n                          {\"selector\": \"tr:nth-child(odd)\",\n                           \"props\": [(\"background-color\", \"#fdfdfd\")]},\n                          {\"selector\": \"tr:hover\",\n                           \"props\": [(\"background-color\", \"#bcbcbc\")]}])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**customerID** has nothing to do with churn prediction, so it's dropped."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(\"customerID\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features and corresponding labels are assigned to **data_X** and **data_Y**, respectively. Using Pandas info function, we inspect column data types and number of non-null values in data_X and data_Y."},{"metadata":{"trusted":true},"cell_type":"code","source":"# disable SettingWithCopyWarning\npd.options.mode.chained_assignment = None\n\ndata_X = data.loc[:, data.columns != \"Churn\"]\ndata_Y = data[[\"Churn\"]]\n\nprint(\"\\ndata_X info:\\n\")\ndata_X.info()\nprint(\"\\ndata_Y info:\\n\")\ndata_Y.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset has 7043 rows (training samples). There are 19 features. Target column **Churn** is also categorical."},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering\n\nThe unique values each feature can take are inspected below."},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in data_X.columns:\n    \n    print(\"Feature name: {}\".format(c))\n    print(\"Unique values:\\n\")\n    print(data_X[c].unique())\n    print(\"\\n--------------------------------------------------\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features gender, Partner, Dependents, PhoneService, MultipleLines, InternetService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies, Contract, PaperlessBilling and PaymentMethod are categorical. Although datatype of SeniorCitizen is int64, it is categorical, it takes 0 and 1 values.\n\nFeatures tenure, MonthlyCharges and TotalCharges are numeric. Note that data type of TotalCharges is object (as info() function shows). But, if we look carefully, the entries of TotalCharges are float values converted to string.\n\nSpaces are removed from each entry of TotalCharges if there are any, then datatype is converted to float."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_X[\"TotalCharges\"] = [s.replace(\" \",\"\")\n                          for s in data_X[\"TotalCharges\"]]\ndata_X[\"TotalCharges\"] = pd.to_numeric(data_X[\"TotalCharges\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have to check if there are any null values in TotalCharges."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_X[\"TotalCharges\"].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Impute null values with mean."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_X[\"TotalCharges\"].fillna(data_X[\"TotalCharges\"].mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Categorical and numeric features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat = [\"gender\", \"SeniorCitizen\", \"Partner\", \"Dependents\", \"PhoneService\",\n       \"MultipleLines\", \"InternetService\", \"OnlineSecurity\",\n       \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\",\n       \"StreamingTV\", \"StreamingMovies\", \"Contract\",\n       \"PaperlessBilling\", \"PaymentMethod\"]\n\nnum = [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Categorical features are converted to numeric. One of the categories is dropped to prevent correlation between features."},{"metadata":{"trusted":true},"cell_type":"code","source":"enc = OneHotEncoder(drop=\"first\")\nenc.fit(data_X[cat]);\n\ncat2 = enc.get_feature_names(cat)\ndata_X_C = pd.DataFrame(enc.transform(data_X[cat]).toarray(),\n                        columns = cat2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Numeric and one-hot-encoded categorical features are combined into a single dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_X = pd.concat([data_X_C, data_X[num]], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature names are:"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = data_X.columns\nprint(feature_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unique values of target variable:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_Y[\"Churn\"].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 2 output classes as expected. Churn is converted to binary."},{"metadata":{"trusted":true},"cell_type":"code","source":"lb = LabelBinarizer()\n\nlb.fit(data_Y[\"Churn\"]);\ndata_Y[\"Churn\"] = lb.transform(data_Y[\"Churn\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split Data\n\nDataset is split as training and test sets. We use stratify parameter of train_test_split function to get the same class distribution across train and test sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X, test_X, train_Y, test_Y = train_test_split(data_X, data_Y,\n                                                    test_size=0.2,\n                                                    shuffle = True,\n                                                    stratify=data_Y,\n                                                    random_state=0)\n\ntrain_X.reset_index(drop=True, inplace=True);\ntest_X.reset_index(drop=True, inplace=True);\ntrain_Y.reset_index(drop=True, inplace=True);\ntest_Y.reset_index(drop=True, inplace=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Outlier Check with IQR\n\nNumeric features are analyzed for outliers using **interquartile range (IQR)**."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(10,6))\nfor i, c in enumerate(train_X[num]):\n    sea.boxplot(train_X[c], orient=\"v\", color = \"#6f7501\",\n                                width = 0.2, ax=axes[i])\n    \nfig.tight_layout(pad=3.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no outliers as can be seen from the box plots."},{"metadata":{},"cell_type":"markdown","source":"## Visualization\n\nCategorical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,66))\ngs = gridspec.GridSpec(nrows=19, ncols=2, figure=fig)\n\nfor i, c in enumerate(train_X[cat2]):\n    y, x = np.int(i/2), i%2 \n    ax = fig.add_subplot(gs[y,x])    \n    sea.distplot(train_X.loc[train_Y[\"Churn\"]==0,c], kde = False,\n                 color = \"#004a4d\", hist_kws = dict(alpha=0.7),\n                 bins=10, label=\"Churn_No\", ax=ax);\n    sea.distplot(train_X.loc[train_Y[\"Churn\"]==1,c], kde = False,\n                 color = \"#7d0101\", hist_kws = dict(alpha=0.7),\n                 bins=10, label=\"Churn_Yes\", ax=ax);\n\nax.legend(loc=\"center left\", bbox_to_anchor=(1.5,0.5),\n          prop={\"size\":12});","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,8))\ngs = gridspec.GridSpec(nrows=2, ncols=2, figure=fig)\n\nfor i, c in enumerate(train_X[num]):\n    y, x = np.int(i/2), i%2 \n    ax = fig.add_subplot(gs[y,x])    \n    sea.distplot(train_X.loc[train_Y[\"Churn\"]==0,c], kde = True,\n                 color = \"#004a4d\", hist_kws = dict(alpha=0.8),\n                 bins=20, label=\"Churn_No\", ax=ax);\n    sea.distplot(train_X.loc[train_Y[\"Churn\"]==1,c], kde = True,\n                 color = \"#7d0101\", hist_kws = dict(alpha=0.5),\n                 bins=20, label=\"Churn_Yes\", ax=ax);\n\nax.legend(loc=\"center left\", bbox_to_anchor=(1.5,0.5),\n          prop={\"size\":12});","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Standardization\n\nStandardScaler is only fit to training data to prevent data leakage."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\n\n# fit to train_X\nscaler.fit(train_X)\n\n# transform train_X\ntrain_X = scaler.transform(train_X)\ntrain_X = pd.DataFrame(train_X, columns = feature_names)\n\n# transform test_X\ntest_X = scaler.transform(test_X)\ntest_X = pd.DataFrame(test_X, columns = feature_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = pd.concat([train_X, train_Y], axis=1).corr()\nmask = np.triu(np.ones_like(corr_matrix, dtype=np.bool))\n\nplt.figure(figsize=(10,8))\nsea.heatmap(corr_matrix,annot=False, fmt=\".1f\", vmin=-1,\n            vmax=1, linewidth = 1,\n            center=0, mask=mask,cmap=\"RdBu_r\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some features are dropped due to high correlation. Then, dataframes are converted to numpy arrays."},{"metadata":{"trusted":true},"cell_type":"code","source":"drop = [\"OnlineSecurity_No internet service\",\n        \"OnlineBackup_No internet service\",\n        \"DeviceProtection_No internet service\",\n        \"TechSupport_No internet service\",\n        \"StreamingTV_No internet service\",\n        \"StreamingMovies_No internet service\",\n        \"MultipleLines_No phone service\"]\n\nfor d in drop:\n    train_X.drop(d, axis=1, inplace=True)\n    test_X.drop(d, axis=1, inplace=True)\n    \nnp_train_X = train_X.values\nnp_train_Y = train_Y.values.ravel()\nnp_test_X = test_X.values\nnp_test_Y = test_Y.values.ravel()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNN with Brute NN Search\n\nWe will try KNN first with brute nearest neighbor search. We use grid search to find the optimal parameters and use stratified 5-fold for cross validation. Minkowski is used as distance metric and its value is searched (1 or 2). When p equals 1, Minkowski is Manhattan distance and when p equals 2, it is Euclidean distance. During grid search, model performance with each parameter combination is measured on cross validation folds. The parameters giving the highest performance is returned as best parameter set."},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_cls = KNeighborsClassifier()\nparameters = {\n    \"n_neighbors\": range(30, 50, 2),\n    \"metric\": [\"minkowski\"],\n    \"p\": [1.0, 2.0],\n    \"algorithm\": [\"brute\"]\n}\n\nskf_cv = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\ngscv = GridSearchCV(\n    estimator=knn_cls,\n    param_grid=parameters,\n    scoring=\"f1\",\n    n_jobs=-1,\n    cv=skf_cv,\n    verbose=False\n)\n\ngscv.fit(np_train_X, np_train_Y)\nprint(\"Best parameters {}\".format(gscv.best_params_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We train a new KNeighborsClassifier with the best parameters on np_train_X. Then we make predictions on np_test_X."},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_cls = KNeighborsClassifier(**gscv.best_params_)\nknn_cls.fit(np_train_X, np_train_Y)\ny_pred = knn_cls.predict(np_test_X)\nprint(classification_report(np_test_Y, y_pred,\n                            target_names=[\"Churn No\", \"Churn Yes\"]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNN with KDTree\n\nIn nearest neighbor search, data structures like kdtree can be incorporated instead of using KNN in its original form with brute search. kdtree learns which training sample is residing on which part of the feature space. On inference, it takes you to the close proximity of test sample and gives you the neighbors. kdtree allows you to search multidimensional space efficiently.\n\nWhen creating kdtree, each node splits data using 1 dimension (1 feature). The split point is determined as the median of points along that dimension. The seperating hyperplane is orthogonal to dimension axis. The points on the left of hyperplane go to left child, the points on the right go to right child node. Choosing the number of points on each leaf, we slice the space into subspaces with the resolution we want. When a leaf is reached, we get a number of training points that we are interested in and KNeighborsClassifier switches to brute nearest neighbor search on this set.\n\nBelow, we do another grid search with KNeighborsClassifier to find optimal K, Minkowski p value when algorithm parameter is set to kd_tree. This time we have an extra hyperparameter, leaf_size denotes the number of points in each leaf."},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_cls = KNeighborsClassifier()\nparameters = {\n    \"n_neighbors\": range(40, 60, 2),\n    \"leaf_size\": [1, 2, 3],\n    \"metric\": [\"minkowski\"],\n    \"p\": [1.0, 2.0],\n    \"algorithm\": [\"kd_tree\"]\n}\n\nskf_cv = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\ngscv = GridSearchCV(\n    estimator=knn_cls,\n    param_grid=parameters,\n    scoring=\"f1\",\n    n_jobs=-1,\n    cv=skf_cv,\n    verbose=False\n)\n\ngscv.fit(np_train_X, np_train_Y)\nprint(\"Best parameters {}\".format(gscv.best_params_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We train a new KNeighborsClassifier with the best parameters on np_train_X. Then we make predictions on np_test_X."},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_cls = KNeighborsClassifier(**gscv.best_params_)\nknn_cls.fit(np_train_X, np_train_Y)\ny_pred = knn_cls.predict(np_test_X)\nprint(classification_report(np_test_Y, y_pred,\n                            target_names=[\"Churn No\", \"Churn Yes\"]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}