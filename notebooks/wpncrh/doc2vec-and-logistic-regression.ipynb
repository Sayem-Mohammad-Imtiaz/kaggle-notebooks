{"metadata":{"language_info":{"name":"python","version":"3.6.3","nbconvert_exporter":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","file_extension":".py","codemirror_mode":{"name":"ipython","version":3}},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"cells":[{"cell_type":"markdown","metadata":{},"source":"# Introduction\n\nHere, we try to train a doc2vec model with the gensim library. Then, we use the numerical vectors for each indiviual's text to train a binary model on whether or not they are introverted or extroverted.\n\nA logistic regression attains a test accuracy of 81% versus a naive, baseline classification of 77%.\n\nThis very much builds off of the work of [Rare](https://github.com/RaRe-Technologies/gensim) Technologies. Thanks to them!\n\nFYI - the model training takes ~20 minutes as currently configured. "},{"cell_type":"code","metadata":{"_uuid":"c4ff9c6b7ef4bbd8a4ec4125a5c8091a25af5fed","collapsed":true,"_cell_guid":"12b41842-1204-4979-a82f-2e0d1e377aa4"},"source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gensim\nimport os\nimport collections\nimport smart_open\nimport random\n\n","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"source":"def read_corpus(fname, tokens_only=False):\n    with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n        for i, line in enumerate(f):\n            if tokens_only:\n                yield gensim.utils.simple_preprocess(line)\n            else:\n                # For training data, add tags\n                if i==0:\n                    pass\n                else:\n                    yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line)[1:], \n                                                                                 gensim.utils.simple_preprocess(line)[0]) # tag is first item, the rest is text\n                                                           \n                                                           ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{},"source":"all_data = list(read_corpus('../input/mbti_1.csv'))\ntotal_num_obs = len(all_data)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"source":"# create train and test by just doing a 75/25% split\nfrom math import floor, ceil\ntrain_corpus = all_data[0:floor(3*total_num_obs/4)]\ntest_corpus = all_data[floor(3*total_num_obs/4):]","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":"## Training the Model"},{"cell_type":"code","metadata":{"collapsed":true},"source":"model = gensim.models.doc2vec.Doc2Vec(size=100, min_count=2, iter=55)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"source":"model.build_vocab(train_corpus) # remove infj, entp... to DO!","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{},"source":"%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter) # 20 mins?","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{},"source":"# play\nmodel.infer_vector(['I', 'feel', 'sad'])","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"source":"train_targets, train_regressors = zip(*[(doc.words, doc.tags[0]) for doc in train_corpus])\ntest_targets, test_regressors = zip(*[(doc.words, doc.tags[0]) for doc in test_corpus])","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{},"source":"X = []\nfor i in range(len(train_targets)):\n    X.append(model.infer_vector(train_targets[i]))\ntrain_x = np.asarray(X)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{},"source":"train_x.shape","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{},"source":"Y = np.asarray(train_regressors)\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(Y)\ntrain_y = le.transform(Y)\nnp.mean(train_y)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{},"source":"unique, counts = np.unique(Y, return_counts=True)\n\nprint(np.asarray((unique, counts)).T)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{},"source":"from sklearn import linear_model\nlogreg = linear_model.LogisticRegression()\nlogreg.fit(train_x, train_y)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"source":"test_list = []\nfor i in range(len(test_targets)):\n    test_list.append(model.infer_vector(test_targets[i]))\ntest_x = np.asarray(test_list)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"source":"test_Y = np.asarray(test_regressors)\ntest_y = le.transform(test_Y)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"source":"preds = logreg.predict(test_x)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{},"source":"np.mean(test_y)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{},"source":"sum(preds == test_y) / len(test_y)","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":"Our model has 81.1% accuracy. If you guess all introverts, you'll be right 77.4% of the time. So that's an improvement!** "},{"cell_type":"code","metadata":{"collapsed":true},"source":"","execution_count":null,"outputs":[]}],"nbformat_minor":1,"nbformat":4}