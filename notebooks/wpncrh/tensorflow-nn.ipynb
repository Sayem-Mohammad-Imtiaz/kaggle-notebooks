{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"e0bbd2eb-2f6c-0bf5-1171-ead8b0960fdd"},"source":"The following is an attempt to use Google's Tensorflow ML library to run a NN classifier. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f785ca3c-fe42-c8f7-e76a-33642418311b"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom sklearn.cross_validation import train_test_split\nimport random\n\nvoice =pd.read_csv('../input/voice.csv')\nvoice = pd.DataFrame(voice)\nprint(voice.head())\n# Any results you write to the current directory are saved as output."},{"cell_type":"markdown","metadata":{"_cell_guid":"897c3fd3-fd0c-fd08-6308-215085ac2b4f"},"source":"Let's get/make more training data. What I do create a new df and slightly edit. I add random noise (a small amount) to the meanfreq variable. Then I append this to the original pd df, making a much larger df. This for loop successively doubles the size of the training data. "},{"cell_type":"markdown","metadata":{"_cell_guid":"f22c7d0d-a759-740b-8f53-af567fdf0f05"},"source":"Now that we have imported pd data frame, we need to split into label and feature sets, and split into test and train np arrays."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"17450ab1-1082-d2f6-c4f8-94335f42edbc"},"outputs":[],"source":"for i in range(6):\n\tcopy = voice\n\tcopy['meanfreq']=copy['meanfreq']+random.gauss(.0001,.001) # add noice to mean freq var\n\tvoice=voice.append(copy,ignore_index=True) # make voice df 2x as big\n\tprint(\"shape of df after {0}th intertion of this loop is {1}\".format(i,voice.shape))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"555f2d7c-9cfc-c973-4f32-dd7a24a82475"},"outputs":[],"source":"label = voice.pop(\"label\")\n\n# converts from dataframe to np array\nvoice=voice.values\n\n# convert train labels to one hots\ntrain_labels = pd.get_dummies(label)\n# make np array\ntrain_labels = train_labels.values\n\nx_train,x_test,y_train,y_test = train_test_split(voice,train_labels,test_size=0.2)\n# # so no we have predictors and y values, separated into test and train\n\nx_train,x_test,y_train,y_test = np.array(x_train,dtype='float32'), np.array(x_test,dtype='float32'),np.array(y_train,dtype='float32'),np.array(y_test,dtype='float32')"},{"cell_type":"markdown","metadata":{"_cell_guid":"8a1c4f16-5153-7787-2485-b82d43e79e6a"},"source":"We now have training and test data. Let's ensure this all looks good:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1123464c-c673-dcfa-a51c-6f3947f2431a"},"outputs":[],"source":"print(\"x_train looks like:\\n\",x_train[1:2],\"\\nand y train looks like:\\n\",y_train[1:2])\n\n\nprint(\"shape of x_train is: \\n\",x_train.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"942cb882-4397-b261-8da3-32d1360d285f"},"source":"We have data on 20 features, with each set of 20 features predicting male or female voice (in a one hot encoded vector)."},{"cell_type":"markdown","metadata":{"_cell_guid":"5b7c9b6f-3b76-0697-8584-a17d7a5aef23"},"source":"Let's do some tensorflow. We need to create placeholders for data we'll feed in, and also placeholders for variables we'll use in our NN."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"362c8c22-09c2-ef76-b68e-8f5377a927a9"},"outputs":[],"source":"# place holder for inputs. feed in later\nx = tf.placeholder(\"float\", [None, 20])\n\n# # take 20 features to 1000 nodes in hidden layer. why? just cuz?\nw1 = tf.Variable(tf.random_normal([20, 1000],stddev=.5,name='w1'))\n# # add biases for each node\nb1 = tf.Variable(tf.zeros([1000]))\n# calculate activations \nhidden_output = tf.nn.softmax(tf.matmul(x, w1) + b1)\n\n# bring from 10 nodes to 2 for my output\nw2 = tf.Variable(tf.random_normal([1000, 2],stddev=.5,name='w2'))\n\nb2 = tf.Variable(tf.zeros([2]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e7d37e8c-b331-88d6-8bd0-5bba834114da"},"outputs":[],"source":"# placeholder for correct values \ny_ = tf.placeholder(\"float\", [None,2])\n# #implement model. these are predicted ys\ny = tf.nn.softmax(tf.matmul(hidden_output, w2) + b2)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"abbc1e68-5c61-1b90-7719-02b98c53627c"},"outputs":[],"source":"loss = tf.reduce_mean(tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(y, y_, name='xentropy')))"},{"cell_type":"markdown","metadata":{"_cell_guid":"be2b9b43-f1a4-fb62-bfe9-5361ed3e71fc"},"source":"Create an optimizer op, and a train step which minimizes the weight and bias vars. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"00e9adab-e1bf-63b0-8cb7-26554220f08e"},"outputs":[],"source":"opt = tf.train.AdamOptimizer(learning_rate=0.001)\ntrain_step = opt.minimize(loss, var_list=[w1,b1,w2,b2])"},{"cell_type":"markdown","metadata":{"_cell_guid":"7d8c879e-6f8f-72f4-2c2c-10e2247d63fc"},"source":"Create a way to assess the accuracy of our prediction:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b7c59dc-1073-e236-c7bf-046cad795cf9"},"outputs":[],"source":"tf_correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\ntf_accuracy = tf.reduce_mean(tf.cast(tf_correct_prediction, \"float\"))"},{"cell_type":"markdown","metadata":{"_cell_guid":"8077db30-5f4c-d94f-4bc6-79463aac8f91"},"source":"Start tf session and initialize variables. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"730cb9a9-da5f-5e6e-59fd-3e2db56440e6"},"outputs":[],"source":"def get_mini_batch(x,y):\n\trows=np.random.choice(x.shape[0], 100)\n\treturn x[rows], y[rows]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"037d6bc1-f632-b8d7-efb7-4e9b6b1366e5"},"outputs":[],"source":"# start session\nsess = tf.Session()\nsummary_writer = tf.train.SummaryWriter('voices')\n# summary_writer = tf.train.SummaryWriter('voices', sess.graph)\n\n# # init all vars\ninit = tf.initialize_all_variables()\nsess.run(init)"},{"cell_type":"markdown","metadata":{"_cell_guid":"bdd28995-d6cf-848f-23ba-3c99839e6472"},"source":"Now, it is time to train. Run a loop for ntrials iterations. Feed the training data into the train_step. Then print our accuracy on the test set and the corresponding value of the loss function."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f3801aa5-73d2-6974-c8de-99f5791d7579"},"outputs":[],"source":"ntrials = 10000\nfor i in range(ntrials):\n    # get mini batch\n    a,b=get_mini_batch(x_train,y_train)\n    # run train step, feeding arrays of 100 rows each time\n    _, cost =sess.run([train_step,loss], feed_dict={x: a, y_: b})\n    if i%100 ==0:\n    \tprint(\"epoch is {0} and cost is {1}\".format(i,cost))"},{"cell_type":"markdown","metadata":{"_cell_guid":"84479181-b723-2fc7-3908-91ad4ca9bbf5"},"source":"Check accuracy on test set:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"56faa7fd-f7cc-c7fa-208d-5c0d59a55971"},"outputs":[],"source":"result = sess.run(tf_accuracy, feed_dict={x: x_test, \n                                          y_: y_test})\n\nprint(\"Test accuracy: {}\".format(result))"},{"cell_type":"markdown","metadata":{"_cell_guid":"95ccc7b8-e328-f7b7-1eb0-d9977bc50ce5"},"source":"To get a sense of what the predictions look like:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5e105643-f591-f71c-e19f-d02873dd2daa"},"outputs":[],"source":"ans = sess.run(y, feed_dict={x: x_test})\nprint(y_test[0:3])\nprint(\"Correct prediction\\n\",ans[0:3])"},{"cell_type":"markdown","metadata":{"_cell_guid":"f7d778b5-ce2a-29bd-97c0-cee999b68320"},"source":"AdamOptimizer and running for 10,000 iterations made big difference in performance. How can we get up to 99,100%?"},{"cell_type":"markdown","metadata":{"_cell_guid":"ca34dc6e-0454-ecb9-d172-ec9238e8b359"},"source":"Hopefully someone else got something out of this, but I am also doing this (primarily) to develop some DS skills. In this kernel, I successfully implemented Tensorflow. Some struggles I had to overcome: \n\n - Using pd.getdummies to convert to a one-hot vector.\n - Use .values to convert pd df (which is much easier to manipulate) to a np array (which tensorflow reads).\n - Ensure np.array dtype.\n - Use cross_entropy_with_logits to deal with outputs close to 1 and 0.\n - Create function to extract random sample for a mini_batch for NN training.\n - Getting accustomed to TF's idea of placeholders and computational graphs.\n\n "}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}