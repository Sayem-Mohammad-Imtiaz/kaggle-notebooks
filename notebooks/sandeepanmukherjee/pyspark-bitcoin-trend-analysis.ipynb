{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bitcoin analysis based on historical data using Pyspark \n\n**Goals**:\n1. Importing Bitcoin Historical dataset to Spark.\n2. Understanding of the dataset\n3. Visualizing the data \n3. Performing analysis by Linear Regression\n4. Present/Visualize your results.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Cryptocurrency** -> \"Digital currency\", used in mathematical operations and with a strong appeal for decentralization of financial resources and greater agility, privacy and security than the current alternatives.\n\n**BTC** -> Bitcoin, the first cryptography developed and launched commercially for trading, considered stronger and more stable than those currently existing due to its \"resilience\" to crises and devaluations.\n\n**USD** -> United States Dollar or American Dollar, currency used as a reference for most exchanges, due to an easy conversion between BTC and USD.\n\n**Bitstamp** -> Online crypto exchange responsible for facilitating negotiations between traders; Information related to the value and how it was made, serving as a database for this study.\n\n**Negotiator** -> Negotiator who operates in the market (in this case, crypto), using, through consecutive purchase and sale operations, to make a profit.\n\n**Long** -> Investment used / \"betting\" profit in case of increase in the value of the asset, normally involving the purchase of lower values ​​and the sale of higher values.\n\n**Short** -> Wins by \"renting\" higher value assets, selling the same rights and buying back a lower price, returning the same amount of assets (which is less than \"rented\") and profiting from a difference between the value at the time of the sale and the repurchase.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Columns\n\n**Timestamp** -> Date (in Epoch Unix format) of data collection; It will later be transformed into a \"human\" date for better understanding; Intervals of approximately 1 in 1 minute, with time zone set to UTC.\n\n**Open** -> Initial currency trading value in that measurement range, in USD.\n\n**High** -> Highest value reached by the asset during that measurement interval, in USD.\n\n**Low** -> Lowest value reached by the asset during that measurement interval, in USD.\n\n**Close** -> Value of the asset at the time of closing the measurement range, in USD.\n\n**Volume_ (BTC)** -> Volume, in BTC, traded on Bitstamp during a given measurement interval\n\n**Volume_ (Currency)** -> Volume, in USD, traded on Bitstamp during a given measurement interval;\n\n**Weighted_Price** -> Average asset price in that range, in USD; Calculated based on traded volumes; It will be considered as the average price for analytical issues.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Installing Spark and importing all required ML and Visualization Libraries","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import StringIndexer\n\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.mllib.evaluation import RegressionMetrics\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.stat import Correlation\nfrom pyspark.ml.feature import MinMaxScaler\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\nfrom pyspark.ml.evaluation import RegressionEvaluator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n#visualization\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_colwidth', 400)\n\nfrom matplotlib import rcParams\nsns.set(context='notebook', style='whitegrid', rc={'figure.figsize': (18,4)})\nrcParams['figure.figsize'] = 18,4\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting random seed for notebook reproducability\nrnd_seed=23\nnp.random.seed=rnd_seed\nnp.random.set_state=rnd_seed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating the Spark Session","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = SparkContext(appName = \"Bitcoin_Analysis\")\nspark = SparkSession.Builder().getOrCreate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sqlContext = SQLContext(spark.sparkContext)\nsqlContext","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Performing Spark commands using Pyspark","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading Data to dataframe\ndf= sqlContext.read.csv('../input/bitcoin-historical-data/bitstampUSD_1-min_data_2012-01-01_to_2020-04-22.csv', header='true', inferSchema='true')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting the schema info from the dataframe\ndf.printSchema()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration and Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count total number of rows\ndf.count()\n# Count the total number of columns\nlen(df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.createOrReplaceTempView('bitstampUSD') #creating a temp view table\nbitstampUSD = sqlContext.sql(\"select *,from_unixtime(Timestamp) as `dateTime` from bitstampUSD\")\nbitstampUSD=bitstampUSD.dropna('any') # Removing null values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bitstampUSD.show()\n# Count total number of rows after removing the null values\nbitstampUSD.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Renaming the columns \nbitstampUSD=bitstampUSD.withColumnRenamed(\"Volume_(BTC)\", \"VolBTC\").withColumnRenamed(\"Volume_(Currency)\", \"VolCurrency\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating another dataframe to perform Datetime Visualization \nDateTime_df= bitstampUSD\n#DateTime_df.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the dataframe by required columns\nsplitDF = DateTime_df.withColumn(\"date\",split(col(\"dateTime\"),\" \").getItem(0))\nsplitDF = splitDF.withColumn(\"time\",split(col(\"dateTime\"),\" \").getItem(1))\n#splitDF.printSchema()\n#splitDF.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the dataframe by required columns\nsplitDFHour = splitDF.withColumn(\"hour\",split(col(\"time\"),\":\").getItem(0))\n#splitDFHour.printSchema()\n#splitDFHour.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Arranging Dataframe with column date and time and day_of_week\nsplitDFHour= splitDFHour.withColumn(\"date\",splitDFHour[\"date\"].cast(DateType())).withColumn(\"hour\",splitDFHour[\"hour\"].cast(DoubleType())).withColumn(\"dateTime\",splitDFHour[\"dateTime\"].cast(DateType()))\nsplitDFHour=splitDFHour.withColumn('day_of_week',dayofweek(splitDFHour.date))\n#splitDFHour.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Arranging Dataframe with column date and time and year\nsplitDFWithYear = splitDFHour.withColumn(\"year\",split(col(\"date\"),\"-\").getItem(0))\nsplitDFWithYear.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Spark to Pandas dataframe\npandas_converted=splitDFWithYear.toPandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting the values in list so that we can plot and visualize it\nhour=pandas_converted[\"hour\"].values.tolist()\nweighted_price=pandas_converted[\"Weighted_Price\"].values.tolist()\nvolume_BTC=pandas_converted[\"VolBTC\"].values.tolist()\ndate_of_week=pandas_converted[\"day_of_week\"].values.tolist()\nyear=pandas_converted[\"year\"].values.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation plotting the heatmap using Seaborn\ncorr=pandas_converted.corr()\nf,ax = plt.subplots(figsize=(12, 12))\nsns.heatmap(corr,annot=True,linewidths=.5, fmt= '.1f',ax=ax)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter Plot usage by Matplotlib\n# x = open, y = close\npandas_converted.plot(kind='scatter', x='VolBTC', y='VolCurrency',alpha = 0.5)\nplt.xlabel('BTC Volume')            \nplt.ylabel('Currency Volume')\nplt.title('BTC-Currency Scatter Plot') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Line Plot usage by Matplotlib plotting Open with High with Time on X axis\n# color = color, label = label, linewidth = width of line, alpha = opacity, grid = gray square background, linestyle = sytle of line\n\nplt.figure(figsize=(16,4))\npandas_converted.Open.plot(kind='line', color='r', label='Open', alpha=0.5, linewidth=5, grid=True, linestyle=':')\npandas_converted.High.plot(color='g', label='High', linewidth=1, alpha=0.5, grid=True, linestyle='-.')\nplt.legend(loc='upper right') #legend put label into plot\nplt.xlabel('Time')\nplt.ylabel('price at the start of the time window')\nplt.title('Line plot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Histogram plot of Open price\npandas_converted.Open.plot(kind='hist', bins=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot of Weighted_price per hour in scatter plot using Matplotlib\nplt.plot(hour,weighted_price , 'bo')\n#group1_pd.plot(kind='scatter', x='hour', y='Weighted_Price',alpha = 0.01)\nplt.xlabel('hour')            \nplt.ylabel('Weighted_Price')\nplt.title('Price by Hour') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot of Weighted_price per week in scatter plot using Matplotlib\nplt.plot(date_of_week, weighted_price, 'ro')\n#group1_pd.plot(kind='scatter', x='hour', y='Weighted_Price',alpha = 0.01)\nplt.xlabel('day_of_week')            \nplt.ylabel('Weighted_Price')\nplt.title('Price By Week') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot of VolBTC per hour change displayed in scatter plot using Matplotlib\nplt.plot(hour, volume_BTC, 'g*')\n#group1_pd.plot(kind='scatter', x='hour', y='Weighted_Price',alpha = 0.01)\nplt.xlabel('hour')            \nplt.ylabel('VolBTC')\nplt.title('Volume by Hour of day') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot of VolBTC per day_of_Week change displayed in scatter plot using Matplotlib\nplt.plot(date_of_week,volume_BTC , 'r*')\nplt.xlabel('day_of_week')            \nplt.ylabel('VolBTC')\nplt.title('Volume By Week') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot of Price per year change displayed in scatter plot using Matplotlib\nplt.plot(year,weighted_price , 'm^')\nplt.xlabel('year')            \nplt.ylabel('Weighted_Price')\nplt.title('Weighted Price plotted yearly basis') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot of Price per year change displayed in scatter plot using Matplotlib\nplt.plot(year,volume_BTC , 'kD')\nplt.xlabel('year')            \nplt.ylabel('volume_BTC')\nplt.title('volume_BTC plotted yearly basis') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering/Extraction\n**Use a VectorAssembler to put features into a feature vector column**\n\nWe have generated enough information post preprocessing our data to feed it to the model.\nWe are aren't caring about missing values; all zero values have been excluded from the data set.\nNext we would normalize our data, as minimize the ranges between Min and max values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# put features into a feature vector column\nassembler = VectorAssembler(inputCols=['Open','High', 'VolBTC', 'VolCurrency','Weighted_Price'], outputCol=\"features\")\nassembled_df = assembler.transform(bitstampUSD)\nassembled_df.show(10, truncate= False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result displays that the features have transformed into a Dense Vector.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Normalization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we can scale the data using Min_Max_Scaler. The input columns are the features, and the output column with the rescaled that will be included in the scaled_df will be named \"scaled_features\".\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the `Min_Max_scaler`\nMin_Max_scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n# Fit the DataFrame to the scaler\nscaled_df= Min_Max_scaler.fit(assembled_df).transform(assembled_df)\n# Inspect the result\nscaled_df.select(\"features\", \"scaled_features\").show(10, truncate=False)\nscaled_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building A Machine Learning Model With Spark ML","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As we have finished the preprocessing of our data, now we would build our Linear Regression model. We first need to split the data into training and test sets which we would do with the randomSplit() method:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data into train,test and Validation sets\ntrain_data, test_data = scaled_df.randomSplit([.7,.3], seed=rnd_seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we create an ElacticNet Linear Regression Model**\nElasticNet is a linear regression model trained with both l1 and l2 -norm regularization of the coefficients. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. We control the convex combination of l1 and l2 using the l1_ratio parameter.\nElastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\nSource: https://scikit-learn.org/stable/modules/linear_model.html#elastic-net","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize `lr`\nlr = (LinearRegression(featuresCol='scaled_features' , labelCol=\"Weighted_Price\", predictionCol='Predicted_price', \n                               maxIter=100, regParam=0.3, elasticNetParam=0.8, standardization=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the data to the model\nlinearModel = lr.fit(train_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Generate Predictions**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate predictions\npredictions = linearModel.transform(test_data)\n# Select the columns and store in a variable\npred_data= predictions.select(\"Predicted_price\", \"Weighted_Price\").show(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Inspect the Metrics**\n\nNow we look at some metrics to get a better idea of how good your model is performing actually. We will use Regression Evaluator and LinearRegressionModel.summary attribute to pull up the rootMeanSquaredError.\nThe RMSE measures how much error there is between two datasets comparing a predicted value and an observed or known value. The smaller an RMSE value, the closer predicted and observed values are.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select (predicted_price, Weighted_price label) and compute test error\nevaluator = RegressionEvaluator(\n    labelCol=\"Weighted_Price\", predictionCol=\"Predicted_price\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Inspect and Model the metrics and Coefficient and Visualize the log of the training error as a function of iteration. \n#The scatter plot visualizes the logarithm of the training error for all 10 iterations.\n\niterations = list(range(1,linearModel.summary.totalIterations + 1))\nlossHistory = np.log(linearModel.summary.objectiveHistory)\nplt.plot(iterations,lossHistory,'*')\nplt.title('Log Training Error vs. Iterations')\nplt.xlabel('Log Training Error')\nplt.ylabel('Iterations')\n# Intercept for the model\nprint(\"MAE: {0}\".format(linearModel.summary.meanAbsoluteError))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Use this model for prediction on test data. Calculate Root Mean Square Error of our model.\npredictionsTest = linearModel.transform(test_data)\npredictionsTest.select(\"Predicted_price\", \"Weighted_Price\").show(10)\n\nrmse = evaluator.evaluate(predictionsTest)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the method \"predict\" to predict an output on the polynomial features,\n# then use the function \"DistributionPlot\" to display the distribution of the Predicted_Price vs the Weighted_Price from the test data\n\npred_results=linearModel.evaluate(test_data)\nY = pred_results.predictions.select('Weighted_Price').toPandas()\n_Y = pred_results.predictions.select(\"Predicted_price\").toPandas()\nsns.set_style(\"dark\")\nax1= sns.distplot(Y, hist=False, color=\"r\", label=\"Actual Values\")\nsns.distplot(_Y, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting Weighted_price vs Predicted_price using Matplotlib\nplt.figure(figsize=(12,7))\nplt.plot(Y, color='green', marker='*', linestyle='dashed', \n         label='Predicted Price')\nplt.plot(_Y, color='red', label='Weighted Price')\nplt.title('BitCoin Weighted Prediction')\nplt.xlabel('Weighted_Price')\nplt.ylabel('Predicted_price')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spark.stop()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **Code Contributors:**\n\nThanks for the help my teammates. \n\nAnoop Kiran Angadi Manjunath\n\nVinay Kashyap\n\nPathey Atulkumar Pandya\n\nRegards,\n\nSandeepan Mukherjee","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}