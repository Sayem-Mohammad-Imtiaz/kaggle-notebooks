{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Mushroom classification using Random Forest Classifier"},{"metadata":{},"cell_type":"markdown","source":"This notebook describes the development of a classification algorithm based on the use of Random Forest Classifier. The objective is to distinguish eadible and poisonous mushrooms, based on their appearance.\n\nThe considered dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide to North American Mushrooms (1981). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one.\n\nThe features of the dataset are the following:\n* classes: edible=e, poisonous=p\n* cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n* cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s\n* cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n* bruises: bruises=t,no=f\n* odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n* gill-attachment: attached=a,descending=d,free=f,notched=n\n* gill-spacing: close=c,crowded=w,distant=d\n* gill-size: broad=b,narrow=n\n* gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g,green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n* stalk-shape: enlarging=e,tapering=t\n* stalk-root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?\n* stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\n* stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\n* stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n* stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n* veil-type: partial=p,universal=u\n* veil-color: brown=n,orange=o,white=w,yellow=y\n* ring-number: none=n,one=o,two=t\n* ring-type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n* spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n* population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n* habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Importing libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading data \ndata = pd.read_csv('/kaggle/input/mushroom-classification/mushrooms.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data cleaning and preparation"},{"metadata":{},"cell_type":"markdown","source":"In this section, the data is analysed to evaluate whether there are missing values, and data types are adjusted."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Inspecting the first rows of the dataset\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking if there are missing values\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is possible to notice how there are no missing values in the dataset. However, the various features are described by strings. There is therefore a need to transform this categorical values into integers. This can be done by applying label encoding to the dataset. \n\nBefore doing that, it is suitable to check whether \"eadible\" and \"poisonous\" are the only available classes in the class feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking unique categories of class feature\nprint(data['class'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying encoder to transform string categorical data into integers\nencoder = LabelEncoder()\ndf = data.apply(encoder.fit_transform)\n\n#Printing first columns of new dataset\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"In this section, the distribution of the various categories in the available features is analysed. This allows to identify if there is any categorical imbalance in the dataset - this could affect the reliability of the proposed classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows =4, ncols = 6, figsize = (30, 20))\n\nsns.countplot(x=\"class\", data=df, ax = ax[0,0])\nax[0,0].set_xlabel('Class', fontsize = 14)\n\nsns.countplot(x=\"cap-shape\", data=df, ax = ax[0,1])\nax[0,1].set_xlabel('Cap shape', fontsize = 14)\n\nsns.countplot(x=\"cap-surface\", data=df, ax = ax[0,2])\nax[0,2].set_xlabel('Cap surface', fontsize = 14)\n\nsns.countplot(x=\"cap-color\", data=df, ax = ax[0,3])\nax[0,3].set_xlabel('Cap color', fontsize = 14)\n\nsns.countplot(x=\"bruises\", data=df, ax = ax[0,4])\nax[0,4].set_xlabel('Bruises', fontsize = 14)\n\nsns.countplot(x=\"odor\", data=df, ax = ax[0,5])\nax[0,5].set_xlabel('Odor', fontsize = 14)\n\nsns.countplot(x=\"gill-attachment\", data=df, ax = ax[1,0])\nax[1,0].set_xlabel('Gill attachment', fontsize = 14)\n\nsns.countplot(x=\"gill-spacing\", data=df, ax = ax[1,1])\nax[1,1].set_xlabel('Gill spacing', fontsize = 14)\n\nsns.countplot(x=\"gill-size\", data=df, ax = ax[1,2])\nax[1,2].set_xlabel('Gill size', fontsize = 14)\n\nsns.countplot(x=\"gill-color\", data=df, ax = ax[1,3])\nax[1,3].set_xlabel('Gill color', fontsize = 14)\n\nsns.countplot(x=\"stalk-shape\", data=df, ax = ax[1,4])\nax[1,4].set_xlabel('Stalk shape', fontsize = 14)\n\nsns.countplot(x=\"stalk-root\", data=df, ax = ax[1,5])\nax[1,5].set_xlabel('Stalk root', fontsize = 14)\n\nsns.countplot(x=\"stalk-surface-above-ring\", data=df, ax = ax[2,0])\nax[2,0].set_xlabel('Stalk surface above ring', fontsize = 14)\n\nsns.countplot(x=\"stalk-surface-below-ring\", data=df, ax = ax[2,1])\nax[2,1].set_xlabel('Stalk surface below ring', fontsize = 14)\n\nsns.countplot(x=\"stalk-color-above-ring\", data=df, ax = ax[2,2])\nax[2,2].set_xlabel('Stalk color above ring', fontsize = 14)\n\nsns.countplot(x=\"stalk-color-below-ring\", data=df, ax = ax[2,3])\nax[2,3].set_xlabel('Stalk color below ring', fontsize = 14)\n\nsns.countplot(x=\"veil-type\", data=df, ax = ax[2,4])\nax[2,4].set_xlabel('Veil type', fontsize = 14)\n\nsns.countplot(x=\"veil-color\", data=df, ax = ax[2,5])\nax[2,5].set_xlabel('Veil color', fontsize = 14)\n\nsns.countplot(x=\"ring-type\", data=df, ax = ax[3,0])\nax[3,0].set_xlabel('Ring type', fontsize = 14)\n\nsns.countplot(x=\"ring-number\", data=df, ax = ax[3,1])\nax[3,1].set_xlabel('Ring color', fontsize = 14)\n\nsns.countplot(x=\"spore-print-color\", data=df, ax = ax[3,2])\nax[3,2].set_xlabel('Stalk color above ring', fontsize = 14)\n\nsns.countplot(x=\"population\", data=df, ax = ax[3,3])\nax[3,3].set_xlabel('Population', fontsize = 14)\n\nsns.countplot(x=\"habitat\", data=df, ax = ax[3,4])\nax[3,4].set_xlabel('Habitat', fontsize = 14)\n\nfig.delaxes(ax[3,5])\n\nfig.suptitle('Number of elements in each category for the available features', fontsize = 40)\nfig.tight_layout()\nfig.subplots_adjust(top=0.88)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plot it is possible to deduce at least 3 pieces of information:\n1. The data is almost equally distributed between eadible and poisonous mushrooms, and therefore the regressor will not have to account for an univen distribution of the labels;\n2. Only the category 0 (\"partial\") appears for the column \"Veil type\". This column can be thus dropped from the dataset as it does not provide any useful information for the purpose of the model;\n3. In some columns there is a substantial imbalance of the labels (i.e. over 95 % of the elements in the column \"Veil color\" are described by the category 2: \"white\")."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping column veil type\ndf.drop(columns = 'veil-type', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification model using Random Forest"},{"metadata":{},"cell_type":"markdown","source":"A classification model is now built using Random Forest Classifier. This classifier is selected as this notebook aims a fulfilling a specific task for the selected dataset, which requires the use of this classifier.\n\nIn order to train the classifier, the dateset is divided into features (X) and labels (y = poisonous/edible) and then split into train and test sets. Splitting the data into train and test sets enables to check the accuracy of the ML model when predicting non-previously seen data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting dataset into labels and features\nX = df.drop(columns = 'class')\ny = df['class']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y,\n                                                   test_size = 0.3,\n                                                   random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given that a Random Forest classifier is relatively complex model, some parameters (hyper-parameters) have to be user-specified before the model can be trained. The proper selection of these parameter is of uttermost importance in order to attain a suitable model. \n\nThis selection process is generally called \"hyper-parameter tuning\" and it is here carried out by means of a random search approach: a list of potential values for the various hyper-parameters is defined and then multiple models are trained as a way to identify the best performing set of hyper-parameters.\n\nThe hyper-parameters here considered are the following:\n1. n_estimators: the number of trees in the forest\n2. max_depth: the maximum depth of the tree\n3. min_samples_leaf: the minimum number of samples required to be at a leaf node (fraction of the number of samples, when a float value is provided);\n4. max_features: the number of features to consider when looking for the best split (sqrt: max_features =  sqrt(n_features), log2: max_features = log2(n_features).\n\nThe scoring method for the random search is the \"accuracy\" of the regressor, which represent the percentage of labels that are correctly predicted. In case the results indicate a large number of false negatives (e.g. mushrooms predicted to be edible but are poisonous), the scoring method can be updated to limit the presence of false negatives."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initiating Random Forest Classifier\nrf_model = RandomForestClassifier(random_state = 42)\n\n#Define the distribution of hyperparameters\nparams_rf = {\n    'n_estimators': range(50,500,50),\n    'max_depth': range(1,10),\n    'min_samples_leaf': np.arange(0.0025,0.02,0.0025),\n    'max_features': ['log2', 'sqrt']   \n}\n\n#Initiate Randomized search\ngrid_rf = RandomizedSearchCV(estimator = rf_model, \n                             param_distributions = params_rf,\n                             cv=3,\n                             scoring='accuracy',\n                             n_iter=20,\n                             n_jobs= -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the grid search\ngrid_rf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extracting best hyperparameters\nrf_best_hyperparams = grid_rf.best_params_\nprint('Best hyperparameters for RF: \\n', rf_best_hyperparams)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extracting best rf model\nrf = grid_rf.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given that Random Forest is a complex model, the chances of overfitting the training set are considerable. When a model is overfitting the training set, it does fit not only its trends, but also its noise. Therefore, an overfitted model will perform poorly on unseen data.\n\nA way to check whether the model overfits the training set is to carry out a cross-validation procedure. During the cross-validation procedure, the training set is split into k folds, and the model is trained k times using a different portion of data from the training set. Each trained model will have a different prediction accuracy. The average accuracy of the models trained during the cross validation procedure is defined \"cross-validation\" accuracy.\n\nTwo scenarios are possible:\n\n1. The cross-validation accuracy is lower than the training set accuracy -> in this case the model is overfitting the training set;\n2. The cross-validation accuracy is similar to the training set accuracy -> it is possible to assume that the model is not overfitting the training set and will perform similarly on unseen data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking if there is overfitting through the use of Cross validation\nrf_accuracy_CV = cross_val_score(rf, X_train, y_train,\n                            cv = 10, \n                            scoring = 'accuracy',\n                            n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Computing the accuracy in the traning set, test set, and cross-validation procedure\nprint('CV accuracy:{:.4f}'.format(rf_accuracy_CV.mean()))\nprint('Train set accuracy:{:.4f}'.format(rf.score(X_train,y_train)))\nprint('Test set accuracy:{:.4f}'.format(rf.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results of the cross validation error indicate that the accuracy of in the cross validation procedure is similar to the one of the training set. Therefore, it is possible to assume that the selected model is not overfitting the training data.\n\nThis is further supported by the fact that the accuracy of the model in the test set is comparable to the previous two."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting the labels of the test set\ny_pred = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Printing the confusion matrix\nprint(confusion_matrix(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The confusion matrix indicates that there are no false positives, thus the risk of predicting a mushroom to be eadible while it is poisonous seems to be negligible.\n\nAnother tool to evaluate the performance of a classification model is the classificaiton report, whic is reported below."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing classification report\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a last step in the evaluation of the performace of the regression model, it is possible to plot its ROC curve."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_proba = rf.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr, label = \"Random Forest Classifier\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('False Negative Rate')\nplt.title('Random Forest Classifier ROC curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, it is possible to identify which features are the most relevant for the classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting feature importances for the classifier\n\n#Creating a pd.Series of feature importances\nimportances_rf = pd.Series(rf.feature_importances_, index = X.columns)\n\n#Sorting importances\nsorted_importances_rf = importances_rf.sort_values()\n\n#Plotting sorted importances\nfig, ax = plt.subplots(figsize = (10,7))\nsorted_importances_rf.plot(kind = 'barh')\nax.set_title('Random Forest Regressor Feature importances')\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification with a reduced number of features"},{"metadata":{},"cell_type":"markdown","source":"The random forest classifier derived using all the possible features resulted in a very high prediction accuracy. In this second step another classification model is built using only the 3 most important features identified at the previous step.\n\nThe aim is to undertand how accurate can a classifier be, when using only \"odor\", \"gill-color\", and \"gill-size\" as input features.\n\nThe same procedure described in the previous section is followed here."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting dataset into labels and features\nX_r = df[['odor','gill-color','gill-size']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting data into train and test sets\nX_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_r,y,\n                                                   test_size = 0.3,\n                                                   random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define the distribution of hyperparameters\nparams_rf = {\n    'n_estimators': range(50,500,50),\n    'max_depth': range(1,10),\n    'min_samples_leaf': np.arange(0.0025,0.02,0.0025),\n    'max_features': ['log2', 'sqrt']   \n}\n\n#Initiate Randomized search\ngrid_rf_r = RandomizedSearchCV(estimator = rf_model, \n                             param_distributions = params_rf,\n                             cv=3,\n                             scoring='accuracy',\n                             n_iter=20,\n                             n_jobs= -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the grid search\ngrid_rf_r.fit(X_train_r, y_train_r)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extracting best hyperparameters\nrf_best_hyperparams_r = grid_rf.best_params_\nprint('Best hyperparameters for RF: \\n', rf_best_hyperparams_r)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extracting best rf model\nrf_r = grid_rf_r.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking if there is overfitting through the use of Cross validation\nrf_accuracy_CV_r = cross_val_score(rf_r, X_train_r, y_train_r,\n                            cv = 10, \n                            scoring = 'accuracy',\n                            n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Computing the accuracy in the traning set, test set, and cross-validation procedure\nprint('CV accuracy:{:.4f}'.format(rf_accuracy_CV_r.mean()))\nprint('Train set accuracy:{:.4f}'.format(rf_r.score(X_train_r,y_train_r)))\nprint('Test set accuracy:{:.4f}'.format(rf_r.score(X_test_r, y_test_r)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting lables of the test set\ny_pred_r = rf_r.predict(X_test_r)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,y_pred_r))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting ROC curve\ny_pred_proba_r = rf_r.predict_proba(X_test_r)[:,1]\nfpr_r, tpr_r, thresholds_r = roc_curve(y_test, y_pred_proba_r)\n\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr_r, tpr_r, label = \"Accuracy optimized\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('False Negative Rate')\nplt.title('Random Forest Classifier ROC curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusions"},{"metadata":{},"cell_type":"markdown","source":"A dataset describing 23 features of edible/poisonous mushrooms was analysed. The analysis indicated that only one \"veil-type\" was present among the considered data and thus this feature was dropped from the dataset. In addition, the data turned out to be almost equally distributed between eadible and poisonous mushrooms, hence simplyfing the training of the classification model.\n\nThe classification model was based on the use of a Random Forest Classifier, whose hyper-parameters where tuned by mean of a random search approach. The accuracy of the classifier was then validated by means of a cross-validation procedure.\n\nThe main results emerging from the investigation of the classification model are the following:\n* The use of a Random Forest classifier is identified as a suitable choice for the considered problem. The estimated classification accuracy (using all the available features) in the test set was found to be equal to 0.992;\n* A second classifier using as features only the 3 top features identified by the first model was developed. In this case the classification accuracy in the test set was found to be equal to 0.981;\n* In both cases no false negatives were found in the predictions of the test set. False negatives would be the worst possible outcome for the considered problem (poisonous mushrooms predited to be edible). The scoring function used to train the regressor could be adjusted in case the number of predicted false negatives was considerable."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}