{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.float_format = '{:.1f}'.format\npd.set_option('display.max_rows', 1000)\npd.get_option(\"display.max_columns\", 1000)\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, r2_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/housesalesprediction/kc_house_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# before tuning\ndef basic_details(df):\n    b = pd.DataFrame()\n    b['Missing value'] = df.isnull().sum()\n    b['N unique value'] = df.nunique()\n    b['dtype'] = df.dtypes\n    return b\nbasic_details(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"bathrooms\"] = df['bathrooms'].round(0).astype(int)\ndf[\"floors\"] = df['floors'].round(0).astype(int)\n\ndf['Is_Basement'] = [1 if i != 0 else 0 for i in df['sqft_basement']]\ndf['Is_Bathrooms'] = [1 if i != 0 else 0 for i in df['bathrooms']]\ndf['Is_Bedrooms'] = [1 if i != 0 else 0 for i in df['bedrooms']]\ndf['Was_Renovated'] = [1 if i != 0 else 0 for i in df['yr_renovated']]\ndf['More_than_1_floor'] = [1 if i > 1 else 0 for i in df['floors']]\ndf['grade'] = [1 if i >= 11 else 3 if i<=3 else 2 for i in df['grade']]\n\n\nfor i in range(0, len(df)):\n    if df['yr_renovated'][i]!=0:\n        df['yr_built'][i]=df['yr_renovated'][i]\n        \n# just take the year from the date column\ndf['sales_yr']=df['date'].astype(str).str[:4]\ndf['sales_yr']=df['sales_yr'].astype(int)\n\ndf['age'] = df['sales_yr'] - df['yr_built'] \ndf['sqft_living_per_lot'] = df['sqft_living']/df['sqft_lot']\ndf['sqft_living_per_floor'] = df['sqft_living']/df['floors']\n\ndf['sqft_living'] = np.log(df.sqft_living + 0.01)\ndf['sqft_lot'] = np.log(df.sqft_lot + 0.01)\ndf['sqft_living15'] = np.log(df.sqft_living15 + 0.01)\ndf['sqft_living_per_floor'] = np.log(df.sqft_living_per_floor + 0.01)\n# df['price'] = np.log(df.price + 0.01)\n\n\n        \ndf = df.drop(['id', 'long', 'lat', 'yr_renovated', 'date','yr_built', 'sqft_basement', 'sqft_above', 'sales_yr', 'sqft_lot15'], axis=1)\n\ncategorial_cols = ['floors', 'view', 'condition', 'grade']\n\nfor cc in categorial_cols:\n    dummies = pd.get_dummies(df[cc], drop_first=False)\n    dummies = dummies.add_prefix(\"{}#\".format(cc))\n    df.drop(cc, axis=1, inplace=True)\n    df = df.join(dummies)\n    \ndummies_zipcodes = pd.get_dummies(df['zipcode'], drop_first=False)\ndummies_zipcodes.reset_index(inplace=True)\ndummies_zipcodes = dummies_zipcodes.add_prefix(\"{}#\".format('zipcode'))\n# dummies_zipcodes = dummies_zipcodes[['zipcode#98004','zipcode#98102','zipcode#98109','zipcode#98112','zipcode#98039','zipcode#98040']]\ndf.drop('zipcode', axis=1, inplace=True)\ndf = df.join(dummies_zipcodes)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns \nsns.distplot(np.log1p(df['price']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# after tuning\ndef basic_details(df):\n    b = pd.DataFrame()\n    b['Missing value'] = df.isnull().sum()\n    b['N unique value'] = df.nunique()\n    b['dtype'] = df.dtypes\n    return b\nbasic_details(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.age.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def spearman(frame, features):\n    spr = pd.DataFrame()\n    spr['feature'] = features\n    spr['spearman'] = [frame[f].corr(frame['price'], 'spearman') for f in features]\n    spr = spr.sort_values('spearman')\n    plt.figure(figsize=(6, 0.25*len(features)))\n    f, ax = plt.subplots(figsize=(12, 9))\n    sns.barplot(data=spr, y='feature', x='spearman', orient='h')\n    \nfeatures = df.drop(['price'], axis = 1).columns\nspearman(df, features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# corrMatrix = df.corr()\n# f, ax = plt.subplots(figsize=(20, 10))\n# sns.heatmap(corrMatrix, annot = True)\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nnum_col = df.columns\n# Outlier detection \n\ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR \n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(df,2, num_col)\ndf.loc[Outliers_to_drop] # Show the outliers rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop outliers\ndf = df.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)\nprint('Outliers dropped')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['price'], axis=1)\ny = df.price","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=29)\n\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\n\n\n\n\nparams = {\n        'objective':'reg:linear',\n        'n_estimators': 10,\n        'booster':'gbtree',\n        'max_depth':2,\n        'eval_metric':'rmse',\n        'learning_rate':0.1, \n        'min_child_weight':3,\n        'subsample':0.85,\n        'colsample_bytree':0.5,\n        'seed':45,\n        'reg_alpha':1,#1e-03,\n        'reg_lambda':1,\n        'gamma':0,\n        'nthread':-1\n\n}\n\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\nclf = xgb.train(params, d_train, 5000, watchlist, early_stopping_rounds=400, maximize=False, verbose_eval=10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_test = clf.predict(d_valid)\nr2_score(y_valid, d_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dmatrix_data = xgb.DMatrix(data=X, label=y)\n\ncv_params = {\n        'objective':'reg:linear',\n        'n_estimators': 10,\n        'booster':'gbtree',\n        'max_depth':2,\n        'eval_metric':'rmse',\n        'learning_rate':0.1, \n        'min_child_weight':3,\n        'subsample':0.85,\n        'colsample_bytree':0.5,\n        'seed':45,\n        'reg_alpha':1,#1e-03,\n        'reg_lambda':1,\n        'gamma':0,\n        'nthread':-1\n}\ncross_val = xgb.cv(\n    params=cv_params,\n    dtrain=dmatrix_data, \n    nfold=5,\n    num_boost_round=5000, \n    early_stopping_rounds=1000, \n    metrics='rmse', \n    as_pandas=True, \n    seed=29)\nprint(cross_val.tail(1))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}