{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import model_selection\nfrom sklearn.model_selection import StratifiedKFold, ShuffleSplit\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nSEED=2020\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler\nfrom scipy import stats\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFECV\nimport eli5 \nfrom eli5.sklearn import PermutationImportance\nimport shap #for SHAP values\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectFromModel, SelectKBest, RFE, chi2\n# from xgboost.sklearn import XGBClassifier \nimport xgboost as xgb\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import QuantileTransformer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot\nfrom xgboost import plot_tree","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load dataset and drop duplicates\ndf = pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')\ndf = df.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# take a look for dataset\ndf.sample(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style = \"white\")\ng = sns.PairGrid(df.loc[:,[\"age\",\"trestbps\",\"chol\", 'thalach', 'oldpeak']],diag_sharey = False,)\ng.map_lower(sns.kdeplot,cmap=\"Blues_d\")\ng.map_upper(plt.scatter)\ng.map_diag(sns.kdeplot,lw =3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 12))\n\n# iterate \nfor i, source in enumerate([\"age\",\"trestbps\",\"chol\", 'thalach', 'oldpeak']):\n    \n    # create a new subplot for each source\n    plt.subplot(3, 2, i + 1)\n    # plot repaid loans\n    sns.kdeplot(df.loc[df['target'] == 0, source], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(df.loc[df['target'] == 1, source], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"target\", data=df)\ndf.loc[:,'target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## basic algo accuracy:\nprint('Basic algo accuracy')\nround(df['target'].value_counts(normalize=True).max(), 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## correlation table\npd.DataFrame(abs(df.corr()['target']).sort_values(ascending = False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop outliers with z-score over 99.9%\nz = np.abs(stats.zscore(df._get_numeric_data()))\ndf= df[(z < 3).all(axis=1)]\nprint('Outliers dropped')\n\n# df = df[(df['chol'] <= 326.9) & (df['oldpeak'] <=3.4)].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some new features based on continious features\ndf['ageCat'] = pd.cut(df['age'].astype(int), 8)\ndf['trestbpsCat'] = pd.cut(df['trestbps'], 8)\ndf['cholCat'] = pd.cut(df['chol'], 8)\ndf['thalachCat'] = pd.cut(df['thalach'], 8)\ndf['oldpeakCat'] = pd.cut(df['oldpeak'], 8)\n\nlabel = LabelEncoder()\ndf['ageCat'] = label.fit_transform(df['ageCat'])\ndf['trestbpsCat'] = label.fit_transform(df['trestbpsCat'])\ndf['cholCat'] = label.fit_transform(df['cholCat'])\ndf['thalachCat'] = label.fit_transform(df['thalachCat'])\ndf['oldpeakCat'] = label.fit_transform(df['oldpeakCat'])\n\ndf['trestbpsLog'] = np.log1p(df['trestbps'])\ndf['cholLog'] = np.log1p(df['chol'])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## more detailed info as for dataset\ndf.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in df.drop(['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'trestbpsLog', 'cholLog', 'target'], axis=1):\n    print('Target Correlation by:', x)\n    print(df[[x, 'target']].groupby(x, as_index=False).mean())\n    print('-'*10, '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## distribution and re-checking for outliers in numeric features\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfeatures = df.drop('target', axis=1).columns\nfor i in features:\n    sns.boxplot(x=\"target\",y=i,data=df)\n    plt.title(i+\" by \"+\"target\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## feature creation script, thanks to @ Vitalii Mokin\n\ndef feature_creation(df):\n    for i in ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'ageCat',\n       'trestbpsCat', 'cholCat', 'thalachCat', 'oldpeakCat', 'trestbpsLog',\n       'cholLog']:\n        for j in ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n       'exang', 'oldpeak', 'slope', 'ca', 'thal','ageCat',\n       'trestbpsCat', 'cholCat', 'thalachCat', 'oldpeakCat', 'trestbpsLog',\n       'cholLog']:\n            df[i + \"*\" + j] = (df[i]*df[j]) # based on multiplication\n            df[i + \"_\" + j] = df[i].astype('str') + \"_\" + df[j].astype('str') ## and concatenation\n\n    return df\n\ndf = feature_creation(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # checking for Object columns & Encoding new categorical features\n\nlencoders = {}\nfor col in df.select_dtypes(include=['object']).columns:\n    lencoders[col] = LabelEncoder()\n    df[col] = lencoders[col].fit_transform(df[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## let's reduce memory for clear mind\n\ndef reduce_memory_usage(df):\n    \"\"\" The function will reduce memory of dataframe\n    Note: Apply this function after removing missing value\"\"\"\n    intial_memory = df.memory_usage().sum()/1024**2\n    print('Intial memory usage:',intial_memory,'MB')\n    for col in df.columns:\n        mn = df[col].min()\n        mx = df[col].max()\n        if df[col].dtype != object:            \n            if df[col].dtype == int:\n                if mn >=0:\n                    if mx < np.iinfo(np.uint8).max:\n                        df[col] = df[col].astype(np.uint8)\n                    elif mx < np.iinfo(np.uint16).max:\n                        df[col] = df[col].astype(np.uint16)\n                    elif mx < np.iinfo(np.uint32).max:\n                        df[col] = df[col].astype(np.uint32)\n                    elif mx < np.iinfo(np.uint64).max:\n                        df[col] = df[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)\n            if df[col].dtype == float:\n                df[col] =df[col].astype(np.float32)\n    \n    red_memory = df.memory_usage().sum()/1024**2\n    print('Memory usage after complition: ',red_memory,'MB')\n    \nreduce_memory_usage(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Threshold for removing correlated variables, thanks to @ Vitalii Mokin\nthreshold = 0.9  ## optimal level 0.9\n# Absolute value correlation matrix\ncorr_matrix = df.corr().abs().round(2)\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n# Select columns with Pearson's correlations above threshold\ncollinear_features = [column for column in upper.columns if any(upper[column] > threshold)]\nfeatures_filtered = df.drop(columns = collinear_features)\nprint('The number of features that passed the collinearity threshold: ', features_filtered.shape[1])\nfeatures_best = []\nfeatures_best.append(features_filtered.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def corr_df(x, corr_val):\n#     '''\n#     Obj: Drops features that are strongly correlated to other features.\n#           This lowers model complexity, and aids in generalizing the model.\n#     Inputs:\n#           df: features df (x)\n#           corr_val: Columns are dropped relative to the corr_val input (e.g. 0.8)\n#     Output: df that only includes uncorrelated features\n#     '''\n\n#     # Creates Correlation Matrix and Instantiates\n#     corr_matrix = x.corr()\n#     iters = range(len(corr_matrix.columns) - 1)\n#     drop_cols = []\n\n#     # Iterates through Correlation Matrix Table to find correlated columns\n#     for i in iters:\n#         for j in range(i):\n#             item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n#             col = item.columns\n#             row = item.index\n#             val = item.values\n#             if abs(val) >= corr_val:\n#                 # Prints the correlated feature set and the corr val\n#                 print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n#                 drop_cols.append(i)\n\n#     drops = sorted(set(drop_cols))[::-1]\n\n#     # Drops the correlated columns\n#     for i in drops:\n#         col = x.iloc[:, (i+1):(i+2)].columns.values\n#         x = x.drop(col, axis=1)\n#     return x\n# df = corr_df(df, 0.92)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# updated dataframe with filtred features\ndf=df[features_best[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# take a look for dataframe\ndef basic_details(df):\n    b = pd.DataFrame()\n    b['Missing value, %'] = round(df.isnull().sum()/df.shape[0]*100)\n    b['N unique value'] = df.nunique()\n    b['dtype'] = df.dtypes\n    return b\nbasic_details(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# more detailed\ndf.describe(include='all').T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some visualisations of features\ndf_hist = df.select_dtypes(exclude = ['bool','object'])\ndf_hist.hist(figsize = [15,15],bins = 50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define cross-val strategy\ncv_split = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=SEED)\n#split dataset\nX = df.drop('target', axis=1)\ny = df.target\n# Add zeros per row as extra feature\nX['n0'] = (X == 0).sum(axis=1)\n# add more features\n# poly = PolynomialFeatures(degree=3)\n# X=poly.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n\n\n## scale features\n# scaler = preprocessing.RobustScaler()\n# scaler = preprocessing.StandardScaler() \nscaler = preprocessing.MinMaxScaler()\n# scaler = preprocessing.Normalizer()\n# scaler = preprocessing.QuantileTransformer()\n\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\nX_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)\nX = pd.DataFrame(scaler.transform(X), columns = X.columns)\n\n# alternative scale strategy\n# X_train = (X_train - np.min(X_train)) / (np.max(X_train) - np.min(X_train))\n# X_test = (X_test - np.min(X_test)) / (np.max(X_test) - np.min(X_test))\n# X=(X - np.min(X)) / (np.max(X) - np.min(X))\n\n# standartization strategy as alternative?\n# X = (X - X.mean()) / (X.std())\n# X_train = (X_train - X_train.mean()) / (X_train.std())\n# X_test = (X_test - X_test.mean()) / (X_test.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(max_depth=3, n_estimators=50, random_state=SEED)\n# rf.fit(X_train, y_train)\n# acc_rf_train = round(rf.score(X_train, y_train)*100,2) \n# acc_rf_test = round(rf.score(X_test,y_test)*100,2)\nscores = cross_val_score(rf, X, y, cv=cv_split, scoring = 'accuracy')\nprint(\"Random Forest based on full dataset\")\n# print(\"Training Accuracy: % {}\".format(acc_rf_train))\n# print(\"Testing Accuracy: % {}\".format(acc_rf_test))\nprint(\"RF CV Accuracy Score Mean:\", scores.mean())\n# print(\"Standard Deviation:\", scores.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=LogisticRegression(C=0.3, random_state=SEED, max_iter=100000)\n# lr.fit(X_train,y_train)\nscores = cross_val_score(lr, X, y, cv=cv_split, scoring = 'accuracy')\n# acc_log_train = round(lr.score(X_train, y_train)*100,2) \n# acc_log_test = round(lr.score(X_test,y_test)*100,2)\nprint(\"Logistic Regression based on full dataset\")\n# print(\"Training Accuracy: % {}\".format(acc_log_train))\n# print(\"Testing Accuracy: % {}\".format(acc_log_test))\nprint(\"LR CV Accuracy Score Mean:\", scores.mean())\n# print(\"Standard Deviation:\", scores.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Feature ranking with recursive feature elimination and cross-validated selection of the best number of features\n# classifier = RandomForestClassifier(random_state=SEED)\n# classifier = ExtraTreesClassifier(random_state=SEED)\nclassifier = LogisticRegression(random_state=SEED)\n# classifier = DecisionTreeClassifier()\nselector = RFECV(classifier, step = 1, cv=cv_split, n_jobs=-1,verbose=1,  scoring='accuracy')\nselector.fit(X, y)\nprint('The optimal number of features is {}'.format(selector.n_features_))\nfeatures_rfecv = [f for f,s in zip(X, selector.support_) if s]\nprint('The selected features are:')\nprint ('{}'.format(features_rfecv)) ## optimal features list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (roc auc)\")\nplt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)\nplt.savefig('feature_auc_nselected.png', bbox_inches='tight', pad_inches=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## let's compare RF after feature selection\nrf = RandomForestClassifier(max_depth=2, n_estimators=25, random_state=SEED)\nX_rfe = X[features_rfecv]\nrf.fit(X_rfe, y)\n# acc_log_train = round(rf.score(X_train_rfe, y_train)*100,2) \n# acc_log_test = round(rf.score(X_test[features_rfecv],y_test)*100,2)\nscores = cross_val_score(rf, X_rfe, y, cv=cv_split, scoring = 'accuracy')\n# print(\"Training Accuracy: % {}\".format(acc_log_train))\n# print(\"Testing Accuracy: % {}\".format(acc_log_test))\nprint(\"RF based on selected dataset\")\nprint(\"FR CV Accuracy Score after selection:\", scores.mean().round(3))\n# print(\"Standard Deviation:\", scores.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=LogisticRegression(C=0.9, random_state=SEED, max_iter=100000)\n# lr.fit(X_train,y_train)\nscores = cross_val_score(lr, X_rfe, y, cv=cv_split, scoring = 'accuracy')\n# acc_log_train = round(lr.score(X_train, y_train)*100,2) \n# acc_log_test = round(lr.score(X_test,y_test)*100,2)\nprint(\"LR based on selected dataset\")\n# print(\"Training Accuracy: % {}\".format(acc_log_train))\n# print(\"Testing Accuracy: % {}\".format(acc_log_test))\nprint(\"LR CV Accuracy Score Mean:\", scores.mean())\n# print(\"Standard Deviation:\", scores.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(X_rfe, y)\nperm = PermutationImportance(lr, random_state=SEED).fit(X_rfe, y)\neli5.show_weights(perm, feature_names = X_rfe.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explainer = shap.TreeExplainer(rf)\nshap_values = explainer.shap_values(X_rfe)\nshap.summary_plot(shap_values[1], X_rfe, plot_type=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values[1], X_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bestfeatures = SelectKBest(score_func=chi2, k='all')\nfit = bestfeatures.fit(X_train, y_train)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Feature','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(15,'Score')) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_kbest=featureScores.nlargest(15,'Score')['Feature'].tolist()\nfeatures_kbest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=LogisticRegression(C=3, random_state=SEED)\nscores = cross_val_score(lr, X[features_kbest], y, cv=cv_split, scoring = 'accuracy')\nprint(\"Kbest CV Acc Score Mean:\", scores.mean().round(3))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}