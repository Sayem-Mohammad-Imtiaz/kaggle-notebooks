{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Load in our libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Em progesso..","metadata":{}},{"cell_type":"code","source":"# settings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = pd.read_csv('../input/credit-card-approval-prediction/application_record.csv')\ndf2 = pd.read_csv('../input/credit-card-approval-prediction/credit_record.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.head(5)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### limpando os dados","metadata":{}},{"cell_type":"code","source":"# Verificando a presença de valores nulos no data frame\ndf1.isnull().sum().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verificando a presença de valores nulos no data frame\ndf2.isnull().sum().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verificando a presença de NA's no data frame\ndf1.isnull().values.any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#REmovendo os Null/NA\ndf1 = df1.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#verificando se sobrou algum Null/NA\ndf1.isnull().values.any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Verificando o numero de linhas restantes","metadata":{}},{"cell_type":"code","source":"index = df1.index\nnumber_of_rows = len(index)\nprint(number_of_rows)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = df2.index\nnumber_of_rows = len(index)\nprint(number_of_rows)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Juntando as duas bases de dados","metadata":{}},{"cell_type":"code","source":"df3 = pd.merge(df1, df2, on='ID')\ndf3.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Verificando a quantidade de dados depois da junção dos conjuntos\nindex = df3.index\nnumber_of_rows = len(index)\nprint(number_of_rows)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* É possível notar que a base de dados aumentou, portanto houve duplicatas de valores. Precisamos remove-lás.","metadata":{}},{"cell_type":"code","source":"#verificando se sobrou algum Null/NA\ndf3.isnull().values.any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tirando os ID duplicados\ndf3 = df3.drop_duplicates('ID',keep='first')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Verificando a quantidade de dados depois da junção dos conjuntos\ndf3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transformando em dummies","metadata":{}},{"cell_type":"code","source":"#Removendo a variável CODE_GENDER para não ter viés sexista na base de dados\ndf3 = df3.drop(columns=['CODE_GENDER'])\ndf3.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Trasnformando todos de valores Y ou N em dummies, sendo 1 para Y\ndummy1 = pd.get_dummies(df3.FLAG_OWN_CAR)\ndf3['FLAG_OWN_CAR'] = dummy1['Y']\n\ndummy2 = pd.get_dummies(df3.FLAG_OWN_REALTY)\ndf3['FLAG_OWN_REALTY'] = dummy2['Y']\n\n\n#Vendo as classes das variáveis categóricas\n#print(df3['NAME_INCOME_TYPE'].unique())\n\n#Vendo as classes das variáveis categóricas\n#print(df3['NAME_EDUCATION_TYPE'].unique())\n\n#Vendo as classes das variáveis categóricas\n#print(df3['OCCUPATION_TYPE'].unique())\n\n#Vendo as classes das variáveis categóricas\n#print(df3['STATUS'].unique())\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###criando uma variável ordinal para o nível de escolaridade\n#df3['NAME_EDUCATION_TYPE'] =\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Vamos tentar entender a capacidade de pagamento dos individuos e enquadrá-lo em categorias","metadata":{}},{"cell_type":"code","source":"#### Vamos ver como são os níveis de consumo por categorias sociais\n\n\nfig, axes = plt.subplots(1, 4, figsize=(15, 5), sharey=True)\nfig.suptitle('Consumo por característica')\n\n# Bulbasaur\nsns.barplot(ax=axes[0], x=df3.NAME_INCOME_TYPE, y=df3.AMT_INCOME_TOTAL).tick_params(labelrotation=45)\naxes[0].set_title(\"Ocupação\")\n\n\n# Charmander\nsns.barplot(ax=axes[1], x=df3.NAME_EDUCATION_TYPE, y=df3.AMT_INCOME_TOTAL).tick_params(labelrotation=45)\naxes[1].set_title(\"Escolaridade\")\n\n# Squirtle\nsns.barplot(ax=axes[2], x=df3.NAME_FAMILY_STATUS, y=df3.AMT_INCOME_TOTAL).tick_params(labelrotation=45)\naxes[2].set_title(\"Estatus Civil\")\n\n#\nsns.barplot(ax=axes[3], x=df3.NAME_HOUSING_TYPE, y=df3.AMT_INCOME_TOTAL).tick_params(labelrotation=45)\naxes[3].set_title(\"Moradia\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n###criando UM catplot individual para a variavel OCCUPATION_TYPE em relação ao poder de compra AMT_INCOME_TOTAL\nplt.figure(figsize =(10,5))\nax = sns.barplot(x=\"OCCUPATION_TYPE\", y=\"AMT_INCOME_TOTAL\",data=df3).set_title('Consumo por profissão')\nplt.xticks(rotation=60)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Como há várias categorias, dividiremos todas elas pelo poder de consumo","metadata":{}},{"cell_type":"code","source":"#média de consumo por profissão\ndf4 = df3.groupby(['OCCUPATION_TYPE']).mean().sort_values(['AMT_INCOME_TOTAL'], ascending=False)\ndf4['AMT_INCOME_TOTAL']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Como temos 18 profissões, vamos criar um indice de impacto de 6 níveis, de acordo com o poder de consumo\ndf3['OCCUPATION_TYPE'] = df3['OCCUPATION_TYPE'].replace(['Managers','Realty agents'],6)\ndf3['OCCUPATION_TYPE'] = df3['OCCUPATION_TYPE'].replace(['Drivers','Accountants','IT staff','Private service staff'],5)\ndf3['OCCUPATION_TYPE'] = df3['OCCUPATION_TYPE'].replace(['High skill tech staff','HR staff','Core staff','Laborers'],4)\ndf3['OCCUPATION_TYPE'] = df3['OCCUPATION_TYPE'].replace(['Security staff','Sales staff','Secretaries','Medicine staff'],3)\ndf3['OCCUPATION_TYPE'] = df3['OCCUPATION_TYPE'].replace(['Drivers','Accountants','IT staff','Private service staff'],2)\ndf3['OCCUPATION_TYPE'] = df3['OCCUPATION_TYPE'].replace(['Waiters/barmen staff','Cleaning staff','Cooking staff','Low-skill Laborers'],1)\ndf3['OCCUPATION_TYPE'] = df3['OCCUPATION_TYPE'].apply(pd.to_numeric)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fazendo o mesmo para educação\ndf5 = df3.groupby(['NAME_EDUCATION_TYPE']).mean().sort_values(['AMT_INCOME_TOTAL'], ascending=False)\ndf5['AMT_INCOME_TOTAL']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3['NAME_EDUCATION_TYPE'] = df3['NAME_EDUCATION_TYPE'].replace(['Academic degree'],5)\ndf3['NAME_EDUCATION_TYPE'] = df3['NAME_EDUCATION_TYPE'].replace(['Higher education'],4)\ndf3['NAME_EDUCATION_TYPE'] = df3['NAME_EDUCATION_TYPE'].replace(['Incomplete higher'],3)\ndf3['NAME_EDUCATION_TYPE'] = df3['NAME_EDUCATION_TYPE'].replace(['Secondary / secondary special'],2)\ndf3['NAME_EDUCATION_TYPE'] = df3['NAME_EDUCATION_TYPE'].replace(['Lower secondary'],1)\ndf3['NAME_EDUCATION_TYPE'] = df3['NAME_EDUCATION_TYPE'].apply(pd.to_numeric)\nprint(df3['NAME_EDUCATION_TYPE'].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fazendo o mesmo para finalidade de uso do crédito\ndf6 = df3.groupby(['NAME_INCOME_TYPE']).mean().sort_values(['AMT_INCOME_TOTAL'], ascending=False)\ndf6['AMT_INCOME_TOTAL']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3['NAME_INCOME_TYPE'] = df3['NAME_INCOME_TYPE'].replace(['Pensioner'],5)\ndf3['NAME_INCOME_TYPE'] = df3['NAME_INCOME_TYPE'].replace(['Commercial associate'],4)\ndf3['NAME_INCOME_TYPE'] = df3['NAME_INCOME_TYPE'].replace(['State servant'],3)\ndf3['NAME_INCOME_TYPE'] = df3['NAME_INCOME_TYPE'].replace(['Working'],2)\ndf3['NAME_INCOME_TYPE'] = df3['NAME_INCOME_TYPE'].replace(['Student'],1)\ndf3['NAME_INCOME_TYPE'] = df3['NAME_INCOME_TYPE'].apply(pd.to_numeric)\nprint(df3['NAME_INCOME_TYPE'].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fazendo o mesmo para finalidade de uso do crédito\ndf7 = df3.groupby(['NAME_HOUSING_TYPE']).mean().sort_values(['AMT_INCOME_TOTAL'], ascending=False)\ndf7['AMT_INCOME_TOTAL']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3['NAME_HOUSING_TYPE'] = df3['NAME_HOUSING_TYPE'].replace(['Office apartment'],6)\ndf3['NAME_HOUSING_TYPE'] = df3['NAME_HOUSING_TYPE'].replace(['Co-op apartment'],5)\ndf3['NAME_HOUSING_TYPE'] = df3['NAME_HOUSING_TYPE'].replace(['Rented apartment'],4)\ndf3['NAME_HOUSING_TYPE'] = df3['NAME_HOUSING_TYPE'].replace(['House / apartment'],3)\ndf3['NAME_HOUSING_TYPE'] = df3['NAME_HOUSING_TYPE'].replace(['Municipal apartment'],2)\ndf3['NAME_HOUSING_TYPE'] = df3['NAME_HOUSING_TYPE'].replace(['With parents'],1)\ndf3['NAME_HOUSING_TYPE'] = df3['NAME_HOUSING_TYPE'].apply(pd.to_numeric)\nprint(df3['NAME_HOUSING_TYPE'].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Como foi visto no gráfico inicial, a variável estado civil não varia muito de consumo de acordo como status,\n# portanto vamos remove-lá junto as demais desnecessárias\n\ndf3 = df3.drop(columns=['NAME_FAMILY_STATUS'])\ndf3 = df3.drop(columns=['ID'])\n#Vamos tira a variável FLAG_MOBIL , CNT_CHILDREN e FLAG_WORK_PHONE pois também não traz informação relevante\ndf3 = df3.drop(columns=['FLAG_MOBIL'])\ndf3 = df3.drop(columns=['FLAG_WORK_PHONE'])\ndf3 = df3.drop(columns=['CNT_CHILDREN'])\ndf3 = df3.drop(columns=['FLAG_PHONE'])\ndf3 = df3.drop(columns=['FLAG_EMAIL'])\ndf3.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"#### Agora vamos analisar a variavel target","metadata":{}},{"cell_type":"markdown","source":"##### Temos duas categorias de indivíduos: com atrasos de pagamentos, e sem atrasos. Portanto vamos categorizá-los como **inadimplentes e adimplentes**. A decisão de quem é adimplente ou inadimplente é relativo e depende dos interesses internos das instituições, mas para simplificação do modelo fazeremos dessa forma.","metadata":{}},{"cell_type":"markdown","source":"Você deve estar se perguntando se não seria interessante estimar uma regressão linear antes de transformar a variável target em dummy. A questão é que as variáveis explicativas precisam ter distribuição normal para obter os melhores estimadores de MQO, o que não acontece no nosso conjunto de dados. Portando levaremos a nossa análise a modelos não linear.","metadata":{}},{"cell_type":"code","source":"#letras são adimplentes e números inadimplentes\ndf3['STATUS'] = df3['STATUS'].replace(['C'],0)\ndf3['STATUS'] = df3['STATUS'].replace(['X'],0)\ndf3['STATUS'] = df3['STATUS'].apply(pd.to_numeric) \ndf3['STATUS'] = np.where(df3['STATUS']<1, 0, 1)\nprint(df3['STATUS'].unique())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Inadimplente = df3.loc[df3['STATUS'] == 1].count()[0]\nAdimplente = df3.loc[df3['STATUS'] == 0].count()[0]\n\nlabels = ['days past due', 'paid off/No loan']\ncolors = ['#d10000', '#6297e3']\nexplode = (.1,.1)\n\n\nplt.pie([Inadimplente, Adimplente], labels = labels, colors = colors, \n        autopct = '%.2f %%', pctdistance= 0.2, startangle=170, explode = explode)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Podemos notar que nosso conjunto de dados está **muito desbalanceado** e a proporção de classes é de 24853 Adimplentes para 281 Inadimplentes. E para isso vamos usar o método de resampling para balancear a base de dados.","metadata":{}},{"cell_type":"code","source":"import imblearn\nfrom imblearn.under_sampling import RandomUnderSampler\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x,y = df3.loc[:,df3.columns != 'STATUS'], df3.loc[:,'STATUS']\n\n# Definindo a proporção de dados da classe onde há menos observações\nsampling_strategy= 0.34\nrus = RandomUnderSampler(sampling_strategy=sampling_strategy)\nX_res, y_res = rus.fit_resample(x, y)\nautopct = \"%.2f\"\nax = y_res.value_counts().plot.pie(autopct=autopct)\n_ = ax.set_title(\"Under-sampling\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Chamar a variavel STATUS de risco ajuda a entener melhor\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Correlation of Features', y=1.05, size=15)\nsns.heatmap(df3.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As variáveis tem poucas correlação entre si, o que pode ser um sinal bom, diminuindo as chances de inflar o modelo.","metadata":{}},{"cell_type":"markdown","source":"## Aplicando a técnica de ensemble stacking","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import StackingClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting the data\n\nx_train,x_test,y_train,y_test = train_test_split(X_res,y_res,test_size = 0.20,random_state = 1)\n\nx_train_0,x_train_1,y_train_0,y_train_1 = train_test_split(X_res,y_res,test_size = 0.60, random_state = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X_res, y_res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#criando uma lista com os modelos\n# Vendo qual tem a melhor acurácia para usa-lo no stacking\n\nmodels = {}\nmodels['knn'] = KNeighborsClassifier()\nmodels['cart'] = DecisionTreeClassifier()\nmodels['svm'] = SVC()\nmodels['bayes'] = GaussianNB()\nmodels['rdm'] = RandomForestClassifier()\nmodels['lgc'] = LogisticRegression(max_iter=1000)\nmodels['ada'] = AdaBoostClassifier()\nmodels['gda'] = GradientBoostingClassifier()\nmodels['bca'] = BaggingClassifier()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Voting method\n# Método de votação\nfor name, model in models.items():\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    accuracy = accuracy_score(y_test,y_pred)\n    print(name, accuracy)\n    \n    #Suspeita de overfitting analisar pela curva roc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Podemos ver que três modelos estão gerando overfitting, portanto iremos revome-los","metadata":{}},{"cell_type":"code","source":"# Rodando novamente\nmodels = {}\nmodels['knn'] = KNeighborsClassifier()\nmodels['svm'] = SVC()\nmodels['bayes'] = GaussianNB()\nmodels['lgc'] = LogisticRegression(max_iter=1000)\nmodels['ada'] = AdaBoostClassifier()\nmodels['gda'] = GradientBoostingClassifier()\n\n# Acurácias\n\n# Voting method\n# Método de votação\nfor name, model in models.items():\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    accuracy = accuracy_score(y_test,y_pred)\n    print(name, accuracy)\n\n    \n    \n# Verificando o intervalo de confiança da acurácia\nfrom sklearn.model_selection import cross_val_score    \n\nknn_IC = cross_val_score(models['knn'], X_res,y_res, cv=5)\nsvm_IC = cross_val_score(models['svm'], X_res,y_res, cv=5)\nbayes_IC = cross_val_score(models['bayes'], X_res,y_res, cv=5)\nlgc_IC = cross_val_score(models['lgc'], X_res,y_res, cv=5)\ngda_IC = cross_val_score(models['gda'], X_res,y_res, cv=5)\nada_IC = cross_val_score(models['ada'], X_res,y_res, cv=5)\n\nscores = {}\n\nscores['knn'] =  knn_IC.mean() + knn_IC.std() * 2, knn_IC.mean() - knn_IC.std() * 2\nscores['svm'] =  svm_IC.mean() + svm_IC.std() * 2, svm_IC.mean() - svm_IC.std() * 2\nscores['bayes'] =   bayes_IC.mean() + bayes_IC.std() * 2,bayes_IC.mean() - bayes_IC.std() * 2\nscores['lgc'] =  lgc_IC.mean() + lgc_IC.std() * 2, lgc_IC.mean() - lgc_IC.std() * 2\nscores['gda'] =  gda_IC.mean() + gda_IC.std() * 2, gda_IC.mean() - gda_IC.std() * 2\nscores['ada'] =  ada_IC.mean() + ada_IC.std() * 2, ada_IC.mean() - ada_IC.std() * 2\n\n#Cofidence interval \nscores\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Avaliando o desempenho dos modelos que tiverem a acurácia dentro do intervalo\n# para evitar o paradoxo da Acurácia\n\nfrom sklearn.metrics import classification_report\n\n\n\nmodels = {}\n\nmodels['knn'] = KNeighborsClassifier()\nmodels['svm'] = SVC()\nmodels['bayes'] = GaussianNB()\nmodels['lgc'] = LogisticRegression(max_iter=1000)\nmodels['ada'] = AdaBoostClassifier()\nmodels['gda'] = GradientBoostingClassifier()\n\n\nfor name, model in models.items():\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    classification = classification_report(y_test,y_pred)\n    print(name, classification)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Veja como a acurácia pode enganar a escolha do modelo. Os únicos que teveram a capacidade de classificar os indíviduos como possíveis inadimplentes (Recall), foram o KNN, com 18%, beysiano com 4%, Ada com 11%, e o Gda com 18%. Ou seja, do total de inadimplentes existentes na base proposta, apenas 4 modelos coseguiram fazer essa classificação. Todos os demais conseguiram prever apenas os não inadimplentes, que não é o objetivo de análise desse trabalho.","metadata":{}},{"cell_type":"markdown","source":"# Qual a importância de analisar o Recall dos modelos nos estudos de crédito?","metadata":{}},{"cell_type":"markdown","source":"####   Quando uma empresa crediticia deseja fornecer crédito aos seus clientes, ela não só analisa as acurácias dos modelos. Na verdade isso componhe a menor parte na análise de crédito. Dado as condições internas da instituição, existe sempre um grau de risco nas aplicações de produtos financeiros, e de acordo com a situação interna da empresa, ela determinará qual individuo receberá seu crédito. Portanto, o ponto que mais afeta quem receberá o crédito, é saber qual é a probabilidade de um cliente com determinadas características vir a se tornar um possível inadimplente, e com isso saber qual é a sua probabilidade de ter atrasos, ou não quitação da dívida, e assim determinar o ponto de corte de acordo com o grau de risco que a instituição escolheu. Por exemplo, o banco X não aumentará o limite de cartão de crédito a clientes que possuem probabilidades maior ou igual a 30% de ser inadimplente. Isso equivale a determinar um ponto de corte de 0,3. Ou seja, nas decisões de quem receberá crédito ou não, não é a acurácia que nos traz o melhor desempenho do modelo, mas sim, o seu desempenho quanto a variações nos pontos de cortes, obtido pela AUC da curva ROC que é traçada a partir do Recall.","metadata":{}},{"cell_type":"markdown","source":"#### No nosso caso, o único modelo que teve a mínima capacidade classificar os positivos, foi o Beysiano. Vamos obter a AUC para comparar.","metadata":{}},{"cell_type":"code","source":"    from sklearn.metrics import roc_auc_score\n\n    model_bayes = GaussianNB().fit(x_train, y_train)\n    model_ada = AdaBoostClassifier().fit(x_train, y_train)\n    model_knn = KNeighborsClassifier().fit(x_train, y_train)\n    model_gda = GradientBoostingClassifier().fit(x_train, y_train)\n    \n  \n    y_bayes = model_bayes.predict(x_test)\n    y_ada = model_ada.predict(x_test)\n    y_knn = model_knn.predict(x_test)\n    y_gda = model_gda.predict(x_test)\n    \n    \nauc = {}\nauc['bayes'] = roc_auc_score(y_test, y_bayes)\nauc['ada'] = roc_auc_score(y_test, y_ada)\nauc['knn'] = roc_auc_score(y_test, y_knn)\nauc['gda'] = roc_auc_score(y_test, y_gda)\n\nauc\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Agora vamos interpretar nosso modelo com o Lime (ou Shap)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}