{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Breast Cancer Diagnosis by Machine Learning (Project)","metadata":{"id":"FOf0MwT8Mrhn"}},{"cell_type":"markdown","source":"#### Breast cancer diagnosis classification project based on EDA (exploratory data analysis) and different machine learning classification algorithm for finding the best classifier fit in order to dignosis and classify the Benign (noncancerous) and Malignant (cancerous) type of breast cancer ","metadata":{"id":"W6zBoTi_kEGz"}},{"cell_type":"markdown","source":"### Loading libraries and dataset","metadata":{"id":"P3cYlqxUj9Mb"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time","metadata":{"id":"ARrqJe6ujF-m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')","metadata":{"id":"Tvz1k9W5krno","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Seperate Target from features","metadata":{"id":"EWUc4wkZkzmT"}},{"cell_type":"code","source":"data.head()","metadata":{"id":"9Kdmzxnyl4Mg","outputId":"762e2482-b5eb-4e77-8f7b-f2f6027c506c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"id":"bfhH_-Qwl-hd","outputId":"5c28090c-795e-4172-f22e-fea3aef71606","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col = data.columns\nprint(col)","metadata":{"id":"qBRRQ9EsmBq5","outputId":"e03e226e-9973-4c96-adc9-d50149900395","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = data.diagnosis                                # Target or label\ndrop_col = ['Unnamed: 32','id', 'diagnosis']\nx = data.drop(drop_col, axis=1)                   # features           \nx.head()","metadata":{"id":"9RV0a2rQmLIQ","outputId":"ccd730b8-728d-4c53-eab8-2649fa2bbe06","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.shape","metadata":{"id":"RX9KD_2zFOnN","outputId":"1e4d9e6e-f85e-4de8-9a70-87a4d880255d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot diagnosis distribution","metadata":{"id":"VrS4O7_2n-7k"}},{"cell_type":"code","source":"ax = sns.countplot(y, label = 'counts')\nB, M = y.value_counts()\nprint('number of Belign tumor', B)\nprint('number of Melignant tumor', M)","metadata":{"id":"kCphZ_uQoDn5","outputId":"2463915b-2eed-489b-f616-22afa551aa3a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.describe()","metadata":{"id":"M-Hvavk3rfv3","outputId":"c79c3578-4372-4977-b2b4-ddd20d5f21bc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize standardised data with Seaborn","metadata":{"id":"acGpb67Nrklk"}},{"cell_type":"code","source":"# Take first 10 features\n\ndata = x\ndata_std = (data - data.mean()) / data.std()\ndata = pd.concat([y, data_std.iloc[:,0:10]],axis= 1)                # take first 10 features to make 1 group of viloinplot of features\ndata = pd.melt(data, id_vars = 'diagnosis',\n               var_name='features',\n               value_name = 'value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x= 'features', y='value',hue='diagnosis',data = data, split= True, inner='quart')\nplt.xticks(rotation=45)","metadata":{"id":"jpzglTVhrqYT","outputId":"efd43346-5405-4b1c-cd48-fc9727834bb6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Take next 10 features\n\ndata = pd.concat([y ,data_std.iloc[:,10:20]], axis = 1)\ndata = pd.melt(data, id_vars= 'diagnosis',\n               var_name = 'features', \n               value_name = 'value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x = 'features', y = 'value', data = data, hue = 'diagnosis', split = True, inner = 'quart')\nplt.xticks(rotation = 45)","metadata":{"id":"KQ6OxZ6ozbJg","outputId":"79f2b050-efe0-441e-ef0d-476bb9e5de78","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Take last 10 features\n\ndata = pd.concat([y ,data_std.iloc[:,20:30]], axis = 1)\ndata = pd.melt(data, id_vars= 'diagnosis',\n               var_name = 'features', \n               value_name = 'value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x = 'features', y = 'value', data = data, hue = 'diagnosis', split = True, inner = 'quart')\nplt.xticks(rotation = 45)","metadata":{"id":"zsT_ViK50vSb","outputId":"fff1b9e7-991c-42e4-bcea-a6d75b322856","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using Joint plots for feature comparison","metadata":{"id":"g04MPO8I2sQc"}},{"cell_type":"code","source":"# As from the viloin plot concavity_worst and concave points_worst are seems to be somewhat identical\n\nsns.jointplot(x.loc[:, 'concavity_worst'], x.loc[:, 'concave points_worst'],\n              kind = 'reg')\n\n# this shows that both the features has high value of correlation between them\n# as scattering is very much close","metadata":{"id":"4m90y3PN2r10","outputId":"5d7c6d35-ac94-4344-ad85-212c7c1015bb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Obsorving the distribution of the values and their varience with Swarm plots","metadata":{"id":"KjRzk8MS5Y8o"}},{"cell_type":"code","source":"# Swarm plots\n# Take fisrt 10 features\n\nsns.set(style = 'whitegrid', palette= 'muted')\ndata = x\ndata_std = (data - data.mean()) / data.std()\ndata = pd.concat([y, data_std.iloc[:, 0:10]], axis = 1)\ndata = pd.melt(data, id_vars= 'diagnosis',\n               var_name = 'features',\n               value_name = 'value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x = 'features', y = 'value', data = data, hue = 'diagnosis')\nplt.xticks(rotation = 45)","metadata":{"id":"h-r6N3zx6p-8","outputId":"c6abfd86-9fe2-46ae-aa94-bd229300fbef","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Take next 10 features\n\nsns.set(style = 'whitegrid', palette= 'muted')\ndata = x\ndata_std = (data - data.mean()) / data.std()\ndata = pd.concat([y, data_std.iloc[:, 10:20]], axis = 1)\ndata = pd.melt(data, id_vars= 'diagnosis',\n               var_name = 'features',\n               value_name = 'value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x = 'features', y = 'value', data = data, hue = 'diagnosis')\nplt.xticks(rotation = 45)","metadata":{"id":"tK-Gvm1s9iOa","outputId":"114c324a-84e9-4d8c-d569-27b002f340a2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Take last 10 features\n\nsns.set(style = 'whitegrid', palette= 'muted')\ndata = x\ndata_std = (data - data.mean()) / data.std()\ndata = pd.concat([y, data_std.iloc[:, 20:30]], axis = 1)\ndata = pd.melt(data, id_vars= 'diagnosis',\n               var_name = 'features',\n               value_name = 'value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x = 'features', y = 'value', data = data, hue = 'diagnosis')\nplt.xticks(rotation = 45)","metadata":{"id":"eHNJT0U_9oRa","outputId":"62a3866e-3ec7-4844-c316-39452d5de167","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observing all pairwise correlation","metadata":{"id":"bgKR_aj5OquB"}},{"cell_type":"code","source":"a, ax = plt.subplots(figsize = (18,18))\nsns.heatmap(x.corr(), annot= True, fmt= '.1f', linewidths= 0.5, ax = ax)\n# these heatmap shows the relation between the correlation of each of the features with each another by heatmap","metadata":{"id":"P_WqXp6qPI-a","outputId":"4be69287-b0af-4e0d-db8c-aae8473984f2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Droping correlated columns from the feature list","metadata":{"id":"-1HgXYXNrA5a"}},{"cell_type":"code","source":"drop_cols = ['perimeter_mean', 'radius_mean', 'compactness_mean', 'concave points_mean', 'radius_se', 'perimeter_se', \n             'radius_worst', 'perimeter_worst','compactness_worst', 'concave points_worst', 'compactness_se',\n             'concave points_se','texture_worst','area_worst']\ndf = x.drop(drop_cols, axis=1)\ndf.head()","metadata":{"id":"T_XqjuYZrLS0","outputId":"38700e32-2cd6-4150-9aec-72d4ec1de978","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"id":"fZ_zXRm5J9bM","outputId":"bf010b25-6683-4bd4-ec98-5d77e80ad806","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize = (14,14))\nsns.heatmap(df.corr(), annot=True, fmt = '.1f', linewidths=0.5, ax = ax)","metadata":{"id":"uOnDezI1tOEP","outputId":"188e987a-e9d0-48d1-e71e-655440561926","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection Techniques to get the prediction and highest accuracy","metadata":{"id":"O26jPOyOmrdF"}},{"cell_type":"markdown","source":"### Feature extraction using principle componant analysis(PCA)","metadata":{"id":"M5C_TZYGVrgO"}},{"cell_type":"code","source":"## get the original feature and label set then spilt again newly\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size =0.3, random_state = 42)\n\nx_train_norm = (x_train - x_train.mean())/ (x_train.max() - x_train.min())\nx_test_norm = (x_test - x_test.mean())/ (x_test.max() - x_test.min())\n\nfrom sklearn.decomposition import PCA\n\npca = PCA()\npca.fit(x_train_norm)","metadata":{"id":"ORlAJPTwVqur","outputId":"e7d22025-ad19-4af2-c392-53f2d963e836","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_norm.shape","metadata":{"id":"wkiYT2JBWF8o","outputId":"394ed0bb-26a1-42c4-a977-3cde17bbcc94","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# here we gonna see the commulative sum varience ratio vs number of feeatures we gonna take to get that much percent of ratio/accuarcy\n\nplt.figure(1, figsize=(10,8))\nsns.lineplot(data = np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components/features')\nplt.ylabel('Cummualtive explained varience')\n\n# these shows that to get around 99% of accuaracy we have to take around 16-17 features for prediction","metadata":{"id":"6dPflqAjWFAx","outputId":"868ebab3-14a6-498b-ff11-18b95237254b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1) XGBoost Classifier","metadata":{"id":"igOWYrtDK3U3"}},{"cell_type":"markdown","source":"### 1_classification using XGBoost (minimal feature selection)","metadata":{"id":"d4WnGLbPRSKf"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score","metadata":{"id":"SzETLb1eRffe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(df, y, test_size = 0.3, random_state = 42)\n\nclf_1 = XGBClassifier(random_state=42)\nclf_1 = clf_1.fit(x_train, y_train)","metadata":{"id":"zvK7PtSLSSrl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_1 = clf_1.predict(x_test)\nprint('accuracy is : ', accuracy_score(y_test, y_pred_1)) \ncm = confusion_matrix(y_test, y_pred_1)\nsns.heatmap(cm, annot= True, fmt = 'd')","metadata":{"id":"cFGyROMWTYgR","outputId":"51a6644e-703e-4ffe-9838-0ca7c2588e75","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2_Univariate feature selection and XGBoost","metadata":{"id":"uPEWhI61V0kE"}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2","metadata":{"id":"2w1lLQyhWBQx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"select_feature = SelectKBest(chi2, k =10).fit(x_train, y_train)\n\nprint('score list: ', select_feature.scores_)\nprint('feature list: ', x_train.columns)","metadata":{"id":"rDR5j9pzWOHr","outputId":"30a99180-aa3d-429a-f8b0-387ba536a8cb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.shape","metadata":{"id":"bm2Dg4hIJ164","outputId":"74d65f67-c0bc-47ac-ab5b-1cd4ced8e811","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from selectkbest function we gonna select the features of top 10 values/scores of k\n\nx_train_2 = select_feature.transform(x_train)\nx_test_2 = select_feature.transform(x_test)\n\nclf_2 = XGBClassifier()\nclf_2.fit(x_train_2, y_train)\n\ny_pred_2 = clf_2.predict(x_test_2)\nprint('accuracy is: ', accuracy_score(y_test, y_pred_2))\ncm = confusion_matrix(y_test, y_pred_2)\nsns.heatmap(cm, annot= True, fmt = 'd')","metadata":{"id":"64Jfvkj4YjA-","outputId":"c1f1d852-59bf-4b49-c85f-323d97b28f35","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_2.shape","metadata":{"id":"zgM5w3lbKR_w","outputId":"6d7536dc-a625-481d-d0e7-406a3e37855d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3_Recursive feature elemination with cross validation","metadata":{"id":"2PX_B_doaoEb"}},{"cell_type":"code","source":"from sklearn.feature_selection import RFECV\n\nclf_3 = XGBClassifier()\nrfecv = RFECV(estimator = clf_3, step = 1, cv = 5, scoring = 'accuracy', n_jobs = -1).fit(x_train, y_train)       # step = 1, means eliminate 1 feature at each step, cv = cross validation folds\n\nprint('optimal number of features: ', rfecv.n_features_)\nprint('best features: ', x_train.columns[rfecv.support_])","metadata":{"id":"HSW3kDp0b-ET","outputId":"28f82562-ac4d-4598-b86d-3aab0cc11804","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('accuracy is: ', accuracy_score(y_test, rfecv.predict(x_test)))","metadata":{"id":"5zaak2JDfk7s","outputId":"d060b083-a7d2-4dbc-93b1-cb2ff8e8bcf5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2) Logistic Regression classifier","metadata":{"id":"EkcqvhXpMdGs"}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclassifier_2 = LogisticRegression(max_iter= 200)\nclassifier_2.fit(x_train,y_train)","metadata":{"id":"NgTBx37uMclQ","outputId":"0bdb792d-f8a7-4e4a-ea7b-9edd9ceeacae","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_lg = classifier_2.predict(x_test)\nprint('accuracy is: ', accuracy_score(y_test, y_pred_lg))\ncm = confusion_matrix(y_test, y_pred_lg)\nsns.heatmap(cm, annot= True, fmt = 'd')","metadata":{"id":"CC8K-4AyMce9","outputId":"b8314107-449e-4fc3-e2da-8cdf7f90d247","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3) SVM classifier","metadata":{"id":"9dMZQR4pQJyl"}},{"cell_type":"code","source":"from sklearn.svm import SVC\nclassifier_3 = SVC(kernel = 'rbf')\nclassifier_3.fit(x_train,y_train)","metadata":{"id":"LzgmtPriMcdF","outputId":"79fcb999-1759-44ab-d990-327bba288da8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_svm = classifier_3.predict(x_test)\nprint('accuracy is: ', accuracy_score(y_test, y_pred_svm))\ncm = confusion_matrix(y_test, y_pred_svm)\nsns.heatmap(cm, annot= True, fmt = 'd')","metadata":{"id":"oJkYwx5VQa8X","outputId":"c1cdf414-df16-4da5-cf7d-fa99891c544f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4) Naive Bayes classifier","metadata":{"id":"L-xN4HMCRc5i"}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nclassifier_4 = GaussianNB()\nclassifier_4.fit(x_train,y_train)","metadata":{"id":"W9Tnk9u0Qa6e","outputId":"272ba910-2d72-4132-a879-278b02071cfd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_nb = classifier_4.predict(x_test)\nprint('Accuracy of model: ', accuracy_score(y_test,y_pred_nb))\ncm = confusion_matrix(y_test,y_pred_nb)\nsns.heatmap(cm, annot= True, fmt= 'd')","metadata":{"id":"Rbb5J6fpSC9S","outputId":"f2a6124a-b638-4859-af43-a9d4c6104a5e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5) Decision Tree classifier","metadata":{"id":"QFh51bnbS0xJ"}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nclassifier_5 = DecisionTreeClassifier(min_samples_split=2)\nclassifier_5.fit(x_train,y_train)","metadata":{"id":"opS_aRoNSzKp","outputId":"b1973e3e-8a8b-4145-bdf9-44769452e952","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_dt = classifier_5.predict(x_test)\nprint('accuracy is: ', accuracy_score(y_test, y_pred_dt))\ncm = confusion_matrix(y_test, y_pred_dt)\nsns.heatmap(cm, annot= True, fmt = 'd')","metadata":{"id":"SqhBjJyoTyx4","outputId":"d197af3b-283f-4816-b026-d1a533437d87","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6) Random forest classifier","metadata":{"id":"C47B4UBkUJNW"}},{"cell_type":"markdown","source":"### 1_minimal feature selection - 16 features","metadata":{"id":"wfKBDrkJeuLt"}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclassifier_6 = RandomForestClassifier()\nclassifier_6.fit(x_train,y_train)","metadata":{"id":"qEHMTAE-UH8D","outputId":"5a11a930-cbae-4ae7-b53b-ba94e8673326","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_rf = classifier_6.predict(x_test)\nprint('accuracy is: ', accuracy_score(y_test, y_pred_rf))\ncm = confusion_matrix(y_test, y_pred_rf)\nsns.heatmap(cm, annot= True, fmt = 'd')","metadata":{"id":"5OVMo59AUibn","outputId":"caf96c99-573b-4253-f357-c20bdf6448f9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2_univariate feature selection","metadata":{"id":"r4hWK4gFfBfI"}},{"cell_type":"code","source":"# from sklearn.feature_selection import SelectKBest\n# from sklearn.feature_selection import chi2\n\n# select_feature = SelectKBest(chi2, k =10).fit(x_train, y_train)\n\n# print('score list: ', select_feature.scores_)\n# print('feature list: ', x_train.columns)\n\n# x_train_2 = select_feature.transform(x_train)\n# x_test_2 = select_feature.transform(x_test)","metadata":{"id":"b_4eyompfQl1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we have allready performed above steps for selectkbest algorithm in XGBoost classifier","metadata":{"id":"OdXRcvnGgRlC"}},{"cell_type":"code","source":"# from selectkbest function we gonna select the features of top 10 values/scores of k\n\nclf_7 = RandomForestClassifier()\nclf_7.fit(x_train_2, y_train)\n\ny_pred_rf10 = clf_2.predict(x_test_2)\nprint('accuracy is: ', accuracy_score(y_test, y_pred_rf10))\ncm = confusion_matrix(y_test, y_pred_rf10)\nsns.heatmap(cm, annot= True, fmt = 'd')","metadata":{"id":"ntHRm61QfNg1","outputId":"a6b17a6e-181e-46ad-d1bc-c2e1643c2cdf","trusted":true},"execution_count":null,"outputs":[]}]}