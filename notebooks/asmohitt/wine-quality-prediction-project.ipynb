{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Wine Quality Prediction!\nHey everyone! In this project we're gonna be looking at the wine quality dataset! <br>\nWe'll be looking at supervised learning machine learning algorithm Random Forest and try to improve the accuracy as we'll tune the hyper parameters! <br>\nHappy Learning!","metadata":{}},{"cell_type":"markdown","source":"Let's start by importing the required libraries! <br>\nA little about the libraries!\n* numpy - for numpy arrays, useful for processing and scientific computing\n* pandas - helpful for creating dataframes and storing data\n* matplotlib.pyplot - useful for creating plots and charts\n* seaborn - useful for data visualization like matplotlib\n* train_test_split - to split the data into training and test set\n* Random forest Classifier - An ensemble model which we'll use to train our model on\n* accuracy_score - to check the accuracy of our model\n\nStandardScalar - in this project we're using support vector machine classification and this class cannot process the data given to it unless the data is standardized.\nsvm - the suport vector machine class in the sklearn package","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2021-08-27T13:17:55.126336Z","iopub.execute_input":"2021-08-27T13:17:55.126715Z","iopub.status.idle":"2021-08-27T13:17:56.315008Z","shell.execute_reply.started":"2021-08-27T13:17:55.126635Z","shell.execute_reply":"2021-08-27T13:17:56.314084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's import our data!\nWe're gonna use the wine quality dataset from the UCI ML Dataset! ","metadata":{}},{"cell_type":"code","source":"wine_data = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\nwine_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-27T13:17:56.316368Z","iopub.execute_input":"2021-08-27T13:17:56.316646Z","iopub.status.idle":"2021-08-27T13:17:56.339865Z","shell.execute_reply.started":"2021-08-27T13:17:56.316619Z","shell.execute_reply":"2021-08-27T13:17:56.339049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's explore the dataset further! <br>\nEploratory data analysis!","metadata":{}},{"cell_type":"code","source":"wine_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T13:17:56.341405Z","iopub.execute_input":"2021-08-27T13:17:56.341673Z","iopub.status.idle":"2021-08-27T13:17:56.372027Z","shell.execute_reply.started":"2021-08-27T13:17:56.341647Z","shell.execute_reply":"2021-08-27T13:17:56.371174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wine_data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T13:17:56.373354Z","iopub.execute_input":"2021-08-27T13:17:56.373617Z","iopub.status.idle":"2021-08-27T13:17:56.418218Z","shell.execute_reply.started":"2021-08-27T13:17:56.373591Z","shell.execute_reply":"2021-08-27T13:17:56.417275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wine_data.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T13:17:56.419527Z","iopub.execute_input":"2021-08-27T13:17:56.420152Z","iopub.status.idle":"2021-08-27T13:17:56.437131Z","shell.execute_reply.started":"2021-08-27T13:17:56.420113Z","shell.execute_reply":"2021-08-27T13:17:56.436211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Doesn't look like there's any missing values! Let's be sure! <br>\nChecking for missing values","metadata":{}},{"cell_type":"code","source":"wine_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T13:17:56.438413Z","iopub.execute_input":"2021-08-27T13:17:56.43903Z","iopub.status.idle":"2021-08-27T13:17:56.447564Z","shell.execute_reply.started":"2021-08-27T13:17:56.438966Z","shell.execute_reply":"2021-08-27T13:17:56.446864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see we have a clean dataset without any missing values!","metadata":{}},{"cell_type":"markdown","source":"Data Analysis and Visualization","metadata":{}},{"cell_type":"code","source":"# Let's check the number of values under each quality classification\nsns.catplot(x='quality',data= wine_data,  kind='count')","metadata":{"execution":{"iopub.status.busy":"2021-08-27T13:17:56.448941Z","iopub.execute_input":"2021-08-27T13:17:56.449472Z","iopub.status.idle":"2021-08-27T13:17:56.716314Z","shell.execute_reply.started":"2021-08-27T13:17:56.449442Z","shell.execute_reply":"2021-08-27T13:17:56.715421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's compare some features with the quality !","metadata":{}},{"cell_type":"code","source":"# volatile acidity vs quality\nplot = plt.figure(figsize=(5,5))\nplt.title('Volatile acidity vs Quality')\nsns.barplot(x='quality', y='volatile acidity', data=wine_data)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T13:17:56.718296Z","iopub.execute_input":"2021-08-27T13:17:56.718557Z","iopub.status.idle":"2021-08-27T13:17:57.016596Z","shell.execute_reply.started":"2021-08-27T13:17:56.718533Z","shell.execute_reply":"2021-08-27T13:17:57.015513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# citric acid vs quality\nplot = plt.figure(figsize=(5,5))\nplt.title('Citric acid vs Quality')\nsns.barplot(x='quality', y='citric acid', data=wine_data)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T13:17:57.018323Z","iopub.execute_input":"2021-08-27T13:17:57.018713Z","iopub.status.idle":"2021-08-27T13:17:57.31626Z","shell.execute_reply.started":"2021-08-27T13:17:57.018673Z","shell.execute_reply":"2021-08-27T13:17:57.315377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's find the correlation! <br>\nAbout the heatmap!\n* corr - correlation calculated\n* cbar - color bar to indicate the values range\n* square - To get a square form\n* fmt - we need one floating point value\n* annot - annotations on the sides!\n* annot_kws - size of annotations\n* cmap - color of the heatmap!","metadata":{}},{"cell_type":"code","source":"corr = wine_data.corr()\n# Let's create a heatmap!\nplot = plt.figure(figsize=(10,10))\nplt.title('Correlation heatmap!')\nsns.heatmap(corr,cbar=True,square=True,fmt='.1f',annot=True,annot_kws={'size':8},cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2021-08-27T13:17:57.31741Z","iopub.execute_input":"2021-08-27T13:17:57.317685Z","iopub.status.idle":"2021-08-27T13:17:58.435368Z","shell.execute_reply.started":"2021-08-27T13:17:57.317658Z","shell.execute_reply":"2021-08-27T13:17:58.434272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data Preprocessing! <br>\nLet's seperate the data and the labels!","metadata":{}},{"cell_type":"code","source":"X = wine_data.drop(columns='quality',axis=1)\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T13:17:58.436551Z","iopub.execute_input":"2021-08-27T13:17:58.436834Z","iopub.status.idle":"2021-08-27T13:17:58.455519Z","shell.execute_reply.started":"2021-08-27T13:17:58.436804Z","shell.execute_reply":"2021-08-27T13:17:58.454603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the quality values, we're gonna binarize the values to either good wine quality **1** or bad wine **0**","metadata":{}},{"cell_type":"code","source":"y = wine_data['quality'].apply(lambda y_value: 1 if y_value>=7 else 0 )\ny","metadata":{"execution":{"iopub.status.busy":"2021-08-27T13:17:58.456791Z","iopub.execute_input":"2021-08-27T13:17:58.457107Z","iopub.status.idle":"2021-08-27T13:17:58.471162Z","shell.execute_reply.started":"2021-08-27T13:17:58.457079Z","shell.execute_reply":"2021-08-27T13:17:58.469618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have our data and labels, let's split the data into train and test split!","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T13:17:58.472529Z","iopub.execute_input":"2021-08-27T13:17:58.472856Z","iopub.status.idle":"2021-08-27T13:17:58.479104Z","shell.execute_reply.started":"2021-08-27T13:17:58.472828Z","shell.execute_reply":"2021-08-27T13:17:58.478193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's train our model using random forest classifier!","metadata":{}},{"cell_type":"code","source":"classifier = RandomForestClassifier()\nclassifier.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T13:17:58.480328Z","iopub.execute_input":"2021-08-27T13:17:58.480657Z","iopub.status.idle":"2021-08-27T13:17:58.750657Z","shell.execute_reply.started":"2021-08-27T13:17:58.480631Z","shell.execute_reply":"2021-08-27T13:17:58.749817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model evaluation using K-fold cross validation! <br>\nWhy k-fold cross validation ? To be sure that we didn't get lucky on the train test!","metadata":{}},{"cell_type":"code","source":"y_pred_train = classifier.predict(X_train)\naccuracy = accuracy_score(y_pred_train, y_train)\nprint(\"Accuracy of the model on training data is:\", accuracy)\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier,X = X_train,y= y_train , cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f}\".format(accuracies.std()*100))","metadata":{"execution":{"iopub.status.busy":"2021-08-27T13:17:58.751936Z","iopub.execute_input":"2021-08-27T13:17:58.752278Z","iopub.status.idle":"2021-08-27T13:18:01.334937Z","shell.execute_reply.started":"2021-08-27T13:17:58.75225Z","shell.execute_reply":"2021-08-27T13:18:01.334045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see that's a pretty good accuracy! <br>\nLet's check our model performance on the test set!","metadata":{}},{"cell_type":"code","source":"y_pred_test = classifier.predict(X_test)\naccuracy = accuracy_score(y_pred_test, y_test)\nprint(\"Accuracy: {:.2f} %\".format(accuracy*100))\ncf_matrix = confusion_matrix(y_pred_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T13:18:01.336019Z","iopub.execute_input":"2021-08-27T13:18:01.336267Z","iopub.status.idle":"2021-08-27T13:18:01.357364Z","shell.execute_reply.started":"2021-08-27T13:18:01.336243Z","shell.execute_reply":"2021-08-27T13:18:01.356687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#.  visualizing the confusion matrix!\nsns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n            fmt='.2%', cmap='Reds')","metadata":{"execution":{"iopub.status.busy":"2021-08-27T13:18:01.358253Z","iopub.execute_input":"2021-08-27T13:18:01.358608Z","iopub.status.idle":"2021-08-27T13:18:01.587794Z","shell.execute_reply.started":"2021-08-27T13:18:01.358583Z","shell.execute_reply":"2021-08-27T13:18:01.586911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's check if there is a better hyperparameter we can tune to improve over all accuracy. We use **Grid Search CV** here","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparameters = [{'bootstrap': [True], 'max_depth': [10, 20], 'max_features': ['auto', 'sqrt'], 'min_samples_leaf': [1, 2, 4], 'min_samples_split': [2, 5, 10], 'n_estimators': [100,200]}]\ngrid_search = GridSearchCV(estimator=classifier,\n                          param_grid=parameters,\n                          scoring='accuracy',\n                          cv=10)\ngrid_search.fit(X_train,y_train)\nprint(\"Best Accuracy: {:.2f} %\".format(grid_search.best_score_*100))\nprint(\"Best Parameters: \", grid_search.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T13:18:01.588975Z","iopub.execute_input":"2021-08-27T13:18:01.589256Z","iopub.status.idle":"2021-08-27T13:22:24.073151Z","shell.execute_reply.started":"2021-08-27T13:18:01.589228Z","shell.execute_reply":"2021-08-27T13:22:24.072213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we've come to an end let's look back upon what we did in this project! <br>\n* imported the required libraries!\n* read our data from the **Red Wine Quality** dataset!\n* Checked for any missing values!\n* Found some useful information between different features using plots and graphs!\n* Made a heatmap to find the correlation between different features!\n* Split the data into training and test sets!\n* Trained our model using supervised learning algorithm - Random forest classification ! \n* Used KFold cross validation to get the accuracy !\n* With a little bit of hyper parameter tuning we we're able to get a good accuracy score of **92**% !\n<br>\nHope you all enjoyed this notebook! <br>\nHappy Learning!!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}