{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Contents\n* EDA\n* Dimension Reduction\n* Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install seaborn==0.11.0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler \nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import roc_auc_score,accuracy_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport math\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fetal_health_df = pd.read_csv('/kaggle/input/fetal-health-classification/fetal_health.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fetal_health_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fetal_health_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"fetal_health_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(21,21))\nfor i, col in enumerate(['baseline value', 'accelerations', 'fetal_movement',\n       'uterine_contractions', 'light_decelerations', 'severe_decelerations',\n       'prolongued_decelerations', 'abnormal_short_term_variability',\n       'mean_value_of_short_term_variability',\n       'percentage_of_time_with_abnormal_long_term_variability',\n       'mean_value_of_long_term_variability', 'histogram_width',\n       'histogram_min', 'histogram_max', 'histogram_number_of_peaks',\n       'histogram_number_of_zeroes', 'histogram_mode', 'histogram_mean',\n       'histogram_median', 'histogram_variance', 'histogram_tendency']):\n    plt.subplot(5,5,i+1)\n    sns.axes_style()\n    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n    sns.boxplot(data=fetal_health_df,x=col)    \n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights**\n* No null values, thats great\n* I don't think there are any an outlier, In *histogram_variance* column there are few values which looks like outliers but not very extreme values. Lets find more about it."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,2,figsize=(25,5))\nsns.kdeplot(fetal_health_df['fetal_movement'],ax=ax[0],hue=fetal_health_df['fetal_health'],multiple=\"stack\");\nsns.kdeplot(fetal_health_df[\"accelerations\"],ax=ax[1],hue=fetal_health_df['fetal_health'],multiple=\"stack\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,2,figsize=(25,5))\nsns.kdeplot(fetal_health_df['baseline value'],ax=ax[0],hue=fetal_health_df['fetal_health'],multiple=\"stack\");\nsns.kdeplot(fetal_health_df[\"uterine_contractions\"],ax=ax[1],hue=fetal_health_df['fetal_health'],multiple=\"stack\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,2,figsize=(25,5))\nsns.kdeplot(fetal_health_df['light_decelerations'],ax=ax[0],hue=fetal_health_df['fetal_health'],multiple=\"stack\");\nsns.kdeplot(fetal_health_df[\"abnormal_short_term_variability\"],ax=ax[1],hue=fetal_health_df['fetal_health'],multiple=\"stack\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,2,figsize=(25,5))\nsns.kdeplot(fetal_health_df['mean_value_of_short_term_variability'],ax=ax[0],hue=fetal_health_df['fetal_health'],multiple=\"stack\");\nsns.kdeplot(fetal_health_df[\"histogram_tendency\"],ax=ax[1],hue=fetal_health_df['fetal_health'],multiple=\"stack\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights** <br>\n* Features uterine_contractions, abnormal_short_term_variability and mean_value_of_short_term_variability can be useful classification because I think these can distinguish the class.\n* For these features, we see that region of fetal health can be clustered, not completely but to an extent."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,2,figsize=(25,5))\nsns.scatterplot(data=fetal_health_df,x='prolongued_decelerations',y='uterine_contractions',hue='fetal_health',palette=\"deep\",s=100,ax=ax[0]);\nsns.scatterplot(data=fetal_health_df,x='prolongued_decelerations',y='abnormal_short_term_variability',hue='fetal_health',palette=\"deep\",s=100,ax=ax[1]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,2,figsize=(25,5))\nsns.scatterplot(data=fetal_health_df,x='prolongued_decelerations',y='uterine_contractions',hue='fetal_health',palette=\"deep\",s=100,ax=ax[0]);\nsns.scatterplot(data=fetal_health_df,x='prolongued_decelerations',y='fetal_movement',hue='fetal_health',palette=\"deep\",s=100,ax=ax[1]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,2,figsize=(25,5))\nsns.scatterplot(data=fetal_health_df,x='prolongued_decelerations',y='abnormal_short_term_variability',hue='fetal_health',palette=\"deep\",s=100,ax=ax[0]);\nsns.scatterplot(data=fetal_health_df,x='prolongued_decelerations',y='mean_value_of_short_term_variability',hue='fetal_health',palette=\"deep\",s=100,ax=ax[1]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights**<br>\n* From the above feature graphs we see that *fetal Pathological* can be can be distinguish, with some error ofcourse, but class *Suspect* is not easily separated."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,2,figsize=(25,5))\nsns.scatterplot(data=fetal_health_df,x='abnormal_short_term_variability',y='baseline value',hue='fetal_health',palette=\"deep\",s=100,ax=ax[0]);\nsns.scatterplot(data=fetal_health_df,x='abnormal_short_term_variability',y='accelerations',hue='fetal_health',palette=\"deep\",s=100,ax=ax[1]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,2,figsize=(25,5))\nsns.scatterplot(data=fetal_health_df,x='abnormal_short_term_variability',y='histogram_width',hue='fetal_health',palette=\"deep\",s=100,ax=ax[0]);\nsns.scatterplot(data=fetal_health_df,x='abnormal_short_term_variability',y='histogram_min',hue='fetal_health',palette=\"deep\",s=100,ax=ax[1]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,2,figsize=(25,5))\nsns.scatterplot(data=fetal_health_df,x='abnormal_short_term_variability',y='histogram_number_of_zeroes',hue='fetal_health',palette=\"deep\",s=100,ax=ax[0]);\nsns.scatterplot(data=fetal_health_df,x='abnormal_short_term_variability',y='histogram_mode',hue='fetal_health',palette=\"deep\",s=100,ax=ax[1]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights**<br>\n* In the plot, *abnormal_short_term_variability* vs *baseline value* there is small cluster formed after abnormal_short_term_variability value 50 and baseline value value greater then 130. clearly *Suspect* cluster is formed.\n* Similarly with the features *abnormal_short_term_variability* vs *histogram_min* similar cluster is formed.\n* Features abnormal_short_term_variability, histogram_min, histogram_mode, prolongued_decelerations and uterine_contractions can be useful for classification."},{"metadata":{},"cell_type":"markdown","source":"### Let look into distribution of these features"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,2,figsize=(25,5))\nsns.violinplot(data=fetal_health_df,y='abnormal_short_term_variability',x='fetal_health',palette=\"deep\",ax=ax[0]);\nsns.violinplot(data=fetal_health_df,y='histogram_mode',x='fetal_health',palette=\"deep\",ax=ax[1]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,2,figsize=(25,5))\nsns.violinplot(data=fetal_health_df,y='prolongued_decelerations',x='fetal_health',palette=\"deep\",ax=ax[0]);\nsns.violinplot(data=fetal_health_df,y='histogram_min',x='fetal_health',palette=\"deep\",ax=ax[1]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(figsize=(25,5))\nsns.violinplot(data=fetal_health_df,y='uterine_contractions',x='fetal_health',palette=\"deep\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights**\n* In feature *abnormal_short_term_variability* majority of the suspected and Pathological points are around 60 and greater than 60.\n* In feature *histogram_mode*  it is little difficult seperate normal and suspected as they are in almost same distribution.\n* In feature prolongued_decelerations, Normal and suspected values are around 0, but Pathological are distributed.\n* Again I don't see any specific pattern in *histogram_min*\n* In feature *uterine_contractions*, Most of the points in suspected and Pathological lies around 0, Most important is that there is little proper sepration for suspected class."},{"metadata":{},"cell_type":"markdown","source":"## Dimension reduction"},{"metadata":{},"cell_type":"markdown","source":"**t-SNE**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = fetal_health_df[['baseline value', 'accelerations', 'fetal_movement',\n       'uterine_contractions', 'light_decelerations', 'severe_decelerations',\n       'prolongued_decelerations', 'abnormal_short_term_variability',\n       'mean_value_of_short_term_variability',\n       'percentage_of_time_with_abnormal_long_term_variability',\n       'mean_value_of_long_term_variability', 'histogram_width',\n       'histogram_min', 'histogram_max', 'histogram_number_of_peaks',\n       'histogram_number_of_zeroes', 'histogram_mode', 'histogram_mean',\n       'histogram_median', 'histogram_variance', 'histogram_tendency']]\ny = fetal_health_df['fetal_health'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE(n_components=2, perplexity=90, learning_rate=200)\nX_embedding = tsne.fit_transform(x)\n\nfor_tsne = np.hstack((X_embedding, y.reshape(-1,1)))\nfor_tsne_df = pd.DataFrame(data=for_tsne, columns=['Dimension_x','Dimension_y','Score'])\nfig, ax =plt.subplots(figsize=(25,5))\nsns.scatterplot(data=for_tsne_df,x='Dimension_x',y='Dimension_y',hue='Score',palette=\"deep\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well well well, t-SNE is giving good results, we can see clusters are differentiable."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = fetal_health_df[['baseline value', 'accelerations', 'fetal_movement',\n       'uterine_contractions', 'light_decelerations', 'severe_decelerations',\n       'prolongued_decelerations', 'abnormal_short_term_variability',\n       'mean_value_of_short_term_variability',\n       'percentage_of_time_with_abnormal_long_term_variability',\n       'mean_value_of_long_term_variability', 'histogram_width',\n       'histogram_min', 'histogram_max', 'histogram_number_of_peaks',\n       'histogram_number_of_zeroes', 'histogram_mode', 'histogram_mean',\n       'histogram_median', 'histogram_variance', 'histogram_tendency']].values\ny = fetal_health_df['fetal_health'].values\nx = StandardScaler().fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA().fit(x)\nfig, ax =plt.subplots(figsize=(15,5))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I think 15 componets would be fine for training, since almost 98% of variance is preserved. We have reduced from 21 features to 15 features."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluation_stats(model,X_train, X_test, y_train, y_test,algo,is_feature=False):\n    print('Train classification report')\n    y_pred_train = model.predict(X_train)                           \n    print(classification_report(y_train, y_pred_train))\n    print('Test classification report')\n    y_pred_test = model.predict(X_test)                           \n    print(classification_report(y_test, y_pred_test))\n    print(\"\\n\")\n    print(\"Train confusion matrix\")\n    print(confusion_matrix(y_train, y_pred_train))\n    print(\"Test confusion_matrix\")\n    print(confusion_matrix(y_test, y_pred_test))\n    \n    if is_feature:\n        plot_feature_importance(rf_model.feature_importances_,X.columns,algo)\n\ndef training(model,X_train, y_train):\n    return model.fit(X_train, y_train)\n\ndef plot_feature_importance(importance,names,model_type):\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n    #Define size of bar plot\n    plt.figure(figsize=(10,8))\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + ' FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fetal_health_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = fetal_health_df.drop([\"fetal_health\"], axis=1)\ny = fetal_health_df[\"fetal_health\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Splitting train and test**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Oversampling**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sm = SMOTE(random_state=101)\nX_res, y_res = sm.fit_resample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LogisticRegression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"C = [10**x for x in range(-5,5)]\ntuned_parameters = [{'C': C}]\nstandardscaler=StandardScaler()\nX_res_scale=standardscaler.fit_transform(X_res)\nX_test_scale=standardscaler.transform(X_test)\n\nmodel = GridSearchCV(LogisticRegression(), tuned_parameters, scoring = 'accuracy', cv=3,return_train_score = True)\nmodel.fit(X_res_scale, y_res);\n\nprint(model.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame.from_dict(model.cv_results_)\nresults = results.sort_values(['param_C'])\n\ntrain_acc= results['mean_train_score']\ntrain_acc_std= results['std_train_score']\ncv_acc = results['mean_test_score'] \ncv_acc_std= results['std_test_score']\nC =  results['param_C']\n\nC = [math.log(x,10) for x in C]\nplt.plot(C, train_acc, label='Train Acc')\n\nplt.plot(C, cv_acc, label='CV Acc')\n\nplt.scatter(C, train_acc, label='Train points')\nplt.scatter(C, cv_acc, label='CV points')\n\nplt.legend()\nplt.xlabel(\"C: hyperparameter\")\nplt.ylabel(\"ACC\")\nplt.title(\"Error Plot\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_model = training(LogisticRegression(C=10),X_res, y_res)\nevaluation_stats(lr_model,X_res_scale, X_test_scale, y_res, y_test,'LogisticRegression',is_feature=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**RandomForestClassifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n        'n_estimators': [200, 500, 1000],\n        'max_depth': [2,4, 5, 8]\n        }\nmodel = GridSearchCV(RandomForestClassifier(), params, scoring = 'accuracy', cv=3,return_train_score = True)\nmodel.fit(X_res_scale, y_res)\n\nprint(model.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = training(RandomForestClassifier(max_depth=8, n_estimators=1000),X_res, y_res)\nevaluation_stats(rf_model,X_res, X_test, y_res, y_test,'RandomForestClassifier',is_feature=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**XGBClassifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n        'n_estimators': [200, 500, 1000],\n        'max_depth': [2,4, 5, 8,10]\n        }\nmodel = GridSearchCV(XGBClassifier(), params, scoring = 'accuracy', cv=3,return_train_score = True)\nmodel.fit(X_res_scale, y_res)\n\nprint(model.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xbg_model = training(XGBClassifier(max_depth=8,n_estimators=200),X_train,y_train)\nevaluation_stats(xbg_model,X_train, X_test, y_train, y_test,'XGB',is_feature=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights** <br>\nWe tried LR, RF and XBG\n* LR gave train and test accuracy around 63% and 66%, F1_score was very less for suspected and pathological in test.\n* RF gave train and test accuracy around 98% and 93%, F1 score is also good and few features which we found as important can be seen in feature importance graph. My only concern here is that model should not be over fitting.\n* XBG is overfitting I believe, Lets not think about it."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}