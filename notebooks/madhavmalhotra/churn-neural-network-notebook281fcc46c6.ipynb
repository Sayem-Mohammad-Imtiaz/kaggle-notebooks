{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-26T18:38:25.364393Z","iopub.execute_input":"2021-07-26T18:38:25.365112Z","iopub.status.idle":"2021-07-26T18:38:25.389921Z","shell.execute_reply.started":"2021-07-26T18:38:25.364953Z","shell.execute_reply":"2021-07-26T18:38:25.388713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# First, I'll read the data and split into training and test sets","metadata":{}},{"cell_type":"code","source":"#Read Raw data\nimport pandas as pd\ndf = pd.read_csv('/kaggle/input/churn-modelling/Churn_Modelling.csv', usecols=range(3,14))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-26T18:38:44.331177Z","iopub.execute_input":"2021-07-26T18:38:44.331588Z","iopub.status.idle":"2021-07-26T18:38:44.520728Z","shell.execute_reply.started":"2021-07-26T18:38:44.331546Z","shell.execute_reply":"2021-07-26T18:38:44.519649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Process data\n\n#Replace countries with numbers\ncountries = df[\"Geography\"].unique()\ncountries_map = dict()\nfor i in range(len(countries)):\n    countries_map[countries[i]] = i\ndf[\"Geography\"] = df[\"Geography\"].replace(countries_map)\n\n#Replace gender with numbers\ngender = df[\"Gender\"].unique()\ngender_map = dict()\nfor i in range(len(gender)):\n    gender_map[gender[i]] = i\ndf[\"Gender\"] = df[\"Gender\"].replace(gender_map)\n\n#Split into test and train\ntest = df.head(int(df.shape[0] * 0.3))\ntrain = df.tail(int(df.shape[0] * 0.7))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T18:38:48.732912Z","iopub.execute_input":"2021-07-26T18:38:48.733268Z","iopub.status.idle":"2021-07-26T18:38:48.764466Z","shell.execute_reply.started":"2021-07-26T18:38:48.733238Z","shell.execute_reply":"2021-07-26T18:38:48.763653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking correlations to see if there are any especially strong features.\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set_theme()\nax = sns.heatmap(test.corr())","metadata":{"execution":{"iopub.status.busy":"2021-07-26T18:38:52.317363Z","iopub.execute_input":"2021-07-26T18:38:52.317916Z","iopub.status.idle":"2021-07-26T18:38:53.768201Z","shell.execute_reply.started":"2021-07-26T18:38:52.31787Z","shell.execute_reply":"2021-07-26T18:38:53.767062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No features are highly correlated with whether an employee exits the company. Thus all features are retained for now.","metadata":{}},{"cell_type":"markdown","source":"# Setting Up a Neural Network","metadata":{}},{"cell_type":"code","source":"#Currently, just randomly initialise parameters\nlayers = [10,100,25,1]\nweights = [np.random.randn(layers[l+1],layers[l]) for l in range(len(layers)-1)]\nbiases = [np.random.randn(layers[l],1) for l in range(1,len(layers))]\nreg_param = 0 #No regularisation right now\nbatch_size = 50\nepochs = 10\nstep_size = 0.01","metadata":{"execution":{"iopub.status.busy":"2021-07-26T19:24:19.312246Z","iopub.execute_input":"2021-07-26T19:24:19.31278Z","iopub.status.idle":"2021-07-26T19:24:19.319787Z","shell.execute_reply.started":"2021-07-26T19:24:19.312746Z","shell.execute_reply":"2021-07-26T19:24:19.318851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef sigmoid_derivative(z):\n    return sigmoid(z) * (1 - sigmoid(z))\n\ndef feedforward(X,weights,biases):\n    activations = [X.T]\n    weighted_inputs = []\n    \n    for l in range(len(biases)):\n        z = np.dot(weights[l],activations[l]) + biases[l]\n        weighted_inputs.append(z)\n        a = sigmoid(z)\n        activations.append(a)\n    \n    return activations,weighted_inputs\n\ndef cost_function(X,y,reg_param,weights,biases):\n    activations,weighted_inputs = feedforward(X,weights,biases)\n    m = y.size\n    \n    #Logistic cost function\n    cost = np.multiply(y.T, np.log(activations[-1])) + np.multiply((1-y).T, np.log(1-activations[-1]))    \n    cost = np.sum(cost) / -m\n    \n    #Regularisation\n    if (not reg_param == 0):\n        for l in range(len(weights)):\n            cost += reg_param / (2*m) * np.sum(weights[l] ** 2)   \n            \n    weights_gradient,biases_gradient = backpropagation(activations,weighted_inputs,weights,biases,y,m)\n    \n    return cost,weights_gradient,biases_gradient\n\ndef backpropagation(activations,weighted_inputs,weights,biases,y,m):\n    #Backpropagation - output layer\n    output_error = (activations[-1].reshape(y.size) - y.reshape(y.size))\n    errors = [output_error.reshape((1, output_error.size))] \n    \n    weights_gradient = [np.dot(errors[0], activations[-2].T) / m]\n    biases_gradient = [np.sum(errors[0], axis=1) / m]\n    \n    #Backpropagation - other layers (I DON'T UNDERSTAND THIS PART. I JUST ALIGNED MATRIX DIMENSIONS TO AVOID ERRORS)\n    for l in range(1, len(biases)):\n        #Weight layers selected = [2],[1]. Errors selected = [2],[1]. Weighted inputs selected = [1],[0]\n        current_error = np.dot(weights[len(weights) - l].T, errors[0]) * sigmoid_derivative(weighted_inputs[len(weighted_inputs) - l - 1])\n        errors.insert(0, current_error)\n    \n        biases_gradient.insert(0, np.sum(current_error, axis=1) / m) #Sum across examples\n        #Activation layers selected = [1], [0]\n        weight_grad = np.dot( current_error, activations[len(activations) - l - 2].T ) / m\n        weights_gradient.insert(0, weight_grad)\n\n    return weights_gradient,biases_gradient\n\ndef miniBatchGradientDescent(X,y,batch_size,epochs,step_size,reg_param,weights,biases):\n    cost_history = []\n    m = y.size\n    \n    for epoch in range(epochs):\n        print(f\"\\n Started Training Epoch: {epoch+1}\")\n        for i in range(0,m,batch_size):\n            print(\".\", end=\"\")\n            \n            #Get cost and gradients\n            last_example = i + batch_size\n            X_batch = X[i:last_example]\n            y_batch = y[i:last_example]\n            cost,weights_gradient,biases_gradient = cost_function(X_batch,y_batch,reg_param,weights,biases)\n\n            #Updates\n            cost_history.append(cost)\n            weights = [layer[0] - step_size * layer[1] for layer in zip(weights,weights_gradient)]\n            biases = [layer[0] - step_size * layer[1].reshape((layer[1].size, 1)) for layer in zip(biases,biases_gradient)]\n    \n    return cost_history,weights,biases","metadata":{"execution":{"iopub.status.busy":"2021-07-26T19:20:14.021545Z","iopub.execute_input":"2021-07-26T19:20:14.02199Z","iopub.status.idle":"2021-07-26T19:20:14.044924Z","shell.execute_reply.started":"2021-07-26T19:20:14.021952Z","shell.execute_reply":"2021-07-26T19:20:14.043584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now I'll actually train and test the model","metadata":{}},{"cell_type":"code","source":"X_train = train.loc[:, train.columns != 'Exited'].to_numpy()\ny_train = train[\"Exited\"].to_numpy()\ny_train = y_train.reshape((y_train.size,1))\ncost_history,weights,biases = miniBatchGradientDescent(X_train,y_train,batch_size,epochs,step_size,reg_param,weights,biases)\n\nimport matplotlib.pyplot as plt\naverage_over = 5\naveraged_cost = []\nfor i in range(0,len(cost_history),average_over):\n    avg = sum(cost_history[i:i+average_over]) / average_over\n    averaged_cost.append(avg)\n\nplt.plot(range(len(averaged_cost)), averaged_cost, 'r-')\nplt.xlabel(\"Num Iterations\")\nplt.ylabel(\"Cost\")","metadata":{"execution":{"iopub.status.busy":"2021-07-26T19:20:16.777121Z","iopub.execute_input":"2021-07-26T19:20:16.777552Z","iopub.status.idle":"2021-07-26T19:20:18.765092Z","shell.execute_reply.started":"2021-07-26T19:20:16.77751Z","shell.execute_reply":"2021-07-26T19:20:18.763607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'm seeing the same cost patterns repeat over the epochs. What I think that suggests is that the model isn't making good parameter updates on the overall dataset and optimising for local batches. \nIt could also be that the model is overfitting SEVERELY.\n\nSo I'm going to create a regular gradient descent algorithm instead of using mini-batches. And then, I'll create learning curves to check bias vs. variance.","metadata":{}},{"cell_type":"code","source":"def gradientDescent(X,y,step_size,max_iters,reg_param,weights,biases):\n    cost_history = []\n    m = y.size\n    \n    for i in range(max_iters):\n        print(\".\", end=\"\")\n        cost,weights_gradient,biases_gradient = cost_function(X,y,reg_param,weights,biases)\n\n        #Updates\n        cost_history.append(cost)\n        weights = [layer[0] - step_size * layer[1] for layer in zip(weights,weights_gradient)]\n        biases = [layer[0] - step_size * layer[1].reshape((layer[1].size, 1)) for layer in zip(biases,biases_gradient)]\n    \n    return cost_history,weights,biases","metadata":{"execution":{"iopub.status.busy":"2021-07-26T19:20:47.599066Z","iopub.execute_input":"2021-07-26T19:20:47.599511Z","iopub.status.idle":"2021-07-26T19:20:47.607295Z","shell.execute_reply.started":"2021-07-26T19:20:47.59947Z","shell.execute_reply":"2021-07-26T19:20:47.60627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Trying, gradient descent without mini-batches","metadata":{}},{"cell_type":"code","source":"X_train = train.loc[:, train.columns != 'Exited'].to_numpy()\ny_train = train[\"Exited\"].to_numpy()\ny_train = y_train.reshape((y_train.size,1))\nmax_iters = 100\n\ncost_history,weights,biases = gradientDescent(X_train,y_train,step_size,max_iters,reg_param,weights,biases)\n\nplt.plot(range(len(cost_history)), cost_history, 'r-')\nplt.xlabel(\"Num Iterations\")\nplt.ylabel(\"Cost\")","metadata":{"execution":{"iopub.status.busy":"2021-07-26T19:24:25.623884Z","iopub.execute_input":"2021-07-26T19:24:25.624401Z","iopub.status.idle":"2021-07-26T19:24:40.409384Z","shell.execute_reply.started":"2021-07-26T19:24:25.624368Z","shell.execute_reply":"2021-07-26T19:24:40.408304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is REALLY weird :O I've never seen a linear cost decrease???? What am I doing wrong???","metadata":{}},{"cell_type":"markdown","source":"# Evaluating Accuracy and Learning Curves","metadata":{}},{"cell_type":"code","source":"#Misclassification accuracy\nX_test = test.loc[:, test.columns != \"Exited\"].to_numpy()\ny_test = test[\"Exited\"].to_numpy()\n\nactivations,weighted_inputs = feedforward(X_test,weights,biases)\npred = activations[-1]\npred = pred.reshape((pred.size))\npred = [1 if x > 0.5 else 0 for x in pred]\n\ncorrect = [1 if x == y else 0 for x,y in zip(pred,y_test)]\naccuracy = sum(correct) / len(correct)\nprint(f\"The model is {accuracy * 100}% accurate\")","metadata":{"execution":{"iopub.status.busy":"2021-07-26T19:24:53.977028Z","iopub.execute_input":"2021-07-26T19:24:53.97764Z","iopub.status.idle":"2021-07-26T19:24:54.031284Z","shell.execute_reply.started":"2021-07-26T19:24:53.977581Z","shell.execute_reply":"2021-07-26T19:24:54.029678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Learning curves\nm_step = 50\nm_levels = range(0,1000,m_step)\ntrain_costs = []\ntest_costs = []\n\nfor i in m_levels:\n    X_batch_train = X_train[i:i+m_step]\n    y_batch_train = y_train[i:i+m_step]\n    X_batch_test = X_test[i:i+m_step]\n    y_batch_test = y_test[i:i+m_step]\n    \n    cost_history,weights,biases = \\\n        gradientDescent(X_batch_train,y_batch_train,step_size,max_iters,reg_param,weights,biases)\n    test_cost,_,_ = cost_function(X_batch_test,y_batch_test,reg_param,weights,biases)\n    \n    train_costs.append(cost_history[-1])\n    test_costs.append(test_cost)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T19:24:59.813589Z","iopub.execute_input":"2021-07-26T19:24:59.813937Z","iopub.status.idle":"2021-07-26T19:25:02.645182Z","shell.execute_reply.started":"2021-07-26T19:24:59.813908Z","shell.execute_reply":"2021-07-26T19:25:02.643385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(list(m_levels), train_costs, 'r-', label=\"train_costs\")\nplt.plot(list(m_levels), test_costs, 'b-', label=\"test_costs\")\nplt.xlabel(\"Num examples\")\nplt.ylabel(\"Cost\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-26T19:25:05.238655Z","iopub.execute_input":"2021-07-26T19:25:05.239055Z","iopub.status.idle":"2021-07-26T19:25:05.527863Z","shell.execute_reply.started":"2021-07-26T19:25:05.239022Z","shell.execute_reply":"2021-07-26T19:25:05.526832Z"},"trusted":true},"execution_count":null,"outputs":[]}]}