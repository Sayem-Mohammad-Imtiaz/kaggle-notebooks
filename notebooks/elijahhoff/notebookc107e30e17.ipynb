{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n#os.chdir('') #your directory\nprint(os.getcwd())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#EDA\nimport pandas as pd\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#Модели\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier \nfrom sklearn.linear_model import LogisticRegression\n\n#Всопомгательные пакеты\nfrom category_encoders.cat_boost import CatBoostEncoder\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GridSearchCV, validation_curve\nfrom sklearn.metrics import roc_auc_score, plot_confusion_matrix, \\\nplot_roc_curve, plot_precision_recall_curve, classification_report, precision_score, recall_score\n\n\n#Кластеризация\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import NearestNeighbors\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/bank-customer-churn-modeling/Churn_Modelling.csv')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Базовые описательные статистики","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.rename(columns = {'Exited':'target'}, inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Пропусков нет, это хорошо =)","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Количество уникальных значений для каждой колонки","metadata":{}},{"cell_type":"code","source":"df.nunique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Выбосим неинформативные, на первый взгляд признаки изи датафрейма","metadata":{}},{"cell_type":"code","source":"df = df.drop(['RowNumber', 'CustomerId'], axis = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим на процентное соотношение классов целевой переменной","metadata":{}},{"cell_type":"code","source":"ax = sns.countplot(y=\"target\", data=df, alpha=0.8)\ntotal = df.shape[0]\n\nfor p in ax.patches:\n    percentage = '{:.1f}%'.format(100 * p.get_width() / total)\n    x = p.get_x() + p.get_width()\n    y = p.get_y() + p.get_height() / 2\n    ax.annotate(percentage, (x, y))\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Дисбаланс классов вообще понятие эмперическое, в нашем случае можно в ходе настройки алгоритмов попробовать техники over и under sampling, а так же настройки с весами классов в самих алгоритмах","metadata":{}},{"cell_type":"markdown","source":"Сделаем мапинг нашей переменной Gender, с помощью двоичной кодировки","metadata":{}},{"cell_type":"code","source":"df['Gender'] = df['Gender'].map({'Female': 0, 'Male': 1})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Сделаем OHE преобразование для переменной Geography, дамми-ловушка нас не пугает ибо будем пользоватся нелиейным алгоритмом (в итоге, скорее всего), поэтому не удаляем никакую из колонок","metadata":{}},{"cell_type":"code","source":"df = pd.concat([df, pd.get_dummies(df['Geography'])], axis = 1)\ndf.drop(columns = ['Geography', 'Surname'], inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df #посмотреть на выбросы Balance и CreditScore и EstimatedSalary","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#с выбросами не успел повозиться, можно по z score или по IQR их детектить, как вариант\n#sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(df[[\"Balance\", \"CreditScore\"]]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.rcParams[\"figure.figsize\"] = (15,10)\n# fig, ax = plt.subplots(2)\n# ax[0].boxplot(df['CreditScore'])\n# ax[1].boxplot(df['Balance'])\n# ax[0].grid()\n# ax[1].grid()\n# fig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Рассмотрим гипотезу о том что те, у кого на балансе 0 и при этом есть кредитные карты более склонны к дефолту","metadata":{}},{"cell_type":"code","source":"print(f\"Число клиентов с нулевым балансом и кредитками - {len(df.query('Balance == 0 & HasCrCard != 0'))}\")\ndisplay(df.query(\"Balance == 0 & HasCrCard != 0\").head())\nprint()\nprint(f\"Число клиентов с ненулевым балансом и отсутствием кредиткок - {len(df.query('Balance != 0 & HasCrCard == 0'))}\")\ndisplay(df.query(\"Balance != 0 & HasCrCard == 0\").head())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Как мы видим у нас примерно поровну тех у кого нет денег на счёте и есть кредитка и тех у кого есть деньги на счёте но нет кредитки. Тут важный момент состоит в том что данные могли быть взяты в момент когда клиент решил, например снять деньги для личных целей, но при этом вполне себе хорошо зарабатывает, посмтортим сколько из первой категории людей не имея денег на счёте и имея кредитки при этом зарабатывают ниже среднего значения по выборке","metadata":{}},{"cell_type":"markdown","source":"p.s. IsActiveMember перменая имеет странное описание на Kaggle - #Subjective, but for the concept\n","metadata":{}},{"cell_type":"code","source":"print(f\"Число клиентов с нулевым балансом, кредитками и зарплатой ниже среднего - {len(df.query('Balance == 0 & HasCrCard != 0 & EstimatedSalary < EstimatedSalary.mean()'))}\")\ndisplay(df.query('Balance == 0 & HasCrCard != 0 & EstimatedSalary < EstimatedSalary.mean()').head())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Почему мы берем среднее в качестве порога отсечения для нашей новой фичи, причина проста - наша зарплата имеет равномерное распредение, а у него в асимптотике средние совпадает с медианой","metadata":{}},{"cell_type":"code","source":"plt.title('EstimatedSalary', fontsize = 20)\nplt.hist(df['EstimatedSalary'])\nplt.xticks(rotation = 45)\nplt.grid();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Преобразуем наши данные применив StandardScaler для последующей кластеризации с помощью K-means, попутно развлечения ради посмотрим на аппрокисмацию на двумерное пространство нашего датафрейма с помощью TSNE","metadata":{}},{"cell_type":"code","source":"cols = df.columns\ndf_sc = df.copy().values \nscaler = sklearn.preprocessing.StandardScaler()\n\ndf_sc = scaler.fit_transform(df_sc)\ndf_sc = pd.DataFrame(df_sc)\ndf_sc.columns = cols\ndf_sc","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=0)\ndigits_2d_tsne = tsne.fit_transform(df_sc.sample(len(df_sc), random_state = 0))\nplt.figure(figsize=(10, 8))\nplt.title('Двумерное представление нормированных данных', fontsize = 20)\nplt.scatter(digits_2d_tsne[:, 0], digits_2d_tsne[:, 1], c = df_sc['target'].sample(len(df_sc), random_state = 0))\nplt.colorbar()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Кластеризовать будем опираясь на метод \"локтя\" по метрике убывания суммы расстояний объектов кластеров от их центра","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\ndistortions = [] \ninertias = [] \nmapping1 = {} \nmapping2 = {} \nK = range(1,10) \nX = df_sc \nfor k in K: \n    kmeanModel = KMeans(n_clusters=k, random_state = 1).fit(X) \n    kmeanModel.fit(X)     \n      \n    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, \n                      'euclidean'),axis=1)) / X.shape[0]) \n    inertias.append(kmeanModel.inertia_) \n  \n    mapping1[k] = sum(np.min(cdist(X, kmeanModel.cluster_centers_, \n                 'euclidean'),axis=1)) / X.shape[0] \n    mapping2[k] = kmeanModel.inertia_\n    \nfor key,val in mapping1.items(): \n    print(str(key)+' : '+str(val))\n    \nplt.plot(K, distortions, 'bx-') \nplt.xlabel('Количество кластеров K') \nplt.ylabel('Distortion') \nplt.title('Метод локтя с помощью Distortion') \nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for key,val in mapping2.items(): \n    print(str(key)+' : '+str(val))\n    \nplt.plot(K, inertias, 'bx-') \nplt.xlabel('Количество кластеров K') \nplt.ylabel('Distortion') \nplt.title('Метод локтя с помощью Interia') \nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Как мы видим оба метода для \"локтевой прикидки\" (Interia и Distortion) дают примерно одинаковый ответ - 3 кластера наиболее опитимальное разбиение. ","metadata":{}},{"cell_type":"code","source":"kmeanModel = KMeans(n_clusters=3, random_state=0).fit(X) \ndf['Cluster'] = kmeanModel.fit_predict(X)\ndf['Cluster'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Отлично, у нас примерно поровну разделились наблюдения, серьёзного перекоса в пользу одного из кластеров не случилось (как часто бывает, например с DBSCAN'ом)","metadata":{}},{"cell_type":"markdown","source":"Получим таким образом почти готовый для обучения модели датафрейм, добавив в OHE кодировке с префиксом 'Cluster' наши переменные, полученные после кластеризации","metadata":{}},{"cell_type":"code","source":"df = pd.concat([df, pd.get_dummies(df['Cluster'], prefix = 'Cluster')], axis = 1)\ndf.drop(columns = ['Cluster'], inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Создадим новую перменную в которой будут клиенты с нулевым балансом, наличием кредитных карт и зарплатой ниже среднего по выборке (название новой переменной - four eyes priniciple aka FEP)","metadata":{}},{"cell_type":"code","source":"df['FEP'] = np.where( ( (df['Balance'] == 0) & \\\n                       (df['EstimatedSalary'] < df['EstimatedSalary'].mean()) & \\\n                        df['HasCrCard'] > 0), 1, 0 )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Разобъём выборку в пропорции 80/20","metadata":{}},{"cell_type":"code","source":"X = df[[i for i in df.columns if i != 'target']]\ny = df['target']\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Простая функция для записи roc_auc в датафрейм","metadata":{}},{"cell_type":"code","source":"def roc_auc_df (model, y_test, y_pred):\n    global res\n    res = res.append(pd.DataFrame([np.round(roc_auc_score(y_test, y_pred), 2)], columns = ['ROC_AUC'], index = [model]))\n    return res","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Переобъявим df_sc с новыми регрессорами чтобы обучить логистическую регрессию, для этого нужно будет удалить пару колонок из-за дамми-ловушки (я брал первые из категорий по порядку 'France' и 'Cluster_0')","metadata":{}},{"cell_type":"code","source":"cols = df.columns\ndf_sc = df.copy().values \nscaler = sklearn.preprocessing.StandardScaler()\n\ndf_sc = scaler.fit_transform(df_sc)\ndf_sc = pd.DataFrame(df_sc)\ndf_sc.columns = cols\ndf_sc","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Обучим логистическую регрессию на нормированных признаках","metadata":{}},{"cell_type":"code","source":"X_sc = df_sc.drop(['target', 'France', 'Cluster_0'], axis=1) \ny_sc = df_sc['target']\n\nX_train_sc, X_test_sc, y_train_sc, y_test_sc = train_test_split(X_sc, y, test_size=0.2, random_state=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train_sc, y_train_sc)\ny_pred = lr.predict(X_test_sc)\n\nprint(classification_report(y_test_sc, y_pred))\nprint(f'LR_sc auc is {np.round(roc_auc_score(y_test, y_pred), 2)}')\n\nres = pd.DataFrame([np.round(roc_auc_score(y_test, y_pred), 2)], columns = ['ROC_AUC'], index = ['LR'])\nres","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n\nplot_precision_recall_curve(lr, X_test_sc, y_test, ax=ax1)\nplot_roc_curve(lr, X_test_sc, y_test, ax=ax2);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Как мы видим accuracy выше чем соотношение классов 0/1 во всей выборке, то есть наша модель уже точнее (хоть и не сильно) простого констаного прогноза, но F1 для меньшего класса предсказуемо мала, будем ориентироватся не только на roc_auc но и на F1 для класса 1, так как нам бы хотелось иметь наиболее корректную модель, не \"испорченную\" дисбалансом классов","metadata":{}},{"cell_type":"markdown","source":"Опробуем случайный лес с параметром class_weight","metadata":{}},{"cell_type":"code","source":"RF = RandomForestClassifier(random_state=1, class_weight='balanced')\nRF.fit(X_train, y_train)\ny_pred = RF.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint(f'RF roc_auc_score is {np.round(roc_auc_score(y_test, y_pred), 2)}') \nf1('RF',y_test,y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n\nplot_precision_recall_curve(RF, X_test, y_test, ax=ax1)\nplot_roc_curve(RF, X_test, y_test, ax=ax2);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"XGB = XGBClassifier(random_state = 1)\nXGB.fit(X_train, y_train)\ny_pred = XGB.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint(f'XGB roc_auc_score is {np.round(roc_auc_score(y_test, y_pred), 2)}')\nf1('XGB',y_test,y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n\nplot_precision_recall_curve(XGB, X_test, y_test, ax=ax1)\nplot_roc_curve(XGB, X_test, y_test, ax=ax2);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LGBM = LGBMClassifier(class_weight='balanced', random_state = 1)\nLGBM.fit(X_train, y_train)\ny_pred = LGBM.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint(f'LGBM roc_auc_score is {np.round(roc_auc_score(y_test, y_pred), 2)}')\nf1('LGBM',y_test,y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n\nplot_precision_recall_curve(LGBM, X_test, y_test, ax=ax1)\nplot_roc_curve(LGBM, X_test, y_test, ax=ax2);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg_cb = CatBoostClassifier(loss_function='Logloss', random_state = 1)\n\nreg_cb.fit(X_train, y_train, verbose = False)\n\ny_pred = reg_cb.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint(f'CB roc_auc_score is {np.round(roc_auc_score(y_test, y_pred), 2)}')\nf1('CB',y_test,y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n\nplot_precision_recall_curve(reg_cb, X_test, y_test, ax=ax1)\nplot_roc_curve(reg_cb, X_test, y_test, ax=ax2);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Итак, мы опробовали 4 алгоритма без настройки, один из них линейный (LR), мы можем перебрать все алгоритмы классификации, но в целях экономии времени, остановимся на этих четырёх","metadata":{}},{"cell_type":"markdown","source":"Попробуем процедуру under sampling'а с помощью TomekLinks и RandomOverSampling'a для всех вышеуказанных алгоритмов","metadata":{}},{"cell_type":"code","source":"from imblearn.under_sampling import TomekLinks\n\nTL = TomekLinks()\nX_train_tl, y_train_tl = TL.fit_resample(X_train, y_train)\n\noversample = RandomOverSampler(sampling_strategy=1.0)\nX_over, y_over = oversample.fit_resample(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RandomForest c undersampling","metadata":{}},{"cell_type":"code","source":"reg = RandomForestClassifier(random_state=1, class_weight='balanced')\nreg.fit(X_train_tl, y_train_tl)\ny_pred = reg.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint(f'RF TL roc_auc_score is {np.round(roc_auc_score(y_test, y_pred), 2)}')\nf1('RF TL',y_test,y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n\nplot_precision_recall_curve(reg, X_test, y_test, ax=ax1)\nplot_roc_curve(reg, X_test, y_test, ax=ax2);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RadnomForest c RandomOS","metadata":{}},{"cell_type":"code","source":"reg = RandomForestClassifier(random_state=1, class_weight='balanced')\nreg.fit(X_over, y_over)\ny_pred = reg.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint(f'RF OS roc_auc_score is {np.round(roc_auc_score(y_test, y_pred), 2)}')\nf1('RF OS',y_test,y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n\nplot_precision_recall_curve(reg, X_test, y_test, ax=ax1)\nplot_roc_curve(reg, X_test, y_test, ax=ax2);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"XGB c undersampling","metadata":{}},{"cell_type":"code","source":"XGB_tl = XGBClassifier(random_state = 1)\nXGB_tl.fit(X_train_tl, y_train_tl)\ny_pred = XGB_tl.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint(f'XGB TL roc_auc_score is {np.round(roc_auc_score(y_test, y_pred), 2)}')\nf1('XGB TL',y_test,y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n\nplot_precision_recall_curve(XGB_tl, X_test, y_test, ax=ax1)\nplot_roc_curve(XGB_tl, X_test, y_test, ax=ax2);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"XGB c RandomOS","metadata":{}},{"cell_type":"code","source":"XGB = XGBClassifier(random_state = 1)\nXGB.fit(X_over, y_over)\ny_pred = XGB.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint(f'XGB OS roc_auc_score is {np.round(roc_auc_score(y_test, y_pred), 2)}')\nf1('XGB OS',y_test,y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n\nplot_precision_recall_curve(XGB, X_test, y_test, ax=ax1)\nplot_roc_curve(XGB, X_test, y_test, ax=ax2);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LGBM c undersampling","metadata":{}},{"cell_type":"code","source":"LGBM = LGBMClassifier(class_weight='balanced', random_state = 1)\nLGBM.fit(X_train_tl, y_train_tl)\ny_pred = LGBM.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint(f'LGBM TL roc_auc_score is {np.round(roc_auc_score(y_test, y_pred), 2)}')\nf1('LGBM TL',y_test,y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n\nplot_precision_recall_curve(LGBM, X_test, y_test, ax=ax1)\nplot_roc_curve(LGBM, X_test, y_test, ax=ax2);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LGBM с RandomOS","metadata":{}},{"cell_type":"code","source":"LGBM_OS = LGBMClassifier(class_weight='balanced', random_state = 1)\nLGBM_OS.fit(X_over, y_over)\ny_pred = LGBM_OS.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint(f'LGBM OS roc_auc_score is {np.round(roc_auc_score(y_test, y_pred), 2)}')\nf1('LGBM OS',y_test,y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n\nplot_precision_recall_curve(LGBM, X_test, y_test, ax=ax1)\nplot_roc_curve(LGBM, X_test, y_test, ax=ax2);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cat_Boost c undersampling","metadata":{}},{"cell_type":"code","source":"reg_cb = CatBoostClassifier(loss_function='Logloss', random_state = 1)\n\nreg_cb.fit(X_train_tl, y_train_tl, verbose = False)\n\ny_pred = reg_cb.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint(f'CB_TL roc_auc_score is {np.round(roc_auc_score(y_test, y_pred), 2)}')\nf1('CB_TL',y_test,y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n\nplot_precision_recall_curve(reg_cb, X_test, y_test, ax=ax1)\nplot_roc_curve(reg_cb, X_test, y_test, ax=ax2);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CatBoost c RandomOS","metadata":{}},{"cell_type":"code","source":"reg_cb = CatBoostClassifier(loss_function='Logloss', random_state = 1)\n\nreg_cb.fit(X_over, y_over, verbose = False)\n\ny_pred = reg_cb.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint(f'CB_OS roc_auc_score is {np.round(roc_auc_score(y_test, y_pred), 2)}')\nf1('CB_OS',y_test,y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n\nplot_precision_recall_curve(reg_cb, X_test, y_test, ax=ax1)\nplot_roc_curve(reg_cb, X_test, y_test, ax=ax2);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Так как мы имеем дело с дисбалансом классов, мы хотим иметь в каждом фолде на кросс-валидации нужную нам пропорцию классов для этого используем стратегию кросс-валидации StratifiedShuffleSplit на 5 фолдах","metadata":{}},{"cell_type":"code","source":"# %%time\n# lgbm = LGBMClassifier(class_weight='balanced', random_state = 1)\n\n# skf = StratifiedShuffleSplit(n_splits = 5, random_state = 1)\n\n# params = {'learning_rate': np.linspace(0.05, 0.5, 10),\n#          'num_leaves':range(10, 100),\n#          'n_estimators' : range(250, 500, 50),\n#          'reg_alpha' : np.linspace(0, 10, 10),\n#          'reg_lambda': np.linspace(0, 10, 10)}\n\n\n# clf_lgbm = GridSearchCV(lgbm, params, scoring = 'roc_auc', cv = skf, verbose = True, n_jobs = -1)\n\n# clf_lgbm.fit(X_over, y_over)\n\n# best_params = clf_lgbm.best_estimator_.get_params()\n# print('Best score: ', clf_lgbm.best_score_)\n# print('Best params: ', best_params)\n\n# print(classification_report(y_test, y_pred))\n# print(f'LGBM_GS roc_auc_score is {np.round(roc_auc_score(y_test, y_pred), 2)}')\n# f1('LGBM_GS',y_test,y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n\n# plot_precision_recall_curve(clf_lgbm, X_test, y_test, ax=ax1)\n# plot_roc_curve(clf_lgbm, X_test, y_test, ax=ax2);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\nfit_params={\"early_stopping_rounds\":30, \n            \"eval_metric\" : 'auc', \n            \"eval_set\" : [(X_test,y_test)],\n            'eval_names': ['valid'],\n            'verbose': 10,\n            'categorical_feature': 'auto'}\n\nparam_test ={'num_leaves': sp_randint(6, 50), \n             'min_child_samples': sp_randint(100, 500), \n             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': sp_uniform(loc=0.2, scale=0.8), \n             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nskf = StratifiedShuffleSplit(n_splits = 5, random_state = 1)\n\nclf = lgb.LGBMClassifier(max_depth=-1, random_state=1, silent=True, metric='None', n_jobs=-1, n_estimators=5000)\ngs = RandomizedSearchCV(\n    estimator=clf, param_distributions=param_test, \n    n_iter=300,\n    scoring='roc_auc',\n    cv=skf,\n    refit=True,\n    random_state=1,\n    verbose=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gs.fit(X_over, y_over, **fit_params)\nprint('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc_auc_score(y_test, gs.predict(X_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### grid search работает очень долго, а RandomizedSearch с нашими параметрами не дал более высокого качества","metadata":{}},{"cell_type":"markdown","source":"Наш лучший результат по метрике roc_auc обусловлен использованием LGBM при оверсэмплинге, мы можем посомтреть какие из регрессоров были наиболее \"важными\" для нашей модели с помощью разных методов.","metadata":{}},{"cell_type":"markdown","source":"1) Shap - теоретико-игровой подход под капотом","metadata":{}},{"cell_type":"code","source":"import shap\nexplainer = shap.TreeExplainer(LGBM_OS)\nshap_vals = explainer.shap_values(X_test, y_test)\n\nshap.summary_plot(shap_values=shap_vals,\n                features=X_test, plot_type='bar')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2) Обычный permutation_importance тест из sklearn","metadata":{}},{"cell_type":"code","source":"from sklearn.inspection import permutation_importance\nresult = permutation_importance(LGBM_OS, X_test, y_test, n_repeats=5, random_state=1, scoring='roc_auc')\npd.DataFrame({'Permutation_importance':result.importances_mean,\\\n              'Feature_name':X.columns}).\\\n            sort_values(by=\"Permutation_importance\", ascending=False).head(15)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3) Стандартный метод оценки важности регрессоров, встроенный в LGBM (по сути, по оси x - количество сплитов в деревьях по каждой из фичей)","metadata":{}},{"cell_type":"code","source":"feature_imp = pd.DataFrame(sorted(zip(LGBM_OS.feature_importances_,X.columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features (avg over folds)', fontsize = 20)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Лучший сабмит что я видел по accuracy и F1 для классов 0 и 1 - https://www.kaggle.com/affanamin/handling-imbalanced-data-in-deeplearning, задача была побить его не используя DL (хотя там TF)","metadata":{}},{"cell_type":"markdown","source":"По поводу предобработки фичей, к сожалению кластеризация нам не пригодилась\n##### то же самое по поводу кластеризации прямо подтверждает XGBOOST \n","metadata":{}},{"cell_type":"code","source":"_ = xgb.plot_importance(XGB, height=0.7, grid=True, max_num_features = None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Вводы","metadata":{},"execution_count":null,"outputs":[]}]}