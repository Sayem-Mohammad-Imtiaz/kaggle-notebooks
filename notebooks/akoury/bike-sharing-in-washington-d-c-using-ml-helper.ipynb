{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bike Sharing in Washington D.C.\n\nTwo datasets from [Bike Sharing in Washington D.C.](https://www.kaggle.com/marklvl/bike-sharing-dataset/home) containing information about the Bike Sharing service in Washington D.C. \"Capital Bikeshare\" are provided.\n\nOne dataset contains hourly data and the other one has daily data from the years 2011 and 2012.\n\nThe following variables are included in the data:\n\n* instant: Record index\n* dteday: Date\n* season: Season (1:springer, 2:summer, 3:fall, 4:winter)\n* yr: Year (0: 2011, 1:2012)\n* mnth: Month (1 to 12)\n* hr: Hour (0 to 23, only available in the hourly dataset)\n* holiday: whether day is holiday or not (extracted from Holiday Schedule)\n* weekday: Day of the week\n* workingday: If day is neither weekend nor holiday is 1, otherwise is 0.\n* weathersit: (extracted from Freemeteo)\n    1: Clear, Few clouds, Partly cloudy, Partly cloudy\n    2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n    3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n    4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n* temp: Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)\n* atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)\n* hum: Normalized humidity. The values are divided to 100 (max)\n* windspeed: Normalized wind speed. The values are divided to 67 (max)\n* casual: count of casual users\n* registered: count of registered users\n* cnt: count of total rental bikes including both casual and registered (Our target variable)\n\nWe will build a predictive model that can determine how many people will use the service on an hourly basis. We will use the first 5 quarters of the data for our training dataset and the last quarter of 2012 will be the holdout against which we perform our validation. Since that data was not used for training, we are sure that the evaluation metric that we get for it (R2 score) is an objective measurement of its predictive power.\n\n### Outline\n\nWe separate the project in 3 steps:\n\nData Loading and Exploratory Data Analysis: Load the data and analyze it to obtain an accurate picture of it, its features, its values (and whether they are incomplete or wrong), its data types among others. Also, the creation of different types of plots in order to help us understand the data and make the model creation easier.\n\nFeature Engineering / Modeling and Pipeline: Once we have the data, we create some features and then the modeling stage begins, making use of different models (and ensembles) and a strong pipeline with different transformers, we will hopefully produce a model that fits our expectations of performance. Once we have that model, a process of tuning it to the training data would be performed.\n\nResults and Conclusions: Finally, with our tuned model, we  predict against the test set we decided to separate initially, then we review those results against their actual values to determine the performance of the model, and finally, outlining our conclusions.\n\n### Helpers\n\n**To run this code you must install my package called ml-helper**, which is a set of helpers to speed up the the machine learning process and provide a formal structure for it. These helpers are the basis for my package ML-Helper and they can be used in your own projects by downloading the package at [Pypi](https://pypi.org/project/ml-helper/) ```(pip install ml-helper)```.\n\nIf you wish to see a working example and explanation of what the package does, without using the package directly, please see my [kernel \"Employee Attrition\"](https://www.kaggle.com/akoury/employee-attrition-helpers-to-speed-up-ml-process/) or take a look at the code [at my GitHub](https://github.com/akoury/ml-helper)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ml_helper","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tempfile import mkdtemp\nfrom sklearn.base import clone\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBRegressor\nfrom sklearn.cluster import KMeans\nfrom ml_helper.helper import Helper\nfrom imblearn import FunctionSampler\nfrom imblearn.combine import SMOTEENN\nfrom sklearn.decomposition import PCA\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.compose import ColumnTransformer\nfrom gplearn.genetic import SymbolicTransformer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score as metric_scorer\nfrom sklearn.feature_selection import RFE, SelectFromModel\nfrom sklearn.preprocessing import PolynomialFeatures, KBinsDiscretizer, PowerTransformer, OneHotEncoder, FunctionTransformer\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Setting Key Values\n\nThe following values are used throught the code, this cell gives a central source where they can be managed. We also create a helper object from the package ML-Helper while passing along these keys, they will help the package run a few functions under the hood."},{"metadata":{"trusted":true},"cell_type":"code","source":"MEMORY = mkdtemp()\n\nKEYS = {\n    'SEED': 1,\n    'DATA_H': '../input/bike-sharing-dataset/hour.csv',\n    'DATA_D' : '../input/bike-sharing-dataset/day.csv',\n    'DATA_P': 'https://gist.githubusercontent.com/akoury/6fb1897e44aec81cced8843b920bad78/raw/b1161d2c8989d013d6812b224f028587a327c86d/precipitation.csv',\n    'TARGET': 'cnt',\n    'METRIC': 'r2',\n    'TIMESERIES': True,\n    'SPLITS': 3,\n    'ESTIMATORS': 150,\n    'MEMORY': MEMORY\n}\n\nhp = Helper(KEYS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Loading\n\nHere we load the necessary data, print its first rows and describe its contents."},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data(input_path):\n    return pd.read_csv(input_path, parse_dates=[1])\n\ndata = read_data(KEYS['DATA_H'])\ndata_daily = read_data(KEYS['DATA_D'])\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Precipitation Data\n\nIn order to generate our model, we will add precipitation data obtained from the [National Climatic Data Center.](https://www.ncdc.noaa.gov/cdo-web/datasets)\n\nHowever, since most of the values are 0, we will convert them to a boolean that determines if rain was present or not at that specific hour."},{"metadata":{"trusted":true},"cell_type":"code","source":"precipitation = read_data(KEYS['DATA_P'])\ndata = pd.merge(data, precipitation,  how='left', on=['dteday','hr'])\ndata['precipitation'].fillna(0, inplace=True)\ndata['precipitation'][data['precipitation'] > 0] = 1\ndata['precipitation'] = data['precipitation'].astype(int).astype('category')\n\ndata_hourly = data.copy()\ndata_hourly = data_hourly[data_hourly['dteday'].isin(pd.date_range('2011-01-01','2012-09-30'))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data types\n\nWe review the data types for each column."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing Data\n\nWe check if there is any missing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"hp.missing_data(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Converting columns to their true categorical type\nNow we convert the data types of numerical columns that are actually categorical."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = hp.convert_to_category(data, data.iloc[:,2:10])\n\ndata.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis\n\nHere we will perform all of the necessary data analysis, with different plots that will help us understand the data and therefore, create a better model.\n\nWe must specify that all of this analysis is performed only on the training data, so that we do not incur in any sort of bias when modeling.\n\nTo start we define some color palettes to be used."},{"metadata":{"trusted":true},"cell_type":"code","source":"palette_tot_cas_reg = ['darkgreen', 'darkred', 'darkblue']\n\npalette_cas = ['darkred', 'salmon']\npalette_reg = ['darkblue', 'skyblue']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Overall distribution of the target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 8))\nsns.distplot(data['cnt'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Usage over time, by type of user and total"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_daily = data_daily[data_daily['dteday'].isin(pd.date_range('2011-01-01','2012-09-30'))]\ndata_daily = hp.convert_to_category(data_daily, data_daily.iloc[:,2:9])\ndata_daily.set_index('dteday')\n\nplt.figure(figsize=(16, 5))\n\nax = sns.lineplot(data = data_daily, x = 'dteday', y = 'cnt', color='darkgreen', size = 1,label = 'count')\nax = sns.lineplot(data = data_daily, x = 'dteday', y = 'casual', color='darkred', size = 1, label = 'casual')\nax = sns.lineplot(data = data_daily, x = 'dteday', y = 'registered', color='darkblue', size = 1, label = 'registered')\n\nhandles, labels = ax.get_legend_handles_labels()\nl = plt.legend(handles[0:1]+handles[3:4]+handles[6:7], labels[0:1]+labels[3:4]+labels[6:7], loc=2)\nplt.xlabel('Date')\nplt.ylabel('Users')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"They all seem to be increasing and have some seasonality."},{"metadata":{},"cell_type":"markdown","source":"### Monthly average usage"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_month = pd.DataFrame(data_daily.groupby(\"mnth\")[[\"cnt\", 'casual', 'registered']].mean()).reset_index()\nmonths = pd.Series([\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]).rename(\"months\")\ndf_month = pd.concat([df_month, months], axis = 1)\n\n\nplt.figure(figsize=(12, 5))\nax = sns.pointplot(data = df_month, x = \"months\", y = \"cnt\", color = 'darkgreen')\nax = sns.pointplot(data = df_month, x = \"months\", y = \"casual\", color = 'darkred')\nax = sns.pointplot(data = df_month, x = \"months\", y = \"registered\", color = 'darkblue')\n\nplt.xlabel('')\nplt.ylabel('Users')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No considerable differences in trends between casual and registered.\n\n### Weekly trend usage"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_week = pd.DataFrame(data_daily.groupby(\"weekday\")[[\"cnt\", 'casual', 'registered']].mean()).reset_index()\ndf_week = pd.melt(df_week, id_vars = ['weekday'], value_vars = ['cnt', 'casual', 'registered'], var_name = 'type', value_name = 'users')\n\nplt.figure(figsize=(12, 5))\nax = sns.lineplot(data = df_week, x = \"weekday\", y = \"users\", hue = \"type\", palette = palette_tot_cas_reg)\nplt.xlabel('Weekday')\nplt.ylabel('Users')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Casual and registered users follow exactly opposed trends throughout the week.\n\n### Daily trend\n\nBy type of users:"},{"metadata":{"trusted":false},"cell_type":"code","source":"data_hourly = hp.convert_to_category(data_hourly, data_hourly.iloc[:,2:9])\ndata_hourly.set_index('dteday')\n\ndf_day = pd.DataFrame(data_hourly.groupby(\"hr\")[[\"cnt\", 'casual', 'registered']].mean()).reset_index()\ndf_day = pd.melt(df_day, id_vars = ['hr'], value_vars = ['cnt', 'casual', 'registered'], var_name = 'type', value_name = 'users')\n\nplt.figure(figsize=(12, 5))\nsns.lineplot(data = df_day, x = \"hr\", y = \"users\", hue = \"type\", palette = palette_tot_cas_reg)\n\nplt.xlabel('Hour')\nplt.ylabel('Users')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Weekends compared with working days"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nsns.lineplot(data = data_hourly, x = \"hr\", y = \"casual\", hue = 'workingday', palette = palette_cas)\nsns.lineplot(data = data_hourly, x = \"hr\", y = \"registered\", hue = 'workingday', palette = palette_reg)\nplt.xlabel('Hour')\nplt.ylabel('Users')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Temperature effect on casual users"},{"metadata":{"trusted":false},"cell_type":"code","source":"atemp_binned = pd.cut(x = data_hourly['atemp'], bins = 4).rename('atemp_binned')\ndata_hourly_binned = pd.concat([data_hourly, atemp_binned], axis = 1)\n\ndf_day_by_day_atemp = pd.DataFrame(data_hourly_binned.groupby([\"hr\", \"atemp_binned\"])[[\"cnt\", 'casual', 'registered']].mean()).reset_index()\ndf_day_by_day_atemp.head()\n\nplt.figure(figsize=(12, 5))\nsns.lineplot(data = df_day_by_day_atemp, x = 'hr', y = 'casual', hue = 'atemp_binned', palette = 'husl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Temperature effect on registered users"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nsns.lineplot(data = df_day_by_day_atemp, x = 'hr', y = 'registered', hue = 'atemp_binned', palette = 'husl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, the effect is much more pronounced on the casual users."},{"metadata":{},"cell_type":"markdown","source":"### Temperature Vs. Usage on working days for casual users"},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.scatterplot(data = data_daily, x = 'atemp', y = 'casual', hue = 'workingday', alpha = .3)\nplt.title('Casual Users')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Temperature Vs. Usage on working days for registered users"},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.scatterplot(data = data_daily, x = 'atemp', y = 'registered', hue = 'workingday', alpha = .3)\nplt.title('Registered Users')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, temperature does affect casual user usage, but registered users do not seem to mind the temperature."},{"metadata":{},"cell_type":"markdown","source":"### Precipitation Vs. Usage throughout the day"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nax = sns.lineplot(data = data_hourly, x = \"hr\", y = \"casual\", hue = 'precipitation', palette = palette_cas, label = 'casual')\n\nax = sns.lineplot(data = data_hourly, x = \"hr\", y = \"registered\", hue = 'precipitation', palette = palette_reg, label = 'registered')\n\nhandles, labels = ax.get_legend_handles_labels()\nl = plt.legend(handles[0:2]+handles[5:7], labels[0:2]+labels[5:7], loc=2)\nplt.xlabel('Hour')\nplt.ylabel('Users')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Boxplot of Numerical Variables\n\nWe review the distribution of scaled numerical data through a boxplot for each variable."},{"metadata":{"trusted":false},"cell_type":"code","source":"hp.boxplot(data, ['instant'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this we know that we should try some outlier treatment"},{"metadata":{},"cell_type":"markdown","source":"### Coefficient of Variation\n\nThe coefficient of variation is a dimensionless meassure of dispersion in data, the lower the value the less dispersion a feature has. We will select columns that have a variance of less than 0.05 since they would probably perform poorly."},{"metadata":{"trusted":true},"cell_type":"code","source":"invariant = hp.coefficient_variation(data, threshold = 0.05, exclude = ['instant'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline\n\nA basic linear model is created in order to set a baseline, further models will be compared against its results."},{"metadata":{"trusted":false},"cell_type":"code","source":"base_holdout = data[data['dteday'].isin(pd.date_range('2012-10-01','2012-12-31'))].copy()\nbase_holdout = hp.drop_columns(base_holdout, ['dteday', 'casual', 'registered'])\nbase_data = data[data['dteday'].isin(pd.date_range('2011-01-01','2012-09-30'))].copy()\nbase_data = hp.drop_columns(data, ['dteday', 'casual', 'registered'])\n\ny, predictions = hp.predict(base_data, base_holdout, LinearRegression())\nbase_score = metric_scorer(y, predictions)\nprint('Baseline score: ' + str(base_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Correlation\n\nNow we will analyze correlation in the data for both numerical and categorical columns and plot them, using a threshold of 90%."},{"metadata":{"trusted":false},"cell_type":"code","source":"training_data = data[data['dteday'].isin(pd.date_range('2011-01-01','2012-09-30'))].copy()\ncorrelated_cols = hp.correlated(training_data, 0.9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Underrepresented Features\n\nNow we determine underrepresented features, meaning those that in more than 97% of the records are composed of a single value."},{"metadata":{"trusted":false},"cell_type":"code","source":"under_rep = hp.under_represented(data, 0.97)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Principal Component Analysis (PCA)\n\nWe plot PCA component variance to define the number of components we wish to consider in the pipeline."},{"metadata":{"trusted":false},"cell_type":"code","source":"hp.plot_pca_components(data.drop('dteday', axis=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importance\n\nHere we plot feature importance using a random forest in order to get a sense of which features have the most importance."},{"metadata":{"trusted":false},"cell_type":"code","source":"hp.feature_importance(hp.drop_columns(data, ['dteday', 'registered', 'casual']), RandomForestRegressor(n_estimators=KEYS['ESTIMATORS'], random_state = KEYS['SEED'], n_jobs = -1), convert = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining Holdout Set for Validation\n\nThe first 5 quarters of the data will be used to train our model, while the remaining quarter will be used later on to validate the accuracy of our model."},{"metadata":{"trusted":false},"cell_type":"code","source":"holdout = data[data['dteday'].isin(pd.date_range('2012-10-01','2012-12-31'))].copy().reset_index()\nholdout_final_plots = holdout.copy() # we will use this for plots at the end\ntrain_data = data[data['dteday'].isin(pd.date_range('2011-01-01','2012-09-30'))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering / Pipeline / Modeling\n\nA number of different combinations of feature engineering steps and transformations will be performed in a pipeline with different models, each one will be cross validated to review the performance of the model.\n\n**Some of the steps are commented, the point is for the user to comment/uncomment the steps they wish to try and those pipelines and scores will be saved for later use**, that way you can see what improves the score and what decreases it.\n\nOverall, we try removing unneeded columns, clustering, removing outliers through isolation forests, quantile binning, polynomial combinations, genetic transformations, one hot encoding, rebalancing techniques, recursive feature elimination, feature selection, PCA and more.\n\nThe pipeline uses the cross evaluation function, which handles time series splits for fold creation (instead of Kfolds which does not work for time series) while also setting a holdout to perform after the cross validation."},{"metadata":{"trusted":false},"cell_type":"code","source":"def day(df):\n    df = df.copy()\n    df['day'] = df['dteday'].dt.day\n    df = hp.convert_to_category(df, ['day'])\n    \n    return df\n\ndef drop_features(df, cols):\n    return df[df.columns.difference(cols)]\n\ndef kmeans(df, clusters = 3):\n    clusterer = KMeans(clusters, random_state=KEYS['SEED'])\n    cluster_labels = clusterer.fit_predict(df)\n    df = np.column_stack([df, cluster_labels])\n    \n    return df\n\ndef outlier_rejection(X, y):\n    model = IsolationForest(random_state=KEYS['SEED'], behaviour='new', n_jobs = -1)\n    model.fit(X)\n    y_pred = model.predict(X)\n    \n    return X[y_pred == 1], y[y_pred == 1]\n\nnum_pipeline = Pipeline([ \n    ('power_transformer', PowerTransformer(method='yeo-johnson', standardize = True))\n])\n\ncategorical_pipeline = Pipeline([\n    ('one_hot', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n])\n\npipe = Pipeline([\n    ('day', FunctionTransformer(day, validate=False)),\n    ('drop_features', FunctionTransformer(drop_features, kw_args={'cols': ['dteday','casual', 'registered'] + correlated_cols + under_rep}, validate=False)),\n    ('column_transformer', ColumnTransformer([\n        ('numerical_pipeline', num_pipeline, ['hum', 'temp', 'windspeed']),\n        ('categorical_pipeline', categorical_pipeline, ['day', 'hr', 'mnth', 'precipitation', 'season', 'weathersit', 'weekday', 'yr']),\n    ], remainder='passthrough'))\n])\n\nmodels = [\n    {'name':'linear_regression', 'model': LinearRegression()},\n    {'name':'random_forest', 'model': RandomForestRegressor(n_estimators = KEYS['ESTIMATORS'], random_state = KEYS['SEED'], n_jobs = -1)},\n    {'name':'xgb', 'model': XGBRegressor(random_state = KEYS['SEED'])}\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scores\n\nHere you can see all of the scores throughout the entire cross validation process for each pipeline. To begin, we run three initial models with basic transformations and then add/remove transformers to see how the score moves. In certain cases errors can happen (for example when a certain fold contains a sparse matrix), therefore you may see errors marked as such in the score."},{"metadata":{"trusted":false},"cell_type":"code","source":"all_scores = hp.pipeline(train_data, models, pipe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After running the initial pipeline, the point of using the package is to try different combinations, therefore you can feel free to uncomment the steps you wish to try and run them here. Since this is not possible in Kaggle, we will edit the pipeline separately.\n\n### Binning And Polynomial Features\nNow we try adding binning and polynomial features to our pipeline and see how it performs."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_pipeline = Pipeline([ \n    ('power_transformer', PowerTransformer(method='yeo-johnson', standardize = True)),\n    ('binning', KBinsDiscretizer(n_bins = 5, encode = 'onehot-dense')),\n    ('polynomial', PolynomialFeatures(degree = 2, include_bias = False)),\n])\n\ncategorical_pipeline = Pipeline([\n    ('one_hot', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n])\n\npipe = Pipeline([\n    ('day', FunctionTransformer(day, validate=False)),\n    ('drop_features', FunctionTransformer(drop_features, kw_args={'cols': ['dteday','casual', 'registered'] + correlated_cols + under_rep}, validate=False)),\n    ('column_transformer', ColumnTransformer([\n        ('numerical_pipeline', num_pipeline, ['hum', 'temp', 'windspeed']),\n        ('categorical_pipeline', categorical_pipeline, ['day', 'hr', 'mnth', 'precipitation', 'season', 'weathersit', 'weekday', 'yr']),\n    ], remainder='passthrough')),\n])\n\nall_scores = hp.pipeline(train_data, models, pipe, all_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PCA\nWe try doing Principal Component Analysis and see how it performs."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_pipeline = Pipeline([ \n    ('power_transformer', PowerTransformer(method='yeo-johnson', standardize = True)),\n])\n\ncategorical_pipeline = Pipeline([\n    ('one_hot', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n])\n\npipe = Pipeline([\n    ('day', FunctionTransformer(day, validate=False)),\n    ('drop_features', FunctionTransformer(drop_features, kw_args={'cols': ['dteday','casual', 'registered'] + correlated_cols + under_rep}, validate=False)),\n    ('column_transformer', ColumnTransformer([\n        ('numerical_pipeline', num_pipeline, ['hum', 'temp', 'windspeed']),\n        ('categorical_pipeline', categorical_pipeline, ['day', 'hr', 'mnth', 'precipitation', 'season', 'weathersit', 'weekday', 'yr']),\n    ], remainder='passthrough')),\n    ('pca', PCA(n_components = 6))\n])\n\nall_scores = hp.pipeline(train_data, models, pipe, all_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection\nWe perform feature selection based on feature importance (using a random forest) and setting a threshold of 0.005"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_pipeline = Pipeline([ \n    ('power_transformer', PowerTransformer(method='yeo-johnson', standardize = True)),\n])\n\ncategorical_pipeline = Pipeline([\n    ('one_hot', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n])\n\npipe = Pipeline([\n    ('day', FunctionTransformer(day, validate=False)),\n    ('drop_features', FunctionTransformer(drop_features, kw_args={'cols': ['dteday','casual', 'registered'] + correlated_cols + under_rep}, validate=False)),\n    ('column_transformer', ColumnTransformer([\n        ('numerical_pipeline', num_pipeline, ['hum', 'temp', 'windspeed']),\n        ('categorical_pipeline', categorical_pipeline, ['day', 'hr', 'mnth', 'precipitation', 'season', 'weathersit', 'weekday', 'yr']),\n    ], remainder='passthrough')),\n    ('feature_selection', SelectFromModel(RandomForestRegressor(n_estimators = KEYS['ESTIMATORS'], random_state = KEYS['SEED'], n_jobs = -1), threshold = 0.005)),\n])\n\nall_scores = hp.pipeline(train_data, models, pipe, all_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Outlier Removal Through Isolation Forest and Polynomial Features\nWe combine outlier removal and polynomial features"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_pipeline = Pipeline([ \n    ('power_transformer', PowerTransformer(method='yeo-johnson', standardize = True)),\n    ('polynomial', PolynomialFeatures(degree = 2, include_bias = False)),\n])\n\ncategorical_pipeline = Pipeline([\n    ('one_hot', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n])\n\npipe = Pipeline([\n    ('day', FunctionTransformer(day, validate=False)),\n    ('drop_features', FunctionTransformer(drop_features, kw_args={'cols': ['dteday','casual', 'registered'] + correlated_cols + under_rep}, validate=False)),\n    ('column_transformer', ColumnTransformer([\n        ('numerical_pipeline', num_pipeline, ['hum', 'temp', 'windspeed']),\n        ('categorical_pipeline', categorical_pipeline, ['day', 'hr', 'mnth', 'precipitation', 'season', 'weathersit', 'weekday', 'yr']),\n    ], remainder='passthrough')),\n    ('outliers', FunctionSampler(func = outlier_rejection)),\n])\n\nall_scores = hp.pipeline(train_data, models, pipe, all_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pipeline Performance by Model\nHere we can see the performance of each model in the different pipelines we created."},{"metadata":{"trusted":true},"cell_type":"code","source":"hp.plot_models(all_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top Pipelines per Model\n\nHere we show the top pipelines per model."},{"metadata":{"trusted":true},"cell_type":"code","source":"hp.show_scores(all_scores, top = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Randomized Grid Search\n\nOnce we have a list of models, we perform a cross validated, randomized grid search on the best performing one to define the final models."},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_grid = {\n    'random_forest__criterion': ['mse', 'mae'],\n    'random_forest__max_depth': [50, 100],\n    'random_forest__min_samples_leaf': [5,10],\n    'random_forest__min_samples_split': [10, 20],\n    'random_forest__max_leaf_nodes': [None, 80],\n}\n\nfinal_scores, f_pipe = hp.cross_val(train_data, model = clone(hp.top_pipeline(all_scores)), grid = rf_grid)\nfinal_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Best Parameters for the models"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(f_pipe.best_params_)\nfinal_pipe = f_pipe.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the scores obtained, we see that the performance of the model worsened, therefore we will stick to the original values"},{"metadata":{},"cell_type":"markdown","source":"# Results\nThis is the final R^2 score of the best pipeline, tested against the holdout set."},{"metadata":{"trusted":false},"cell_type":"code","source":"y, predictions = hp.predict(train_data, holdout, clone(hp.top_pipeline(all_scores)))\nscore = metric_scorer(y, predictions)\nscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plots of Predictions\n\nHere we plot the different results obtained.\n\nFor this scatter plot, the straighter the diagonal line is, the better the predictions since they are closer to the actual values."},{"metadata":{"trusted":false},"cell_type":"code","source":"hp.scatter_predict(y, predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hourly Three week prediction vs. reality plot"},{"metadata":{"trusted":false},"cell_type":"code","source":"hp.plot_predict(y, predictions, subset = (3*7*24), x_label = 'Hour', y_label = 'Users')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Entire daily predictions vs. reality plot"},{"metadata":{"trusted":false},"cell_type":"code","source":"hp.plot_predict(y, predictions, group = 24, x_label = 'Day', y_label = 'Users')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusions\n\nWe created a model that, based on certain parameters, determine bike usage on an hourly basis, with these results we can provide an estimation of usage which can be of great importance for all of the involved parties.\n\nOne of the key findings is that there is a great difference in usage from weekends to normal working days, this situation would need to be considered by the company to supply the correct amount of bicicles depending on the day of the week, since the demand changes drastically. Then, as can be guessed, temperature plays a big role in usage, although it is more significant in casual users.\n\nInitially we had a baseline model with a very low r2 score, however, after performing multiple data preparation steps and transformations we achieved a much higher score, but not only that, we can see from the prediction plots that the model follows along many of the peaks and valleys of the real data, this proves that our predicting capabilities improved immensely.\n\nMany different bike-sharing companies accross the world could use this model to estimate bike usage, planify better for expected demand and even help their governments transportation requirements. Measuring the impact of new bike infrastructure on cycling traffic and behavior is top of mind for many planners and advocacy groups."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"oldHeight":265.98978,"position":{"height":"288.212px","left":"1221.23px","right":"20px","top":"120px","width":"358.767px"},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"varInspector_section_display":"block","window_display":false}},"nbformat":4,"nbformat_minor":1}