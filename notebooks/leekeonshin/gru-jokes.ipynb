{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nimport numpy as np\n\nimport logging\nlogging.getLogger().setLevel(logging.CRITICAL)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# IPython candies...\nfrom IPython.display import Image\nfrom IPython.core.display import HTML\n\nfrom IPython.display import clear_output\n\nfrom nltk import word_tokenize, sent_tokenize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport json\nimport csv\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"darkgrid\")\nsns.set(rc={'figure.figsize':(12, 8)})\n\n\ntorch.manual_seed(42)\n\nfrom collections import namedtuple\n\nimport numpy as np\nfrom tqdm import tqdm\n\nimport pandas as pd\n\nfrom gensim.corpora import Dictionary\n\nimport torch\nfrom torch import nn, optim, tensor, autograd\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#jokes_dataset_path = 'jokes_data/'\n#short_jokes_path = os.path.join(jokes_dataset_path, 'shortjokes.csv')\nshort_jokes_path = \"/kaggle/input/short-jokes/shortjokes.csv\"\n\njoke_list = []\nend_of_text_token = \"</s>\"\n\nwith open(short_jokes_path) as csv_file:\n    csv_reader = csv.reader(csv_file, delimiter=',')\n\n    x = 0\n    for row in csv_reader:\n        joke_str = row[1]\n        joke_list.append(joke_str)\njoke_list.pop(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize the text.\ntokenized_text = [list(map(str.lower, word_tokenize(sent))) \n                  for sent in joke_list]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tokenized_text_sub= tokenized_text[:70000]\n#len(tokenized_text_sub)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class JokesDataset(nn.Module):\n    def __init__(self, texts):\n        self.texts = texts\n        \n        # Initialize the vocab \n        special_tokens = {'<pad>': 0, '<unk>':1, '<s>':2, '</s>':3}\n        self.vocab = Dictionary(texts)\n        self.vocab.patch_with_special_tokens(special_tokens)\n        \n        # Keep track of the vocab size.\n        self.vocab_size = len(self.vocab)\n        \n        # Keep track of how many data points.\n        self._len = len(texts)\n        \n        # Find the longest text in the data.\n        self.max_len = max(len(txt) for txt in texts) \n        \n    def __getitem__(self, index):\n        vectorized_sent = self.vectorize(self.texts[index])\n        x_len = len(vectorized_sent)\n        # To pad the sentence:\n        # Pad left = 0; Pad right = max_len - len of sent.\n        pad_dim = (0, self.max_len - len(vectorized_sent))\n        vectorized_sent = F.pad(vectorized_sent, pad_dim, 'constant')\n        return {'x':vectorized_sent[:-1], \n                'y':vectorized_sent[1:], \n                'x_len':x_len}\n    \n    def __len__(self):\n        return self._len\n    \n    def vectorize(self, tokens, start_idx=2, end_idx=3):\n        \"\"\"\n        :param tokens: Tokens that should be vectorized. \n        :type tokens: list(str)\n        \"\"\"\n        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n        # Lets just cast list of indices into torch tensors directly =)\n        \n        vectorized_sent = [start_idx] + self.vocab.doc2idx(tokens) + [end_idx]\n        return torch.tensor(vectorized_sent)\n    \n    def unvectorize(self, indices):\n        \"\"\"\n        :param indices: Converts the indices back to tokens.\n        :type tokens: list(int)\n        \"\"\"\n        return [self.vocab[i] for i in indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#jokes_data = JokesDataset(tokenized_text)\njokes_data = JokesDataset(tokenized_text)\nlen(jokes_data.vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers):\n        super(Generator, self).__init__()\n\n        # Initialize the embedding layer with the \n        # - size of input (i.e. no. of words in input vocab)\n        # - no. of hidden nodes in the embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        \n        # Initialize the GRU with the \n        # - size of the input (i.e. embedding layer)\n        # - size of the hidden layer \n        self.gru = nn.GRU(embedding_size, hidden_size, num_layers, batch_first=True)\n        \n        # Initialize the \"classifier\" layer to map the RNN outputs\n        # to the vocabulary. Remember we need to -1 because the \n        # vectorized sentence we left out one token for both x and y:\n        # - size of hidden_size of the GRU output.\n        # - size of vocabulary\n        self.classifier = nn.Linear(hidden_size, vocab_size)\n        \n    def forward(self, inputs, use_softmax=False, hidden=None):\n        # Look up for the embeddings for the input word indices.\n        embedded = self.embedding(inputs)\n        # Put the embedded inputs into the GRU.\n        output, hidden = self.gru(embedded, hidden)\n        \n        # Matrix manipulation magic.\n        batch_size, sequence_len, hidden_size = output.shape\n        # Technically, linear layer takes a 2-D matrix as input, so more manipulation...\n        output = output.contiguous().view(batch_size * sequence_len, hidden_size)\n        # Apply dropout.\n        output = F.dropout(output, 0.5)\n        # Put it through the classifier\n        # And reshape it to [batch_size x sequence_len x vocab_size]\n        output = self.classifier(output).view(batch_size, sequence_len, -1)\n        \n        return (F.softmax(output,dim=2), hidden) if use_softmax else (output, hidden)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the hidden_size of the GRU \n#embed_size = 12\n#hidden_size = 10\n#num_layers = 1\n\n#_encoder = Generator(len(jokes_data.vocab), embed_size, hidden_size, num_layers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n_hyper = ['embed_size', 'hidden_size', 'num_layers',\n          'loss_func', 'learning_rate', 'optimizer', 'batch_size']\nHyperparams = namedtuple('Hyperparams', _hyper)\n\n\nhyperparams = Hyperparams(embed_size=250, hidden_size=250, num_layers=1,\n                          loss_func=nn.CrossEntropyLoss,\n                          learning_rate=0.03, optimizer=optim.Adam, batch_size=245)\n\nhyperparams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a batch.\n#batch_size = 15\n#dataloader = DataLoader(dataset=jokes_data, batch_size=batch_size, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training routine.\ndef train(num_epochs, dataloader, model, criterion, optimizer):\n    losses = []\n    plt.ion()\n    for _e in range(num_epochs):\n        for batch in tqdm(dataloader):\n            # Zero gradient.\n            optimizer.zero_grad()\n            x = batch['x'].to(device)\n            x_len = batch['x_len'].to(device)\n            y = batch['y'].to(device)\n            # Feed forward. \n            output, hidden = model(x, use_softmax=False)\n            # Compute loss:\n            # Shape of the `output` is [batch_size x sequence_len x vocab_size]\n            # Shape of `y` is [batch_size x sequence_len]\n            # CrossEntropyLoss expects `output` to be [batch_size x vocab_size x sequence_len]\n            _, prediction = torch.max(output, dim=2)\n            loss = criterion(output.permute(0, 2, 1), y)\n            loss.backward()\n            optimizer.step()\n            losses.append(loss.float().data)\n\n        clear_output(wait=True)\n        plt.plot(losses)\n        plt.pause(0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_data_model_optim_loss(hyperparams):\n    # Initialize the dataset and dataloader.\n    jokes_data = JokesDataset(tokenized_text)\n    dataloader = DataLoader(dataset=jokes_data, \n                            batch_size=hyperparams.batch_size, \n                            shuffle=True)\n\n    # Loss function.\n    criterion = hyperparams.loss_func(ignore_index=jokes_data.vocab.token2id['<pad>'], \n                                      reduction='mean')\n\n    # Model.\n    model = Generator(len(jokes_data.vocab), hyperparams.embed_size, \n                      hyperparams.hidden_size, hyperparams.num_layers).to(device)\n\n    # Optimizer.\n    optimizer = hyperparams.optimizer(model.parameters(), lr=hyperparams.learning_rate)\n    \n    return dataloader, model, optimizer, criterion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyperparams = Hyperparams(embed_size=512, hidden_size=512, num_layers=1,\n                          loss_func=nn.CrossEntropyLoss,\n                          learning_rate=0.03, optimizer=optim.Adam, batch_size=64)\n\ndataloader, model, optimizer, criterion = initialize_data_model_optim_loss(hyperparams)\n\ntrain(5, dataloader, model, criterion, optimizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_example(model, temperature=1.0, max_len=100, hidden_state=None):\n    start_token, start_idx = '<s>', 2\n    # Start state.\n    inputs = torch.tensor(jokes_data.vocab.token2id[start_token]).unsqueeze(0).unsqueeze(0).to(device)\n\n    sentence = [start_token]\n    i = 0\n    while i < max_len and sentence[-1] not in ['</s>', '<pad>']:\n        i += 1\n        \n        embedded = model.embedding(inputs)\n        output, hidden_state = model.gru(embedded, hidden_state)\n\n        batch_size, sequence_len, hidden_size = output.shape\n        output = output.contiguous().view(batch_size * sequence_len, hidden_size)    \n        output = model.classifier(output).view(batch_size, sequence_len, -1).squeeze(0)\n        #_, prediction = torch.max(F.softmax(output, dim=2), dim=2)\n        \n        word_weights = output.div(temperature).exp().cpu()\n        if len(word_weights.shape) > 1:\n            word_weights = word_weights[-1] # Pick the last word.    \n        word_idx = torch.multinomial(word_weights, 1).view(-1)\n        \n        sentence.append(jokes_data.vocab[int(word_idx)])\n        \n        inputs = tensor([jokes_data.vocab.token2id[word] for word in sentence]).unsqueeze(0).to(device)\n    print(' '.join(sentence))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for _ in range(10):\n    generate_example(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\ntorch.save(model.state_dict(), 'gru-model-512.pth')\n\n#hyperparams_str = Hyperparams(embed_size=250, hidden_size=250, num_layers=1,\n#                          loss_func='nn.CrossEntropyLoss',\n#                          learning_rate=0.03, optimizer='optim.Adam', batch_size=250)\n\nhyperparams_str = Hyperparams(embed_size=250, hidden_size=250, num_layers=1,\n                          loss_func='nn.CrossEntropyLoss',\n                          learning_rate=0.03, optimizer='optim.Adam', batch_size=64)\n\nwith open('gru-model-512.json', 'w') as fout:\n    json.dump(dict(hyperparams_str._asdict()), fout)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}