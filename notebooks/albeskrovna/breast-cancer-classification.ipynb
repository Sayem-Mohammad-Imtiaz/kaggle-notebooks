{"cells":[{"metadata":{"_uuid":"4507ce2b965fd5be6d3a30d42b7f5cf3afa3be91"},"cell_type":"markdown","source":"In this notebook, I will build a K-NN classification model to predict  whether breast cancer is malignant or benign based on the features computed from a digitized image of a fine needle aspirate of a breast mass. The data is from the Breast Cancer Wisconsin data set. I will aslo use seaborn to visualize the data and help select the best features for my model."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33397dc03321b94377a8b1266f7ba3112d3bb776"},"cell_type":"code","source":"# importing other libraries I will need\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Other:\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set_style('darkgrid')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# importing the dataset\ndata = pd.read_csv(\"../input/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f14febe22c13b4bd06107324a7cc22be03c4b491"},"cell_type":"markdown","source":"**Exploratory Data Analysis**"},{"metadata":{"_uuid":"a1fd8e878c12776e4420c3b719df4cccb7f08bfe"},"cell_type":"markdown","source":"First, lets get to know the data. "},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"59e12e3570039f0ab1cd6f212d9d10edcc420964"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe64db062d91da734c118ef60decc950969e5642"},"cell_type":"markdown","source":"I see two columns that would be useless in my further analysis: the id column is not relevant, and the last column is empty. I will drop them."},{"metadata":{"trusted":true,"_uuid":"9e6986cdc5c24867dca5b457f2c0bdd7194d909f"},"cell_type":"code","source":"# Drop useless variables\ndata = data.drop(['Unnamed: 32','id'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f3b7702b47b9649a649efe8684bcb6bd68476d7"},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f609755077f8d0087a600cdb42ebd6eb068890b"},"cell_type":"markdown","source":"My dataset consists of 31 columns and 569 observations. Lets see if there is any missing data."},{"metadata":{"trusted":true,"_uuid":"1b0ff9a43bc78f0ff56231d1c034236b205ca5fc"},"cell_type":"code","source":"# checking for missing values\ndata.isna().any()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cff7c376d4f6606d86cbd4befdf5de1b1cde6e38"},"cell_type":"markdown","source":"There are almost twice as many benign cases in the dataset as malignant ones:"},{"metadata":{"trusted":true,"_uuid":"bbd61089e1d4cc5d49e00370d9b81dc038ed5c46"},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(10,5))\nsns.countplot(y = data['diagnosis'], palette = \"husl\", ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27ec3e0043eb0aff44b67156f5e2022c0b035b2c"},"cell_type":"markdown","source":"Before analyzing further, I need to normlalize the data in the features columns to be able to adequately present it in plots."},{"metadata":{"trusted":true,"_uuid":"ce51558d11d5e909009f1a938d1b2a98b1ef08dc"},"cell_type":"code","source":"features = data.iloc[:, 1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"386220f8e609c2ffd39553ff3368b35ef3e129f5"},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nx = features.values #returns a numpy array\nmin_max_scaler = MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\nfeat = pd.DataFrame(x_scaled, index = features.index, columns = features.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1be20e992a9afeaa8b40575d95290bf4840df2aa","scrolled":true},"cell_type":"code","source":"feat.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f616927577f2b48c31affbb99f4e85b9f1e31aec"},"cell_type":"code","source":"diag = data.iloc[:,0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c3be601c33e13e04bd144d503a0383c55fd10d2"},"cell_type":"markdown","source":"Now that the features have been scaled, lets take a closer look at them. "},{"metadata":{"trusted":true,"_uuid":"ce17ec8428dbb86ad46ed19edd8f9080a4f0928d"},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(feat.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b2e8a8d616d6b5c20a255bb55d474319510ef16"},"cell_type":"markdown","source":"Looks like we have some highly correlated features. Features with high correlation are more linearly dependent and hence have almost the same effect on the dependent variable. When two features have high correlation, we should drop one. I will identify and drop them."},{"metadata":{"trusted":true,"_uuid":"ea7b02f787828adfb030a286ad7c19c1c53a0d4a"},"cell_type":"code","source":"# Create correlation matrix\ncorr_matrix = feat.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.9\nto_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\nto_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"051bff45a72cce52cd01b22f579f0e3ee14965ef"},"cell_type":"code","source":"# Drop features \nfeat = feat.drop(to_drop, axis=1)\nfeat.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f5ae0f666b422367a20adf48c072f08e8a08b95"},"cell_type":"markdown","source":"Putting my dataframe back together: the diagnosis column first, the features deemed not too highly correlated next."},{"metadata":{"trusted":true,"_uuid":"6733d1c7744588916cb2bf32ddaaf284178071a6"},"cell_type":"code","source":"df = pd.concat([diag, feat], axis=1, sort=False)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6732fa3e88f48ed4127ed6c9835947ccf9f8b254"},"cell_type":"code","source":"# rewriting the categorical values in the target column as numerical\ny = data['diagnosis'].apply(lambda x: 1 if 'M' in x else 0)\ny.head()\n\n# putting the dataframe back together\ndf_encoded = pd.concat([y, feat], axis=1, sort=False)\ndf_encoded.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6dbda25e324355e1f5212ef128f5a631d283d43d"},"cell_type":"markdown","source":"Lets see how each predictor variable varies by diagnosis. For a lot of the predictor variables, average values are higher in the malignant group. There are also plenty of outliers, especially in the benign data."},{"metadata":{"trusted":true,"_uuid":"a2d3e4921dd575f4f717bc3f5074f1e18e273b43"},"cell_type":"code","source":"import math\n\nvars = df_encoded.drop('diagnosis', axis = 1).keys()\nplot_cols = 5\nplot_rows = math.ceil(len(vars)/plot_cols)\n\nplt.figure(figsize = (5*plot_cols,5*plot_rows))\n\nfor idx, var in enumerate(vars):\n    plt.subplot(plot_rows, plot_cols, idx+1)\n    sns.boxplot(x = 'diagnosis', y = var, data = df_encoded)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb543078d5bc17f0f41fcde76e59fbf3ce108d5f"},"cell_type":"markdown","source":"Lets see how the not so highly correlated features relate to each other. Looks like diagnosis is most highly correlated with the radius (radius_mean and radius_se), concavity (concavity_mean and concavity_worst), and compactness (compactness_mean and compactness_worst)."},{"metadata":{"trusted":true,"_uuid":"5b9116e343908fe9060c842299bb890dca3f23aa"},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(df_encoded.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d5c20eee4ca2d2f8eecc550ed16c427e059bb8d"},"cell_type":"markdown","source":"My suspicion is that compactness and concavity are still too highly correlated in all three groups (mean, se, and worst) to include them in the model. Lets check if I am right."},{"metadata":{"trusted":true,"_uuid":"a209214f6f9165a0f0d2bb9f83f72bc8dfc0199e"},"cell_type":"code","source":"df_select = df.iloc[:,0:8]\nsns.pairplot(df_select, hue = 'diagnosis')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"438d2a868c9acd183d0769a01c752039c88721be"},"cell_type":"code","source":"df_select = df.iloc[:,[0,8,9,10,11,12,13,14,15]]\nsns.pairplot(df_select, hue = 'diagnosis')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"256cdc687b4650fbf94d3727096014cfc451ae68"},"cell_type":"code","source":"df_select = df.iloc[:,[0,16,17,18,19,20]]\nsns.pairplot(df_select, hue = 'diagnosis')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ad23c8b51f0965e08eaf9c67a0cb8f3dda0e3fe"},"cell_type":"markdown","source":"Looks like compactness and concavity are too highly correlated indeed. I will drop concavity from my choice of selected features."},{"metadata":{"trusted":true,"_uuid":"d18ca9d5d66216bc93a59afa1ed3f9240720e438"},"cell_type":"code","source":"df_encoded = df_encoded.drop('concavity_mean', axis = 1)\ndf_encoded = df_encoded.drop('concavity_se', axis = 1)\ndf_encoded = df_encoded.drop('concavity_worst', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bc3b00c5f9dfc8f6e3d73cc51ee32ed65745137"},"cell_type":"code","source":"df_encoded.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73739c72467daf6dd8d4f3a15ea4ab24c735b172"},"cell_type":"code","source":"df_encoded.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fba3be5f8ed06e54c8178d27aed167ea6040b22d"},"cell_type":"markdown","source":"My final choice for the model are the 17 features."},{"metadata":{"_uuid":"b8cbf21a1ce76958fb7fba316e7b332cb754fbcf"},"cell_type":"markdown","source":"**K-NN Classifier**"},{"metadata":{"trusted":true,"_uuid":"6e03b92310c32e33c80c82223459444335ccef58"},"cell_type":"code","source":"X = df_encoded.iloc[:,1:].values\ny = df_encoded.iloc[:,0].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b281b4f3869758086c12d4116805f82576f8be74"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7166b6e3ea1808b6bff3332ca03c54706ee0c635"},"cell_type":"code","source":"# Fitting K-NN to the Training set\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24704e67cebc3fdbf273906d8df8964c298292e4"},"cell_type":"code","source":"# Predicting the Test set results\ny_pred = classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cba8c4f9e1799cb9cae15aea9d0678144b515f0"},"cell_type":"code","source":"# Making the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a81152d9501922abe88e4cece0fec50fac8e1339","scrolled":false},"cell_type":"code","source":"# checking the accuracy score\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02c073daa19b71d9b54ca552a8819e3917192568"},"cell_type":"markdown","source":"The model is 92% accurate. I will use K-fold cross-validation to better evaluate my model's performance."},{"metadata":{"trusted":true,"_uuid":"20371540e3eca7b4ba22dc1c9a91bfa5ed72504a"},"cell_type":"code","source":"# Applying 10-fold cross-validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed7b40a64957aa541b4f6f2b937a519f6a01ce65","scrolled":true},"cell_type":"code","source":"accuracies.mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1a3741dc33b85e3ed33c63c47b9e8c266e13173"},"cell_type":"markdown","source":"After performing cross-validation, the model seems to be 95% accurate. However, I picked the initial number of K nearest neighbors and weights at random. Lets see if we can improve the model's performance even better by finding the optimal hyperparameters through gridsearch."},{"metadata":{"trusted":true,"_uuid":"8118f87432abb27794f74aab6e4f5e4c47cc7eaa"},"cell_type":"code","source":"# Applying grid search to find the best model and the best parameters\nfrom sklearn.model_selection import GridSearchCV\n# specifying the parameters I want to find optimal values for\nparameters = [{'n_neighbors': [3,5,8,10], 'weights':['uniform']}, \n              {'n_neighbors': [3,5,8,10], 'weights':['distance']}\n             ]\ngrid_search = GridSearchCV(estimator = classifier,\n                          param_grid = parameters,\n                          scoring = 'accuracy',\n                          cv = 10)\ngrid_search = grid_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f38f26bd4198ee10581222a74d48cca7b59e1089"},"cell_type":"code","source":"best_accuracy = grid_search.best_score_\nbest_accuracy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4da369553e23405da57f082e7a95282aeff800db"},"cell_type":"markdown","source":"Looks like the best accuracy score I could get for my model is 95.16%, which is not much better from the already achieved 95.13%. Let's check which parameter choices would assure the highest possible accuracy."},{"metadata":{"trusted":true,"_uuid":"41dcfb125b1c56057e0609fa561961a42ab18fd4"},"cell_type":"code","source":"best_parameters = grid_search.best_params_\nbest_parameters","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8005500160639501a1475af1c2c7c8c2822071b9"},"cell_type":"markdown","source":"The most appropriate parameters for our K-NN model are the initially selected uniform as weights and K=5. \n\nThe default data included 33 features. By selecting the most appropriate 17 features, I was able to build a classification model with a 95% accuracy. "},{"metadata":{"_uuid":"4a008774bca06fd567bc8b31f9dbb1bcb5cd89de"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}