{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Time Series - ARIMA\n\n**Time series analysis is a statistical method to analyse the past data within a given duration of time to forecast the future. It comprises of ordered sequence of data at equally spaced interval.To understand the time series data & the analysis let us consider an example. Consider an example of Airline Passenger data. It has the count of passenger over a period of time.**\n\n![](https://i.pinimg.com/originals/6e/8a/45/6e8a45b41425813c606412e3b9e78cb8.jpg)"},{"metadata":{},"cell_type":"markdown","source":"Here the **Objective** is- Build a model to forecast the demand(passenger traffic) in Airplanes. The data is classified in date/time and the passengers travelling per month\n\n**Time Series:**<br>\nTime Series is a series of observations taken at particular time intervals (usually equal intervals). Analysis of the series helps us to predict future values based on previous observed values. In Time series, we have only 2 variables, time & the variable we want to forecast.\n\n**Why & where Time Series is used?**<br>\nTime series data can be analysed in order to extract meaningful statistics and other charecteristsics. It's used in atleast the 4 scenarios:\n1. Business Forecasting\n2. Understanding past behavior\n3. Plan the future\n4. Evaluate current accomplishment\n\n**Importance of Time Series Analysis:**<br>\nAmple of time series data is being generated from a variety of fields. And hence the study time series analysis holds a lot of applications. Let us try to understand the importance of time series analysis in different areas.\n1. Economics\n2. Finance\n3. Healthcare\n4. Environmental Science\n5. Sales Forecasting\n\n**What are the components of Time Series?**<br>\nThere are 4 components:\n**1. Trend** - Upward & downward movement of the data with time over a large period of time. Eq: Appreciation of Dollar vs rupee.\n**2. Seasonality** - seasonal variances. Eq: Ice cream sales increases in Summer only\n**3. Noise or Irregularity** - Spikes & troughs at random intervals\n**4. Cyclicity** - behavior that repeats itself after large interval of time, like months, years etc.\n\n**What is Stationarity?**<br>\nBefore applying any statistical model on a Time Series, the series has to be staionary, which means that, over different time periods,\n1. It should have constant mean.\n2. It should have constant variance or standard deviation.\n3. Auto-covariance should not depend on time.\n\nTrend & Seasonality are two reasons why a Time Series is not stationary.\n\n**Why does Timeseries has to be Stationary?**<br>\nStationary data means mean and standard deviation of data does not change WRT time. The mean across many time periods is only informative if the expected value is the same across those time periods. We make the data stationary in case of ARIMA because the arima model looks at the past data to predict the future values.\n\n**Tests to check if a series is stationary or not:**<br>\nThere are 2 ways to check for Stationarity of a Timeseries:<br>\n**1. Rolling Statistics** - Plot the moving avg(mean) or moving standard deviation to see if it varies with time. It is a visual technique.<br>\n**2. ADCF Test** - Augmented Dickey–Fuller test is used to gives us various values that can help in identifying stationarity. The Null hypothesis says that a Timeseries is non-stationary. It comprises of a Test Statistics & some critical values for some confidence levels. If the Test statistics is less than the critical values, we can reject the null hypothesis & say that the series is stationary. THE ADCF test also gives us a p-value. Acc to the null hypothesis, lower values of p is better.\n\n**What Is ADCF test?**<br>\nIn statistics and econometrics, an augmented Dickey–Fuller test (ADF) tests the null hypothesis that a unit root is present in a time series sample. The alternative hypothesis is different depending on which version of the test is used, but is usually stationarity or trend-stationarity. It is an augmented version of the Dickey–Fuller test for a larger and more complicated set of time series models.\n\nThe augmented Dickey–Fuller (ADF) statistic, used in the test, is a negative number. The more negative it is, the stronger the rejection of the hypothesis that there is a unit root at some level of confidence.\n\np value(0<=p<=1) should be as low as possible. Critical values at different confidence intervals should be close to the Test statistics value.\n\n**What is ARIMA model?**<br>\nARIMA(Auto Regressive Integrated Moving Average) is a combination of 2 models AR(Auto Regressive) & MA(Moving Average). It has 3 hyperparameters - P(auto regressive lags), d(order of differentiation) and Q(moving avg.) which respectively comes from the AR, I & MA components. The AR part is correlation between prev & current time periods. To smooth out the noise, the MA part is used. The I part binds together the AR & MA parts.\n\n**How to find value of P & Q for ARIMA ?**<br>\nWe need to take help of **ACF(Auto Correlation Function)** & **PACF(Partial Auto Correlation Function)** plots. ACF & PACF graphs are used to find value of P & Q for ARIMA. We need to check, for which value in x-axis, graph line drops to 0 in y-axis for 1st time.\nFrom PACF(at y=0), get P\nFrom ACF(at y=0), get Q\n\n**What is Exponential Smoothing?**<br>\nExponential smoothing is a rule of thumb technique for smoothing time series data using the exponential window function. Whereas in the simple moving average the past observations are weighted equally, exponential functions are used to assign exponentially decreasing weights over time. It is an easily learned and easily applied procedure for making some determination based on prior assumptions by the user, such as seasonality. Exponential smoothing is often used for analysis of time-series data."},{"metadata":{},"cell_type":"markdown","source":"**Loading the basic libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.arima_model import ARIMA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Loading AirPassenger data set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass = pd.read_csv('../input/air-passengers/AirPassengers.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's check first 5 and last 5 records of data set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass.tail(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's rename \"#Passengers\", seems really annoying the column name.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass.rename(columns={'#Passengers':'Passengers'},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There are 144 records in 2 datasets and 2 columns. There are no null records present. But, look at the Month column. We need to convert them in to datetime datatype.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\nairpass['Month']=pd.to_datetime(airpass['Month'],infer_datetime_format=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now, we will need to index Month column.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"airpassind = airpass.set_index('Month',inplace=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airpassind.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's plot the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.xlabel('Date')\nplt.ylabel('Number Of Air Passengers')\nplt.plot(airpassind)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the above below, we can see that there is a Trend compoenent in the series. Hence, we now check for stationarity of the data.**"},{"metadata":{},"cell_type":"markdown","source":"**Let's make one function consisting of stationary data checking and ADCF test working. Because we will need to repeat the steps many times, therefore, making function will become very handy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_stationarity(timeseries):\n    \n    #Determine rolling statistics\n    movingAverage = timeseries.rolling(window=12).mean()\n    movingSTD = timeseries.rolling(window=12).std()\n    \n    #Plot rolling statistics\n    plt.plot(timeseries, color='blue', label='Original')\n    plt.plot(movingAverage, color='red', label='Rolling Mean')\n    plt.plot(movingSTD, color='black', label='Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    \n    #Perform Dickey–Fuller test:\n    print('Results of Dickey Fuller Test:')\n    airpass_test = adfuller(timeseries['Passengers'], autolag='AIC')\n    dfoutput = pd.Series(airpass_test[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in airpass_test[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's determine & plot rolling statistics.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stationarity(airpassind)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From above plot, we can see that Rolling Mean itself has a trend component even though Rolling Standard Deviation is fairly constant with time.**\n\n**For time series to be stationary, we need to ensure that both Rolling Mean and Rolling Standard Deviation remain fairly constant WRT time.**\n\n**Both the curves needs to be parallel to X-Axis, in our case it is not so.**\n\n**We've also conducted the ADCF ie Augmented Dickey Fuller Test. Having the Null Hypothesis to be Time Series is Non Stationary.**"},{"metadata":{},"cell_type":"markdown","source":"For a Time series to be stationary, the ADCF test should have:\n\n1. p-value should be low (according to the null hypothesis)\n2. The critical values at 1%,5%,10% confidence intervals should be as close as possible to the Test Statistics\n\nFrom the above ADCF test result, we can see that p-value(near to 1.0) is very large. Also critical values are no where close to the Test Statistics. Hence, we can safely say that our Time Series at the moment is **NOT STATIONARY**"},{"metadata":{},"cell_type":"markdown","source":"### **Data Transformation To Achieve Stationarity**\n\n**Now, we will have to perform some data transformation to achieve Stationarity. We can perform any of the transformations like taking log scale, square, square root, cube, cube root, time shift, exponential decay, etc.**\n\n**Let's perform Log Transformation.**\n\n**Basically we need to remove the trend component.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass_log = np.log(airpassind)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(airpass_log)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Working on Rolling stats seperately (not using function) because we would need Rolling stats separately for computing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"rollmean_log = airpass_log.rolling(window=12).mean()\nrollstd_log = airpass_log.rolling(window=12).std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(airpass_log, color='blue', label='Original')\nplt.plot(rollmean_log, color='red', label='Rolling Mean')\nplt.plot(rollstd_log, color='black', label='Rolling Std')\nplt.legend(loc='best')\nplt.title('Rolling Mean & Standard Deviation (Logarithmic Scale)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above graph we can say that, we slightly bettered our previous results. Now, we are heading into the right direction.\n\nFrom the above graph, Time series with log scale as well as Rolling Mean(moving avg) both have the trend component. Thus subtracting one from the other should remove the trend component.\n\n**R (result) = Time Series Loca Scale - Rolling Mean Log Scale -> this can be our final non trend curve**"},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass_new = airpass_log - rollmean_log","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass_new.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stationarity(airpass_new)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the above plot, we came to know that \"indeed subtracting two related series having similar trend components actually removed trend and made the dataset stationary\"**"},{"metadata":{},"cell_type":"markdown","source":"Also, after concluding the results from ADFC test:\n\n1. p-value has reduced from 0.99 to 0.022\n2. Critical values at 1%,5%,10% confidence intervals are pretty close to the Test Statistic\n\nSo we can now say that given series is now **STATIONARY**"},{"metadata":{},"cell_type":"markdown","source":"### **Time Shift Transformation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass_log_diff = airpass_log - airpass_log.shift()\nplt.plot(airpass_log_diff)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass_log_diff.dropna(inplace=True)\nplt.plot(airpass_log_diff)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stationarity(airpass_log_diff)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above plot, we can see that, visually this is the very best result as our series along with rolling stats values of moving avg(mean) & moving standard deviation is very much flat & stationary. \n\nBut, the ADCF test shows us that:\n\n1. p-value of 0.07 is not as good as 0.02 of previous instance.\n2. Test Statistic value not as close to the critical values as that of previous instance."},{"metadata":{},"cell_type":"markdown","source":"**Let us now break down the 3 components of the log scale series using a system libary function. Once, we separate our the components, we can simply ignore trend & seasonality and check on the nature of the residual part.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"decomposition = seasonal_decompose(airpass_log)\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot(411)\nplt.plot(airpass_log, label='Original')\nplt.legend(loc='best')\n\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\n\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\n\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There can be cases where an observation simply consist of trend & seasonality. In that case, there won't be any residual component & that would be a null or NaN. Hence, we also remove such cases.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass_decompose = residual\nairpass_decompose.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rollmean_decompose = airpass_decompose.rolling(window=12).mean()\nrollstd_decompose = airpass_decompose.rolling(window=12).std()\n\nplt.plot(airpass_decompose, color='blue', label='Original')\nplt.plot(rollmean_decompose, color='red', label='Rolling Mean')\nplt.plot(rollstd_decompose, color='black', label='Rolling Std')\nplt.legend(loc='best')\nplt.title('Rolling Mean & Standard Deviation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Plotting ACF & PACF**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_acf = acf(airpass_log_diff, nlags=20)\nlag_pacf = pacf(airpass_log_diff, nlags=20, method='ols')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ACF:\nplt.subplot(121)\nplt.plot(lag_acf)\nplt.axhline(y=0, linestyle='--', color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(airpass_log_diff)), linestyle='--', color='gray')\nplt.axhline(y=1.96/np.sqrt(len(airpass_log_diff)), linestyle='--', color='gray')\nplt.title('Autocorrelation Function')            \n\n#Plot PACF\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0, linestyle='--', color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(airpass_log_diff)), linestyle='--', color='gray')\nplt.axhline(y=1.96/np.sqrt(len(airpass_log_diff)), linestyle='--', color='gray')\nplt.title('Partial Autocorrelation Function')\n            \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the ACF graph, we can see that curve touches y=0.0 line at x=2. Thus, from theory, Q = 2 \nFrom the PACF graph, we see that curve touches y=0.0 line at x=2. Thus, from theory, P = 2\n\n**ARIMA is AR + I + MA.** Before, we see an ARIMA model, let us check the results of the individual AR & MA model. Note that, these models will give a value of RSS. Lower the RSS values indicates a better model."},{"metadata":{},"cell_type":"markdown","source":"### **AR Model**\nMaking order = (2,1,0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = ARIMA(airpass_log, order=(2,1,0))\nresults_AR = model1.fit(disp=-1)\nplt.plot(airpass_log_diff)\nplt.plot(results_AR.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_AR.fittedvalues - airpass_log_diff['Passengers'])**2))\nprint('Plotting AR model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **MA Model**\nMaking order = (0,1,2)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = ARIMA(airpass_log, order=(0,1,2))\nresults_MA = model2.fit(disp=-1)\nplt.plot(airpass_log_diff)\nplt.plot(results_MA.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_MA.fittedvalues - airpass_log_diff['Passengers'])**2))\nprint('Plotting MA model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **AR+I+MA = ARIMA Model**\nMaking order = (2,1,2)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ARIMA(airpass_log, order=(2,1,2))\nresults_ARIMA = model.fit(disp=-1)\nplt.plot(airpass_log_diff)\nplt.plot(results_ARIMA.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_ARIMA.fittedvalues - airpass_log_diff['Passengers'])**2))\nprint('Plotting ARIMA model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**RSS value for:**\nAR Model - 1.5023\nMA Model - 1.4721\n\nARIMA Model - 1.0292\n\nBy combining AR & MA into ARIMA, we see that RSS value has decreased from either case to 1.0292, indicating ARIMA to be better than its individual component models.\n\nWith the ARIMA model built, we will now generate predictions. But, before we do any plots for predictions ,we need to reconvert the predictions back to original form. This is because, our model was built on log transformed data."},{"metadata":{},"cell_type":"markdown","source":"### **Prediction & Reverse Transformation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\npredictions_ARIMA_diff.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\npredictions_ARIMA_diff_cumsum.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_ARIMA_log = pd.Series(airpass_log['Passengers'].iloc[0], index=airpass_log.index)\npredictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum, fill_value=0)\npredictions_ARIMA_log.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Inverse of log is exp**"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_ARIMA = np.exp(predictions_ARIMA_log)\nplt.plot(airpassind)\nplt.plot(predictions_ARIMA)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From above plot, we can see that our predicted forecasts are very close to the real time series values. It also indicates a fairly accurate model.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"airpass_log.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We have 144 (existing data of 12 yrs in months) data points. \nNow, we want to forecast for additional 10 yrs (10x12 months=120 data points).**\n\n**144+120 = 264 records/data points**"},{"metadata":{"trusted":true},"cell_type":"code","source":"results_ARIMA.plot_predict(1,264)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}