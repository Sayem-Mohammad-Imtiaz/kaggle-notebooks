{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%matplotlib inline \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels as sm\n\nimport warnings \nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dir_data = \"/kaggle/input/temperature-timeseries-for-some-brazilian-cities\"\ninput_data = {}\nfor filename in os.listdir(dir_data):\n    if filename.endswith(\".csv\"):\n        variable_name = filename.split('.')[0]\n        input_data[variable_name] = pd.read_csv(os.path.join(dir_data,filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data.keys()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Can not apply the reduce memory usuage as this changes the values a little bit for me , like it changed 999.90 to 1000.000"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data['station_vitoria'][:30]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data['station_vitoria'].replace(999.90, np.NaN).fillna(method='ffill')[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can go for forward fill also, I have chosen to go with mean of last 12 data points. "},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in input_data.keys():\n    for j in input_data[i].columns:\n        input_data[i][j] = input_data[i][j].replace(999.90, np.NaN)\n        input_data[i][j] = input_data[i][j].fillna(input_data[i][j].rolling(12,1).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pattern over the months for given year "},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in input_data.keys():\n    input_data[i].drop(['D-J-F','M-A-M','J-J-A','S-O-N'], axis=1, inplace=True)\n    df = input_data[i].T\n    df.columns = df.iloc[0]\n    df.drop(['YEAR'], axis=0, inplace=True)\n    \n    #plt.figure(figsize=(18,12))\n    df.iloc[:-1,-11:].plot(figsize=(12,5), title=i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in input_data.keys():\n    input_data[i] = pd.melt(input_data[i], id_vars=['YEAR','metANN'], value_vars=['JAN','FEB','MAR','APR','MAY','JUN','JUL','AUG','SEP','OCT','NOV','DEC',], \n                            var_name='month',value_name='Temp')\n    input_data[i]['Date'] = pd.to_datetime(input_data[i]['YEAR'].astype(str)+'/'+input_data[i]['month'].astype(str)+'/01')\n    input_data[i].drop(['YEAR','month'],axis=1,inplace=True)\n    input_data[i].sort_values(by='Date',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_data = {}\nmetANN_data = {}\n\nfor i in input_data.keys():\n    temp_data[i] = input_data[i][['Date','Temp']]\n    temp_data[i] = temp_data[i].set_index('Date')\n    metANN_data[i] = input_data[i][['Date','metANN']]\n    metANN_data[i] = metANN_data[i].groupby(pd.Grouper(key='Date', freq='Y')).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Imp: Another way is to merge these station's dataframe into one keeping the column as temp_fortaleza, temp_belem and so on. In this way processing time and space can be saved. \nYou can do this way too :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=6, ncols=2, figsize=(20,12), constrained_layout=True)\nfig.suptitle(\"temp of stations over the years\", fontsize=22)\n\nstations = [list(temp_data.keys())[:2], list(temp_data.keys())[2:4],list(temp_data.keys())[4:6],list(temp_data.keys())[6:8],list(temp_data.keys())[8:10],\n           list(temp_data.keys())[10:12]]\n\nfor row, s in zip(ax,stations):\n    for col,i in zip(row, s):\n        col.plot(temp_data[i])\n        col.set_title(i)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations:\n    1.  Min recorded temperature is in station \"station_curitiba\" i.e 12.5 in so many years.\n    2.  Max recorded temp is in station \"station_manaus\" i.e. 32 around in late 20's decade.\n    3. \"station_belem\", \"station_fortaleza\" and \"station_manaus\" has increasing trend somewhat over the years.\n    4. \"station_macapa\" has trend also variarble trend. \n    5. \"station_recife\" has temp increasing trend over the years 1970 to 1990. "},{"metadata":{},"cell_type":"markdown","source":"Now will check for the metANN of stations over the years: "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=6, ncols=2, figsize=(20,12),constrained_layout=True)\nfig.suptitle(\"metANN over the years\", fontsize=22)\n\nstations = [list(metANN_data.keys())[:2], list(metANN_data.keys())[2:4],list(metANN_data.keys())[4:6],list(metANN_data.keys())[6:8],\n            list(metANN_data.keys())[8:10],list(metANN_data.keys())[10:12]]\n\nfor row, s in zip(ax,stations):\n    for col,i in zip(row, s):\n        col.plot(metANN_data[i])\n        col.set_title(i)\n\n#fig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations:\n    1. Global warming and other factors seems to be powerful as all has increasing trend majorly after 1990. "},{"metadata":{},"cell_type":"markdown","source":"Deompose the data to have look at seasonlity, trend and residulals"},{"metadata":{},"cell_type":"markdown","source":"Merge all temp dataframe into one for easy processing:"},{"metadata":{"trusted":true},"cell_type":"code","source":"zero_index = list(temp_data.keys())[0]\none_index = list(temp_data.keys())[1]\ntemp_df = temp_data[zero_index].merge(temp_data[one_index], left_on=\"Date\", right_on='Date', suffixes=('_'+zero_index,'_'+one_index))\n\nfor i in list(temp_data.keys())[2:]:\n    temp_df = temp_df.merge(temp_data[i], left_on='Date', right_on='Date').rename(columns={'Temp':'Temp_'+i+''})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\nfor i in temp_df.columns:\n    #print(i)\n    try:\n        decomposition = seasonal_decompose(temp_df[i], model=\"additive\")\n    except Exception as e:\n        #print(e)\n        pass\n        \n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(18,3), constrained_layout=True)\n    fig.subplots_adjust(wspace=0.15)\n\n    ax1= plt.subplot(121)\n    ax1.plot(decomposition.trend)\n    ax1.set_title(\"Trend--> \"+i+\"\")\n\n    ax2 = plt.subplot(122)\n    ax2.plot(decomposition.seasonal)\n    ax2.set_title(\"Seasonality--> \"+i+\"\")\n    \n\nplt.tight_layout()\nplt.show()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"A pattern of seasonlality is clearly visible in all stations "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now lets analyze the stationarity of time series :\nimport statsmodels\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import coint, adfuller","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Rolling Mean and Standard Deviation \ndef TestStationaryPlot(ts):\n    rol_mean = ts.rolling(window=12, center=False).mean()\n    rol_std = ts.rolling(window=12, center=False).std()\n    \n    plt.figure(figsize=(18,6))\n    \n    plt.plot(ts, color=\"red\",label=\"Time Series\")\n    plt.plot(rol_mean, color=\"blue\", label=\"Rolling mean\")\n    plt.plot(rol_std, color=\"yellow\", label=\"Rolling standard deviation\")\n    plt.xticks(fontsize=15)\n    plt.yticks(fontsize=15)\n    \n    plt.xlabel(\"Time\", fontsize=15)\n    plt.ylabel(\"Consumption\", fontsize=15)\n    plt.legend(loc=\"best\", fontsize=15)\n    \n    plt.title(\"Rolling Mean and Standard Deviation of Series Data\", fontsize=15)\n    plt.show(block=True)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adfuller test\n#null hypothesis : Series has a unit root \n#True ---> Stationary\n#False ---> Non-stationary \n\ndef TestStationaryAdfuller(ts, cutoff=0.05):\n    ts_test = adfuller(ts, autolag='AIC')\n    ts_test_output = pd.Series(ts_test[0:4], index = ['Test Stats', 'p-value', '#Lags Used', 'Number of observation used'])\n    \n    for k, v in ts_test[4].items():\n        ts_test_output['Critical Value (%s)'%k] = v\n        \n    \n    if ts_test[1] <= cutoff:\n        #print(\"ADF TEST :  Weak evidence against the null hypothesis, reject the null hypothesis. Data has no unit root, hence it is stationary\")\n        return True\n    else:\n        #print(\"ADF TEST :  Strong evidence against null hypothesis, time series has a unit root, indicating it is non-stationary\")\n        return False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#KPSS test \n#null hyposthesis : Series is trend stationary \n\nfrom statsmodels.tsa.stattools import kpss\n\ndef testKPSStationary(timeseries, cutoff=0.05):\n    kpsstest = kpss(timeseries, regression='c')\n    kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic','p-value','Lags Used'])\n    \n    for key,value in kpsstest[3].items():\n        kpss_output['Critical Value (%s)'%key] = value\n \n    if kpsstest[1] <= cutoff:\n        #print(\"KPSS TEST : Weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary\")\n        return False\n    else:\n        #print(\"KPSS TEST : Strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root, hence it is stationary\")\n        return True\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def isStationary(timeseries, station):\n    \n    Station = station\n    adf_result = TestStationaryAdfuller(timeseries)\n    kpss_result = testKPSStationary(timeseries)\n    \n    if (adf_result==False and kpss_result==True):\n        Type = 'Trend'\n        Stationary = 'No'\n    elif(adf_result==True and kpss_result==True):\n        Type = np.NaN\n        Stationary = 'Yes'\n    elif(adf_result==True and kpss_result==False):\n        Type = 'Difference'\n        Stationary = 'No'\n    else:\n        Type = np.NaN\n        Stationary = 'No'\n    \n    return pd.DataFrame([{'Station':station, 'Adfuller_result':adf_result, 'KPSS_result':kpss_result, 'Type':Type, 'Stationary':Stationary}])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Temperature Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the data into train and test \ntemp_df_train = temp_df[:430]\ntemp_df_test = temp_df[430:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check if the training temperature data is stationary or not:"},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\nstationary_df = pd.DataFrame(columns=['Station','Adfuller_result', 'KPSS_result', 'Type','Stationary'])\nfor i in temp_df_train.columns:\n    try:\n        stationary_df = stationary_df.append(isStationary(temp_df_train[i], i))\n    except Exception as e:\n        print(\"This series of __\"+i+\"__ contains nans, will see it later(NaN at the start of the series we can safely drop it)\")\nprint(stationary_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is clear from the above data that only two station has stationary series i.e. Curitiba and Rio. \nRest all are non-stationary, few are diff stationary. We can make them stationary by differencing. "},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in temp_df_train.columns:\n    for diff_order in range(1,13):\n        d = isStationary((temp_df_train[i] - temp_df_train[i].shift(diff_order)).dropna(), i)\n        if d.Stationary.values == 'Yes':\n            print(i, diff_order)\n            break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is clearly visible that all the station series are stationary after 1 diff only. while Curitiba and Rio were stationary without diff also "},{"metadata":{"trusted":true},"cell_type":"code","source":"#This is mean absolute error:\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt \n\ndef mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applying holt winters model in data of every city of Brazil and checking... "},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\nfor i in temp_df_train.columns:\n    print('\\n\\n')\n    print(\"********************************************\"+i+\"*************************************************************\")\n    y_hat_avg = temp_df_test[i].copy()\n    fit1 = ExponentialSmoothing(np.asarray(temp_df_train[i].dropna()) ,seasonal_periods=12 ,trend='add', seasonal='add',).fit()\n    y_hat_avg['Holt_Winter'] = fit1.forecast(len(temp_df_test[i]))\n\n    plt.figure(figsize=(16,8))\n    plt.plot(temp_df_train[i][252:], label='Train')\n    plt.plot(temp_df_test[i], label='Test')\n    plt.plot(temp_df_test.index, y_hat_avg['Holt_Winter'], label='Holt_Winter')\n    plt.legend(loc='best')\n    plt.show()\n    print(\"---------------------------------Mean Absolute Percentage Error------------------------------------------------------\")\n    print(mean_absolute_percentage_error(temp_df_test[i], y_hat_avg.Holt_Winter))\n    print(\"---------------------------------------RMS----------------------------------------------------------------\")\n    print(sqrt(mean_squared_error(temp_df_test[i], y_hat_avg.Holt_Winter)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note : Maximum percentage error in station \"Temp_station_sao_luiz\" because it contains NaN and \"Temp_station_curitiba\" "},{"metadata":{},"cell_type":"markdown","source":"**metANN Data**"},{"metadata":{},"cell_type":"markdown","source":"The same analysis can be applied to the metANN too"},{"metadata":{},"cell_type":"markdown","source":"\n*If you like this kernal, please upvote :) \n    Thank you*"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}