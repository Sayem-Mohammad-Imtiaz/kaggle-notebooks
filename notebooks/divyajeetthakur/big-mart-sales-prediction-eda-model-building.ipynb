{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_columns',None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/big-mart-sales-prediction/Train.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftest = pd.read_csv('../input/big-mart-sales-prediction/Test.csv')\ndftest.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Type of Data\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unique values in each column\ndf.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Item_Fat_Content classes are not organized\nprint(df['Item_Fat_Content'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mapping a dictionary to map all the fat types into low or regular.\nitem_fat = {'Low Fat':'low', 'Regular':'regular', 'LF':'low', 'reg':'regular','low fat':'low'}\n\ndf['Item_Fat_Content'] = df['Item_Fat_Content'].map(item_fat)\ndftest['Item_Fat_Content'] = dftest['Item_Fat_Content'].map(item_fat)\nprint(df['Item_Fat_Content'].unique())  # All classes are now changed into low or regular.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Treating Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Null Values in terms of percentage\ndf.isnull().sum() / df.shape[0]*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Item Pre Processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Null values based on 'Item_Fat_Content'\nprint(df[ df['Item_Weight'].isnull() ]['Item_Fat_Content'].value_counts())\n\nsns.countplot(df[ df['Item_Weight'].isnull() ]['Item_Fat_Content'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Null values based on 'Item_Fat_Content'\nprint(df[ df['Item_Weight'].isnull() ]['Item_Type'].value_counts())\n\nplt.figure(figsize=(15,5))\nsns.countplot(df[ df['Item_Weight'].isnull() ]['Item_Type'])\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating an array for those unique item_identifier which are having null values.\nitem_having_null = df[df['Item_Weight'].isnull()]['Item_Identifier'].unique()\n\n# Showing products from those item_identifier which have missing values.\ndf[ df['Item_Identifier'].isin(item_having_null)].sort_values(by='Item_Identifier', ascending=True).head(5)\n\n#  Item_weight and item_mrp is correlated for each item_identifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a new column called Item_MRP_per_unit weight\n\ndf.insert(6,'Item_MRP_per_unit_weight',float )\ndf['Item_MRP_per_unit_weight'] = df['Item_MRP']/df['Item_Weight']\n\ndftest.insert(6,'Item_MRP_per_unit_weight',float )\ndftest['Item_MRP_per_unit_weight'] = dftest['Item_MRP']/dftest['Item_Weight']\n\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bifurcating all the item columns with outlet columns\nitem_train = df.iloc[:,:7]\noutlet_train = df.iloc[:,7:-1]\n\nitem_test = dftest.iloc[:,:7]\noutlet_test = dftest.iloc[:,7:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a dataframe which contains all the item_identifier and item_mrp_per_unit_weight\nitem_train_mean_mrp = item_train[['Item_Identifier','Item_MRP_per_unit_weight']]\nitem_train_mean_mrp = item_train_mean_mrp.groupby(by='Item_Identifier').mean()\nitem_train_mean_mrp['Item_MRP_per_unit_weight'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating new column to specify whether item_weight contains null value or not for a record\nitem_train['MRP_null'] = item_train['Item_MRP_per_unit_weight'].isnull()\nitem_test['MRP_null'] = item_test['Item_MRP_per_unit_weight'].isnull()\n\nprint(item_train['MRP_null'].value_counts()) , print(item_test['MRP_null'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(item_train.shape[0]):\n    e = item_train.iloc[i,0]\n    if item_train.iloc[i,-1] == True:\n        item_train.loc[i, 'Item_MRP_per_unit_weight'] =  item_train_mean_mrp['Item_MRP_per_unit_weight'][e]\n\nfor i in range(item_test.shape[0]):\n    e = item_test.iloc[i,0]\n    if item_test.iloc[i,-1] == True:\n        item_test.loc[i, 'Item_MRP_per_unit_weight'] =  item_train_mean_mrp['Item_MRP_per_unit_weight'][e]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Item_MRP_per_unit_weight is filled with average Item_MRP_per_unit_weight value based on Item_Identifier\nitem_train[item_train['Item_Identifier'] == 'DRI11']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Item_weight would be equal to Item_MRP / Item_MRP_per_unit_weight\nitem_train.loc[ item_train['Item_Weight'].isnull() , 'Item_Weight'] = item_train['Item_MRP']/item_train['Item_MRP_per_unit_weight']\nitem_test.loc[ item_test['Item_Weight'].isnull() , 'Item_Weight'] = item_test['Item_MRP']/item_test['Item_MRP_per_unit_weight']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Still Some items are having null values, because they were the only record with respect to Item_Identifier\n# and thats why group mean did not imputed null values. For these we will use KNN imputation\nitem_train.loc[ item_train['Item_Weight'].isnull() ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_train_knn = item_train[['Item_Weight','Item_Visibility','Item_MRP']]\nitem_test_knn = item_test[['Item_Weight','Item_Visibility','Item_MRP']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling of the numerical data\nfrom sklearn.preprocessing import StandardScaler\nst = StandardScaler()\nitem_train_knn = pd.DataFrame( st.fit_transform(item_train_knn), columns=item_train_knn.columns )\nitem_test_knn = pd.DataFrame(st.transform(item_test_knn), columns=item_test_knn.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN imputation of the data\nfrom sklearn.impute import KNNImputer\nknn = KNNImputer()\nitem_train_knn = pd.DataFrame(knn.fit_transform(item_train_knn), columns=item_train_knn.columns)\nitem_test_knn = pd.DataFrame(knn.transform(item_test_knn), columns=item_test_knn.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_train_knn.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_train = item_train.drop(columns=['Item_MRP_per_unit_weight','MRP_null'])\nitem_train[item_train_knn.columns] = item_train_knn\nitem_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_test = item_test.drop(columns=['Item_MRP_per_unit_weight','MRP_null'])\nitem_test[item_test_knn.columns] = item_test_knn\nitem_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Outlet Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"outlet_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing values percentage wise\noutlet_train.isnull().sum()/outlet_train.shape[0]*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Value count of outlet size in data set\noutlet_train['Outlet_Size'].value_counts()\n\nsns.countplot(outlet_train['Outlet_Size'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outlet_train['outlet_null'] = outlet_train['Outlet_Size'].isnull()\noutlet_test['outlet_null'] = outlet_test['Outlet_Size'].isnull()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# All the records and null values grouped by outlet_type, outlet_location_type, establishment year\noutlet_train.drop(columns=['Outlet_Identifier']).groupby(['Outlet_Type','Outlet_Location_Type','Outlet_Establishment_Year']).count()\n\n# We can see some particular pattern is present for the missing values.\n# Only Grocery store in Tier 3 data is missing\n# and Supermarket Type1 in Tier 2 data is missing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# All the missing outlet_size values are belongs to come particular outler_identifier\noutlet_train.groupby(by='Outlet_Identifier').count()\n\n# Those missing values are actually belongs to particular outlet_identifier\n# only 3 outlet_identifier is having missing values i.e. OUT010, OUT017 and OUT045","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Every outlet description\noutlet_train[ outlet_train.duplicated() == False ].sort_values(by='Outlet_Identifier')\n\n# For OUT017 and OUT045 both belongs to Tier2 and Supermarket Type1. We can inpute the missing values with Small\n# since for OUT035, also belongs to Tier2 and Supermarket Type1 and have 'Small' outlet size.\n\n# For 'OUT010', imputing it with 'Small' since grocery store would be smaller in size as compared to supermarkets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imputing outlet size with 'Small'\noutlet_train.loc[ (outlet_train['Outlet_Size'].isnull()) , 'Outlet_Size' ] = 'Small'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"outlet_train.loc[ (outlet_train['Outlet_Identifier'] == 'OUT017') , 'Outlet_Size' ] = 'Small'\noutlet_train.loc[ (outlet_train['Outlet_Identifier'] == 'OUT045') , 'Outlet_Size' ] = 'Small'\noutlet_train.loc[ (outlet_train['Outlet_Identifier'] == 'OUT010') , 'Outlet_Size' ] = 'Small'"},{"metadata":{},"cell_type":"raw","source":"outlet_test.loc[ (outlet_test['Outlet_Identifier'] == 'OUT017') , 'Outlet_Size' ] = 'Small'\noutlet_test.loc[ (outlet_test['Outlet_Identifier'] == 'OUT045') , 'Outlet_Size' ] = 'Small'\noutlet_test.loc[ (outlet_test['Outlet_Identifier'] == 'OUT010') , 'Outlet_Size' ] = 'Small'"},{"metadata":{"trusted":true},"cell_type":"code","source":"outlet_train = outlet_train.drop(columns=['outlet_null'])\noutlet_test = outlet_test.drop(columns=['outlet_null'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Joining Item /Outlet"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['Item_Outlet_Sales']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = pd.concat([item_train, outlet_train], axis=1)\nx.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.concat([item_test, outlet_test], axis=1)\ntest.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.concat([x,y], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of sales by outlet_identifier\nplt.figure(figsize=(12,5))\nsns.countplot(x = df1['Outlet_Identifier'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of sales by Fat_count\nplt.figure(figsize=(5,5))\nsns.countplot(x = df1['Item_Fat_Content'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking correlation\nsns.heatmap( df1.corr(), annot=True )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see item_mrp is aving mild positive correlaton with item_outlet_sales\nplt.figure(figsize=(15,5))\nsns.scatterplot(x = df1['Item_MRP'], y = df1['Item_Outlet_Sales'])\nplt.show()\n\n# As the MRP is high, Sales is also getting higher.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.scatterplot(x = df1['Item_Visibility'], y = df1['Item_Outlet_Sales'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.boxplot(x='Item_Type',y='Item_Outlet_Sales',data=df1)\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.boxplot(x='Outlet_Identifier',y='Item_Outlet_Sales',data=df1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## One Hot Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing Unique columns i.e, Item_Identifier\nx = x.drop(columns=['Item_Identifier'])\ntest = test.drop(columns=['Item_Identifier'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Chaning outlet_establishment_year to numerical by changing it to how long it was operating.\nx['Outlet_Establishment_Year'] = x['Outlet_Establishment_Year'].apply(lambda x : 2020-x)\n\nst = StandardScaler()\n\nx['Outlet_Establishment_Year'] = st.fit_transform(x[['Outlet_Establishment_Year']])\n\ntest['Outlet_Establishment_Year'] = test['Outlet_Establishment_Year'].apply(lambda x : 2020-x)\ntest['Outlet_Establishment_Year'] = st.transform(test[['Outlet_Establishment_Year']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since Outlet size is ordinal to changing it to -1, 0 and 1 for small , medium and high\nx['Outlet_Size'] = x['Outlet_Size'].map({'Small':-1, 'Medium':0, 'High':1})\ntest['Outlet_Size'] = test['Outlet_Size'].map({'Small':-1, 'Medium':0, 'High':1})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"# Since Outlet_Location_Type\nx['Outlet_Location_Type'] = x['Outlet_Location_Type'].map({'Tier 1':-1, 'Tier 2':0, 'Tier 3':1})\ntest['Outlet_Location_Type'] = test['Outlet_Location_Type'].map({'Tier 1':-1, 'Tier 2':0, 'Tier 3':1})"},{"metadata":{"trusted":true},"cell_type":"code","source":"def f1(data):\n    num_data = data.select_dtypes(include=np.number)\n    cat_data = data.select_dtypes(exclude=np.number)\n    \n    cat_data = pd.get_dummies(cat_data, drop_first=True)\n    data = pd.concat([num_data, cat_data], axis=1, )\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = f1(x)\ntest = f1(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, random_state=16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\n\nlr.fit(xtrain, ytrain)\n\nytrain_pred = lr.predict(xtrain)\nytest_pred = lr.predict(xtest)\n\nprint(r2_score(ytrain, ytrain_pred))\nprint(mean_squared_error(ytrain, ytrain_pred)**0.5)\n\nprint(r2_score(ytest, ytest_pred))\nprint(mean_squared_error(ytest, ytest_pred)**0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom scipy.stats import randint as sp_randint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtree = DecisionTreeRegressor() # estimator\n\n\nparam_dist = {'max_depth':sp_randint(1,20),\n             'min_samples_leaf':sp_randint(1,50),\n              'min_samples_split':sp_randint(2,50)}\n\n\nrsearch  = RandomizedSearchCV(dtree, param_distributions = param_dist, cv=4) \n\nrsearch.fit(x,y)\nrsearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtree_rand_tuned = DecisionTreeRegressor(**rsearch.best_params_)\ndtree_rand_tuned.fit(xtrain,ytrain)\n\n\nytrain_pred = dtree_rand_tuned.predict(xtrain)\nprint('RMSE on train data: ', mean_squared_error(ytrain, ytrain_pred)**0.5 )\nprint('R^2 on train data: ', r2_score(ytrain, ytrain_pred))\n\n\nytest_pred = dtree_rand_tuned.predict(xtest)\nprint('RMSE on test data: ', mean_squared_error(ytest, ytest_pred)**0.5 )\nprint('R^2 on test data: ', r2_score(ytest, ytest_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extra Tree Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"etr = ExtraTreesRegressor()\n\n\nparam_dist = { 'n_estimators':sp_randint(50,100),\n              'max_features': sp_randint(1,25),\n              'max_depth' : sp_randint(5,20),\n             'min_samples_leaf':sp_randint(10,50),\n              'min_samples_split':sp_randint(2,50)}\n\n\nrsearch_etr  = RandomizedSearchCV(estimator=etr, param_distributions = param_dist, cv=4, random_state=4) \n\nrsearch_etr.fit(x,y)\nrsearch_etr.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"etr_tuned = ExtraTreesRegressor(**rsearch_etr.best_params_)\netr_tuned.fit(xtrain, ytrain)\n\nytrain_pred = etr_tuned.predict(xtrain)\nprint('RMSE on train data: ', mean_squared_error(ytrain, ytrain_pred)**0.5) \nprint('R^2 on train data: ', r2_score(ytrain, ytrain_pred))\n\nytest_pred = etr_tuned.predict(xtest)\nprint('RMSE on test data: ', mean_squared_error(ytest, ytest_pred)**0.5)\nprint('R^2 on test data: ', r2_score(ytest, ytest_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor()\n\n\nparam_dist = { 'n_estimators':sp_randint(100,150),\n              'max_features': sp_randint(1,10),\n              'max_depth' : sp_randint(5,20),\n             'min_samples_leaf':sp_randint(10,50),\n              'min_samples_split':sp_randint(2,50)}\n\n\nrsearch_rf  = RandomizedSearchCV(estimator=rf, param_distributions = param_dist, cv=4, random_state=16) \n\nrsearch_rf.fit(x,y)\nrsearch_rf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf= RandomForestRegressor(**rsearch_rf.best_params_)\n\nrf.fit(xtrain, ytrain)\n\nytrain_pred = rf.predict(xtrain)\nprint('RMSE on train data: ', mean_squared_error(ytrain, ytrain_pred)**0.5 )\nprint('R^2 on train data: ', r2_score(ytrain, ytrain_pred))\n\n\nytest_pred = rf.predict(xtest)\nprint('RMSE on test data: ', mean_squared_error(ytest, ytest_pred)**0.5 )\nprint('R^2 on test data: ', r2_score(ytest, ytest_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gradient Boost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr = GradientBoostingRegressor(criterion='mse')\ngbr.fit(xtrain, ytrain)\n\nytrain_pred = gbr.predict(xtrain)\nprint('RMSE on train data: ', mean_squared_error(ytrain, ytrain_pred)**0.5 )\nprint('R^2 on train data: ', r2_score(ytrain, ytrain_pred))\n\n\nytest_pred = gbr.predict(xtest)\nprint('RMSE on test data: ', mean_squared_error(ytest, ytest_pred)**0.5 )\nprint('R^2 on test data: ', r2_score(ytest, ytest_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\nlgbmc = lgb.LGBMRegressor()\n\nparams = {\n    'n_estimators': sp_randint(100, 200),\n    'learning_rate': sp_uniform(0, 0.5),\n    'max_depth': sp_randint(1, 15),\n    'num_leaves': sp_randint(10, 50)}\n\nrsearch_lg = RandomizedSearchCV(lgbmc, param_distributions=params, cv=4, n_iter=50, random_state=4)\n\nrsearch_lg.fit(x, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbr = lgb.LGBMRegressor(**rsearch_lg.best_params_, random_state=4)  \n\n\nlgbr.fit(xtrain, ytrain)\n\nytrain_pred = lgbr.predict(xtrain)\nytest_pred = lgbr.predict(xtest)\n\n\nytrain_pred = rf.predict(xtrain)\nprint('RMSE on train data: ', mean_squared_error(ytrain, ytrain_pred)**0.5 )\nprint('R^2 on train data: ', r2_score(ytrain, ytrain_pred))\n\n\nytest_pred = rf.predict(xtest)\nprint('RMSE on test data: ', mean_squared_error(ytest, ytest_pred)**0.5 )\nprint('R^2 on test data: ', r2_score(ytest, ytest_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stacking Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_tuned = RandomForestRegressor(**rsearch_rf.best_params_)\netr_tuned = ExtraTreesRegressor(**rsearch_etr.best_params_)\nlgbr = lgb.LGBMRegressor(**rsearch_lg.best_params_)\ngbr = GradientBoostingRegressor(criterion='mse')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = [('rf_tuned', rf_tuned),('etr_tuned',etr_tuned),('lgbr', lgbr), ('gbr',gbr)]\n\nstack1 = VotingRegressor(estimators=estimators)\n\nstack1.fit(xtrain, ytrain)\n\nytrain_pred = stack1.predict(xtrain)\nprint('RMSE on train data: ', mean_squared_error(ytrain, ytrain_pred)**0.5) \nprint('R^2 on train data: ', r2_score(ytrain, ytrain_pred))\n\nytest_pred = stack1.predict(xtest)\nprint('RMSE on test data: ', mean_squared_error(ytest, ytest_pred)**0.5)\nprint('R^2 on test data: ', r2_score(ytest, ytest_pred))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}