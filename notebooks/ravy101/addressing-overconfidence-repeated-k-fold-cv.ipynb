{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><center>Addressing Overconfidence: Repeated K-Fold CV</center></h1>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Introduction\nI have seen quite a few public notebooks for this task that report unrealistically high accuracies and AUC metrics. These notebooks rely on using a specific train test split random state so that the test data is very easy to predict, rather than producing high-quality generalizable models. I thought it would be a good idea to explore whats really going on and how dangerous it is to trust models produced in this way. Then I will demonstrate how easy it is to implement a more realistic method of evaluating accuracy and apply that to some of the high-performing models shared by other users.\n\nThis is not intendended to disparage the great contributions of other users participating in this task but to help fellow Kagglers avoid overconfidence and disappointment.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Import Required Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\nimport statistics\nimport os\n\nimport matplotlib.pyplot as plt\nimport xgboost\nimport lightgbm\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_val_predict\nfrom sklearn import linear_model\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read Dataset","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"heart_df = pd.read_csv(\"/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")\nheart_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting Started\n\nSince there are already a number of very well presented explorations of this dataset we will skip EDA and feature selection, going directly to model fitting - where we encounter the problem at hand. We start by performing a simple train-test split and fitting a basic model as seen in [this](https://www.kaggle.com/nayansakhiya/heart-fail-analysis-and-quick-prediction-96-rate) very nice notebook.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_features = ['time','ejection_fraction','serum_creatinine','age']\nX = heart_df[selected_features]\nX_all_features = heart_df[heart_df.columns.difference(['DEATH_EVENT'])]\ny = heart_df['DEATH_EVENT']\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2698)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting Simple Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nr_clf = RandomForestClassifier(max_features=0.5, max_depth=15, random_state=1)\nr_clf.fit(x_train, y_train)\nacc =  r_clf.score(x_test,y_test)\nprint(f\"Random Forest Test Accuracy: {round(acc*100, 3)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mission Accomplished! Or is it?\n\nOur Random Forest Classifier with just 4 features scores accuracy over 96% on unseen test data - a fabulous result. Since the test data is unseen we expect our model to have comparable accuracy when we share it with colleagues and reach deployment, right?\n\nThis is a very dangerous trap!\n\nEven though the test data is not seen by the model, the test accuracy is seen by the person choosing the random state, and that person likes high test accuracy numbers.\n\nLets fit a model with exactly the same parameters to different random splits of the data to see what accuracy we might expect if our random state was actually random.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Same parameters for the RF Classifier\nr_clf = RandomForestClassifier(max_features=0.5, max_depth=15, random_state=1)\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\nr_clf.fit(x_train, y_train)\nacc =  r_clf.score(x_test,y_test)\nprint(f\"Random Forest Test 1 Accuracy: {round(acc*100, 3)}\")\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4321)\nr_clf.fit(x_train, y_train)\nacc =  r_clf.score(x_test,y_test)\nprint(f\"Random Forest Test 2 Accuracy: {round(acc*100, 3)}\")\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1337)\nr_clf.fit(x_train, y_train)\nacc =  r_clf.score(x_test,y_test)\nprint(f\"Random Forest Test 3 Accuracy: {round(acc*100, 3)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yikes! Thats not good, accuracy in all three cases is more than 10% lower than before, but are we worried? Is it possible that the model generated from our first seed is just a better, more generalizable model?\n\nIt is possible but unlikely, lets try create a situation where we can check. This time we will split the model into 3 parts train, validation and test, then try find the seed that gives great validation accuracy and see what this implies for test accuracy. \n\nStarting with an initial split, the test set from this first split we will set aside for later, it represents what we might encounter as real world data.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#start with initial split\nx_train_val, x_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=1111)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will split the remaining data and this time we will exploit the random state to get high validation accuracy:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1000):\n    x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.25, random_state=i)\n    # Same parameters for the RF Classifier\n    r_clf = RandomForestClassifier(max_features=0.5, max_depth=15, random_state=1)\n    r_clf.fit(x_train, y_train)\n    acc =  r_clf.score(x_val,y_val)*100   \n    if acc > 94:\n        print(f\"Random Forest Val Accuracy: {round(acc, 3)}\")\n        print(f\"High Accuracy at: {i}\")\n        break\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OK! Our cheesy exploits have produced a model with high validation accuracy, lets see how it generalizes to the test set we split out earlier:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = r_clf.score(x_test,y_test)*100\nprint(f\"Random Forest Test Accuracy: {round(acc, 3)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Oh No!\n\nEven though our model scored high accuracy on the validation set, we lost more than 16% accuracy moving to test. If this test set is representative, we might have had a very embarrassing deployment. We definitely can't trust accuracy gained by changing the random state.\n\n**Random state is for reproducability not optimization!** Selecting a split that gives great test results does not make your model better and will lead us into all kinds of embarrassing disappointments. \n\nSo we know not to manipulate the random state, but it can still happen by chance! \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## How Can We Be More Confident in Our Models?\n\nWe saw above that using a 3-way train, test, validation split can help to avoid overconfidence by giving us an additional unseen data check, but in order to really have confidence in the accuracy of our models there are a number of tools at our disposal.\n\nNow we will demonstrate how easy it is to get a more stable and reliable estimate of our models accuracy using 3 ideas, all of which are well supported by sklearn:\n1. Stratification - Ensuring the test data has representative propotions of the response variable.\n2. K-Fold Cross Validation -  Spliting data into K equal groups, where each group is used as the test set once as multiple models are fit.\n3. Repeated Estimates - Repeating the entire process and aggregating the accuracy to reduce random noise.\n\nPlease read [this](https://machinelearningmastery.com/k-fold-cross-validation/) excellent article for a complete explanation.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We set our random seed once in our setup section and never touch it.\n\n### Setup","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(19566390)\nFOLDS = 5\nREPEATS = 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will define a function that takes a model, a dataset and some options and applies repeated cross validation scoring. This kind of function will work for models/pipelines that conform to the sklearn style interface.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def repeat_cross_validation_accuracy(model, x, y, n_folds = 5, n_repeats = 5, metric = 'accuracy'):\n    oof_acc = []\n    oof_predictions = []\n    for i in range(n_repeats):\n        kf = StratifiedKFold(n_folds, shuffle=True)\n        acc = cross_val_score(model, x.values, y=y.values,scoring=metric, cv = kf)\n        oof_acc.append(acc.mean())\n        predictions = cross_val_predict(model, x, y=y.values, cv=kf)\n        oof_predictions.append(predictions)\n    return oof_acc, oof_predictions ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will setup a number of models and select features based on the public notebooks shared by other Kaggle users reporting 90% + accuracy.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nselected_features = ['time','ejection_fraction','serum_creatinine','age']\n\nX = heart_df[selected_features]\n#X_all_features = heart_df[heart_df.columns.difference(['DEATH_EVENT'])]\ny = heart_df['DEATH_EVENT']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = RandomForestClassifier(max_features=0.5, max_depth=15)\nknn_model = KNeighborsClassifier(n_neighbors=6)\ndt_model = DecisionTreeClassifier(max_leaf_nodes=10, criterion='entropy')\ngb_model = GradientBoostingClassifier(max_depth=2 )\nxgb_model = xgboost.XGBRFClassifier(max_depth=3 )\nlgb_model = lightgbm.LGBMClassifier(max_depth=2)\ncat_model = CatBoostClassifier(verbose=0)\n\nmodels = dict()\nmodels['Random Forest'] = rf_model\nmodels['KNN'] = knn_model\nmodels['Decision Tree'] = dt_model\nmodels['Gradient Boosting'] = gb_model\nmodels['XGB'] = xgb_model\nmodels['LGB'] = lgb_model\nmodels['Cat Boost'] = cat_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will fit and score the models multiple times to get reliable estimate of out-of-fold (OOF) accuracy.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies = []\ntraining_times = []\n\nprint(f\"Fitting models with {REPEATS} iterations of {FOLDS} fold CV\\n\")\nfor k in models.keys():\n    print(f\"####################################\\nTraining Model: {k}\")\n    start = time.time()\n    acc, preds = repeat_cross_validation_accuracy(models[k], X, y, n_folds = FOLDS, n_repeats = REPEATS)\n    end = time.time()\n    elapsed = end - start\n    print(f\"Total Training Time: {round(elapsed,4)} seconds\")\n    print(f\"Mean OOF Accuracy: {round(statistics.mean(acc)*100, 2)} %\")\n    print(f\"####################################\\n\\n\")\n    accuracies.append(statistics.mean(acc))\n    training_times.append(elapsed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results Table","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"res_df = pd.DataFrame({'Model': list(models.keys()), 'Accuracy': accuracies, f\"Train Time ({FOLDS} by {REPEATS})\": training_times})\nres_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nAccuracy estimates for all models are significantly lower than the previously reported values but better reflect what we would expect to find on actually unseen data.\n\nIts obvious that a method like this using repeated cross validation requires significantly more computation and resources, and this kind of approach may not be suitable for models that require extensive training. However, as we can see in the results table and the code above, the training time and coding required to obtain a more reliable measure of accuracy for most models is trivial even using 5 repeats of 5 fold CV.\n\nThere are also concerns about the time variable and whether it is valid to include this in predictive models, I may revisit this notebook to address that at a later date.\n\nThanks to everyone who has contributed public notebooks and participated in discussions regarding this task. I hope someone finds this helpful.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}