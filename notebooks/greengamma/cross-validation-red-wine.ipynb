{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Mapping the \"quality\" column from 0 to 6","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"quality\"].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quality_mapping = {\n    3: 0,\n    4: 1,\n    5: 2,\n    6: 3,\n    7: 4,\n    8: 5,\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[:, \"quality\"] = df.quality.map(quality_mapping)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Splitting the dataset into a \"train\" and a \"test\" set","metadata":{}},{"cell_type":"code","source":"# Shuffling the dataset\ndf = df.sample(frac=1).reset_index(drop=True)\n\n# Selecting the first 1,000 rows for \"train\"\ndf_train = df.head(1000)\n\n# Selecting the remaining 599 rows for \"test\"\ndf_test = df.tail(599)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Training a Decision Tree model","metadata":{}},{"cell_type":"code","source":"from sklearn import tree\nfrom sklearn import metrics\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nclf = tree.DecisionTreeClassifier(max_depth=7)\ncols = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n       'pH', 'sulphates', 'alcohol']\n\nclf.fit(df_train[cols], df_train.quality)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Testing the accuracy of the model","metadata":{}},{"cell_type":"code","source":"# Predictions on the train set\ntrain_predictions = clf.predict(df_train[cols])\n# Predictions on the test set\ntest_predictions = clf.predict(df_test[cols])\n# Calculating accuracy\ntrain_accuracy = metrics.accuracy_score(df_train[\"quality\"], train_predictions)\n# Calculating accuracy of predictions on test set\ntest_accuracy = metrics.accuracy_score(df_test[\"quality\"], test_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.1 Creating a plot with different values for \"max_depth\"","metadata":{}},{"cell_type":"code","source":"matplotlib.rc(\"xtick\", labelsize=20)\nmatplotlib.rc(\"ytick\", labelsize=20)\n\ntrain_accuracies = [0.5]\ntest_accuracies = [0.5]\n\nfor depth in range(1, 25):\n    clf = tree.DecisionTreeClassifier(max_depth=depth)\n    cols = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n       'pH', 'sulphates', 'alcohol']\n    \n    # Train model\n    clf.fit(df_train[cols], df_train.quality)\n    # Create train and test predictions\n    train_predictions = clf.predict(df_train[cols])\n    test_predictions = clf.predict(df_test[cols])\n    # Calculate training and test accuracies\n    train_accuracy = metrics.accuracy_score(df_train[\"quality\"], train_predictions)\n    test_accuracy = metrics.accuracy_score(df_test[\"quality\"], test_predictions)\n    # Append accuracies\n    train_accuracies.append(train_accuracy)\n    test_accuracies.append(test_accuracy)\n    \n# Create plots\nplt.figure(figsize=(10, 5))\nsns.set_style(\"whitegrid\")\nplt.plot(train_accuracies, label=\"train accuracy\")\nplt.plot(test_accuracies, label=\"test accuracy\")\nplt.legend(loc=\"upper left\", prop={\"size\" : 15})\nplt.xticks(range(0, 26, 5))\nplt.xlabel(\"max_depth\", size=20)\nplt.ylabel(\"accuracy\", size=20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### => Use a \"max depth\" of 11 for the DecisionTreeClassifier to achieve highest accuracy","metadata":{}},{"cell_type":"code","source":"b = sns.countplot(x=\"quality\", data=df)\nb.set_xlabel(\"quality\", fontsize=20)\nb.set_ylabel(\"count\", fontsize=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### => Choose a stratified k-fold because of unequal distribution!","metadata":{}}]}