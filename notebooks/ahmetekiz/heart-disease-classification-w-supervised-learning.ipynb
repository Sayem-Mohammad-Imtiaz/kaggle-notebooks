{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nHeart disease describes a range of conditions that affect your heart. Diseases under the heart disease umbrella include blood vessel diseases, such as coronary artery disease; heart rhythm problems (arrhythmias); and heart defects you're born with (congenital heart defects), among others.\n\nWe have patients data that shows us a few types of symptoms about heart diseases. We want to classify who has heart disease or not. \nSo, we can use some supervised learning techniques.\n\n<font color = 'blue'>\nContent:\n    \n1. [Load and Check Data](#1)\n1. [Variable Description](#2)\n1. [Take a Look Data](#3)\n1. [Supervised Learning](#4)\n    * [Logistic Regression Classification](#5)\n    * [K-Nearest Neighbour (KNN) Classification](#6)\n    * [Support Vector Machine (SVM) Classification](#7)\n    * [Naive Bayes Classification](#8)\n    * [Decision Tree Classification](#9)\n    * [Random Forest Classification](#10)\n1. [Comparison of Accuracy](#11)\n1. [Confusion Matrix](#12)\n1. [Conclusion](#13)\n    ","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '1'></a><br>\n# Load and Check Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/heart-disease-uci/heart.csv\")  # patients data frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '2'></a><br>\n# Variable Description","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- **age:** age in years \n- **sex:** (1 = male; 0 = female) \n- **cp:** chest pain type (4 values)\n- **trestbps:** resting blood pressure (in mm Hg on admission to the hospital) \n- **chol:** serum cholestoral in mg/dl\n- **fbs:** fasting blood sugar > 120 mg/dl (1 = true; 0 = false) \n- **restecg:** resting electrocardiographic results (values 0,1,2)\n- **thalach:** maximum heart rate achieved\n- **exang:** exercise induced angina (1 = yes; 0 = no) \n- **oldpeak:** ST depression induced by exercise relative to rest \n- **slope:** the slope of the peak exercise ST segment\n- **ca:** number of major vessels (0-3) colored by flourosopy\n- **thal:** 3 = normal; 6 = fixed defect; 7 = reversable defect\n- **target:** 0 = has heart disease; 1 = no heart disease.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = '3'></a><br>\n# Take a Look Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# libraries for Visualization\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = ['M' if i == 1 else 'F' for i in df.sex.value_counts().index]\ncolors = ['gray','red']\nexplode = [0,0.1]\nsizes = [df.sex.value_counts().values]\n\n# visual\nplt.figure(figsize = (8,8))\nplt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%')   # matplot methodu , 'autopct' ile 1 tane ondalik kismini gostermek icin\nplt.title('Sex Ratio of Patients',color = 'blue',fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(df.age)\nplt.title(\"Age Distribution\",color = 'black',fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nheart_disease = ['Has Heart Disease' if i == 0 else 'No Heart Disease' for i in df.target]\n\ndf_1 =  pd.DataFrame({'target':heart_disease})\n\nplt.figure(figsize=(10,7))\n\nsns.countplot(x = df_1.target)\nplt.ylabel('Count')\nplt.xlabel('Has Heart Disease or No Heart Disease')\nplt.title('Heart Disease Count',color = 'blue',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '4'></a><br>\n# Supervised Learning\n\nIn this section, we'll use some of the widely used supervised learning classification algorithms. But first we'll prepare data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df.target.values\nx_data = df.drop([\"target\"],axis=1)\n\n# normalization \nx = ( x_data - np.min(x_data) ) / ( np.max(x_data) - np.min(x_data) ).values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data was normalized. We will split the data as test data and train data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# %% split data\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 42) # test data = 0.2 data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies = {} # Create a dictionary to save algorithms accuracies.\n\ndef save_score(name,score):\n    \"\"\"\n    Parameters\n    ----------\n    name : Algorithm name\n    score : Algorithm test score    \n\n    Returns\n    -------\n    None.\n    \"\"\"\n    accuracies[name] = score*100\n    \n    return None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '5'></a><br>\n## Logistic Regression Classification","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\n\nlr.fit(x_train,y_train)\n\nprint(\"Test Accuracy {}\".format(lr.score(x_test,y_test)))\n\nsave_score( \"Logistic Regression Classification\",lr.score(x_test,y_test) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color = 'red'>\nLogistic Regression Classification algorithm accuracy is %85.24.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = '6'></a><br>\n## K-Nearest Neighbour (KNN) Classification","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# knn model\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 3) # n_neighbors = k\n\nknn.fit(x_train,y_train)\n\nprediction = knn.predict(x_test)\n\nprint(\" {} nn score: {} \".format(3,knn.score(x_test,y_test)))\n\nsave_score( \"K-Nearest Neighbour (KNN) Classification\",knn.score(x_test,y_test) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color = 'red'>\nK-Nearest Neighbour (KNN) Classification algorithm  accuracy is %83.60 .","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# find k value\n\nscore_list = []\n\nfor each in range(1,50):\n    \n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    \n    knn2.fit(x_train,y_train)\n    \n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.figure(figsize=(30,6))   \nplt.plot(range(1,50),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '7'></a><br>\n## Support Vector Machine (SVM) Classification","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVM\n\nfrom sklearn.svm import SVC\n\nsvm = SVC(random_state = 1)\n\nsvm.fit(x_train,y_train)\n\nprint(\"print accuracy of svm algo: \",svm.score(x_test,y_test))\n\nsave_score( \"Support Vector Machine (SVM) Classification\",svm.score(x_test,y_test) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color = 'red'>\nSupport Vector Machine (SVM) Classification algorithm accuracy is %83.60 .","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = '8'></a><br>\n## Naive Bayes Classification","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\n\nnb.fit(x_train,y_train)\n\nprint(\"Accuracy of Naive Bayes Algo: \",nb.score(x_test,y_test))\n\nsave_score( \"Naive Bayes Classification\",nb.score(x_test,y_test) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color = 'red'>\nNaive Bayes Classification algorithm accuracy is %86.88 .","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = '9'></a><br>\n## Decision Tree Classification","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\n\ndt.fit(x_train,y_train)\n\nprint(\"score: \", dt.score(x_test,y_test))\n\nsave_score( \"Decision Tree Classification\",dt.score(x_test,y_test) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color = 'red'>\nDecision Tree Classification algorithm accuracy is %78.68 .","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = '10'></a><br>\n## Random Forest Classification","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 100,random_state = 1)  # n_estimater = kac agac olacak\n\nrf.fit(x_train,y_train)\n\nprint(\"random forest algo result: \",rf.score(x_test,y_test))\n\naccuracies[\"Random Forest Classification\"] = [rf.score(x_test,y_test)*100]\n\nsave_score( \"Random Forest Classification\",rf.score(x_test,y_test) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color = 'red'>\nRandom Forest Classification algorithm accuracy is %85.24 .","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = '11'></a><br>\n## Comparison of Accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,8))\nsns.barplot(x = list( accuracies.keys() ), y = list( accuracies.values() ) )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '12'></a><br>\n## Confusion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predictions\n\ny_lr_pred = lr.predict(x_test)\ny_knn_pred = knn.predict(x_test)\ny_svm_pred = svm.predict(x_test)\ny_nb_pred = nb.predict(x_test)\ny_dt_pred = dt.predict(x_test)\ny_rf_pred = rf.predict(x_test)\n\ny_true = y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ncm_lr = confusion_matrix(y_true,y_lr_pred)\ncm_knn = confusion_matrix(y_true,y_knn_pred)\ncm_svm = confusion_matrix(y_true,y_svm_pred)\ncm_nb = confusion_matrix(y_true,y_nb_pred)\ncm_dt = confusion_matrix(y_true,y_dt_pred)\ncm_rf = confusion_matrix(y_true,y_rf_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cm visualization\n\nplt.figure(figsize=(24,12))\n\nplt.suptitle(\"Confusion Matrixes\",fontsize=24)\nplt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n\nplt.subplot(2,3,1)\nplt.title(\"Logistic Regression Confusion Matrix\")\nsns.heatmap(cm_lr,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,2)\nplt.title(\"K Nearest Neighbors Confusion Matrix\")\nsns.heatmap(cm_knn,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,3)\nplt.title(\"Support Vector Machine Confusion Matrix\")\nsns.heatmap(cm_svm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,4)\nplt.title(\"Naive Bayes Confusion Matrix\")\nsns.heatmap(cm_nb,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,5)\nplt.title(\"Decision Tree Classifier Confusion Matrix\")\nsns.heatmap(cm_dt,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,6)\nplt.title(\"Random Forest Confusion Matrix\")\nsns.heatmap(cm_rf,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '13'></a><br>\n## Conclusion\nI am new with ML and just trying to apply what I have just learned. Thanks for reading.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}