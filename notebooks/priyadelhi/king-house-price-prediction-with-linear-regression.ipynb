{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing required libraries\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n# warnings triggered during the process of importing a module (ignored by default)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset contains:**\n\nId: a notation for a house\n\nDate: Date house was sold\n\nPrice: Price is prediction target\n\nBedrooms: Number of Bedrooms/House\n\nBathrooms: Number of bathrooms/House\n\nSqft_Living: square footage of the home\n\nSqft_Lot: square footage of the lot\n\nFloors: Total floors (levels) in house\n\nWaterfront: House which has a view to a waterfront\n\nView: Has been viewed\n\nCondition: How good the condition is ( Overall )\n\nGrade: overall grade given to the housing unit, based on King County grading system\n\nSqft_Above: square footage of house apart from basement\n\nSqft_Basement: square footage of the basement\n\nYr_Built: Built Year\n\nYr_Renovated: Year when house was renovated\n\nZipcode: Zip\n\nLat: Latitude coordinate\n    \nLong: Longitude coordinate\n\nSqft_Living15: Living room area in 2015(implies-- some renovations) This might or might not have affected the lotsize area\n\nSqft_Lot15: lotSize area in 2015(implies-- some renovations)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading of Dataset\ndf=pd.read_csv('../input/housesalesprediction/kc_house_data.csv')\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape\n# gives rows and column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# date column is in object datatype, we need to convert it after taking only year that is required for our analysis.\ndfnew=[ ]\nfor i in df['date'].values:\n    dfnew.append(i[0:4])\ndf['date']=dfnew\n# to extract year from date column\n\ndf['date']=df['date'].astype(int)  # the column was in object datatype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['age']=df['date']-df['yr_built']  \n# calculating age of house ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['yr_renovated']=df['yr_renovated'].apply(lambda x: 1 if x!=0 else 0)\n# treating 0 in this column as house is renovated or not","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['id','date','yr_built'],axis=1,inplace=True)\n\n# dropping not required columns in the dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()\n# isnull() is used to find the null values present in the system \n# sum() gives the total of (null values==True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.duplicated().sum()\n# checking for duplicate values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop_duplicates(inplace=True)\n# dropping duplicated record and also saving the changes by passing (inplace=True) parameter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.duplicated().sum()\n# cross-checking for duplicated values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_features =[]\n\nfor i , r in df.corr().iterrows():\n    k=0\n    for j in range(len(r)):\n        if i!= r.index[k]:\n            if r.values[k] >=0.5:\n                corr_features.append([i, r.index[k], r.values[k]])\n        k += 1\ncorr_features\n\n# correlaion between the columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat =[]\nfor i in corr_features:\n    if i[2] >= 0.8:\n        feat.append(i[0])\n        feat.append(i[1])\n        print(feat)\n        \n# highly correlated features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df.corr()\nplt.figure(figsize=(14,14))\nsns.heatmap(corr, annot=True, fmt= '.2f',annot_kws={'size': 15}, cmap= 'coolwarm')\nplt.show()\nprint(corr)\n\n# heatmap ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df.iloc[0:,1:].columns:\n    sns.boxplot(df[i],data=df)\n    print(i)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"L =[1,2,3,4,9,10,14]\ndef outlier(df):\n    for x in df.iloc[:,L].columns :        \n        Q1=df[x].quantile(0.25)\n        Q3=df[x].quantile(0.75)\n        IQR=Q3-Q1\n        Lower = Q1-(1.5*IQR)\n        Upper = Q3+(1.5*IQR)\n        df.loc[:,x]= np.where(df[x].values > Upper, Upper-1, df[x].values)\n        df.loc[:,x]= np.where(df[x].values < Lower, Lower+1, df[x].values)\n\noutlier(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df.iloc[:,1:].columns:\n    sns.boxplot(df[i],data=df)\n    print(i)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data Visualisation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=df['price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.30,random_state=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(X_train.head(),y_train.head(),X_test.head(),y_test.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#SCALING "},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.preprocessing import StandardScaler,MinMaxScaler\n# sc=StandardScaler()\n\n# X_train_transform= sc.fit_transform(X_train)\n# X_test_transform= sc.transform(X_test)\n\n# Scaling the dataframe using StandardScaler and can also use Minmax scaler also","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr=LinearRegression()\n\n# using LinearRegression Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(X_train,y_train)\n#fitting the model (training the model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_lr_train=lr.predict(X_train)\n# prediction on training data in order to evaluate model is not overfit or underfit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_lr=lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Evaluation Metrics for Regression Problem**\n#MSE(Mean Squared Error)\n#RMSE(Root MeanSquared Error)\n#MAE(Mean Absolute Error)\n#MAPE(Mean Absolute Percentage Error)\n#R2 Score\n#adjusted R2 score"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nprint('mean_squared_error',mean_squared_error(y_test,pred_lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nprint('mean_absolute_error',mean_absolute_error(y_test,pred_lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nRMS=mean_squared_error(y_test,pred_lr,squared=False)\nprint('RMS: ',RMS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nr2_score(y_test,pred_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nr2_score(y_train,pred_lr_train)\n# checking R2 score for training data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#our model is neither underfit nor overfit\n# lets see it in another model as R2 score is 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"KNN Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\nkn=KNeighborsRegressor(n_neighbors=3,metric='euclidean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# finding optimal value for k neighbors\nfrom sklearn.neighbors import KNeighborsRegressor\n\nerror_rate = []\nfor i in range(1,20):\n    knn1 = KNeighborsRegressor(n_neighbors=i)\n    knn1.fit(X_train,y_train)\n    pred_i = knn1.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\nplt.figure(figsize=(10,6))\nplt.plot(range(1,20),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=5)   ## plot() used to plot the points on the canvas\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')  ## X axis ki labelling hogi\nplt.ylabel('Error Rate')  ## Y-axis ki labelling hogi\ny_pred = knn1.predict(X_test) ## predict() use \n\n\n# method to find value of k","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kn.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_knn=kn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_knn_train=kn.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nr2_score(y_test,pred_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nr2_score(y_train,pred_knn_train)\n# checking R2 score for training data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# our model is neither underfit nor overfit","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":""},{"metadata":{},"cell_type":"markdown","source":"# **ASSUMPTIONS IN LINEAR REGRESSION MODEL**\n\nThere are basically 5 assumptions in Linear Regression\n1) Linearity of Model \n2) Heteroskedasticity\n3) Normal distribution of Error terms\n4) Multicollinearity\n5) AutoCorrelation Normal distribution of Error terms"},{"metadata":{"trusted":true},"cell_type":"code","source":" #1. Visualizing the differences between actual prices and predicted ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_test,\n            pred_lr , \n            color = 'red')\n\n\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices\")\nplt.title(\"Prices vs Predicted prices\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  2.  Checking residuals - Heteroskedasticity - The presence of non-constant variance in \n#     the error terms results n heteroskedasticity\n# # The  variance in the error terms should be costant. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(pred_lr,\n            y_test-pred_lr)\n\nplt.title(\"Predicted vs residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3. Checking Normality of errors\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(y_test-pred_lr)\n\nplt.title(\"Histogram of Residuals\")\nplt.xlabel(\"Residuals\")\nplt.ylabel(\"Frequency\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4. Check for Multicollinearity\n\n# \"multicollinearity\" refers to predictors that are correlated with other predictors\n\n# * If the VIF is equal to 1 there is no multicollinearity among factors,\n#   but if the VIF is greater than 1, the predictors may be moderately correlated. \n  \n# *  A VIF between 5 and 10 indicates high multicollinearity that may be problematic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[i for i in range(X.shape[1])] # Coz we want VIF score for all columns , by this command we \n# are genrating VIF score for each column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VIF(array , index)\nh = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nprint(h)\n\npd.DataFrame(h, index=X.columns, columns = ['VIF Score'])\n\n# VIF < 4 - no multicollinearity\n# VIF > 4 - 10 - multicollinearity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#5   Autocorrelation(Durbin- Watson Test)\nDW Statistic Test\nif DW<=2  (no auto correlation)\n   2< DW<10 (auto correlation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.stattools import durbin_watson\ny_test - pred_lr # SKLEARN LINEAR REGRESSION PREDICTED VALUE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"durbin_watson(y_test-pred_lr)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}