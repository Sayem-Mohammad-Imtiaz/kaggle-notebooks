{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier,GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom mlxtend.classifier import StackingClassifier\nfrom sklearn import model_selection\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nsns.set(color_codes=True) # adds a nice background to the graphs\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/fetal-health-classification/fetal_health.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10).style.background_gradient(cmap=\"RdYlBu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Univariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check if there is any duplicates in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dup = df.drop_duplicates(subset = None , keep = 'first', inplace = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dup.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Target = df[\"fetal_health\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(15, 15))\n    ax = sns.heatmap(corr,mask=mask,square=True,linewidths=2.5,cmap=\"viridis\",annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is strong correlation between baseline value and histogram mode , histogram median and histogram mean.\n\nHistogram number of peaks and histogram width is also having good correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(Target)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Count of type 1.0 fetal health in the dataset \",len(df.loc[df[\"fetal_health\"]==1.0]))\nprint(\"Count of type 2.0 fetal health in the dataset \",len(df.loc[df[\"fetal_health\"]==2.0]))\nprint(\"Count of type 3.0 fetal health in the dataset \",len(df.loc[df[\"fetal_health\"]==3.0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"updated_cols = list(df.columns)\nfor column in updated_cols:\n    print(column,\" : \", len(df.loc[df[column]<0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_dup.iloc[:,:-1]\ny = df_dup.iloc[:,-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scale = StandardScaler()\nX = scale.fit_transform(X)\nX = pd.DataFrame(X,columns=df_dup.iloc[:,:-1].columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random sample Imputer for improving the class imbalance in the target column."},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\nROS = RandomOverSampler(random_state=42)\nX_ros, y_ros = ROS.fit_resample(X,y)\nfrom collections import Counter\nprint('Resampled dataset shape %s' % Counter(y_ros))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nX = sm.add_constant(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 10, test_size = 0.2)\n\nprint('X_train', X_train.shape)\nprint('y_train', y_train.shape)\n\nprint('X_test', X_test.shape)\nprint('y_test', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"{0:0.2f}% data is in training set\".format((len(X_train)/len(df.index)) * 100))\nprint(\"{0:0.2f}% data is in test set\".format((len(X_test)/len(y.index)) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_report(model):\n    \n    train_pred = model.predict(X_train)\n    return(classification_report(y_train, train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_test_report(model):\n    test_pred = model.predict(X_test)\n    return(classification_report(y_test, test_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Decision Tree Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree_classification = DecisionTreeClassifier(criterion = 'entropy', random_state = 10)\ndecision_tree = decision_tree_classification.fit(X_train, y_train)\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_report = get_train_report(decision_tree)\nprint(train_report)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_report = get_test_report(decision_tree)\nprint(test_report)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_model = DecisionTreeClassifier(criterion = 'gini',\n                                  max_depth = 5,\n                                  min_samples_split = 4,\n                                  max_leaf_nodes = 6,\n                                  random_state = 10)\n\n# fit the model using fit() on train data\ndecision_tree = dt_model.fit(X_train, y_train)\ntrain_report = get_train_report(decision_tree)\n\n# print the performance measures\nprint('Train data:\\n', train_report)\ntest_report = get_test_report(decision_tree)\n\n# print the performance measures\nprint('Test data:\\n', test_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest**"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_classification = RandomForestClassifier(n_estimators = 10, random_state = 10)\n\n# use fit() to fit the model on the train set\nrf_model = rf_classification.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_report = get_train_report(rf_model)\nprint(train_report) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_report = get_test_report(rf_model)\nprint(test_report) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"important_features = pd.DataFrame({'Features': X_train.columns, \n                                   'Importance': rf_model.feature_importances_})\n\n# sort the dataframe in the descending order according to the feature importance\nimportant_features = important_features.sort_values('Importance', ascending = False)\n\n# create a barplot to visualize the features based on their importance\nsns.barplot(x = 'Importance', y = 'Features', data = important_features)\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.title('Feature Importance', fontsize = 15)\nplt.xlabel('Importance', fontsize = 15)\nplt.ylabel('Features', fontsize = 15)\n\n# display the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above bar plot, we can see that short term variability is the most important feature in the dataset."},{"metadata":{},"cell_type":"markdown","source":"**K Nearest Neighbors**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,roc_curve\nknn_classification = KNeighborsClassifier(n_neighbors = 3)\n\n# fit the model using fit() on train data\nknn_model = knn_classification.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_report = get_train_report(knn_model)\nprint(train_report) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_report = get_test_report(knn_model)\nprint(test_report)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuned_paramaters = {'n_neighbors': np.arange(1, 25, 2),\n                   'metric': ['hamming','euclidean','manhattan','Chebyshev']}\n \n# instantiate the 'KNeighborsClassifier' \nknn_classification = KNeighborsClassifier()\n\nknn_grid = GridSearchCV(estimator = knn_classification, \n                        param_grid = tuned_paramaters, \n                        cv = 5, \n                        scoring = 'accuracy')\n\n# fit the model on X_train and y_train using fit()\nknn_grid.fit(X_train, y_train)\n\n# get the best parameters\nprint('Best parameters for KNN Classifier: ', knn_grid.best_params_, '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best parameters for KNN Classifier:  {'metric': 'manhattan', 'n_neighbors': 7}"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nerror_rate = []\n\n# use for loop to build a knn model for each K\nfor i in np.arange(1,25,2):\n    \n    # setup a knn classifier with k neighbors\n    # use the 'euclidean' metric \n    knn = KNeighborsClassifier(i, metric = 'euclidean')\n   \n    # fit the model using 'cross_val_score'\n    # pass the knn model as 'estimator'\n    # use 5-fold cross validation\n    score = cross_val_score(knn, X_train, y_train, cv = 5)\n    \n    # calculate the mean score\n    score = score.mean()\n    \n    # compute error rate \n    error_rate.append(1 - score)\n\n# plot the error_rate for different values of K \nplt.plot(range(1,25,2), error_rate)\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.title('Error Rate', fontsize = 15)\nplt.xlabel('K', fontsize = 15)\nplt.ylabel('Error Rate', fontsize = 15)\n\n# set the x-axis labels\nplt.xticks(np.arange(1, 25, step = 2))\n\n# plot a vertical line across the minimum error rate\nplt.axvline(x = 7, color = 'red')\n\n# display the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the optimal value of K = 7 obtained from the GridSearchCV results in a lowest error rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_report = get_train_report(knn_grid)\nprint(train_report) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_report = get_test_report(knn_grid)\nprint(test_report) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gaussian Naive Bayes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb = GaussianNB()\n\n# fit the model using fit() on train data\ngnb_model = gnb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_report = get_test_report(gnb_model)\nprint(test_report) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Adaboost Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ada_model = AdaBoostClassifier(n_estimators = 40, random_state = 10)\nada_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_report = get_test_report(ada_model)\nprint(test_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gradient Boosting Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"gboost_model = GradientBoostingClassifier(n_estimators = 150, max_depth = 10, random_state = 10)\ngboost_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_report = get_test_report(gboost_model)\nprint(test_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**XG Boost Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model = XGBClassifier(max_depth = 10, gamma = 1)\nxgb_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_report = get_test_report(xgb_model)\nprint(test_report)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#important_features = pd.DataFrame({'Features': X_train.columns, 'Importance': xgb_model.feature_importances_})\n#important_features = important_features.sort_values('Importance', ascending = False)\n\n#sns.barplot(x = 'Importance', y = 'Features', data = important_features)\n#plt.title('Feature Importance', fontsize = 15)\n#plt.xlabel('Importance', fontsize = 15)\n#plt.ylabel('Features', fontsize = 15)\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Support Vector Machine**"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_model = SVC(kernel='poly',probability=True)\nsvc_model.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_report = get_test_report(svc_model)\nprint(test_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Voting Classifier "},{"metadata":{"trusted":true},"cell_type":"code","source":"clf1 = KNeighborsClassifier(n_neighbors = 7 , weights = 'distance', metric='manhattan' )\nclf2 = GradientBoostingClassifier(n_estimators = 150,max_depth = 10,random_state=1)\n\nvotingclf = VotingClassifier(estimators=[('knn',clf1),('grb', clf2)],voting='hard')\nvotingclf = votingclf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_report = get_test_report(votingclf)\nprint(test_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We tried different algorithms for this dataset among them the boosting based algorithms i.e, XG Boost and Gradient boosting algorithms are performing best for the dataset with an accuracy of 94% on the test dataset and f1 score for XGBoost are 0.96 , 0.85 and 0.87 respectively for three different classes followed by Decision Tree Classifier without hypertuning it with an accuracy of 93% on the test data."},{"metadata":{},"cell_type":"markdown","source":"*PLEASE UPVOTE IF YOU LIKE THE ANALYSIS*"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}