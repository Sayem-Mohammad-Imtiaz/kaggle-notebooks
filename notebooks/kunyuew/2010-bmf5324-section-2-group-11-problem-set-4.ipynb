{"cells":[{"metadata":{},"cell_type":"markdown","source":"# SETUP"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# data wrangling\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, date, timedelta\n\n# data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n# offline interactive visualization\nfrom plotly.offline import plot, iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\n# regression\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nimport statsmodels.graphics.api as smg\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Data Wrangling\nimport numpy as np\nimport pandas as pd\nimport datetime as dt\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Worldometer data\n# ================\n\nworldometer_data = pd.read_csv('../input/corona-virus-report/worldometer_data.csv')\n\n# Replace missing values '' with NAN and then 0\nworldometer_data = worldometer_data.replace('', np.nan).fillna(0)\n\n# Correcting Country name \nworldometer_data['Country/Region'].replace({'USA':'US', 'UAE':'United Arab Emirates', 'S. Korea':'South Korea', \\\n                                           'UK':'United Kingdom'}, inplace=True)\n\n# Grouped by day, country\n# =======================\n\nfull_grouped = pd.read_csv('../input/corona-virus-report/full_grouped.csv')\n\n# Merge in population data\nfull_grouped = full_grouped.merge(worldometer_data[['Country/Region', 'Population']], how='left', on='Country/Region')\n\nfull_grouped['Date'] = pd.to_datetime(full_grouped['Date'], format = '%Y-%m-%d')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_grouped.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# When are the plateau and inflection points for South Korea?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# date = date of the most recent subwave of covid19 to project into the future\n# date format yyyy-mm-dd, e.g., '2020-07-04'\n\ndef plot_country(country, date): \n    temp = full_grouped[full_grouped['Country/Region']==country]\n    temp['recent_wave'] = np.where(temp['Date'] >= date,1,0)\n\n    fig = px.line(temp, x='Date', y='Confirmed', color='recent_wave', \\\n                  title = 'Infections for ' + str(country), height=600)      \n    fig.show()\n    \n    fig = px.line(temp, x='Date', y='Recovered', color='recent_wave', \\\n              title = 'Recovered Patients ' + str(country), height=600)      \n    fig.show()\n    \n    \n    return country, date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country, date = plot_country('South Korea', '2020-03-22')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def estimate_sir_param(country, date):\n    \n    # Assume everyone is at risk\n    # Identify the maximum population and the latest date in the time series for the country\n    population  = full_grouped[full_grouped['Country/Region']==country][\"Population\"].max()\n    latest_date = full_grouped[full_grouped['Country/Region']==country][\"Date\"].max()\n    \n    time_series_length = (latest_date - datetime.strptime(date,'%Y-%m-%d')).days + 1\n\n    temp = full_grouped[full_grouped['Country/Region']==country]\n    temp['recent_wave'] = np.where(temp['Date'] >= date,1,0)\n    \n    # Initialize Numpy arrays for total population (the maximum population), \n    # susceptible population (empty), and change in time (i.e., 1 day)\n    N  = np.array([population] * time_series_length)\n    S  = np.array([])\n    dt = np.array([1] * (time_series_length-1))\n\n    # Apply the condition N = S+I+(R+D)\n    # Filter time-series to those of the recent wave\n    I = np.array(temp[temp['recent_wave']==1]['Active'])\n    R = np.array(temp[temp['recent_wave']==1]['Recovered'])\n    D = np.array(temp[temp['recent_wave']==1]['Deaths'])\n    \n    # R includes both Recovered and Death for brevity\n    S = N - I - (R + D)\n\n    ## 1. Estimate beta\n    \n    x = (S * I) / N\n    \n    # Copy all elements except the last\n    x = x[:-1].copy()\n    \n    # Take the first difference\n    dS = np.diff(S)\n    y = dS/dt\n\n    # Fit into a linear regression\n    results = sm.OLS(y, x, missing='drop').fit()\n    beta = results.params\n    print(results.summary())\n    print('\\n')\n    print('*'*80)\n    print(f\"Transmission rate or Beta is: {beta}\")\n    print('*'*80)\n    \n    ## 2. Estimate gamma\n    \n    x = I[:-1].copy()\n    dR = np.diff(R+D)\n    y = dR/dt\n    \n    results = sm.OLS(endog=y, exog=x, missing='drop').fit()\n    gamma = results.params\n    print (results.summary())\n    print('\\n')\n    print('*'*80)\n    print(f\"Recovery (and Mortality) rate or Gamma is: {gamma}\")\n    print('*'*80)\n    \n    #3. Calculate R\n\n    print('\\n')\n    print('*'*80)\n    print(f\"Reproduction number or R is: {-beta/gamma}\")\n    print('*'*80)\n    \n    return -beta.astype('float'), gamma.astype('float'), datetime.strptime(date,'%Y-%m-%d').date()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"beta, gamma, date = estimate_sir_param(country, date)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sir_model(I0=0.01, beta=0.6, gamma=0.1, days=365, date=date.today()):\n    \"\"\"\n    Function will take in initial state for infected population,\n    Transmission rate (beta) and recovery rate(gamma) as input.\n    \n    The function returns the maximum percentage of infectious population,\n    the number of days to reach the maximum (inflection point),\n    the maximum percentage of population infected,\n    the number of days to reach 80% of the maximum percentage of population infected.\n    \n    \"\"\"\n    ## Initialize model parameters\n    N = 1          #Total population in percentage, i.e., 1 = 100%\n    I = I0         #Initial state of I default value 1% of population, i.e., I0 = 0.01\n    S = N - I      #Initial state of S\n    R = 0          #Initial State of R\n    C = I          #Initial State of Total Cases\n    beta  = beta   #Transmission Rate\n    gamma = gamma  #Recovery Rate\n    \n    ## Initialize empty lists\n    inf  = []       # List of Infectious population for each day\n    day  = []       # Time period in day\n    suc  = []       # List of Susceptible population for each day\n    rec  = []       # List of Recovered population for each day\n    conf = []       # List of Total Cases population for each day\n    \n    ## Project into the future\n    for i in range(days):\n        day.append(i)\n        inf.append(I)\n        suc.append(S)\n        rec.append(R)\n        conf.append(C)\n\n        new_inf= I*S*beta/N            #New infections equation (1)   \n        new_rec= I*gamma               #New Recoveries equation (2)\n        \n        I=I+new_inf-new_rec            #Total infectious population for next day\n        S=max(min(S - new_inf, N), 0)  #Total infectious population for next day\n        R=min(R + new_rec, N)          #Total recovered population for next day\n        \n        C=C+new_inf                    #Total confirmed cases for next day\n    \n    ## Pinpoint important milestones    \n    max_inf = round(np.array(inf).max()*100,2)        #Peak infectious population in percentage\n    inflection_day = inf.index(np.array(inf).max())   #Peak infectious population in days\n    max_conf = round(np.array(conf).max()*100,2)      #Overall infected population in percentage\n    plateau_day = np.array(np.where(np.array(conf) >= 0.8*np.array(conf).max())).min()   #Peak infectious population in days\n        \n    print(f\"Maximum Infectious population at a time :{max_inf}%\")\n    print(f\"Number of Days to Reach Maximum Infectious Population (Inflection Point):{inflection_day} days or {date + timedelta(days=inflection_day)}\")\n    print(f\"Total Infected population :{max_conf}%\")\n    print(f\"Number of Days to Reach 80% of the Projected Confirmed Cases (Plateau Point):{plateau_day} days or {date + timedelta(days=plateau_day.item())}\")\n    \n    ## Visualize the model outputs\n    sns.set(style=\"darkgrid\")\n    plt.figure(figsize=(10,6))\n    plt.title(f\"SIR Model: R = {round(beta/gamma,2)}\", fontsize=18)\n    sns.lineplot(day,inf, label=\"Infectious\")\n    sns.lineplot(day,suc,label=\"Succeptible\")\n    sns.lineplot(day,rec, label=\"Recovered\")\n    \n    plt.legend()\n    plt.xlabel(\"Time (in days)\")\n    plt.ylabel(\"Fraction of Population\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sir_model(I0=0.00011425, beta = beta.item(), gamma = gamma.item(), days=365, date = date)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What do you learn from the general negative runs in South Korea? "},{"metadata":{},"cell_type":"markdown","source":"### How do we identify the negative runs (i.e., peak-to-peak) in the KOSPI?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import and wrangle with stock_ret dataset\nstock_ret = pd.read_csv('../input/kospi-monthly/KS11.csv')\nstock_ret['Date'] = pd.to_datetime(stock_ret['Date'])\nstock_ret.tail()\n\n# Calculate monthly Total Returns for the S&P500 (excluding dividends)\nstock_ret['mth_return'] = stock_ret['KOSPI']/stock_ret['KOSPI'].shift(1) - 1\n\n# Analysis post world war 2 (i.e., 1950 onwards)\nstock_ret = stock_ret.loc[0:].copy()\nstock_ret['cum_return'] = np.cumprod(stock_ret['mth_return']+1)\nstock_ret.info()\nstock_ret.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the negative runs in the S&P 500 (i.e., from one peak to another)\n# Initialize an empty list for cumulative returns from one peak to another \nneg_run = []\n\n# Store the previous maximum cumulative return\nmax_cum_ret_now = stock_ret['cum_return'].iloc[0]   \n\n# enumerate() method adds counter (t) to an iterable (stock_ret['mth_return']) and \n# returns a tuple (t, stock_ret['mth_return'])\nfor t, val in enumerate(stock_ret['mth_return']):\n    \n    # First return in the monthly return series\n    if t == 0:\n        \n        # If monthly return is negative\n        if val < 0:\n            \n            # Append the negative return to neg_run list\n            neg_run.append(val)\n            \n        else:\n            \n            # Append a zero to neg_run list\n            neg_run.append(0)\n            \n    # Not the first return in the monthly return series\n    else:\n        \n        # If the cumulative return at time t is less than the previous maximum cumulative return\n        # i.e., the previous all time high\n        if stock_ret['cum_return'].iloc[t] < max_cum_ret_now:\n            \n            # cumulate/compound the return at time t with the return at time t-1\n            # i.e., tally the loss\n            neg_run.append((1 + neg_run[t-1])*(1 + val) - 1) \n            \n        # If the cumulative return at time t is more than the previous maximum cumulative return\n        else:\n            \n            # stop the loss tally and append a zero to the negative run list\n            neg_run.append(0)                                \n            \n            # replace the previous all time high with the new high\n            max_cum_ret_now = stock_ret['cum_return'].iloc[t]\n\n# Add the variable to the dataframe stock_ret\nstock_ret['neg_run'] = neg_run","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the KOSPI time series\nsns.lineplot(x='Date', y='KOSPI', data=stock_ret, color='red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the peak-to-peak negative run\nsns.lineplot(x='Date', y='neg_run', data=stock_ret, color='red')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How do we label the negative runs (peak-to-peak) and their maximum drawdown (peak-to-trough)?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Recap that a neg_run is the peak-to-peak run \n# Identify and label each neg_run sequentially (e.g., the 10th neg_run is tagged as 10)\n# The label serves as the groupby variable to examine the characteristics of each run\n\n# Initialize label value\nlabel = 1\n\n# Initialize the indicator value of whether stock_ret['neg_run'] (or loss tally) is within a peak-to-peak run\nwithin_negative_run = False\n\n# Initialize an empty list for negative run number\nneg_run_num = []\n\n# Identify and label each cycle of negative run, which ends with a zero\n# The cumulative return (or loss tally) during the cycle is negative\nfor i in stock_ret['neg_run']:\n    \n    # Loss tally is negative\n    if i < 0:\n        \n        # Append the label to neg_run_num list\n        neg_run_num.append(label)\n        \n        # Switch the state for within_negative_run\n        within_negative_run = True\n        \n    # Loss tally is zero - negative run ends\n    else:\n        \n        # Append a zero to neg_run_num list\n        neg_run_num.append(0)\n        \n        # Increment label value by 1 if within_negative_run is True\n        # This happens only for a 'new' cycle of negative run\n        # The label doesn't increment by 1 in market run-up after the exit from a negative run\n        # i.e., reaching new all-time highs after exiting from a cycle of negative run\n        if within_negative_run:\n            label += 1\n            within_negative_run = False\n            \nstock_ret['neg_run_num'] = neg_run_num","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identify and label each peak (previous all time high) to trough (the lowest point) within each peak-to-peak run\n# This is also known as the maximum drawdown\n# The integer label runs sequentially (e.g., the 10th peak-to-trough is tagged as 10)\n\n# Initialize the label value\nlabel = 1\n\n# Initialize the search status of whether the lowest point within a negative run has been discovered\nis_neg_run_min = False\n\n# Initialize an empty list for peak-to-trough run number\npeak_trough_num = []\n\nfor t, val in enumerate(stock_ret['neg_run_num']):\n    \n    # Identify the lowest point (i.e., cumulated returns) within a negative run\n    trough = min(stock_ret[stock_ret['neg_run_num']==val]['neg_run'])\n    \n    # Recap that if the cumulative return at time t is more than the previous maximum cumulative return\n    # The loss tally will stop with a zero appended to the negative run list (i.e., the negative run has ended)\n    # neg_run_num will also be appended with a zero when neg_run is zero\n\n    # While still within a peak-to-peak negative run\n    if val > 0:\n        \n        # Append zero to peak_trough_num if the lowest point has been discovered\n        if is_neg_run_min:\n            peak_trough_num.append(0)\n        # Lowest point within a negative run has not been discovered\n        else:\n            if stock_ret.iloc[t]['neg_run'] == trough:\n                is_neg_run_min = True\n                peak_trough_num.append(val)\n            else:\n                peak_trough_num.append(val)\n                \n    # Out of the peak-to-peak negative run\n    else:\n        is_neg_run_min = False\n        peak_trough_num.append(val)\n            \nstock_ret['peak_trough_num'] = peak_trough_num","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How do we calculate the duration (peak-to-peak, peak-to-trough, and trough-to-peak) and maximum drawdown (loss) for each negative run?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# By peak-to-peak run number, count the number of months \nrun_len = stock_ret[stock_ret['neg_run_num']>0].groupby('neg_run_num').count()['neg_run']\n\n# By peak-to-peak run number, count lowest cumulative returns (i.e., maximum drawdown)\nmaximum_drawdown = stock_ret[stock_ret['neg_run_num']>0].groupby('neg_run_num').min()['neg_run']\n\n# By peak-to-trough run number, count the number of months\npeak_trough_dur = stock_ret[stock_ret['peak_trough_num']>0].groupby('peak_trough_num').count()['neg_run']\n\nfig, ax = plt.subplots(3)\nax[0].plot(run_len.sort_values(ascending=False).reset_index(drop=True))\nax[0].set_title(\"Time between Two Peaks (Months)\")\nax[1].plot(peak_trough_dur.sort_values(ascending=False).reset_index(drop=True))\nax[1].set_title(\"Time to Maximum Drawdown (Months)\")\nax[2].plot(maximum_drawdown.sort_values(ascending=False).reset_index(drop=True))\nax[2].set_title(\"Maximum Drawdown (%)\")\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Store groupby results in a new dataframe with the 16 runs\ndeclines_df = pd.DataFrame()\n\ndeclines_df['run_len'] = run_len\ndeclines_df['maximum_drawdown'] = maximum_drawdown\ndeclines_df['peak_trough_dur'] = peak_trough_dur\n\ndeclines_df.tail(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How do the different types of market downturns (by severity) behave?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create 6 buckets by the magnitude of drawdown\ndrawdown_bin = []\nfor i in maximum_drawdown:\n    if i >= 0.00:\n        drawdown_bin.append(0)\n    elif i >= -0.05:\n        drawdown_bin.append(1)\n    elif i >= -0.10:\n        drawdown_bin.append(2)\n    elif i >= -0.30:\n        drawdown_bin.append(3)\n    elif i >= -0.50:\n        drawdown_bin.append(4)\n    else:\n        drawdown_bin.append(5)\n\ndeclines_df['drawdown_bin'] = drawdown_bin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Overall means for drawdown metrics\nnp.mean(declines_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count the number of drawdowns in each drawdown bucket\ndeclines_df.groupby('drawdown_bin').count()['run_len']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the number of declines in each magnitude bucket in probability term\n\n# Calculate the probability of being in a drawdown bin relative to all drawdown bins\nprob_bucket = declines_df.groupby('drawdown_bin').count()['run_len']/sum(declines_df.groupby('drawdown_bin').count()['run_len'])\n\n# Plot the probabilities for each drawdown bin\nfig, ax = plt.subplots(figsize=(10,6))\nbin_names = ['-5% or Better','-5% to -10%','-10% to -30%','-30% to -50%','-50% or Worse']\nsns.barplot(x=prob_bucket, y=bin_names);\nax.set_xlabel(\"Probability\",fontsize=14)\nax.set_ylabel(\"Drawdown Bin\",fontsize=14)\n\n# Probability is between 0 and 1 - limit the range of possible value for x-axis\nax.set_xlim(0, 1)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What happens after the market has already dropped by 5%\n\n# Calculate the probability for \nworst_probs = prob_bucket[1:]/sum(prob_bucket[1:])\n\n# probability of decline more than 10%\nprint(\"The probability of a further decline of more than 10% is\", sum(worst_probs[1:]))     \n\n# probability of decline being more than 30%\nprint(\"The probability of a further decline of more than 20% is\", sum(worst_probs[2:]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the mean maximum drawdown for each drawdown bucket of negative runs \ndeclines_df.groupby('drawdown_bin').mean()['maximum_drawdown']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the metrics of each drawdown bucket and store in a dataframe for plots\n\n# Calculate the peak-to-peak and peak-to-trough duration for each run\nduration_df = declines_df.groupby('drawdown_bin').mean()[['peak_trough_dur','run_len']]\nduration_df.reset_index(inplace=True)\n\n# Time to recover (in months)\nduration_df['recover_dur'] = duration_df['run_len'] - duration_df['peak_trough_dur']\n\n# Time to recover relative to time to the trough\nduration_df['recover_to_peak_trough_ratio'] = duration_df['recover_dur'] / duration_df['peak_trough_dur']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the metrics\nfig, ax = plt.subplots(figsize=(10,6))\nbin_names = ['-5% or Better','-5% to -10%','-10% to -30%','-30% to -50%','-50% or Worse']\nsns.barplot(x=bin_names, y=duration_df['recover_dur'])\nax.set_xlabel(\"Market Decline Bin\",fontsize=14)\nax.set_ylabel(\"Recovery Time in Months\",fontsize=14)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duration_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the Number and percentage of negative months\nprint(\"The number of negative monthly returns: \", len([i for i in stock_ret['mth_return'] if i<0]))\nprint(\"The number of monthly returns: \", stock_ret.shape[0])\nprint(\"The fraction of negative monthly returns: \", len([i for i in stock_ret['mth_return'] if i<0])/stock_ret.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the Mean length of drawdown\nprint(\"The average length of peak-to-trough market downturn: \", np.mean(declines_df['peak_trough_dur']), \"months\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}