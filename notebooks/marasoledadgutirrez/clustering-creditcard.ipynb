{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CLUSTERING PROBLEM\n\nPurpose: The goal is to develop a customer segmentation model to define a credit card company's marketing strategy.\n\nModel Class: *Unsupervised*\n\nModel Type: *Clustering*\n\nEdit Date: 4/8/2020\n\nCluster Models Include:\n- K-Means\n- Hierarchical\n\nResources:\n* https://afnan.io/2017-10-31/using-k-means-clustering-in-scikit-learn/\n* https://www.kaggle.com/ainslie/credit-card-data-clustering-analysis/data\n* https://lab.pwc.com/automation/details/3850\n* https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n\nData:\nhttps://www.kaggle.com/ainslie/credit-card-data-clustering-analysis/data\n\nThe dataset summarizes the usage behavior of about 9000 active credit card holders during 6 months.\nThe file is at a customer level with 18 behavioral variables.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# DEPENDENCIES\n\nLoad the dependencies for model development. Current package requirements include:\n* Sklearn\n* Pandas\n* Numpy\n* Scipy\n* Matplotlib","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# data management\nimport pandas as pd\nimport numpy as np\n\n# visualization\nfrom pylab import*\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\n\n# preprocessing\nimport sklearn as sk\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer\n\n# clusters models\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering, DBSCAN\nfrom sklearn import metrics\nfrom sklearn.metrics import silhouette_score, silhouette_samples\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.cluster.hierarchy import dendrogram, linkage","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/ccdata/CC GENERAL.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Basic Data Analysis - Overview","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = data.columns[1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n1. **CUST_ID** : Identification of Credit Card holder\n2. **BALANCE** : Balance amount left in their account to make purchases\n3.**BALANCE_FREQUENCY** : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n4. **PURCHASES** : Amount of purchases made from account\n5. **ONEOFF_PURCHASES** : Maximum purchase amount done in one-go\n6. **INSTALLMENTS_PURCHASES** : Amount of purchase done in installment\n7. **CASH_ADVANCE** : Cash in advance given by the user\n8. **PURCHASES_FREQUENCY** : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n9. **ONEOFF_PURCHASES_FREQUENCY** : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n10. **PURCHASES_INSTALLMENTS_FREQUENCY** : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n11. **CASH_ADVANCE_FREQUENCY** : How frequently the cash in advance being paid\n12. **CASH_ADVANCE_TRX** : Number of Transactions made with \"Cash in Advanced\"\n13. **PURCHASES_TRX** : Numbe of purchase transactions made\n14. **CREDIT_LIMIT** : Limit of Credit Card for user\n15. **PAYMENTS** : Amount of Payment done by user\n16. **MINIMUM_PAYMENTS** : Minimum amount of payments made by user\n17. **PRC_FULL_PAYMENT** : Percent of full payment paid by user\n18. **TENURE** : Tenure of credit card service for user\n\nCUSTOMER_ID will not be taken into account as a model variable because (in my point of view) it doesn't give information about customer behavior.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[features].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"We can see from the table above, that variables the following variables have outliers:\n* BALANCE,\n* PURCHASES,\n* ONEOFF_PURCHASES, \n* INSTALLMENTS_PURCHASES, \n* CASH_ADVANCE, \n* CASH_ADVANCE_TRX, \n* PURCHASE_TRX, \n* CREDIT_LIMIT, \n* PAYMENTS and \n* MINIMUM_PAYMENTS\n\nA data point is an outlier if any of the two following conditions apply:\n1. data point that falls outside of 1.5 times of an interquartile range above the 3rd quartile and below 1st quartile.\n2. data point that falls outside of 3 or 4 standard deviations,\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Missing values**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analisys (EDA)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Missing Values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**CREDIT_LIMIT**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data[data.CREDIT_LIMIT.isna()].shape[0],' clientes')\nprint(\"{0:.2f}%\".format(100*data[data.CREDIT_LIMIT.isna()].shape[0]/data.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data.CREDIT_LIMIT.isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.CREDIT_LIMIT.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since one possibility is to fill the missing values with zero, I analyze if there are clients with Credit limit equal to zero. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Customers with zero credit limit:' , data[data.CREDIT_LIMIT==0].shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that is not a good option to fill with zero. \n\nTaking into account the characteristics of the customer 15349, I look for special values in the CREDIT_LIMIT column for customers without purchases but with cash advances. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_aux = data[(data.PURCHASES_TRX==0)&(data.CASH_ADVANCE_TRX>0)][['CASH_ADVANCE','CASH_ADVANCE_TRX','CREDIT_LIMIT']]\nprint(data_aux.describe())\ndata_aux.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe above that there are not significant  differences with respect to the complete data. So, in this case, I decided to fill with the median value.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**MINIMUM_PAYMENTS**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data[data.MINIMUM_PAYMENTS.isna()].shape[0],' clientes')\nprint(\"{0:.2f}%\".format(100*data[data.MINIMUM_PAYMENTS.isna()].shape[0]/data.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data.MINIMUM_PAYMENTS.isna()].head(7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When PAYMENTS = 0, the value of MINIMUM_PAYMENTS is always NaN:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data[(data.PAYMENTS==0)].shape[0] == data[(data.PAYMENTS==0)&(data.MINIMUM_PAYMENTS.isna())].shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When MINIMUM_PAYMENTS is NaN, the value of PRC_FULL_PAYMENT is always zero:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data[(data.MINIMUM_PAYMENTS.isna())&(data.PRC_FULL_PAYMENT==0)].shape[0] == data[data.MINIMUM_PAYMENTS.isna()].shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.MINIMUM_PAYMENTS.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I didn't find anything special regarding the missing values of the column, so I will use the median to fill.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**CONCLUSIONS OF MISSING VALUES:** Fill the missing values in CREDIT_LIMIT and MINIMUM_PAYMENTS with the median of the column.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Outliers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**z_scr method**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect_col_outliers(ls_data):\n     # z_score and filter\n\n    mean = np.mean(ls_data)\n    std = np.std(ls_data)\n   \n    return [i for i in ls_data if np.abs(i-mean) > 4*std]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_outliers = ['BALANCE','PURCHASES','ONEOFF_PURCHASES','INSTALLMENTS_PURCHASES','CASH_ADVANCE','CASH_ADVANCE_TRX','PURCHASES_TRX','CREDIT_LIMIT','PAYMENTS','MINIMUM_PAYMENTS']\nfor name_col in features_outliers:\n    rtdo = detect_col_outliers(data[name_col])\n    print('-'*50)\n    print(name_col)\n    print('# values outlier: ', len(rtdo))\n    print('{0:.2f}% of the total data'.format(100*len(rtdo)/data.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**IQR method**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.boxplot(data=data[features])\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Columns transformation**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nr_rows = len(features_outliers)\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n\nfor r, col in enumerate(features_outliers):\n    sns.distplot(data[col], ax = axs[r][0]).set_title('Original')\n    sns.distplot(np.sqrt(data[col].tolist()), ax = axs[r][1]).set_title(\"Root Square\")\n    sns.distplot(np.log1p(data[col]), ax = axs[r][2]).set_title('log(1+x)')\nplt.tight_layout()    \nplt.show()  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CONCLUSION OF OUTLIERS:** The columns with outliers problems are 10: \n* BALANCE,\n* PURCHASES,\n* ONEOFF_PURCHASES, \n* INSTALLMENTS_PURCHASES, \n* CASH_ADVANCE, \n* CASH_ADVANCE_TRX, \n* PURCHASE_TRX, \n* CREDIT_LIMIT, \n* PAYMENTS and \n* MINIMUM_PAYMENTS\n\nand for these variables I think it is appropiate to apply a logarithmic transformation.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Discrete variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"int_cols = data[features].select_dtypes(include=['int']).columns\nint_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in int_cols:\n    print(data[col].value_counts().sort_values(ascending=False))\n    print('-'*30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[int_cols].hist(figsize=(15,8))\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Pearson Correlation\nplt.figure(figsize=(12,10))\ncorr_m = data[features].corr()\nsns.heatmap(corr_m, annot=True, cmap=plt.cm.Reds).set_title('Correlation Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* PURCHASES has a higher lever of correlation with ONEOFF_PURCHASES.\n* CASH_ADVANCE_TRX has a higher lever of correlation with CASH_ADVANCE_FREQUENCY.\n* PURCHASES_TRX has a good level of correlation with INSTALLMENTS_PURCHASES, PURCHASES_FREQUENCY.\n* BALANCE has a negative correlation with PRC_FULL_PAYMENT","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**PURCHASE analysis**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_purchases = abs(corr_m[\"PURCHASES\"])\ncor_purchases[cor_purchases>0.5].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('{0:.2f}%'.format(100*sum(data.PURCHASES == data.ONEOFF_PURCHASES + data.INSTALLMENTS_PURCHASES)/data.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data.PURCHASES != data.ONEOFF_PURCHASES + data.INSTALLMENTS_PURCHASES].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data[['PURCHASES','ONEOFF_PURCHASES','INSTALLMENTS_PURCHASES']],\n             markers=\"+\",\n             kind='reg',\n             diag_kind=None, \n             height=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CASH_ADVANCE analysis**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data[['CASH_ADVANCE_FREQUENCY','CASH_ADVANCE_TRX']],\n             markers=\"+\",\n             kind='reg',\n             height=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CONCLUSIONS OF CORRELATIONS:** Taking into account the high correlation between PURCHASES and ONEOFF_PURCHASES, and the interpretation of the variables in the problem, I think PURCHASES can be remove or not from the variables to include in the models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features = data.columns[1:]\nfeatures_group1 = ['BALANCE','ONEOFF_PURCHASES','INSTALLMENTS_PURCHASES','CASH_ADVANCE','CASH_ADVANCE_TRX','PURCHASES_TRX','PAYMENTS','CREDIT_LIMIT','MINIMUM_PAYMENTS']\nfeatures_group2 = list(set(features)-set(features_group1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using median in columns with outliers \ng1_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('log', FunctionTransformer(np.log1p)),\n    #('scaler', MinMaxScaler(feature_range=(0, 1)))\n    ('scaler', StandardScaler())\n    ])\n\n# using median in columns without outliers \ng2_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n    ])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('group1', g1_transformer, features_group1),\n        ('group2', g2_transformer, features_group2),\n        ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis of Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessor.fit(data) \nnp_data = preprocessor.transform(data) \nprint(np_data[np.isnan(np_data)])\ndf_data = pd.DataFrame(np_data, columns=features_group1+features_group2)\nprint(df_data.isna().sum())\nprint(df_data.shape)\ndf_data.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to check StandardScaler\ndf_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to check outliers\nplt.figure(figsize=(15,10))\nsns.boxplot(data=df_data)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing - Extra Miscellaneous","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Another way to deal with outliers is to make ranges.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_range = data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns=['BALANCE', 'PURCHASES', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'CREDIT_LIMIT', 'PAYMENTS', 'MINIMUM_PAYMENTS']\n\nfor c in columns:\n    \n    Range=c+'_RANGE'\n    data_range[Range]=0        \n    data_range.loc[((data[c]>0)&(data[c]<=500)),Range]=1\n    data_range.loc[((data[c]>500)&(data[c]<=1000)),Range]=2\n    data_range.loc[((data[c]>1000)&(data[c]<=3000)),Range]=3\n    data_range.loc[((data[c]>3000)&(data[c]<=5000)),Range]=4\n    data_range.loc[((data[c]>5000)&(data[c]<=10000)),Range]=5\n    data_range.loc[((data[c]>10000)),Range]=6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns=['BALANCE_FREQUENCY', 'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY', 'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY', 'PRC_FULL_PAYMENT']\n\nfor c in columns:  \n\n    Range=c+'_RANGE'\n    data_range[Range]=0\n    for i in range(10):\n        data_range.loc[((data[c]>i*0.1)&(data[c]<=(i+1)*0.1)), Range]=i+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns=['PURCHASES_TRX', 'CASH_ADVANCE_TRX']  \n\nfor c in columns:\n    \n    Range=c+'_RANGE'\n    data_range[Range]=0\n    data_range.loc[((data[c]>0)&(data[c]<=5)),Range]=1\n    data_range.loc[((data[c]>5)&(data[c]<=10)),Range]=2\n    data_range.loc[((data[c]>10)&(data[c]<=15)),Range]=3\n    data_range.loc[((data[c]>15)&(data[c]<=20)),Range]=4\n    data_range.loc[((data[c]>20)&(data[c]<=30)),Range]=5\n    data_range.loc[((data[c]>30)&(data[c]<=50)),Range]=6\n    data_range.loc[((data[c]>50)&(data[c]<=100)),Range]=7\n    data_range.loc[((data[c]>100)),Range]=8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_range.drop(['CUST_ID', 'BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES',\n       'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE',\n       'PURCHASES_FREQUENCY',  'ONEOFF_PURCHASES_FREQUENCY',\n       'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY',\n       'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS',\n       'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT' ], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data.columns), len(data_range.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_range.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_range.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.boxplot(data=data_range)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_group3 = ['INSTALLMENTS_PURCHASES_RANGE','MINIMUM_PAYMENTS_RANGE','ONEOFF_PURCHASES_FREQUENCY_RANGE','CASH_ADVANCE_FREQUENCY_RANGE','PRC_FULL_PAYMENT_RANGE','CASH_ADVANCE_TRX_RANGE']\nfeatures_group4 = list(set(data_range.columns)-set(features_group3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using median in columns with outliers \ng1_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('log', FunctionTransformer(np.log1p)),\n    #('scaler', MinMaxScaler(feature_range=(0, 1)))\n    ('scaler', StandardScaler())\n    ])\n\n# using median in columns without outliers \ng2_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n    ])\n\npreprocessor2 = ColumnTransformer(\n    transformers=[\n        ('group1', g1_transformer, features_group3),\n        ('group2', g2_transformer, features_group4),\n        ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_range.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessor2.fit(data_range) \nnp_data_range = preprocessor2.transform(data_range) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np_data_range[np.isnan(np_data_range)])\ndf_data2 = pd.DataFrame(np_data_range, columns=features_group3+features_group4)\nprint(df_data2.isna().sum())\nprint(df_data2.shape)\ndf_data2.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.boxplot(data=df_data)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=2)\npca.fit(np_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pca = pca.transform(np_data)\nplt.figure(figsize=(8,6))\nplt.scatter(np_data[:,0],np_data[:,1])\nplt.xlabel('First principal component')\nplt.ylabel('Second Principal Component')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pca.noise_variance_)\nprint(pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The estimated noise covariance is not a good value, so we cannot rely on the shape of the data using the visual of PCA in 2 dimensions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# MODELS","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## K-means","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Below is a toy example to illustrate how the algorithm works.\n\n![image.png](https://stanford.edu/~cpiech/cs221/img/kmeansViz.png)\n\n\nImage: https://stanford.edu/~cpiech/cs221/img/kmeansViz.png","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### N°Clusters for K-means: Elbow Method","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The idea behind elbow method is to run k-means clustering on a given dataset for a range of values of k (e.g k=1 to 10), for each value of k, calculate sum of squared errors (SSE).\n\nCalculate the mean distance between data points and their cluster centroid. Increasing the number of clusters(K) will always reduce the distance to data points, thus decrease this metric, to the extreme of reaching zero when K is as same as the number of data points. **So the goal is to choose a small value of k that still has a low SSE.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Sum_of_squared_distances = []\nK = range(1, 20)\nfor k in K:\n    km = KMeans(n_clusters=k, \n                init='k-means++',\n                max_iter=400, \n                n_init=80, \n                random_state=0).fit(np_data)\n    Sum_of_squared_distances.append(km.inertia_)\n\nplt.figure(figsize=(10,10))\nplt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### N° Clusters for K-means: Silhouette Coefficient Method:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"to a model with better-defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores:\n\n$a$: The mean distance between a sample and all other points in the same class.\n\n$b$: The mean distance between a sample and all other points in the next nearest cluster.\n\nThe Silhouette Coefficient is for a single sample is then given as:\n\n$$s=\\dfrac{b−a}{max(a,b)}$$\n \nTo find the optimal value of k for KMeans, loop through 1..n for n_clusters in KMeans and calculate Silhouette Coefficient for each sample.\n\nA higher Silhouette Coefficient indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The Silhouette coefficient, between -1 and 1, gives an indication of how close each point in one cluster is to points in the neighbouring clusters. Values close to 1 are furthest from other clusters whereas negative points overlap with others. In an ideal situation we would expect all the points of a cluster to have Silhouette coefficients close to 1. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"silhouette_scores = [] \nK = range(2, 20)\n\nfor k in K:\n    km = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=45).fit_predict(np_data)\n    scr = silhouette_score(np_data, km)\n    silhouette_scores.append(scr)\n    print(\"For n_clusters =\", k, \"The average silhouette_score is :\", scr)\nplt.plot(K, silhouette_scores, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Method For Optimal k')\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"K = range(2,10)\n\nfor k in K:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    ax1.set_xlim([-0.1, 1])\n    ax1.set_ylim([0, len(np_data) + (k + 1) * 10])\n\n    clusterer = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=45)\n    cluster_labels = clusterer.fit_predict(np_data)\n\n    silhouette_avg = silhouette_score(np_data, cluster_labels)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(np_data, cluster_labels)\n\n    y_lower = 10\n    for i in range(k):\n        # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) / k)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.nipy_spectral(cluster_labels.astype(float) / k)\n    pca = PCA(n_components=2)\n    pca.fit(np_data)\n    X = pca.transform(np_data)\n\n    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors, edgecolor='k')\n\n    # Labeling the clusters\n    pca_centers = pca.transform(clusterer.cluster_centers_)\n    # Draw white circles at cluster centers\n    ax2.scatter(pca_centers[:, 0], pca_centers[:, 1], marker='o',\n                c=\"white\", alpha=1, s=200, edgecolor='k')\n\n    for i, c in enumerate(pca_centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n                    s=50, edgecolor='k')\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st principal feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd principal feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % k),\n                 fontsize=14, fontweight='bold')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Set clusters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"km = KMeans(n_clusters=6, \n            init='k-means++',\n            max_iter=400, \n            n_init=80, \n            random_state=0)\n\nkm_pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('km', km)])\n\nkm_pipe.fit(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = km.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clusters = pd.concat([data, pd.DataFrame({'CLUSTER':labels})], axis=1)\nclusters.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clusters.CLUSTER.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clusters.CLUSTER.hist(figsize=(10, 8))\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save clusters to csv\nclusters.to_csv('Clusters_CreditCards_Kmeans.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interpretation of clusters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in clusters:\n    grid= sns.FacetGrid(clusters, col='CLUSTER')\n    grid.map(plt.hist, c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clusters.groupby(['CLUSTER']).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Cluster 0***  People with high level of income (balance) and high credit limit who take cash in advance.\n\n***Cluster 1*** People with low level of income. Not Frequent purchases.\n\n***Cluster 2*** Low balance but the balance gets updated frequently ie. more no. of transactions. They purchase mostly in installments\n\n***Cluster 3*** They purchase mostly in one-go with a high frequency. the percent of full payment paid is low (debtors).\n\n***Cluster 4***: People with a medium level of income who don't spend much money and who accept large amounts of cash advances but not frequently.\n\n***Cluster 5*** High spenders with high credit limit who make expensive purchases and take more cash in advance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dist = 1 - cosine_similarity(np_data)\n\npca = PCA(2)\npca.fit(dist)\nX_PCA = pca.transform(dist)\nX_PCA.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x, y = X_PCA[:, 0], X_PCA[:, 1]\n\ncolors = {0: 'red',\n          1: 'blue',\n          2: 'green', \n          3: 'yellow', \n          4: 'orange',  \n          5:'purple'}\n\nnames = {0: 'high level of income and high credit limit who take cash in advance', \n         1: 'low level of income. Not Frequent purchases', \n         2: 'who purchases mostly in installments', \n         3: 'They purchase mostly in one-go with a high frequency. the percent of full payment paid is low (debtors)', \n         4: 'do not spend much money and who accept large amounts of cash advances but not frequently',\n         5: 'High spenders who take more cash in advance'}\n  \ndf = pd.DataFrame({'x': x, 'y':y, 'label':labels}) \ngroups = df.groupby('label')\n\nfig, ax = plt.subplots(figsize=(20, 13)) \n\nfor name, group in groups:\n    ax.plot(group.x, group.y, marker='o', linestyle='', ms=5,\n            color=colors[name],label=names[name], mec='none')\n    ax.set_aspect('auto')\n    ax.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off')\n    ax.tick_params(axis= 'y',which='both',left='off',top='off',labelleft='off')\n    \nax.legend()\nax.set_title(\"Customers Segmentation based on their Credit Card usage bhaviour.\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hierarchical","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Hierarchical clustering starts by treating each observation as a separate cluster. Then, it repeatedly executes the following two steps: \n1. identify the two clusters that are closest together, and\n2. merge the two most similar clusters. This iterative process continues until all the clusters are merged together.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://dpzbhybb2pdcj.cloudfront.net/rhys/v-7/Figures/CH17_FIG_2_MLR.png\" width=\"400\">\n\nimage: https://dpzbhybb2pdcj.cloudfront.net/rhys/v-7/Figures/CH17_FIG_2_MLR.png","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![image.png](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRhdKUnlaTWstr6eoGVrHV6iDhOLr4ZhBXValerAT4vUfbWXrgA&usqp=CAU)\n\nimage from: https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRhdKUnlaTWstr6eoGVrHV6iDhOLr4ZhBXValerAT4vUfbWXrgA&usqp=CAU","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### N° of clusters - Visualization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![image](https://3.bp.blogspot.com/-TQYHVkgesMg/WbTcMIOuquI/AAAAAAAAD3Y/dY4YpxJ3OhU5VGppwcrS6j-ewvlddxSjwCLcBGAs/s1600/hcust.PNG)\n\nimage from: https://3.bp.blogspot.com/-TQYHVkgesMg/WbTcMIOuquI/AAAAAAAAD3Y/dY4YpxJ3OhU5VGppwcrS6j-ewvlddxSjwCLcBGAs/s1600/hcust.PNG","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessor.fit(data) \nnp_data = preprocessor.transform(data) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"siliuette_list_hierarchical = []\nfor cluster in range(2,10):\n    for linkage_method in ['ward', 'average','single']:\n        agglomerative = AgglomerativeClustering(linkage=linkage_method, affinity='euclidean',n_clusters=cluster).fit_predict(np_data)\n        sil_score = metrics.silhouette_score(np_data, agglomerative, metric='euclidean')\n        siliuette_list_hierarchical.append((cluster, sil_score, linkage_method))\n        \ndf_hierarchical = pd.DataFrame(siliuette_list_hierarchical, columns=['cluster', 'sil_score','linkage_method'])\ndf_hierarchical.sort_values('sil_score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dendogram can be hard to read when the original observation matrix from which the linkage is derived is large. Truncation is used to condense the dendrogram.\n\nI'm going to plot with different parameters to see the best option.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Z_avg = linkage(np_data, 'average')\n\nplt.figure(figsize=(15,10))\ndendrogram(Z_avg, leaf_rotation=90, p=5, color_threshold=20, leaf_font_size=10, truncate_mode='level')\nplt.axhline(y=125, color='r', linestyle='--')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Z_ward = linkage(np_data, 'ward')\n\nplt.figure(figsize=(15,10))\ndendrogram(Z_ward, leaf_rotation=90, p=5, color_threshold=20, leaf_font_size=10, truncate_mode='level')\nplt.axhline(y=125, color='r', linestyle='--')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Z_ward = linkage(np_data, 'single')\n\nplt.figure(figsize=(15,10))\ndendrogram(Z_ward, leaf_rotation=90, p=15, color_threshold=20, leaf_font_size=10, truncate_mode='level')\nplt.axhline(y=125, color='r', linestyle='--')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Set 2 clusters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hierarchical = AgglomerativeClustering(n_clusters=2, linkage='average')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_hierar = Pipeline(steps=[\n                              ('preprocessor', preprocessor),\n                              ('hierarchical', hierarchical)]\n                       )\n\npipe_hierar.fit(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clusters_hierar = pd.concat([data, pd.DataFrame({'CLUSTER':hierarchical.labels_})], axis=1)\nclusters_hierar.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clusters_hierar.to_csv('Clusters_CreditCard_Hierarchical.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clusters_hierar.groupby('CLUSTER').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clusters_hierar.CLUSTER.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}