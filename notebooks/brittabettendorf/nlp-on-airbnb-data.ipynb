{"cells":[{"metadata":{},"cell_type":"markdown","source":"## What to expect as an Airbnb Host in Berlin\n\nAirbnb has successfully disrupted the traditional hospitality industry as more and more travelers decide to use Airbnb as their primary accommodation provider. Since its inception in 2008, Airbnb has seen an enormous growth, with the number of rentals listed on its website growing exponentially each year.\n\nIn Germany, no city is more popular than Berlin. That implies that Berlin is one of the hottest markets for Airbnb in Europe, with over 22,552 listings as of November 2018. With a size of 891 km², this means there are roughly 25 homes being rented out per km² in Berlin on Airbnb!\n\nThe following question will drive this project:\n\n> **3. What do visitors like and dislike?**\n\n<br> To find out, we process the reviews to find out what peoples' likes and dislikes are. Natural Language Processing (NLP) and specifically Sentiment Analysis are what we will use here.\n\n### The datasets\n\nI will use the reviews data and combine it with some features from the detailed Berlin listings data, sourced from the Inside Airbnb website. Both datasets were scraped on November 07th 2018."},{"metadata":{},"cell_type":"markdown","source":"## > Table of Contents\n<a id='Table of contents'></a>\n\n### <a href='#1. Obtaining and Viewing the Data'> 1. Obtaining and Viewing the Data </a>\n\n### <a href='#2. Preprocessing the Data'> 2. Preprocessing the Data </a>\n* <a href='#2.1. Dealing with Missing Values'> 2.1. Dealing with Missing Values </a>\n* <a href='#2.2. Language Detection'> 2.2. Language Detection </a>\n\n### <a href='#3. Visualizing the Data with WordClouds'> 3. Visualizing the Data with WordClouds </a>\n\n### <a href='#4. Sentiment Analysis'> 4. Sentiment Analysis </a>\n* <a href='#4.1. Get used to VADER package'> 4.1. Get used to VADER package </a>\n* <a href='#4.2. Calculating Sentiment Scores'> 4.2. Calculating Sentiment Scores </a>\n* <a href='#4.3. Comparing Negative and Positive Comments'> 4.3. Comparing Negative and Positive Comments </a>\n* <a href='#4.4. Investigating Positive Comments'> 4.4. Investigating Positive Comments </a>\n* <a href='#4.5. Investigating Negative Comments'> 4.5. Investigating Negative Comments </a>\n\n### <a href='#5. Appendix'> 5. Appendix </a>"},{"metadata":{},"cell_type":"markdown","source":"### 1. Obtaining and Viewing the Data \n<a id='1. Obtaining and Viewing the Data'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport time\nimport datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1 = pd.read_csv('../input/berlin-airbnb-data/reviews_summary.csv')\n# checking shape ...\nprint(\"The dataset has {} rows and {} columns.\".format(*df_1.shape))\n\n# ... and duplicates\nprint(\"It contains {} duplicates.\".format(df_1.duplicated().sum()))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, it may be valuable to have more details, such as the latitude and longitude of the accommodation that has been reviewed, the neighbourhood it's in, the host id, etc. \n\nTo get this information, let's **combine our reviews_dataframe** with the **listings_dataframe** and take only the columns we need from the latter one:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2 = pd.read_csv('../input/berlin-airbnb-data/listings_summary.csv')\ndf_2.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merging full df_1 + add only specific columns from df_2\ndf = pd.merge(df_1, df_2[['neighbourhood_group_cleansed', 'host_id', 'latitude',\n                          'longitude', 'number_of_reviews', 'id', 'property_type']], \n              left_on='listing_id', right_on='id', how='left')\n\ndf.rename(columns = {'id_x':'id', 'neighbourhood_group_cleansed':'neighbourhood_group'}, inplace=True)\ndf.drop(['id_y'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking shape\nprint(\"The dataset has {} rows and {} columns.\".format(*df.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hosts with many properties**\n\nBy the way, I am curious to find out if any private hosts have started to run a professional business through Airbnb - at least this is what was in the press. Let's work this out:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# group by hosts and count the number of unique listings --> cast it to a dataframe\nproperties_per_host = pd.DataFrame(df.groupby('host_id')['listing_id'].nunique())\n\n# sort unique values descending and show the Top20\nproperties_per_host.sort_values(by=['listing_id'], ascending=False, inplace=True)\nproperties_per_host.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a closer look at the top 3 hosts. How many properties do they have in the different areas? And are these private apartments, or something else, like a hostel?"},{"metadata":{},"cell_type":"markdown","source":"**> No. 1 Host**"},{"metadata":{"trusted":true},"cell_type":"code","source":"top1_host = df.host_id == 1625771\ndf[top1_host].neighbourhood_group.value_counts()\n\npd.DataFrame(df[top1_host].groupby('neighbourhood_group')['listing_id'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(df[top1_host].groupby('property_type')['listing_id'].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> This host owns apartments in 8 (!) districts. It looks like he was really able to deeply expand a well working business into different neighbourhoods..."},{"metadata":{},"cell_type":"markdown","source":"**> No. 2 Host**"},{"metadata":{"trusted":true},"cell_type":"code","source":"top2_host = df.host_id == 8250486\ndf[top2_host].neighbourhood_group.value_counts()\n\npd.DataFrame(df[top2_host].groupby('neighbourhood_group')['listing_id'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(df[top2_host].groupby('property_type')['listing_id'].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Well, looks like the second biggest player turned out to be a hostel."},{"metadata":{},"cell_type":"markdown","source":"**> No. 3 Host**"},{"metadata":{"trusted":true},"cell_type":"code","source":"top3_host = df.host_id == 2293972\ndf[top3_host].neighbourhood_group.value_counts()\n\npd.DataFrame(df[top3_host].groupby('neighbourhood_group')['listing_id'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(df[top3_host].groupby('property_type')['listing_id'].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> And host No. 3 also seems to be a professional lodging supplier."},{"metadata":{},"cell_type":"markdown","source":"*Back to: <a href='#Table of contents'> Table of contents</a>*\n### 2. Preprocessing the Data \n<a id='2. Preprocessing the Data'></a>"},{"metadata":{},"cell_type":"markdown","source":"#### 2.1. Dealing with Missing Values\n<a id='2.1. Dealing with Missing Values'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace=True)\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Back to: <a href='#Table of contents'> Table of contents</a>*\n#### 2.2. Language Detection\n<a id='2.2. Language Detection'></a>"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# we use Python's langdetect \nfrom langdetect import detect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# write the function that detects the language\ndef language_detection(text):\n    try:\n        return detect(text)\n    except:\n        return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf['language'] = df['comments'].apply(language_detection)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# write the dataframe to a csv file in order to avoid the long runtime\n#df.to_csv('../input/processed_df', index=False)\n#df = pd.read_csv('../input/processed_df')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.language.value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizing the comments' languages a) quick and dirty\nax = df.language.value_counts(normalize=True).head(6).sort_values().plot(kind='barh', figsize=(9,5));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizing the comments' languages b) neat and clean\nax = df.language.value_counts().head(6).plot(kind='barh', figsize=(9,5), color=\"lightcoral\", \n                                             fontsize=12);\n\nax.set_title(\"\\nWhat are the most frequent languages comments are written in?\\n\", \n             fontsize=12, fontweight='bold')\nax.set_xlabel(\" Total Number of Comments\", fontsize=10)\nax.set_yticklabels(['English', 'German', 'French', 'Spanish', 'Italian', 'Dutch'])\n\n# create a list to collect the plt.patches data\ntotals = []\n# find the ind. values and append to list\nfor i in ax.patches:\n    totals.append(i.get_width())\n# get total\ntotal = sum(totals)\n\n# set individual bar labels using above list\nfor i in ax.patches:\n    ax.text(x=i.get_width(), y=i.get_y()+.35, \n            s=str(round((i.get_width()/total)*100, 2))+'%', \n            fontsize=10, color='black')\n\n# invert for largest on top \nax.invert_yaxis()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting the dataframes in language related sub-dataframes\ndf_eng = df[(df['language']=='en')]\ndf_de  = df[(df['language']=='de')]\ndf_fr  = df[(df['language']=='fr')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Back to: <a href='#Table of contents'> Table of contents</a>*\n### 3. Visualizing the Data with WordClouds\n<a id='3. Visualizing the Data with WordClouds'></a>"},{"metadata":{},"cell_type":"markdown","source":"**Preparing Steps**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import necessary libraries\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom PIL import Image\n\nimport re\nimport string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# wrap the plotting in a function for easier access\ndef plot_wordcloud(wordcloud, language):\n    plt.figure(figsize=(12, 10))\n    plt.imshow(wordcloud, interpolation = 'bilinear')\n    plt.axis(\"off\")\n    plt.title(language + ' Comments\\n', fontsize=18, fontweight='bold')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**English WordCloud**"},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(max_font_size=None, max_words=200, background_color=\"lightgrey\", \n                      width=3000, height=2000,\n                      stopwords=stopwords.words('english')).generate(str(df_eng.comments.values))\n\nplot_wordcloud(wordcloud, 'English')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**German WordCloud**"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(max_font_size=None, max_words=150, background_color=\"powderblue\",\n                      width=3000, height=2000,\n                      stopwords=stopwords.words('german')).generate(str(df_de.comments.values))\n\nplot_wordcloud(wordcloud, 'German')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**French WordCloud**"},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(max_font_size=200, max_words=150, background_color=\"lightgoldenrodyellow\",\n                      #width=1600, height=800,\n                      width=3000, height=2000,\n                      stopwords=stopwords.words('french')).generate(str(df_fr.comments.values))\n\nplot_wordcloud(wordcloud, 'French')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Back to: <a href='#Table of contents'> Table of contents</a>*\n### 4. Sentiment Analysis\n<a id='4. Sentiment Analysis'></a>"},{"metadata":{},"cell_type":"markdown","source":"#### 4.1. Get used to VADER package\n<a id='4.1. Get used to VADER package'></a>"},{"metadata":{},"cell_type":"markdown","source":"An excellent and easy-to-read overview of sentiment analysis and the VADER package can be found on Jodie Burchell's <a href='http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html'> blogpost</a>. (I don't want to repeat what she said, I'd recommend reading it in her own words.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the SentimentIntensityAnalyser object in\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# assign it to another name to make it easier to use\nanalyzer = SentimentIntensityAnalyzer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use the polarity_scores() method to get the sentiment metrics\ndef print_sentiment_scores(sentence):\n    snt = analyzer.polarity_scores(sentence)\n    print(\"{:-<40} {}\".format(sentence, str(snt)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"VADER belongs to a type of sentiment analysis that is based on lexicons of sentiment-related words. In this approach, each of the words in the lexicon is rated as positive or negative, and in many cases, **how** positive or negative. <br> Let's play around a bit and get familiar with this package:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print_sentiment_scores(\"This raspberry cake is good.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"VADER produces four sentiment metrics from these word ratings, which you can see above. The first three - positive, neutral and negative - represent the proportion of the text that falls into those categories. As you can see, our example sentence was rated as 42% positive, 58% neutral, and 0% negative. \n\nThe final metric, **the compound score**, is the sum of all of the lexicon ratings which have been standardised to range between -1 and 1. In this case, our example sentence has a rating of 0.44, which is pretty neutral."},{"metadata":{"trusted":true},"cell_type":"code","source":"print_sentiment_scores(\"This raspberry cake is good.\")\nprint_sentiment_scores(\"This raspberry cake is GOOD!\")\nprint_sentiment_scores(\"This raspberry cake is VERY GOOD!!\")\nprint_sentiment_scores(\"This raspberry cake is really GOOD! But the coffee is dreadful.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting only the negative score\ndef negative_score(text):\n    negative_value = analyzer.polarity_scores(text)['neg']\n    return negative_value\n\n# getting only the neutral score\ndef neutral_score(text):\n    neutral_value = analyzer.polarity_scores(text)['neu']\n    return neutral_value\n\n# getting only the positive score\ndef positive_score(text):\n    positive_value = analyzer.polarity_scores(text)['pos']\n    return positive_value\n\n# getting only the compound score\ndef compound_score(text):\n    compound_value = analyzer.polarity_scores(text)['compound']\n    return compound_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_score(\"The food is really GOOD! But the service is dreadful.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neutral_score(\"The food is really GOOD! But the service is dreadful.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_score(\"The food is really GOOD! But the service is dreadful.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compound_score(\"The food is really GOOD! But the service is dreadful.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Back to: <a href='#Table of contents'> Table of contents</a>*\n#### 4.2. Calculating Sentiment Scores\n<a id='4.2. Calculating Sentiment Scores'></a>"},{"metadata":{},"cell_type":"markdown","source":"Let's now have VADER produce all four scores for each of our English-language comments. As this takes roughly a quarter of an hour, it's a good idea to save the dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndf_eng['sentiment_neg'] = df_eng['comments'].apply(negative_score)\ndf_eng['sentiment_neu'] = df_eng['comments'].apply(neutral_score)\ndf_eng['sentiment_pos'] = df_eng['comments'].apply(positive_score)\ndf_eng['sentiment_compound'] = df_eng['comments'].apply(compound_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# write the dataframe to a csv file in order to avoid the long runtime\n#df_eng.to_csv('data/sentimentData/sentiment_df_eng', index=False)\n#df = pd.read_csv('data/sentimentData/sentiment_df_eng')\ndf = df_eng","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's investigate the distribution of all scores:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# all scores in 4 histograms\nfig, axes = plt.subplots(2, 2, figsize=(10,8))\n\n# plot all 4 histograms\ndf.hist('sentiment_neg', bins=25, ax=axes[0,0], color='lightcoral', alpha=0.6)\naxes[0,0].set_title('Negative Sentiment Score')\ndf.hist('sentiment_neu', bins=25, ax=axes[0,1], color='lightsteelblue', alpha=0.6)\naxes[0,1].set_title('Neutral Sentiment Score')\ndf.hist('sentiment_pos', bins=25, ax=axes[1,0], color='chartreuse', alpha=0.6)\naxes[1,0].set_title('Positive Sentiment Score')\ndf.hist('sentiment_compound', bins=25, ax=axes[1,1], color='navajowhite', alpha=0.6)\naxes[1,1].set_title('Compound')\n\n# plot common x- and y-label\nfig.text(0.5, 0.04, 'Sentiment Scores',  fontweight='bold', ha='center')\nfig.text(0.04, 0.5, 'Number of Reviews', fontweight='bold', va='center', rotation='vertical')\n\n# plot title\nplt.suptitle('Sentiment Analysis of Airbnb Reviews for Berlin\\n\\n', fontsize=12, fontweight='bold');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remember what we said earlier: VADER produces four sentiment metrics from these word ratings (...). The first three - positive, neutral and negative - represent the **proportion of the text** that falls into those categories. (...). The final metric, **the compound score**, is the sum of all of the lexicon ratings which have been standardized to range between -1 and 1.\n\nFinally, let’s use the method described to generate descriptive statistics that summarize the central tendency and dispersion of our dataset's compound score:"},{"metadata":{"trusted":true},"cell_type":"code","source":"percentiles = df.sentiment_compound.describe(percentiles=[.05, .1, .2, .3, .4, .5, .6, .7, .8, .9])\npercentiles","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Back to: <a href='#Table of contents'> Table of contents</a>*\n#### 4.3. Comparing Negative and Positive Comments\n<a id='4.3. Comparing Negative and Positive Comments'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# assign the data\nneg = percentiles['5%']\nmid = percentiles['20%']\npos = percentiles['max']\nnames = ['Negative Comments', 'Okayish Comments','Positive Comments']\nsize = [neg, mid, pos]\n\n# call a pie chart\nplt.pie(size, labels=names, colors=['lightcoral', 'lightsteelblue', 'chartreuse'], \n        autopct='%.2f%%', pctdistance=0.8,\n        wedgeprops={'linewidth':7, 'edgecolor':'white' })\n\n# create circle for the center of the plot to make the pie look like a donut\nmy_circle = plt.Circle((0,0), 0.6, color='white')\n\n# plot the donut chart\nfig = plt.gcf()\nfig.set_size_inches(7,7)\nfig.gca().add_artist(my_circle)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly, the bulk of the reviews are tremendously positive. Wouldn't it be interesting to know what the negative and positive comments are about? Let's have a look:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# full dataframe with POSITIVE comments\ndf_pos = df.loc[df.sentiment_compound >= 0.95]\n\n# only corpus of POSITIVE comments\npos_comments = df_pos['comments'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# full dataframe with NEGATIVE comments\ndf_neg = df.loc[df.sentiment_compound < 0.0]\n\n# only corpus of NEGATIVE comments\nneg_comments = df_neg['comments'].tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's compare the length of both positive and negative comments:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pos['text_length'] = df_pos['comments'].apply(len)\ndf_neg['text_length'] = df_neg['comments'].apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nplt.figure(figsize=(8,5))\n\nsns.distplot(df_pos['text_length'], kde=True, bins=50, color='chartreuse')\nsns.distplot(df_neg['text_length'], kde=True, bins=50, color='lightcoral')\n\nplt.title('\\nDistribution Plot for Length of Comments\\n')\nplt.legend(['Positive Comments', 'Negative Comments'])\nplt.xlabel('\\nText Length')\nplt.ylabel('Percentage of Comments\\n');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mode for the text length of positive comments can be found more to the right than for the negative comments, which means most of the positive comments are longer than most of the negative comments. But the tail for negative comments is thicker."},{"metadata":{"trusted":true},"cell_type":"code","source":"# read some positive comments\npos_comments[10:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read some negative comments\nneg_comments[10:15]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's quickly check if a scatter plot may reveal some differences in the comments' sentiment with respect to the districts:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"white\")\ncmap = sns.cubehelix_palette(rot=-.4, as_cmap=True)\nfig, ax = plt.subplots(figsize=(11,7))\n\nax = sns.scatterplot(x=\"longitude\", y=\"latitude\", size='number_of_reviews', sizes=(5, 200),\n                     hue='sentiment_compound', palette=cmap,  data=df)\nax.legend(bbox_to_anchor=(1.3, 1), borderaxespad=0.)\nplt.title('\\nAccommodations in Berlin by Number of Reviwws & Sentiment\\n', fontsize=12, fontweight='bold')\n\nsns.despine(ax=ax, top=True, right=True, left=True, bottom=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not really..."},{"metadata":{},"cell_type":"markdown","source":"*Back to: <a href='#Table of contents'> Table of contents</a>*\n#### 4.4. Investigating Positive Comments\n<a id='4.4. Investigating Positive Comments'></a>"},{"metadata":{},"cell_type":"markdown","source":"**WordCloud**\n\nAfter reading some of these reviews to get a feeling for what visitors applaud or complain about, WordClouds are a great tool to help us peek behind the curtain:"},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(max_font_size=200, max_words=200, background_color=\"palegreen\",\n                      width= 3000, height = 2000,\n                      stopwords = stopwords.words('english')).generate(str(df_pos.comments.values))\n\nplot_wordcloud(wordcloud, '\\nPositively Tuned')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Frequency Distribution**\n\nAnother method for visually exploring text is with frequency distributions. In the context of a text corpus, such a distribution tells us the prevalence of certain words. Here we use the Yellowbrick library."},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing libraries\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom yellowbrick.text.freqdist import FreqDistVisualizer\nfrom yellowbrick.style import set_palette","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vectorizing text\nvectorizer = CountVectorizer(stop_words='english')\ndocs = vectorizer.fit_transform(pos_comments)\nfeatures = vectorizer.get_feature_names()\n\n# preparing the plot\nset_palette('pastel')\nplt.figure(figsize=(18,8))\nplt.title('The Top 30 most frequent words used in POSITIVE comments\\n', fontweight='bold')\n\n# instantiating and fitting the FreqDistVisualizer, plotting the top 30 most frequent terms\nvisualizer = FreqDistVisualizer(features=features, n=30)\nvisualizer.fit(docs)\nvisualizer.poof;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Topic Modelling**\n\nNext we'll explore topic modeling, an unsupervised machine learning technique for abstracting topics from collections of documents or, in our case, for identifying which topic is being discussed in a comment. \n\nMethods for topic modeling have evolved significantly over the last decade. In this section, we'll explore a technique called *Latent Dirichlet Allocation (LDA)*, a widely used topic modelling technique."},{"metadata":{},"cell_type":"markdown","source":"*1. Cleaning and Preprocessing*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing libraries\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare the preprocessing\nstop = set(stopwords.words('english'))\nexclude = set(string.punctuation)\nlemma = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing stopwords, punctuations and normalizing the corpus\ndef clean(doc):\n    stop_free = \" \".join([word for word in doc.lower().split() if word not in stop])\n    punc_free = \"\".join(token for token in stop_free if token not in exclude)\n    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n    return normalized\n\ndoc_clean = [clean(comment).split() for comment in pos_comments]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*2. LDA the Gensim way*\n\nFirst, we create a Gensim dictionary from the normalized data, then we convert this to a bag-of-words corpus, and save both dictionary and corpus for future use."},{"metadata":{"trusted":false},"cell_type":"code","source":"from gensim import corpora\ndictionary = corpora.Dictionary(doc_clean)\ncorpus = [dictionary.doc2bow(text) for text in doc_clean]\n\nimport pickle \n# uncomment the code if working locally\n#pickle.dump(corpus, open('data/sentimentData/corpus.pkl', 'wb'))\n#dictionary.save('data/sentimentData/dictionary.gensim')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\n\n# let LDA find 3 topics\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=15)\n\n# uncomment the code if working locally\n#ldamodel.save('../input/sentimentData/model3.gensim')\n\ntopics = ldamodel.print_topics(num_words=4)\nfor topic in topics:\n    print(topic)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The first topic includes words like *apartment*, *great*, and *walk*, and *minute*. This sounds like a topic related to convenient distances from the accommodation to wherever something interesting was to go to.\n- The second topic includes words like *place*, *room*, *even*, and a mysterious *u* (perhaps u-bahn for the underground?). It seems unclear to me what this was supposed to be about. \n- And the third topic combines words like *great*, *place*, *stay*, and *apartment*, which sounds like a cluster related to overall satisfaction with the home."},{"metadata":{"trusted":true},"cell_type":"code","source":"# now let LDA find 5 topics\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n\n# uncomment the code if working locally\n#ldamodel.save('../input/sentimentData/model5.gensim')\n\ntopics = ldamodel.print_topics(num_words=4)\nfor topic in topics:\n    print(topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# and finally 10 topics\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)\n\n# uncomment the code if working locally\n#ldamodel.save('../input/sentimentData/model10.gensim')\n\ntopics = ldamodel.print_topics(num_words=4)\nfor topic in topics:\n    print(topic)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Putting it all together - the WordCloud, the Frequency Distribution and the Topic Modelling - it is often the following criteria that make someone rate an apartment **positively:**\n1. **The apartment is clean, the bathroom is clean, the bed is comfortable.**\n2. **The apartment is quiet and conducive to getting sound sleep.**\n3. **The area is centrally located with short walking distances, good public transport connections, and has cafes and restaurants nearby.**\n\nApparently, getting the last two means trying to square the circle... but this is true for tourists all over the world.\n\nBefore we move on to the negative comments, let's visualize the LDA model:"},{"metadata":{},"cell_type":"markdown","source":"*3. Visualizing topics*\n\nThe pyLDAvis library is designed to provide a visual interface for interpreting the topics derived from a topic model by extracting information from a fitted LDA topic model.\n\n***The following code should be run locally only!***"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dictionary = gensim.corpora.Dictionary.load('../input/sentimentData/dictionary.gensim')\n#corpus = pickle.load(open('../input/sentimentData/corpus.pkl', 'rb'))\n\n#import pyLDAvis.gensim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizing 5 topics\n#lda = gensim.models.ldamodel.LdaModel.load('../input/sentimentData/model5.gensim')\n#lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n#pyLDAvis.display(lda_display)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In short, the interface provides:\n\n- a left panel that depicts a global view of the model (how prevalent each topic is and how topics relate to each other);\n- a right panel containing a bar chart – the bars represent the terms that are most useful in interpreting the topic currently selected (what the meaning of each topic is).\n\nOn the left, the topics are plotted as circles, whose centers are defined by the computed distance between topics (projected into 2 dimensions). The prevalence of each topic is indicated by the circle’s area. On the right, two juxtaposed bars show the topic-specific frequency of each term (in red) and the corpus-wide frequency (in blueish gray). When no topic is selected, the right panel displays the top 30 most salient terms for the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizing 3 topics\n#lda = gensim.models.ldamodel.LdaModel.load('../input/sentimentData/model3.gensim')\n#lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n#pyLDAvis.display(lda_display)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Back to: <a href='#Table of contents'> Table of contents</a>*\n#### 4.5. Investigating Negative Comments\n<a id='4.5. Investigating Negative Comments'></a>"},{"metadata":{},"cell_type":"markdown","source":"**WordCloud**"},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(max_font_size=200, max_words=200, background_color=\"mistyrose\",\n                      width=3000, height=2000,\n                      stopwords=stopwords.words('english')).generate(str(df_neg.comments.values))\n\nplot_wordcloud(wordcloud, '\\nNegatively Tuned')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Frequency Distribution**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# vectorizing text\nvectorizer = CountVectorizer(stop_words='english')\ndocs = vectorizer.fit_transform(neg_comments)\nfeatures = vectorizer.get_feature_names()\n\n# preparing the plot\nset_palette('flatui')\nplt.figure(figsize=(18,8))\nplt.title('The Top 30 most frequent words used in NEGATIVE comments\\n', fontweight='bold')\n\n# instantiating and fitting the FreqDistVisualizer, plotting the top 30 most frequent terms\nvisualizer = FreqDistVisualizer(features=features, n=30)\nvisualizer.fit(docs)\nvisualizer.poof;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Topic Modelling**"},{"metadata":{},"cell_type":"markdown","source":"*1. Cleaning and Preprocessing*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# calling the cleaning function we defined earlier\ndoc_clean = [clean(comment).split() for comment in neg_comments]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*2. LDA the Gensim way*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a dictionary from the normalized data, convert this to a bag-of-words corpus\ndictionary = corpora.Dictionary(doc_clean)\ncorpus = [dictionary.doc2bow(text) for text in doc_clean]\n\n# save for later use\n# uncomment the code if working locally\n#pickle.dump(corpus, open('data/sentimentData/corpus_neg.pkl', 'wb'))\n#dictionary.save('data/sentimentData/dictionary_neg.gensim')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let LDA find 3 topics\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=15)\n\n# uncomment the code if working locally\n#ldamodel.save('../input/sentimentData/model3_neg.gensim')\n\ntopics = ldamodel.print_topics(num_words=4)\nfor topic in topics:\n    print(topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now let LDA find 5 topics\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n\n# uncomment the code if working locally\n#ldamodel.save('../input/sentimentData/model5_neg.gensim')\n\ntopics = ldamodel.print_topics(num_words=4)\nfor topic in topics:\n    print(topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# and finally 10 topics\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)\n\n# uncomment the code if working locally\n#ldamodel.save('../input/sentimentData/model10_neg.gensim')\n\ntopics = ldamodel.print_topics(num_words=4)\nfor topic in topics:\n    print(topic)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once again, let's put all of the visualizations together and summarize what makes someone rate an apartment **negatively:**\n1. **The apartment and/or bathroom (especially the shower) are dirty.**\n2. **Problems in communicating with the host, e.g. one-sided cancellations by the host or to not being able to get a hold of him/her when having issues.**\n3. **The area is too far away from public transport connections or doesn't meet vistors' expectations in some way.**\n\nBefore we finish analyzing the negative comments, let's visualize the LDA model:"},{"metadata":{},"cell_type":"markdown","source":"*3. Visualizing topics*\n\n***The following code should be run locally only!***"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dictionary = gensim.corpora.Dictionary.load('./input/sentimentData/dictionary_neg.gensim')\n#corpus = pickle.load(open('./input/sentimentData/corpus_neg.pkl', 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizing 5 topics\n#lda = gensim.models.ldamodel.LdaModel.load('../input/sentimentData/model5_neg.gensim')\n#lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n#pyLDAvis.display(lda_display)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizing 3 topics\n#lda = gensim.models.ldamodel.LdaModel.load('../input/sentimentDatamodel3_neg.gensim')\n#lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n#pyLDAvis.display(lda_display)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Back to: <a href='#Table of contents'> Table of contents</a>*\n### 5. Appendix \n<a id='5. Appendix'></a>"},{"metadata":{},"cell_type":"markdown","source":"All resources used in this notebook are listed below.\n\nData\n- Inside Airbnb: http://insideairbnb.com/get-the-data.html\n\nWordClouds\n- https://vprusso.github.io/blog/2018/natural-language-processing-python-3/\n- https://www.datacamp.com/community/tutorials/wordcloud-python\n\nBar Charts\n- http://robertmitchellv.com/blog-bar-chart-annotations-pandas-mpl.html\n\nYellowBrick Visualization\n- http://www.scikit-yb.org/en/latest/index.html\n\nLanguage Detection\n- TextBlob:\n    - https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/\n    - https://github.com/shubhamjn1/TextBlob/blob/master/Textblob.ipynb\n    - https://stackoverflow.com/questions/43485469/apply-textblob-in-for-each-row-of-a-dataframe\n    - https://textblob.readthedocs.io/en/dev/quickstart.html\n<br>\n- Spacy:\n    - https://github.com/nickdavidhaynes/spacy-cld\n    - https://spacy.io/usage/models\n<br>\n- Langdetect & LangId:\n    - https://pypi.org/project/langdetect/ \n    - https://www.probytes.net/blog/python-language-detection/\n    - https://github.com/hb20007/hands-on-nltk-tutorial/blob/master/8-1-The-langdetect-and-langid-Libraries.ipynb\n\nSentiment Analysis\n- *\"Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning\"* (Paperback) by B. Bengfort, R. Bilbro, T. Ojeda, published by O′Reilly\n- Jodie Burchell: http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html\n- Jodie Burchell: http://t-redactyl.io/blog/2017/01/how-do-we-feel-about-new-years-resolutions-according-to-sentiment-analysis.html\n- Jodie Burchell: https://github.com/t-redactyl/Blog-posts/blob/master/2017-04-15-sentiment-analysis-in-vader-and-twitter-api.ipynb\n- http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf\n\n- Susan Li: https://towardsdatascience.com/latent-semantic-analysis-sentiment-classification-with-python-5f657346f6a3\n- Sakshi Gupta (in R): https://towardsdatascience.com/uncovering-hidden-trends-in-airbnb-reviews-11eb924f2fec\n- Dmytro Iakubovskyi: https://towardsdatascience.com/digging-into-airbnb-data-reviews-sentiments-superhosts-and-prices-prediction-part1-6c80ccb26c6a\n- Dmytro Iakubovskyi: https://github.com/Dima806/Airbnb_project/blob/master/airbnb_final_analysis_v3.ipynb\n- Maurizio Santamicone: https://medium.com/@mauriziosantamicone/seattle-confidential-unpacking-airbnb-reviews-with-sentiment-d421c15d8b8f\n- Zhenyu: https://www.kaggle.com/zhenyufan/nlp-for-yelp-reviews/notebook?utm_medium=email&utm_source=intercom&utm_campaign=datanotes-2019\n\nTopic Modeling / LDA\n- Analytics Vidhya: https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n- Susan Li: https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21\n- https://radimrehurek.com/gensim/models/ldamodel.html\n- https://www.objectorientedsubject.net/2018/08/experiments-on-topic-modeling-pyldavis/\n\nDiverse\n- https://data-viz-for-fun.com/2018/08/airbnb-data-viz/"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}