{"cells":[{"metadata":{},"cell_type":"markdown","source":"**In this script we train a neuronal network to classify fake news.**\n\n![](http://)The usage of this trained model as a telegram bot can be found in the following github repo:\nhttps://github.com/tschomacker/fake-news-detection-bot\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"news = list()\n\nfor i, entry in enumerate(open(\"/kaggle/input/fake-and-real-news-dataset/True.csv\").readlines()):\n    if i == 0:\n        continue\n    x = entry.split(\",\")\n    if len(x) <2:\n        continue\n    news.append((0, x[1]))\n    \nfor i, entry in enumerate(open(\"/kaggle/input/fake-and-real-news-dataset/Fake.csv\").readlines()):\n    if i == 0:\n        continue\n    x = entry.split(\",\")\n    if len(x) < 2:\n        continue\n    news.append((1, x[1]))\n\nfrom random import shuffle\nshuffle(news)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/bert-base-multilingual-cased-sentence\")\n\nmodel = AutoModel.from_pretrained(\"DeepPavlov/bert-base-multilingual-cased-sentence\").to(device)\n\n_input = tokenizer(\"Das ist ein text\", padding = True, truncation= True, return_tensors = \"pt\").to(device)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model(**_input)[0].mean(1).size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BinaryClassifier(torch.nn.Module):\n    def __init__(self,input_dim, num_classes):\n        super(BinaryClassifier, self).__init__()\n        self.dropout = torch.nn.Dropout(0.1)\n        self.f1 = torch.nn.Linear(input_dim, input_dim * 2)\n        self.activation = torch.nn.Softsign()\n        self.f2 = torch.nn.Linear( input_dim * 2, input_dim)\n        self.f3 = torch.nn.Linear( input_dim, num_classes)\n        self.softmax = torch.nn.Softmax()\n        \n        \n    def forward(self,embedding, label = None):\n        x = self.dropout(embedding)\n        x = self.f1(x)\n        x = self.activation(x)\n        x = self.f2(x)\n        x = self.activation(x)\n        x = self.f3(x)\n        loss = 0\n        if label:\n            label = torch.LongTensor(label).to(device)\n            loss_fc = torch.nn.CrossEntropyLoss()\n            loss = loss_fc(x, label)\n        # todo use softmax, wenn noch labels enabled\n        return (loss, x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\nwith torch.no_grad():\n    model.eval()\n    new = list()\n    for sent in tqdm(news):\n        #print(sent)\n        inp = tokenizer(sent[1], padding = True, truncation= True, return_tensors = \"pt\").to(device)\n        embedding = model(**inp)[1]\n        print(embedding.size())\n        #print()\n        label = sent[0]\n        new.append((embedding, label, sent[1]))\n    news = new\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nbs_size = 64\nepochs = 200\n\nbatches = [news[i:i+bs_size] for i in range(0,len(news), bs_size)]\nbatches = batches[0:int(len(batches)*0.8)]\nbatches_eval = batches[int(len(batches)*0.8):len(batches)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = BinaryClassifier(768, 2).to(device)\n\noptimizer = torch.optim.Adam(classifier.parameters(), lr = 0.00001)\npbar = tqdm\nfor epoch in range(epochs):\n    losses = list()\n    shuffle(batches)\n    for batch in batches:\n        classifier.train()\n        optimizer.zero_grad()\n        embeddings = torch.cat([e[0] for e in batch]).to(device)\n        \n        loss = classifier(embeddings, [e[1] for e in batch])[0]\n        losses.append(loss.item())\n\n        loss.backward()\n        optimizer.step()\n    print(epoch,(sum(losses) / len(losses)))\n\n\nclassifier.cpu()\ntorch.save(classifier.state_dict(), \"./classifier.pt\")\nclassifier.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.load_state_dict(torch.load('./classifier.pt'))\nclassifier.to(device)\ntrue_count = 0\nfalse_count = 0\nfake = 0\nnot_fake = 0\nclassifier.eval()\na = torch.nn.Softmax()\nfor batch in tqdm(batches_eval):\n    classifier.eval()\n    embeddings = torch.cat([e[0] for e in batch]).to(device)\n    with torch.no_grad():\n        classes = a(classifier(embeddings, [e[1] for e in batch])[1]).tolist()\n    for pred, true in zip(classes, [e[1] for e in batch]):\n        if pred[0] > pred[1] and true == 0:\n            true_count += 1\n            not_fake += 1\n        elif pred[0] < pred[1] and true == 1:\n            true_count += 1\n            fake += 1\n        else:\n            false_count += 1\nprint(true_count, false_count, fake, not_fake)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(text):\n    model.to(device)\n    inp = tokenizer(text, padding = True, truncation= True, return_tensors = \"pt\").to(device)\n    embedding = model(**inp)[1]\n    classifier.eval()\n    a = torch.nn.Softmax()\n    x = a(classifier(embedding)[1])\n    #print(x)\n    return x\n\ndef pred_in_human_lang(text):\n    x = predict(text).tolist()[0]\n    if x[0] > x[1]:\n        print(\"not fake\")\n        return \"not fake\"\n    else:\n        print(\"fake\")\n        return \"fake\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_in_human_lang(\"Die US Wahlen wurden nicht manipuliert\")\npred_in_human_lang(\"Die US Wahlen wurden manipuliert\")\npred_in_human_lang(\"Election fraud decided the us elections\")\npred_in_human_lang(\"QAnon reshaped Trump’s party and radicalized believers. The Capitol siege may just be the start.\")\npred_in_human_lang(\"Die corona pandemie gerät außer Kontrolle\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}