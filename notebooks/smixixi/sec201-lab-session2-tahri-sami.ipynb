{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-07T20:29:17.744764Z","iopub.execute_input":"2021-06-07T20:29:17.745167Z","iopub.status.idle":"2021-06-07T20:29:17.764051Z","shell.execute_reply.started":"2021-06-07T20:29:17.74508Z","shell.execute_reply":"2021-06-07T20:29:17.762485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analysis goal","metadata":{}},{"cell_type":"markdown","source":"1. You will perform Principal Component Analysis, Linear Regression, and well as XGBoost learning.\nFor each of these analysis algorithms, list the information you expect to extract","metadata":{}},{"cell_type":"markdown","source":"- PCA will be used to project our dataset into principal components (e.g a new base), we will be using it to reduce dimensions from our datas, and find the best way to represent them.\n- Linear Regression is used to find the best fit for a lineary combination our data vectors (and n-dimensions line, easy to explain). This could be use in the PC space.\n- XGBoost is a tree gradient boosting algorithm that try to infers rules to classify our data. We expect it to give us insight of why a data should be considerer as anomaly or not through a decision tree.\n","metadata":{}},{"cell_type":"markdown","source":"# Loading data","metadata":{}},{"cell_type":"markdown","source":"2. Rebuild the analysis environment from lab session 1\n2.1. In File > Add or Include Data, search for “UNSW_NB15” dataset and include it\n2.2. In ‘Data > input > unsw-nb15‘, get the exact path of CSV file 'UNSW_NB15_training-set.csv'\nand load it as training_set using Pandas.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_training-set.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:29:19.986838Z","iopub.execute_input":"2021-06-07T20:29:19.987183Z","iopub.status.idle":"2021-06-07T20:29:20.41894Z","shell.execute_reply.started":"2021-06-07T20:29:19.987153Z","shell.execute_reply":"2021-06-07T20:29:20.418249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"3. Create a new DataFrame X you will use for cleaning. Store the labels in dataframe Y","metadata":{}},{"cell_type":"code","source":"X = df.copy()\nY = X.label","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:29:21.11405Z","iopub.execute_input":"2021-06-07T20:29:21.114486Z","iopub.status.idle":"2021-06-07T20:29:21.124924Z","shell.execute_reply.started":"2021-06-07T20:29:21.114457Z","shell.execute_reply":"2021-06-07T20:29:21.123557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. Remove labels bound to attacks as well as the ‘id’ field to avoid biased learning","metadata":{}},{"cell_type":"code","source":"col = [\"attack_cat\", \"label\", \"id\"]\nX.drop(col, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:29:21.682169Z","iopub.execute_input":"2021-06-07T20:29:21.682516Z","iopub.status.idle":"2021-06-07T20:29:21.697807Z","shell.execute_reply.started":"2021-06-07T20:29:21.68247Z","shell.execute_reply":"2021-06-07T20:29:21.696811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5. List string fields, and perform one-hot encoding.","metadata":{}},{"cell_type":"code","source":"string_fields = X.select_dtypes('object').columns.values","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:29:23.521923Z","iopub.execute_input":"2021-06-07T20:29:23.522398Z","iopub.status.idle":"2021-06-07T20:29:23.531281Z","shell.execute_reply.started":"2021-06-07T20:29:23.522368Z","shell.execute_reply":"2021-06-07T20:29:23.530349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(string_fields)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:29:23.681944Z","iopub.execute_input":"2021-06-07T20:29:23.682429Z","iopub.status.idle":"2021-06-07T20:29:23.687578Z","shell.execute_reply.started":"2021-06-07T20:29:23.682399Z","shell.execute_reply":"2021-06-07T20:29:23.686333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encode those fields\nX = pd.get_dummies(X, columns=string_fields)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:29:24.802771Z","iopub.execute_input":"2021-06-07T20:29:24.803116Z","iopub.status.idle":"2021-06-07T20:29:24.891672Z","shell.execute_reply.started":"2021-06-07T20:29:24.803085Z","shell.execute_reply":"2021-06-07T20:29:24.890281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Machine Learning Analysis","metadata":{}},{"cell_type":"markdown","source":"6. Through train_test_split on X and Y, and the XGBClassifier\n6.1. Extract train and test dataframes for X and Y","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nSEED = 1 # let's make it less 'random' for the notebook","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:29:59.047723Z","iopub.execute_input":"2021-06-07T20:29:59.048125Z","iopub.status.idle":"2021-06-07T20:29:59.052067Z","shell.execute_reply.started":"2021-06-07T20:29:59.048088Z","shell.execute_reply":"2021-06-07T20:29:59.05125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state = SEED)\nprint(f\"Train size = {len(X_train)}, Test_size = {len(X_test)}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:29:59.224159Z","iopub.execute_input":"2021-06-07T20:29:59.224697Z","iopub.status.idle":"2021-06-07T20:29:59.515685Z","shell.execute_reply.started":"2021-06-07T20:29:59.224667Z","shell.execute_reply":"2021-06-07T20:29:59.51511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6.2. Train the model","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier, plot_importance, plot_tree, to_graphviz\nfrom sklearn.metrics import roc_auc_score\n\nxgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', objective='binary:logistic')\nxgb_model.fit(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:29:59.516746Z","iopub.execute_input":"2021-06-07T20:29:59.51714Z","iopub.status.idle":"2021-06-07T20:30:18.123229Z","shell.execute_reply.started":"2021-06-07T20:29:59.517113Z","shell.execute_reply":"2021-06-07T20:30:18.122094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6.3. Extract the AUPRC","metadata":{}},{"cell_type":"code","source":"roc_auc = roc_auc_score(Y_test, xgb_model.predict(X_test))\nprint(f\"Roc Auc score = {roc_auc}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:30:18.124814Z","iopub.execute_input":"2021-06-07T20:30:18.125037Z","iopub.status.idle":"2021-06-07T20:30:18.212742Z","shell.execute_reply.started":"2021-06-07T20:30:18.125013Z","shell.execute_reply":"2021-06-07T20:30:18.211752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6.4. Plot the relative importance of fields through plot_importance, using importance_type\n‘gain’, ‘weight’, ‘cover’","metadata":{}},{"cell_type":"code","source":"for importance in ['gain', 'weight', 'cover']:\n    plot_importance(xgb_model, importance_type=importance, max_num_features=15)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:30:18.213896Z","iopub.execute_input":"2021-06-07T20:30:18.214285Z","iopub.status.idle":"2021-06-07T20:30:18.969006Z","shell.execute_reply.started":"2021-06-07T20:30:18.214257Z","shell.execute_reply":"2021-06-07T20:30:18.968089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6.5. What do you conclude wrt. the security status of the target system?\n\nWe find that the most distinctive feature is sttl (according to the gain), the most weighted feature is sbytes (it may be used to discerns type of attacks in each decision tree ?), and the top3  covered features are protocols (sun-nd, mobile and swipe), which may means those are frequent to attacks because they are vulnerable. Reminder: those protocols don't appear in the normal traffic.\n\n\n6.6. Show the classifier visually","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, dpi=100)\nfig.set_size_inches(100, 50)\nplot_tree(xgb_model, ax=ax)\nplt.plot()\nplt.savefig(\"tree.png\")\n# Available in the output ","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:30:18.970038Z","iopub.execute_input":"2021-06-07T20:30:18.970339Z","iopub.status.idle":"2021-06-07T20:30:26.44866Z","shell.execute_reply.started":"2021-06-07T20:30:18.970315Z","shell.execute_reply":"2021-06-07T20:30:26.447299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6.7. For XGBClassifier\n\n6.7.1.List the observations you make\n\nWe can see the decision process used by the graph. For instance, sttl is used first to determine if there is an attack or not. Then, we can see the left part of the tree check protocols, as mentionned, this is easy, because there is no normal traffic with those proto (i expect it to expand to all others proto in the dataset that are in attacks and not in normal traffic). We can see afterward how the tree splits the dataset using features : synack, smean, ct_dst_src_ltm ...\n\n\n\n6.7.2.Which security-related information do you draw?\n\nAny non-authorized protocols should be banned in the firewall, and if the traffic is too high (right part of the graph), it mights be attack (potential DDoS). \n\n\n6.7.3.Which recommendations can you emit based on these observations?\n\nCheck firewalls rules for unwanted protocols, implement an anti-DDOS system.","metadata":{}},{"cell_type":"markdown","source":"# Statistics analysis","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n# We will use 3 PCA, but we can discard the last one anyway.\nx=StandardScaler().fit_transform(X.copy())\npca = PCA(n_components=3)\npca_x = pca.fit_transform(x)\n\nprincipalDf = pd.DataFrame(data=pca_x, columns = ['PC1', \"PC2\", \"PC3\"])\nfinalDf = pd.concat([principalDf, Y], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:29:36.258165Z","iopub.execute_input":"2021-06-07T20:29:36.25856Z","iopub.status.idle":"2021-06-07T20:29:38.149658Z","shell.execute_reply.started":"2021-06-07T20:29:36.258524Z","shell.execute_reply":"2021-06-07T20:29:38.148823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize point first in 3D before a 2D projection\nimport plotly.express as px\nfig = px.scatter_3d(finalDf, x='PC1', y='PC2', z='PC3',\n              color='label')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:29:40.626015Z","iopub.execute_input":"2021-06-07T20:29:40.626485Z","iopub.status.idle":"2021-06-07T20:29:42.574803Z","shell.execute_reply.started":"2021-06-07T20:29:40.626454Z","shell.execute_reply":"2021-06-07T20:29:42.572408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(8,8))\nax = fig.add_subplot()\nax.set_xlabel('PC 1', fontsize=15)\nax.set_ylabel('PC 2', fontsize=15)\nax.set_title('2 component PCA', fontsize=20)\n\ntargets = [0,1]\ncolors = ['r', 'b']\n\nfor target, color in zip(targets, colors):\n    indicesToKeep = finalDf.label == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'PC1'],\n               finalDf.loc[indicesToKeep, 'PC2'],\n               c = color,\n               s = 50)\n    ax.legend(targets)\n    ax.grid()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:37:44.735735Z","iopub.execute_input":"2021-06-07T20:37:44.736206Z","iopub.status.idle":"2021-06-07T20:37:46.086853Z","shell.execute_reply.started":"2021-06-07T20:37:44.736171Z","shell.execute_reply":"2021-06-07T20:37:46.085349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we use only 2 PC, we will loose a LOT of information, as the PC3 make it easy to identify if its attack or not ...","metadata":{}},{"cell_type":"code","source":"print(\"Independant parameters = \" + str(pca.explained_variance_))","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:37:46.088807Z","iopub.execute_input":"2021-06-07T20:37:46.089189Z","iopub.status.idle":"2021-06-07T20:37:46.096636Z","shell.execute_reply.started":"2021-06-07T20:37:46.08915Z","shell.execute_reply":"2021-06-07T20:37:46.095194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n# Linear reg in PCA for attacks \nlr = LinearRegression()\nattacks = finalDf.loc[finalDf.label == 1]\nlr.fit(attacks[['PC1']], attacks['PC2'])\nscore= lr.score(attacks[['PC1']], attacks['PC2'])\nprint(\"Intercept for PC1-PC2 for attacks: \" + str(lr.intercept_))\nprint(\"Coef for PC1-PC2 for attacks: \" + str(lr.coef_))\nprint(\"Score: \" + str(score))","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:37:46.098351Z","iopub.execute_input":"2021-06-07T20:37:46.098637Z","iopub.status.idle":"2021-06-07T20:37:46.123645Z","shell.execute_reply.started":"2021-06-07T20:37:46.098612Z","shell.execute_reply":"2021-06-07T20:37:46.122969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clearly, this is not convincing for a linear regression ! We can split manually the linear regression in two","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n# Linear reg in PCA for attacks \nlr = LinearRegression()\nattacks_part1 = finalDf.loc[(finalDf.label == 1) & (finalDf['PC1']<2)]\nlr.fit(attacks_part1[['PC1']], attacks_part1['PC2'])\nscore= lr.score(attacks_part1[['PC1']], attacks_part1['PC2'])\nprint(\"----- PC 1 < 2 ----\")\nprint(\"Intercept for PC1-PC2 for attacks: \" + str(lr.intercept_))\nprint(\"Coef for PC1-PC2 for attacks: \" + str(lr.coef_))\nprint(\"Score: \" + str(score))\nc1s = lr.coef_","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:37:46.124803Z","iopub.execute_input":"2021-06-07T20:37:46.125225Z","iopub.status.idle":"2021-06-07T20:37:46.143714Z","shell.execute_reply.started":"2021-06-07T20:37:46.125195Z","shell.execute_reply":"2021-06-07T20:37:46.143079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n# Linear reg in PCA for attacks \nlr = LinearRegression()\nattacks_part1 = finalDf.loc[(finalDf.label == 1) & (finalDf['PC1']>2)]\nlr.fit(attacks_part1[['PC1']], attacks_part1['PC2'])\nscore= lr.score(attacks_part1[['PC1']], attacks_part1['PC2'])\nprint(\"----- PC 1 > 2 ----\")\nprint(\"Intercept for PC1-PC2 for attacks: \" + str(lr.intercept_))\nprint(\"Coef for PC1-PC2 for attacks: \" + str(lr.coef_))\nprint(\"Score: \" + str(score))\nc2s = lr.coef_","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:37:46.144805Z","iopub.execute_input":"2021-06-07T20:37:46.145207Z","iopub.status.idle":"2021-06-07T20:37:46.162122Z","shell.execute_reply.started":"2021-06-07T20:37:46.145177Z","shell.execute_reply":"2021-06-07T20:37:46.161474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ah, here we have two of them. Let's try first 3D for part 2.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n# Linear reg in PCA for attacks \nlr = LinearRegression()\nattacks_part2 = finalDf.loc[(finalDf.label == 1) & (finalDf['PC1']>2)]\nlr.fit(attacks_part2[['PC1', 'PC2']], attacks_part2['PC3'])\nscore= lr.score(attacks_part2[['PC1', 'PC2']], attacks_part2['PC3'])\nprint(\"----- PC 1 > 2 ----\")\nprint(\"Intercept for PC1-PC2 for attacks: \" + str(lr.intercept_))\nprint(\"Coef for PC1-PC2 for attacks: \" + str(lr.coef_))\nprint(\"Score: \" + str(score))\n\ncoefs_attacks_part2 = lr.coef_","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:37:46.25596Z","iopub.execute_input":"2021-06-07T20:37:46.256294Z","iopub.status.idle":"2021-06-07T20:37:46.280241Z","shell.execute_reply.started":"2021-06-07T20:37:46.256265Z","shell.execute_reply":"2021-06-07T20:37:46.279013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's compare with normal traffic.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n# Linear reg in PCA for attacks \nlr = LinearRegression()\nnormal_part2 = finalDf.loc[(finalDf.label == 0) & (finalDf['PC1']>2)]\nlr.fit(normal_part2[['PC1', 'PC2']], normal_part2['PC3'])\nscore= lr.score(attacks_part2[['PC1', 'PC2']], attacks_part2['PC3'])\nprint(\"----- PC 1 > 2 ----\")\nprint(\"Intercept for PC1-PC2 for normal: \" + str(lr.intercept_))\nprint(\"Coef for PC1-PC2 for normal: \" + str(lr.coef_))\nprint(\"Score: \" + str(score))\n\ncoefs_normal_part2 = lr.coef_","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:37:46.581205Z","iopub.execute_input":"2021-06-07T20:37:46.581583Z","iopub.status.idle":"2021-06-07T20:37:46.60345Z","shell.execute_reply.started":"2021-06-07T20:37:46.581551Z","shell.execute_reply":"2021-06-07T20:37:46.602283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n# Linear reg in PCA for attacks \nlr = LinearRegression()\nnormal_part1 = finalDf.loc[(finalDf.label == 0) & (finalDf['PC1']>2)]\nlr.fit(normal_part1[['PC1', 'PC2']], normal_part1['PC3'])\nscore= lr.score(attacks_part2[['PC1', 'PC2']], attacks_part2['PC3'])\nprint(\"----- PC 1 > 2 ----\")\nprint(\"Intercept for PC1-PC2 for normal: \" + str(lr.intercept_))\nprint(\"Coef for PC1-PC2 for normal: \" + str(lr.coef_))\nprint(\"Score: \" + str(score))\n\ncoefs_normal_part2 = lr.coef_","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:37:46.770105Z","iopub.execute_input":"2021-06-07T20:37:46.77048Z","iopub.status.idle":"2021-06-07T20:37:46.794068Z","shell.execute_reply.started":"2021-06-07T20:37:46.770444Z","shell.execute_reply":"2021-06-07T20:37:46.793206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Just to compute how those two vector are different using angle as metric. For part 2, using 3 PC\nimport math\nvector_1 = coefs_attacks_part2\nvector_2 = coefs_normal_part2\nunit_vector_1 = vector_1 / np.linalg.norm(vector_1)\nunit_vector_2 = vector_2 / np.linalg.norm(vector_2)\ndot_product = np.dot(unit_vector_1, unit_vector_2)\nangle = np.arccos(dot_product)\n\nprint(math.degrees(angle))","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:37:46.931822Z","iopub.execute_input":"2021-06-07T20:37:46.932146Z","iopub.status.idle":"2021-06-07T20:37:46.939742Z","shell.execute_reply.started":"2021-06-07T20:37:46.932116Z","shell.execute_reply":"2021-06-07T20:37:46.938752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok, now keeping on 2D PCA, with part 1 and 2","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n# Linear reg in PCA for attacks \nlr = LinearRegression()\nnormal_part2 = finalDf.loc[(finalDf.label == 0) & (finalDf['PC1']>2)]\nlr.fit(normal_part2[['PC1']], normal_part2['PC2'])\nscore= lr.score(attacks_part2[['PC1']], attacks_part2['PC2'])\nprint(\"----- PC 1 > 2 ----\")\nprint(\"Intercept for PC1-PC2 for normal: \" + str(lr.intercept_))\nprint(\"Coef for PC1-PC2 for attacks: \" + str(lr.coef_))\nprint(\"Score: \" + str(score))\n\ncn2 = lr.coef_","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:37:49.478802Z","iopub.execute_input":"2021-06-07T20:37:49.479134Z","iopub.status.idle":"2021-06-07T20:37:49.500631Z","shell.execute_reply.started":"2021-06-07T20:37:49.479104Z","shell.execute_reply":"2021-06-07T20:37:49.499135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n# Linear reg in PCA for attacks \nlr = LinearRegression()\nnormal_part1 = finalDf.loc[(finalDf.label == 0) & (finalDf['PC1']<2)]\nlr.fit(normal_part1[['PC1']], normal_part1['PC2'])\nscore= lr.score(normal_part1[['PC1']], normal_part1['PC2'])\nprint(\"----- PC 1 < 2 ----\")\nprint(\"Intercept for PC1-PC2 for normal: \" + str(lr.intercept_))\nprint(\"Coef for PC1-PC2 for attacks: \" + str(lr.coef_))\nprint(\"Score: \" + str(score))\n\ncn1 = lr.coef_\n\n# The score is too low on this one !","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:37:49.783934Z","iopub.execute_input":"2021-06-07T20:37:49.784458Z","iopub.status.idle":"2021-06-07T20:37:49.80393Z","shell.execute_reply.started":"2021-06-07T20:37:49.784427Z","shell.execute_reply":"2021-06-07T20:37:49.802644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Just to compute how those two vector are different using angle as metric. For part 1, using 2 PC\n\nprint(\"Diff angle between normal and attack for PC1 <2\")\nprint(math.degrees(np.arctan(cn1) - np.arctan(c1s)))\n\n\n# Just to compute how those two vector are different using angle as metric. For part 2, using 2 PC\n\nprint(\"Diff angle between normal and attack for PC1 >2\")\nprint(math.degrees(np.arctan(cn2) - np.arctan(c2s)))","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:37:49.861886Z","iopub.execute_input":"2021-06-07T20:37:49.862263Z","iopub.status.idle":"2021-06-07T20:37:49.870227Z","shell.execute_reply.started":"2021-06-07T20:37:49.862228Z","shell.execute_reply":"2021-06-07T20:37:49.868717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The issue is that the data in part 1 are way too mixed, we cannot really use the regression to differentiate between the do. We will work now on part 2.","metadata":{}},{"cell_type":"markdown","source":"We now have PC2 = PC1 * 1.51, let's reverse to the normal space","metadata":{}},{"cell_type":"code","source":"vpc1 = pca.components_[0] \nvpc2 = pca.components_[1]\n\nreg_p1 = vpc1+c1s*vpc1\nreg_p2 = vpc1+c2s*vpc2","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:37:51.242969Z","iopub.execute_input":"2021-06-07T20:37:51.243309Z","iopub.status.idle":"2021-06-07T20:37:51.249336Z","shell.execute_reply.started":"2021-06-07T20:37:51.24328Z","shell.execute_reply":"2021-06-07T20:37:51.248024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_max_index(i, array):\n    return np.where(array == sorted(array)[i])\n\nprint(\"Top 5 components for first regression\")\nfor i in range(5):\n    index = find_max_index(i, reg_p2)[0][0]\n    print(X.columns[index])\n\nprint(\"****************\")\n    \nprint(\"Top 5 components for second regression\")\nfor i in range(5):\n    index = find_max_index(i, reg_p1)[0][0]\n    print(X.columns[index])","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:37:54.462647Z","iopub.execute_input":"2021-06-07T20:37:54.463109Z","iopub.status.idle":"2021-06-07T20:37:54.474794Z","shell.execute_reply.started":"2021-06-07T20:37:54.463072Z","shell.execute_reply":"2021-06-07T20:37:54.473591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"9. For statistical analysis\n\n9.1.1.List the observations you make\n    We can clearly identify attacks to normal traffic on a part of the PCA space, but half of them seems to be mixed with normal traffic (we can explore in PCA3). We used linear regression in PC space and transformed back, and used two regression. While normal trafffic isn't really linear, attacks is half linear.\n\n9.1.2.Which security-related information do you draw?\n\nWe found that a lot of attack traffic is similar to normal traffic, and some are not. We can identify the top features (above) for both linear regression that participate the most to being an attack or not.\n\n9.1.3.Which recommendations can you emit based on these observations?\n\nCheck the dwin, proto, swin, state_FIN and dttl values in the traffic, they may reveal potential scans in the traffic (nmap, topology scanner).\n","metadata":{}},{"cell_type":"markdown","source":"10) Building graph and metrics of the model.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import PrecisionRecallDisplay\n\nY_predicted = xgb_model.predict_proba(X_test)\nY_predicted_2 = [v[1] for v in Y_predicted]\nprec, recall, _ = precision_recall_curve(Y_test, Y_predicted_2)\npr_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:43:29.202304Z","iopub.execute_input":"2021-06-07T20:43:29.202703Z","iopub.status.idle":"2021-06-07T20:43:29.405299Z","shell.execute_reply.started":"2021-06-07T20:43:29.202673Z","shell.execute_reply":"2021-06-07T20:43:29.404496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import RocCurveDisplay\nfrom sklearn.metrics import roc_curve\n\nfpr, tpr, _ = roc_curve(Y_test, Y_predicted_2)\nroc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:43:29.434736Z","iopub.execute_input":"2021-06-07T20:43:29.435069Z","iopub.status.idle":"2021-06-07T20:43:29.554928Z","shell.execute_reply.started":"2021-06-07T20:43:29.435033Z","shell.execute_reply":"2021-06-07T20:43:29.553446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\n\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate 3 plots: the test and training learning curve, the training\n    samples vs fit times curve, the fit times vs score curve.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        An estimator instance implementing `fit` and `predict` methods which\n        will be cloned for each validation.\n\n    title : str\n        Title for the chart.\n\n    X : array-like of shape (n_samples, n_features)\n        Training vector, where ``n_samples`` is the number of samples and\n        ``n_features`` is the number of features.\n\n    y : array-like of shape (n_samples) or (n_samples, n_features)\n        Target relative to ``X`` for classification or regression;\n        None for unsupervised learning.\n\n    axes : array-like of shape (3,), default=None\n        Axes to use for plotting the curves.\n\n    ylim : tuple of shape (2,), default=None\n        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 5-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like of shape (n_ticks,)\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the ``dtype`` is float, it is regarded\n        as a fraction of the maximum size of the training set (that is\n        determined by the selected validation method), i.e. it has to be within\n        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n        sets. Note that for classification the number of samples usually have\n        to be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1)\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n\n    return plt","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:43:32.965483Z","iopub.execute_input":"2021-06-07T20:43:32.965957Z","iopub.status.idle":"2021-06-07T20:43:33.23879Z","shell.execute_reply.started":"2021-06-07T20:43:32.965925Z","shell.execute_reply":"2021-06-07T20:43:33.237756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_learning_curve(xgb_model, \"Learning curve for XGBoost\", X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:43:34.750188Z","iopub.execute_input":"2021-06-07T20:43:34.750709Z","iopub.status.idle":"2021-06-07T20:47:05.029342Z","shell.execute_reply.started":"2021-06-07T20:43:34.750679Z","shell.execute_reply":"2021-06-07T20:47:05.027948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"11. Which information do you draw?\nThe model seems really good PRC and ROC curve, as they almost have an area of 1 on the validation dataset. It means it is has few false positive and detect almost all attacks.\nConcerning the learning curve, while they're is a little overfit (the training and cross validated datasets are off by 1.5%), it generalizes quite good, as the margin are stable during the learning process, and the performance too.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"12. Which recommendation do you emit\nWe can use our classifier only if we are sure the dataset is really representing the real life, which means we should try it live :).","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}