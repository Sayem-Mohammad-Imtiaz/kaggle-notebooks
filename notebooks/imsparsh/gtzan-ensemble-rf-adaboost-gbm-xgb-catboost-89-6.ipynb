{"cells":[{"metadata":{},"cell_type":"markdown","source":"# GTZAN - ML Ensembles (RF/AdaBoost/GBM/XGB/CatBoost)\n\n`Music Genre Classification Problem`. Experts have been trying for a long time to understand sound & what differentiates one from another. How to visualize sound. What makes one tone different from another.\n\nWe are going to analyze the features extracted from the GTZAN dataset and build different type of ensemble models to see how better we can differentiate one genre from another.\n\nOur Datasets contains 10 genres:-\n- Blues\n- Classical\n- Country\n- Disco\n- Hiphop\n- Jazz\n- Metal\n- Pop\n- Reggae\n- Rock\n\nWe will be applying following Ensemble Algorithms:-\n\n- Random Forest (RF)\n- AdaBoost\n- Gradient Boosting Machine (GBM)\n- Extreme Gradient Boosting (XGB)\n- CatBoost"},{"metadata":{},"cell_type":"markdown","source":"# Reading & Understanding Data\n## Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy.stats import uniform, randint\nimport sklearn.metrics as skm\nimport sklearn.model_selection as skms\nimport sklearn.preprocessing as skp\nfrom sklearn.linear_model import LogisticRegression\nimport sklearn.ensemble as ske\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom pprint import pprint\nimport random\nimport librosa, IPython\nimport librosa.display as lplt\nseed = 12\nnp.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/gtzan-dataset-music-genre-classification/Data/features_3_sec.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## About dataset"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"print(\"Dataset has\",df.shape)\nprint(\"Count of Positive and Negative samples\")\ndf.label.value_counts().reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"audio_fp = '/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original/blues/blues.00000.wav'\naudio_data, sr = librosa.load(audio_fp)\naudio_data, _ = librosa.effects.trim(audio_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# play sample file\nIPython.display.Audio(audio_data, rate=sr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot sample file\nplt.figure(figsize=(15,5))\nlplt.waveplot(audio_data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Default FFT window size\nn_fft = 2048 # window size\nhop_length = 512 # window hop length for STFT\n\nstft = librosa.stft(audio_data, n_fft=n_fft, hop_length=hop_length)\nstft_db = librosa.amplitude_to_db(stft, ref=np.max)\n\nplt.figure(figsize=(12,4))\nlplt.specshow(stft, sr=sr, x_axis='time', y_axis='hz')\nplt.colorbar()\nplt.title(\"Spectrogram with amplitude\")\nplt.show()\n\nplt.figure(figsize=(12,4))\nlplt.specshow(stft_db, sr=sr, x_axis='time', y_axis='log', cmap='cool')\nplt.colorbar()\nplt.title(\"Spectrogram with decibel log\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot zoomed audio wave \nstart = 1000\nend = 1200\nplt.figure(figsize=(16,4))\nplt.plot(audio_data[start:end])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mel_spec = librosa.feature.melspectrogram(audio_data, sr=sr)\nmel_spec_db = librosa.amplitude_to_db(mel_spec, ref=np.max)\nplt.figure(figsize=(16,6))\nlplt.specshow(mel_spec_db, sr=sr, hop_length=hop_length, x_axis='time', y_axis='log', cmap='cool')\nplt.colorbar()\nplt.title(\"Mel Spectrogram\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chroma = librosa.feature.chroma_stft(audio_data, sr=sr)\nplt.figure(figsize=(16,6))\nlplt.specshow(chroma, sr=sr, x_axis='time', y_axis='chroma', cmap='coolwarm')\nplt.colorbar()\nplt.title(\"Chroma Features\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Computing the Correlation Matrix\nspike_cols = [col for col in df.columns if 'mean' in col]\ncorr = df[spike_cols].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(16, 11));\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(0, 25, as_cmap=True, s = 90, l = 45, n = 5)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\nplt.title('Correlation Heatmap (MEAN variables)', fontsize = 20)\nplt.xticks(fontsize = 10)\nplt.yticks(fontsize = 10);\nplt.savefig(\"Corr_Heatmap.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df[[\"label\", \"tempo\"]]\n\nfig, ax = plt.subplots(figsize=(16, 8));\nsns.boxplot(x = \"label\", y = \"tempo\", data = x, palette = 'husl');\n\nplt.title('BPM Boxplot for Genres', fontsize = 20)\nplt.xticks(fontsize = 14)\nplt.yticks(fontsize = 10);\nplt.xlabel(\"Genre\", fontsize = 15)\nplt.ylabel(\"BPM\", fontsize = 15)\nplt.savefig(\"BPM_Boxplot.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df.iloc[0:, 1:]\ny = data['label']\nX = data.loc[:, data.columns != 'label']\n\n# normalize\ncols = X.columns\nmin_max_scaler = skp.MinMaxScaler()\nnp_scaled = min_max_scaler.fit_transform(X)\nX = pd.DataFrame(np_scaled, columns = cols)\n\n# Top 2 pca components\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(X)\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['pc1', 'pc2'])\n\n# concatenate with target label\nfinalDf = pd.concat([principalDf, y], axis = 1)\n\nplt.figure(figsize = (16, 9))\nsns.scatterplot(x = \"pc1\", y = \"pc2\", data = finalDf, hue = \"label\", alpha = 0.7, s = 100);\n\nplt.title('PCA on Genres', fontsize = 20)\nplt.xticks(fontsize = 14)\nplt.yticks(fontsize = 10);\nplt.xlabel(\"Principal Component 1\", fontsize = 15)\nplt.ylabel(\"Principal Component 2\", fontsize = 15)\nplt.savefig(\"PCA_Scattert.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation\n\n- Treat missing values.\n- Outlier Treatment\n- Define dummy variables for categorical variables."},{"metadata":{},"cell_type":"markdown","source":"## Missing Value Treatment"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# find all columns with any NA values\nprint(\"Columns with NA values are\",list(df.columns[df.isnull().any()]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`No null values in the dataset`\n\n\n\n`There are no categorical variable as such. Hence, Dummy variable creation is not needed.`"},{"metadata":{},"cell_type":"markdown","source":"## Encode Genre Label"},{"metadata":{"trusted":true},"cell_type":"code","source":"# map labels to index\nlabel_index = dict()\nindex_label = dict()\nfor i, x in enumerate(df.label.unique()):\n    label_index[x] = i\n    index_label[i] = x\nprint(label_index)\nprint(index_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# update labels in df to index\ndf.label = [label_index[l] for l in df.label]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split Train Dev & Test Sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# shuffle samples\ndf_shuffle = df.sample(frac=1, random_state=seed).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove irrelevant columns\ndf_shuffle.drop(['filename', 'length'], axis=1, inplace=True)\ndf_y = df_shuffle.pop('label')\ndf_X = df_shuffle\n\n# split into train dev and test\nX_train, df_test_valid_X, y_train, df_test_valid_y = skms.train_test_split(df_X, df_y, train_size=0.7, random_state=seed, stratify=df_y)\nX_dev, X_test, y_dev, y_test = skms.train_test_split(df_test_valid_X, df_test_valid_y, train_size=0.66, random_state=seed, stratify=df_test_valid_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Train set has {X_train.shape[0]} records out of {len(df_shuffle)} which is {round(X_train.shape[0]/len(df_shuffle)*100)}%\")\nprint(f\"Dev set has {X_dev.shape[0]} records out of {len(df_shuffle)} which is {round(X_dev.shape[0]/len(df_shuffle)*100)}%\")\nprint(f\"Test set has {X_test.shape[0]} records out of {len(df_shuffle)} which is {round(X_test.shape[0]/len(df_shuffle)*100)}%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_train.value_counts()[0]/y_train.shape[0]*100)\nprint(y_dev.value_counts()[0]/y_dev.shape[0]*100)\nprint(y_test.value_counts()[0]/y_test.shape[0]*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scale the Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = skp.StandardScaler()\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\nX_dev = pd.DataFrame(scaler.transform(X_dev), columns=X_train.columns)\nX_test = pd.DataFrame(scaler.transform(X_test), columns=X_train.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"## Sample Logistic Regression Model\n\nBuilding model with all the available features."},{"metadata":{"trusted":true},"cell_type":"code","source":"def predictProba(clf, X, dev=False):\n    y_true = y_train\n    if dev:\n        X = X_dev[X.columns]\n        y_true = y_dev\n    y_pred_proba_X = clf.predict_proba(X)\n    y_pred_X = clf.predict(X)\n    fig, ax = plt.subplots(figsize=(6, 6))\n    skm.plot_confusion_matrix(clf, X, y_true, display_labels=X.columns, cmap=plt.cm.Blues, xticks_rotation=90, ax=ax)\n    plt.show()\n    print(skm.classification_report(y_true, y_pred_X, digits=3))\n    print(\"=====================================================\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(random_state=seed)\nlr.fit(X_train,y_train)\npredictProba(lr, X_train)\npredictProba(lr, X_train, dev=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Permutation Importance Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"perm = PermutationImportance(lr, random_state=seed).fit(X_train, y_train, n_iter=10)\nprint(\"Feature Importances using Permutation Importance\")\neli5.show_weights(perm, feature_names = X_dev.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plot Permutation Importances"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the permutation importances\nperm_indices = np.argsort(perm.feature_importances_)[::-1]\nperm_features = [X_dev.columns.tolist()[xx] for xx in perm_indices]\nplt.figure(figsize=(14, 14))\nplt.title(\"Logistic Regression feature importance via permutation importance\")\nplt.barh(range(X_dev.shape[1]), perm.feature_importances_[perm_indices])\nplt.yticks(range(X_dev.shape[1]), perm_features)\nplt.ylim([X_dev.shape[1], -1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model Score using Permutation Importances"},{"metadata":{"trusted":true},"cell_type":"code","source":"# build model using perm selected top 30 features\nlr = LogisticRegression()\nX_train_perm = X_train[perm_features[:30]]\nX_train_rfe = X_train_perm\nlr.fit(X_train_perm,y_train)\npredictProba(lr, X_train_perm)\npredictProba(lr, X_train_perm, dev=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation Heatmap"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot correlation among top 30 selected featuers\nplt.figure(figsize = (18, 10))\nsns.heatmap(X_train_perm.corr(method='spearman'), annot = True, linewidths=.2, cmap=sns.diverging_palette(220, 10, as_cmap=True))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building\n\n## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train_rfe,y_train)\npredictProba(lr, X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictProba(lr, X_train_rfe, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = ske.RandomForestClassifier(random_state=seed, n_jobs=-1)\nrfc.fit(X_train_rfe, y_train)\npredictProba(rfc, X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictProba(rfc, X_train_rfe, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## AdaBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"abc = ske.AdaBoostClassifier(n_estimators=100, random_state=seed)\nabc.fit(X_train_rfe, y_train)\npredictProba(abc, X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictProba(abc, X_train_rfe, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"gbc = ske.GradientBoostingClassifier(n_estimators=100, random_state=seed)\ngbc.fit(X_train_rfe, y_train)\npredictProba(gbc, X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictProba(gbc, X_train_rfe, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nprint(xgb.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbc = xgb.XGBClassifier(n_estimators=100, random_state=seed)\nxgbc.fit(X_train_rfe, y_train)\npredictProba(xgbc, X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictProba(xgbc, X_train_rfe, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CatBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import catboost as cb\nprint(cb.__version__)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"cbc = cb.CatBoostClassifier(random_state=seed, verbose=0, eval_metric='Accuracy', loss_function='MultiClass')\ncbc.fit(X_train_rfe, y_train)\npredictProba(cbc, X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictProba(cbc, X_train_rfe, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Tuning\n\nTuning selected models which performed good as base models."},{"metadata":{},"cell_type":"markdown","source":"## RF Model HP Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 400, num = 4)]\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(1, 60, num = 5)]\n# Minimum number of samples required to split a node\nmin_samples_split = [5, 10, 15]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split}\n\npprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Use grid search to find best hyperparameters\nrfc_n = ske.RandomForestClassifier(random_state=seed, n_jobs=-1)\nrf_random = skms.GridSearchCV(estimator = rfc_n, param_grid=random_grid, cv = 5, verbose=2, n_jobs = -1, scoring='f1_weighted')\n# Fit the random search model\nrf_random.fit(X_train_rfe, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# best random model \nprint(rf_random.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Performance metrics\npredictProba(rf_random.best_estimator_, X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictProba(rf_random.best_estimator_, X_train_rfe, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGB Model HP Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=seed)\n\nxgb_params = {\n    \"colsample_bytree\": uniform(0.7, 0.3),\n    \"gamma\": uniform(0, 0.5),\n    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n    \"max_depth\": randint(2, 6), # default 3\n    \"n_estimators\": randint(100, 150), # default 100\n    \"subsample\": uniform(0.6, 0.4)\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\nxgb_random = skms.RandomizedSearchCV(estimator = xgb_model, param_distributions=xgb_params, n_iter=20, cv = 3, verbose=2, n_jobs = -1, random_state=seed, scoring='f1_weighted', return_train_score=True)\n# Fit the random search model\nxgb_random.fit(X_train_rfe, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# best xgb_random model \nprint(xgb_random.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Performance metrics\npredictProba(xgb_random.best_estimator_, X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictProba(xgb_random.best_estimator_, X_train_rfe, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_X = rf_random.best_estimator_.predict(X_test[X_train_rfe.columns])\nprint(skm.classification_report(y_test, y_pred_X, digits=3))\nprint(\"RF Test Accuracy -\",skm.accuracy_score(y_test, y_pred_X)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_X = xgb_random.best_estimator_.predict(X_test[X_train_rfe.columns])\nprint(skm.classification_report(y_test, y_pred_X, digits=3))\nprint(\"XGB Test Accuracy -\",skm.accuracy_score(y_test, y_pred_X)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_X = cbc.predict(X_test[X_train_rfe.columns])\nprint(skm.classification_report(y_test, y_pred_X, digits=3))\nprint(\"CatBoost Test Accuracy -\",skm.accuracy_score(y_test, y_pred_X)*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`Here, we had CatBoost as the best model among all ensemble techniques without any HP tuning on the same.`"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}