{"cells":[{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"# Customer Churning\n\nIn this notebook I go through the process of evaluating different Classification Models. I end up using `CatBoost`, as\nit yielded the highest `recall` of all.\n\n## Disclaimer\n\nThis notebook doesn't include an EDA nor any other type of analysis, given that I already submitted another\n[notebook](https://www.kaggle.com/augusto1982/credit-card-customers-analysis) for that.\n\n## Loading the data"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate, cross_val_score\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import confusion_matrix, recall_score, accuracy_score\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nimport seaborn as sns\n\nfrom sklearn.model_selection import GridSearchCV\n\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"df = pd.read_csv('../input/credit-card-customers/BankChurners.csv')\ndf = df.iloc[:, :-2]\n\n# Setting the index\ndf.set_index('CLIENTNUM', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"# Replacing 'Unknown' values.\ncategorical = ['Education_Level', 'Marital_Status', 'Income_Category']\n\nencoders = {}\n\nfor cat in categorical:\n    encoder = LabelEncoder()\n    encoders[cat] = encoder\n    values = df[cat]\n    known_values = values[values != 'Unknown']\n    df[cat] = pd.Series( encoder.fit_transform(known_values), index=known_values.index)\n\nimp_cat = IterativeImputer(estimator=RandomForestClassifier(),\n                           initial_strategy='most_frequent',\n                           max_iter=10, random_state=0)\n\n\ndf[categorical] = imp_cat.fit_transform(df[categorical])\n\nfor cat in categorical:\n    df[cat] = encoders[cat].inverse_transform(df[cat].astype(int))\n","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"def make_categorical(data: pd.DataFrame, column: str, categories: list, ordered: bool = False):\n    data[column] = pd.Categorical(df[column],\n                                       categories=categories,\n                                       ordered=ordered)\n\n\ndf['Attrition_Flag'] = df['Attrition_Flag'].map({'Attrited Customer':1, 'Existing Customer':0})\n\nmake_categorical(df, 'Gender', ['F', 'M'])\n\nmake_categorical(df, 'Education_Level', ['Uneducated', 'High School', 'Graduate', 'College', 'Post-Graduate', 'Doctorate'], True)\n\nmake_categorical(df, 'Marital_Status', ['Married', 'Single', 'Divorced'])\n\nmake_categorical(df, 'Income_Category', ['Less than $40K', '$40K - $60K', '$60K - $80K', '$80K - $120K', '$120K +'], True)\n\nmake_categorical(df, 'Card_Category', ['Blue', 'Silver', 'Gold', 'Platinum'], True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adding additional columns"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"# These columns I added while doing the EDA.\n\nage_bins = [20, 40, 60, 80]\nage_labels = ['20 - 40', '40 - 60', '60 - 80']\ndf['Age_Range'] = pd.cut(df['Customer_Age'], age_bins, labels=age_labels, ordered=True)\n\ndf['No_Revolving_Bal'] = df['Total_Revolving_Bal'] == 0\n\ndf['New_Customer'] = df['Months_on_book'] <= 24\n\ndf['Optimal_Utilization'] = df['Avg_Utilization_Ratio'] <= 0.3\n\n# The next two columns I added after doing some Feature Selection analysis (more on that below).\n\ndf['Avg_Transaction'] = df['Total_Trans_Amt'] / df['Total_Trans_Ct']\n\ndef get_avg_q4_q1(row):\n    if row['Total_Ct_Chng_Q4_Q1'] == 0:\n        return 0\n    return row['Total_Amt_Chng_Q4_Q1'] / row['Total_Ct_Chng_Q4_Q1']\n\n\ndf['Avg_Q4_Q1'] = df.apply(get_avg_q4_q1, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"## Encoding the categorical variables"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"label_encoding_columns = ['Education_Level', 'Marital_Status']\n\ndummy_encoding_columns = ['Gender', 'Income_Category', 'Card_Category', 'Age_Range']\n\ndf[label_encoding_columns]= df[label_encoding_columns].apply(LabelEncoder().fit_transform)\ndf = pd.get_dummies(df, columns=dummy_encoding_columns, prefix=dummy_encoding_columns, drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"## Splitting the target and independent variables"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"X = df.iloc[:, 1:]\ny = df.iloc[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"## Feature Selection\n\nHere I don't use Feature Selection for selecting a subset of relevant features, as that didn't improve the score of the model.\nInstead, I use it to determine which of the whole group turn out to be more relevant and see if there's any other column\nI create to reinforce the model.\n\nThe process determined these are the most relevant:\n```\n[\n    'Total_Relationship_Count', 'Months_Inactive_12_mon', 'Contacts_Count_12_mon',\n    'Total_Revolving_Bal', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt', 'Total_Trans_Ct',\n    'Total_Ct_Chng_Q4_Q1', 'No_Revolving_Bal'\n]\n```\n\nAs we can see we have the columns regarding Q4/Q1, and the two for the total of transactions. Therefore, I decided\nto create two additional columns, as I previously mentioned (`Avg_Transaction` and `Avg_Q4_Q1`).\n"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"# forest = ExtraTreesClassifier(n_estimators=250)\n# forest.fit(X, y)\n#\n# feat_importances = pd.Series(forest.feature_importances_, index=X.columns).sort_values(ascending=False)\n#\n# sel = SelectFromModel(forest)\n# sel.fit(X, y)\n# selected_feat= X.columns[sel.get_support()]\n#\n# df_sel = df[selected_feat]","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"## Scaling the data"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"X = RobustScaler().fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"## Split into train and test sets"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"## Evaluate different models with K-Fold"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"base_models = [\n    (\"LR_model\", LogisticRegression(random_state=42,n_jobs=-1)),\n    (\"KNN_model\", KNeighborsClassifier(n_jobs=-1)),\n    (\"SVM_model\", SVC(random_state=42, kernel = 'rbf')),\n    (\"DT_model\", DecisionTreeClassifier(random_state=42)),\n    (\"RF_model\", RandomForestClassifier(random_state=42,n_jobs=-1)),\n    (\"XGB_model\", XGBClassifier(random_state=42, n_jobs=-1, scale_pos_weight=5)),\n    (\"CXGB_model\", CatBoostClassifier(random_state=42, auto_class_weights='Balanced'))\n]\n\n\nsplit = KFold(n_splits=4, shuffle=True, random_state=42)\n\n# Preprocessing, fitting, making predictions and scoring for every model:\nfor name, model in base_models:\n\n    # get cross validation score for each model:\n    cv_results = cross_val_score(model,\n                                 X, y,\n                                 cv=split,\n                                 scoring=\"recall\",\n                                 n_jobs=-1)\n    # output:\n    min_score = round(min(cv_results), 4)\n    max_score = round(max(cv_results), 4)\n    mean_score = round(np.mean(cv_results), 4)\n    std_dev = round(np.std(cv_results), 4)\n    print(f\"{name} cross validation recall score: {mean_score} +/- {std_dev} (std) min: {min_score}, max: {max_score}\")","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"As we can see, `CatBoost` seems to be the best option.\n\n## Search for optimal hyperparameters\n\nI commented the code below, given that it takes hours to run. Its execution produced the following combination of parameters:\n\n```\n{\n    'border_count': 100,\n    'depth': 6,\n    'iterations': 250,\n    'l2_leaf_reg': 100,\n    'learning_rate': 0.1\n}\n```\n\nHowever, I ran this before adding the last two columns, so I tweak them manually some more afterwards."},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"# grid_params = {\n#     'depth':[4, 5, 6, 7, 8 ,9, 10],\n#     'iterations':[250, 500, 1000],\n#     'learning_rate':[0.001, 0.1, 0.2, 0.3],\n#     'l2_leaf_reg':[3, 5, 10, 100],\n#     'border_count':[10, 20, 50, 100],\n# }\n#\n# gd_sr = GridSearchCV(estimator=CatBoostClassifier(random_state=42, auto_class_weights='Balanced'),\n#                      param_grid=grid_params,\n#                      scoring='recall',\n#                      cv=5,\n#                      n_jobs=-1)\n#\n# gd_sr.fit(X_train, y_train)\n#\n# best_parameters = gd_sr.best_params_\n# print(best_parameters)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"## Construction and execution of the optimal? model"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"best_classifier = CatBoostClassifier(\n    random_state=42,\n    border_count=100,\n    depth=6,\n    iterations=140,\n    l2_leaf_reg=100,\n    learning_rate=0.1,\n    auto_class_weights='Balanced',\n    verbose=False\n)\n\nbest_classifier.fit(X_train, y_train)\ny_pred = best_classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"## Confusion Matrix"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\n# labels = ['Survived', 'No Survived']\nax = sns.heatmap(cm, annot=True)\nprint(\"recall: {}\".format(recall))","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"## K-Fold and CatBoost"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"\nnp.mean(\n    cross_val_score(\n        best_classifier,\n        X,\n        y,\n        cv=split,\n        scoring=\"recall\",\n        n_jobs=-1)\n)\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}