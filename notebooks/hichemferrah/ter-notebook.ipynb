{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"BENGOUNIA Mahmoud\n\nFERRAH Hichem","metadata":{}},{"cell_type":"code","source":"!pwd\n!ls ../\n!pwd ../","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n'''\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root_path = '/kaggle/input/CORD-19-research-challenge/'\nmetadata_path = f'{root_path}/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head(2)\nmeta_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimport json\npdf_json = glob.glob(f'{root_path}document_parses/pdf_json/*.json', recursive=True)\npmc_json = glob.glob(f'{root_path}document_parses/pmc_json/*.json', recursive=True)\nall_json = pdf_json\nall_json.append(pmc_json)\nlen(all_json)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we transform each file into a line in our dataset\ndef FileToLine(path):\n    line=[]\n    json_content= None\n    with open(path) as json_file:\n        json_content=json.load(json_file)\n        \n    paper_id=json_content['paper_id'] #... ID\n    \n    meta_data = meta_df.loc[meta_df['sha'] == paper_id]\n    if len(meta_data)==0: return None #... Return Nothing / skip paper\n    \n    #Added the index of each column in a comment so it can be easly seen\n    line.append(paper_id) #...0        \n    abstract=[]\n        \n    for paragraph in json_content['abstract']: #... Abstract\n        abstract.append(paragraph['text'])\n    abstract=';'.join(abstract)\n    line.append(abstract) #...1\n    body=[]\n        \n    for paragraph in json_content['body_text']:#... Body Content by Paragraph\n        body.append(paragraph['text'])\n    body=';'.join(body) # Hopefully ';' doesn't mess up our file like '\\n' did\n    line.append(body) #...2\n        \n    line.append(meta_data['authors'].values[0])  #...3\n    line.append(meta_data['title'].values[0]) #...4\n    line.append(meta_data['journal'].values[0]) #..5\n    line.append(meta_data['doi'].values[0]) #..6\n    \n    return line\n#print(FileToLine(all_json[0]))\na=FileToLine(all_json[0])\nfor i in a:\n    print(type(i))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ndata= {'paper_id': [], 'doi':[], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': []} #data_set format\n#some values to track our progression\ntotal_entries=len(all_json)\nnumber_entries=total_entries//10\n#number_entries=20 #test-functionaliy\npart=1\nstart_time=time.time()\nlist=['paper_id','abstract','body_text','authors','title','journal','doi']\n#we could have done everything in one bloc, but we might need every part indepandently later for improvements\nfor idx, entry in enumerate(all_json):\n    elapsed_time=time.time()-start_time\n    minutes=round(elapsed_time // 60)\n    seconds=round(elapsed_time % 60)\n    print(f'......Processing index: {idx} of {total_entries} -°o0o°- {number_entries} left for part N°{part} ... {minutes} mins {seconds} secs passed......',end='\\r')\n    try:\n        line =FileToLine(entry)\n    except: # Unknown File\n        continue\n    if line == None: continue\n    \n    #line[i]-->0.paper_id 1.abstract 2.body 3.authors 4.title 5.journal 6.doi\n    for i in range(len(line)):\n        if type(line[i]) == type(\"this is a string\"):\n            data[list[i]].append(line[i].replace(\",\",\" \")) # we delete the commas so they can be our seperator in the data frame we will save\n        else:\n            data[list[i]].append(line[i])\n    number_entries-=1\n    if number_entries <= 0: #saving our data set while making sure that it's divided into parts so we can \n        print(f'\\n saving part{part}...')\n        df_covid = pd.DataFrame(data, columns=['paper_id', 'doi', 'abstract', 'body_text', 'authors', 'title', 'journal'])\n        df_covid.to_csv (f'all_CORD_data_part{part}.csv', index = False, header=True)\n        print(f\"part{part} saved!\")\n        data= {'paper_id': [], 'doi':[], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': []} #data_set format\n        number_entries=total_entries//10\n        #number_entries=20 #test\n        part+=1\n    ###SINCE IT TAKES A LOT OF TIME TO PROCESS ALL ENTRIES WE WILL JUST GONNA PICK THE FIRST PART TO SPEED WORK, DELETE/COMMENT THE NEXT PART FOR THE REAL EXECUTION:\n    #if part >1: break # <- Delete this and \"part > 2\" below\n        \nif number_entries >0: #remaining entries in the last part\n    print(f'saving part{part}...')\n    df_covid = pd.DataFrame(data, columns=['paper_id', 'doi', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\n    df_covid.to_csv (f'all_CORD_data_part{part}.csv', index = False, header=True)\n    print(f\"part{part} saved!\")\nprint(f\"Success! all data saved into {part} parts\")\n        \n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data= {'paper_id': [], 'doi':[], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': []} #data_set format\n#some values to track our progression\ntotal_entries=len(all_json)\n#number_entries=total_entries//10\n#part=1\nstart_time=time.time()\nlist=['paper_id','abstract','body_text','authors','title','journal','doi']\n#we could have done everything in one bloc, but we might need every part indepandently later for improvements\nfor idx, entry in enumerate(all_json):\n    elapsed_time=time.time()-start_time\n    minutes=round(elapsed_time // 60)\n    seconds=round(elapsed_time % 60)\n    print(f'......Processing index: {idx} of {total_entries} -°o0o°-... {minutes} mins {seconds} secs passed......',end='\\r')\n    try:\n        line =FileToLine(entry)\n    except: # Unknown File\n        continue\n    if line == None: continue\n    \n    #line[i]-->0.paper_id 1.abstract 2.body 3.authors 4.title 5.journal 6.doi\n    for i in range(len(line)):\n        if type(line[i]) == type(\"this is a string\"):\n            data[list[i]].append(line[i].replace(\",\",\" \")) # we delete the commas so they can be our seperator in the data frame we will save\n        else:\n            data[list[i]].append(line[i])\n    #number_entries-=1\nprint(f'\\n saving File...')\ndf_covid = pd.DataFrame(data, columns=['paper_id', 'doi', 'abstract', 'body_text', 'authors', 'title', 'journal'])\ndf_covid.to_csv (f'all_CORD_data_part.csv', index = False, header=True)\nprint(f\"Saved\")\n#data= {'paper_id': [], 'doi':[], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': []} #data_set format\n#number_entries=total_entries//10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_covid.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}