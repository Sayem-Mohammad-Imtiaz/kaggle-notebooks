{"cells":[{"metadata":{"_uuid":"6fdae6829ac931eb6302023de80ab39d87f42710","_cell_guid":"dc9e8033-7f68-4dde-bd5d-709e03c6a4b8"},"cell_type":"markdown","source":"# Import Dataset, drop NaN's, select Proteins\nThis notebooks shows how to recogniuze protein families soley based on the sequence of aminoacids. Please note, that there are notable search engines such as BLAST for this task.\n\n## Preprocessing and visualization of dataset\npreprocessing of the data:\n1. merge on *structureId*\n2. drop rows without labels\n3. drop rows without sequence\n4. select proteins\n\n**Ideally: **For comparison I also decided to focus only on those classes where the number of instances is greater than 1000 (as in [this kernel of Akil](https://www.kaggle.com/abharg16/predicting-protein-classification/code)) which corresponds to the 43 most common classes. \n\n**But:** one hour on 4 CPU is not sufficient for such big datasets, instead only 10 most common classes are considered.\n\n## Important disclaimer:\nAs there are "},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nos.listdir()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Merge the two Data set together\ndf1 = pd.read_csv('../input/protein-data-set/pdb_data_no_dups.csv')\ndf2 = pd.read_csv('../input/protein-data-set/pdb_data_seq.csv')\ndf = df1.merge(df2, how='inner', on='structureId')\n# Drop rows with missing labels\ndf = df[[type(c) == type('') for c in df.classification.values]]\ndf = df[[type(c) == type('') for c in df.sequence.values]]\n# select proteins\ndf = df[df.macromoleculeType_x == 'Protein']\ndf.reset_index()\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import innvestigate","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Disclaimer\n### As there are multiple chain_ids per structure_id, train test split has to be done on unique structure_ids in order to avoid redunancy bias!"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.structureId.value_counts()[:30].plot(kind='bar')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbcecef0b6257c846b81465881ab253a09722958","_cell_guid":"0611c0ab-8993-4a55-8299-990a4bde485c","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom collections import Counter\n\ncnt = Counter(df.classification)\ntop_classes = 42\n# sort classes\nsorted_classes = cnt.most_common()[:top_classes]\nclasses = [c[0] for c in sorted_classes]\ncounts = [c[1] for c in sorted_classes]\nprint(\"at least \" + str(counts[-1]) + \" instances per class\")\n\n# apply to dataframe\nprint(str(df.shape[0]) + \" instances before\")\ndf = df[[c in classes for c in df.classification]]\nprint(str(df.shape[0]) + \" instances after\")\n\nseqs = df.sequence.values\nlengths = [len(s) for s in seqs]\n\n# visualize\nfig, axarr = plt.subplots(1,2, figsize=(20,5))\naxarr[0].bar(range(len(classes)), counts)\nplt.sca(axarr[0])\nplt.xticks(range(len(classes)), classes, rotation='vertical')\naxarr[0].set_ylabel('frequency')\n\naxarr[1].hist(lengths, bins=100, normed=False)\naxarr[1].set_xlabel('sequence length')\naxarr[1].set_ylabel('# sequences')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\n\n# Transform labels to one-hot\nlb = LabelBinarizer()\nY = lb.fit_transform(df.classification)\ninv_dic = {i:c for i,c in enumerate(lb.classes_)}\n\nseqs = df.sequence.values\nlengths = np.array([len(s) for s in seqs])\n\nplt.figure(figsize=(20,10))\nx = []\nys = []\nfor y in np.unique(Y.argmax(axis=1)):\n    sel = lengths[Y.argmax(axis=1) == y]\n    x.append(sel)\n    ys.append(str(inv_dic[y]))\nplt.hist(x, bins=100, label=ys, histtype='bar', stacked=True)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72ef0fa23c88562a3261bc4ac784e2b4dd3576ae","_cell_guid":"6bd20d9f-6b43-46bd-821b-4ceb27882a50","trusted":true,"scrolled":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\n#create and fit tokenizer\ntokenizer = Tokenizer(char_level=True)\ntokenizer.fit_on_texts(seqs)\ntokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c31e3f33f189cc7bd1a63556b159229d69aecde1"},"cell_type":"code","source":"from keras.utils import to_categorical\nfrom keras.preprocessing import text, sequence\nfrom sklearn.model_selection import train_test_split\n\n# maximum length of sequence, everything afterwards is discarded!\nmax_length = 1024\n\ndef unsupervised_generator(dataframe, seq_tokenizer, batch_size):\n    while True:\n        ridxs = np.random.choice(len(dataframe), batch_size)\n        X = seq_tokenizer.texts_to_sequences(dataframe.iloc[ridxs].sequence.values)\n        X = sequence.pad_sequences(X, maxlen=max_length)\n        X = to_categorical(X)\n        yield X, X\n        \ndef supervised_generator(dataframe, seq_tokenizer, label_tokenizer, batch_size):\n    while True:\n        ridxs = np.random.choice(len(dataframe), batch_size)\n        X = seq_tokenizer.texts_to_sequences(dataframe.iloc[ridxs].sequence.values)\n        X = sequence.pad_sequences(X, maxlen=max_length)\n        X = to_categorical(X, len(seq_tokenizer.word_index)+1)\n        Y = label_tokenizer.transform(dataframe.iloc[ridxs].classification.values)\n        yield X, Y\n\ntrain_df, test_df = train_test_split(df, test_size=.1)\ntrain_df.shape, test_df.shape, df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# THIS IS WRONG! INSTEAD DO FOLLOWING:"},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = np.array(list(set(df.structureId)))\ntrain_ids, test_ids = train_test_split(ids, test_size=.1)\ntrain_df = df[df.structureId.isin(train_ids)]\ntest_df = df[df.structureId.isin(test_ids)]\ntrain_ids.shape, test_ids.shape, train_df.shape, test_df.shape, df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1a50e14370f7431ad5f19b4aea830f16e065129","_cell_guid":"85a605d3-3c06-462e-8374-7a626c50f511"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"0c741a77feafefa6bf17380a9c36da8ab3414cc9","_cell_guid":"e284ceed-8c91-4a1c-9495-dddda0c05d63","trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Convolution1D, Flatten, Dropout\n\nmodel = Sequential()\nmodel.add(Convolution1D(1024, 3, strides=2, activation='relu', padding='same', input_shape=(max_length, len(tokenizer.word_index) + 1)))\n#model.add(Convolution1D(512, 3, strides=2, activation='relu', padding='same'))\n#model.add(Convolution1D(512, 3, activation='relu', padding='same'))\nmodel.add(Convolution1D(512, 3, strides=2, activation='relu', padding='same'))\n#model.add(Convolution1D(128, 3, activation='relu', padding='same'))\nmodel.add(Convolution1D(256, 3, strides=2, activation='relu', padding='same'))\n#model.add(Convolution1D(64, 3, activation='relu', padding='same'))\nmodel.add(Convolution1D(128, 3, strides=2, activation='relu', padding='same'))\nmodel.add(Convolution1D(64, 3, strides=2, activation='relu', padding='same'))\n#model.add(Convolution1D(32, 3, activation='relu', padding='same'))\nmodel.add(Flatten())\nmodel.add(Dropout(.25))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(top_classes, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * "},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size=32\ntr_gen = supervised_generator(train_df, tokenizer, lb, batch_size=batch_size)\nte_gen = supervised_generator(test_df, tokenizer, lb, batch_size=batch_size)\n\nmodel.fit_generator(\n    tr_gen,\n    validation_data=te_gen,\n    steps_per_epoch=len(train_df)//batch_size,\n    validation_steps =len(test_df)//batch_size,\n    epochs=10\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('fucking_cnn.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nos.listdir()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import innvestigate","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5617004d5cca721264ae2de1927a8249c17d326","_cell_guid":"4195d3fc-1c0e-4c15-8532-e2531c04c763"},"cell_type":"markdown","source":"# Evaluate model\nEvaluation is only done based on accuracy and a confusion matrix which is already implemented in sklearn.\n"},{"metadata":{"_uuid":"7d52f860e1bd49795b5b8b17afc566934758f223","_cell_guid":"5da1002b-21b6-4a77-aa43-cb8131d99b3c","trusted":true},"cell_type":"code","source":"#X_train = tokenizer.texts_to_sequences(train_df.sequence.values)\n#X_train = sequence.pad_sequences(X_train, maxlen=max_length)\n#X_train = to_categorical(X_train, len(tokenizer.word_index)+1)\n#y_train = lb.transform(train_df.classification.values)\n\nX_test = tokenizer.texts_to_sequences(test_df.sequence.values)\nX_test = sequence.pad_sequences(X_test, maxlen=max_length)\nX_test = to_categorical(X_test, len(tokenizer.word_index)+1)\ny_test = lb.transform(test_df.classification.values)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_uuid":"f29dc7ca14851abaadbb41220e0e9339e5b9fafe","_cell_guid":"fc019cd0-6f99-475c-acde-d1710695b1b2","trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nimport itertools\n\n#train_scores = model.predict(X_train)\ntest_scores = model.predict(X_test)\n#train_pred = np.argmax(train_scores, axis=1)\ntest_pred = np.argmax(test_scores, axis=1)\n#Y_train = np.argmax(y_train, axis=1)\nY_test = np.argmax(y_test, axis=1)\n\n#print(\"train-acc = \" + str(accuracy_score(Y_train, train_pred)))\nprint(\"test-acc = \" + str(accuracy_score(Y_test, test_pred)))\n\n# Compute confusion matrix\ncm = confusion_matrix(Y_test, test_pred)\n\n# Plot normalized confusion matrix\ncm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nnp.set_printoptions(precision=2)\nplt.figure(figsize=(10,10))\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion matrix')\nplt.colorbar()\ntick_marks = np.arange(len(lb.classes_))\nplt.xticks(tick_marks, lb.classes_, rotation=90)\nplt.yticks(tick_marks, lb.classes_)\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n    plt.text(j, i, format(cm[i, j], '.2f'), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > cm.max() / 2. else \"black\")\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(classification_report(Y_test, test_pred, target_names=lb.classes_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}