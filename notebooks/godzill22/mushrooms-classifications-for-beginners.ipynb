{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](http://www.woodlandtrust.org.uk/media/1997/fly-agaric-mushroom-close-up-alamy-dhfcm9-ivan-kmit.jpg?center=0.49618320610687022,0.63948497854077258&mode=crop&width=1110&height=624&rnd=132078488660000000)"},{"metadata":{},"cell_type":"markdown","source":"In this notebook I would like to introduce 3 methods we can use to transform categorical columns into numerical form. This notebook is for people who have just started their journey. For those who code for some time it would be propably to simple."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mushroom dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/mushroom-classification/mushrooms.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All columns are categorical and there are not ordered, so we treat them as nominal. Therefore, we can use one hot encoder, we can create dummy variables or use Label encoder. I will try all techniques and find out which gives us better performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df.columns:\n    print(f\"Column {col} unique values: {df[col].unique()}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=df['class'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our prediction label is well balanced so we don't have to worry about it. For the classification problem we could leave our label as string type as some algorithms can cope with categorical label, but for binary clasification it is better to use boolean values(0, 1). I wiil map edible as 0, and poisonous as 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"class\"] = df[\"class\"].apply(lambda x: 1 if x == \"e\" else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(\"class\", axis=1).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16, 30))\nfor i, col in enumerate(df.columns):\n    plt.subplot(12,2,i+1)\n    sns.countplot(x=df[col])\n    plt.tight_layout()\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 4))\nsns.countplot(x=df[\"odor\"], hue=df['class']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like most of poisnonous mushrooms have no odor. Fresh mushrooms should smell slightly sweet and earthy, but not foul. If they smell fishy or pungent, it's time to toss them."},{"metadata":{},"cell_type":"markdown","source":"### Seperate our label from features"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(\"class\", axis=1)\ny = df[\"class\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nXtrain, Xvalid, ytrain, yvalid = train_test_split(X, y, test_size=0.2, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### One hot encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot = OneHotEncoder()\nXtrain_onehot = one_hot.fit_transform(Xtrain)\nXvalid_onehot = one_hot.transform(Xvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, plot_confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_base = RandomForestClassifier()\nrfc_base.fit(Xtrain_onehot, ytrain)\nbase_preds = rfc_base.predict(Xvalid_onehot)\nacc = accuracy_score(yvalid, base_preds)\nprint(f\"Random Forest accuracy: {acc}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({\"predictions\":base_preds,\n              \"ytrue\": yvalid})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(yvalid, base_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(rfc_base, Xvalid_onehot, yvalid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see our base model achieved 100%, but in this case it is hard to find out which features we most important as onehot produce sparse matrix."},{"metadata":{},"cell_type":"markdown","source":"### Using pipeline to do the same task"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([\n    (\"onehot\", OneHotEncoder()),\n    (\"rfc_base\", RandomForestClassifier())\n])\n\npipe.fit(Xtrain, ytrain)\npipe_preds = pipe.predict(Xvalid)\nacc = accuracy_score(yvalid, pipe_preds)\nprint(f\"Random Forest acc={acc}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Label Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in X.columns:\n    le = LabelEncoder()\n    Xtrain.loc[:, col] = le.fit_transform(Xtrain[col].values)\n    Xvalid.loc[:, col] = le.transform(Xvalid[col].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_le = RandomForestClassifier()\nrfc_le.fit(Xtrain, ytrain)\nle_preds = rfc_le.predict(Xvalid)\nacc = accuracy_score(yvalid, le_preds)\nprint(f\"Random Forest accuracy: {acc}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(yvalid, le_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(rfc_le, Xvalid, yvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_imp = pd.DataFrame(rfc_le.feature_importances_, index=Xtrain.columns, columns=[\"feat_imp\"])\nfeat_imp = feat_imp.sort_values(\"feat_imp\", ascending=False)\nfeat_imp.style.background_gradient(\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dummy variables"},{"metadata":{},"cell_type":"markdown","source":"Similar method to OneHotEncoding is creating dummy variables, but we don't lose informtion about which features importance in our model. Couple things we need to remember, first dummy trap(i.e multicollinearity) and second course of dimentionality( in huge datasets it is not that easy to use this method in my opinion)."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(\"class\", axis=1)\ny = df[\"class\"].values\n\nXtrain, Xvalid, ytrain, yvalid = train_test_split(X, y, test_size=0.2, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_Xtrain = pd.get_dummies(Xtrain, drop_first=True)\ndummy_Xvalid = pd.get_dummies(Xvalid, drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_d = RandomForestClassifier()\nrfc_d.fit(dummy_Xtrain, ytrain)\nd_preds = rfc_d.predict(dummy_Xvalid)\nacc = accuracy_score(yvalid, d_preds)\nprint(f\"Random Forest accuracy: {acc}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_imp = pd.DataFrame(rfc_d.feature_importances_, index=dummy_Xtrain.columns, columns=[\"feat_imp\"])\nfeat_imp = feat_imp.sort_values(\"feat_imp\", ascending=False)[:20]\nfeat_imp.style.background_gradient(\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}