{"cells":[{"metadata":{"_cell_guid":"87c7b371-53b1-4d4d-bfbd-373d2b84b33a","_uuid":"da5343fb3f6b3942909c94bf8e2add04fd3ff1a3","trusted":true},"cell_type":"code","source":"#pandas and plot tool\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"800f9ff4-79bf-4785-9569-23e1d9b9b03b","_uuid":"07c8409e4eccd80507d9846e8cc70ea42e58cbe6","trusted":true},"cell_type":"code","source":"#Loading dataset\nwine = pd.read_csv('../input/winequality-red.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"019cbcee-6cb2-478c-922b-ccebe4962769","_uuid":"45ff42953e9082cd55612a4774408cc97a05fb11","trusted":true},"cell_type":"code","source":"#Let's check how the data is distributed\nwine.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"704f4830-5804-436d-9b78-6ca00f5ae510","_uuid":"af141503385967f92d409c5e111e2724b4c9636f","trusted":true},"cell_type":"code","source":"#Information about the data columns\nwine.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"12b9e3c1-006d-4b1d-b01d-02be5a594bbb","_uuid":"e9ad3ce0e67ea7a5178222164d784a974846bc54"},"cell_type":"markdown","source":"## **Let's do some plotting to know how the data columns are distributed in the dataset**"},{"metadata":{"trusted":true,"_uuid":"f7209b80a364cfaeea12f6f4f5abf305031fd7b5"},"cell_type":"code","source":"#Check the data quality\nprint('wine:Columns with null value:\\n', wine.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8558c2a3efad830411a7d8b40fb9344d24685796"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aca5f44701d9d065cae58b51f82a9d07bf308c1b"},"cell_type":"code","source":"#Review correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    \n    _ = sns.heatmap(\n        df.corr(), \n        center=0,\n        cmap=\"Blues\",\n        ax=ax,\n        linewidths=0.1,\n        annot=True, \n        annot_kws={'fontsize':14 }\n    )\n    plt.title('Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(wine)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c421f3ebf4039ebe47784e940ab1af9c552a98e"},"cell_type":"code","source":"g = sns.FacetGrid(wine, col='quality', hue='quality', col_wrap=3, height=4)\ng.map(plt.scatter, 'alcohol','volatile acidity',s=150,alpha=0.8, edgecolors='w')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7dec7b6efcfa5b92eab7e3053b1a2f10e6e988b2"},"cell_type":"code","source":"#Making binary classificaion for the response variable.\n#Dividing wine as good and bad by giving the limit for the quality\nbins = (0, 6.5, 10)\ngroup_names = ['bad', 'good']\nwine['quality_bin'] = pd.cut(wine['quality'], bins = bins, labels = group_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d813625bb0e1dece5166b618313bed926981084"},"cell_type":"code","source":"#Now lets assign a labels to our quality variable\nlabel_quality = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc9b98e149abcc20d84e2891065f9c21e4e1c220"},"cell_type":"code","source":"#Bad becomes 0 and good becomes 1 \nwine['quality_bin'] = label_quality.fit_transform(wine['quality_bin'])\nwine['quality_bin'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29362d1111da33719365da0484c16d175753566a"},"cell_type":"code","source":"#Define X and Y\n#Define y veriable as target\nTarget=['quality_bin']\n\n#Define x variables##remove SibSp based on the correlation\nx=['fixed acidity','volatile acidity','citric acid','residual sugar','chlorides','free sulfur dioxide','total sulfur dioxide','density','pH','sulphates','alcohol']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0751a82e5847afa62ba8501dbf7b8c21880ca898"},"cell_type":"code","source":"#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    #xgboost: \n    XGBClassifier()    \n    ]\n\n#split dataset in cross-validation with this splitter class\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = wine[Target]\n\n#index through MLA and save performance to table\nrow_index = 1\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    \n    #score model with cross validation\n    cv_results = model_selection.cross_validate(alg, wine[x], wine[Target], cv  = cv_split)\n\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n    \n\n    #save MLA predictions - see section 6 for usage\n    alg.fit(wine[x], wine[Target])\n    MLA_predict[MLA_name] = alg.predict(wine[x])\n    \n    row_index+=1\n\n    \n#print and sort table\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}