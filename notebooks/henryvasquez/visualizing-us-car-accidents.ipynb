{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#start by loading data set and looking at first few rows\ndata = pd.read_csv('/kaggle/input/us-accidents/US_Accidents_Dec19.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#See how many values are missing from each column - doesn't matter for some variables since they're not that important\nfor col in range(0, len(list(data))):\n    print('{column} has {nas} missing values'.format(column = list(data)[col], nas = data[[list(data)[col]]].isnull().sum().sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We do see that some of the variables have quiet a few numbers of missing values. To get a better idea of this we can use a barplot to show the percentage of NAs by variable. Since we have 49 possible variables, we should ignore the variables that have 0% missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"#first create list of NA percentages\nna_percentages = []\nlam = lambda x: np.round(100*data[[x]].isnull().sum().sum()/len(data),2)\n\nfor col in list(data):\n    na_percentages.append(lam(col))\n\n#create data frame\nplotdata = pd.DataFrame({'Variable': list(data),\n                        'NA Percentage': na_percentages})\n\n#let's go ahead and factor out the variables that have 0% - sort values- reset index\nplotdata = plotdata[plotdata['NA Percentage'] > 0.00].sort_values('NA Percentage', ascending = True).reset_index(drop = True)\n\n#now plot\nplt.barh(plotdata['Variable'], plotdata['NA Percentage'])\nplt.title('NA Percentages by Column') #title\nplt.xlabel('Percentage of NAs') #xaxis label\nplt.ylabel('Variable') #yaxis label\nplt.xticks(rotation = 90) #rotate ticks so they're legible\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the barchart, End_Lat, End_Lng, Precipitation(in), Number, Wind_Chill has a majority of values as NA. TMC and Wind_Speed as have a large portion of missing values. The remaining variables have very little to no issue with NA. Given the task is to visualize accidents across the 49 states, it might not make a huge deal that we have many NAs in some of these variables. If we plan on building a model for to predict severity of the accidents then some of these variables will need to be excluded or handled in another way. We are given the coordinates of the accidents, which have no missing values and that is what's going to help us visualize this by state."},{"metadata":{"trusted":true},"cell_type":"code","source":"#get unique states-49\nstates = list(set(data['State']))\ncounts = []\n\n#get counts for each state\nfor state in states:\n    counts.append(len(data[data['State'] == state]))\n\n# create df - sort by \nstate_plotdata = pd.DataFrame({'State': states,\n                        'Total': counts}).sort_values('Total', ascending = False).reset_index(drop = True)\n\n#now plot\nplt.bar(state_plotdata['State'], state_plotdata['Total'])\nplt.title('Accidents by State') #title\nplt.ylabel('State') #yaxis label\nplt.xlabel('Total Accidents') #xaxis label\nplt.xticks(rotation = 90)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that California, TX, FL, SC and NC are the states with the most accidents. When graphing these we should also notice that this is the case. Since we're working with a big dataset that has roughly 3 million rows, we can use datashader to help us visualize this. Datashader is meant for visualizng large datasets of at least 100,000 rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"#start by importing necessary packages\nimport datashader as ds\nimport datashader.transfer_functions as tf\nfrom datashader.utils import lnglat_to_meters\nfrom datashader.colors import colormap_select, inferno\nfrom datashader.utils import export_image\nfrom functools import partial","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we need to tranform lat and lng to meters from origin using lnglat_to_meters, we'll set this to x and y\ndata.loc[:, 'x'], data.loc[:, 'y'] = lnglat_to_meters(data.Start_Lng, data.Start_Lat)\n\n#now set the image\nbackground = 'black'\nplot_width = int(2500)\nplot_height = int(plot_width*7.00/12)\nexport = partial(export_image, background = background, export_path = 'export')\ncm = partial(colormap_select, reverse=(background!='black'))\ncvs = ds.Canvas(plot_width, plot_height)\nagg = cvs.points(data, 'x', 'y')\nexport(tf.shade(agg, cmap = cm(inferno), how = 'log'), 'US_car_crash_by_state')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My recent understanding of datashader is that it's not so good at informing the user what the colors on the graph actually mean. In order to understand the graph you need to be able to interpret the color scheme: darker colors mean less accidents, brighter colors mean more accidents. Now if we compare our new understanding with what our bar chart from above told us we see that:\n* California does have a lot of accidents, especially the LA and SF areas\n* Florida has a lot of accidents in Miami and Tampa\n* Texas, although hard to see the outline, does have major cities with lots of accidents\n* South Carolina has an entire shade of purple, meaning more accidents\n* North Carolina appears to have a heavily dense portion of accidents\nWe can recreate the graph, and this time specify colors based on the severity of the accident. This will enable us view which parts of the states have the most severe accidents and which don't."},{"metadata":{"trusted":true},"cell_type":"code","source":"#first create a column that states color based on Severity\ndata['Color'] = np.where(data['Severity'] == 1, '#ffa600',\n                         np.where(data['Severity'] == 2, '#ff6e54',\n                                  np.where(data['Severity'] == 3, '#955196', \n                                           '#003f5c')\n                                 )\n                          )\n#if color isn't a categorical var, you run into an issue later with tf.shade()\ndata['Color'] = pd.Categorical(data['Color'])\n\n#begin plot\nbackground = 'white'\nexport = partial(export_image, background = background, export_path = 'export')\ncvs = ds.Canvas(plot_width*2, plot_height*2)\nagg = cvs.points(data, 'x', 'y', ds.count_cat('Color'))\nview = tf.shade(agg, color_key = data['Color'])\nexport(tf.spread(view, px = 2), 'US_car_crash_severity')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this graph, the background is set to white to view the severity of the crashes easier. The darker the point the more severe. The yellow points are severity = 1, orange is severity = 2, purple is severity = 3, and the dark bluegreen. is severity = 4. In general, most crashes are severity = 2. There are some visible 3 and 4 spottings if we zoom into the graph. We can check this by aggregating the dataset. It's also notable that the Michigan area has a lot of severity 3 crashes as does most of the east coast whereas other areas of the graph this is not so visible."},{"metadata":{"trusted":true},"cell_type":"code","source":"#check severity totals\ndata.groupby(['Severity']).size()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}