{"metadata":{"language_info":{"name":"python","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.0","nbconvert_exporter":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1,"cells":[{"source":"# LDA and Document Similarity\n\nWe are again working with the same fake news articles supplied by Kaggle.\n\n**I do not endorse and am not expressing any political affiliation or intent expressed in the articles in this dataset.**\n\nWe will explain LDA and train an LDA model on this corpus of fake news to see what topics emerge.\n\nWe will hold out some documents for testing to infer their topic distributions and compare them to the rest of the corpus to find the most similar documents.\n\nWe use the [gensim](https://radimrehurek.com/gensim/models/ldamodel.html) package to do this, as it is highly optimised in C and has many features that make the implementation easy to use and very flexible.","metadata":{},"cell_type":"markdown"},{"source":"# import dependencies\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nimport gensim\nfrom gensim.models import LdaModel\nfrom gensim import models, corpora, similarities\nimport re\nfrom nltk.stem.porter import PorterStemmer\nimport time\nfrom nltk import FreqDist\nfrom scipy.stats import entropy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"cell_type":"code"},{"source":"Read in data; only keep essential columns and English language articles","metadata":{},"cell_type":"markdown"},{"source":"df = pd.read_csv('fake.csv', usecols = ['uuid','author','title','text','language','site_url','country'])\ndf = df[df.language == 'english']\ndf = df[df['text'].map(type) == str]\ndf['title'].fillna(value=\"\", inplace=True)\ndf.dropna(axis=0, inplace=True, subset=['text'])\n# shuffle the data\ndf = df.sample(frac=1.0)\ndf.reset_index(drop=True,inplace=True)\ndf.head()","outputs":[],"execution_count":null,"metadata":{"scrolled":false},"cell_type":"code"},{"source":"Define some functions to clean and tokenize the data","metadata":{},"cell_type":"markdown"},{"source":"def initial_clean(text):\n    \"\"\"\n    Function to clean text of websites, email addresess and any punctuation\n    We also lower case the text\n    \"\"\"\n    text = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", text)\n    text = re.sub(\"[^a-zA-Z ]\", \"\", text)\n    text = text.lower() # lower case the text\n    text = nltk.word_tokenize(text)\n    return text\n\nstop_words = stopwords.words('english')\ndef remove_stop_words(text):\n    \"\"\"\n    Function that removes all stopwords from text\n    \"\"\"\n    return [word for word in text if word not in stop_words]\n\nstemmer = PorterStemmer()\ndef stem_words(text):\n    \"\"\"\n    Function to stem words, so plural and singular are treated the same\n    \"\"\"\n    try:\n        text = [stemmer.stem(word) for word in text]\n        text = [word for word in text if len(word) > 1] # make sure we have no 1 letter words\n    except IndexError: # the word \"oed\" broke this, so needed try except\n        pass\n    return text\n\ndef apply_all(text):\n    \"\"\"\n    This function applies all the functions above into one\n    \"\"\"\n    return stem_words(remove_stop_words(initial_clean(text)))","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# clean text and title and create new column \"tokenized\"\nt1 = time.time()\ndf['tokenized'] = df['text'].apply(apply_all) + df['title'].apply(apply_all)\nt2 = time.time()\nprint(\"Time to clean and tokenize\", len(df), \"articles:\", (t2-t1)/60, \"min\")","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"### Get word frequency\n\nWe'll use nltk to get a word frequency (by count) here and only keep the top most used words to train the LDA model on","metadata":{},"cell_type":"markdown"},{"source":"# first get a list of all words\nall_words = [word for item in list(df['tokenized']) for word in item]\n# use nltk fdist to get a frequency distribution of all words\nfdist = FreqDist(all_words)\nlen(fdist) # number of unique words","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# choose k and visually inspect the bottom 10 words of the top k\nk = 50000\ntop_k_words = fdist.most_common(k)\ntop_k_words[-10:]","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# choose k and visually inspect the bottom 10 words of the top k\nk = 15000\ntop_k_words = fdist.most_common(k)\ntop_k_words[-10:]","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"k = 50,000 is too high, as the bottom words aren't even real words and are very rarely used (once in entire corpus)\n\nk = 15,000 is much more reasonable as these have been used at least 13 times in the corpus","metadata":{},"cell_type":"markdown"},{"source":"# define a function only to keep words in the top k words\ntop_k_words,_ = zip(*fdist.most_common(k))\ntop_k_words = set(top_k_words)\ndef keep_top_k_words(text):\n    return [word for word in text if word in top_k_words]","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"cell_type":"code"},{"source":"df['tokenized'] = df['tokenized'].apply(keep_top_k_words)","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# document length\ndf['doc_len'] = df['tokenized'].apply(lambda x: len(x))\ndoc_lengths = list(df['doc_len'])\ndf.drop(labels='doc_len', axis=1, inplace=True)\n\nprint(\"length of list:\",len(doc_lengths),\n      \"\\naverage document length\", np.average(doc_lengths),\n      \"\\nminimum document length\", min(doc_lengths),\n      \"\\nmaximum document length\", max(doc_lengths))","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# plot a histogram of document length\nnum_bins = 1000\nfig, ax = plt.subplots(figsize=(12,6));\n# the histogram of the data\nn, bins, patches = ax.hist(doc_lengths, num_bins, normed=1)\nax.set_xlabel('Document Length (tokens)', fontsize=15)\nax.set_ylabel('Normed Frequency', fontsize=15)\nax.grid()\nax.set_xticks(np.logspace(start=np.log10(50),stop=np.log10(2000),num=8, base=10.0))\nplt.xlim(0,2000)\nax.plot([np.average(doc_lengths) for i in np.linspace(0.0,0.0035,100)], np.linspace(0.0,0.0035,100), '-',\n        label='average doc length')\nax.legend()\nax.grid()\nfig.tight_layout()\nplt.show()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"We can see that, compared to our histogram in exploring_news notebook, the average document length is about half when all stop words are removed and only the top 15,000 words are used.","metadata":{},"cell_type":"markdown"},{"source":"### Drop short articles\n\nLDA does not work very well on short documents, which we will explain later, so we will drop some of the shorter articles here before training the model.\n\nFrom the histogram above, droping all articles less than 40 tokens seems appropriate.","metadata":{},"cell_type":"markdown"},{"source":"# only keep articles with more than 30 tokens, otherwise too short\ndf = df[df['tokenized'].map(len) >= 40]\n# make sure all tokenized items are lists\ndf = df[df['tokenized'].map(type) == list]\ndf.reset_index(drop=True,inplace=True)\nprint(\"After cleaning and excluding short aticles, the dataframe now has:\", len(df), \"articles\")","outputs":[],"execution_count":null,"metadata":{"scrolled":true},"cell_type":"code"},{"source":"df.head()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"### Split the corpus into training and testing\nHere we will split the corpus into training and testing sets.\n\nThe training set will be used to train the LDA model on, while the testing set will be used to retrieve similar articles later in our recommendation algorithm.\n\nThe dataframe is already shuffled from the begining, so no need to do it again.","metadata":{},"cell_type":"markdown"},{"source":"# create a mask of binary values\nmsk = np.random.rand(len(df)) < 0.999","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"cell_type":"code"},{"source":"train_df = df[msk]\ntrain_df.reset_index(drop=True,inplace=True)\n\ntest_df = df[~msk]\ntest_df.reset_index(drop=True,inplace=True)","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"print(len(df),len(train_df),len(test_df))","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# LDA\n\nLatent Dirichlet Allocation, is an unsupervised generative model that assigns topic distributions to documents.\n\nAt a high level, the model assumes that each document will contain several topics, so that there is topic overlap within a document. The words in each document contribute to these topics. The topics may not be known a priori, and needn't even be specified, but the **number** of topics must be specified a priori. Finally, there can be words overlap between topics, so several topics may share the same words.\n\nThe model generates to **latent** (hidden) variables\n1) A distribution over topics for each document\n2) A distribution over words for each topics\n\nAfter training, each document will have a discrete distribution over all topics, and each topic will have a discrete distribution over all words.\n\nIt is best to demonstrate this with an example. Let's say a document about the presidential elections may have a high contribution from the topics \"presidential elections\", \"america\", \"voting\" but have very low contributions from topics \"himalayan mountain range\", \"video games\", \"machine learning\" (assuming the corpus is varied enough to contain such articles); the topics \"presidential elections\" may have top contributing words [\"vote\",\"election\",\"people\",\"usa\",\"clinton\",\"trump\",...] whereas the top contributing words in the topic \"himalayan mountain range\" may be [\"nepal\",\"everest\",\"china\",\"altitude\",\"river\",\"snow\",....]. This very rough example should give you an idea of what LDA aims to do.\n\nAn important point to note: although I have named some topics in the example above, the model itself does not actually do any \"naming\" or classifying of topics. But by visually inspecting the top contributing words of a topic i.e. the discrete distribution over words for a topic, one can name the topics if necessary after training. We will show this more later.\n\nThere a several ways to implement LDA, however I will speak about collapsed gibbs sampling as I usually find this to be the easiest way to understand it.\n\nThe model initialises by assigning every word in every document to a **random** topic. Then, we iterate through each word, unassign it's current topic, decrement the topic count corpus wide and reassign the word to a new topic based on the local probability of topic assignemnts to the current document, and the global (corpus wide) probability of the word assignments to the current topic. This may be hard to understand in words, so the equations are below.","metadata":{},"cell_type":"markdown"},{"source":"### The mathematics of collapsed gibbs sampling (cut back version)\n\nRecall that when we iterate through each word in each document, we unassign its current topic assignment and reassign the word to a new topic. The topic we reassign the word to is based on the probabilities below.\n\n$$\nP\\left(\\text{document \"likes\" the topic}\\right) \\times P\\left(\\text{topic \"likes\" the word } w'\\right)\n$$\n\n$$\n\\Rightarrow \\frac{n_{i,k}+\\alpha}{N_i-1+K\\alpha} \\times \\frac{m_{w',k}+\\gamma}{\\sum_{w\\in V}m_{w,k} + V\\gamma}\n$$\n\nwhere\n\n$n_{i,k}$ - number of word assignments to topic $k$ in document $i$\n\n$n_{i,k}$ - number of assignments to topic $k$ in document $i$\n\n$\\alpha$ - smoothing parameter (hyper parameter - make sure probability is never 0)\n\n$N_i$ - number of words in document $i$\n\n$-1$ - don't count the current word you're on\n\n$K$ - total number of topics\n\n\n$m_{w',k}$ - number of assignments, corpus wide, of word $w'$ to topic $k$\n\n$m_{w',k}$ - number of assignments, corpus wide, of word $w'$ to topic $k$\n\n$\\gamma$ - smoothing parameter (hyper parameter - make sure probability is never 0)\n\n$\\sum_{w\\in V}m_{w,k}$ - sum over all words in vocabulary currently assigned to topic $k$\n\n$V$ size of vocabulary i.e. number of distinct words corpus wide","metadata":{},"cell_type":"markdown"},{"source":"### Notes and Uses of LDA\n\nLDA has many uses; understanding the different varieties topics in a corpus (obviously), getting a better insight into the type of documents in a corpus (whether they are about news, wikipedia articles, business documents), quantifying the most used / most important words in a corpus, and even document similarity and recommendation.\n\nLDA does not work well with very short documents, like twitter feeds, as explained here [[1]](https://pdfs.semanticscholar.org/f499/5dc2a4eb901594578e3780a6f33dee02dad1.pdf) [[2]](https://stackoverflow.com/questions/29786985/whats-the-disadvantage-of-lda-for-short-texts), which is why we dropped articles under 40 tokens previously. Very briefly, this is because the model infers parameters from observations and if there are not enough observations (words) in a document, the model performs poorly. For short texts, although yet to be rigoursly tested, it may be best to use a [biterm model](https://pdfs.semanticscholar.org/f499/5dc2a4eb901594578e3780a6f33dee02dad1.pdf).\n\nUnlike the word2vec algorithm, which performs extremely well with full structured sentences, LDA is a bag of words model, meaning word order in a document doesnt count. This also means that stopwords and rare words should be excluded, so that the model doesnt overcompensate for very frequent words and very rare words, both of which do not contribute to general topics.\n\n#### Hyperparameters\n\nLDA has 2 hyperparameters: $\\alpha$ and $\\eta$\n\n$\\alpha$ - A low value for $\\alpha$ means that documents have only a low number of topics contributing to them. A high value of $\\alpha$ yields the inverse, meaning the documents appear more alike within a corpus.\n\n$\\eta$ - A low value for $\\eta$ means the topics have a low number of contributing words. A high value of $\\eta$ yields the inverse, meaning topics will have word overlap and appear more alike.\n\nThe values of $\\alpha$ and $\\eta$ really depend on the application, and may need to be tweaked several times before the desired results are found... even then, LDA is non-deterministic since parameters are randomly initialised, so the outcome of any run of the model can never be known in advance.","metadata":{},"cell_type":"markdown"},{"source":"def train_lda(data):\n    \"\"\"\n    This function trains the lda model\n    We setup parameters like number of topics, the chunksize to use in Hoffman method\n    We also do 2 passes of the data since this is a small dataset, so we want the distributions to stabilize\n    \"\"\"\n    num_topics = 100\n    chunksize = 300\n    dictionary = corpora.Dictionary(data['tokenized'])\n    corpus = [dictionary.doc2bow(doc) for doc in data['tokenized']]\n    t1 = time.time()\n    # low alpha means each document is only represented by a small number of topics, and vice versa\n    # low eta means each topic is only represented by a small number of words, and vice versa\n    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,\n                   alpha=1e-2, eta=0.5e-2, chunksize=chunksize, minimum_probability=0.0, passes=2)\n    t2 = time.time()\n    print(\"Time to train LDA model on \", len(df), \"articles: \", (t2-t1)/60, \"min\")\n    return dictionary,corpus,lda","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"dictionary,corpus,lda = train_lda(train_df)","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"### Let's inspect some topics!\n\nBear in mind, when we see the words they may seem shortened. Recall this is because of our stemming function we previously implemented.","metadata":{},"cell_type":"markdown"},{"source":"# show_topics method shows the the top num_words contributing to num_topics number of random topics\nlda.show_topics(num_topics=10, num_words=20)","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#### We can inspect individual topics as such\n\nNote that if you re run the model again, as it is non-deterministic, word contributions to topics and topic ID's will change.","metadata":{"collapsed":true},"cell_type":"markdown"},{"source":"#### This topic is about court cases","metadata":{},"cell_type":"markdown"},{"source":"lda.show_topic(topicid=4, topn=20)","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#### This topic is about (supposedly) Illegal Immigration","metadata":{},"cell_type":"markdown"},{"source":"lda.show_topic(topicid=85, topn=20)","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#### This topic is about Religion","metadata":{},"cell_type":"markdown"},{"source":"lda.show_topic(topicid=75, topn=20)","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"#### This topic is about Climate Change","metadata":{},"cell_type":"markdown"},{"source":"lda.show_topic(topicid=39, topn=20)","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"What the about above means, is that topic 4 has top contributing words [\"judge\",\"case\",\"court\",...], which indicates the topic is about court cases. Topic 75 has top contributing words [\"god\",\"christian\",\"love\",...], which indicates the topic is about religion.\n\nNow, not only can we see the word contribution for each topic, but we can also visualise the topic contribution for each article.","metadata":{},"cell_type":"markdown"},{"source":"# select and article at random from train_df\nrandom_article_index = np.random.randint(len(train_df))\nbow = dictionary.doc2bow(train_df.iloc[random_article_index,7])\nprint(random_article_index)","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"print(train_df.iloc[random_article_index,3])","outputs":[],"execution_count":null,"metadata":{"scrolled":false},"cell_type":"code"},{"source":"# get the topic contributions for the document chosen at random above\ndoc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow)])","outputs":[],"execution_count":null,"metadata":{"scrolled":true},"cell_type":"code"},{"source":"# bar plot of topic distribution for this document\nfig, ax = plt.subplots(figsize=(12,6));\n# the histogram of the data\npatches = ax.bar(np.arange(len(doc_distribution)), doc_distribution)\nax.set_xlabel('Topic ID', fontsize=15)\nax.set_ylabel('Topic Contribution', fontsize=15)\nax.set_title(\"Topic Distribution for Article \" + str(random_article_index), fontsize=20)\nax.set_xticks(np.linspace(10,100,10))\nfig.tight_layout()\nplt.show()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"Ok, so clearly this document has various contributions from different topics. But what are these topics? Lets find out!","metadata":{},"cell_type":"markdown"},{"source":"# print the top 5 contributing topics and their words\nfor i in doc_distribution.argsort()[-5:][::-1]:\n    print(i, lda.show_topic(topicid=i, topn=10), \"\\n\")","outputs":[],"execution_count":null,"metadata":{"scrolled":false},"cell_type":"code"},{"source":"Let's interpret this.\n\nTopic 9  - Protests\n\nTopic 72 - Middl Eastern Countries\n\nTopic 36 - Islam\n\nTopic 55 - Power (socio political sense)\n\nTopic 38 - Peoples actions\n\nThese are rough interpretations for these topics, most of which make sense. Reading the article we see the it is about riots in the Middle East. So the model seems to have worked well, at least in this one case.","metadata":{},"cell_type":"markdown"},{"source":"# Similarity Queries and Unseen Data\n\nWe will now turn our attention to the test set of data which the model has not yet seen. Although the articles in *test_df* have been unseen by the model, gensim has a way of infering their topic distributions given the trained model. Of course, the correct approach to yield accurate results would be to retrain the model with these new articles part of the corpus, but this can be timely and infeasable in a real case scenario where results are needed quickly.\n\nFirst, lets show how we can infer document topics for a new unseen article.","metadata":{},"cell_type":"markdown"},{"source":"# select and article at random from test_df\nrandom_article_index = np.random.randint(len(test_df))\nprint(random_article_index)","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"Here's the important bit. In obtaining the BOW representation for this unseen article, gensim cleverly only considers words in the existing dictionary we used to train the model. So if there are new words in this article, they will not be considered when infering the topic distribution. This is good in that no errors arise for unseen words, but bad in that some words may be cut out, and therefore we could miss out on an accurate topic distribution for this article.\n\nHowever, we mitigate this risk because the training set is very much representative of the entire corpus; 99.9% of the observations are in the training set, with only 0.01% of observations in the test set. So most, if not all, words from the test set should be in the training set's dictionary.","metadata":{},"cell_type":"markdown"},{"source":"new_bow = dictionary.doc2bow(test_df.iloc[random_article_index,7])","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"cell_type":"code"},{"source":"print(test_df.iloc[random_article_index,3])","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"cell_type":"code"},{"source":"Let's do the same visual analysis as before on this new unseen document","metadata":{},"cell_type":"markdown"},{"source":"# bar plot of topic distribution for this document\nfig, ax = plt.subplots(figsize=(12,6));\n# the histogram of the data\npatches = ax.bar(np.arange(len(new_doc_distribution)), new_doc_distribution)\nax.set_xlabel('Topic ID', fontsize=15)\nax.set_ylabel('Topic Contribution', fontsize=15)\nax.set_title(\"Topic Distribution for an Unseen Article\", fontsize=20)\nax.set_xticks(np.linspace(10,100,10))\nfig.tight_layout()\nplt.show()","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"# print the top 8 contributing topics and their words\nfor i in new_doc_distribution.argsort()[-5:][::-1]:\n    print(i, lda.show_topic(topicid=i, topn=10), \"\\n\")","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"And there we have it! An accurate topic distribution for an unseen document.","metadata":{},"cell_type":"markdown"},{"source":"### Similarity query\n\nOk, now that we have a topic distribution for a new unseen document, let's say we wanted to find the most similar documents in the corpus. We can do this by comparing the topic distribution of the new document to all the topic distributions of the documents in the corpus. We use the [Jensen-Shannon distance](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence) metric to find the most similar documents.\n\nWhat the Jensen-Shannon distance tells us, is which documents are statisically \"closer\" (and therefore more similar), by comparing the divergence of their distributions. Jensen-Shannon is symmetric, unlike Kullback-Leibler on which the formula is based. This is good, because we want the similarity between documents A and B to be the same as the similarity between B and A.\n\nThe formula is described below.\n\nFor discrete distirbutions $P$ and $Q$, the Jensen-Shannon divergence, $JSD$ is defined as\n\n$$JSD\\left(P||Q\\right) = \\frac{1}{2}D\\left(P||M\\right)+\\frac{1}{2}D\\left(Q||M\\right)$$\n\nwhere $M = \\frac{1}{2}\\left(P+Q\\right)$\n\nand $D$ is the Kullback-Leibler divergence\n\n$$D\\left(P||Q\\right) = \\sum_iP(i)\\log\\left(\\frac{P(i)}{Q(i)}\\right)$$\n\n$$\\Rightarrow JSD\\left(P||Q\\right) = \\frac{1}{2}\\sum_i\n\\left[\nP(i)\\log\\left(\\frac{P(i)}{\\frac{1}{2}\\left(P(i)+Q(i)\\right)}\\right)\n+\nQ(i)\\log\\left(\\frac{Q(i)}{\\frac{1}{2}\\left(P(i)+Q(i)\\right)}\\right)\n\\right]$$\n\nThe square root of the Jensen-Shannon divergence is the Jensen-Shannon Distance: $\\sqrt{JSD\\left ( P||Q\\right )}$\n\n**The smaller the Jensen-Shannon Distance, the more similar two distributions are (and in our case, the more similar any 2 documents are)**","metadata":{},"cell_type":"markdown"},{"source":"We can use the scipy implementation of entropy to do this. Entropy calculates the KL divergence.\n\nBut first, we need to get all our LDA topic distributions into a dense matrix. This will enable fast and efficient computation.\n\nWe will create a dense matrix, **doc_topic_dist**, of size $M\\times K$ where $M$ is the number of documents and $K$ is the number of topics.","metadata":{"collapsed":true},"cell_type":"markdown"},{"source":"# we need to use nested list comprehension here\n# this may take 1-2 minutes...\ndoc_topic_dist = np.array([[tup[1] for tup in lst] for lst in lda[corpus]])\ndoc_topic_dist.shape","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"def jensen_shannon(query, matrix):\n    \"\"\"\n    This function implements a Jensen-Shannon similarity\n    between the input query (an LDA topic distribution for a document)\n    and the entire corpus of topic distributions.\n    It returns an array of length M where M is the number of documents in the corpus\n    \"\"\"\n    # lets keep with the p,q notation above\n    p = query[None,:].T # take transpose\n    q = matrix.T # transpose matrix\n    m = 0.5*(p + q)\n    return np.sqrt(0.5*(entropy(p,m) + entropy(q,m)))","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"cell_type":"code"},{"source":"Let's compare the new unseen document, to the corpus, and see which articles are most similar.","metadata":{},"cell_type":"markdown"},{"source":"def get_most_similar_documents(query,matrix,k=10):\n    \"\"\"\n    This function implements the Jensen-Shannon distance above\n    and retruns the top k indices of the smallest jensen shannon distances\n    \"\"\"\n    sims = jensen_shannon(query,matrix) # list of jensen shannon distances\n    return sims.argsort()[:k] # the top k positional index of the smallest Jensen Shannon distances","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"cell_type":"code"},{"source":"#### Query time + most similar documents... at last!\n\nOk, let's be 100% clear about what we are doing here.\n\nWe are comparing the new unseen document above to the entire corpus of ~10k documents to find which one is most similar to the new document.\n\nHow are we doing that? Well, we have the new documents LDA topic distribution in stored as varibale **new_doc_distribution**, and we have the entire corpus of documents topic distributions stored in the dense matrix **doc_topic_dist**. So now, we pass each row of **doc_topic_dist** through the Jensen-Shannon function above as the Q distribution, while the P distribution remains static as **new_doc_distribution**. Then we get the smallest distances and their corresponding index in the array, which we can pass to the **train_df** dataframe to print out the most similar documents.","metadata":{},"cell_type":"markdown"},{"source":"# this is surprisingly fast\nmost_sim_ids = get_most_similar_documents(new_doc_distribution,doc_topic_dist)","outputs":[],"execution_count":null,"metadata":{},"cell_type":"code"},{"source":"most_similar_df = train_df[train_df.index.isin(most_sim_ids)]\nmost_similar_df['title']","outputs":[],"execution_count":null,"metadata":{"scrolled":true},"cell_type":"code"},{"source":"I think we can see, the top most similar articles are quite similar indeed to the query article ;)\n\nOur query article is about Trump, Huffington Post and the election. The top 10 most similar documents in the corpus also contain these topics, as their title show above. The reader can print out the full articles, or visualise the topic distributions for the most similar document and compare them to the query document to check the overlap.","metadata":{},"cell_type":"markdown"},{"source":"## Conclusion\n\n- After cleaning the corpus and keeping only the top 15,000 words, we reduced the unique words in the corpus by 84%\n- The average document length is halved to 345 tokens after cleaning, compared to the raw version we saw in our explore notebook using word2vec\n- The LDA algorithm was explained in detail\n- The LDA model was able to accurately identify different topics in the fake news corpus. We visually inspected these topics to see that the top words were related\n- We were able to infer a topic distribution from a new unseen document\n- We quickly retrieved the most similar documents in the trained corpus when comparing to the new unseen document. These most similar documents were in fact closely related to the query document","metadata":{},"cell_type":"markdown"}],"nbformat":4}