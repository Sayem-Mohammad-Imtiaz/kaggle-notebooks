{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Deep Learning 01: Logistic Regression\n\nThis is the first notebook of hands on by following the deep learning specialization course in Couresera. Logistic regression is a fundamental unit in deep neural network. In this notebook, we will use a single unit of logistic regression to categorize real image data. The images are Gaussian filtered retina scan to detect diabetic retinopathy. In the end we will see that with a simple logistic regression, we can detect the diabetic retinopathy on the images with an accuracy around 85%."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Initialize the notebook\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\nnp.random.seed(17)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1) Data pre-processing\n\nThe images are 224x224 pixels RGB images. We load the images into 224x224x3 numpy arrays. For simple classification, we will classify only \"No_DR\" (no diabetic retinopathy) and others. We label \"No_DR\" is 0 and the others are 1."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load images and create label vector\nroot_path = \"../input/diabetic-retinopathy-224x224-gaussian-filtered/\"\nimages_path = root_path + \"gaussian_filtered_images/gaussian_filtered_images/\"\nimages = [] # images\nY = [] # label vector\n\ndir_names = [dir_name for dir_name in os.listdir(images_path) if os.path.isdir(images_path + dir_name)]\nfor dir_name in dir_names:\n    if dir_name == \"No_DR\": # Create a label\n        y = 0 \n    else:\n        y = 1\n    for file_name in os.listdir(images_path + dir_name):\n        images.append(plt.imread(images_path + dir_name + \"/\" + file_name))\n        Y.append(y)\n    print(\"Finish reading \" + dir_name + \" condition images.\")\n    print(\"Total image read: \" + str(len(images)))\nprint(\"End of the data\")\nY = np.reshape(np.array(Y), (1, -1)) # Make Y to be a row vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us see some detail and some images that we loaded."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print some detail\nprint(\"Size of each image: \" + str(images[0].shape))\nprint(\"Size of labels: \" + str(Y.shape))\nprint(\"Number of DR (y = 1): \" + str(np.sum(Y)))\nprint(\"Number of No_DR (y = 0): \" + str(Y.shape[1] - np.sum(Y)))\n\n# Randomly show images\nn = 4 # show 4 images\nfig, axes = plt.subplots(nrows = 1, ncols = n, figsize=(16, 16))\nrand_idx = np.random.randint(0, len(images), n)\nfor i in range(n):\n    axes[i].imshow(images[rand_idx[i]])\n    axes[i].title.set_text(\"Image #\" + str(rand_idx[i]) + \"\\n y label = \" + str(Y[0][rand_idx[i]]))\n    axes[i].axis(\"off\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"For simplicity and less time consuming, we will use grayscale images intead of RGB images for classification. We turn the RGB images to grayscale images and therefore each image is 224x224 2D array."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to convert RGB to grayscale\ndef rgb_to_gray(image):\n    rgb_weights = [0.2989, 0.5870, 0.1140]\n    return np.dot(image, rgb_weights)\n\ngray_images = list(map(rgb_to_gray, images)) # Convert RGB images to grayscale images\n\n# Print the image size\nprint(\"Size of each grayscale image: \" + str(gray_images[0].shape))\n\n# Show grayscale images\nfig, axes = plt.subplots(nrows = 1, ncols = n, figsize=(16, 16))\nfor i in range(n):\n    axes[i].imshow(gray_images[rand_idx[i]], cmap='gray')\n    axes[i].title.set_text(\"Image #\" + str(rand_idx[i]) + \"\\n y label = \" + str(Y[0][rand_idx[i]]))\n    axes[i].axis(\"off\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use all pixels as features for one sample. Therefore each sample will have 224x224 = 50176 features. For convinience, we flatten the features into one column for each sample and stack all samples together horizontally. Let us denote $n_x$ as a number of features and $m$ as a number of samples. Then the dataset $X$ and the label $Y$ can be written as\n\n$X = \\begin{bmatrix}\nx_1^{(1)} & x_1^{(2)} & x_1^{(3)} & \\cdots & x_1^{(m)} \\\\\nx_2^{(1)} & x_2^{(2)} & x_2^{(3)} & \\cdots & x_2^{(m)} \\\\\nx_3^{(1)} & x_3^{(2)} & x_3^{(3)} & \\cdots & x_3^{(m)} \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots \\\\\nx_{n_x}^{(1)} & x_{n_x}^{(2)} & x_{n_x}^{(3)} &  & x_{n_x}^{(m)}\n\\end{bmatrix},\n\\quad \\quad\nY = \\begin{bmatrix}\ny^{(1)} & y^{(2)} & y^{(3)} & \\cdots & y^{(m)}\n\\end{bmatrix} $\n\nwhere the superscript $(i)$ means the $i^{th}$ sample and the lowerscript $j$ means the $j^{th}$ feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare dataset matrix X\nX = np.array(gray_images).reshape(len(gray_images), -1).T # flatten and reshape\nprint(\"Size of data set (n_x, m): \" + str(X.shape))\nprint(\"Size of label (1, m): \" + str(Y.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now our data are ready for logistic regression."},{"metadata":{},"cell_type":"markdown","source":"## 2) Logistic regression\n\nLogistic regression is a basic unit in deep nerual network. Let us start with the first two steps:\n* linear transformation with parameters called weight ($w$) and bias ($b$)\n* Calculate activation function, for a simple logistic regression, we will use a sigmoid function as an activation function."},{"metadata":{},"cell_type":"markdown","source":"### 2.1) Linear transformation\n\nThe first step of doing logistic regression is doing a linear transformation for each sample using a weight vector ($w$) and a bias scalar($b$),\n\n$z^{(i)} = w^T x^{(i)} + b$\n\nwhere the weight vector is $(n_x, 1)$ column vector. The vectorization form of the linear transformation can be expressed easily as\n\n$Z = w^T X + b$\n\nwhere we utilize the broadcasting feature of Python for $b$ here.\n\nThe weight and the bias are the parameters we have to find so that it can predict the output as well as possible. To begin with, we have to initiate the weight vector and the bias. There is a few ways to initiate the parameters. Here, the weight vector and the bias are initiated to be zero."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize w and b function\ndef initialize_parameters(n_x):\n    w = np.zeros((n_x, 1))\n    b = 0\n    return w, b","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2) Sigmoid function\n\nFor a simple logistic regression, we will use a sigmoid function as an activation function. Sigmoid function is defined as\n\n$ f(z) = \\displaystyle\\frac{1}{1 + e^{-z}} $"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sigmoid function\ndef sigmoid(z):\n    return 1/(1 + np.exp(-z))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is a plot of the sigmoid function"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot of sigmoid function\nz = np.linspace(-4, 4, 101)\nplt.figure(figsize = (5, 3))\n_ = plt.plot(z, sigmoid(z))\nplt.title(\"Sigmoid(z)\")\nplt.xlabel(\"z\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In logistic regression, the input of the sigmoid function is the linear transformation of the features and the output is a probability that the label will be 1."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 2.3) Forward and backward propagation\n\nTo find appropriate parameters $w$ and $b$, we have to minimize a cost function,\n\n$J = -\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n\nwhere $y$ is a true label from the training set and $a$ is a value of the activation function. This cost function comes from that we want to maximize likelihood estimation.\n\nTo minimize the cost function, we use a common algorithm called \"gradient decent\" which an update rule can be described as\n\n$ w = w - \\alpha \\displaystyle \\frac{\\partial J}{\\partial w}$\n\n$ b = b - \\alpha \\displaystyle \\frac{\\partial J}{\\partial b}$\n\nwhere $\\alpha$ is a learning rate. We can run the altorithm as many iterations as we want to minimize the cost function as much as possible. The partial derivatives for logistic regression can be derived as\n\n$\\displaystyle \\frac{\\partial J}{\\partial w} = \\displaystyle \\frac{1}{m}X(A-Y)^T$\n\n$\\displaystyle \\frac{\\partial J}{\\partial b} = \\displaystyle \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$\n\nwhere $A$ is a row vector of the activation values. Derivation of the partial derivatives can be found in the last section of the notebook.\n\nOverall, the algorithm can be implemented as follow.\n1. For a given weight $w$, bias $b$ and dataset $X$, calculate the prediction value. This is a forward propagation.\n2. Calculate the cost function\n3. Calculate the derivatives. This is a backward propagation.\n4. Update the weight and bias.\n\nWe can repeat step one to four as much as we like and check that how the cost function evolves. The cost function should decrease with more iterations.\n\nFor convenience, we implement step one to three as a function called \"propagate\" and another function to run the step one to four iteratively called \"logistic_regression\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Forward and backward propagation\ndef propagate(X, Y, w, b):\n    m = Y.shape[1]\n    grads = {}\n    A = sigmoid(w.T @ X + b) # forward propagation\n    cost = -1/m * (np.sum (Y * np.log(A)) + np.sum((1 - Y) * np.log(1 - A))) # cost function\n    \n    # backward propagation\n    grads['dw'] = 1/m * X @ (A - Y).T\n    grads['db'] = 1/m * np.sum(A - Y, axis = 1, keepdims = True)\n    \n    return A, grads, cost   \n\n# Logistic regression function\ndef logistic_regression(X, Y, learning_rate = 0.0006, num_iter = 200, print_cost = True):\n    w, b = initialize_parameters(X.shape[0]) # initailize the parameters\n    costs = []\n    \n    # logistic regression\n    for i in range(num_iter):\n        A, grads, cost = propagate(X, Y, w, b)\n        if print_cost and i % 20 == 0:\n            print(\"Iteration #\" + str(i) + \"\\tCost value = \" + str(cost))\n        costs.append(cost)\n        w -= learning_rate * grads['dw']\n        b -= learning_rate * grads['db']\n    \n    # compute the cost of the final parameter\n    A, grads, cost = propagate(X, Y, w, b)\n    print(\"Final cost value = \" + str(cost))\n    costs.append(cost)\n    \n    return w, b, costs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3) Train the model\nNow we are ready. let us split the dataset into train/test set to be 75:25 roughly."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data to train/test set\nX_train, X_test, Y_train, Y_test = train_test_split(X.T, Y.T, test_size = 0.25, random_state = 5)\nX_train, X_test, Y_train, Y_test = X_train.T, X_test.T, Y_train.T, Y_test.T\n\n# Print some detail\nm = Y.shape[1]\nm_train, m_test = Y_train.shape[1], Y_test.shape[1]\ny1_train, y1_test = np.sum(Y_train), np.sum(Y_test)\nprint(\"number of train samples: \" + str(m_train) + \"(\" + \"{0:.2f}\".format(m_train/m*100) + \"%)\")\nprint(\"number of DR cases in train samples: \" + str(y1_train) + \"(\" + \"{0:.2f}\".format(y1_train/m_train*100) + \"%)\")\nprint(\"number of test samples: \" + str(m_test) + \"(\" + \"{0:.2f}\".format(m_test/m*100) + \"%)\")\nprint(\"number of DR cases in test samples: \" + str(y1_test) + \"(\" + \"{0:.2f}\".format(y1_test/m_test*100) + \"%)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We train the model with 200 iterations (with the train dataset) and learning rate = 0.0006."},{"metadata":{"trusted":true},"cell_type":"code","source":"w, b = initialize_parameters(X.shape[0]) # initailize the parameters\nw, b, costs = logistic_regression(X_train, Y_train, learning_rate = 0.0006, num_iter = 200)\n_ = plt.plot(costs)\nplt.xlabel(\"number of iterations\")\nplt.ylabel(\"Cost value\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got the learnt parameters. Let us use the parameters to predict the output. If the output is greater than 0.5, it will predict 1 (DR) and vise versa. The accuracies are calculated on both training dataset and test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict function\ndef predict(X, Y, w, b):\n    A, _, _ = propagate(X, Y, w, b)\n    A[A >= 0.5] = 1\n    A[A < 0.5] = 0\n    diff = np.abs(A - Y)\n    acc = 1 - np.sum(diff)/diff.shape[1]\n    return A, diff, acc\n\nyhat_train, _, acc_train = predict(X_train, Y_train, w, b) # Accuracy on train set\nprint(\"Accuracy on train set: \" + \"{0:.2f}\".format(acc_train*100) + \"%\")\n\nyhat_test, _, acc_test = predict(X_test, Y_test, w, b) # Accuracy on test set\nprint(\"Accuracy on test set: \" + \"{0:.2f}\".format(acc_test*100) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, we get the model for diabetic retinopathy detection using Gaussian filtered retina images with accuracy around 85%"},{"metadata":{},"cell_type":"markdown","source":"## 4 Derivatives derivation\n\nIn this section, we will roughly show how the derivatives below in back propagation can be derived. Let us introduce a lost function as\n\n$ \\mathcal{L}^{(i)} = y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}) $.\n\n\nRecall that $a^{(i)}$ is a sigmoid function of $z^{(i)}$ which the derivative can be found as\n\n\n$ \\displaystyle \\frac{\\partial a^{(i)}}{\\partial z^{(i)}} = \\frac{e^{-z^{(i)}}}{(1 + e^{-z^{(i)}})^2} = \\frac{1 + e^{-z^{(i)}} - 1}{(1 + e^{-z^{(i)}})^2} = (1/a^{(i)} - 1)\\cdot(a^{(i)})^2 = a^{(i)}(1 - a^{(i)})$.\n\nWe also know that $z^{(i)}$ is a linear transfrom of the input features. The derivatives respect to the parameters are\n\n$ \\displaystyle \\frac{\\partial z^{(i)}}{\\partial b} = 1$\n\n$ \\displaystyle \\frac{\\partial z^{(i)}}{\\partial w_j} = x_j^{(i)}$\n\n### 4.1) Proof of the derivative respect to $b$\n\nLet us compute\n\n$\\displaystyle \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial b} = \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial a^{(i)}} \\cdot \\frac{\\partial a^{(i)}}{\\partial z^{(i)}} \\cdot \\frac{\\partial z^{(i)}}{\\partial b}= \\left( \\frac{y^{(i)}}{a^{(i)}} - \\frac{1-y^{(i)}}{1-a^{(i)}}\\right) \\cdot a^{(i)}(1 - a^{(i)}) \\cdot 1 = y^{(i)} - a^{(i)}$\n\nand therefore,\n\n$\\displaystyle \\frac{\\partial J}{\\partial b} = -\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\frac{\\partial \\mathcal{L}^{(i)}}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$\n\n\n### 4.2) Proof of the derivative repect to $w$\n\nLet us compute\n\n$\\displaystyle \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial w_j} = \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial a^{(i)}} \\cdot \\frac{\\partial a^{(i)}}{\\partial z^{(i)}} \\cdot \\frac{\\partial z^{(i)}}{\\partial w_j}= \\left( \\frac{y^{(i)}}{a^{(i)}} - \\frac{1-y^{(i)}}{1-a^{(i)}}\\right) \\cdot a^{(i)}(1 - a^{(i)}) \\cdot x_j^{(i)} = x_j^{(i)}(y^{(i)} - a^{(i)})$\n\nand \n\n$\\displaystyle \\frac{\\partial J}{\\partial w_j} = -\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\frac{\\partial \\mathcal{L}^{(i)}}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m x_j^{(i)}(a^{(i)}-y^{(i)})$.\n\nTo get the vectorization of the derivative, we can show that\n\n$\\displaystyle \\frac{\\partial J}{\\partial w} = \n\\begin{bmatrix}\n\\partial J/\\partial w_1 \\\\\n\\partial J/\\partial w_2 \\\\\n\\partial J/\\partial w_3 \\\\\n\\vdots \\\\\n\\partial J/\\partial w_{n_x}\n\\end{bmatrix} = \\displaystyle \\frac{1}{m}\n\\begin{bmatrix}\n\\sum_{i=1}^m x_1^{(i)} (a^{(i)}-y^{(i)})\\\\\n\\sum_{i=1}^m x_2^{(i)} (a^{(i)}-y^{(i)}) \\\\\n\\sum_{i=1}^m x_3^{(i)} (a^{(i)}-y^{(i)}) \\\\\n\\vdots \\\\\n\\sum_{i=1}^m x_{n_x}^{(i)} (a^{(i)}-y^{(i)})\n\\end{bmatrix} = \\displaystyle \\frac{1}{m} X(A - Y)^T$"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}