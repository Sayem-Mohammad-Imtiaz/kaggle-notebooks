{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Get Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom scipy.sparse import csr_matrix\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\n\nfrom nltk.corpus import stopwords\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport nltk\nimport string\nimport re\n\nfrom collections import Counter\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want to gain some info about data."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/spam.csv\", encoding='latin-1')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The 3 last columns seem to be useless, as they do not mostly contain a value and are null."},{"metadata":{"trusted":true},"cell_type":"code","source":"unimportant_col = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"]\nuseful_data = data.drop(unimportant_col, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's better to rename the columns names for better understanding."},{"metadata":{"trusted":true},"cell_type":"code","source":"useful_data = useful_data.rename(columns={\"v1\": \"Type\", \"v2\": \"Text\"})\nuseful_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create a Test Set"},{"metadata":{},"cell_type":"markdown","source":"At the beginning, we choose a part of data for test and we will put it aside. So, our computation will not depend on it."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, test_data = train_test_split(useful_data, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyze Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.groupby(\"Type\").describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.Type.value_counts().plot.pie();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add Length Feature"},{"metadata":{},"cell_type":"markdown","source":"Now, we will add new features and check if they are useful."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['Length'] = train_data['Text'].apply(len)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen below, ham messages are usually much shorter than the spam ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.hist(column='Length',by='Type',bins=60,figsize=(12,4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add UpperCase Count Feature"},{"metadata":{},"cell_type":"markdown","source":"We now want to analyze the number of words which are fully in uppercase."},{"metadata":{"trusted":true},"cell_type":"code","source":"findCapitalCount = lambda x: sum(map(str.isupper, x['Text'].split())) \ntrain_data[\"CapitalCount\"] = train_data.apply(findCapitalCount, axis=1)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This feature can be useful too.\n\nAs visualized below, spam messages are more likely to have these kind of words."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.hist(column='CapitalCount',by='Type',bins=30,figsize=(12,4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add Word Count Feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"findWordCount = lambda x: len(x['Text'].split()) \ntrain_data[\"WordCount\"] = train_data.apply(findWordCount, axis=1)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add UpperCase Rate Feature"},{"metadata":{},"cell_type":"markdown","source":"The number of words in general or the number of uppercase words may not be accurate separately and may not lead to preceise results. \n\nIt is better to get the ratio of the uppercase words count to all words."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[\"CapitalRate\"] = train_data[\"CapitalCount\"] / train_data[\"WordCount\"]\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.hist(column='CapitalRate',by='Type',bins=30,figsize=(12,4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Common Words"},{"metadata":{},"cell_type":"markdown","source":"Some words such as 'and', 'it', etc. are very common in english texts and can be misleading. We can delete them from our text."},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords.words(\"english\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stemmer"},{"metadata":{},"cell_type":"markdown","source":"It is better to analyze the root of words as they can come in different forms.\n\nStemmer can do the job for us!"},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = nltk.PorterStemmer()\nfor word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\n    print(word, \"=>\", stemmer.stem(word))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pipeline"},{"metadata":{},"cell_type":"markdown","source":"We need to define some transformers, in order to create a pipeline."},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attr):\n        self.attr_names = attr\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X = X.rename(columns={\"v1\": \"Type\", \"v2\": \"Text\"})\n        return X[self.attr_names]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As mentioned above, we can add useful attributes such as Length, and CapitalRate."},{"metadata":{"trusted":true},"cell_type":"code","source":"class AddExtraAttr(BaseEstimator, TransformerMixin):\n    def __init__(self, attr):\n        self.attr_name = attr\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X = X.values\n        X = pd.DataFrame(X, columns=[self.attr_name])\n        X['Length'] = X[self.attr_name].apply(len)\n#         findCapitalCount = lambda x: sum(map(str.isupper, x[self.attr_name].split())) \n#         X[\"CapitalCount\"] = X.apply(findCapitalCount, axis=1)\n#         findWordCount = lambda x: len(x[self.attr_name].split()) \n#         X[\"WordCount\"] = X.apply(findWordCount, axis=1)\n#         X[\"CapitalRate\"] = X[\"CapitalCount\"] / X[\"WordCount\"]\n        return X[[\"Length\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this step, we replace the sequence of integers with a NUMBER string. Then we seperate the text from punctuation for easier analysis. Then we need to eliminate the common english words from SMS content, as they can easily mislead us, and finally stem the words.\n\nNow we can count the words."},{"metadata":{"trusted":true},"cell_type":"code","source":"class FindCount(BaseEstimator, TransformerMixin):\n    def __init__(self, attr):\n        self.attr_name = attr\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        Y = []\n        for index, row in X.iterrows():\n            row[self.attr_name] = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', row[self.attr_name])\n            no_punc = [ch for ch in row[self.attr_name] if ch not in string.punctuation]\n            punc = [(\" \" + ch + \" \") for ch in row[self.attr_name] if ch in string.punctuation]\n            word_list = \"\".join(no_punc + punc).split()\n            useful_words = [word.lower() for word in word_list if word.lower() not in stopwords.words(\"english\")]\n            word_counts = Counter(useful_words)\n            stemmed_word_counts = Counter()\n            for word, count in word_counts.items():\n                stemmed_word = stemmer.stem(word)\n                stemmed_word_counts[stemmed_word] += count\n            word_counts = stemmed_word_counts\n            Y.append(word_counts)\n        return Y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we convert the count data to vector, and create a sparse matrix.\n\nThis will show us, the number of occurances of each word in each text."},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConvertToVector(BaseEstimator, TransformerMixin):\n    def __init__(self, vocabulary_size=1000):\n        self.vocabulary_size = vocabulary_size\n    def fit(self, X, y=None):\n        total_count = Counter()\n        for word_count in X:\n            for word, count in word_count.items():\n                total_count[word] += count\n        most_common = total_count.most_common()[:self.vocabulary_size]\n        self.most_common_ = most_common\n        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n        return self\n    def transform(self, X, y=None):\n        rows = []\n        cols = []\n        data = []\n        for row, word_count in enumerate(X):\n            for word, count in word_count.items():\n                rows.append(row)\n                cols.append(self.vocabulary_.get(word, 0))\n                data.append(count)\n        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the transformers above, we can make a pipeline to automate the process."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_attr = [\"Text\"]\n\nmain_pipeline = Pipeline([\n        ('selector', DataFrameSelector(cat_attr)),\n        ('find_count', FindCount(cat_attr[0])),\n        ('convert_to_vector', ConvertToVector()),\n    ])\n\nextra_pipeline = Pipeline([\n        ('selector', DataFrameSelector(cat_attr)),\n        ('add_extra', AddExtraAttr(cat_attr[0])),\n    ])\n\nfull_pipeline = FeatureUnion(transformer_list=[\n    ('main_pipeline', main_pipeline),\n    ('extra_pipeline', extra_pipeline),\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_prepared = full_pipeline.fit_transform(train_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Model"},{"metadata":{},"cell_type":"markdown","source":"As the types are \"spam\" and \"ham\", we will use LabelEncoder to encode these values to integers.\n\nThen we try a few models."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train_data[\"Type\"]\nencoder = LabelEncoder()\ntype_encoded = encoder.fit_transform(y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\nscore = cross_val_score(log_clf, train_prepared, type_encoded, cv=3, verbose=3)\nscore.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### SGD"},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd_clf = SGDClassifier(random_state=42)\nscore = cross_val_score(sgd_clf, train_prepared, type_encoded, cv=3, verbose=3)\nscore.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MultinomialNB"},{"metadata":{"trusted":true},"cell_type":"code","source":"mnb_clf = MultinomialNB()\nscore = cross_val_score(mnb_clf, train_prepared, type_encoded, cv=3, verbose=3)\nscore.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this step, we use the test data and analyze our recall and precision."},{"metadata":{},"cell_type":"markdown","source":"### BernoulliNB"},{"metadata":{"trusted":true},"cell_type":"code","source":"bnb_clf = BernoulliNB()\nscore = cross_val_score(bnb_clf, train_prepared, type_encoded, cv=3, verbose=3)\nscore.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\n\ny_test = test_data[\"Type\"]\nencoder = LabelEncoder()\ntype_encoded = encoder.fit_transform(y_test)\n\nX_test_transformed = full_pipeline.transform(test_data)\n\nbnb_clf = BernoulliNB()\nbnb_clf.fit(train_prepared, y_train)\n\ny_pred = bnb_clf.predict(X_test_transformed)\nencoder = LabelEncoder()\ny_pred = encoder.fit_transform(y_pred)\n\nprint(\"Precision BernoulliNB: {:.2f}%\".format(100 * precision_score(type_encoded, y_pred)))\nprint(\"Recall BernoulliNB: {:.2f}%\\n\".format(100 * recall_score(type_encoded, y_pred)))\n\nmnb_clf = MultinomialNB()\nmnb_clf.fit(train_prepared, y_train)\n\ny_pred = mnb_clf.predict(X_test_transformed)\nencoder = LabelEncoder()\ny_pred = encoder.fit_transform(y_pred)\n\nprint(\"Precision MultinomialNB: {:.2f}%\".format(100 * precision_score(type_encoded, y_pred)))\nprint(\"Recall MultinomialNB: {:.2f}%\\n\".format(100 * recall_score(type_encoded, y_pred)))\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(train_prepared, y_train)\n\ny_pred = sgd_clf.predict(X_test_transformed)\nencoder = LabelEncoder()\ny_pred = encoder.fit_transform(y_pred)\n\nprint(\"Precision SGD: {:.2f}%\".format(100 * precision_score(type_encoded, y_pred)))\nprint(\"Recall SGD: {:.2f}%\\n\".format(100 * recall_score(type_encoded, y_pred)))\n\nlog_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\nlog_clf.fit(train_prepared, y_train)\n\ny_pred = log_clf.predict(X_test_transformed)\nencoder = LabelEncoder()\ny_pred = encoder.fit_transform(y_pred)\n\nprint(\"Precision Logistic Regression: {:.2f}%\".format(100 * precision_score(type_encoded, y_pred)))\nprint(\"Recall Logistic Regression: {:.2f}%\".format(100 * recall_score(type_encoded, y_pred)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}