{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from os import listdir\nfrom os.path import isfile, join\nimport pandas as pd\nimport librosa\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom timeit import default_timer as timer\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sn\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras import regularizers, optimizers\nfrom tensorflow.keras.layers import Dense, Conv1D, Flatten, Activation, MaxPooling1D, Dropout\nfrom tensorflow.keras.utils import plot_model,to_categorical","metadata":{"id":"BbVepFu3S32R","execution":{"iopub.status.busy":"2021-06-22T13:21:25.695319Z","iopub.execute_input":"2021-06-22T13:21:25.696039Z","iopub.status.idle":"2021-06-22T13:21:34.517565Z","shell.execute_reply.started":"2021-06-22T13:21:25.695931Z","shell.execute_reply":"2021-06-22T13:21:34.516577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parsing the Dataset","metadata":{}},{"cell_type":"code","source":"class Diagnosis():\n  def __init__ (self, id, diagnosis, image_path):\n    self.id = id\n    self.diagnosis = diagnosis \n    self.image_path = image_path   ","metadata":{"id":"Q1bkcmLQ_6dZ","execution":{"iopub.status.busy":"2021-06-22T13:21:34.518868Z","iopub.execute_input":"2021-06-22T13:21:34.519456Z","iopub.status.idle":"2021-06-22T13:21:34.52411Z","shell.execute_reply.started":"2021-06-22T13:21:34.519418Z","shell.execute_reply":"2021-06-22T13:21:34.523359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_wav_files():\n  audio_path = '../input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files/'\n  files = [f for f in listdir(audio_path) if isfile(join(audio_path, f))]  #Gets all files in dir\n  wav_files = [f for f in files if f.endswith('.wav')]  # Gets wav files \n  wav_files = sorted(wav_files)\n  return wav_files, audio_path","metadata":{"id":"XEw8uPp301Nr","execution":{"iopub.status.busy":"2021-06-22T13:21:34.525751Z","iopub.execute_input":"2021-06-22T13:21:34.526276Z","iopub.status.idle":"2021-06-22T13:21:34.537821Z","shell.execute_reply.started":"2021-06-22T13:21:34.526244Z","shell.execute_reply":"2021-06-22T13:21:34.536832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def diagnosis_data():\n  diagnosis = pd.read_csv('../input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/patient_diagnosis.csv')\n  \n  wav_files, audio_path = get_wav_files()\n  diag_dict = { 101 : \"URTI\"}  \n  diagnosis_list = []\n  \n  for index , row in diagnosis.iterrows():\n    diag_dict[row[0]] = row[1]     \n\n  c = 0\n  for f in wav_files:\n    diagnosis_list.append(Diagnosis(c, diag_dict[int(f[:3])], audio_path+f))  \n    c+=1  \n\n  return diagnosis_list","metadata":{"id":"wRqqAOFZdz4r","execution":{"iopub.status.busy":"2021-06-22T13:21:34.5393Z","iopub.execute_input":"2021-06-22T13:21:34.539675Z","iopub.status.idle":"2021-06-22T13:21:34.550231Z","shell.execute_reply.started":"2021-06-22T13:21:34.539644Z","shell.execute_reply":"2021-06-22T13:21:34.54914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{}},{"cell_type":"code","source":"def audio_features(filename): \n  sound, sample_rate = librosa.load(filename)\n  #Преобразует данные в кратковременное преобразование Фурье.\n  stft = np.abs(librosa.stft(sound))\n\n  # (40)Мел-частотные кепстральные коэффициенты (MFCC)\n  #Представляют собой небольшой набор признаков (обычно около 10–20), которые кратко описывают \n  #общую форму спектральной огибающей, моделируют характеристики человеческого голоса.\n  mfccs = np.mean(librosa.feature.mfcc(y=sound, sr=sample_rate, n_mfcc=40),axis=1)\n  # (12)Вычисляет хромаграмму по форме волны или спектрограмме мощности.\n  chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate),axis=1)\n  # (128)Вычислияет спектрограмму в масштабе mel.\n  mel = np.mean(librosa.feature.melspectrogram(sound, sr=sample_rate),axis=1)\n  # (7)Вычисление спектрального контраста\n  contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate),axis=1)\n  # (6)Вычисляет признак тонального центроида\n  tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(sound), sr=sample_rate),axis=1)\n    \n  concat = np.concatenate((mfccs,chroma,mel,contrast,tonnetz))\n  return concat\n\ndef data_points():\n  labels = []\n  images = []\n\n  classes = {\"COPD\":0, \"Healthy\":1, \"URTI\":2, \"Bronchiectasis\":3, \"Pneumonia\":4, \"Bronchiolitis\":5, \"Asthma\":6, \"LRTI\":7}\n\n  for f in diagnosis_data():\n    labels.append(classes[f.diagnosis]) \n    images.append(audio_features(f.image_path))\n\n  return np.array(labels), np.array(images)","metadata":{"id":"85SsSiJrX9f6","execution":{"iopub.status.busy":"2021-06-22T13:21:34.551709Z","iopub.execute_input":"2021-06-22T13:21:34.552018Z","iopub.status.idle":"2021-06-22T13:21:34.563605Z","shell.execute_reply.started":"2021-06-22T13:21:34.55198Z","shell.execute_reply":"2021-06-22T13:21:34.562448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"def preprocessing(labels, images):    \n\n  # Remove Asthma and LRTI\n  images = np.delete(images, np.where((labels == 7) | (labels == 6))[0], axis=0) \n  labels = np.delete(labels, np.where((labels == 7) | (labels == 6))[0], axis=0)      \n\n  # Split data\n  X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=10)\n\n  # Encode the labels\n  y_train = to_categorical(y_train)\n  y_test = to_categorical(y_test)\n\n  # Format new data\n  y_train = np.reshape(y_train, (y_train.shape[0], 6))\n  X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n  y_test = np.reshape(y_test, (y_test.shape[0], 6))\n  X_test = np.reshape(X_test, (X_test.shape[0], X_train.shape[1],  1))\n\n  return X_train, X_test, y_train, y_test","metadata":{"id":"KRc768XTi7su","outputId":"cdf4fc15-73e8-4249-bb24-73cdc55939d5","execution":{"iopub.status.busy":"2021-06-22T13:21:34.56569Z","iopub.execute_input":"2021-06-22T13:21:34.566333Z","iopub.status.idle":"2021-06-22T13:21:34.582236Z","shell.execute_reply.started":"2021-06-22T13:21:34.566279Z","shell.execute_reply":"2021-06-22T13:21:34.581139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = timer()\n\nlabels, images = data_points()\nX_train, X_test, y_train, y_test = preprocessing(labels, images)\n\nprint('Time taken: ', (timer() - start))","metadata":{"id":"wsKxTYDTVGcK","outputId":"abbadb4c-86c6-4c4d-a57e-36895f1e9249","execution":{"iopub.status.busy":"2021-06-22T13:21:34.583781Z","iopub.execute_input":"2021-06-22T13:21:34.58412Z","iopub.status.idle":"2021-06-22T14:06:15.727206Z","shell.execute_reply.started":"2021-06-22T13:21:34.584087Z","shell.execute_reply":"2021-06-22T14:06:15.725044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-22T14:06:15.733143Z","iopub.execute_input":"2021-06-22T14:06:15.733724Z","iopub.status.idle":"2021-06-22T14:06:15.751445Z","shell.execute_reply.started":"2021-06-22T14:06:15.733669Z","shell.execute_reply":"2021-06-22T14:06:15.749971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Convolutional Neural Network ","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv1D(64, kernel_size=5, activation='relu', input_shape=(193, 1)))\n\nmodel.add(Conv1D(128, kernel_size=5, activation='relu'))\nmodel.add(MaxPooling1D(2)) \n\nmodel.add(Conv1D(256, kernel_size=5, activation='relu'))\nmodel.add(MaxPooling1D(2)) \n\nmodel.add(Dropout(0.3))\nmodel.add(Flatten())\n\nmodel.add(Dense(256, activation='relu'))   \nmodel.add(Dense(6, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=36, batch_size=64, verbose=1)","metadata":{"id":"Xwge0iUqTlmZ","outputId":"d2a61ca9-2476-4c6d-dad1-1ff0499d2994","execution":{"iopub.status.busy":"2021-06-22T14:06:15.753802Z","iopub.execute_input":"2021-06-22T14:06:15.754291Z","iopub.status.idle":"2021-06-22T14:07:12.642991Z","shell.execute_reply.started":"2021-06-22T14:06:15.754243Z","shell.execute_reply":"2021-06-22T14:07:12.642177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\n\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T14:07:12.644972Z","iopub.execute_input":"2021-06-22T14:07:12.645669Z","iopub.status.idle":"2021-06-22T14:07:13.285144Z","shell.execute_reply.started":"2021-06-22T14:07:12.645623Z","shell.execute_reply":"2021-06-22T14:07:13.283956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"score = model.evaluate(X_test, y_test, batch_size=60, verbose=0)\nprint('Accuracy: {0:.0%}'.format(score[1]/1))\nprint(\"Loss: %.4f\\n\" % score[0])\n\n# Plot accuracy and loss graphs\nplt.figure(figsize = (15,5))\nplt.subplot(1,2,1)\nplt.title('Accuracy')\nplt.plot(history.history['accuracy'], label = 'training acc')\nplt.plot(history.history['val_accuracy'], label = 'validation acc')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.title('Loss')\nplt.plot(history.history['loss'], label = 'training loss')\nplt.plot(history.history['val_loss'], label = 'validation loss')\nplt.legend()","metadata":{"id":"y5aGCpC2N4XV","outputId":"ffc1fa85-ce74-4d62-dcfd-73c6883ea0c1","execution":{"iopub.status.busy":"2021-06-22T14:07:13.286542Z","iopub.execute_input":"2021-06-22T14:07:13.286961Z","iopub.status.idle":"2021-06-22T14:07:13.855772Z","shell.execute_reply.started":"2021-06-22T14:07:13.286837Z","shell.execute_reply":"2021-06-22T14:07:13.854649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix_index = [\"COPD\", \"Healthy\", \"URTI\", \"Bronchiectasis\", \"Pneumoina\", \"Bronchiolitis\"]\n\npreds = model.predict(X_test)\nclasspreds = np.argmax(preds, axis=1) # predicted classes \ny_testclass = np.argmax(y_test, axis=1) # true classes\n\ncm = confusion_matrix(y_testclass, classpreds)\nprint(classification_report(y_testclass, classpreds, target_names=matrix_index))\n\n# Get percentage value for each element of the matrix\ncm_sum = np.sum(cm, axis=1, keepdims=True)\ncm_perc = cm / cm_sum.astype(float) * 100\nannot = np.empty_like(cm).astype(str)\nnrows, ncols = cm.shape\nfor i in range(nrows):\n    for j in range(ncols):\n        c = cm[i, j]\n        p = cm_perc[i, j]\n        if i == j:\n            s = cm_sum[i]\n            annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n        elif c == 0:\n            annot[i, j] = ''\n        else:\n            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n\n\n# Display confusion matrix \ndf_cm = pd.DataFrame(cm, index = matrix_index, columns = matrix_index)\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nfig, ax = plt.subplots(figsize=(10,7))\nsn.heatmap(df_cm, annot=annot, fmt='')","metadata":{"id":"j6ZgLoJAwXOM","outputId":"65e08504-c137-4bae-9fea-235044ef018b","execution":{"iopub.status.busy":"2021-06-22T14:07:13.857705Z","iopub.execute_input":"2021-06-22T14:07:13.858188Z","iopub.status.idle":"2021-06-22T14:07:14.537878Z","shell.execute_reply.started":"2021-06-22T14:07:13.858134Z","shell.execute_reply":"2021-06-22T14:07:14.536817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"diagnosis_df = pd.read_csv('../input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/patient_diagnosis.csv', names=['Patient number', 'Diagnosis'])\ndiagnosis_df.head(8)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T14:07:14.539303Z","iopub.execute_input":"2021-06-22T14:07:14.539634Z","iopub.status.idle":"2021-06-22T14:07:14.576337Z","shell.execute_reply.started":"2021-06-22T14:07:14.539599Z","shell.execute_reply":"2021-06-22T14:07:14.575526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsn.countplot(diagnosis_df['Diagnosis'])","metadata":{"execution":{"iopub.status.busy":"2021-06-22T14:07:14.577563Z","iopub.execute_input":"2021-06-22T14:07:14.578121Z","iopub.status.idle":"2021-06-22T14:07:14.76172Z","shell.execute_reply.started":"2021-06-22T14:07:14.578076Z","shell.execute_reply":"2021-06-22T14:07:14.760592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}