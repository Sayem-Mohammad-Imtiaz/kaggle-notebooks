{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# install external library for text augmentation\n!pip install nlpaug\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# plotting libraries\nimport seaborn as sn \nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go # Plotly for interactive data visualizatoins\n\nimport re # regex for text cleaning\nimport nltk # natural language processing tool kit\nimport pickle # for saving/serializing machine learning models\nimport nlpaug.augmenter.word as naw # text augmentor\n# neural networks and embeddings stuff\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Dropout, Activation\nfrom keras.layers.embeddings import Embedding\n\nfrom nltk.stem import WordNetLemmatizer # lemmatizing tool\nfrom sklearn.feature_extraction.text import TfidfVectorizer # tfidf for text representation\nfrom sklearn.model_selection import train_test_split # for data splitting\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score # for classifier evaluation\nfrom sklearn.ensemble import RandomForestClassifier # random forest machine learning model\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# read csv file into a dataframe\nraw_tweets=pd.read_csv('/kaggle/input/twitter-airline-sentiment/Tweets.csv')\n# show sample of data\nraw_tweets.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Exploratory Analysis**"},{"metadata":{},"cell_type":"markdown","source":"When we notice the nature of our dataset , that most of features are categorical features , \nso i think that par and pie plots should be a very good visualization techinques for our dataset EDA\n"},{"metadata":{},"cell_type":"markdown","source":"**Stacked par chart to show the distribution of reviews per company**"},{"metadata":{"trusted":true},"cell_type":"code","source":"crosstab_sentiments=pd.crosstab(raw_tweets.airline, raw_tweets.airline_sentiment)\ncompanies=list(crosstab_sentiments.index)\n\nfig = go.Figure(data=[\n    go.Bar(name=col_name, x=companies, y=list(crosstab_sentiments[col_name]))\nfor col_name in list(crosstab_sentiments.columns)])\n# Change the bar mode\nfig.update_layout(barmode='stack',\n                  title='Sentiment distribution per company',\n                  yaxis=dict(title='Sentiment distribution'),\n                 xaxis=dict(title='Companies'))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we see that most of reviews are negative for most of the companies , to help these companies take better decesions we need to focus on negative reviews\n\n\n**Stacked par chart to show negative reasons distributions per company**"},{"metadata":{"trusted":true},"cell_type":"code","source":"crosstab_neg_reasons=pd.crosstab(raw_tweets.airline,raw_tweets.negativereason)\ncompanies=list(crosstab_neg_reasons.index)\n\nfig = go.Figure(data=[\n    go.Bar(name=col_name, x=companies, y=list(crosstab_neg_reasons[col_name]))\nfor col_name in list(crosstab_neg_reasons.columns)])\n# Change the bar mode\nfig.update_layout(barmode='stack',\n                  title='Negative reasons distribution per company',\n                  yaxis=dict(title='Negative reasons distribution'),\n                 xaxis=dict(title='Companies'))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pie plot to check the overall distribution for negative reasons**"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = list(crosstab_neg_reasons.columns)\nvalues = [crosstab_neg_reasons[col_name].sum() for col_name in labels]\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])\nfig.update_layout(title='Overall distribution for negative reasons')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Par plot to show sentiment class distributions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(\n    [go.Bar(x=list(raw_tweets['airline_sentiment'].unique()),\n            y=raw_tweets['airline_sentiment'].value_counts())]\n)\n# Change the bar mode\nfig.update_layout(\n                  title='Sentiment distribution',\n                  yaxis=dict(title='Count'),\n                 xaxis=dict(title='Sentiment'))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from previous plot , we notice that the dataset may suffer from class imbalance problem , we may try to oversample the minority classes using text augmentation technique , nlpaug is very helpful tool , I will use it to oversample neutral and positive classes , note : nlpaug does not blindly copies data , but it apply random word variations with the same meaninig (high cosine similarity)\ncheck text augmentation section below after data cleaning and stemming"},{"metadata":{},"cell_type":"markdown","source":"Now we shall start second part of the problem , we need to build an efficient text / sentiment classifier to predict customer review sentiment"},{"metadata":{},"cell_type":"markdown","source":"**Text cleaning and preprocessing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(dataframe):\n    '''\n    function to clean tweets dataframe text , eg : remove white spaces , special chars and website urls\n    '''\n    # select nedded columns\n    dataframe=dataframe[['airline_sentiment','text']]\n    # drop nans\n    dataframe=dataframe.dropna()\n    # convert review into lower case\n    dataframe['text']=dataframe['text'].apply(lambda row : row.lower())\n    # remove numbers\n    dataframe['text']=dataframe['text'].apply(lambda row : re.sub('\\d+','',row))\n    # remove tweet account name\n    dataframe['text']=dataframe['text'].apply(lambda row:re.sub(r'@\\w+', '',row))\n    # remove website urls\n    dataframe['text']=dataframe['text'].apply(lambda row:re.sub(r'http\\S+', '',row))\n    # rwmove special characters\n    dataframe['text']=dataframe['text'].apply(lambda row:re.sub(r\"[^A-Za-z0-9']+\", ' ',row))\n    # remove white spaces\n    dataframe['text']=dataframe['text'].apply(lambda row:re.sub(r'\\s+', ' ',row))\n    \n    return dataframe\n\ntraindf=clean_text(raw_tweets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lem = WordNetLemmatizer()\ndef stem_review(review):\n    return ' '.join([lem.lemmatize(word) for word in review.split(' ')])\ndef normalize_text(dataframe):\n    '''\n    function used for rooting / stemming tokens\n    '''\n    dataframe['text']=dataframe['text'].apply(lambda x : stem_review(x))\n\nnormalize_text(traindf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we will build a classifier using classical machine learning algorithms , then we will use advanced deep learning techniques , from my experience with text classifiers the combination between Tfidfvectorizer with Random forest algorithm has never failed me :)"},{"metadata":{},"cell_type":"markdown","source":"**Text augmentation**\n\nto solve class imbalance problem we shall augment/oversample minor classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"def augment_text(dataframe):\n    '''\n    function used to augment minor classes in tweets dataset\n    '''\n    augmented_df = pd.DataFrame(columns=['text', 'airline_sentiment'])\n    augmentor=naw.WordNetAug(aug_p=0.5)\n    aug_sent={'neutral':3,\n             'positive':4}\n    for i in range(len(dataframe)):\n        current_label=dataframe['airline_sentiment'].iloc[i]\n        if current_label in list(aug_sent.keys()):\n            current_text=dataframe['text'].iloc[i]\n            for j in range(aug_sent[current_label]):\n                text = augmentor.augment(current_text)\n                label = current_label\n                tempdf = pd.DataFrame(list(zip([text], [label])), columns=['text', 'airline_sentiment'])\n                augmented_df = augmented_df.append(tempdf)\n        else :\n                text = dataframe['text'].iloc[i]\n                label = current_label\n                tempdf = pd.DataFrame(list(zip([text], [label])), columns=['text', 'airline_sentiment'])\n                augmented_df = augmented_df.append(tempdf)\n    return augmented_df\naugmented_traindf=augment_text(traindf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Text Representation**\n\ni will use TfidfVectorizer for sentiment vectorization , from many experiments i have done before with sentiment/reviews datasets , below configuration gives good results "},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert text into numeric values using tf-idf vectorizer\ntfidfconverter = TfidfVectorizer(\n                                 min_df=5, \n                                 max_df=0.7,\n                                 ngram_range=(1,2),\n                                 stop_words='english'\n                                )  \nX = tfidfconverter.fit_transform(augmented_traindf['text']).toarray()\ny = augmented_traindf['airline_sentiment'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model training**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\ntext_classifier = RandomForestClassifier(n_estimators=400,n_jobs=-1,verbose=True)\ntext_classifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model evaluation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model testing\npredictions = text_classifier.predict(X_test)\nprint(\"Classification Report :\\n {} \\n Model Acurracy = {}\".format(classification_report(y_test,predictions),\n                                                                 accuracy_score(y_test, predictions)))\n# confusion matrix\nprint(\"\\n Confusion Matrix\")\ndf_cm = pd.DataFrame(confusion_matrix(y_test, predictions),['negative','neutral','positive'],['negative','neutral','positive'])\nsn.set(font_scale=1.2)#for label size\nsn.heatmap(df_cm, annot=True,annot_kws={\"size\": 20})\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the model shows good performance in the evaluation stage , so we may save it to a use as a base model for future enhancements"},{"metadata":{"trusted":true},"cell_type":"code","source":"pkl_filename = \"classifier_random_forest.pkl\"  \nwith open(pkl_filename, 'wb') as file:  \n    pickle.dump(text_classifier, file)\nprint(\"Model Saved\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Classification using Deep learning techniques**\n\n\nwe will train an **embedding layer followed by dense layer of 3 neurons with batch size =32 for 4 epochs** , i choosed this arhitecture to keep the network simple as i tried adding more dense layers and the model was training for 10 epochs but the model has symptoms of overfitting also the train and validation error curves were increasing after 4 epochs ."},{"metadata":{},"cell_type":"markdown","source":"train an embedding layer with Fully connected neural network"},{"metadata":{"trusted":true},"cell_type":"code","source":"ycat=pd.get_dummies(augmented_traindf['airline_sentiment']).values\nX=augmented_traindf['text'].values\ntk = Tokenizer()\ntk.fit_on_texts(X)\nX_seq = tk.texts_to_sequences(X)\nX_pad = pad_sequences(X_seq, maxlen=100, padding='post')\nX_train, X_test, y_train, y_test = train_test_split(X_pad, ycat, test_size = 0.25, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocabulary_size = len(tk.word_counts.keys())+1\nmax_words = 100\nembedding_size = 32\nmodel = Sequential()\nmodel.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\nmodel.add(Flatten())\nmodel.add(Dense(3, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(X_train,y_train,validation_data=(X_test,y_test),batch_size=32,epochs=4,verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"visualizing model behavior during training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model testing\npredictions = [np.argmax(i) for i in model.predict(X_test)]\ny_test=[np.argmax(i) for i in y_test]\nprint(\"Classification Report :\\n {} \\n Model Acurracy = {}\".format(classification_report(y_test,predictions,\n                                                                                         target_names=['negative','neutral','positive']),\n                                                                 accuracy_score(y_test, predictions)))\n# confusion matrix\nprint(\"\\n Confusion Matrix\")\ndf_cm = pd.DataFrame(confusion_matrix(y_test, predictions),['negative','neutral','positive'],['negative','neutral','positive'])\nsn.set(font_scale=1.2)#for label size\nsn.heatmap(df_cm, annot=True,annot_kws={\"size\": 20})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the model shows good performance in the evaluation stage , so we may save it to a use as a base model for future enhancements"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('text_classifier_neural_network.h5')\nprint('model_saved')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"finally as it was predicted , training an embedding layer with fully connected neural network (deep learning methods) shows better performance than conventional text vectorization techniques and machine learning algorithms"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}