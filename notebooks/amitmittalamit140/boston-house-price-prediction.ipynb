{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Boston House Price Prediction"},{"metadata":{},"cell_type":"markdown","source":"*In this project we are going to use Machine Learning to predict the house prices of city named Boston in US.*\n\n*The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970.*\n\n*There are several features given for a house and we have to predicts its value as accurate as possible.*"},{"metadata":{},"cell_type":"markdown","source":"# 0. Overview\n\nBelow is the overview of the whole project, what all things we will be doing, step wise.\n\n\n- 1. Importing Libraries\n\n\n- 2. Exploring Dataset\n    - 2.1. We will be importing the dataset using Pandas library.\n    - 2.2. Finding variables which are useful for prediction.\n\n\n- 3. Univariate and Multivariate Analysis  \n    - 3.1 MEDV\n    - 3.2 TAX\n    - 3.3 PTRATIO\n    - 3.4 LSTAT\n    - 3.5 RM\n\n\n- 4. Splitting Dataset into Train and Test Set\n\n\n- 5. Multiple Linear Regression\n    - 5.1 Model Prepration\n    - 5.2 Model Evaluation\n    - 5.3 Model Interpretation\n\n\n- 6. Decision Tree\n    - 6.1 Model Prepration\n    - 6.2 Model Interpretation\n\n\n- 7. Random Forest\n    - 7.1 Model Prepration\n    - 7.2 Model Interpretation\n\n\n- 8. Conclusion\n---"},{"metadata":{},"cell_type":"markdown","source":"# 1. Importing Libraries"},{"metadata":{},"cell_type":"markdown","source":"First we are importing all the important libraries we are going to use in this project and if we need any other library, we will import it at that time only."},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings #to remove warning from the notebook\nwarnings.filterwarnings(action='ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Exploring Dataset"},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Loading Dataset"},{"metadata":{},"cell_type":"markdown","source":"Here we are going to import our **Boston House Price** dataset and will see how it looks o_o"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#loading dataset\nname= ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndf = pd.read_csv(filepath_or_buffer=\"../input/boston-house-prices/housing.csv\",delim_whitespace=True,names=name)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Boston House Price** dataset has 14 features and their description is given as follows:\n- CRIM     per capita crime rate by town\n- ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n- INDUS    proportion of non-retail business acres per town\n- CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n- NOX      nitric oxides concentration (parts per 10 million)\n- RM       average number of rooms per dwelling\n- AGE      proportion of owner-occupied units built prior to 1940\n- DIS      weighted distances to five Boston employment centres\n- RAD      index of accessibility to radial highways\n- TAX      full-value property-tax rate per dollar 10,000.\n- PTRATIO  pupil-teacher ratio by town\n- B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n- LSTAT    % lower status of the population\n- MEDV     Median value of owner-occupied homes in $1000's\n\nHere main thing to notice is that **MEDV** is the outcome variable which we need to predict and all other variables are predictor variables."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#shape of our dataset\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This data set has 14 features and 506 rows i.e. details of 506 houses."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#information about the data\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that all features in the dataset are numeric type either float or int. There is no categorical variable, which makes our life little easier here :)"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#checking for missing data\ndf.isnull().sum()\n#there is no missing value in the data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We noticed that there are *No Missing* values in the dataset which again reduced our work load. Cheers!"},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Finding variables which are useful for prediction"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.heatmap(data=df.corr().round(2),annot=True,cmap='coolwarm',linewidths=0.2,square=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Big colorful picture above which is called *Heatmap* helps us to understand how features are correlated to each other.\n- Postive sign implies postive correlation between two features whereas Negative sign implies negative correlation between two features.\n\n\n- I am here interested to know which features have good correlation with our dependent variable MEDV and can help in having good predictions.\n\n\n- I observed that INDUS, RM, TAX, PTRATIO and LSTAT shows some good correaltion with MEDV and I am interested to know more about them.\n\n\n- However I noticed that INDUS shows good correlation with TAX and LSAT which is a pain point for us :(\n  \n  because it leads to **Multicollinearity**. So I decided NOT to consider this feature and do further analysis with other 5 remaining features."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#since some of these features shows quite good and very good correlation with our predictive variable Houese Price(MEDV)\ndf1 = df[['RM','TAX','PTRATIO','LSTAT','MEDV']]\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have created a new dataset consisting of only those variables which we selected after analysing Heatmap."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"sns.pairplot(data=df1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These 5x5 figures above helps us to understand how data in each variable (feature) is distributed with itself and with others.\n\n**Observations**\n- As we can see that RM, LSTAT and MEDV are quite normally distributed.\n\n\n- Also we can see that RM and LSTAT shows kind of good Linear relationship with MEDV.\n\n\n- There seems to have presence of some outliers in the dataset, we will study about them in some time."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#description about data\ndesc = df1.describe().round(2)\ndesc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above table displays measures of central tendency like Mean, Median (50%) etc. We can see number of entries for each variable which is same as 506.\n\n**Observations**\n- Maximum value in MEDV and LSTAT are much higher than 75% of data points, which is kind of alarming situtaion for me.\n\n\n- We will study each of the feature seprately and see how data is distributed and if there are any outliers or not."},{"metadata":{},"cell_type":"markdown","source":"# 3. Univariate and Multivariate Analysis"},{"metadata":{},"cell_type":"markdown","source":"## 3.1 MEDV"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Box Plot and Distribution Plot for Dependent variable MEDV\nplt.figure(figsize=(20,3))\n\nplt.subplot(1,2,1)\nsns.boxplot(df1.MEDV,color='#005030')\nplt.title('Box Plot of MEDV')\n\nplt.subplot(1,2,2)\nsns.distplot(a=df1.MEDV,color='#500050')\nplt.title('Distribution Plot of MEDV')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above two figures we can see observe that:\n- MEDV is normally distributed\n- It contains some extreme values which could be potential outliers\n\nNext we are going to observe data points which lies outside wiskers.\n\n*Q3 + 1.5 * IQR*  <  **Potential Outliers**  <  *Q1 - 1.5 * IQR*\n- Q3 -> Quartile 3, Under which 75% of data lies\n- Q1 -> Quartile 1, Under which 25% of data lies\n- IQR -> Inter-Quartile Range, Q3 - Q1"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"MEDV_Q3 = desc['MEDV']['75%']\nMEDV_Q1 = desc['MEDV']['25%']\nMEDV_IQR = MEDV_Q3 - MEDV_Q1\nMEDV_UV = MEDV_Q3 + 1.5*MEDV_IQR\nMEDV_LV = MEDV_Q1 - 1.5*MEDV_IQR\n\ndf1[df1['MEDV']<MEDV_LV]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**\n- For these two low house prices, we can see that TAX = 666 which is very high for a house with approx 5 rooms.\n- For these two low house prices, we can see that LSTAT is also  high.\n\n**Conclusion:**\n- Since both TAX and LSTAT are negatively correlated to MEDV which means higher the TAX and LSTAT lower will be the house price and vica-versa.\n- I find it meaningful to have such low house prices.\n- Therefore, I will keep these data points."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df1[df1['MEDV']>MEDV_UV].sort_values(by=['MEDV','RM'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**\n- For house prices = 50, it is observed that number of Room ranges from 5 to 9 (approx.) which is quite unusual.\n- Also for these houses TAX ranges from low to high.\n- For houses price between 37 to less than 50, RM is higher than 75% of the total data points. Since RM is positively correlated to MEDV, so this could be reason for little higher house prices.\n- Also for these houses PTRATIO and LSTAT lies in 25% - 50% of the total observation respectively. Since PTRATIO and LSAT are negatively correlated to MEDV so this could be reason for little higher house prices.\n\n**Conclusion:**\n- I am going to DROP ALL entries whose MEDV = 50 because I feel these entries are outliers and can create problem in having good predicitions.\n- I am going to keep all entries having MEDV between 37 to less than 50, since I could not observe any unusual behaviour for them."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(f'Shape of dataset before remving Outliers: {df1.shape}')\ndf2 = df1[~(df1['MEDV']==50)]\nprint(f'Shape of dataset after remving Outliers: {df2.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that we have deleted 16 rows from out dataset having MEDV = 50"},{"metadata":{},"cell_type":"markdown","source":"## 3.2 TAX"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Box Plot, Distribution Plot and Scatter Plot for TAX\nplt.figure(figsize=(20,3))\n\nplt.subplot(1,3,1)\nsns.boxplot(df2.TAX,color='#005030')\nplt.title('Box Plot of TAX')\n\nplt.subplot(1,3,2)\nsns.distplot(a=df2.TAX,color='#500050')\nplt.title('Distribution Plot of TAX')\n\nplt.subplot(1,3,3)\nsns.scatterplot(df2.TAX,df2.MEDV)\nplt.title('Scatter Plot of TAX vs MEDV')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above three figures we can observe that:\n- TAX is NOT normally distributed\n- Though Boxplot does not show any outlier but there are some extreme TAX values in the dataset which is bothering me.\n- Also from the scatter plot we can observe that for these extreme TAX values, MEDV ranges from low to high."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"temp_df = df2[df1['TAX']>600].sort_values(by=['RM','MEDV'])\ntemp_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are total 132 entries in TAX mostly having value 666 which I thinks is a *DEVIL'S* number. Now lets deep dive inside them."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"temp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**\n- RM for these entries lies between 3.5 to 8.78.\n- PTRATIO for almost all of these entries is same and equal to 20.20.\n- LSTAT for these entries lies between 2.96 to 37.97.\n- MEDV for these entries lies between 5 to 29.80.\n- All these observations are very unusual, it seems impossible to have such high TAX values for all these houses.\n- These values most likely missing values which were imputed casually by someone.\n\n**Conclusion:**\n- Since LSTAT is most correlated to TAX as seen above in Heatmap, so I am going to replace those 132 TAX values with mean of remaining TAX values dividing in some intervals with the help of LSTAT.\n- Interval 1: TAX_10 -> Replacing extreme TAX values having LSTAT is between 0 to 10 with mean of other TAX values whose LSTAT is between 0 to 10.\n- Interval 2: TAX_20 -> Replacing extreme TAX values having LSTAT is between 10 to 20 with mean of other TAX values whose LSTAT is between 10 to 20.\n- Interval 3: TAX_30 -> Replacing extreme TAX values having LSTAT is between 20 to 30 with mean of other TAX values whose LSTAT is between 20 to 30.\n- Interval 4: TAX_40 -> Replacing extreme TAX values having LSTAT >= 30 with mean of other TAX values whose LSTAT >= 30."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"TAX_10 = df2[(df2['TAX']<600) & (df2['LSTAT']>=0) & (df2['LSTAT']<10)]['TAX'].mean()\nTAX_20 = df2[(df2['TAX']<600) & (df2['LSTAT']>=10) & (df2['LSTAT']<20)]['TAX'].mean()\nTAX_30 = df2[(df2['TAX']<600) & (df2['LSTAT']>=20) & (df2['LSTAT']<30)]['TAX'].mean()\nTAX_40 = df2[(df2['TAX']<600) & (df2['LSTAT']>=30)]['TAX'].mean()\n\nindexes = list(df2.index)\nfor i in indexes:\n    if df2['TAX'][i] > 600:\n        if (0 <= df2['LSTAT'][i] < 10):\n            df2.at[i,'TAX'] = TAX_10\n        elif (10 <= df2['LSTAT'][i] < 20):\n            df2.at[i,'TAX'] = TAX_20\n        elif (20 <= df2['LSTAT'][i] < 30):\n            df2.at[i,'TAX'] = TAX_30\n        elif (df2['LSTAT'][i] >30):\n            df2.at[i,'TAX'] = TAX_40\n\nprint('Values imputed successfully')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This show all those extreme TAX values are replaced successfully\ndf2[df2['TAX']>600]['TAX'].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows that those values are replaced succesfully :)"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.distplot(a=df2.TAX,color='#500050')\nplt.title('Distribution Plot of TAX after replacing extreme values')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3 PTRATIO"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Box Plot, Distribution Plot and Scatter Plot for PTRATIO\nplt.figure(figsize=(20,3))\n\nplt.subplot(1,3,1)\nsns.boxplot(df2.PTRATIO,color='#005030')\nplt.title('Box Plot of PTRATIO')\n\nplt.subplot(1,3,2)\nsns.distplot(a=df2.PTRATIO,color='#500050')\nplt.title('Distribution Plot of PTRATIO')\n\nplt.subplot(1,3,3)\nsns.scatterplot(df2.PTRATIO,df2.MEDV)\nplt.title('Scatter Plot of PTRATIO vs MEDV')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above three figures we can observe that:\n- PTRATIO is NOT normally distributed\n- There are few low PRATIO values in the dataset which is bothering me."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df2[df2['PTRATIO']<14].sort_values(by=['LSTAT','MEDV'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**\n- PTRATIO for all above data points is same.\n- RM and MEDV is increasing simultaneously, as RM and MEDV are positively correlated, which is fine.\n- As LSTAT increases MEDV decreases, which follows negative correlation.\n\n**Conclusion:**\n- I don't observe any unusual behaviour for these data points. Therefore, I will keep them."},{"metadata":{},"cell_type":"markdown","source":"## 3.4 LSTAT"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#Box Plot, Distribution Plot and Scatter Plot for LSTAT\nplt.figure(figsize=(20,3))\n\nplt.subplot(1,3,1)\nsns.boxplot(df2.LSTAT,color='#005030')\nplt.title('Box Plot of LSTAT')\n\nplt.subplot(1,3,2)\nsns.distplot(a=df2.LSTAT,color='#500050')\nplt.title('Distribution Plot of LSTAT')\n\nplt.subplot(1,3,3)\nsns.scatterplot(df2.LSTAT,df2.MEDV)\nplt.title('Scatter Plot of LSTAT vs MEDV')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above three figures we can observe that:\n- LSTAT is  normally distributed and skewed to right.\n- There are some high LSTAT values in the dataset which we will analyse."},{"metadata":{"trusted":true},"cell_type":"code","source":"LSTAT_Q3 = desc['LSTAT']['75%']\nLSTAT_Q1 = desc['LSTAT']['25%']\nLSTAT_IQR = LSTAT_Q3 - LSTAT_Q1\nLSTAT_UV = LSTAT_Q3 + 1.5*LSTAT_IQR\nLSTAT_LV = LSTAT_Q1 - 1.5*LSTAT_IQR\n\ndf2[df2['LSTAT']>LSTAT_UV].sort_values(by='LSTAT')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**\n- From above data, I observed that since LSAT value for these 7 houses is high resulting in low MEDV, which follows the negative correaltion and is True.\n- RM is low and TAX is little higher which means low MEDV and which is True.\n\n**Conclusion:**\n- I don't find any strong reason  to exclude these data points. Therefore, I will keep this data also for our model"},{"metadata":{},"cell_type":"markdown","source":"## 3.5 RM"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#Box Plot, Distribution Plot and Scatter Plot for RM\nplt.figure(figsize=(20,3))\n\nplt.subplot(1,3,1)\nsns.boxplot(df2.RM,color='#005030')\nplt.title('Box Plot of MEDV')\n\nplt.subplot(1,3,2)\nsns.distplot(a=df2.RM,color='#500050')\nplt.title('Distribution Plot of MEDV')\n\nplt.subplot(1,3,3)\nsns.scatterplot(df2.RM,df2.MEDV)\nplt.title('Scatter Plot of RM vs MEDV')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above three figures we can observe that:\n- RM is normally distributed .\n- There are some low and high RM values in the dataset which we will analyse.\n- Scatter plot of RM vs MEDV show good Positive Linear Relationship."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"RM_Q3 = desc['RM']['75%']\nRM_Q1 = desc['RM']['25%']\nRM_IQR = RM_Q3 - RM_Q1\nRM_UV = RM_Q3 + 1.5*RM_IQR\nRM_LV = RM_Q1 - 1.5*RM_IQR\n\ndf2[df2['RM']<RM_LV].sort_values(by=['RM','MEDV'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**\n- I am more concerned about two data points (row index - 365 & 367) where MEDV is higher while RM is very low, though RM and MEDV are positively correlated.\n- Also for these two data points TAX and PTRATIO are above 50% of data points respectively, though both are negatively correlated to MEDV.\n- For rest data points, I don't see any unusual behaviour.\n\n**Conclusion:**\n- I am going to delete those two data points (row index - 365 & 367) as it may influence the prediction capability of our model.\n- Also I am going to keep all other points."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(f'Shape of dataset before removing data points: {df2.shape}')\ndf3 = df2.drop(axis=0,index=[365,367])\nprint(f'Shape of dataset before removing data points: {df3.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see in the difference of shape of dataset after removing two data points (outliers)."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df3[df3['RM']>RM_UV].sort_values(by=['RM','MEDV'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**\n- In the above data points, I am more concerned about one data point only (row index - 364) where MEDV is very low while RM is very high, though RM and MEDV are positively correlated.\n- Also for this data point LSTAT is low and MEDV is also low, though both are negatively correlated.\n- For rest data points, I don't see any unusual behaviour.\n\n**Conclusion:**\n- I am going to delete the data point (row index - 364) as I believe this could be human error while inputting the data.\n- Also I am going to keep all other points."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(f'Shape of dataset before removing data points: {df3.shape}')\ndf3 = df3.drop(axis=0,index=[364])\nprint(f'Shape of dataset before removing data points: {df3.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see in the difference of shape of dataset after removing one data point (outlier).\n\n---\n\nNow, we are done with univariate and multivariate analysis and I feel data is ready to put into the **Black Box** i.e. model.\n\nBut before doing that we need to split our data into Training set and Test set and then we will make our model on Training set and test its accracy on Test set."},{"metadata":{},"cell_type":"markdown","source":"## 4. Splitting Dataset into Train and Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now will split our dataset into Dependent variable and Independent variable\n\nX = df3.iloc[:,0:4].values\ny = df3.iloc[:,-1:].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, we have divided our data into two sets:\n\n**X** contains all independent variables\n\n**y** contains independent variable MEDV"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(f\"Shape of Dependent Variable X = {X.shape}\")\nprint(f\"Shape of Independent Variable y = {y.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def FeatureScaling(X):\n    \"\"\"\n    is function takes an array as an input, which needs to be scaled down.\n    Apply Standardization technique to it and scale down the features with mean = 0 and standard deviation = 1\n    \n    Input <- 2 dimensional numpy array\n    Returns -> Numpy array after applying Feature Scaling\n    \"\"\"\n    mean = np.mean(X,axis=0)\n    std = np.std(X,axis=0)\n    for i in range(X.shape[1]):\n        X[:,i] = (X[:,i]-mean[i])/std[i]\n\n    return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Feature Scaling is a technique to standardize the independent features present in the data in a fixed range. \n\n\n- Few advantages of Feature Scaling the data are as follows:\n    - It makes training of model faster.\n    - It prevents the model from getting stuck in local optima.\n\n\n- Here, we are using Standard Scalar which will scale Independent variables such that distribution is now centred around 0, with a Standard Deviation of 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = FeatureScaling(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set of Independent variables X is now scaled down."},{"metadata":{"trusted":true},"cell_type":"code","source":"m,n = X.shape\nX = np.append(arr=np.ones((m,1)),values=X,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to add a variable for **Bias** also. So, we are adding a new column of 1's in X as the fist column. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we will spit our data into Train set and Test Set\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state = 42)\n\nprint(f\"Shape of X_train = {X_train.shape}\")\nprint(f\"Shape of X_test = {X_test.shape}\")\nprint(f\"Shape of y_train = {y_train.shape}\")\nprint(f\"Shape of y_test = {y_test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that we have split the data into Training Set (80% of total data) and Test Set (20% of total data)"},{"metadata":{},"cell_type":"markdown","source":"# 5. Multiple Linear Regression\n##### Here we are building Multiple Linear Regression from Scratch"},{"metadata":{},"cell_type":"markdown","source":"## 5.1 Model Prepration"},{"metadata":{"trusted":true},"cell_type":"code","source":"#ComputeCost function determines the cost (sum of squared errors) \n\ndef ComputeCost(X,y,theta):\n    \"\"\"\n    This function takes three inputs and uses the Cost Function to determine the cost (basically error of prediction vs\n    actual values)\n    Cost Function: Sum of square of error in predicted values divided by number of data points in the set\n    J = 1/(2*m) *  Summation(Square(Predicted values - Actual values))\n    \n    Input <- Take three numoy array X,y and theta\n    Return -> The cost calculated from the Cost Function\n    \"\"\"\n    m=X.shape[0] #number of data points in the set\n    J = (1/(2*m)) * np.sum((X.dot(theta) - y)**2)\n    return J","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the function which computes the Cost of sum of squared errors of our Multiple Linear Regression function."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Gradient Descent Algorithm to minimize the Cost and find best parameters in order to get best line for our dataset\n\ndef GradientDescent(X,y,theta,alpha,no_of_iters):\n    \"\"\"\n    Gradient Descent Algorithm to minimize the Cost\n    \n    Input <- X, y and theta are numpy arrays\n            X -> Independent Variables/ Features\n            y -> Dependent/ Target Variable\n            theta -> Parameters \n            alpha -> Learning Rate i.e. size of each steps we take\n            no_of_iters -> Number of iterations we want to perform\n    \n    Return -> theta (numpy array) which are the best parameters for our dataset to fit a linear line\n             and Cost Computed (numpy array) for each iteration\n    \"\"\"\n    m=X.shape[0]\n    J_Cost = []\n    for i in range(no_of_iters):\n        error = np.dot(X.transpose(),(X.dot(theta)-y))\n        theta = theta - alpha * (1/m) * error\n        J_Cost.append(ComputeCost(X,y,theta))\n    \n    return theta, np.array(J_Cost)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is our Gradient Descent Algorithm which will minimize the *Error in Prediction*.\n\nBasically, it will find the best coefficients **theta** for our data which will represt Best Linear Line for our data.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"iters = 1000\n\nalpha1 = 0.001\ntheta1 = np.zeros((X_train.shape[1],1))\ntheta1, J_Costs1 = GradientDescent(X_train,y_train,theta1,alpha1,iters)\n\nalpha2 = 0.003\ntheta2 = np.zeros((X_train.shape[1],1))\ntheta2, J_Costs2 = GradientDescent(X_train,y_train,theta2,alpha2,iters)\n\nalpha3 = 0.01\ntheta3 = np.zeros((X_train.shape[1],1))\ntheta3, J_Costs3 = GradientDescent(X_train,y_train,theta3,alpha3,iters)\n\nalpha4 = 0.03\ntheta4 = np.zeros((X_train.shape[1],1))\ntheta4, J_Costs4 = GradientDescent(X_train,y_train,theta4,alpha4,iters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Now we run the Gradient Descent Algorithm using different **learning rate** *alpha*. Number of iterations we will be performing = 1000\n\n\n- After that we will see what is *best learing rate* for our algorithm by visualizing the results.\n\n\n- Finally we will get best *theta*, which represents the best linear line for our data."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.plot(J_Costs1,label = 'alpha = 0.001')\nplt.plot(J_Costs2,label = 'alpha = 0.003')\nplt.plot(J_Costs3,label = 'alpha = 0.01')\nplt.plot(J_Costs4,label = 'alpha = 0.03')\nplt.title('Convergence of Gradient Descent for different values of alpha')\nplt.xlabel('No. of iterations')\nplt.ylabel('Cost')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**\n- We can see that for ***alpha = 0.03***, Gradient Descent algorithm converges to minimum much faster than for any other value of alpha (taken).\n\n\n- We can see that Gradient Descent algorithm converged to minimum Cost somewhere before 50 iterations for *alpha = 0.03*.\n\n\n- Gradient Descent convergenced fastest for *alpha = 0.03 -> 0.01 -> 0.003 -> 0.001*.\n\n\n- Thus, the best value of *alpha = 0.03* and corrosponding to it we will get best *theta* which is equal to '*theta4*."},{"metadata":{"trusted":true},"cell_type":"code","source":"theta4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above is the value of theta corrosponding to alpha = 0.03"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Predict(X,theta):\n    \"\"\"\n    This function predicts the result for the unseen data\n    \"\"\"\n    y_pred = X.dot(theta)\n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predict fucntion predicts the house price i.e. MEDV on the new unseen data using the regression coefficients i.e. theta."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"y_pred = Predict(X_test,theta4)\ny_pred[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predicted value for Test Set is saved in *y_pred* successfully."},{"metadata":{},"cell_type":"markdown","source":"## 5.2 Model Evaluation"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.scatter(x=y_test,y=y_pred,alpha=0.5)\nplt.xlabel('y_test',size=12)\nplt.ylabel('y_pred',size=12)\nplt.title('Predicited Values vs Original Values (Test Set)',size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above scatter plot we can see that the diagonal line is not that straight, which represents the differences in the actual and predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.residplot(y_pred,(y_pred-y_test))\nplt.xlabel('Predicited Values',size=12)\nplt.ylabel(\"Residues\",size=12)\nplt.title('Residual Plot',size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.distplot(y_pred-y_test)\nplt.xlabel('Residual',size=12)\nplt.ylabel('Frquency',size=12)\nplt.title('Distribution of Residuals',size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**\n- *Distribution of Residuals Plot* shows residuals are quite normally distributed.\n\n\n- From above *Residual Plot*, I do not found any significant pattern in residues (errors or predicition).\n\n\n- I can conclude that our model is neither under fitting nor over fitting the data."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nr2= metrics.r2_score(y_test,y_pred)\nN,p = X_test.shape\nadj_r2 = 1-((1-r2)*(N-1))/(N-p-1)\nprint(f'R^2 = {r2}')\nprint(f'Adjusted R^2 = {adj_r2}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*R square* value above is calcualted on Test Set, though it is not very good but still it explains quite good linear relationship among independent variable and dependent variables."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nmse = metrics.mean_squared_error(y_test,y_pred)\nmae = metrics.mean_absolute_error(y_test,y_pred)\nrmse = np.sqrt(metrics.mean_squared_error(y_test,y_pred))\nprint(f'Mean Squared Error: {mse}',f'Mean Absolute Error: {mae}',f'Root Mean Squared Error: {rmse}',sep='\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above *Evaluation Metrices*, we can notice that Root Mean Squared Error is low for our Multiple Regression Model and that is good thing for us."},{"metadata":{},"cell_type":"markdown","source":"## 5.3 Model Interpretation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#coefficients of regression model\ncoeff=np.array([y for x in theta4 for y in x]).round(2)\nfeatures=['Bias','RM','TAX','PTRATIO','LSTAT']\neqn = 'MEDV = '\nfor f,c in zip(features,coeff):\n    eqn+=f\" + ({c} * {f})\";\n\nprint(eqn)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"sns.barplot(x=features,y=coeff)\nplt.ylim([-5,25])\nplt.xlabel('Coefficient Names',size=12)\nplt.ylabel('Coefficient Values',size=12)\nplt.title('Visualising Regression Coefficients',size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**\n\n - **MEDV = (21.74 * Bias) + (2.74 * RM) + (-1.06 * TAX) + (-1.93 * PTRATIO) + (-3.03 * LSTAT)**\n\n\n- From above equation we can conclude that, for 1 unit increase in RM the House Price will go up by 2.74 units and vica-versa, considering other factors remaining constant.\n\n\n- Also for 1 unit increase in TAX the House Price will go down by 1.06 units and vica-versa, considering other factors remaining constant.\n\n\n- Also for 1 unit increase in PTRATIO the House Price will go down by 1.93 units and vica-versa, considering other factors remaining constant.\n\n\n- Also for 1 unit increase in LSTAT the House Price will go down by 3.03 units and vica-versa, considering other factors remaining constant.\n\n\n(Above four observations are quite meaningful also, since RM is positively correlated to MEDV and TAX, PRTATIO & LSTAT are negatively correlated to MEDV.)\n\n**Conclusion:**\n\n- *As we know, as the number of rooms increases price of the house increases. Whereas if the number of lower class people is high in a region (LSTAT) or if the student-teacher ratio is bigger (PTRATIO) i.e. less number of teachers for more number of students or if TAX rate is more, obiously House price will gp down.*\n\n\n- Our multiple regression model does not explains the data perfectly (as R sqare value is 0.77) but it still it explains the good relationship of House Price (i.e. MEDV) and other factors affecting the price.\n\n\n- We will fit few more models on this dataset and at the end will choose the model which explains the data best among all models.\n---"},{"metadata":{},"cell_type":"markdown","source":"# 6. Decision Tree\n#### We will be using sklearn lirbrary to build Decision Tree model on the dataset. "},{"metadata":{},"cell_type":"markdown","source":"## 6.1 Model Prepration"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_dt = df3.iloc[:,:-1].values\ny_dt = df3.iloc[:,-1].values","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_dt,X_test_dt,y_train_dt,y_test_dt = train_test_split(X_dt,y_dt,test_size=0.2,random_state=42)\n\nprint(f\"Shape of X_train_dt = {X_train_dt.shape}\")\nprint(f\"Shape of X_test_dt = {X_test_dt.shape}\")\nprint(f\"Shape of y_train_dt = {y_train_dt.shape}\")\nprint(f\"Shape of y_test_dt = {y_test_dt.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have divided the dataset in Training set and Test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor()\ndt.fit(X_train_dt,y_train_dt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We fiitted a Decision Tree Regressor with default parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_dt = dt.predict(X_test_dt)\ny_pred_dt[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We used the Decision Tree Regressor to predict the House Price on the Test Set."},{"metadata":{},"cell_type":"markdown","source":"## 6.2 Model Interpretation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x=y_test_dt,y=y_pred_dt,alpha=0.5)\nplt.xlabel('y_test',size=12)\nplt.ylabel('y_pred',size=12)\nplt.title('Predicited Values vs Original Values (Test Set)',size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above scatter plot we can see that the diagonal line is not that straight, which represents the differences in the actual and predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.residplot(y_pred_dt,(y_pred_dt-y_test_dt))\nplt.xlabel('Predicited Values',size=12)\nplt.ylabel(\"Residues\",size=12)\nplt.title('Residual Plot',size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(y_pred_dt-y_test_dt)\nplt.xlabel('Residual',size=12)\nplt.ylabel('Frquency',size=12)\nplt.title('Distribution of Residuals',size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**\n- *Distribution of Residuals Plot* shows residuals are normally distributed.\n\n\n- From above *Residual Plot*, I do not found any significant pattern in residues (errors or predicition).\n\n\n- I can conclude that our model is neither under fitting nor over fitting the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nr2_dt= metrics.r2_score(y_test_dt,y_pred_dt)\nN,p = X_test_dt.shape\nadj_r2_dt = 1-((1-r2_dt)*(N-1))/(N-p-1)\nprint(f'R^2 = {r2_dt}')\nprint(f'Adjusted R^2 = {adj_r2_dt}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*R square* value above is calcualted on Test Set, though it is not very good but still its better than our Multiple Linear Regression Score. This means that Decision Tree model fits better than Multiple Linear Regression Model."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nmse_dt = metrics.mean_squared_error(y_test_dt,y_pred_dt)\nmae_dt = metrics.mean_absolute_error(y_test_dt,y_pred_dt)\nrmse_dt = np.sqrt(metrics.mean_squared_error(y_test_dt,y_pred_dt))\nprint(f'Mean Squared Error: {mse_dt}',f'Mean Absolute Error: {mae_dt}',f'Root Mean Squared Error: {rmse_dt}',sep='\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above *Evaluation Metrices*, we can notice that Root Mean Squared Error is low for our Decision Tree Model and that is good thing for us. Also all these error scores are less then Linear Regression Model.\n\n**Conclusion:**\n- Decision Tree Model gives better R square than one we got in Linear Regression, it means that this model is able to predict house prices more accurate than Linear Regression and we may use this model for predicting the house prices.\n\n\n- We will try one more model Random Forest before reaching to our final conclusion.\n---"},{"metadata":{},"cell_type":"markdown","source":"# 7. Random Forest\n#### We will be using sklearn lirbrary to build Random Forest model on the dataset. "},{"metadata":{},"cell_type":"markdown","source":"## 7.1 Model Prepration"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_rf = df3.iloc[:,:-1].values\ny_rf = df3.iloc[:,-1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_rf,X_test_rf,y_train_rf,y_test_rf = train_test_split(X_rf,y_rf,test_size=0.2,random_state=42)\n\nprint(f\"Shape of X_train_rf = {X_train_rf.shape}\")\nprint(f\"Shape of X_test_rf = {X_test_rf.shape}\")\nprint(f\"Shape of y_train_rf = {y_train_rf.shape}\")\nprint(f\"Shape of y_test_rf = {y_test_rf.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have divided the dataset in Training set and Test set"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"warnings.filterwarnings(action='ignore')\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(random_state=42)\nrf.fit(X_train_rf,y_train_rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We fiitted a Random Forest Regressor with default parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_rf = rf.predict(X_test_rf)\ny_pred_rf[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We used the Random Forest Regressor to predict the House Price on the Test Set."},{"metadata":{},"cell_type":"markdown","source":"## 7.2 Model Interpretation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x=y_test_rf,y=y_pred_rf,alpha=0.5)\nplt.xlabel('y_test',size=12)\nplt.ylabel('y_pred',size=12)\nplt.title('Predicited Values vs Original Values (Test Set)',size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above scatter plot we can see that the diagonal line is not that straight, which represents the differences in the actual and predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.residplot(y_pred_rf,(y_pred_rf-y_test_rf))\nplt.xlabel('Predicited Values',size=12)\nplt.ylabel(\"Residues\",size=12)\nplt.title('Residual Plot',size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(y_pred_rf-y_test_rf)\nplt.xlabel('Residual',size=12)\nplt.ylabel('Frquency',size=12)\nplt.title('Distribution of Residuals',size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n**Observations:**\n- *Distribution of Residuals Plot* shows residuals are normally distributed.\n\n\n- From above *Residual Plot*, I do not found any significant pattern in residues (errors or predicition).\n\n\n- I can conclude that our model is neither under fitting nor over fitting the data.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nr2_rf= metrics.r2_score(y_test_rf,y_pred_rf)\nN,p = X_test_dt.shape\nadj_r2_rf = 1-((1-r2_rf)*(N-1))/(N-p-1)\nprint(f'R^2 = {r2_rf}')\nprint(f'Adjusted R^2 = {adj_r2_rf}')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*R square* value above is calcualted on Test Set, though it is not very good but still its better than our Decision Tree Score. This means that Random Forest model fits better than Decision Tree Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nmse_rf = metrics.mean_squared_error(y_test_rf,y_pred_rf)\nmae_rf = metrics.mean_absolute_error(y_test_rf,y_pred_rf)\nrmse_rf = np.sqrt(metrics.mean_squared_error(y_test_rf,y_pred_rf))\nprint(f'Mean Squared Error: {mse_rf}',f'Mean Absolute Error: {mae_rf}',f'Root Mean Squared Error: {rmse_rf}',sep='\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above *Evaluation Metrices*, we can notice that Root Mean Squared Error is low for our Random Forest Model and that is good thing for us. Also all these error scores are less then Decision Tree and Linear Regression.\n\n**Conclusion:**\n- Random Forest Model gives better R square than both the models we made earlier, it means that this model is able to predict house prices more accurate than previous both models and we may use this model for predicting the house prices.\n\n---"},{"metadata":{},"cell_type":"markdown","source":"# 8. Conclusion\n\n*Since we have made three models for our dataset and all of them have some difference in their prediction capability, now it's time to wrap up all we learned and reach to our final conclusion.*\n\n*So guyz it's Show time!*"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"results=pd.DataFrame({'Linear Regression':[r2,adj_r2],'Decision Tree':[r2_dt,adj_r2_dt],\n                      'Random Forest':[r2_rf,adj_r2_rf]},index=['R square','Adj R square'])\n\nresults.plot(kind='bar',alpha=0.7,grid=True,title='Interpreting Results',rot=0,figsize=(10,5),colormap='jet')\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- *As from the above figure it is clearly visible that R square and Adjusted R square value for* **Random Forest** *Model is highest among all three models we used for predicting house price.*\n\n\n- *We can say that it will be good to use Random Forest model for this dataset as it will help in predicitng house prices best without overfitting.*\n\n\n- *Also, this data was collected many years ago and many things has changed since then, so it may not be feasible to implement this model for current house price predictions of Boston. Other new features must be taken into consideration and then we can make a better model for current scenario.*\n\n\n- *Though this data is enough to learn and understand how to predicit House Price frpm given number of features using Machine Learning.*"},{"metadata":{},"cell_type":"markdown","source":" # Thank You\nPlease leave your valuable feedback on this"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":1}