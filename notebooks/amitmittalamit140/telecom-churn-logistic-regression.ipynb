{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Telecom Churn Case Study\n\nIn this notebook, we are going to work on a Telecom company case study where using the past information we need to build a model that can predict whether a particular customer will  switch to different service provider or not (Churn or not). So for us the variable of interest is `Churn` which will tell us whether or not a particular customer has churned. It is a binary variable, 1 means that the customer has churned and 0 means the customer has not churned.\n\nCompany also needs to know the factors (variables) which influences the Churn variable and how much they impact individually. This will help the company to improve those area to retain their customers. Company needs a descent model that can predict good percentage of Churn and Non-Churn customer correctly.\n\nWe will build a Logistic Regression model for this problem because it is easy to interpret and this will help company in decision making better.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will perform following steps in this notebook:\n\n- **Step 1**: Importing Libraries\n- **Step 2**: Exploring Data Frame\n- **Step 3**: Data Preparation\n- **Step 4**: Splitting the Dataset\n- **Step 5**: Feature Scaling\n- **Step 6**: Model Building\n- **Step 7**: Model Evaluation\n- **Step 8**: Model Validation\n- **Step 9**: Model Interpretation\n- **Step 10**: Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Step 1: Importing Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing all libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n#building model\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\n\n#model evaluation\nfrom sklearn.feature_selection import RFE\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_recall_curve\n\n#model validation\nfrom sklearn.model_selection import train_test_split\n\n#visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Data display coustomization\npd.set_option('display.max_columns', 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 2: Exploring Data Frame","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom = pd.read_csv(\"/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Now we have one data frame consisting all data. Now we will see first five rows of the new data frame\ntelecom.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#prinitng shape of the dataset\nr,c = telecom.shape\nprint(f\"Shape of telecom dataset: {telecom.shape}\")\nprint(f\"Number of rows: {r}\")\nprint(f\"Number of columns: {c}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This means we have **21 features** about a customer including target variable `Churn` and we have details for **7043 customers**.\n\nBrief description about each feature (column) is given below:\n1. `customerID`: The unique ID of each customer\n2. `tenure`: Number of monthscustomer has been using the service\n3. `PhoneService`: Whether a customer has a Phone services or not (Yes, No)\n4. `Contract`: The contract term of the customer (Month-to-month, One year, Two year)\n5. `PaperlessBilling`: Whether a customer has opted for paperless billing (Yes, No)\n6. `PaymentMethod`: The customer’s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))\n7. `MonthlyCharges`: Specifies the money paid by a customer each month\n8. `TotalCharges`: The total money paid by the customer to the company\n9. `Churn`: This is the target variable which specifies if a customer has churned or not (Yes, No)\n10. `gender`: The gender of a person (Male, Female)\n11. `SeniorCitizen`: Whether a customer can be classified as a senior citizen (1=Yes, 0=No)\n12. `Partner`: Whether the customer has a partner or not (Yes, No)\n13. `Dependents`: Whether the customer has dependents(children/ retired parents) or not (Yes, No)\n14. `MultipleLines`: Whether the customer has multiple lines or not (Yes, No, No phone service)\n15. `InternetService`: Customer’s internet service provider (DSL, Fiber optic, No)\n16. `OnlineSecurity`: Whether the customer has online security or not (Yes, No, No internet service)\n17. `OnlineBackup`: Whether the customer has online backup or not (Yes, No, No internet service)\n18. `DeviceProtection`: Whether the customer has device protection or not (Yes, No, No internet service)\n19. `TechSupport`: Whether the customer has tech support or not (Yes, No, No internet service)\n20. `StreamingTV`: Whether the customer has streaming TV or not (Yes, No, No internet service)\n21. `StreamingMovies`: Whether the customer has streaming movies or not (Yes, No, No internet service)","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# let's look at the some statistics of the dataframe\ntelecom.describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Let's look at the data type of each feature\ntelecom.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Variables are of different types, which are categorized below\n\n- **Categorical**:\n    - **Binary (7)**: `SeniorCitizen`, `gender`, `Partner`, `Dependents`, `PhoneService`, `PaperlessBilling`, and `Churn`\n    \n    - **Multimonial (11)**: `CustomerID`, `MultipleLines`,`InternetService`, `OnlineSecurity`, `OnlineBackup`, `DeviceProtection`, `TechSupport`, `StreamingTV`, `StreamingMovies`, `Contract`, `PaymentMethod`\n    \n    \n- **Continuous(3)**: `TotalCharges `, `MonthlyCharges` and `Tenure` ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Step 3: Data Preparation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Converting Binary Variables (Yes/ No) into (1/ 0)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining method to convert them\ndef binary_map(x):\n    return x.map({'Yes': 1, \"No\": 0})\n\nbin_var =  ['PhoneService', 'PaperlessBilling', 'Churn', 'Partner', 'Dependents']\n\n# Applying the method to the data frame\ntelecom[bin_var] = telecom[bin_var].apply(binary_map)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Converting Binary Variable gender (Male/ Female) into (1/ 0)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating dummies for gender and dropping first column because single column can capture the whole data\ngender = pd.get_dummies(telecom['gender'], drop_first=True)\n\n# Merging the above results with telecom data frame \ntelecom = pd.concat([telecom, gender], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#printing first 5 rows of the data frame after converting Binary variables\ntelecom.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`Male` column represents the gender column now, 1 = Male and 0 = Female","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Coverting Multinomial Variables by creating dummy variables\n\nA dummy variable is a numeric variable that represents categorical data.\n\nTechnically, dummy variables are dichotomous, quantitative variables. Their range of values is small; they can take on only two quantitative values. As a practical matter, regression results are easiest to interpret when dummy variables are limited to two specific values, 1 or 0. Typically, 1 represents the presence of a qualitative attribute, and 0 represents the absence.\n\n\n##### Avoid the Dummy Variable Trap\nWhen defining dummy variables, a common mistake is to define too many variables. If a categorical variable can take on k values, it is tempting to define k dummy variables. Resist this urge. Remember, you only need k - 1 dummy variables.\n\nA kth dummy variable is redundant; it carries no new information. And it creates a severe multicollinearity problem for the analysis.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a dummy variable for some of the categorical variables and dropping the first one.\ndummy1 = pd.get_dummies(telecom[['Contract', 'PaymentMethod', 'InternetService']], drop_first=True)\n\n# Adding the results to the telecom dataframe\ntelecom = pd.concat([telecom, dummy1], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating dummy variables for the remaining categorical variables and dropping the level with big names.\n\n# Creating dummy variables for the variable 'MultipleLines'\nml = pd.get_dummies(telecom['MultipleLines'], prefix='MultipleLines')\n# Dropping MultipleLines_No phone service column\nml1 = ml.drop(['MultipleLines_No phone service'], 1)\n# Adding the results to the telecom dataframe\ntelecom = pd.concat([telecom,ml1], axis=1)\n\n# Creating dummy variables for the variable 'OnlineSecurity'.\nos = pd.get_dummies(telecom['OnlineSecurity'], prefix='OnlineSecurity')\nos1 = os.drop(['OnlineSecurity_No internet service'], 1)\n# Adding the results to the telecom dataframe\ntelecom = pd.concat([telecom,os1], axis=1)\n\n# Creating dummy variables for the variable 'OnlineBackup'.\nob = pd.get_dummies(telecom['OnlineBackup'], prefix='OnlineBackup')\nob1 = ob.drop(['OnlineBackup_No internet service'], 1)\n# Adding the results to the telecom dataframe\ntelecom = pd.concat([telecom,ob1], axis=1)\n\n# Creating dummy variables for the variable 'DeviceProtection'. \ndp = pd.get_dummies(telecom['DeviceProtection'], prefix='DeviceProtection')\ndp1 = dp.drop(['DeviceProtection_No internet service'], 1)\n# Adding the results to the telecom dataframe\ntelecom = pd.concat([telecom,dp1], axis=1)\n\n# Creating dummy variables for the variable 'TechSupport'. \nts = pd.get_dummies(telecom['TechSupport'], prefix='TechSupport')\nts1 = ts.drop(['TechSupport_No internet service'], 1)\n# Adding the results to the telecom dataframe\ntelecom = pd.concat([telecom,ts1], axis=1)\n\n# Creating dummy variables for the variable 'StreamingTV'.\nst =pd.get_dummies(telecom['StreamingTV'], prefix='StreamingTV')\nst1 = st.drop(['StreamingTV_No internet service'], 1)\n# Adding the results to the telecom dataframe\ntelecom = pd.concat([telecom,st1], axis=1)\n\n# Creating dummy variables for the variable 'StreamingMovies'. \nssm = pd.get_dummies(telecom['StreamingMovies'], prefix='StreamingMovies')\nssm1 = ssm.drop(['StreamingMovies_No internet service'], 1)\n# Adding the results to the telecom dataframe\ntelecom = pd.concat([telecom,ssm1], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We dropped `No phone service` and `No internet service` because they are already included from columns `InternetService` and `PhoneService` as we can see below","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"telecom.InternetService.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom.PhoneService.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dropping the repeated Variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have created dummies for the below variables, so we can drop them\ntelecom = telecom.drop(['Contract','PaymentMethod','gender','MultipleLines','InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n       'TechSupport', 'StreamingTV', 'StreamingMovies'], 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The varaible TotalCharges is of String data type so converting it into float type\ntelecom['TotalCharges'] = pd.to_numeric(telecom[\"TotalCharges\"].replace(\" \",\"\"),downcast=\"float\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking data types of variables \ntelecom.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have transformed all the variables, now next step is to check for outlier in the dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Checking for Outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot Box Plot for all there continuous variables \n\nplt.figure(figsize=(15,3))\nplt.subplot(1,3,1)\nsns.boxplot(telecom[[\"tenure\"]])\nplt.title(\"Tenure\",size=15)\n\nplt.subplot(1,3,2)\nsns.boxplot(telecom[[\"MonthlyCharges\"]])\nplt.title(\"MonthlyCharges\",size=15)\n\nplt.subplot(1,3,3)\nsns.boxplot(telecom[[\"TotalCharges\"]])\nplt.title(\"TotalCharges\",size=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above Box Plots we can see that there are no outliers.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Checking for Missing Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding up the missing values (column-wise)\ntelecom.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It means that 11/7043 = 0.001561834 i.e 0.1%, best is to remove these observations from the analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing NaN TotalCharges rows\ntelecom = telecom[~np.isnan(telecom['TotalCharges'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking again for missing values (column-wise)\ntelecom.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we don't have any missing values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Checking the Correlation among Variables.\nWe are using Pearson's correlation to compute correlation matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation Matrix\nplt.figure(figsize = (20,10))\nsns.heatmap(round(telecom.corr(),1),annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping the highly correlated variables\n\ntelecom.drop(['MultipleLines_No','OnlineSecurity_No','OnlineBackup_No','DeviceProtection_No','TechSupport_No',\n                       'StreamingTV_No','StreamingMovies_No'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Step 4: Splitting the Dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We are done with data processing steps,  now the data is ready to fetch in the model.\n\nWe will first split the dataset into towo part:\n- X = All independent variables\n- y = Dependent variable `Churn`\n\nThen we will split the dataset into Training set and Testing set:\n- Training Set: Model is build using this dataset\n- Testing Set: Model Validation is done using this set\n\nWe will be doing In-sample validation for this problem. Training set and Testing set will be in ratio 7:3 respectively. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#customerID column is of no use for the model so we drop that column also\nX = telecom.drop(['Churn','customerID'], axis=1)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"y = telecom['Churn']\ny.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we split the dataset into Training set and Testing set with ratio 7:3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the dataset into training set and testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Step 5: Feature Scaling\n\nMost of the times, your dataset will contain features highly varying in magnitudes, units and range. But since, most of the machine learning algorithms use Eucledian distance between two data points in their computations. To supress this effect, we need to bring all features to the same level of magnitudes.\n\nThere are several ways for performing Feature Scaling, here we will be using `Standard Scalar` or `Standardization`\n\\begin{equation*}\n\\mathbf{X_*}   = \\frac{X - Mean}{Standard Deviation}\n\\end{equation*}","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#we will standardize continous variables only and not categorical variables\nsc = StandardScaler()\nsc.fit(X_train[['tenure','MonthlyCharges','TotalCharges']])\nX_train[['tenure','MonthlyCharges','TotalCharges']] = sc.transform(X_train[['tenure','MonthlyCharges','TotalCharges']])\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Minimum samples required for building a Logistic Regression Model\n\nFor multivariable logistic regression, Peduzzi, Concato, Kemper, Holford, & Feinstein (1996) suggested a very simple guideline for a minimum number of cases for logistic regression study.\n\n**p:** Smallest of the proportions of negative or positive cases in the population\n\n**k:** Number of Independent variables\n\\begin{equation*}\n\\mathbf{N}   = \\frac{10*k}{p}\n\\end{equation*}","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"### Checking the \np = (sum(y_train)/len(y_train))\nprint(f\"p: {p}\")\n\nk = X_train.shape[1]\nprint(f\"k: {k}\")\n\nN = 10 * k / p\nprint(f\"N: {int(N)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This means we need to have minimum **879** samples to build a Logistic Regression model and in the `X_train` we have **4922** samples i.e. we can build a Logistic model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Step 6: Model Building\n\nNow we will build Logistic Regression model using `statsmodel` and `sklearn` libraries.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will perform Coarse tuning and Fine tuning technique to do `Feature Selection` and select best features for the model\n\n- **Coarse Tuning**: Recursive Feature Elimination (RFE) \n- **Fine Tuning**: Variable Inflation Factors (VIF) and p-value","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Building First Model\n\nBuilding first model with all selected varaiables","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection Using RFE\n\nSelecting top 15 features for the model out of 23 features using RFE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression()\nrfe = RFE(logreg, 15)\nrfe = rfe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#top 15 columns returned by RFE\ncol = X_train.columns[rfe.support_]\ncol","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Second Model\n\nBuilding second model after selecting top 15 features from RFE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#building the model with top 15 features which we got from RFE\nX_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can notice that p-value of all features is < 0.05","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the predicted values on the train set and showing first 10 predictions in terms of probabilities\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reshaping the predicted array\ny_train_pred = y_train_pred.values.reshape(-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Comparing Actual Churn and Predicted Churn on Training set ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating data frame with actual churn and predicted probablilities\ny_train_pred_final = pd.DataFrame({'Churn':y_train.values, 'Churn_Prob':y_train_pred})\ny_train_pred_final['CustID'] = y_train.index\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Creating new column 'Predicted' with 1 if Churn_Prob > 0.5 else 0","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking Accuracy of the Model","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Checking Accuracy of the model\nprint(\"Accuracy (Training Set): \",round(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted),4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy of the model looks good with **81%**. Let's check Variable Inflation Factors (VIF) to check multicollinearity because Pearson's correlation calculates one-to-one correlation only. However, VIF determines the strength of the correlation between the independent variables. It is predicted by taking a variable and regressing it against every other variable. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Checking VIFs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a few variables with high VIF. It's best to drop these variables as they aren't helping much with prediction and unnecessarily making the model complex. The variable 'PhoneService' has the highest VIF. So let's start by dropping that.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will drop variables one by one, droping MonthlyCharges column\ncol = col.drop('MonthlyCharges',1)\ncol","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Third Model\n\nBuilding third model after dropping `MonthlyCharges`variable.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can notice that p-value of `MultipleLines_Yes` is 0.07 (> 0.05).\n\nThis means that variable is insignificant for us and hence we can drop it.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Making predictions on the Training Set","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred_final['Churn_Prob'] = y_train_pred\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Creating new column 'Predicted' with 1 if Churn_Prob > 0.5 else 0","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking Accuracy of the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(\"Accuracy (Training Set): \",round(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted),4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So overall the accuracy hasn't dropped much.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Checking VIFs","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have seen `MultipleLines_Yes` variable is insignificant due to p-value and `TotalCharges` has high VIF value. However, we will drop only one variable at a time and there we give priority to p-value more than VIF.\n\nTherefore, we will drop `MultipleLines_Yes`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#We are droping MultipleLines_Yes variable as it is insignificant\n#We give priority to p-value than VIF\ncol = col.drop('MultipleLines_Yes',1)\ncol","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Fourth Model\n\nBuilding Fourth model after dropping `MultipleLines_Yes` variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can notice that p-value for all variables are < 0.05","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Making predictions on the Training Set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred_final['Churn_Prob'] = y_train_pred\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Creating new column 'Predicted' with 1 if Churn_Prob > 0.5 else 0","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking Accuracy of the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(\"Accuracy (Training Set): \",round(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted),4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy is still practically the same.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Checking VIFs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's drop TotalCharges since it has a high VIF\ncol = col.drop('TotalCharges')\ncol","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Fifth Model\n\nBuilding fifth model after dropping `TotalCharges` variable","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can notice that p-value for all variables are < 0.05","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Making predictions on the Training Set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred_final['Churn_Prob'] = y_train_pred\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Creating new column 'Predicted' with 1 if Churn_Prob > 0.5 else 0","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking Accuracy of the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(\"Accuracy (Training Set): \",round(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted),4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy is still practically the same.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Checking VIFs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### All variables have a good value of VIF and none of the variable is insignificant. Therefore we don't need to drop any more variables and we can proceed with this model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Step 7: Model Evaluation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Confusion Metrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take a look at the confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted )\nconfusion","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"print(\"Predicted     |  Not Churn (0)  |  Churn (1)\")\nprint(\"Actual        |                 | \")\nprint(\"--------------|-----------------|----------------\")\nprint(\"Not Churn (0) |     3270        |     365\")\nprint(\"--------------|-----------------|----------------\")\nprint(\"Churn     (1) |      604        |     683\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metrics for Evaluation","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Accuracy of the final model\naccuracy = (TN + TP)/float(TN+FN+TP+FP)\nprint(\"Accuracy of the model: \",round(accuracy,3))\n\n# Sensitivity of the final model\nsensitivity = TP / float(TP+FN)\nprint(\"Sensitivity of the model: \",round(sensitivity,3))\n\n# Specificity of the final model\nspecificity = TN / float(TN+FP)\nprint(\"Specificity of the model: \",round(specificity,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can notice that we have got very good Accuracy and Specificity score, however, Sensitivity score is not that good.\n\nIt means that our model is not able to capture `churned` customer very well and this can be a problematic for the business. We wish to capture them properly, but How?\n\n**Remember**: We declared the customer as churn (1) or not churn (0) from probabilities based on some arbitrary thresh-hold. We chose that thresh-hold to be 0.5 i.e. any customer with prob > 0.5 marked as churn (1) else not churn (0).\n\nTherefore, now we need to find optimal value of the thresh-hold so that our model can capture churn customer well.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Finding the Optimal Thresh-hold value","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Optimal thresh-hold probability is that probability where we get balanced sensitivity and specificity","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['Probability','Accuracy','Sensitivity','Specificity'])\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])/total1\n    \n    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='Probability', y=['Accuracy','Sensitivity','Specificity'])\nplt.xlabel(\"Thresh-hold\")\nplt.ylabel(\"Scores\")\nplt.title(\"Sensitivity and Specificity Trade-off\",size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the above curve ,we can notice that 0.3 is the optimum thresh-hold value","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"y_train_pred_final['final_predicted'] = y_train_pred_final.Churn_Prob.map( lambda x: 1 if x > 0.3 else 0)\n\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's look at the Confustion Matrix, Accuracy, Sensitivity and Specificity for the final optimal thresh-hold value","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.final_predicted )\nconfusion2","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(\"Predicted     |  Not Churn (0)  |  Churn (1)\")\nprint(\"Actual        |                 | \")\nprint(\"--------------|-----------------|----------------\")\nprint(\"Not Churn (0) |     2787        |     848\")\nprint(\"--------------|-----------------|----------------\")\nprint(\"Churn     (1) |      288        |     999\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Accuracy of the final model\naccuracy = (TN + TP)/float(TN+FN+TP+FP)\nprint(\"Accuracy of the model: \",round(accuracy,3))\n\n# Sensitivity of the final model\nsensitivity = TP / float(TP+FN)\nprint(\"Sensitivity of the model: \",round(sensitivity,3))\n\n# Specificity of the final model\nspecificity = TN / float(TN+FP)\nprint(\"Specificity of the model: \",round(specificity,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can notice that, we have got `Sesitivity` score along with good `Accuracy` and `Specificity`.\n\nOur model performance is similar on all three metrics, now let's look at the **Receiver Operating Characteristic (ROC) curve** of the model.\n\nAn ROC curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## ROC Curve","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    #plt.savefig(\"E:/1. NITW/Project 4th Sem/ROC Curve.jpg\")\n    plt.show()\n    \n    return None","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Churn, y_train_pred_final.Churn_Prob, drop_intermediate = False )\n\ndraw_roc(y_train_pred_final.Churn, y_train_pred_final.Churn_Prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can notice that ROC curve look very good and **Area under the curve (AUC)** is `0.85` which is a very good score and tells the goodness of the model.\n\nTherefore, all this shows that our the model we build using Training dataset fits goon on that and the optimal value of the thresh-hold gives us good scores.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now let's look at some different metrics for evaluation which we come accross in the theoritical part. However for this dataset we will refer `Sensitivity` and `Specificity` only.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Precision and Recall","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Precision \n\nIt tells us the percentage of 1’s predicted correctly out of total 1’s predicted.\n\nPrecision = TP / (TP + FP)\n\n##### Recall\n\nIt tell us the 1’s predicted correctly out of total actual 1’s. It's basically sensitivity.\n\nRecall = TP / (TP + FN)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Precision of the final model\nprecision = TP / float(TP+FP)\nprint(\"Precision of the model: \",round(precision,3))\n\n# Recall of the final model\nrecall = TP / float(TP+FN)\nprint(\"Recall of the model: \",round(recall,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Precision and Recall Tradeoff\n\nLet's look at the Precision and Recall trade-off now","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Churn, y_train_pred_final.Churn_Prob)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.plot(thresholds, p[:-1], \"g-\",label=\"Precision\")\nplt.plot(thresholds, r[:-1], \"r-\",label=\"Recall\")\nplt.xlabel(\"Thresh-hold\")\nplt.ylabel(\"Scores\")\nplt.title(\"Precision and Recall Trade-off\",size=15)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\n\nWe can see that thresh-hold value of `0.5` would be preffered to choose if we use `Precision` and `Recall` for model evaluation.\n\nWe can in the graph that the curve for precision is quite jumpy towards the end. This is because the denominator of precision, i.e. (TP+FP) is not constant as these are the predicted values of 1s. And because the predicted values can swing wildly, we get a very jumpy curve.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Step 8: Model Validation\n\nWe are done with the model building and evaluation steps, now let's check for the stability of the model.\n\nWhether the model gives similar scores on the `Testing Set` also. We are using `In-sample` validation for this problem. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Let's first scale down the continous varibales using Standardization.\n\nWe have already trained the scalar on the `Training Set`, now we just need to transform the `Testing Set` using the same scalar.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test[['tenure','MonthlyCharges','TotalCharges']] = sc.transform(X_test[['tenure','MonthlyCharges','TotalCharges']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Now selecting only those columns which we used to build the final model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = X_test[col]\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making Predictions using the trained model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_sm = sm.add_constant(X_test)\ny_test_pred = res.predict(X_test_sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the head\ny_pred_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Putting CustID to index\ny_test_df['CustID'] = y_test_df.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Final Probabilities corrosponding to the customerID","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Churn_Prob'})","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Let's see the head of y_pred_final\ny_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Creating new column 'final_predicted' with 1 if Churn_Prob > 0.3 else 0\n\nChoosing the optimal thresh-hold value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final['final_predicted'] = y_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.3 else 0)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"y_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's look at the Confustion Matrix, Accuracy, Sensitivity and Specificity for the final optimal thresh-hold value","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"confusion2 = metrics.confusion_matrix(y_pred_final.Churn, y_pred_final.final_predicted )\nconfusion2","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(\"Predicted     |  Not Churn (0)  |  Churn (1)\")\nprint(\"Actual        |                 | \")\nprint(\"--------------|-----------------|----------------\")\nprint(\"Not Churn (0) |     1144        |     384\")\nprint(\"--------------|-----------------|----------------\")\nprint(\"Churn     (1) |      163        |     419\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Evaluation (Testing Set)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Accuracy of the final model\naccuracy = (TN + TP)/float(TN+FN+TP+FP)\nprint(\"Accuracy of the model: \",round(accuracy,3))\n\n# Sensitivity of the final model\nsensitivity = TP / float(TP+FN)\nprint(\"Sensitivity of the model: \",round(sensitivity,3))\n\n# Specificity of the final model\nspecificity = TN / float(TN+FP)\nprint(\"Specificity of the model: \",round(specificity,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\n\nWe can notice that, for the Testing Set, all three metrics shows similar score as observed in the Training Set.\n\n**Training Set**\n- Accuracy of the model:  0.769\n- Sensitivity of the model:  0.776\n- Specificity of the model:  0.767\n\n**Testing Set**\n- Accuracy of the model:  0.741\n- Sensitivity of the model:  0.72\n- Specificity of the model:  0.749\n\nWe can notice that scores are similar for both Training and Testing Set, this shows that the model we build using Training Set also fits good and generalizes on the Testing Set.\n\nHence, we are ready to deploy the model and make predicitions and decisions using that.\n\nHowever, from time to time we need to monitor its performance and if the accuracy drops on the new data then we need to rebuild the model using new data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Step 9: Model Interpretation\n\nNow we reached to the final step in this notebook which is interpretation of model coefficients and making the final conclusion.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"model  = pd.DataFrame({\"Features\": X_train_sm.columns,\"Coefficient\":res.params.values})\nmodel[\"Odds_Ratio\"] = model[\"Coefficient\"].apply(lambda x: np.exp(x))\nmodel[[\"Coefficient\",\"Odds_Ratio\"]] = model[[\"Coefficient\",\"Odds_Ratio\"]].apply(lambda x: round(x,2))\nmodel[\"Perc_Impact\"] = model[\"Odds_Ratio\"].apply(lambda x: (x-1)*100)\nmodel","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\n\n**Tenure:**\n\n- Coefficient: -.90\n- Odds Ratio: 0.41\n\n*Tenure is continous variable which was standarized using Standard Scalar. Therefore, for 1 stardardized unit increase the odds of getting churned reduces by 59%. We know that 1 stardardized unit of tenure is equal to 24.5 months, therefore for increase in tenure by 24.5 months will lead to decrease in customer getting churned by 59%.*\n\n**PaperlessBilling**\n\n- Coefficient: 0.35\n- Odds Ratio: 1.42\n\n*The odds of a customer to get churned in case he/ she has opted for Paperless Billing are 1.42 higher than in case of Not opted for Paperless Billing, considering every other variable same.*\n*In terms of percentage change, the odds of customer with Paperless Billing getting churned is 42% higher than the odds of customer with not Paperless Billing getting churned.*\n\n**SeniorCitizen**\n\n- Coefficient: 0.47\n- Odds Ratio: 1.60\n\n*The odds of a Senior Citizen customer to get churned are 1.60 higher than in case of non-Senior Citizen, considering every other variable same.*\n*In terms of percentage change, the odds of a Senior Citizen customer getting churned is 60% higher than the odds of not Senior Citizen customer getting churned.*\n\n**Contract_One year**\n\n- Coefficient: -0.74\n- Odds Ratio: 0.48\n\n*The odds of a customer with One Year contract to get churned are 0.52 lower than in case of customer not having One Year contract, considering every other variable same.*\n*In terms of percentage change, the odds of a customer with One Year contract getting churned is 52% lesser than the odds of a customer not having One Year contract getting churned.*\n\n**Contract_Two year**\n\n- Coefficient: -1.31\n- Odds Ratio: 0.27\n\n*The odds of a customer with Two Year contract to get churned are 0.73 lower than in case of customer not having Two Year contract, considering every other variable same.*\n*In terms of percentage change, the odds of a customer with Two Year contract getting churned is 73% lesser than the odds of a customer not having Two Year contract getting churned.*\n\n**PaymentMethod_Credit card (automatic)**\n\n- Coefficient: -0.39\n- Odds Ratio: 0.68\n\n*The odds of a customer with Automatic Payment via Credit Card to get churned are 0.32 lower than in case of customer not having Automatic Payment via Credit Card, considering every other variable same.*\n*In terms of percentage change, the odds of a customer with Automatic Payment via Credit Card getting churned is 32% lesser than the odds of a customer not having Automatic Payment via Credit Card getting churned.*\n\n**PaymentMethod_Credit card (automatic)**\n\n- Coefficient: -0.39\n- Odds Ratio: 0.68\n\n*The odds of a customer with Automatic Payment via Credit Card to get churned are 0.32 lower than in case of customer not having Automatic Payment via Credit Card, considering every other variable same.*\n*In terms of percentage change, the odds of a customer with Automatic Payment via Credit Card getting churned is 32% lesser than the odds of a customer not having Automatic Payment via Credit Card getting churned.*\n\n**PaymentMethod_Mailed check**\n\n\n- Coefficient: -0.34\n- Odds Ratio: 0.71\n\n*The odds of a customer with have enabled Payment Method via Mail Check to get churned are 0.29 lower than in case of customer not enabled Payment Method via Mail Check, considering every other variable same.*\n*In terms of percentage change, the odds of a customer enabled Payment Method via Mail Check getting churned is 29% lesser than the odds of a customer not enabled Payment Method via Mail Check getting churned.*\n\n**InternetService_Fiber Optic**\n\n- Coefficient: 0.86\n- Odds Ratio: 2.37\n\n*The odds of a customer with having Fiber Optic service to get churned are 2.37 higher than in case of customer not having Fiber Optic service, considering every other variable same.*\n*In terms of percentage change, the odds of a customer having Fiber Optic service getting churned is 137% higher than the odds of a customer not having Fiber Optic service getting churned.*\n\n**InternetService_No**\n\n- Coefficient: -0.97\n- Odds Ratio: 0.38\n\n*The odds of a customer with Not having Interner Services to get churned are 0.62 lower than in case of customer having Interner Services, considering every other variable same.*\n*In terms of percentage change, the odds of a customer Not having Interner Services getting churned is 62% lesser than the odds of a customer having Interner Services getting churned.*\n\n**TechSupport_Yes**\n\n- Coefficient: -0.41\n- Odds Ratio: 0.67\n\n*The odds of a customer with having Tech Support to get churned are 0.33 lower than in case of customer not having Tech Support , considering every other variable same.*\n*In terms of percentage change, the odds of a customer having Tech Support  getting churned is 33% lesser than the odds of a customer not having Tech Support getting churned.*\n\n**StreamingTV_Yes**\n\n- Coefficient: 0.35\n- Odds Ratio: 1.41\n\n*The odds of a customer with Streaming TV services to get churned are 1.41 higher than in case of customer not having Streaming TV services, considering every other variable same.*\n*In terms of percentage change, the odds of a customer Streaming TV services getting churned is 41% higher than the odds of a customer not having Streaming TV services getting churned.*\n\n**StreamingMovies_Yes**\n\n- Coefficient: 0.25\n- Odds Ratio: 1.28\n\n*The odds of a customer with Streaming Movies services to get churned are 1.28 higher than in case of customer not having Streaming Movies services, considering every other variable same.*\n*In terms of percentage change, the odds of a customer Streaming Movies services getting churned is 28% higher than the odds of a customer not having Streaming Movies services getting churned.*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Step 10: Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We have completed all steps for solving a classification problem. We have seen that the model we built gives good accuracy score of 77% on the Training dataset and 74% on the Testing dataset along with other metrics. For this problem we preffered to use Sensitivity and Specificity metrics for the evauation. We have also seen impact of each variable on the probability of churn. Below are few observations about the model:\n\n- A customer with long term contracts like One year and Two Year are less likely to churn than the customer having Monthly contract.\n\n- A customer who is associated with the company from longer time is less likely to churn than a customer who is associated from few months. Reason can be the customer is happy with the services and wishes to continue with them.\n\n- Customer using Internet Services, Fiber Optics, Streaming TV and Movies servies are more likely to churn than customer who are not using these services. Reason can be company not providing good Internet services and need to work on that.\n\n- Customer who have opted for Payment Method through Credit Card or Mailed check are less likely to churn then other customers.\n\nOverall, company need to provide better internet services and other services associated with internet to retain their customers.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}