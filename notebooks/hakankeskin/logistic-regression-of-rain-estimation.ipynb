{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font color='red'>\n<br>Content:\n    \n* [Introduction](#1)\n* [Overview the Data Set](#2)\n* [Logistic Regression](#3)\n    * [Initializing parameters](#4)\n        * Sigmoid Function\n    * [Optimization Algorithm with Gradient Descent](#5)\n        * Forward & Backward Propagation\n        * Loss(error) Function\n        * Cost Function\n        * Updating parameters\n        * Comparison of Logistic Regression Results\n    * [KNN Algorithm](#6)\n* [Conclusion](#7)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n# Introduction\n\n* This dataset include estimation of rain for tomorrow in Austrilia. In this work, tomorrow rain possibility (1 or 0) results will learn then testing with logistic regression. After that, KNN Classification will apply to data to find out KNN Score and number of neighbors.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\n# filter warnings\nwarnings.filterwarnings('ignore')\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n# Overview the Data Set\n\n* Firstly we will start to understand data. Therefore, we will check data columns and types."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/weather-dataset-rattle-package/weatherAUS.csv\")\ndf.head()\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As you can see our dataset is occurred with 23 columns. These columns types can define as float and object. However, in this work we need to relationship data values that is occurred as float. Beucause of that some columns that have object data types will drop in this part to obtain train and test data frames. After the dropping, we will drop NaN values from to data. But, if you want you can fill NaN values with mean values of each columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf.drop([\"Date\", \"Location\",\"WindDir9am\",\"WindDir3pm\", \"WindGustDir\", \"RainToday\"], axis = 1, inplace = True)\ndf.dropna(inplace=True)\ndf.info()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In this work we need to two results for logictic regression in variable. This variable is called as \"RainTomorrow in this dataset. However, RainTomorrow columns are including \"Yes\" or \"No\". To apply logistic regression and KNN classification to data, we have to change our outputs with 0 or 1 to convert results as integer\n* Yes = 1\n* No = 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.RainTomorrow = [1 if each == \"Yes\" else 0 for each in df.RainTomorrow]\ndf.RainTomorrow.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n# Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n## Initilazing Parameters\n\n* Our aim to split result data and other datas that effect to result data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df.RainTomorrow.values\nx_data = df.drop([\"RainTomorrow\"], axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets normalize the dataset to pretend higher scaling factor between data features.\n\nx = (x_data - np.min(x_data))/(np.max(x_data) - np.min(x_data)).values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now, to learn our data we have to split dataset as train and test. Test data will be occured %20 of the all data."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 42)\n\nx_train = x_train.T\ny_train = y_train.T\nx_test = x_test.T\ny_test = y_test.T\n\nprint(\"x_train:\", x_train.shape)\nprint(\"y_train:\", y_train.shape)\nprint(\"x_test:\", x_test.shape)\nprint(\"y_test:\", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We know Python has library to find accuracy of the test data. However, firstly we want to find accuracy wind some functions that write by hand calculation methods."},{"metadata":{},"cell_type":"markdown","source":"### Sigmoid Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_weigths_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\n\nw,b = initialize_weigths_and_bias(5) # example\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(z):\n    y_head = 1/(1 + np.exp(-z))\n    return y_head\n\ny_head = sigmoid(0.9) # example\ny_head","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n## Optimization Algorithm with Gradient Descent\n\n* Now we will train our data with Forward and Backward propagation. It will help to update our data to find sufficent slope of the sigmoid function. If slope will going to 0, that means our data will learn as much as correctly. "},{"metadata":{},"cell_type":"markdown","source":"### Forward and Backward Propagation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward_backward_propagation(w,b,x_train,y_train):\n    #forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/x_train.shape[1]              # x_train.shape[1] is for scailing\n    \n    #backward propagation\n    derivative_weight = (np.dot(x_train, ((y_head-y_train).T)))/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    return cost,gradients\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loss & Cost Function & Update Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"def update(w,b,x_train,y_train,learning_rate, number_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    #updating learning parameters is number of iteration times\n    \n    for i in range(number_of_iteration):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        \n        #lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b- learning_rate * gradients[\"derivative_bias\"]\n        \n        if i % 10  == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iteration %i: %f\" %(i,cost))\n    # we update(learn) parameters weigths and bias\n    parameters = {\"weight\": w, \"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now we define the function to check prediction of the test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(w,b,x_test):\n    \n    z = sigmoid(np.dot(w.T,x_test)+b)\n    y_predict = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5:\n            y_predict[0,i] = 0\n        else:\n            y_predict[0,i] = 1\n            \n    return y_predict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Finally we reach to logistic regression function. Now, this function will learn our dataset with the other functions that we defined above. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,num_iterations):\n    dimension = x_train.shape[0]\n    w,b = initialize_weigths_and_bias(dimension)\n    \n    parameters, gradients, cost_list = update(w,b,x_train,y_train,learning_rate,num_iterations)\n    y_predict_test = predict(parameters[\"weight\"], parameters[\"bias\"], x_test)\n    \n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_predict_test - y_test)) * 100))\n    \nlogistic_regression(x_train,y_train,x_test,y_test,learning_rate = 3,num_iterations=1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Lets find our test accuracy with Python LogisticRegression method that based on sklearn library."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\n\nlr.fit(x_train.T, y_train.T)\n\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As you can see, our test accuracy results that found out with personal functions and LogisticRegression method.is so close."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n## KNN Algorithm\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN Algorithm of Data\n\nx_train = x_train.T\ny_train = y_train.T\nx_test = x_test.T\ny_test = y_test.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"x_train:\", x_train.shape)\nprint(\"y_train:\", y_train.shape)\nprint(\"x_test:\", x_test.shape)\nprint(\"y_test:\", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 25)\n\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\n\nprint(\" {} nn score: {}\".format(25,knn.score(x_test,y_test)))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How can we find best number of neighbors with for loop\n\nscore_list = []\n\nfor each in range(1,25):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,25), score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n# Conclusion\n\n* Our functional and LogisticRegression method results is find so close each other. We just present functional method to understand and show of the Logistic Regression methodology. After the this work, we can use LogisticRegression method that based on sklearn libray.\n\n* KNN Algorithm is applied with KNeighborsClassifier method. In this part, we have to decide number of neighbors of the algorithm. For this that, we used for loop to find the best number of neighbors that is given the best accuracy result.\n\n* The other Machine Learning and Deep Learning methodology will use in this notebook. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}