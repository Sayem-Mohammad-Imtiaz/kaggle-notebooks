{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Basic packages\nimport pandas as pd \nimport numpy as np\nimport re\nimport collections\nimport matplotlib.pyplot as plt\n\n# Packages for data preparation\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\n\n# Packages for modeling\nfrom keras import models\nfrom keras import layers\nfrom keras import regularizers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2102d1b79c75b8c7871d9d2f5b7dff7f9b157d29"},"cell_type":"code","source":"NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary\nVAL_SIZE = 1000  # Size of the validation set\nNB_START_EPOCHS = 20  # Number of epochs we usually start to train with\nBATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Loading"},{"metadata":{"trusted":true,"_uuid":"487201c5621cc484a91d8d9251aa5c9dbc30dc95"},"cell_type":"code","source":"df = pd.read_csv('../input/Tweets.csv')\ndf = df.reindex(np.random.permutation(df.index))  \ndf = df[['text', 'airline_sentiment']]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre"},{"metadata":{"trusted":true,"_uuid":"eb0f333b0828d5546eb4aa7736a000c9bea9350b"},"cell_type":"code","source":"def remove_stopwords(input_text):\n        stopwords_list = stopwords.words('english')\n        # Some words which might indicate a certain sentiment are kept via a whitelist\n        whitelist = [\"n't\", \"not\", \"no\"]\n        words = input_text.split() \n        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n        return \" \".join(clean_words) \n    \ndef remove_mentions(input_text):\n        return re.sub(r'@\\w+', '', input_text)\n       \ndf.text = df.text.apply(remove_stopwords).apply(remove_mentions)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train/Test Split"},{"metadata":{"trusted":true,"_uuid":"76d5820695a7aac44bce53f6022a87f4c6e6b7f0"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df.text, df.airline_sentiment, test_size=0.1, random_state=37)\nprint('# Train data samples:', X_train.shape[0])\nprint('# Test data samples:', X_test.shape[0])\nassert X_train.shape[0] == y_train.shape[0]\nassert X_test.shape[0] == y_test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenization"},{"metadata":{"trusted":true,"_uuid":"c9a65408d52c47e0c3d980e9b4b25d0d0c5c1bc2"},"cell_type":"code","source":"tk = Tokenizer(num_words=NB_WORDS,\n               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n               lower=True,\n               split=\" \")\ntk.fit_on_texts(X_train)\n\nprint('Fitted tokenizer on {} documents'.format(tk.document_count))\nprint('{} words in dictionary'.format(tk.num_words))\nprint('Top 5 most common words are:', collections.Counter(tk.word_counts).most_common(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49b2ab8dd60f1cbb6f0604ab0d00abbb156eb36b"},"cell_type":"code","source":"X_train_seq = tk.texts_to_sequences(X_train)\nX_test_seq = tk.texts_to_sequences(X_test)\n\nprint('\"{}\" is converted into {}'.format(X_train[0], X_train_seq[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"481fe1eb15e43b59ce33a898fb1afdc4acd6933f"},"cell_type":"code","source":"def one_hot_seq(seqs, nb_features = NB_WORDS):\n    ohs = np.zeros((len(seqs), nb_features))\n    for i, s in enumerate(seqs):\n        ohs[i, s] = 1.\n    return ohs\n\nX_train_oh = one_hot_seq(X_train_seq)\nX_test_oh = one_hot_seq(X_test_seq)\n\nprint('\"{}\" is converted into {}'.format(X_train_seq[0], X_train_oh[0]))\nprint('For this example we have {} features with a value of 1.'.format(X_train_oh[0].sum()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c56b7b134e708108260e05f9a73d1421d396b83c"},"cell_type":"code","source":"le = LabelEncoder()\ny_train_le = le.fit_transform(y_train)\ny_test_le = le.transform(y_test)\ny_train_oh = to_categorical(y_train_le)\ny_test_oh = to_categorical(y_test_le)\n\nprint('\"{}\" is converted into {}'.format(y_train[0], y_train_le[0]))\nprint('\"{}\" is converted into {}'.format(y_train_le[0], y_train_oh[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2decf9509ebc1fbf5690576cc768b624eba301e"},"cell_type":"code","source":"X_train_rest, X_valid, y_train_rest, y_valid = train_test_split(X_train_oh, y_train_oh, test_size=0.1, random_state=37)\n\nassert X_valid.shape[0] == y_valid.shape[0]\nassert X_train_rest.shape[0] == y_train_rest.shape[0]\n\nprint('Shape of validation set:',X_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3bca0c713abc9b4a6c547475e3faa289bfedfb6"},"cell_type":"markdown","source":"**Deep Learning**"},{"metadata":{"trusted":true,"_uuid":"44724e4345bec65b39a946a2868a96122aa4f38d"},"cell_type":"code","source":"base_model = models.Sequential()\nbase_model.add(layers.Dense(64, activation='relu', input_shape=(NB_WORDS,)))\nbase_model.add(layers.Dense(64, activation='relu'))\nbase_model.add(layers.Dense(3, activation='softmax'))\nbase_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5b8881e1cc8353c61afeb9b2952a39098f3cf54"},"cell_type":"code","source":"def deep_model(model):\n    model.compile(optimizer='rmsprop'\n                  , loss='categorical_crossentropy'\n                  , metrics=['accuracy'])\n    \n    history = model.fit(X_train_rest\n                       , y_train_rest\n                       , epochs=NB_START_EPOCHS\n                       , batch_size=BATCH_SIZE\n                       , validation_data=(X_valid, y_valid)\n                       , verbose=0)\n    \n    return history\nbase_history = deep_model(base_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"627704ab00ce189908887c499f8d939d4e3907e0"},"cell_type":"code","source":"def eval_metric(history, metric_name):\n    metric = history.history[metric_name]\n    val_metric = history.history['val_' + metric_name]\n\n    e = range(1, NB_START_EPOCHS + 1)\n\n    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n    plt.legend()\n    plt.show()\n\neval_metric(base_history, 'loss')\neval_metric(base_history, 'acc')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nThe results are not very encouraging. Validation error is increasing which shows poor fitting. We need to look into different parameters once again to find better feature representation and model for better solution.\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}