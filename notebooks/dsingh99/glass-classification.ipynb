{"cells":[{"metadata":{},"cell_type":"markdown","source":"Greetings to the kaggle community this is my first notebook contribution. \n\nMy approach to the problem:\n\n1. Checking for data imbalancing\n\n2. Removing outliers in data (if any)\n\n3. Finding correlation among data elements\n\n4. Glass is primarily composed of Si, I will prove it by plotting a graph\n\n5. Build classification models on dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import itertools\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\n\nfrom collections import Counter\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/glass/glass.csv')\nfeatures = data.columns[:-1].tolist()\ndata.head","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"214 rows and 10 columns\n\n# Data analysis\n\n## Descriptive analaysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Scale**: Not on the same scale, the mean is as high as 72.6 for Si and as low as 0.05 for Fe\n\n**Logistic Regression** may not converge smoothly\n\n## Null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No null data\n\n# Class imbalancing"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.subplot()\n\ng = sns.countplot(data.Type)\ng.set_xticklabels(['2', '1', '7', '3', '5', '6'])\ng.set_yticklabels(['0', '10', '20', '30', '40', '50', '60', '70'])\n\n# function to show values on bars\ndef show_values_on_bars(axs):\n    def _show_on_single_plot(ax):        \n        for p in ax.patches:\n            _x = p.get_x() + p.get_width() / 2\n            _y = p.get_y() + p.get_height()\n            value = '{:.0f}'.format(p.get_height())\n            ax.text(_x, _y, value, ha=\"center\") \n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)\nshow_values_on_bars(ax)\n\nsns.despine(left=True, bottom=True)\nplt.xlabel('Types')\nplt.ylabel('Count')\nplt.title('Visualizing class imbalancing', fontsize=30)\nplt.tick_params(axis = 'x', which='major', labelsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data is imbalanced.\n\n## Check for outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_outliers(data):\n    indices = []\n    \n    # iterate over columns\n    for col in data.columns.tolist():\n        Q1 = np.percentile(data[col], 25) # first quartile\n        Q3 = np.percentile(data[col], 75) # third quartile\n        outlier = 1.5 * (Q3 - Q1)\n        \n        outlier_list = data[(data[col] < Q1 - outlier) | (data[col] > Q3 + outlier)].index\n        \n        indices.extend(outlier_list)\n        \n    indices = Counter(indices)\n    multiple_outliers = list(k for k, v in indices.items() if v > 2)\n    \n    return multiple_outliers\n\nprint(len(count_outliers(data[features])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"14 fetaures with outliers!\n\n## Correlation among elements"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation = data[features].corr()\n#plt.figure(figsize = (8, 8)) -- too clumsy\nplt.figure(figsize = (16, 16))\nsns.heatmap(correlation, cbar = True, square = True, annot = True, fmt = '.2f', xticklabels = features, \n            yticklabels = features, alpha = 0.7, cmap = 'coolwarm')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:**\n\nOrder of correlation-\n\n1. **Ca-Ri = 0.81**\n\n2. Al-Ba = 0.48\n\n3. Na-Ba = 0.33\n\n# Data cleaning\n\n## Removing outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"indices = count_outliers(data[features])\ndata = data.drop(indices).reset_index(drop = True)\nprint(data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"200 rows and 10 columns. 14 rows removed!\n\n## Check class imbalancing again"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.subplot()\n\ng = sns.countplot(data.Type)\ng.set_xticklabels(['2', '1', '7', '3', '5', '6'])\ng.set_yticklabels(['0', '10', '20', '30', '40', '50', '60', '70'])\n\n# function to show values on bars\ndef show_values_on_bars(axs):\n    def _show_on_single_plot(ax):        \n        for p in ax.patches:\n            _x = p.get_x() + p.get_width() / 2\n            _y = p.get_y() + p.get_height()\n            value = '{:.0f}'.format(p.get_height())\n            ax.text(_x, _y, value, ha=\"center\") \n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)\nshow_values_on_bars(ax)\n\nsns.despine(left=True, bottom=True)\nplt.xlabel('Types')\nplt.ylabel('Count')\nplt.title('Visualizing class imbalancing', fontsize=30)\nplt.tick_params(axis = 'x', which='major', labelsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removal of classifiers has reduced imbalancing in type1. Now the data has two balanced classes of data, segrating the minority classes from majority classes.\n\n## Oversampling minority data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data7 = data[data['Type'] == 7]\ndata7 = pd.concat([data7] * 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data3 = data[data['Type'] == 3]\ndata3 = pd.concat([data3] * 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data5 = data[data['Type'] == 5]\ndata5 = pd.concat([data5] * 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data6 = data[data['Type'] == 6]\ndata6 = pd.concat([data6] * 7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = data[data['Type'] == 1]\ndata2 = data[data['Type'] == 2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([data1, data2, data3, data5, data6, data7])\nprint(data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A result of oversampling - number of rows increases from 200 to 364."},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.subplot()\n\ng = sns.countplot(data.Type)\ng.set_xticklabels(['2', '1', '7', '3', '5', '6'])\ng.set_yticklabels(['0', '10', '20', '30', '40', '50', '60', '70'])\n\n# function to show values on bars\ndef show_values_on_bars(axs):\n    def _show_on_single_plot(ax):        \n        for p in ax.patches:\n            _x = p.get_x() + p.get_width() / 2\n            _y = p.get_y() + p.get_height()\n            value = '{:.0f}'.format(p.get_height())\n            ax.text(_x, _y, value, ha=\"center\") \n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)\nshow_values_on_bars(ax)\n\nsns.despine(left=True, bottom=True)\nplt.xlabel('Types')\nplt.ylabel('Count')\nplt.title('Visualizing class imbalancing', fontsize=30)\nplt.tick_params(axis = 'x', which='major', labelsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not perfectly balanced out but enough to make models.\n\n## Correlation in cleaned data"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation = data[features].corr()\nplt.figure(figsize = (16, 16))\nsns.heatmap(correlation, cbar = True, square = True, annot = True, fmt = '.2f', xticklabels = features, \n            yticklabels = features, alpha = 0.7, cmap = 'coolwarm')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:**\n\nOrder of correlation-\n\n1. **Ca-Ri = 0.72** (decreases after data cleaning)\n\n2. Al-Ba = 0.49 (increases after data cleaning)\n\n3. Na-Ba = 0.44 (increases after data cleaning)\n\n## Primary constituent of glass"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This proves Silicon is the primary element of glass\n\n# Making classification models\n\n## Model1- Decision Tree Classifier (entropy)"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data.iloc[:, :-1].values # input attributes\ny = data.iloc[:, -1].values # target attributes\n\n# split data into train and test\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0) # 80% train data, 20% test data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_clf = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\ndt_clf.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_predict = dt_clf.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for confusion matrix\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    \n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy score: ', accuracy_score(y_test, dt_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_cm = confusion_matrix(y_test, dt_predict)\nplot_confusion_matrix(dt_cm, [2, 1, 7, 3, 5, 6])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model2- Decision Tree Classifier (gini)"},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_clf = DecisionTreeClassifier(criterion = 'gini', random_state = 0)\ndt_clf.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_predict = dt_clf.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy score: ', accuracy_score(y_test, dt_predict))\ndt_cm = confusion_matrix(y_test, dt_predict)\nplot_confusion_matrix(dt_cm, [2, 1, 7, 3, 5, 6])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision tree classifier based on 'Entropy' is a better classifier compared to 'Gini'.\n\nPerformance of decision tree model is good, using **ensemble techniques** in attempt to improve the performance.\n\n## Model3 - Random Forest Classifier (Gini)"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_classifier = RandomForestClassifier(n_estimators = 500, criterion = 'gini', random_state = 0)\nrf_classifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_predict = rf_classifier.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy score: ', accuracy_score(y_test, dt_predict))\ndt_cm = confusion_matrix(y_test, dt_predict)\nplot_confusion_matrix(dt_cm, [2, 1, 7, 3, 5, 6])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest with Gini gives same results as Decision Tree with Gini\n\n## Model4 - Random Forest Classifier (Entropy)"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_classifier = RandomForestClassifier(n_estimators = 500, criterion = 'entropy', random_state = 0)\nrf_classifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_predict = rf_classifier.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy score: ', accuracy_score(y_test, dt_predict))\ndt_cm = confusion_matrix(y_test, dt_predict)\nplot_confusion_matrix(dt_cm, [2, 1, 7, 3, 5, 6])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No difference in performance for random forest based on criteria. **The used ensemble technique did not improve the performance of decison tree**. Using another classification algorithm\n\n## Model5 - K Nearest Neighbor Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# finding the best K\n\nscore = []\n\nfor k in range(32):\n    knn = KNeighborsClassifier(k)\n    score_val = cross_val_score(knn, x_train, y_train, scoring = 'accuracy', cv = 10)\n    score_mean = score_val.mean()\n    score.append(score_mean)\n\nbest_K = np.argmax(score) + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(best_K)\nknn.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_predict = knn.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy score: ', accuracy_score(y_test, knn_predict))\nknn_cm = confusion_matrix(y_test, knn_predict)\nplot_confusion_matrix(knn_cm, [2, 1, 7, 3, 5, 6])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the confusion matrix it can be concluded that Decision tree is a better classifier compared to KNeighborClassifier\n\n**Decision tree (entropy)** is the best classifier model having an accuracy of 89.04 percent"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}