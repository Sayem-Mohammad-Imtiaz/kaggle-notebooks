{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# I am going to predict mobile price according to it's feature.  Random Forest Classifier and Gradient Boosting Classifier  ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/mobile-price-classification/train.csv')\ntest_data = pd.read_csv('/kaggle/input/mobile-price-classification/test.csv')\ntrain_data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check what features are mostly correlated with the price.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('fivethirtyeight')\n\nplt.figure(figsize=(16,8))\nsns.heatmap(train_data.corr(),annot=True, fmt='.0%')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"getting only the positive correlated features\n\nbettery_power,\nblue,\ndual_sim,\nfour_g,\nint_memory,\npc,\npx_height,\npx_width,\nram,\nsc_h,\nsc_w,\ntalk_time,\nthree_g,\nwifi,\n","metadata":{}},{"cell_type":"code","source":"filtered_data = train_data[['battery_power', 'blue', 'dual_sim', 'four_g', 'int_memory', 'pc', 'px_height', 'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time', 'three_g', 'wifi', 'price_range']]\nprint('Filter data shape: {}'.format(filtered_data.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Split the data intp X and y. We need to remove price range from the X and keep only the price range in the y**","metadata":{}},{"cell_type":"code","source":"X = filtered_data.iloc[:, 0:-1]\nX","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = filtered_data.iloc[:, -1:]\ny","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state= 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(max_features = 5, n_estimators=100, random_state = 0,)\nclf.fit(X_train, y_train.values.ravel())\n\npred_random_Forest = clf.predict(X_test)\n\nprint('Accuracy score: {}'.format(accuracy_score(y_test, pred_random_Forest)))\nprint('Accuracy of RF classifier on training set: {:.2f}'\n     .format(clf.score(X_train, y_train)))\nprint('Accuracy of RF classifier on test set: {:.2f}'\n     .format(clf.score(X_test, y_test)))\nprint('y_test and pred: {}'.format(y_test[:20]))\nprint('pred: {}'.format(pred_random_Forest))\n\nreportLReg = classification_report(y_test, pred_random_Forest, output_dict=True)\ncrLReg = pd.DataFrame(reportLReg).transpose()\n\ncrLReg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\nclf_gr = GradientBoostingClassifier(n_estimators= 100,learning_rate =0.01, max_depth = 3, random_state = 0)\nclf_gr.fit(X_train, y_train.values.ravel())\n\npred_gr = clf_gr.predict(X_test)\n\nprint('(learning_rate=0.01, max_depth=2)')\nprint('Accuracy of GBDT classifier on training set: {:.2f}'\n     .format(clf_gr.score(X_train, y_train)))\nprint('Accuracy of GBDT classifier on test set: {}'\n     .format(clf_gr.score(X_test, y_test)))\nprint('Accuracy score: {}'.format(accuracy_score(y_test, pred_gr)))\n\nprint('y_test and pred: {}'.format(y_test[:20]))\nprint('pred: {}'.format(pred_gr))\n\nreportLReg_gf = classification_report(y_test, pred_gr, output_dict=True)\ncrLReg_gf = pd.DataFrame(reportLReg).transpose()\n\ncrLReg_gf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Gradient Boosting Classifier has best model fit than the Random Forest Classifier. Random Forest Classifier is overfitting.\n","metadata":{}},{"cell_type":"markdown","source":"Lest's predict the price for test data.\n","metadata":{}},{"cell_type":"code","source":"#remove uncorrelated data from the test dataset and keep positive correlated features\nfiltered_data = test_data[['battery_power', 'blue', 'dual_sim', 'four_g', 'int_memory', 'pc', 'px_height', 'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time', 'three_g', 'wifi']]\nfiltered_data.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_for_test_dataset_by_random_forest = clf.predict(filtered_data)\n\nrf_predicted_data = test_data\n#prediced data by random forest\nrf_predicted_data['Price Range'] = predict_for_test_dataset_by_random_forest\n\nrf_predicted_data.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predicted with GradientBoostingClassifier\ngc_predicted_data = test_data\npred_gc = clf_gr.predict(filtered_data)\ngc_predicted_data['price range'] = pred_gc\n\ngc_predicted_data.head(10)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nLeave a comment!\nthank you!","metadata":{}}]}