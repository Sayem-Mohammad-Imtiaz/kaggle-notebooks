{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this kernel I have performed Exploratory Data Analysis on the **Red Wine Quality** dataset and tried to identify relationship between heart the quality of wine and various other features. After EDA data pre-processing is done I have applied **k-NN(k-Nearest Neighbors)**,  **Logistic Regression**  and **Decision Tree** Algorithm to make the predictions. I will use various other algorithms for predictions in future and add them in this kernel.\n\nI hope you find this kernel helpful and some **<font color='red'>UPVOTES</font>** would be very much appreciated"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":false},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"### Importing required libraries"},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n%matplotlib inline\n\n# setting plot style for all the plots\nplt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading the data"},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dimensions of the dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Number of rows in the dataset: ',df.shape[0])\nprint('Number of columns in the dataset: ',df.shape[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Features in the data set"},{"metadata":{"trusted":false},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Basic statistical details about the dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"df.describe().round(decimals=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The features described in the above data set are:**\n\n**1. Count** tells us the number of NoN-empty rows in a feature.\n\n**2. Mean** tells us the mean value of that feature.\n\n**3. Std** tells us the Standard Deviation Value of that feature.\n\n**4. Min** tells us the minimum value of that feature.\n\n**5. 25%, 50%, and 75%** are the percentile/quartile of each features.\n\n**6. Max** tells us the maximum value of that feature."},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis(EDA)"},{"metadata":{},"cell_type":"markdown","source":"### 1. Number of wines of a given quality in the dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.countplot(x='quality', data=df)\nplt.title('Number of wines present in the dataset of a given quality')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting the relationship between quality of wine and various other features"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Function to plot barplot and boxplot of a given feature\ndef plot(x_val, y_val, palette='pastel'):\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n    sns.barplot(x=x_val, y=y_val, data=df, ax=ax[0], palette=palette)\n    sns.boxplot(x= x_val, y= y_val, data=df, ax=ax[1],palette=palette, linewidth=3)\n    plt.tight_layout(w_pad=2)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Fixed Acidity vs. Quality"},{"metadata":{"trusted":false},"cell_type":"code","source":"plot('quality','fixed acidity')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Volatile Acidity vs. Quality"},{"metadata":{"trusted":false},"cell_type":"code","source":"plot('quality', 'volatile acidity')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Citric Acid vs. Quality"},{"metadata":{"trusted":false},"cell_type":"code","source":"plot('quality', 'citric acid')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Residual Sugar vs. Quality"},{"metadata":{"trusted":false},"cell_type":"code","source":"plot('quality', 'residual sugar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Chlorides vs. Quality"},{"metadata":{"trusted":false},"cell_type":"code","source":"plot('quality', 'chlorides')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. Correlation Heatmap between various features"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12,8))\ncorr = df.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr,mask=mask, annot=True, linewidths=1, cmap='YlGnBu')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preporcessing the data before applying Machine Learning algorithms"},{"metadata":{},"cell_type":"markdown","source":"### 1. Dividing the wine quality as good or bad to make it a binary classification problem"},{"metadata":{"trusted":false},"cell_type":"code","source":"bins = (2, 6.5, 8)\ngroup_names = ['bad', 'good']\ndf['quality'] = pd.cut(df['quality'], bins = bins, labels = group_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The quality column in the dataset now has only two values i.e. good and bad."},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Number of good and bad quality wines in the dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(7,6))\nsns.countplot(x='quality', data=df, palette='pastel')\nplt.title('Number of good and bad quality wines')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Assigning a label(numerical value) to the quality variable."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"label_encoder = LabelEncoder()\ndf['quality'] = label_encoder.fit_transform(df['quality'])\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The **'quality'** column now contains values 0 and 1. Although Label encoder assigns incremental values i.e 1, 2, 3, 4, ... it can be used here in place of OneHot Encoder since there are only two values in the quality column."},{"metadata":{},"cell_type":"markdown","source":"## Implementing Machine Learing Algorithms"},{"metadata":{},"cell_type":"markdown","source":"### 1. Splitting the features and target variables"},{"metadata":{"trusted":false},"cell_type":"code","source":"X = df.drop('quality', axis=1)\ny = df['quality']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Scaling the features"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import scale","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_scaled = scale(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Splitting the dataset into Training and Testing sets"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, stratify=y, random_state=41)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Applying ML Algorithms"},{"metadata":{},"cell_type":"markdown","source":"### i. K Nearest Neighbors"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"knn = KNeighborsClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"params = {\n    'n_neighbors':list(range(1,15)),\n    'p':[1, 2, 3, 4],\n    'leaf_size':list(range(1,50)),\n    'weights':['uniform', 'distance']\n}","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# Doing Gridsearch to find optimal parameters\nknn_grid = GridSearchCV(estimator=knn, param_grid=params, scoring='accuracy',cv=5,n_jobs=-1)\nknn_grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Best parameters for the model"},{"metadata":{"trusted":false},"cell_type":"code","source":"knn_grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Best score for the model"},{"metadata":{"trusted":false},"cell_type":"code","source":"knn_grid.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Making predictions"},{"metadata":{"trusted":false},"cell_type":"code","source":"knn_predict = knn_grid.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Accuracy"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import accuracy_score,confusion_matrix\nprint('Accuracy Score: ',accuracy_score(y_test,knn_predict))\nprint('Using k-NN we get an accuracy score of: ',\n      round(accuracy_score(y_test,knn_predict),5)*100,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion Matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n\n# Fucntion to create confusion Matrix\ndef conf_matrix(actual, predicted, model_name):\n    cnf_matrix = confusion_matrix(actual, predicted)\n#     cnf_matrix\n    class_names = [0,1]\n    fig,ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks,class_names)\n    plt.yticks(tick_marks,class_names)\n\n    #create a heat map\n    sns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'YlGnBu',\n               fmt = 'g')\n    ax.xaxis.set_label_position('top')\n    plt.tight_layout()\n    plt.title('Confusion matrix for ' + model_name + ' Model', y = 1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"conf_matrix(y_test, knn_predict, 'k-Nearest Neighbors')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Classification report"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_test, knn_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Receiver Operating Characterstic(ROC) Curve"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score,roc_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_probabilities = knn_grid.predict_proba(X_test)[:,1]\n\n#Create true and false positive rates\nfalse_positive_rate_knn,true_positive_rate_knn,threshold_knn = roc_curve(y_test,y_probabilities)\n\n#Plot ROC Curve\nplt.figure(figsize=(10,6))\nplt.title('Revceiver Operating Characterstic')\nplt.plot(false_positive_rate_knn,true_positive_rate_knn, linewidth=2)\nplt.plot([0,1],ls='--', linewidth=2)\nplt.plot([0,0],[1,0],c='.5', linewidth=2)\nplt.plot([1,1],c='.5',linewidth=2)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Calculate area under the curve\nroc_auc_score(y_test,y_probabilities)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ii. Logistic Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"logreg = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"params = {'C':[0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n             'class_weight': [{1:0.5, 0:0.5}, {1:0.4, 0:0.6},{1:0.6, 0:0.4}, {1:0.7, 0:0.3},{1:0.3, 0:0.7}],\n             'penalty': ['l1', 'l2'],\n             'solver': ['liblinear', 'saga'],\n             'max_iter':[50,100,150,200]\n             }","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# Doing Gridsearch to find optimal parameters\nlog_grid = GridSearchCV(estimator=logreg, param_grid=params, scoring='accuracy', cv=5, n_jobs=-1)\nlog_grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Best parameters for the model"},{"metadata":{"trusted":false},"cell_type":"code","source":"log_grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Best score for the model"},{"metadata":{"trusted":false},"cell_type":"code","source":"log_grid.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Making predictions"},{"metadata":{"trusted":false},"cell_type":"code","source":"log_predict = log_grid.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Accuracy"},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Accuracy Score: ',accuracy_score(y_test,log_predict))\nprint('Using k-NN we get an accuracy score of: ',\n      round(accuracy_score(y_test,log_predict),5)*100,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion Matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"conf_matrix(y_test, log_predict, 'Logistic Regression')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Classification report"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_test, knn_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Receiver Operating Characterstic(ROC) Curve"},{"metadata":{"trusted":false},"cell_type":"code","source":"y_probabilities = log_grid.predict_proba(X_test)[:,1]\n\n#Create true and false positive rates\nfalse_positive_rate_log,true_positive_rate_log,threshold_log = roc_curve(y_test,y_probabilities)\n\n#Plot ROC Curve\nplt.figure(figsize=(10,6))\nplt.title('Revceiver Operating Characterstic')\nplt.plot(false_positive_rate_log,true_positive_rate_log, linewidth=2)\nplt.plot([0,1],ls='--', linewidth=2)\nplt.plot([0,0],[1,0],c='.5', linewidth=2)\nplt.plot([1,1],c='.5', linewidth=2)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Calculate area under the curve\nroc_auc_score(y_test,y_probabilities)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### iii. Decision Tree"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dt = DecisionTreeClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"param_grid = {\n    'criterion': ['gini','entropy'],\n    'max_depth': [None, 1, 2, 3, 4, 5, 6],\n    'max_features': ['auto', 'sqrt','log2'],\n    'max_leaf_nodes': [None, 1, 2, 3, 4, 5, 6],\n    'min_samples_leaf': [1,2,3,4,5,6,7],\n    'min_samples_split': [2,3,4,5,6,7,8,9,10]\n}","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# Doing Gridsearch to find optimal parameters\ndt_grid = GridSearchCV(estimator=dt, param_grid=param_grid, scoring='accuracy',cv=5, n_jobs=-1)\ndt_grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Best parameters for the model"},{"metadata":{"trusted":false},"cell_type":"code","source":"dt_grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Best score for the model"},{"metadata":{"trusted":false},"cell_type":"code","source":"dt_grid.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Making predictions"},{"metadata":{"trusted":false},"cell_type":"code","source":"dt_predict = dt_grid.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Accuracy"},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Accuracy Score: ',accuracy_score(y_test,dt_predict))\nprint('Using Decision Tree Classifier we get an accuracy score of: ',\n      round(accuracy_score(y_test,dt_predict),5)*100,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion Matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"conf_matrix(y_test, log_predict, 'Decision Tree')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Classification report"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_test, dt_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Receiver Operating Characterstic(ROC) Curve"},{"metadata":{"trusted":false},"cell_type":"code","source":"y_probabilities = dt_grid.predict_proba(X_test)[:,1]\n\n#Create true and false positive rates\nfalse_positive_rate_dt,true_positive_rate_dt,threshold_dt = roc_curve(y_test,y_probabilities)\n\n#Plot ROC Curve\nplt.figure(figsize=(10,6))\nplt.title('Revceiver Operating Characterstic')\nplt.plot(false_positive_rate_dt,true_positive_rate_dt, linewidth=2)\nplt.plot([0,1],ls='--', linewidth=2)\nplt.plot([0,0],[1,0],c='.5', linewidth=2)\nplt.plot([1,1],c='.5', linewidth=2)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Calculate area under the curve\nroc_auc_score(y_test,y_probabilities)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comparing ROC Curve of k-Nearest Neighbors, Logistic Regression and Decision Tree\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plot ROC Curve\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(false_positive_rate_knn,true_positive_rate_knn,linewidth=2, label='k-Nearest Neighbor')\nplt.plot(false_positive_rate_log,true_positive_rate_log, linewidth=2, label='Logistic Regression')\nplt.plot(false_positive_rate_dt,true_positive_rate_dt, linewidth=2, label='Decision Tree')\nplt.plot([0,1],ls='--', linewidth=2)\nplt.plot([0,0],[1,0],c='.5', linewidth=2)\nplt.plot([1,1],c='.5', linewidth=2)\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**What's next?**\n1. Applying SVM and Random Forest Algorithms\n2. Applying various ensemble methods such as bagging, boosting.\n3. Compare the models on the basis of their accuracy score."},{"metadata":{},"cell_type":"markdown","source":"**Suggestions are welcome**\n\n**<font color='red'>UPVOTE</font>** if you found the notebook helpful."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}