{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split,KFold,GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score,confusion_matrix,precision_recall_curve,auc,roc_auc_score,\\\nroc_curve,recall_score,classification_report ,f1_score,precision_score\nfrom sklearn.svm import SVC\n\nplt.style.use('fivethirtyeight')\nimport itertools\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration & Prepration\n"},{"metadata":{},"cell_type":"markdown","source":"#### Missing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_cols=(df.isnull().sum()/df.shape[0])*100\nmissing_cols= missing_cols[missing_cols>0]\nmissing_cols.plot(kind='bar')\nplt.xlabel('Features')\nplt.ylabel('Missing Percentage %')\nplt.title('Missing Data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some random column \" Unamed : 32 \" got added , dropping as everything is NaN"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.id.unique().shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation :\n\n- Id Column has all unique values , no pattern\n- Unamed : 32 column has all NaN \n- Dropping these columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop=['id','Unnamed: 32']\ndf.drop(cols_to_drop,axis=1,inplace=True)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Target Feature\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(12,3))\ndf['diagnosis'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('diagnosis')\nax[0].set_ylabel('')\nsns.countplot('diagnosis',data=df,ax=ax[1])\nax[1].set_title('classLabel')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n    - balanced dataset\n    - Class distribution: 357 benign, 212 malignant"},{"metadata":{},"cell_type":"markdown","source":"#### Variable Identification\n- Variable DataType : Numerical or Categorical"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### LabelEncoding target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['diagnosis'] = df['diagnosis'].map({'M':1,'B':0})\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- All features are in different scale , we need to make them comparable"},{"metadata":{},"cell_type":"markdown","source":"### Univariate Analysis\n- Histograms\n- Boxplots\n\nhighlight missing and outlier values"},{"metadata":{},"cell_type":"markdown","source":"#### Starting with mean  features"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = df.iloc[:,1:11].columns\nlength  = len(columns)\ncolors  = [\"r\",\"g\",\"b\",\"m\",\"y\",\"c\",\"k\",\"orange\",'pink','brown'] \n\nplt.figure(figsize=(15,20))\nfor i,j,k in itertools.zip_longest(columns,range(length),colors):\n    #plt.subplot(length/3,length/2,j+1)\n    plt.subplot(5,3,j+1)\n    #print(length/2,length/3,j+1)\n    sns.distplot(df.iloc[:,1:11][i],color=k)\n    #sns.boxplot(df_num[i+1],color=k)\n    plt.title(i)\n    plt.subplots_adjust(hspace = .3)\n    plt.axvline(df.iloc[:,1:11][i].mean(),color = \"k\",linestyle=\"dashed\",label=\"MEAN\")\n    plt.axvline(df.iloc[:,1:11][i].std(),color = \"b\",linestyle=\"dotted\",label=\"STANDARD DEVIATION\")\n    plt.legend(loc=\"upper right\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Boxplots Mean Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = df.iloc[:,1:11].columns\nlength  = len(columns)\ncolors  = [\"r\",\"g\",\"b\",\"m\",\"y\",\"c\",\"k\",\"orange\",'pink','brown'] \n\nplt.figure(figsize=(15,20))\nfor i,j,k in itertools.zip_longest(columns,range(length),colors):\n    plt.subplot(length/2,length/3,j+1)\n    sns.boxplot(df.iloc[:,1:11][i],color=k)\n    plt.title(i)\n    plt.subplots_adjust(hspace = .4)\n    #plt.axvline(df_num[i].mean(),color = \"k\",linestyle=\"dashed\",label=\"MEAN\")\n    #plt.axvline(df_num[i].std(),color = \"b\",linestyle=\"dotted\",label=\"STANDARD DEVIATION\")\n    plt.legend(loc=\"upper right\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Looking at distribution of SE variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = df.iloc[:,11:21].columns\nlength  = len(columns)\ncolors  = [\"r\",\"g\",\"b\",\"m\",\"y\",\"c\",\"k\",\"orange\",'pink','brown'] \n\nplt.figure(figsize=(15,20))\nfor i,j,k in itertools.zip_longest(columns,range(length),colors):\n    #plt.subplot(length/3,length/2,j+1)\n    plt.subplot(5,3,j+1)\n    #print(length/2,length/3,j+1)\n    sns.distplot(df.iloc[:,11:21][i],color=k)\n    #sns.boxplot(df_num[i+1],color=k)\n    plt.title(i)\n    plt.subplots_adjust(hspace = .3)\n    plt.axvline(df.iloc[:,11:21][i].mean(),color = \"k\",linestyle=\"dashed\",label=\"MEAN\")\n    plt.axvline(df.iloc[:,11:21][i].std(),color = \"b\",linestyle=\"dotted\",label=\"STANDARD DEVIATION\")\n    plt.legend(loc=\"upper right\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Boxplot SE Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = df.iloc[:,11:21].columns\nlength  = len(columns)\ncolors  = [\"r\",\"g\",\"b\",\"m\",\"y\",\"c\",\"k\",\"orange\",'pink','brown'] \n\nplt.figure(figsize=(15,20))\nfor i,j,k in itertools.zip_longest(columns,range(length),colors):\n    plt.subplot(length/2,length/3,j+1)\n    sns.boxplot(df.iloc[:,11:21][i],color=k)\n    plt.title(i)\n    plt.subplots_adjust(hspace = .4)\n    #plt.axvline(df_num[i].mean(),color = \"k\",linestyle=\"dashed\",label=\"MEAN\")\n    #plt.axvline(df_num[i].std(),color = \"b\",linestyle=\"dotted\",label=\"STANDARD DEVIATION\")\n    plt.legend(loc=\"upper right\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Largest Value features"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = df.iloc[:,21:].columns\nlength  = len(columns)\ncolors  = [\"r\",\"g\",\"b\",\"m\",\"y\",\"c\",\"k\",\"orange\",'pink','brown'] \n\nplt.figure(figsize=(15,20))\nfor i,j,k in itertools.zip_longest(columns,range(length),colors):\n    #plt.subplot(length/3,length/2,j+1)\n    plt.subplot(5,3,j+1)\n    #print(length/2,length/3,j+1)\n    sns.distplot(df.iloc[:,21:][i],color=k)\n    #sns.boxplot(df_num[i+1],color=k)\n    plt.title(i)\n    plt.subplots_adjust(hspace = .3)\n    plt.axvline(df.iloc[:,21:][i].mean(),color = \"k\",linestyle=\"dashed\",label=\"MEAN\")\n    plt.axvline(df.iloc[:,21:][i].std(),color = \"b\",linestyle=\"dotted\",label=\"STANDARD DEVIATION\")\n    plt.legend(loc=\"upper right\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Boxplot Wrost Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = df.iloc[:,21:].columns\nlength  = len(columns)\ncolors  = [\"r\",\"g\",\"b\",\"m\",\"y\",\"c\",\"k\",\"orange\",'pink','brown'] \n\nplt.figure(figsize=(15,20))\nfor i,j,k in itertools.zip_longest(columns,range(length),colors):\n    plt.subplot(length/2,length/3,j+1)\n    sns.boxplot(df.iloc[:,21:][i],color=k)\n    plt.title(i)\n    plt.subplots_adjust(hspace = .4)\n    #plt.axvline(df_num[i].mean(),color = \"k\",linestyle=\"dashed\",label=\"MEAN\")\n    #plt.axvline(df_num[i].std(),color = \"b\",linestyle=\"dotted\",label=\"STANDARD DEVIATION\")\n    plt.legend(loc=\"upper right\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inference :\n    - All features are normally distributed.\n    - No significant outliers."},{"metadata":{},"cell_type":"markdown","source":"#### Mean Values distribution w.r.t Target Varaible"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = df.iloc[:,1:11].columns\nlength  = len(columns)\n#colors  = [\"r\",\"g\",\"b\",\"m\",\"y\",\"c\",\"k\",\"orange\"] \n\nplt.figure(figsize=(15,20))\nfor i,j in itertools.zip_longest(columns,range(length)):\n    plt.subplot(length/2,length/3,j+1)\n    plt.hist(x = [df[df['diagnosis']==1][i], df[df['diagnosis']==0][i]], \n         stacked=True, color = ['brown','green'],label = ['M','B'])\n    plt.title(i)\n    plt.subplots_adjust(hspace = .3)\n    #plt.axvline(df_num[i].mean(),color = \"k\",linestyle=\"dashed\",label=\"MEAN\")\n    #plt.axvline(df_num[i].std(),color = \"b\",linestyle=\"dotted\",label=\"STANDARD DEVIATION\")\n    plt.legend(loc=\"upper right\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations\n- mean values of radius, perimeter, area, compactness and concave points can be used in classification of the cancer. Larger values of these parameters tends to show a correlation with malignant tumors.\n- Other does not show a particular preference of one diagnosis over the other. Overlapping Distributions."},{"metadata":{},"cell_type":"markdown","source":"#### SE features w.r.t target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = df.iloc[:,11:21].columns\nlength  = len(columns)\n#colors  = [\"r\",\"g\",\"b\",\"m\",\"y\",\"c\",\"k\",\"orange\"] \n\nplt.figure(figsize=(15,20))\nfor i,j in itertools.zip_longest(columns,range(length)):\n    plt.subplot(length/2,length/3,j+1)\n    plt.hist(x = [df[df['diagnosis']==1][i], df[df['diagnosis']==0][i]], \n         stacked=True, color = ['brown','green'],label = ['M','B'])\n    plt.title(i)\n    plt.subplots_adjust(hspace = .3)\n    #plt.axvline(df_num[i].mean(),color = \"k\",linestyle=\"dashed\",label=\"MEAN\")\n    #plt.axvline(df_num[i].std(),color = \"b\",linestyle=\"dotted\",label=\"STANDARD DEVIATION\")\n    plt.legend(loc=\"upper right\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Obsertvations :\n    \n- se values of Concave points can also be used in classification of the cancer.Smaller values of these parameters tends to show a correlation with benign tumors.\n- Other does not show a particular preference of one diagnosis over the other. Overlapping Distributions."},{"metadata":{},"cell_type":"markdown","source":"#### Wrost type features w.r.t target variable \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = df.iloc[:,21:].columns\nlength  = len(columns)\n#colors  = [\"r\",\"g\",\"b\",\"m\",\"y\",\"c\",\"k\",\"orange\"] \n\nplt.figure(figsize=(15,20))\nfor i,j in itertools.zip_longest(columns,range(length)):\n    plt.subplot(length/2,length/3,j+1)\n    plt.hist(x = [df[df['diagnosis']==1][i], df[df['diagnosis']==0][i]], \n         stacked=True, color = ['brown','green'],label = ['M','B'])\n    plt.title(i)\n    plt.subplots_adjust(hspace = .3)\n    #plt.axvline(df_num[i].mean(),color = \"k\",linestyle=\"dashed\",label=\"MEAN\")\n    #plt.axvline(df_num[i].std(),color = \"b\",linestyle=\"dotted\",label=\"STANDARD DEVIATION\")\n    plt.legend(loc=\"upper right\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations\n- wrost values of radius, perimeter, area, compactness and concave points can be used in classification of the cancer. Larger values of these parameters tends to show a correlation with malignant tumors.\n- Other does not show a particular preference of one diagnosis over the other. Overlapping Distributions."},{"metadata":{},"cell_type":"markdown","source":"**Observation Summary**\n- Mean features to be used radius, perimeter, area, compactness and concave points ( 5 features)\n- SE features to be used concave points ( 1 features)\n- Wrost features to be used radius, perimeter, area, compactness and concave points. (5 features)\n- 11 features "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_use=['diagnosis', 'radius_mean',  'perimeter_mean',\\\n       'area_mean', 'compactness_mean', \\\n       'concave points_mean','concave points_se','radius_worst','perimeter_worst', 'area_worst','compactness_worst','concave points_worst']\ndf1=df[cols_to_use]\ndf1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation = df1.corr()\nplt.figure(figsize=(9,7))\nsns.heatmap(correlation,annot=True,edgecolor=\"k\",cmap=sns.color_palette(\"magma\"))\nplt.title(\"CORRELATION BETWEEN VARIABLES\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation \n- High correlation between mean features & wrost features\n- High correlation within mean features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.scatter(x=df1['radius_mean'],y=df['radius_worst'],)\nsns.scatterplot(x=df1['radius_mean'], y=df1['radius_worst'], hue=df.diagnosis)\nplt.title('Scatter radius_mean vs radius_wrost')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.scatter(x=df1['radius_mean'],y=df['radius_worst'],)\nsns.scatterplot(x=df1['radius_mean'], y=df1['perimeter_worst'], hue=df.diagnosis)\nplt.title('Scatter radius_mean vs perimeter_worst')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.scatter(x=df1['radius_mean'],y=df['radius_worst'],)\nsns.scatterplot(x=df1['radius_mean'], y=df1['area_worst'], hue=df.diagnosis)\nplt.title('Scatter radius_mean vs area_worst')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.scatter(x=df1['radius_mean'],y=df['radius_worst'],)\nsns.scatterplot(x=df1['concave points_mean'], y=df1['concave points_worst'], hue=df.diagnosis)\nplt.title('Scatter concave points_mean vs concave points_worst')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.scatter(x=df1['radius_mean'],y=df['radius_worst'],)\nsns.scatterplot(x=df1['radius_mean'], y=df1['perimeter_mean'], hue=df.diagnosis)\nplt.title('Scatter radius_mean vs perimeter_mean')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.scatter(x=df1['radius_mean'],y=df['radius_worst'],)\nsns.scatterplot(x=df1['radius_mean'], y=df1['area_mean'], hue=df.diagnosis)\nplt.title('Scatter radius_mean vs area_mean')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dropping  correlated features**\n- Dropping radius_worst,perimeter_worst,area_worst,compactness_worst,concave points_worst"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_use=['diagnosis', 'radius_mean',  \\\n       'compactness_mean', \\\n       'concave points_se']\ndf2=df[cols_to_use]\ndf2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation = df2.corr()\nplt.figure(figsize=(9,7))\nsns.heatmap(correlation,annot=True,edgecolor=\"k\",cmap=sns.color_palette(\"magma\"))\nplt.title(\"CORRELATION BETWEEN VARIABLES\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"y=df2.diagnosis\nX=df[['radius_mean','compactness_mean','concave points_se']]\nX.shape,y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalization\nscaler = StandardScaler()\nX = scaler.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Splitting Data into training & Testing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regresion"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(X_train,y_train)\nprediction1=model.predict(X_test)\nprint(\"__\"*50,\"\\n\")\nprint('The accuracy of the Logistic Regression is',accuracy_score(y_test,prediction1))\nprint(\"__\"*50,\"\\n\")\nprint(classification_report(y_test,prediction1))\nprint(\"__\"*50)\nsns.heatmap(confusion_matrix(y_test,prediction1),annot=True,fmt=\"d\")\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machine"},{"metadata":{},"cell_type":"markdown","source":"**Linear Kernel**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SVC(kernel='linear')\nmodel.fit(X_train,y_train)\nprediction1=model.predict(X_test)\nprint(\"__\"*50,\"\\n\")\nprint('The accuracy of the Linear SVM is',accuracy_score(y_test,prediction1))\nprint(\"__\"*50,\"\\n\")\nprint(classification_report(y_test,prediction1))\nprint(\"__\"*50)\nsns.heatmap(confusion_matrix(y_test,prediction1),annot=True,fmt=\"d\")\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Polynomial Kernel of degree 3**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SVC(kernel='poly',degree=3)\nmodel.fit(X_train,y_train)\nprediction1=model.predict(X_test)\nprint(\"__\"*50,\"\\n\")\nprint('The accuracy of the Polynomial SVM is',accuracy_score(y_test,prediction1))\nprint(\"__\"*50,\"\\n\")\nprint(classification_report(y_test,prediction1))\nprint(\"__\"*50)\nsns.heatmap(confusion_matrix(y_test,prediction1),annot=True,fmt=\"d\")\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Polynomial Kernel of degree 4**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SVC(kernel='poly',degree=4)\nmodel.fit(X_train,y_train)\nprediction1=model.predict(X_test)\nprint(\"__\"*50,\"\\n\")\nprint('The accuracy of the Polynomial SVM is',accuracy_score(y_test,prediction1))\nprint(\"__\"*50,\"\\n\")\nprint(classification_report(y_test,prediction1))\nprint(\"__\"*50)\nsns.heatmap(confusion_matrix(y_test,prediction1),annot=True,fmt=\"d\")\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**RBF Kernel**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SVC(kernel='rbf')\nmodel.fit(X_train,y_train)\nprediction1=model.predict(X_test)\nprint(\"__\"*50,\"\\n\")\nprint('The accuracy of the SVM is',accuracy_score(y_test,prediction1))\nprint(\"__\"*50,\"\\n\")\nprint(classification_report(y_test,prediction1))\nprint(\"__\"*50)\nsns.heatmap(confusion_matrix(y_test,prediction1),annot=True,fmt=\"d\")\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model Accuracy**\n1. Logistic Regression : 88 %\n2. Linear SVM : 88 %\n3. Polynomial Kernel of degree 3 : 87 %\n4. Polynomial Kernel of degree 4 : 74 %\n4. RBF Kernel : 88 %"},{"metadata":{},"cell_type":"markdown","source":"#### Hyperparameter Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams_dict=\\\n{'C':[0.001,0.01,0.1,1,10,100],\n 'gamma':[0.001,0.01,0.1,1,10,100],\n 'kernel':['linear','rbf']}\nmodel1=GridSearchCV(estimator=SVC(),param_grid=params_dict,scoring='accuracy',cv=10)\nmodel1.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Best parameters for our svc model\nmodel1.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's run our SVC again with the best parameters.\nmodel_final = SVC(C = 0.01, gamma =  0.001, kernel= 'linear')\nmodel_final.fit(X_train,y_train)\nprediction1=model_final.predict(X_test)\nprint(\"__\"*50,\"\\n\")\nprint('The accuracy of the SVM  is',accuracy_score(y_test,prediction1))\nprint(\"__\"*50,\"\\n\")\nprint(classification_report(y_test,prediction1))\nprint(\"__\"*50)\nsns.heatmap(confusion_matrix(y_test,prediction1),annot=True,fmt=\"d\")\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Improved accuracy from 88% to 89% by tuning hyperparameters**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}