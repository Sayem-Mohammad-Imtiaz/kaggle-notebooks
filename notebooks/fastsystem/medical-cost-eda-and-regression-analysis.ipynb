{"cells":[{"metadata":{"_uuid":"4f2f0d51ce13d9d914a5a1b2d0df7818b1c8e1e3"},"cell_type":"markdown","source":"* [1. INTRODUCTION](#1)\n* [2. USED LIBRARIES](#2)\n* [3. DATA EXPLORATION](#3)\n    * [3.1. Detailed Information of the Dataset](#31)\n    * [3.2. Correlation of Columns(Attributes)](#32)\n    * [3.3. Various Visualizations from Dataset](#33)\n* [4. DATA PREPARATION AND CLEANING](#4)\n    * [4.1. Drop Irrelevant Columns](#41)\n    * [4.2. Correction of Column(Attribute) Names](#42)\n    * [4.3. Split Data and Target](#43)\n    * [4.4. Handling Missing Values](#44)\n    * [4.5. Type Conversions and Encoding](#45)\n    * [4.6. Normalization of Data](#46)\n    * [4.7. Preparation of Test and Train Data](#47)\n* [5. BUILDING MODELS](#5)\n    * [5.1. Multiple Linear Regression](#51)\n    * [5.2. Polynomial Regression](#52)\n    * [5.3. Decision Tree Regression](#53)\n    * [5.4. Random Forest Regression](#54)\n    * [5.5. Support Vector Regression](#55)\n* [6. EVALUATING MODELS](#6)\n    * [6.1. Evaluating Multiple Linear Regression Model](#61)\n    * [6.2. Evaluating Polynomial Regression Model](#62)\n    * [6.3. Evaluating Decision Tree Regression Model](#63)\n    * [6.4. Evaluating Random Forest Regression Model](#64)\n    * [6.5. Evaluating Support Vector Regression Model](#65)\n* [7. EXPLORATION OF RESULTS](#7)\n* [8. CONCLUSION](#8)"},{"metadata":{"_uuid":"3984c8d7ffe2bce3497eb2cc1a04c4c48f14cdbe"},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n## 1. INTRODUCTION\n\nIn this study, I will give you an example of EDA (Exploratory Data analysis) and I will make an analysis of the various Regression algorithms in Machine Learning. In this analysis, I will use the \"Medical Cost Personal Datasets\" dataset. In this dataset, 1338 people have anonymous information. In addition, annual insurance premiums given to these people are also included in the dataset. We will create and test regression models that estimate their annual premiums based on their other information. Then we will discuss the results and see which algorithm is successful. We will also perform visualization using various data in dataset."},{"metadata":{"_uuid":"2a470980ac513272127293b80f99c7abc614d745"},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## 2. USED LIBRARIES\n\nThis section will give information about Python libraries to be used in the study and these libraries will be imported into the project. Here are the libraries and explanations we will use:\n\n* **NumPy :** This library is actually a dependency for other libraries. The main purpose of this library is to provide a variety of mathematical operations on matrices and vectors in Python. Our project will be used this library to provide support to other libraries.\n* **Pandas :** This library performs import and processing of dataset in Python. In our project, it will be used to include the CSV extension dataset in the project and to perform various operations on it.\n* **Matplotlib :** This library, which is usually used to visualize data. It will perform the same task in our project.\n* **Seaborn :** This library which has similar features to Matplotlib is another library used for data visualization in Python. In our project, it will be used for the implementation of various features not included in the Matplotlib library.\n* **Sckit-Learn :** This library includes the implementation of various machine larning algorithms. With this library, we will perform all operations from building to evaluation of regression models using functions and classes in this library.\n\nNow let's import NumPy, Pandas, Matplotlib and Seaborn libraries into our project and get them ready for use:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np  # Importing NumPy library\nimport pandas as pd  # Importing Pandas library\nimport matplotlib.pyplot as plt  # Importing Matplotlib library's \"pyplot\" module\nimport seaborn as sns  # Imorting Seaborn library\n\n# This lines for Kaggle:\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d427dd10c1220411dc39000fe7ceab50a080ee7"},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n## 3. DATA EXPLORATION\n\nIn this section, various explanations will be made about dataset."},{"metadata":{"_uuid":"fc4e2ddc876d8e807fb4ceed916dca3426f8bd1a"},"cell_type":"markdown","source":"<a id=\"31\"></a> <br>\n### 3.1. Detailed Information of the Dataset\n\nHere we will import the dataset first. Then, we will explain the columns(features) of the dataset one by one."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/insurance.csv\")  # Read CSV file and load into \"data\" variable\ndata.info()  # Show detailed information for dataset columns(attributes)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f990b5f0a6ec06c49339da46283129ce84fd430e"},"cell_type":"markdown","source":"As you can see from the output here, there are 1338 rows so record. There are also 7 columns(attributes). Fortunately, our dataset doesn't have any missing values. In other words, all the columns of all rows are filled with data. The rows are also indexed from 0 to 1337. Now, let's explain what the columns mean:\n\n* **age :** Indicates the age of the person. It contains data of type \"_int64_\".\n* **sex :** It refers to the gender of the person. It contains \"_object_\" type data.\n* **bmi :** It refers to the Body Mass Index of the person and contains the data of type \"_float64_\". BMI is a measure of the weight of a person, divided by the square of its length. Determines the person's obesity value. The formula for USA and METRIC units is as follows:\n![](https://i2.wp.com/www.marathonnewbie.com/wp-content/uploads/2016/07/body-mass-index-formula.jpg)\n* **children :** It refers to the number of children that a person has. It contains data of type \"_int64_\".\n* **smoker :** Indicates whether the person smokes or not. It contains \"_object_\" type data.\n* **region :** Specifies which region the person is from. It contains \"_object_\" type data.\n* **charges :** The person's total insurance premium is specified. Although not specified, it is assumed to be in dollars ($). It contains \"__float64__\" type data.\n\nWe gave the necessary information about dataset. Now, looking at the first 5 and last 5 entries of dataset, what are the values that are being held:"},{"metadata":{"trusted":true,"_uuid":"96e537d9164cfcf12f5caa06a25e9a6cef03f058"},"cell_type":"code","source":"data.head()  # Print first 5 entry of the dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22d22db96e432a7a6f33a983b22aec7316ad00f2"},"cell_type":"code","source":"data.tail()  # Prints last 5 entries of the dataset","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82d8e27fbec20d7aba92544b45797301ffb2f23d"},"cell_type":"markdown","source":"There is no problem with numeric data. However, we may need to apply numerical transformation in the future for categorical data of type \"object\". Now let's see the various statistics about the numeric data:"},{"metadata":{"trusted":true,"_uuid":"f5d6150dcbf545249d9bda8a674c0b83b2ceae65"},"cell_type":"code","source":"data.describe()  # Print table which contain statistical data of the dataset","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4f961893a29bb820ae2d0f021eea7937cfbd4d2"},"cell_type":"markdown","source":"<a id=\"32\"></a> <br>\n### 3.2. Correlation of Columns(Attributes)\n\nIn this section, we'll find the correlation matrix between the columns and we'll visualize it into a Heatmap. In this way, we will be able to see the relationship between the attributes more clearly and visualize them in the future."},{"metadata":{"trusted":true,"_uuid":"4227a731f2e063135e1de65a116a33360006bb4e"},"cell_type":"code","source":"data.corr()  # Prints correlation matrix for the numerical columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6b867702332d23a5b8608bd338dcafda34ce6c2"},"cell_type":"markdown","source":"Correlation is a number that indicates how the two attributes are related to each other. As this number approaches 1.0, the relationship is strengthened in the right direction. As it approaches -1.0, it is strengthened in the opposite direction. If this value is close to zero, the bond between the two data is weak. For example in the above matrix, we see a little (but no more) bound with person's age and charge values. Other bounds are so weak. Now we visualize this correlation matrix with Heatmap:"},{"metadata":{"trusted":true,"_uuid":"3064edd99f90aeb67e1115c6bc8087c7eeb4cf2a"},"cell_type":"code","source":"fig, axes = plt.subplots(figsize=(8, 8))  # This method creates a figure and a set of subplots\nsns.heatmap(data=data.corr(), annot=True, linewidths=.5, ax=axes)  # Figure out heatmap\n# Parameters:\n# data : 2D data for the heatmap.\n# annot : If True, write the data value in each cell.\n# linewidths : Width of the lines that will divide each cell.\n# ax : Axes in which to draw the plot, otherwise use the currently-active Axes.\nplt.show()  # Shows only plot and remove other informations","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"170623b96b3d1b0ec8ce219b9dcd3ca2500c8cc1"},"cell_type":"markdown","source":"<a id=\"33\"></a> <br>\n### 3.3. Various Visualizations from Dataset\n\nIn this section, we will make various visualizations related to dataset and after understanding the connection between the data in dataset we will move to the next section. Let's look at the distribution of numerical attributes:"},{"metadata":{"trusted":true,"_uuid":"6229a6f97739c33091af89779b86757afe03ec2d"},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\ndata.plot(kind=\"hist\", y=\"age\", bins=70, color=\"b\", ax=axes[0][0])\ndata.plot(kind=\"hist\", y=\"bmi\", bins=200, color=\"r\", ax=axes[0][1])\ndata.plot(kind=\"hist\", y=\"children\", bins=5, color=\"g\", ax=axes[1][0])\ndata.plot(kind=\"hist\", y=\"charges\", bins=200, color=\"orange\", ax=axes[1][1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a23516f5adad2c025209c051f9d02af62309a218"},"cell_type":"markdown","source":"We can make inferences from these images. For example, the number of children who have no children in the dataset is more than the others. In addition, the total charge amount usually looks less than 20000. Now look at the male and female numbers in dataset:"},{"metadata":{"trusted":true,"_uuid":"5e4a9819503653d501924ac71c64a1e358f89343"},"cell_type":"code","source":"sns.catplot(x=\"sex\", kind=\"count\", palette=\"Set1\", data=data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"331308eff00ab2a847decd52920cf9ba032eda5a"},"cell_type":"markdown","source":"Since the value we will examine as an output here is \"_charges_\", we need to examine the relationship of the other columns with it. To do this, let's draw Scatter Plots between the numeric columns and the \"_charges_\" column:"},{"metadata":{"trusted":true,"_uuid":"4faadb467d75e86fe0b52458491d5e88141f2101"},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\ndata.plot(kind='scatter', x='age', y='charges', alpha=0.5, color='green', ax=axes[0], title=\"Age vs. Charges\")\ndata.plot(kind='scatter', x='bmi', y='charges', alpha=0.5, color='red', ax=axes[1], title=\"Sex vs. Charges\")\ndata.plot(kind='scatter', x='children', y='charges', alpha=0.5, color='blue', ax=axes[2], title=\"Children vs. Charges\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea0dcc844ddf5e98c7fac10492dc19769795f231"},"cell_type":"markdown","source":"Finally, look at the distribution of smokers and non-smokers in the BMI vs. Charges Scatter Plot:"},{"metadata":{"trusted":true,"_uuid":"24fd908016a9de761e95a1195dfab9872ecd6b2a"},"cell_type":"code","source":"sns.scatterplot(x=\"bmi\", y=\"charges\", data=data, palette='Set2', hue='smoker')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"682701fedd3c0c742710b6573684271dba6d910a"},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n## 4. DATA PREPARATION AND CLEANING\n\nIn this section, we will be able to use various preprocesses in order to use the data correctly. First, let us explain what data will be used as a target. Here we will use \"age\", \"sex\", \"bmi\", \"children\" and \"smoker\" columns as data ie X. We will use \"charges\" column as the target ie Y. We will drop the \"region\" column from this dataset. Because we haven't done much analysis about it, so we can slow down our process. Now, let's do the related operations one by one."},{"metadata":{"_uuid":"5d07f27fd69d0ca53f35e1aa3868325d39e25686"},"cell_type":"markdown","source":"<a id=\"41\"></a> <br>\n### 4.1. Drop Irrelevant Columns\n\nIn this section, we will delete the columns that will not work for us from the dataset. Here is the only one we will not use the column is \"_region_\", so we just need to delete it:"},{"metadata":{"trusted":true,"_uuid":"4b572331a805fb1aa6d5630258880cef1d565a04"},"cell_type":"code","source":"data.drop([\"region\"], axis=1, inplace=True)  # Drop \"region\" column from dataset","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6176b8be81cb31a37cb31022a6d77ff3c01592b"},"cell_type":"markdown","source":"<a id=\"42\"></a> <br>\n### 4.2. Correction of Column(Attribute) Names\n\nIn this section we will use more readable column names instead existing column names if there are spaces and unwanted characters. There is no problem with column names in the dataset we use. But in order to show how the process is done, I'll change them all to the name I want and all to be capitalized:"},{"metadata":{"trusted":true,"_uuid":"0aec70fb71194d03754c6e876a744dad149f4f34"},"cell_type":"code","source":"data.rename(columns={\"age\" : \"AGE\", \"sex\" : \"GENDER\", \"bmi\" : \"BMI\", \"children\" : \"CHILDREN\", \"smoker\": \"SMOKER\", \"charges\" : \"CHARGES\"}, inplace=True)\ndata.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"883ae46c54dccf4914284b2b5c9136eecc3e02c3"},"cell_type":"markdown","source":"<a id=\"43\"></a> <br>\n### 4.3. Split Data and Target\n\nWe can't give the whole dataset to the model as it is. First we need to set the data and target part of it. Here, the data part is called X, while the target part is called Y. Now split the data and target partitions and assign each of them to variables named X and Y:"},{"metadata":{"trusted":true,"_uuid":"341525c8aa03ec1e5617844f584edc4eb7628e9c"},"cell_type":"code","source":"X = data.drop([\"CHARGES\"], axis=1)  # Put all data (except \"__CHARGES__\" column) to the X variable\ny = data.CHARGES.values  # Put only \"__CHARGES__\" column to the Y variable","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"edd0c97743b68cd004a9c7bda6280adb6e099c84"},"cell_type":"markdown","source":"<a id=\"44\"></a> <br>\n### 4.4. Handling Missing Values\n\nThere is no missing value in this dataset. Therefore, we do not have any action here."},{"metadata":{"_uuid":"2a0b2e25f120512b6e66d19a41761909f0400054"},"cell_type":"markdown","source":"<a id=\"45\"></a> <br>\n### 4.5. Type Conversions and Encoding\n\nHere we will convert categorical data into numeric data. In our dataset, categorical columns are gender and smoking. We will convert them numerically:\n\n* **GENDER :** If \"male\" is 0, \"female\" is 1.\n* **SMOKER :** If \"no\" is 0, \"yes\" is 1."},{"metadata":{"trusted":true,"_uuid":"a56392ab8349b7644aaaa2c1f2edb1f09edeb5a9"},"cell_type":"code","source":"X.GENDER = [1 if each == \"female\" else 0 for each in X.GENDER]\nX.SMOKER = [1 if each == \"yes\" else 0 for each in X.SMOKER]\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b37475af2a9091c6fdd8207a0003006c233e16a"},"cell_type":"markdown","source":"<a id=\"46\"></a> <br>\n### 4.6. Normalization of Data\n\nThe values of the data may be so far from each other. This can sometimes lead to undesirable situations in regression algorithms. Therefore, we need to normalize the data."},{"metadata":{"trusted":true,"_uuid":"1033622967a2bb2d67b040d6a50a4459afed65b0"},"cell_type":"code","source":"X[\"BMI\"] = (X - np.min(X))/(np.max(X) - np.min(X)).values\nX.BMI","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"508730e8f2b6876d4545d5ffe70b15f98588b913"},"cell_type":"markdown","source":"As you can see, the values were normalized between 0 and 1. Now let's take our Train and Test data and finish this part."},{"metadata":{"_uuid":"98e70ea069a6525855eaca9e6e3cd5bdff03388e"},"cell_type":"markdown","source":"<a id=\"47\"></a> <br>\n### 4.7. Preparation of Test and Train Data\n\nThe final process here is the smooth and random separation of test and train data. For this, we will benefit from the method named \"_train_test_split_\" from the Scikit-Learn library. I would like to use 20% of our data for testing and 80% for training purposes. The process is very simple:"},{"metadata":{"trusted":true,"_uuid":"9f32638e7f88b35e63099580c1da0da263c93795"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split  # Import \"train_test_split\" method\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Parameters:\n# test_size : It decides how many test data in percentage.\n# random_state : This parameter can take any value. This value decides randomness seed.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1dfa0987295e13b69fd59c16b6daadd11715929"},"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n## 5. BUILDING MODELS\n\nIn this section, we will build regression models and fit them wit data. The regression algorithms used in this section are:\n\n1. Multiple Linear Regression\n2. Polynomial Regression\n3. Decision Tree Regression\n4. Random Forest Regression\n5. Support Vector Regression"},{"metadata":{"_uuid":"a8853f194dca24a0134691544e52b216a0c56be4"},"cell_type":"markdown","source":"<a id=\"51\"></a> <br>\n### 5.1. Multiple Linear Regression\n\nCreate the Multiple Linear Regression model and fit the data:"},{"metadata":{"trusted":true,"_uuid":"338eb5eeb7ba4d28d8a13b851e753ae9eb43b41b"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression  # Import Linear Regression model\n\nmultiple_linear_reg = LinearRegression(fit_intercept=False)  # Create a instance for Linear Regression model\nmultiple_linear_reg.fit(x_train, y_train)  # Fit data to the model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e74e260c1e2195c96745fe236c903fc6d24284e"},"cell_type":"markdown","source":"<a id=\"52\"></a> <br>\n### 5.2. Polynomial Regression\n\nIn the Polynomial Regression model, we must first convert the data to the specified degree as a polynomial feature. For this, we will take advantage of the PolynomialFeatures class of the Scikit-Learn library. Now we will use the X data for training and testing as a polynomial feature and perform the fit process:"},{"metadata":{"trusted":true,"_uuid":"de5b702ce8092acdf7c7c2fc5876dfdfd82e75b0"},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\npolynomial_features = PolynomialFeatures(degree=3)  # Create a PolynomialFeatures instance in degree 3\nx_train_poly = polynomial_features.fit_transform(x_train)  # Fit and transform the training data to polynomial\nx_test_poly = polynomial_features.fit_transform(x_test)  # Fit and transform the testing data to polynomial\n\npolynomial_reg = LinearRegression(fit_intercept=False)  # Create a instance for Linear Regression model\npolynomial_reg.fit(x_train_poly, y_train)  # Fit data to the model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"185e2dcc3e55f161452e753a5c1561ee78ebdd53"},"cell_type":"markdown","source":"<a id=\"53\"></a> <br>\n### 5.3. Decision Tree Regression\n\nCreate a Decision Tree Regression model and fit the data (we use 13 for randomness seed value):"},{"metadata":{"trusted":true,"_uuid":"09e386603a312fc84c9578da710e09ae8803a74d"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor  # Import Decision Tree Regression model\n\ndecision_tree_reg = DecisionTreeRegressor(max_depth=5, random_state=13)  # Create a instance for Decision Tree Regression model\ndecision_tree_reg.fit(x_train, y_train)  # Fit data to the model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04e14b567ef59db2f97eb64c7bbc620399462076"},"cell_type":"markdown","source":"<a id=\"54\"></a> <br>\n### 5.4. Random Forest Regression\n\nCreate a Random Forest Regression model using 400 Estimators to fit the data (we use 13 for randomness seed value):"},{"metadata":{"trusted":true,"_uuid":"04f7c7990bfe822d5103b1a4320c152bd2b3acbc"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor  # Import Random Forest Regression model\n\nrandom_forest_reg = RandomForestRegressor(n_estimators=400, max_depth=5, random_state=13)  # Create a instance for Random Forest Regression model\nrandom_forest_reg.fit(x_train, y_train)  # Fit data to the model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27a53233e30e462221216599032ce384dee970bf"},"cell_type":"markdown","source":"<a id=\"55\"></a> <br>\n### 5.5. Support Vector Regression\n\nCreate the Support Vector Regression model and fit the data:"},{"metadata":{"trusted":true,"_uuid":"1d7edcace3e8ac9eee5b93de1b14976ceddd3dea"},"cell_type":"code","source":"from sklearn.svm import SVR  # Import SVR model\n\nsupport_vector_reg = SVR(gamma=\"auto\", kernel=\"linear\", C=1000)  # Create a instance for Support Vector Regression model\nsupport_vector_reg.fit(x_train, y_train)  # Fit data to the model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44b764960855c477d1553c7e0c3f5b644dfa895a"},"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n## 6. EVALUATING MODELS\n\nIn this section we will do some measurements to evaluate the performance on the models we fit. In addition, 10-Fold Cross Validation method will perform the validation process. R Squared Score method will be used for calculating the accuracy of the models. Mean Squared Error (MSE) method will be used for error measurement. As the MSE result can be very large, its square root will be taken and converted into RMSE. The error and accuracy calculation shall be performed on both the test and training dataset. Let's first perform evaluation for each model by importing the necessary functions:"},{"metadata":{"trusted":true,"_uuid":"7751c8cb820ee535fcd61754559c34184703a5fc"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict  # For K-Fold Cross Validation\nfrom sklearn.metrics import r2_score  # For find accuracy with R2 Score\nfrom sklearn.metrics import mean_squared_error  # For MSE\nfrom math import sqrt  # For squareroot operation","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01fd4702a36e8903241537691d074c2d1b75900f"},"cell_type":"markdown","source":"<a id=\"61\"></a> <br>\n### 6.1. Evaluating Multiple Linear Regression Model"},{"metadata":{"trusted":true,"_uuid":"f925dfad3fd6f412558d5b294b23589b343d90a9"},"cell_type":"code","source":"# Prediction with training dataset:\ny_pred_MLR_train = multiple_linear_reg.predict(x_train)\n\n# Prediction with testing dataset:\ny_pred_MLR_test = multiple_linear_reg.predict(x_test)\n\n# Find training accuracy for this model:\naccuracy_MLR_train = r2_score(y_train, y_pred_MLR_train)\nprint(\"Training Accuracy for Multiple Linear Regression Model: \", accuracy_MLR_train)\n\n# Find testing accuracy for this model:\naccuracy_MLR_test = r2_score(y_test, y_pred_MLR_test)\nprint(\"Testing Accuracy for Multiple Linear Regression Model: \", accuracy_MLR_test)\n\n# Find RMSE for training data:\nRMSE_MLR_train = sqrt(mean_squared_error(y_train, y_pred_MLR_train))\nprint(\"RMSE for Training Data: \", RMSE_MLR_train)\n\n# Find RMSE for testing data:\nRMSE_MLR_test = sqrt(mean_squared_error(y_test, y_pred_MLR_test))\nprint(\"RMSE for Testing Data: \", RMSE_MLR_test)\n\n# Prediction with 10-Fold Cross Validation:\ny_pred_cv_MLR = cross_val_predict(multiple_linear_reg, X, y, cv=10)\n\n# Find accuracy after 10-Fold Cross Validation\naccuracy_cv_MLR = r2_score(y, y_pred_cv_MLR)\nprint(\"Accuracy for 10-Fold Cross Predicted Multiple Linaer Regression Model: \", accuracy_cv_MLR)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11c5d39c81465f176368cd3e94cecff927dc2e83"},"cell_type":"markdown","source":"<a id=\"62\"></a> <br>\n### 6.2. Evaluating Polynomial Regression Model"},{"metadata":{"trusted":true,"_uuid":"cab7ae537d218e481c428d51f541f652379bdf9d"},"cell_type":"code","source":"# Prediction with training dataset:\ny_pred_PR_train = polynomial_reg.predict(x_train_poly)\n\n# Prediction with testing dataset:\ny_pred_PR_test = polynomial_reg.predict(x_test_poly)\n\n# Find training accuracy for this model:\naccuracy_PR_train = r2_score(y_train, y_pred_PR_train)\nprint(\"Training Accuracy for Polynomial Regression Model: \", accuracy_PR_train)\n\n# Find testing accuracy for this model:\naccuracy_PR_test = r2_score(y_test, y_pred_PR_test)\nprint(\"Testing Accuracy for Polynomial Regression Model: \", accuracy_PR_test)\n\n# Find RMSE for training data:\nRMSE_PR_train = sqrt(mean_squared_error(y_train, y_pred_PR_train))\nprint(\"RMSE for Training Data: \", RMSE_PR_train)\n\n# Find RMSE for testing data:\nRMSE_PR_test = sqrt(mean_squared_error(y_test, y_pred_PR_test))\nprint(\"RMSE for Testing Data: \", RMSE_PR_test)\n\n# Prediction with 10-Fold Cross Validation:\ny_pred_cv_PR = cross_val_predict(polynomial_reg, polynomial_features.fit_transform(X), y, cv=10)\n\n# Find accuracy after 10-Fold Cross Validation\naccuracy_cv_PR = r2_score(y, y_pred_cv_PR)\nprint(\"Accuracy for 10-Fold Cross Predicted Polynomial Regression Model: \", accuracy_cv_PR)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b53a0622502e9f1bbb65e240f36eae3df35872c4"},"cell_type":"markdown","source":"<a id=\"63\"></a> <br>\n### 6.3. Evaluating Decision Tree Regression Model"},{"metadata":{"trusted":true,"_uuid":"57e2b34eba71d05c573b9896af61953b1c099316"},"cell_type":"code","source":"# Prediction with training dataset:\ny_pred_DTR_train = decision_tree_reg.predict(x_train)\n\n# Prediction with testing dataset:\ny_pred_DTR_test = decision_tree_reg.predict(x_test)\n\n# Find training accuracy for this model:\naccuracy_DTR_train = r2_score(y_train, y_pred_DTR_train)\nprint(\"Training Accuracy for Decision Tree Regression Model: \", accuracy_DTR_train)\n\n# Find testing accuracy for this model:\naccuracy_DTR_test = r2_score(y_test, y_pred_DTR_test)\nprint(\"Testing Accuracy for Decision Tree Regression Model: \", accuracy_DTR_test)\n\n# Find RMSE for training data:\nRMSE_DTR_train = sqrt(mean_squared_error(y_train, y_pred_DTR_train))\nprint(\"RMSE for Training Data: \", RMSE_DTR_train)\n\n# Find RMSE for testing data:\nRMSE_DTR_test = sqrt(mean_squared_error(y_test, y_pred_DTR_test))\nprint(\"RMSE for Testing Data: \", RMSE_DTR_test)\n\n# Prediction with 10-Fold Cross Validation:\ny_pred_cv_DTR = cross_val_predict(decision_tree_reg, X, y, cv=10)\n\n# Find accuracy after 10-Fold Cross Validation\naccuracy_cv_DTR = r2_score(y, y_pred_cv_DTR)\nprint(\"Accuracy for 10-Fold Cross Predicted Decision Tree Regression Model: \", accuracy_cv_DTR)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c851b551292cf448fd29d36bd54efaecfa78ba9"},"cell_type":"markdown","source":"<a id=\"64\"></a> <br>\n### 6.4. Evaluating Random Forest Regression Model"},{"metadata":{"trusted":true,"_uuid":"c2c5fb94bc1d56d18f0725ac3acd6b0356ca3406"},"cell_type":"code","source":"# Prediction with training dataset:\ny_pred_RFR_train = random_forest_reg.predict(x_train)\n\n# Prediction with testing dataset:\ny_pred_RFR_test = random_forest_reg.predict(x_test)\n\n# Find training accuracy for this model:\naccuracy_RFR_train = r2_score(y_train, y_pred_RFR_train)\nprint(\"Training Accuracy for Random Forest Regression Model: \", accuracy_RFR_train)\n\n# Find testing accuracy for this model:\naccuracy_RFR_test = r2_score(y_test, y_pred_RFR_test)\nprint(\"Testing Accuracy for Random Forest Regression Model: \", accuracy_RFR_test)\n\n# Find RMSE for training data:\nRMSE_RFR_train = sqrt(mean_squared_error(y_train, y_pred_RFR_train))\nprint(\"RMSE for Training Data: \", RMSE_RFR_train)\n\n# Find RMSE for testing data:\nRMSE_RFR_test = sqrt(mean_squared_error(y_test, y_pred_RFR_test))\nprint(\"RMSE for Testing Data: \", RMSE_RFR_test)\n\n# Prediction with 10-Fold Cross Validation:\ny_pred_cv_RFR = cross_val_predict(random_forest_reg, X, y, cv=10)\n\n# Find accuracy after 10-Fold Cross Validation\naccuracy_cv_RFR = r2_score(y, y_pred_cv_RFR)\nprint(\"Accuracy for 10-Fold Cross Predicted Random Forest Regression Model: \", accuracy_cv_RFR)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d4fe27253143f8748867955e7a4c448adbe05a9"},"cell_type":"markdown","source":"<a id=\"65\"></a> <br>\n### 6.5. Evaluating Support Vector Regression Model"},{"metadata":{"trusted":true,"_uuid":"4ff55fed3543502a3230572616f5fcaa45272afe"},"cell_type":"code","source":"# Prediction with training dataset:\ny_pred_SVR_train = support_vector_reg.predict(x_train)\n\n# Prediction with testing dataset:\ny_pred_SVR_test = support_vector_reg.predict(x_test)\n\n# Find training accuracy for this model:\naccuracy_SVR_train = r2_score(y_train, y_pred_SVR_train)\nprint(\"Training Accuracy for Support Vector Regression Model: \", accuracy_SVR_train)\n\n# Find testing accuracy for this model:\naccuracy_SVR_test = r2_score(y_test, y_pred_SVR_test)\nprint(\"Testing Accuracy for Support Vector Regression Model: \", accuracy_SVR_test)\n\n# Find RMSE for training data:\nRMSE_SVR_train = sqrt(mean_squared_error(y_train, y_pred_SVR_train))\nprint(\"RMSE for Training Data: \", RMSE_SVR_train)\n\n# Find RMSE for testing data:\nRMSE_SVR_test = sqrt(mean_squared_error(y_test, y_pred_SVR_test))\nprint(\"RMSE for Testing Data: \", RMSE_SVR_test)\n\n# Prediction with 10-Fold Cross Validation:\ny_pred_cv_SVR = cross_val_predict(support_vector_reg, X, y, cv=10)\n\n# Find accuracy after 10-Fold Cross Validation\naccuracy_cv_SVR = r2_score(y, y_pred_cv_SVR)\nprint(\"Accuracy for 10-Fold Cross Predicted Support Vector Regression Model: \", accuracy_cv_SVR)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dfa4608ed3fbc5b0c5ceca4074339386c9f7ed7a"},"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n## 7. EXPLORATION OF RESULTS\n\nIn general, let's put all the results of the models into the table:"},{"metadata":{"trusted":true,"_uuid":"0c7984a08d8b4b55bdb8353127a2dd29a0f6112a"},"cell_type":"code","source":"training_accuracies = [accuracy_MLR_train, accuracy_PR_train, accuracy_DTR_train, accuracy_RFR_train, accuracy_SVR_train]\ntesting_accuracies = [accuracy_MLR_test, accuracy_PR_test, accuracy_DTR_test, accuracy_RFR_test, accuracy_SVR_test]\ntraining_RMSE = [RMSE_MLR_train, RMSE_PR_train, RMSE_DTR_train, RMSE_RFR_train, RMSE_SVR_train]\ntesting_RMSE = [RMSE_MLR_test, RMSE_PR_test, RMSE_DTR_test, RMSE_RFR_test, RMSE_SVR_test]\ncv_accuracies = [accuracy_cv_MLR, accuracy_cv_PR, accuracy_cv_DTR, accuracy_cv_RFR, accuracy_cv_SVR]\nparameters = [\"fit_intercept=False\", \"fit_intercept=False\", \"max_depth=5\", \"n_estimators=400, max_depth=5\", \"kernel=”linear”, C=1000\"]\n\ntable_data = {\"Parameters\": parameters, \"Training Accuracy\": training_accuracies, \"Testing Accuracy\": testing_accuracies, \n              \"Training RMSE\": training_RMSE, \"Testing RMSE\": testing_RMSE, \"10-Fold Score\": cv_accuracies}\nmodel_names = [\"Multiple Linear Regression\", \"Polynomial Regression\", \"Decision Tree Regression\", \"Random Forest Regression\", \"Support Vector Regression\"]\n\ntable_dataframe = pd.DataFrame(data=table_data, index=model_names)\ntable_dataframe","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bbb16a9caeb2b4283861fce13d75f12ad2a3063"},"cell_type":"markdown","source":"Now let's compare the training and testing accuracy of each model:"},{"metadata":{"trusted":true,"_uuid":"c835d769d5f99f7cad695c3d563a643692e5be8e"},"cell_type":"code","source":"table_dataframe.iloc[:, 1:3].plot(kind=\"bar\", ylim=[0.0, 1.0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cda1dd614519a12b3a2a6dc4a34029298b91af79"},"cell_type":"markdown","source":"Let's compare each model's training and testing RMSE:"},{"metadata":{"trusted":true,"_uuid":"fe99e551bdae04d3735e8c4f5fcb65fb9d76e895"},"cell_type":"code","source":"table_dataframe.iloc[:, 3:5].plot(kind=\"bar\", ylim=[0, 8000])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70aad8da65a04b85cb2fb90e6108d58467a28c93"},"cell_type":"markdown","source":"Finally, compare the score values for 10-Fold Cross Validation:"},{"metadata":{"trusted":true,"_uuid":"167fdfd47226a31f448e354ceb86c9f6662287af"},"cell_type":"code","source":"table_dataframe.iloc[:, 5].plot(kind=\"bar\", ylim=[0.0, 1.0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5af9463c29c9d69e7b59c0bda3d45fcc0d93e4a"},"cell_type":"markdown","source":"As you can see, all models gave very close results. It is necessary that the values of training accuracy and testing accuracy should be close to each other. There may be many reasons why the results are not so high. First of all, we didn't do a proper feature engineering here and we didn't resort to methods like PCA (Principal Component Analysis). Moreover, the fact that the dataset is low can also be a factor."},{"metadata":{"_uuid":"ab0d2dab624a71b5bf639832f5478a5124a0b797"},"cell_type":"markdown","source":"<a id=\"8\"></a> <br>\n## 8. CONCLUSION\n\nThis kernel is designed to completely illustrate regression and EDA stages in machine learning. You can use the code and information in the example as desired. But please use his license to use the dataset. In this kernel, we tried to get information about:\n\n* Data Science processes\n* How a Dataset Exploratory Data Analysis (EDA) is made\n* Find and visualize the correlation between features in data\n* How to make various visualizations about dataset\n* Use of libraries such as Pandas, Matplotlib, Seaborn and Scikit-Learn in Python\n* How to do Data Cleaning simply\n* How to split Dataset into Test and Train\n* How to install and learn Machine Learning models\n* How polynomial is made in Polynomial Regression\n* How to do the Evaluating on the installed models\n* How the R-Squared Score is located\n* How RMSE values are located\n* How K-Fold Cross Validation is done and how the score is calculated\n* How the models evaluated are visually compared and interpreted\n\nI didn't give a detailed explanation about each subject. You can find a detailed description on any topic on the internet or ask me to comment on it.\n\n_Best regards..._<br>\n__Mustafa YEMURAL__"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}