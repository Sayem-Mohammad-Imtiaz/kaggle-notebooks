{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9b72322a-74b4-8b22-fb75-a52cb8fe2d05"},"outputs":[],"source":"# Load packages and data.\n\n# Packages for data manipulation and data visualization.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Packages for data modelling. \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.grid_search import GridSearchCV\n\n%matplotlib inline\n\ndf = pd.read_csv(\"../input/diabetes.csv\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"705e7bcb-a79b-0d96-f668-25aa1a1cf149"},"outputs":[],"source":"# Inspect the data set.\n\ndf.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0fb7ffda-65f5-ad92-b256-5f96a02cf80b"},"outputs":[],"source":"df.info()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"65f4e130-6d3f-4b37-2124-35dde2de0dca"},"outputs":[],"source":"# We can see that the data frame is made up of numerical data only. \n\ndf.describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b795306c-26bb-c387-dc17-54ac6b8f47e9"},"outputs":[],"source":"# Some takeaways from the data above:\n# Around 35% of the patients have diabetes.\n# There average age is around 33.\n# The bulk of patients are aged between 24 and 41.\n# There is a wide diversity in insulin levels.\n# The patients have had just under 4 pregnancies on average.\n\ndf.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9b64a49b-b984-809a-b49a-aed996afc50f"},"outputs":[],"source":"# Check for nulls.\n\ndf.apply(lambda x: sum(x.isnull()))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0c2d5a5d-51a5-634e-cfcc-164de5d042cb"},"outputs":[],"source":"# There are no null values in the data set. We tend to see clean data sets like this on UCI.\n\n# Plot a heatmap using seaborn. \n\ncorr = df.corr()\nplt.figure(figsize=(12, 12))\nsns.heatmap(corr, vmax=1, square=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a12b4371-8240-495d-48c3-aee819ddf41a"},"outputs":[],"source":"# The heatmap shows that all of the variables have some degree of correlation\n# with the target variable 'outcome'. The variable with the strongest correlation\n# is glucose. We can also see that there is correlation between the variables.\n# Some of these make intuitive sense e.g pregnancies and age."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e4b1700c-62fc-5e4a-1a4e-07547481dd05"},"outputs":[],"source":"# Show precise correlations with the target variable.\n\ncor_dict = corr['Outcome'].to_dict()\ndel cor_dict['Outcome']\nprint(\"List the numerical features decendingly by their correlation with Outcome:\\n\")\nfor ele in sorted(cor_dict.items(), key = lambda x: -abs(x[1])):\n    print(\"{0}: \\t{1}\".format(*ele))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1875f2b0-320b-46e7-a15b-95f7db31a2a9"},"outputs":[],"source":"# Plot a pairplot using seaborn.\n\nsns.pairplot(df)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4f2b9e39-18d6-73be-e48d-f8b94a17004f"},"outputs":[],"source":"sns.distplot(df.BMI, bins=40) "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d420972f-7c0a-a983-04bb-513010cbf244"},"outputs":[],"source":"# BMI guide\n\n#under 18.5 - underweight\n#18.5 to 25 - healthy\n#25 to 30 - overweight\n#over 30 - obese\n\n# Most people in the data set are overweight or obese. Worryingly there are even people with\n# very low BMIs (this could be an error with caused during data entry)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"86a9384a-788a-5463-bb04-dcd32cf7de58"},"outputs":[],"source":"# Let's make our data ready for modelling. \n\n# Separate the target and features.\ntarget = df.Outcome\nfeatures = df.drop('Outcome', axis=1)\n\n# The column names will be used later to help us make sense of the models.\ncols = features.columns.values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8445a049-1693-737d-4623-357773f5dc01"},"outputs":[],"source":"# Standardize features by removing the mean and scaling to unit variance.\nstandard_scaler = StandardScaler()\nfeatures = standard_scaler.fit_transform(features)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"639db7cb-a9a6-0c69-6be1-174198584c2f"},"outputs":[],"source":"# Split the data up in train and test sets using Sklearn's train_test_split module.\n\nX_train, X_test, y_train, y_test = train_test_split(features, target)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"72d4019c-cb9b-05c0-5bfb-b22383aa6100"},"outputs":[],"source":"# Create a function for running logisitic regression on the data. \n# Use grid search to find optimal hyperparameters.\n# The reason we have to use a function is due to the way Python uses parallelization on Windows.\n# http://tinyurl.com/h3g3m8m\n# For more info on Sklearn's logisitc regression function please visit the link below.\n# http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n\ndef log_reg_model(X_train, X_test, y_train, y_test):\n    if __name__ == '__main__':\n\n        param_grid = {'penalty' : ['l1', 'l2'],\n                      'C' : [0.001, 0.01, 0.1, 1, 10]}\n\n        classifier = GridSearchCV(estimator=LogisticRegression(),\n                                  param_grid=param_grid,\n                                  n_jobs=-1,\n                                  cv=3)\n\n        classifier.fit(X_train, y_train)\n\n        best_params = classifier.best_params_\n\n        print('Best parameters: ', best_params)\n\n        validation_accuarcy = classifier.score(X_test, y_test)\n        \n        print('Validation accuracy: ', validation_accuarcy)\n\n        coefficients = classifier.best_estimator_.coef_\n        print('Coefficients: ', list(zip(cols, coefficients[0])))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"64a8e714-efe9-6d91-ebcc-fa509694d7b1"},"outputs":[],"source":"#Find model with best paramters.\n\nlog_reg_model(X_train, X_test, y_train, y_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"502acf13-172b-1d79-9e2f-252968cdb1ba"},"outputs":[],"source":"# Results from 5 runs of the model.\n\n'''\n1st run: Best parameters:  {'penalty': 'l1', 'C': 1}\n         Validation accuracy:  0.776041666667\n    \n2nd run: Best parameters:  {'penalty': 'l1', 'C': 1}\n         Validation accuracy:  0.734375\n    \n3rd run: Best parameters:  {'penalty': 'l1', 'C': 0.1}\n         Validation accuracy:  0.78125\n    \n4th run: Best parameters:  {'penalty': 'l2', 'C': 1}\n         Validation accuracy:  0.791666666667\n    \n5th run: Best parameters:  {'penalty': 'l2', 'C': 0.1}\n         Validation accuracy:  0.760416666667\n'''"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"10a0f010-907d-d65f-a204-00e0c8c256fd"},"outputs":[],"source":"avg_val_accuarcy = ((0.776 + 0.734 + 0.781 + 0.791 + 0.76) / 5) * 100\nprint('average validation accuracy: ',\n      round(avg_val_accuarcy,2), '%')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a100f91-4ce6-c2ba-98e4-98f180e248b3"},"outputs":[],"source":"# Let's see if using Linear SVC will provide a better model for our data.\n# For more info on Linear SVC in Sklearn please see the link below.\n# http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n\ndef svc_model(X_train, X_test, y_train, y_test):\n    if __name__ == '__main__':\n\n        param_grid = {'C':[0.001, 0.01, 0.1, 1.0],\n                      'class_weight':[None, 'balanced']}\n\n        classifier = GridSearchCV(estimator=LinearSVC(),\n                                  param_grid=param_grid,\n                                  n_jobs=-1,\n                                  cv=3)\n\n        classifier.fit(X_train, y_train)\n\n        best_params = classifier.best_params_\n\n        print('Best parameters: ', best_params)\n\n        validation_accuarcy = classifier.score(X_test, y_test)\n        \n        print('Validation accuracy: ', validation_accuarcy)\n\n        coefficients = classifier.best_estimator_.coef_\n        print('Coefficients: ', list(zip(cols, coefficients[0])))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dc663149-7456-18e6-e5d3-312342c34366"},"outputs":[],"source":"# Results from 5 runs of the model.\n\n'''\n1st run: Best parameters:  {'C': 0.1, 'class_weight': None}\n         Validation accuracy:  0.796875\n        \n2nd run: Best parameters:  {'class_weight': None, 'C': 0.01}\n         Validation accuracy:  0.713541666667\n\n3rd run: Best parameters:  {'class_weight': None, 'C': 0.1}\n         Validation accuracy:  0.802083333333\n\n4th run: Best parameters:  {'class_weight': None, 'C': 0.1}\n         Validation accuracy:  0.760416666667\n\n5th run: Best parameters:  {'class_weight': None, 'C': 0.001}\n         Validation accuracy:  0.734375\n'''"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b5f85e44-05fc-9ce3-9459-e2634d71fece"},"outputs":[],"source":"avg_val_accuarcy = ((0.797 + 0.714 + 0.802 + 0.76 + 0.734) / 5) * 100\nprint('average validation accuracy: ',\n      round(avg_val_accuarcy,2), '%')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c86bb66a-1c0e-5ca7-36e5-9128ba8c8c5a"},"outputs":[],"source":"# We see a similar performance to logisitc regression. Next up it's a gradient boosting ensemble\n# algorithm: Gradient Boosting Classifier.\n\n# Note numerous different values were used in the param_grid to hone in on the best paramater\n# combinations. The param grid below is what I ended up with after running the model several \n# times.\n\ndef gradient_boosting_model(X_train, X_test, y_train, y_test):\n    if __name__ == '__main__':\n\n        param_grid = {'learning_rate': [0.015, 0.013, 0.011],\n                      'max_depth': [20, 25, None],\n                      'min_samples_leaf': [9, 10, 11],\n                      'max_features': [0.25, 0.27, 0.3],\n                      'n_estimators': [225, 250, 235]} \n\n        classifier = GridSearchCV(estimator=GradientBoostingClassifier(),\n                                  param_grid=param_grid,\n                                  n_jobs=-1,\n                                  cv=5)\n\n        classifier.fit(X_train, y_train)\n\n        best_params = classifier.best_params_\n\n        print('Best parameters: ', best_params)\n\n        validation_accuarcy = classifier.score(X_test, y_test)\n        \n        print('Validation accuracy: ', validation_accuarcy)\n\n        feature_importances = classifier.best_estimator_.feature_importances_\n        \n        print('Feature importances: ', feature_importances)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa77ca6c-559c-3f39-b016-f67b028d14eb"},"outputs":[],"source":"# Results from 5 runs of the model.\n\n'''\n1st run: Validation accuracy:  0.776\n         Best Parameters: {'max_depth': 20, 'min_samples_leaf': 9,\n         'n_estimators': 250, 'max_features': 0.25, 'learning_rate': 0.011}\n        \n2nd run: Validation accuracy:  0.776\n         Best Paramaters: {'learning_rate': 0.011, 'min_samples_leaf': 9,\n                 'n_estimators': 225, 'max_features': 0.25, 'max_depth': None}\n\n3rd run: Validation accuracy:  0.781\n         Best Parameters: {'max_depth': 20, 'min_samples_leaf': 11, 'n_estimators': 250,\n                 'max_features': 0.27, 'learning_rate': 0.013}\n\n4th run: Validation accuracy:  0.781\n         Best Parameters: {'max_depth': 20, 'n_estimators': 225, 'max_features': 0.3,\n                'min_samples_leaf': 9, 'learning_rate': 0.011}\n\n5th run: Validation accuracy:  0.786\n         Best Parameters: {'learning_rate': 0.015, 'max_depth': 20,\n              'n_estimators': 250,'min_samples_leaf': 11, 'max_features': 0.3}\n'''    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d45301cc-fb53-9313-b993-909e30a37edb"},"outputs":[],"source":"avg_val_accuarcy = ((0.776 + 0.7776 + 0.781 + 0.781 + 0.786) / 5) * 100\nprint('average validation accuracy: ',\n      round(avg_val_accuarcy,2), '%')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"500bee8a-fa70-05e1-46ce-017b695189b7"},"outputs":[],"source":"# The more complex model marginally improved the accuracy on the validation set."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}