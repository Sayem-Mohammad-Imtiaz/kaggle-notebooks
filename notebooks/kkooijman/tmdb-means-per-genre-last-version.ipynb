{"nbformat_minor":1,"metadata":{"language_info":{"version":"3.6.3","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","nbconvert_exporter":"python","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"cells":[{"metadata":{"_uuid":"32ffe50da42e7b2859676479bfd58970981ca547","_cell_guid":"43233258-f411-4372-8545-966de997838e"},"source":"<h1>Assignment TMDB <h1>\n    \n\n    - Part 1: Exploring and Preparing the data for analysis\n    - Part 2: Analyses of different genres\n    - Part 3: ","cell_type":"markdown"},{"metadata":{"_uuid":"01090834384d4d38704f7fd71c67fd838ca95a0f","_cell_guid":"13ca24a5-5033-45e2-af40-059be883228a"},"source":"<h2> Part 1: Exploring and  preparing the data for analysis:  </h2> \n We start with the import of packages we will eventually need. Furthermore, we import the datasets and start with exploring and preparing the data for further analysis.","cell_type":"markdown"},{"metadata":{"_uuid":"5d7762e7e6a3a1948c2badf10cdd6ab594bb47fb","_cell_guid":"b38e4ac8-fc0f-4461-b711-528ae409ea62"},"source":"\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.decomposition import PCA # Principal Component Analysis module\nfrom sklearn.cluster import KMeans # KMeans clustering \nimport nltk\nfrom nltk.corpus import wordnet\nPS = nltk.stem.PorterStemmer()\nimport matplotlib.pyplot as plt\nimport plotly.offline as pyo\npyo.init_notebook_mode()\nfrom plotly.graph_objs import *\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode,iplot\ninit_notebook_mode(connected=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\n\nimport json\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.decomposition import PCA # Principal Component Analysis module\nfrom sklearn.cluster import KMeans # KMeans clustering \n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\n\nmovies = pd.read_csv('../input/tmdb_5000_movies.csv')\ncredits = pd.read_csv('../input/tmdb_5000_credits.csv')\n\n","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"06c444c86f78e4492cb641f21f31e95b8e734ce2","collapsed":true,"_cell_guid":"1b993148-9acb-46fa-b90e-26711e3d44e6"},"source":"Let's just start with some easy questions to get familiar with the data. So what does the data look like? We'll start with taking a look at the movies data frame.","cell_type":"markdown"},{"metadata":{"_uuid":"0eaa70314d70d4b70f12ab71b08d5fc3c99f1f4d","_cell_guid":"2725269d-378e-4efd-b31d-fd0336493087"},"source":"movies.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"5166b5cb4418ad0ca96ed2298a6cec0c2b9ba3cd","_cell_guid":"df7fac77-3d46-468e-950c-d067de697ec6"},"source":"The first thing we notice is that the columns are a bit in an awkward order to take a fine look at the data. A preferable first column of this data frame, would, for example, be the title of the movie and not the movie's budget. \n\nWe also notice that the columns 'genres', 'keywords', 'production_companies', 'production_countries' and 'spoken_languages' are of the dictionary type, so right now they are quite hard to read, but later on we will find a way to work with them.\n\nAmongst the numerical columns, there's a movie budget, a movie ID, popularity, revenue, runtime, a vote average and the amount of votes a movie has received. \n\nA good description of what the popularity variable should be telling us, is no where to be found, so it will be hard to use this column for our predictions later on. Besides the fact that the ID column is numerical, it is also not of interest for making predictions about, for example, the revenue of a movie. For now, we leave this data frame as it is and we'll take a quick look at the other one.","cell_type":"markdown"},{"metadata":{"_uuid":"c4e3cff701a1ce5d1de33592e8417683e86807c6","_cell_guid":"fd8ad6e3-5e36-4995-856a-935ed96bc8e4"},"source":"credits.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"b9c8445c34c73ae7aae19b5d6f748f05d80222f8","_cell_guid":"71164a29-f3d1-4f09-840d-26989b2bf99e"},"source":"So this data frame has way fewer columns. The cast and crew might be interesting later on. Since this data frame contains only two extra columns, we'll try to merge it with the  movies data frame. If they are in the same order, we can just concatenate the data frames, so let's see if in both data frames every row is about the same movie:","cell_type":"markdown"},{"metadata":{"_uuid":"cd5e12e3c4ed20b5157b547be3e64b204254b081","_cell_guid":"25476689-3372-489b-ab6c-dbf0ea6479c7"},"source":"(credits['title']==movies['title']).describe()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"5c662bcfaab873f88a5e185bbad3bd3a4fda7203","_cell_guid":"17e2e700-97e9-4c6c-aac2-543d3d40703e"},"source":"This tells us that every row in the credits data base has the same movie title as the same row in the movies data base. To prevent getting duplicate columns, we'll remove the movie_id and title column from the credits data frame and concatenate them.","cell_type":"markdown"},{"metadata":{"_uuid":"6fc2bf03e1d0570c783dfd363760709b9d11e782","collapsed":true,"_cell_guid":"ce281788-be9f-46dd-b5e2-a756a39eeb14"},"source":"del credits['title']\ndel credits['movie_id']\nmovie_df = pd.concat([movies, credits], axis=1)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"e2f0de4f49e66d38adace911036914d86b07942b","_cell_guid":"4b65a09c-6d3f-4ca2-bdce-493d197934fd"},"source":"movie_df.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4459fabdb2936abbcc81a89f4cffbce8018f6ed9","_cell_guid":"834bfaa0-ca5d-4d40-82c1-9c3b5a7eafff"},"source":"The concatenation worked. However, the columns are a bit in an awkward order and columns like homepage aren't that interesting for us. We choose the interesting columns, put them in a nice order and create a new data frame","cell_type":"markdown"},{"metadata":{"_uuid":"fb9e4b891a2fd5bc19626076fbc6df93cab00a59","_cell_guid":"e8d67d4c-37c7-45f4-91dc-a2c8001523db"},"source":"newCols = ['id','title','release_date','popularity','vote_average','vote_count',\n           'budget','revenue','genres','keywords','cast','crew','tagline', 'runtime', 'production_companies', \n           'production_countries', 'status']\n\ndf2 = movie_df[newCols]\ndf2.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"900f21ba6b884e523eed2eb371bdb4b230a91864","_cell_guid":"1016db94-005b-4ec9-b086-99178d5daa2a"},"source":"Let's explore our data frame a bit more in depth, let's take a look at our numerical columns.","cell_type":"markdown"},{"metadata":{"_uuid":"a8e6c47342dc80d0e11c96a56b4517faca3f5c9f","_cell_guid":"b9b52a14-2417-4f9d-8b8a-d443c8ce6bc4"},"source":"df2.describe().round()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"b50b2b2ecf1cef2040286b9f4233a9882328f695","_cell_guid":"eb8681d4-2fe1-49b8-a63d-f58dbc3071d3"},"source":"Note that runtime consists of a few empty values, before we can really work with our data frame, we need to solve this. We use an imputer for this:","cell_type":"markdown"},{"metadata":{"_uuid":"2b9c00768110931ae1550d75ca75dd454728a0f6","_cell_guid":"50d3d414-705e-4d0e-b8e2-bc7e3d6c8896"},"source":"my_imputer = Imputer()\n\ntemp=df2\nX2 = my_imputer.fit_transform(df2[['runtime']])\ndf2['runtime'] = X2\ndf2.describe().round()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"b8c1c085567949fcde734b31931d29d1e214fcd0","_cell_guid":"261ca333-5e32-44c4-9c50-62c3a731fae9"},"source":"So now at least all the numerical columns are complete. Let's take a quick look at how all the variables are distributed.","cell_type":"markdown"},{"metadata":{"_uuid":"1a6d6ffa43b4e5390defd68f39243cb96910f317","collapsed":true,"_cell_guid":"7cd268fd-c669-48b3-a3f0-9451e91e3b87"},"source":"del df2['id']","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"f2ddbf0dfd9d454d2911bb0c90f66eb13aacc48f","collapsed":true,"_cell_guid":"1ad95288-ddfb-4e05-a759-176c0500955d"},"source":"#df2['vote_classes'] = pd.cut(df2['vote_average'],10, labels=[\"1\", \"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"])\ndf2['vote_classes'] = pd.cut(df2['vote_average'],4, labels=[\"low\", \"medium-low\",\"medium-high\",\"high\"])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"6082811e3b941dbe30427ada8b5c431f80ae8013","collapsed":true,"_cell_guid":"36adf6e7-acfb-4f96-80a0-e43cc6090ea3"},"source":"df2['log_budget'] = np.log(df2['budget'])\ndf2['log_popularity'] = np.log(df2['popularity'])\ndf2['log_vote_average'] = np.log(df2['vote_average'])\ndf2['log_vote_count'] = np.log(df2['vote_count'])\ndf2['log_revenue']= np.log(df2['revenue'])\ndf2['log_runtime']= np.log(df2['runtime'])\ndf3=df2[df2.columns[-5:]]\n\n","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"50fcfbb5f58ed8f3988fd15fda441247e08112cd","collapsed":true,"_cell_guid":"cf6d5f17-2432-4d25-a623-9aa31e689cef"},"source":"#df3.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\ndf3=df3[df3.replace([np.inf, -np.inf], np.nan).notnull().all(axis=1)]\ndf3=df3.dropna(axis=1)\n#df3[~df3.isin([np.nan, np.inf, -np.inf]).any(1)]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"scrolled":false,"_uuid":"f82df58e8ad11a0a8a1f62cac09e1ed8e9f99289","_cell_guid":"012a07bb-20c9-4d84-9c7d-6b2250f495b0"},"source":"from pandas.plotting import scatter_matrix\nscatter_matrix(df3,alpha=0.2, figsize=(20, 20), diagonal='kde')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"240f1bc78084c6b6a8190d2ac5738dde94c7356f","collapsed":true,"_cell_guid":"e540dc5f-dece-4f1d-aa47-f7f675505b20"},"source":"Early_df = df2[df2.columns[0:16]]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"aa490730bc7e9b332dfaad6d3e32e15e14675a77","_cell_guid":"8afcc9a3-2734-4d98-a39d-a182550cf242"},"source":"Early_df.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"8e4138df3db3cdbf2e46cfa8a2f26816d6dd2f52","_cell_guid":"8d4fdc1a-e93e-4bfc-a071-0683c3e72fe5"},"source":"Note that everything is quite skewed. We'll try getting more in depth into this later.","cell_type":"markdown"},{"metadata":{"_uuid":"a2a7eadcb28f782698aa5137833b6d775287c5d4","collapsed":true,"_cell_guid":"b5db640e-b726-456c-bf24-fe75c6ec53c3"},"source":"<h2> Part 3: Analyze genres: <h2>","cell_type":"markdown"},{"metadata":{"_uuid":"fc43c19c83caa27d529a8e3b7aca2943d7ac9ea4","_cell_guid":"5d761fd4-ea8d-46bb-9494-8ded461964c6"},"source":"Now that we've got a good overview of the distribution of our numerical variables, let's take a closer look at our non-numerical variables. We choose to start with looking at the genres, since this variable has got the least variability, should be the most easy target for analysis.\n\nThe genres column contains variables of the string type, while they are in dictionaries. Moreover, the colomn is a json column. To analyse and understand the data it is necessary to change the type of the variable and filter the columns.\nDespite the fact that we already loaded our data for the exploration, we'll reload it here and make sure to load the json columns correctly. To do this, we made use of a few tricks found in another Kernel*","cell_type":"markdown"},{"metadata":{"_uuid":"ae548a0a33350d637c8a605eaabf9a7b81358687","collapsed":true,"_cell_guid":"7cd6fbee-ea88-41e0-b78a-8f762e08dd09"},"source":"def load_tmdb_movies(path):\n    df = pd.read_csv(path)\n    df['release_date'] = pd.to_datetime(df['release_date']).apply(lambda x: x.date())\n    json_columns = ['genres', 'keywords', 'production_countries', 'production_companies', 'spoken_languages']\n    for column in json_columns:\n        df[column] = df[column].apply(json.loads)\n    return df\n\ndef load_tmdb_credits(path):\n    df = pd.read_csv(path)\n    json_columns = ['cast', 'crew']\n    for column in json_columns:\n        df[column] = df[column].apply(json.loads)\n    return df\n\ndef pipe_flatten_names(keywords):\n    return '|'.join([x['name'] for x in keywords])\n\ncredits = load_tmdb_credits(\"../input/tmdb_5000_credits.csv\")\nmovies = load_tmdb_movies(\"../input/tmdb_5000_movies.csv\")\n\ndel credits['title']\ndf = pd.concat([movies, credits], axis=1)\n\ndf['genres'] = df['genres'].apply(pipe_flatten_names)\n\nliste_genres = set()\nfor s in df['genres'].str.split('|'):\n    liste_genres = set().union(s, liste_genres)\nliste_genres = list(liste_genres)\nliste_genres.remove('')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"7359db8de5160173d2fb51428f79d380b2e180ef","_cell_guid":"213962e1-cfe9-459c-a4fb-fff5daa088d6"},"source":"\nSo what happened here is the following: first, we changed the type of the genres variable. Aferwards, we made use of the structure of the column and the *split()* function.  Because the genre always appears after the word *name*, we were able to filter out al the words after the word name and create a list of every genre that occurs in the genre-column.\n\nNow, let's reduce our data frame. To get more insight about the influence of a movie's genre, title, vote_average, release_data, runtime, budget and revenue are the most import important variables. We also add a column for every genre, containing only 1s and 0s whether a movie is of a specific genre or not.  ","cell_type":"markdown"},{"metadata":{"_uuid":"881dc2bf35a21984f07801a8d2bfa28d77657d1e","_cell_guid":"8ab44f36-f209-4f16-9822-1c9f3bcaadb8"},"source":"df_reduced = df[['title','vote_average','release_date','runtime','budget','revenue']].reset_index(drop=True)\n\nfor genre in liste_genres:\n    df_reduced[genre] = df['genres'].str.contains(genre).apply(lambda x:1 if x else 0)\ndf_reduced[:5]\n\ndf_reduced.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"e7c965e688f8a84716b585b787f5633894b14617","_cell_guid":"5e162a45-3bcb-4f6f-b9a6-3bc636e33da8"},"source":"Now that we've got an easy to work with data frame for the movie genres, we can take a look to the distribution of the genres by creating a pie chart*. ","cell_type":"markdown"},{"metadata":{"_uuid":"95e084d16b6e4ad9671d49dcd27411608e481ca2","_cell_guid":"31407e63-2509-499e-8ffa-095c69523b65"},"source":"plt.rc('font', weight='bold')\nf, ax = plt.subplots(figsize=(5,5))\ngenre_count = []\nfor genre in liste_genres:\n    genre_count.append([genre, df_reduced[genre].values.sum()])\ngenre_count.sort(key = lambda x:x[1], reverse = True)\nlabels, sizes = zip(*genre_count)\nlabels_selected = [n if v > sum(sizes) * 0.01 else '' for n, v in genre_count]\nax.pie(sizes, labels=labels_selected,\n      autopct = lambda x:'{:2.0f}%'.format(x) if x>1 else '',\n      shadow = False, startangle=0)\nax.axis('equal')\nplt.tight_layout()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"1748a70022387bb720dafced4697b287a8e3fa50","_cell_guid":"bad2ec31-e627-470a-83fb-47416b3a8c59"},"source":"This pie chart shows which genres are most common in the movies dataset.We find that drama movies are most common, followed by comedy. Afterwards, thriller and action movies are the most popular. Interestingly, half of the movies is from the top 5 genres. (51%). This suggest that the main genre of the most movies are drama, comedy, thriller, action. However, the top 5 most common genres could be seen as more general descriptions. For example, movies with the genre war might also be tagged as action movies or drama movies.\n\nNow let's try to get a more in depth view of the genres. In this cell we calculate the average votes, budget, and revenue for the different genres. we create a new data frame consisiting of every genre and the calculated averages. **","cell_type":"markdown"},{"metadata":{"_uuid":"8746016cd7d4978250628d685a94581a810afaa2","_cell_guid":"ec92952c-0c80-452a-ba30-b63ec341d862"},"source":"mean_per_genre = pd.DataFrame(liste_genres)\n\n#Mean votes average\nnewArray = []*len(liste_genres)\nfor genre in liste_genres:\n    newArray.append(df_reduced.groupby(genre, as_index=True)['vote_average'].mean())\nnewArray2 = []*len(liste_genres)\nfor i in range(len(liste_genres)):\n    newArray2.append(newArray[i][1])\n\nmean_per_genre['mean_votes_average']=newArray2\n\n#Mean budget\nnewArray = []*len(liste_genres)\nfor genre in liste_genres:\n    newArray.append(df_reduced.groupby(genre, as_index=True)['budget'].mean())\nnewArray2 = []*len(liste_genres)\nfor i in range(len(liste_genres)):\n    newArray2.append(newArray[i][1])\n\nmean_per_genre['mean_budget']=newArray2\n\n#Mean revenue \nnewArray = []*len(liste_genres)\nfor genre in liste_genres:\n    newArray.append(df_reduced.groupby(genre, as_index=True)['revenue'].mean())\nnewArray2 = []*len(liste_genres)\nfor i in range(len(liste_genres)):\n    newArray2.append(newArray[i][1])\n\nmean_per_genre['mean_revenue']=newArray2\n\nmean_per_genre['profit'] = mean_per_genre['mean_revenue']-mean_per_genre['mean_budget']\n\nmean_per_genre    ","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"5f9bc16e0291f09f6b02b48f9ff82a2e04392f95","_cell_guid":"e108e1a1-75d6-4e59-b5d5-1602287d3059"},"source":"Let's see which genres are the best scoring ones in each category:","cell_type":"markdown"},{"metadata":{"_uuid":"4b3ec808c1a23885a30017cf0d82b61e5dec77bc","_cell_guid":"4d11c090-22bf-453e-bbed-505734ee6ab0"},"source":"mean_per_genre.sort_values('mean_votes_average', ascending=False).head()\n","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"d08d1db2a1729095d3913bf6833d9ac845c08546","_cell_guid":"98c154de-f21e-495e-a05d-13bc0cffff64"},"source":"mean_per_genre.sort_values('mean_budget', ascending=False).head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"3b102756ffebeb09f231b2b8328e9db381b5966c","_cell_guid":"d938df06-ec40-424a-9511-0a62dbde2397"},"source":"mean_per_genre.sort_values('mean_revenue', ascending=False).head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"06263167d241c26eac291a3132cd4972b12b15e4","_cell_guid":"880e9d11-0f84-45f6-9155-6516f990e29b"},"source":"mean_per_genre.sort_values('profit', ascending=False).head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"5bb5ea965f943ff090ac68d15a63e7a9eb02e109","_cell_guid":"346a51ce-b42d-4e0f-a01b-122096aeddeb"},"source":"It's very interesting to see that the top 5 highest vote average consists of *History, War, Drama, Music* and *Foreign*, while none of these genres are in either one of the other three categories, which all have the same top 3: *Animation, Adventure, Fantasy*. On the one hand, this is easily explained, since budget and revenue should be closely elated and profit is directly derived from budget and revenue. However, we would have expected a higher correlation between the budget and the quality of a movie.\n\nTo go even more in depth, we want to analyse the averages per genre per year.  Therefore, we first extend the dataframe. with the year of release per movie.  Afterwards, we create a new dataframe which contains the average votes, average runtime, and average budget per release year and per genre. \n\nIn the last step in the cell below, only the rows that contain a 1 for genre are kept, so we create a data frame with only the specific genres. ","cell_type":"markdown"},{"metadata":{"_uuid":"756fbc41e0f7b112e6b60e7b2495d17a3b4e6706","collapsed":true,"_cell_guid":"9fcdbaa0-1d9e-4a50-a07c-c58bc5e19746"},"source":"from datetime import datetime\n\nt = df_reduced['release_date']\nt = pd.to_datetime(t)\nt = t.dt.year\ndf_reduced['release_year'] = t\n\ndf_list = []*len(liste_genres)\nfor genre in liste_genres:\n    df_list.append(df_reduced.groupby([genre,'release_year']).mean().reset_index())\n\ndf_per_genre = []*len(liste_genres)\nfor i in range(len(df_list)):\n    df_per_genre.append(df_list[i][df_list[i].ix[:,0] == 1])\n","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4747dbe0851e7d52e6430bdccc42abb8c117dad6","_cell_guid":"42932903-d22d-4533-a46f-67a0883f26e9"},"source":"Now we create tables which contain the average budget, average revenue, and average votes per year per genre. We start with creating a new table with the cloumns 1988 till 2017. Afterwards, the data for the different variables is implemented. **","cell_type":"markdown"},{"metadata":{"_uuid":"203b23b2c8b1a0f415abdb1235833bcf3948675c","collapsed":true,"_cell_guid":"002c17d2-abf5-4b3b-a912-9deb8132d3cc"},"source":"# Budget\ncolumns = range(1988,2018)\nbudget_genre = pd.DataFrame( columns = columns)\n\nfor genre in liste_genres:\n    temp=(df_per_genre[liste_genres.index(genre)].pivot_table(index = genre, columns = 'release_year', values = 'budget', aggfunc = np.mean))\n    temp = temp[temp.columns[-30:]].loc[1]\n    budget_genre.loc[liste_genres.index(genre)]=temp\nbudget_genre['genre']=liste_genres\n\n# Revenue \n\ncolumns = range(1988,2018)\nrevenue_genre = pd.DataFrame( columns = columns)\n\nfor genre in liste_genres:\n    temp=(df_per_genre[liste_genres.index(genre)].pivot_table(index = genre, columns = 'release_year', values = 'revenue', aggfunc = np.mean))\n    temp = temp[temp.columns[-30:]].loc[1]\n    revenue_genre.loc[liste_genres.index(genre)]=temp\nrevenue_genre['genre']=liste_genres\n\n# Vote average \ncolumns = range(1988,2018)\nvote_avg_genre = pd.DataFrame( columns = columns)\n\nfor genre in liste_genres:\n    temp=(df_per_genre[liste_genres.index(genre)].pivot_table(index = genre, columns = 'release_year', values = 'vote_average', aggfunc = np.mean))\n    temp = temp[temp.columns[-30:]].loc[1]\n    vote_avg_genre.loc[liste_genres.index(genre)]=temp\nvote_avg_genre['genre']=liste_genres\n\n#vote_avg_genre.index = vote_avg_genre['genre']","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"2fa64e74ad476f3b29cde956a6f7cc1cb02c5a6a","_cell_guid":"95b4fcf2-6e96-40d1-b3c7-ec8d057da90e"},"source":"Let's take a look at the data frames we generated.","cell_type":"markdown"},{"metadata":{"_uuid":"04218eb4ff4005182b6a3e3223dcac6ec7a7d6e9","_cell_guid":"a2eeae17-0d16-45e1-a18e-717f921d9190"},"source":"### Mean budget per genre per year:","cell_type":"markdown"},{"metadata":{"_uuid":"7b20331d3843949bfa4542a3eae492782a57c8ae","_cell_guid":"782c6822-8bca-472d-9c4e-a89820f35719"},"source":"budget_genre.index = budget_genre['genre']\nbudget_genre","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"07ece8c3632bb39c95bbdf98861e96f1f7b8eecd","_cell_guid":"5e8ff604-af59-4c26-b6a3-461145a1b6d4"},"source":"### Mean revenue per genre per year:","cell_type":"markdown"},{"metadata":{"_uuid":"deda1f2c4f05904804a1f4947c3fa4fb51ec586e","_cell_guid":"411df1fc-e087-4499-b508-02fc1ac35dce"},"source":"revenue_genre.index = revenue_genre['genre']\nrevenue_genre\n","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"04907ef7fd7a8b39c94ea62e4f45fb75867bee72","_cell_guid":"e5047da0-2154-4d27-b34c-3fb509459691"},"source":"### Mean vote average per genre per year:","cell_type":"markdown"},{"metadata":{"_uuid":"9509d23876cfa146e060abc2d4fca507e41994c1","_cell_guid":"c51a50d4-4ec2-4f00-be0f-d59b41a46cc6"},"source":"vote_avg_genre.index = vote_avg_genre['genre']\nvote_avg_genre","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"c1498941ce46ab691c20536666e7d3c6eece9b9f","collapsed":true,"_cell_guid":"6bacfdb4-14f5-45f3-8b10-41fb503d9550"},"source":"#revenue_genre[revenue_genre.columns[1]]\n#budget_genre[budget_genre.columns[1]]\nprofit_genre = revenue_genre[revenue_genre.columns[0:29]]-budget_genre[budget_genre.columns[0:29]]\n#df2[df2.columns[0:16]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"351d48695d117a084b202ae30e565f610c1a2ff5","_cell_guid":"691b10e6-b9a5-4027-93d5-6ac2f867d653"},"source":"We can create more insight in these tables by making heatmaps**. ","cell_type":"markdown"},{"metadata":{"_uuid":"1890ca6fc6b64bc4da01f8e1af29d1209df6db34","_cell_guid":"3a47d56c-5adf-4a0f-84ba-103f230c88f9"},"source":"### Budget:","cell_type":"markdown"},{"metadata":{"_uuid":"a63481acf40045e027cb78c035f0f72d74ab692d","_cell_guid":"0a4a4992-3f40-404b-8167-2d162494d2ab"},"source":"fig, ax = plt.subplots(figsize=(9,9))\ncmap = sns.cubehelix_palette(start = 1.5, rot = 1.5, as_cmap = True)\nsns.heatmap(budget_genre.ix[:,0:30], xticklabels=3, cmap=cmap, linewidths=0.05)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4fb033ce2500812842d37c7464af5928e502d4c1","_cell_guid":"9790a686-a240-4a82-9494-63a9ca7e4119"},"source":"The heatmap shows that in general, movies had  an increasing budget over the years. Especially the genres Fantasy, advernture, family, action, science fiction, and animation. The heatmap also shows that Western movies had an extremely high budget in 2013. This could mean that a costly movie is produced in 2013 which has great influence on the average.  We might later on remove this possible outlier, to get a better overview of the distribution of the rest of the movies.","cell_type":"markdown"},{"metadata":{"_uuid":"bbb6ee57e7b0dc097844ab3da37aa12ce1726b51","_cell_guid":"f39845d7-9268-416d-8ff5-7cf085525c5d"},"source":"## Revenue:","cell_type":"markdown"},{"metadata":{"_uuid":"fa901321d89c0ac0507b0c966398ff0a943547ae","_cell_guid":"2cca70eb-fca1-482c-adba-a5f05cfe25db"},"source":"\nfig, ax = plt.subplots(figsize=(9,9))\ncmap = sns.cubehelix_palette(start = 1.5, rot = 1.5, as_cmap = True)\nsns.heatmap(revenue_genre.ix[:,0:30], xticklabels=3, cmap=cmap, linewidths=0.05)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"a2651aa14cc3d4c5b61dbcde80db57ad77013a1c","_cell_guid":"12e3f28c-f44e-4388-b8e3-b22aa75e612c"},"source":"This heatmap shows the average revenue of genres from 1988 till 2017. The most clear increase of average is in the genres fantasy, adventure, family, action, science fiction. Interestingly, the graph shows that the revenues of the genre animation are colored black in 1994. This is surprisingly because there are no black colored revenues in the graph and in general revenues are lower in 1994 than movies that are produced in later years.  A reason for this could be that there are only a few movies in the genre animation in 1994 and that those movies did extremely well.  The previous heatmap does not show an above average budget for animation movies in 1994. \n","cell_type":"markdown"},{"metadata":{"_uuid":"b961d767163a91f2aef6df00f545015b44fb2327","_cell_guid":"9fe7a05c-496d-48cf-86e0-46e296bbf12f"},"source":"## Profit","cell_type":"markdown"},{"metadata":{"_uuid":"35be4d5c39f5c7553f2e08e3aafbdc9d923e7b07","_cell_guid":"751b98df-8d94-4369-9f87-f208b4e2d372"},"source":"fig, ax = plt.subplots(figsize=(9,9))\ncmap = sns.cubehelix_palette(start = 1.5, rot = 1.5, as_cmap = True)\nsns.heatmap(profit_genre, xticklabels=3, cmap=cmap, linewidths=0.05)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"5bcb408f2d7e4a98a4c010fb2c3183d7d12858a2","_cell_guid":"c57dd905-df4a-4bbf-825f-bfca6caef5eb"},"source":"","cell_type":"markdown"},{"metadata":{"_uuid":"66bcb8d994e6e0979f384b41ee30a8bc710ab93d","_cell_guid":"c8cad215-9830-4fb3-86ff-ebc3642ad972"},"source":"","cell_type":"markdown"},{"metadata":{"_uuid":"42721ee3590329102fb385605398256d90a8139a","_cell_guid":"fe6df8e5-d727-4f0a-8823-7380ecd09286"},"source":"## Vote average:","cell_type":"markdown"},{"metadata":{"_uuid":"465ce8c77199e58d25da58cfc1a03043d1eb0111","_cell_guid":"f32c90a3-5775-4e13-8ef4-d04f04f86d06"},"source":"fig, ax = plt.subplots(figsize=(9,9))\ncmap = sns.cubehelix_palette(start = 1.5, rot = 1.5, as_cmap = True)\nsns.heatmap(vote_avg_genre.ix[:,0:30], xticklabels=3, cmap=cmap, linewidths=0.05)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"d7b212beb3bc44f5d271002072d304a86df07107","_cell_guid":"ea12fc5a-ab2e-4956-b714-49b4e5ded171"},"source":"This heatmap is way darker than the previous two, which suggests that the average is relatively higher than in the other two categories. Most of the categories seem to be getting somewhere around a 6 out of 10 score. Especially notable is the fact that there are very few green or orange colored cells, which should mean that the most movies are on average just a nice watch.","cell_type":"markdown"},{"metadata":{"_uuid":"eb3c3798334132b0b891f8555b080804834e80dd","_cell_guid":"76ab1f38-14b1-4b2a-8383-5551b1132dd9"},"source":"As said before, we would like to remove the very high budget input from the Western genre, to make the heatmap less skewed. Let's see what happens:","cell_type":"markdown"},{"metadata":{"_uuid":"0f8f0acda5902cc103cefab3be2e739703f50cd0","collapsed":true,"_cell_guid":"2fbe958c-f9a1-4b25-99af-635ff92dedde"},"source":"temp = budget_genre\ntemp[2013]=temp[2013].replace(2.550000e+08, 0)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"51b321a729d054067ce40e054d63ea18829074d9","_cell_guid":"d5a4f57b-0db6-4c89-9b37-e15b8329c3ea"},"source":"This heatmap obviously shows that Fantasy Adventure, Science Fiction, and Animation have on average the highest budget. It is also clear that movies had an increasing budget over the years. However, there are a few exceptions. For example  Western movies had an above average budget in 2004 and history in 2000. This might be an effect of individual movies with a high budget. ","cell_type":"markdown"},{"metadata":{"_uuid":"1ecc9a386a0e4eaf29272967cb17103bc5905973","_cell_guid":"60bcb993-b92e-41ea-bf6d-31e7f764ef08"},"source":"fig, ax = plt.subplots(figsize=(9,9))\ncmap = sns.cubehelix_palette(start = 1.5, rot = 1.5, as_cmap = True)\nsns.heatmap(temp.ix[:,0:30], xticklabels=3, cmap=cmap, linewidths=0.05)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"88005a3915074c94c59adf32fdf5a9ae5198a6cd","_cell_guid":"96188e12-4ded-40fa-8e0c-7fd843fe45d8"},"source":"Revenue also has some very extreme outlier, in 1994 in the animation category.","cell_type":"markdown"},{"metadata":{"scrolled":true,"_uuid":"6d0dd832fe4ce7c72c1b0b408cc44bf3f6a91c49","_cell_guid":"952dc5fc-cfd7-4410-8123-76a73daaa37c"},"source":"revenue_genre[1994]\ntemp2 = revenue_genre\ntemp2[1994] = temp2[1994].replace(788241776.0, 0)\ntemp2[1992] = temp2[1992].replace(504050219.0, 0)\ntemp2","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"1f98da4163cbfcb3f71b968fede67fd7f2bdb041","_cell_guid":"e7356e3c-e621-4fb3-93f5-e0b6db73567b"},"source":"temp2[1992][9]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"02c48aa59a77badcbdf87c03c576875f2cb71b25","_cell_guid":"1e20212b-c425-4ca0-ab74-06987498cecd"},"source":"fig, ax = plt.subplots(figsize=(9,9))\ncmap = sns.cubehelix_palette(start = 1.5, rot = 1.5, as_cmap = True)\nsns.heatmap(temp2.ix[:,0:30], xticklabels=3, cmap=cmap, linewidths=0.05)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"02d374f7c8d9481fff775f0d0dea1b1e49173ca1","_cell_guid":"e433ee46-12dc-4323-98e5-ddadfd01d86e"},"source":"from datetime import datetime\n\ndf_genre = pd.DataFrame(columns = ['genre', 'cgenres', 'budget', 'gross', 'year'])\n#list(map(datetime.year, df_reduced[\"release_date\"]))\nt = df['release_date']\nt = pd.to_datetime(t)\nt = t.dt.year\ndf_genre['release_year'] = t\n\ncolnames = ['budget', 'genres', 'revenue']\ndf_clean = df[colnames]\ndf_clean['release_year'] = t\ndf_clean = df_clean.dropna()\ndf_genre = df_genre.dropna()\ndf_clean.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"c55730f23f7b53d4c1f5cba8ee7dd3e7c47fca1c","collapsed":true,"_cell_guid":"95b750ac-70b0-4706-9fce-5ab7d7eabf22"},"source":"def genreRemap(row):\n    global df_genre\n    d = {}\n    genres = np.array(row['genres'].split('|'))\n    n = genres.size\n    d['budget'] = [row['budget']]*n\n    d['revenue'] = [row['revenue']]*n\n    d['year'] = [row['release_year']]*n\n    d['genre'], d['cgenres'] = [], []\n    for genre in genres:\n        d['genre'].append(genre)\n        d['cgenres'].append(genres[genres != genre])\n    df_genre = df_genre.append(pd.DataFrame(d), ignore_index = True)\n\ndf_clean.apply(genreRemap, axis = 1)\ndf_genre['year'] = df_genre['year'].astype(np.int16)\ndf_genre = df_genre[['genre', 'budget', 'gross', 'year', 'cgenres']]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"9ecabdecd9d81edc3b69eb65ab1f6bef14055d8f","_cell_guid":"6d2da110-8bff-4dc7-ab41-72e4208982a0"},"source":"####################\n# make connections #\n####################\nd_genre = {}\ndef connect(row):\n    global d_genre\n    genre = row['genre']\n    cgenres = row['cgenres']\n    if genre not in d_genre:\n        d_cgenres = dict(zip(cgenres, [1]*len(cgenres)))\n        d_genre[genre] = d_cgenres\n    else:\n        for cgenre in cgenres:\n            if cgenre not in d_genre[genre]:\n                d_genre[genre][cgenre] = 1\n            else:\n                d_genre[genre][cgenre] += 1\n                \ndf_genre.apply(connect, axis = 1)\nl_genre = list(d_genre.keys())\nl_genre.sort()\n###########################\n# find largest connection #\n###########################\ncmax = 0\nfor key in d_genre:\n    for e in d_genre[key]:\n        if d_genre[key][e] > cmax:\n            cmax = d_genre[key][e]\n#########################\n# visualize connections #\n#########################\nfrom matplotlib.path import Path\nimport matplotlib.patches as patches\nfrom matplotlib import cm\ncolor = cm.get_cmap('rainbow')\nf, ax = plt.subplots(figsize = (7, 9))\n\ncodes = [Path.MOVETO, Path.CURVE4, Path.CURVE4, Path.CURVE4]\n\nX, Y = 1, 1\nwmin, wmax = 1, 32\namin, amax = 0.1, 0.25\ngetPy = lambda x: Y*(1 - x/len(l_genre))\nfor i, genre in enumerate(l_genre):\n    yo = getPy(i)\n    ax.text(0, yo, genre, ha = 'right')\n    ax.text(X, yo, genre, ha = 'left')\n    for cgenre in d_genre[genre]:\n        yi = getPy(l_genre.index(cgenre))\n        verts = [(0.0, yo), (X/4, yo), (2*X/4, yi), (X, yi)]\n        path = Path(verts, codes)\n        r, g, b, a = color(i/len(l_genre))\n        width = wmin + wmax*d_genre[genre][cgenre]/cmax\n        alpha = amin + amax*(1 - d_genre[genre][cgenre]/cmax)\n        patch = patches.PathPatch(path, facecolor = 'none', edgecolor = (r, g, b), lw = width, alpha = alpha)\n        ax.add_patch(patch)\n\nax.grid(False)\nax.set_xlim(0.0, X)\nax.set_ylim(0.0, Y + 1/len(l_genre))\nax.set_yticklabels([])\nax.set_xticklabels([])\nplt.show()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"ecc8a854eb18dc2233a0d4e2afd7a1b923f9198e","_cell_guid":"1dcaafac-bc89-4a1a-b213-34ea4955f5cf"},"source":"Now we have some more insight on the different genres, let's take a look at different keywords. Are there keywords which influence a movie's rating in one way or another? What about the revenue? We take quite the same approach as with our genre analysis.","cell_type":"markdown"},{"metadata":{"_uuid":"0f87efe68e5abadc03458caae3b3b8ca86400f8b","collapsed":true,"_cell_guid":"f621b6c4-19d5-4f7f-b93d-f59b685278e6"},"source":"credits = load_tmdb_credits(\"../input/tmdb_5000_credits.csv\")\nmovies = load_tmdb_movies(\"../input/tmdb_5000_movies.csv\")\n\ndel credits['title']\ndf = pd.concat([movies, credits], axis=1)\n\ndf['keywords'] = df['keywords'].apply(pipe_flatten_names)\n\nliste_keywords = set()\nfor s in df['keywords'].str.split('|'):\n    liste_keywords = set().union(s, liste_keywords)\nliste_keywords = list(liste_keywords)\nliste_keywords.remove('')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"8643331bafc6267be5e5f40741cc7737ae317cbb","_cell_guid":"5474f169-b489-41f4-bd2a-3610a51bfc97"},"source":"We are interested in which keywords occur the most in our dataset. We use the following function to count them.","cell_type":"markdown"},{"metadata":{"_uuid":"8432c7d552222ef57767ad791098ac29b14e918e","collapsed":true,"_cell_guid":"3b867832-6566-49dd-9f8d-4cb558948410"},"source":"def count_word(df, ref_col, liste):\n    keyword_count = dict()\n    for s in liste: keyword_count[s] = 0\n    for liste_keywords in df[ref_col].str.split('|'):        \n        if type(liste_keywords) == float and pd.isnull(liste_keywords): continue        \n        for s in [s for s in liste_keywords if s in liste]: \n            if pd.notnull(s): keyword_count[s] += 1\n    #______________________________________________________________________\n    # convert the dictionary in a list to sort the keywords by frequency\n    keyword_occurences = []\n    for k,v in keyword_count.items():\n        keyword_occurences.append([k,v])\n    keyword_occurences.sort(key = lambda x:x[1], reverse = True)\n    return keyword_occurences, keyword_count","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"ccd42679760d6c5f7b674cce06632180dfe0ee97","_cell_guid":"f9513036-8c83-470d-becc-cdb38ac7e605"},"source":"keyword_occurences, dum = count_word(df, 'keywords', liste_keywords)\nkeyword_occurences[:5]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"7d95c861cc15901e66eef1d7089fd859cdfc71d4","collapsed":true,"_cell_guid":"6a2775d2-1eae-4602-b214-1b520620ee3e"},"source":"#We collect all the keywords:\ndef keywords_inventory(dataframe, colonne = 'keywords'):\n    PS = nltk.stem.PorterStemmer()\n    keywords_roots  = dict()  # collect the words / root\n    keywords_select = dict()  # association: root <-> keyword\n    category_keys = []\n    icount = 0\n    for s in dataframe[colonne]:\n        if pd.isnull(s): continue\n        for t in s.split('|'):\n            t = t.lower() ; racine = PS.stem(t)\n            if racine in keywords_roots:                \n                keywords_roots[racine].add(t)\n            else:\n                keywords_roots[racine] = {t}\n    \n    for s in keywords_roots.keys():\n        if len(keywords_roots[s]) > 1:  \n            min_length = 1000\n            for k in keywords_roots[s]:\n                if len(k) < min_length:\n                    clef = k ; min_length = len(k)            \n            category_keys.append(clef)\n            keywords_select[s] = clef\n        else:\n            category_keys.append(list(keywords_roots[s])[0])\n            keywords_select[s] = list(keywords_roots[s])[0]\n                   \n    print(\"Nb of keywords in variable '{}': {}\".format(colonne,len(category_keys)))\n    return category_keys, keywords_roots, keywords_select","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4b150c4029d1473da9c8790ad0b7335644d7f4cf","_cell_guid":"eefc7c3b-91d1-414f-aac7-e44b9ed8abaf"},"source":"Of course, different movies use different keywords for their movies. A problem is, that often a lot of those keywords are the same, although they are communicated in a different form by the different movie producers. The function above inventorizes the different keywords using nltk. The package identifies the 'roots' of different words and groups the different words according to its root. Then, we can replace the words that have a common root with their root. In this way, similar words that are phrased differently are assigned a common 'root'.\n\nWhen executing the function, it also shows the amount of different keywords, 9474 in our case.","cell_type":"markdown"},{"metadata":{"_uuid":"a65ef3f0ef7191dc313bd65d3fe5b203a49b831e","_cell_guid":"e97448ba-8f73-46f2-ab32-439dca079246"},"source":"keywords, keywords_roots, keywords_select = keywords_inventory(df, colonne = 'keywords')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"939e8063843c40052a1bc54aa409a6780afede50","_cell_guid":"a4388612-83d4-4f63-80b8-deea6810589d"},"source":"Below are 14 examples of different words with similar roots.","cell_type":"markdown"},{"metadata":{"_uuid":"ade58b9757503f6f0466739f52ed9b38fd8396d6","_cell_guid":"8a1526d7-ecf2-4182-aaad-4cf239dac3af"},"source":"# Plot of a sample of keywords that appear in close varieties \n#------------------------------------------------------------\nicount = 0\nfor s in keywords_roots.keys():\n    if len(keywords_roots[s]) > 1: \n        icount += 1\n        if icount < 15: print(icount, keywords_roots[s], len(keywords_roots[s]))","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"146faeaec8eeb505af8c9f6b7cbd62f63576905a","_cell_guid":"7b501c0a-f5ed-4e7d-a500-89c427fc2e55"},"source":"The function below replaces the different forms of the words by their root.","cell_type":"markdown"},{"metadata":{"_uuid":"75ae5808084a7fc3ba3d315979952adb3d6addf1","collapsed":true,"_cell_guid":"aae7e093-4bcf-4e06-99da-ebde50a7d3b6"},"source":"def remplacement_df_keywords(df, dico_remplacement, roots = False):\n    df_new = df.copy(deep = True)\n    for index, row in df_new.iterrows():\n        chaine = row['keywords']\n        if pd.isnull(chaine): continue\n        nouvelle_liste = []\n        for s in chaine.split('|'): \n            clef = PS.stem(s) if roots else s\n            if clef in dico_remplacement.keys():\n                nouvelle_liste.append(dico_remplacement[clef])\n            else:\n                nouvelle_liste.append(s)       \n        df_new.set_value(index, 'keywords', '|'.join(nouvelle_liste)) \n    return df_new\n","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"01234f25695318259ff9c2553e4e921e928330b3","_cell_guid":"3ca87197-9ab0-4bb7-ba59-0865a546a78b"},"source":"We store the cleaned keywords in a new dataframe.","cell_type":"markdown"},{"metadata":{"_uuid":"97a90e85fac45313868147906ae197bd6c8fdf3b","collapsed":true,"_cell_guid":"737f88f0-b8a9-45c2-b1d4-18a4f927f3eb"},"source":"df_keywords_cleaned = remplacement_df_keywords(df, keywords_select,\n                                               roots = True)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"458cc937f258e6557681b69db97202f3bbd7c3eb","_cell_guid":"dc76bf28-5832-4eea-9df8-f1324ad7fe7f"},"source":"df_keywords_cleaned.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"89249ad4d24d08877ca29294e8d53d6402868912","_cell_guid":"78e53090-aab7-46ec-880a-ee1dbfc71207"},"source":"Next, we will use the nltk package to get rid of synonyms. The function below take a word as a parameter and returns all of the synonyms of that word according to the nltk package.","cell_type":"markdown"},{"metadata":{"_uuid":"7e149d7cec697fad7939ae168352ab6a3db078fd","collapsed":true,"_cell_guid":"bb8509ec-6e55-4269-b523-1c8075eb43f7"},"source":"def get_synonymes(word):\n    lemma = set()\n    for ss in wordnet.synsets(word):\n        for w in ss.lemma_names():\n            #_______________________________\n            # We just get the 'nouns':\n            index = ss.name().find('.')+1\n            if ss.name()[index] == 'n': lemma.add(w.lower().replace('_',' '))\n    return lemma   \n","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"b90684195cae4a2a181b89da0a7296fafef42ca9","collapsed":true,"_cell_guid":"31c09f1a-17d8-4c9a-b3b1-65254f50b644"},"source":"def test_keyword(mot, key_count, threshold):\n    return (False , True)[key_count.get(mot, 0) >= threshold]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"8aea3e8ed2471cbc816fbaea0a6ceeebcc339ab0","_cell_guid":"978ee4ba-a264-401b-b142-68d27cf28c7b"},"source":"keyword_occurences.sort(key = lambda x:x[1], reverse = False)\nkey_count = dict()\nfor s in keyword_occurences:\n    key_count[s[0]] = s[1]\n#__________________________________________________________________________\n# Creation of a dictionary to replace keywords by higher frequency keywords\nremplacement_mot = dict()\nicount = 0\nfor index, [mot, nb_apparitions] in enumerate(keyword_occurences):\n    if nb_apparitions > 5: continue  # only the keywords that appear less than 5 times\n    lemma = get_synonymes(mot)\n    if len(lemma) == 0: continue     # case of the plurals\n    #_________________________________________________________________\n    liste_mots = [(s, key_count[s]) for s in lemma \n                  if test_keyword(s, key_count, key_count[mot])]\n    liste_mots.sort(key = lambda x:(x[1],x[0]), reverse = True)    \n    if len(liste_mots) <= 1: continue       # no replacement\n    if mot == liste_mots[0][0]: continue    # replacement by himself\n    icount += 1\n    if  icount < 8:\n        print('{:<12} -> {:<12} (init: {})'.format(mot, liste_mots[0][0], liste_mots))    \n    remplacement_mot[mot] = liste_mots[0][0]\n\nprint(90*'_'+'\\n'+'The replacement concerns {}% of the keywords.'\n      .format(round(len(remplacement_mot)/len(keywords)*100,2)))","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"3870bf1cae98b50c63176fe4d73970ca44741508","_cell_guid":"6db09c41-37cb-40e8-ae5f-f4bf5e808a45"},"source":"# 2 successive replacements\n#---------------------------\nprint('Keywords that appear both in keys and values:'.upper()+'\\n'+45*'-')\nicount = 0\nfor s in remplacement_mot.values():\n    if s in remplacement_mot.keys():\n        icount += 1\n        if icount < 10: print('{:<20} -> {:<20}'.format(s, remplacement_mot[s]))\n\nfor key, value in remplacement_mot.items():\n    if value in remplacement_mot.keys():\n        remplacement_mot[key] = remplacement_mot[value]            ","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"330f65c2643eff3e11d2aff0b383be24fdd0ecd2","_cell_guid":"69008356-9b38-4c6a-ac10-5a2ed916d74a"},"source":"# replacement of keyword varieties by the main keyword\n#----------------------------------------------------------\ndf_keywords_synonyms = remplacement_df_keywords(df_keywords_cleaned, remplacement_mot, roots = False)   \nkeywords, keywords_roots, keywords_select = keywords_inventory(df_keywords_synonyms, colonne = 'keywords')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"7f33d834f4d0340b64e6db72fd935be7292b6914","_cell_guid":"bdb7e09d-291d-4db7-a2a5-d3b2011e7198"},"source":"# New count of keyword occurences\n#-------------------------------------\nkeywords.remove('')\nnew_keyword_occurences, keywords_count = count_word(df_keywords_synonyms,\n                                                    'keywords',keywords)\nnew_keyword_occurences[:5]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"e9f28bfb4f267b0ff583c6f9f9e3c716c8f50574","collapsed":true,"_cell_guid":"0416746c-4e9b-47d9-ab82-697b7e247963"},"source":"# deletion of keywords with low frequencies\n#-------------------------------------------\ndef remplacement_df_low_frequency_keywords(df, keyword_occurences):\n    df_new = df.copy(deep = True)\n    key_count = dict()\n    for s in keyword_occurences: \n        key_count[s[0]] = s[1]    \n    for index, row in df_new.iterrows():\n        chaine = row['keywords']\n        if pd.isnull(chaine): continue\n        nouvelle_liste = []\n        for s in chaine.split('|'): \n            if key_count.get(s, 4) > 3: nouvelle_liste.append(s)\n        df_new.set_value(index, 'keywords', '|'.join(nouvelle_liste))\n    return df_new","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"64d4e1302d23a88a099f1db35637e0a971a8f5c2","_cell_guid":"4b0716e8-7e0a-4681-aaeb-1cde4a0e21c6"},"source":"If we analyze the amount of keywords again, we see a drastic decrease in different keywords","cell_type":"markdown"},{"metadata":{"_uuid":"e2ea94db3f70ad4c795665a748710eca37ffb733","_cell_guid":"dca5bfdd-ab00-41b1-8ea2-5db2ef992039"},"source":"# Creation of a dataframe where keywords of low frequencies are suppressed\n#-------------------------------------------------------------------------\ndf_keywords_occurence = remplacement_df_low_frequency_keywords(df_keywords_synonyms, new_keyword_occurences)\nkeywords, keywords_roots, keywords_select = keywords_inventory(df_keywords_occurence, colonne = 'keywords')   ","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"6124ed193e807b071fd60ff65893f8f6f46cc2df","_cell_guid":"06c7b7ee-2934-4108-95a0-fde95c9cbde2"},"source":"df_keywords_occurence.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"a6dbc1b410edcf0fe04be581f97e03757dfd11fb","_cell_guid":"8535d3f3-c3b8-45bb-833a-b556519d87d4"},"source":"Now let's try to analyze the influence of keywords on movie ratings or revenue. We apply the same method as where we analyzed the genres.","cell_type":"markdown"},{"metadata":{"_uuid":"684b45d8bbcccf7d98f7920f927507a0757dcdc0","_cell_guid":"5ff10e46-046a-4575-aff8-ef1da73f447d"},"source":"df_keywords= df_keywords_occurence\nkeyword_list = set()\nfor s in df_keywords['keywords'].str.split('|'):\n    keyword_list = set().union(s, keyword_list)\nkeyword_list = list(keyword_list)\nkeyword_list.remove('')\nkeyword_list[:5]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"0130caf3cf8d4fa0ed4fafbe10ee211ffeee9df1","_cell_guid":"e9ceea38-f508-44ad-b26a-97017d9e14f3"},"source":"df_reduced = df_keywords[['title','vote_average','release_date','runtime','budget','revenue']].reset_index(drop=True)\n\nfor keyword in keyword_list:\n    df_reduced[keyword] = df['keywords'].str.contains(keyword).apply(lambda x:1 if x else 0)\ndf_reduced[:5]\n\ndf_reduced.head()\n","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"0f84ce45eba0060d7be00403486202ac1ad3394b","collapsed":true,"_cell_guid":"f1fa8e91-0577-4d60-8d68-fac1441e8ebb"},"source":"mean_per_keyword = pd.DataFrame(keyword_list)\n\n#Mean votes average\nnewArray1 = []*len(keyword_list)\nfor keyword in keyword_list:\n    newArray1.append(df_reduced.groupby(keyword, as_index=True)['vote_average'].mean())\n    \n#Mean budget\nnewArray2 = []*len(keyword_list)\nfor keyword in keyword_list:\n    newArray2.append(df_reduced.groupby(keyword, as_index=True)['budget'].mean())\n    \n#Mean revenue\nnewArray3 = []*len(keyword_list)\nfor keyword in keyword_list:\n    newArray3.append(df_reduced.groupby(keyword, as_index=True)['revenue'].mean())\n\nmean_per_keyword['mean_vote_average']=list(pd.DataFrame(newArray1)[1])\nmean_per_keyword['mean_budget']=list(pd.DataFrame(newArray2)[1])\nmean_per_keyword['mean_revenue']=list(pd.DataFrame(newArray3)[1])\n\n","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"1f622aa7fce04fbd217f501f6ed105b4234b979b","_cell_guid":"dd922f96-c8c4-4105-b6a8-464e2299adf4"},"source":"mean_per_keyword.sort_values('mean_vote_average', ascending=False).head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"e27d2a7d1e04c819a4286ad44435c2333f1653b6","_cell_guid":"4015210c-bbf9-46b7-905c-6ff20e143619"},"source":"mean_per_keyword.sort_values('mean_budget', ascending=False).head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"d7dd4756ffdd032bd4fbc9c009aa637148522e10","_cell_guid":"448686c6-95fb-4adb-aa8e-3181fa54a6fc"},"source":"mean_per_keyword.sort_values('mean_revenue', ascending=False).head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"b25723432fd616f8e9f5f0ca776c29da4c04f16a","_cell_guid":"eb17eef6-7343-4fbe-830c-7bb008e74f17"},"source":"We want to create the tables above again, but then for only the 50 most occuring keywords. Of course it's cool to see the keywords 'hobbit' and 'school of witchcraft' as high-revenue-keywords, but they probably don't occur outside of he Lord of the Rings and Harry Potter movies, so they're actually not that interesting. We start by showing the 50 most occuring keywords in a bar plot.","cell_type":"markdown"},{"metadata":{"_uuid":"f6d51fda9a825d04b1e661a3eeb91dc7072e4445","_cell_guid":"36803a81-c60c-4818-8101-f7ac0caccd5d"},"source":"fig = plt.figure(1, figsize=(18,13))\ntrunc_occurences = new_keyword_occurences[0:50]\n# LOWER PANEL: HISTOGRAMS\nax2 = fig.add_subplot(2,1,2)\ny_axis = [i[1] for i in trunc_occurences]\nx_axis = [k for k,i in enumerate(trunc_occurences)]\nx_label = [i[0] for i in trunc_occurences]\nplt.xticks(rotation=85, fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.xticks(x_axis, x_label)\nplt.ylabel(\"Nb. of occurences\", fontsize = 18, labelpad = 10)\nax2.bar(x_axis, y_axis, align = 'center', color='g')\n#_______________________\nplt.title(\"Keywords popularity\",bbox={'facecolor':'k', 'pad':5},color='w',fontsize = 25)\nplt.show()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"17d68747a5bc6e4474987a23e1481e4bc7e5fdb7","_cell_guid":"95bad9ad-ee1c-487e-b6a4-c1fa91728831"},"source":"We now create a data frame with average movie scores, budget and revenue for the most occuring keywords.","cell_type":"markdown"},{"metadata":{"_uuid":"42f1de04c018d77b935105f6ca16ad77be090bed","collapsed":true,"_cell_guid":"091ab889-f0a3-4791-bd0f-b6e888046cb6"},"source":"Df1 = pd.DataFrame(trunc_occurences)\nDf2 = mean_per_keyword\nresult = Df1.merge(Df2, left_on=0, right_on=0, how='inner')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"8f57f0c5157220d325e583d83cd80988b074aab0","collapsed":true,"_cell_guid":"587087df-1a30-4720-9c0b-3823d0649ee5"},"source":"result = result.rename(columns ={0:'keyword', 1:'occurences'})","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"80df8be2ed5cc75530c0be05c9751362ef06eb37","_cell_guid":"581dca59-09b4-4171-8083-d18379cbf8f5"},"source":"result.sort_values('mean_vote_average', ascending= False)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"f965c3a109f835085a66afdae82b29a7f9461bd4","_cell_guid":"7aa0fa0b-3d61-4e2c-b035-5b419783ac2a"},"source":"Let's try to visualize this table a little bit clearer. We want to see which keywords pop out, so let's make a few plots.","cell_type":"markdown"},{"metadata":{"_uuid":"074bef086da4175c2b872f471d59d4d629d9fb94","_cell_guid":"74f0aa01-bb09-4c05-83ce-062850ff253e"},"source":"result['mean_vote_average'].mean()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"85275a4a97904f3ced2e14ba9b5bed3ff249aa17","_cell_guid":"e0f14016-87f5-49a6-b763-f209a66c04b6"},"source":"import matplotlib.pyplot as plt\n\nax = result.plot.bar(x = 'keyword', y='mean_vote_average', title=\"mean vote average\",\n                     figsize=(15,4), legend=True, fontsize=12, color='green', label = \"mean vote average\")\nax.set_ylim(5, 8)\nax.axhline(y=result['mean_vote_average'].mean(),c=\"blue\",linewidth=0.5, label='mean')\nax.legend()\nplt.show()\n","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"dbeb6b19c63c105b89b334fef31eaf081ae3c369","_cell_guid":"978d521d-da51-427d-9cbd-3be086f00e8d"},"source":"import matplotlib.pyplot as plt\n\nax = result.plot.bar(x = 'keyword', y='mean_budget', title=\"mean budget\",\n                     figsize=(15,4), legend=True, fontsize=12, color='green', label=\"mean budget\")\nax.axhline(y=result['mean_budget'].mean(),c=\"blue\",linewidth=0.5, label='mean')\nax.legend()\nplt.show()\n","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"57ae4f12c0955932c83e6dc3fa977dc6000bbce5","_cell_guid":"8e9d9c25-d4b2-4b70-98d2-1e1d9cb72a09"},"source":"result.sort_values('mean_budget').head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"bd47eb9eddd16d3b83bffc83bd3cf9ff4bbedb36","_cell_guid":"e5cf9c0c-e642-4126-965f-83725df9f2f2"},"source":"Now this is interesting. The keyword with by far the highest average rating - Serial Killer - also has by far the lowest average budget. Also note that superhero movies score below average, but have a highly above average budget. Let's take a look at the revenue:","cell_type":"markdown"},{"metadata":{"_uuid":"06d2179208ef4ca94889297d47d964fd3846a8a4","_cell_guid":"1884a3e9-937b-461f-b1fb-12ca942669ba"},"source":"ax = result.plot.bar(x = 'keyword', y='mean_revenue', title=\"mean revenue\",\n                     figsize=(15,4), legend=True, fontsize=12, color='green', label=\"mean revenue\")\nax.axhline(y=result['mean_revenue'].mean(),c=\"blue\",linewidth=0.5, label='mean')\nax.legend()\nplt.show()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"459aeb3514efda9588f4f476cf832c006eb9c9f8","_cell_guid":"57cc19e5-a74f-430a-b3dc-e9a5d4d08cad"},"source":"So superhero movies do have a high revenue and serial killer movies do not. Let's take a look at the differences:","cell_type":"markdown"},{"metadata":{"_uuid":"35dd4a6f52c93be1af995ad96bafbc3cd5acff68","_cell_guid":"7cf0da2c-c928-4a66-8b41-e514ef38e9ca"},"source":"result['profit'] = result['mean_revenue'] - result['mean_budget']\nresult.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"b021190c0bd54c8029f2516919003a7b1ac312a4","_cell_guid":"fc48e38f-bc07-4198-b2c9-2e8528b89b05"},"source":"ax = result.plot.bar(x = 'keyword', y='profit', title=\"profit\",\n                     figsize=(15,4), legend=True, fontsize=12, color='green', label=\"profit\")\nax.axhline(y=result['profit'].mean(),c=\"blue\",linewidth=0.5, label='mean')\nax.legend()\nplt.show()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"87f83fc1507891087267358298ab7b9861b7b854","_cell_guid":"ec083c10-8c1b-434c-b000-4f4a9350545e"},"source":"What's also notable is that despite the popularity of female directors and independent movies, they do not have high scores on either revenue or vote_average.","cell_type":"markdown"},{"metadata":{"_uuid":"32537261107136dcd697728063e94af56fc90fd2","_cell_guid":"d9a54300-83e7-4eb3-ab8a-d406766509d6"},"source":"# Cast analysis","cell_type":"markdown"},{"metadata":{"_uuid":"abe1066b4ddaecd68127cc2873d972f2d998956c","_cell_guid":"e2951afd-0530-4f7f-ac97-06a627fbf7a6"},"source":"A previous version of this dataset only contained the top three actors per movie. Since we only want to analyze the most important actors of a movie and since the old dataset was a bit more suited to do that, we convert the dataset back to its previous state using Sohier Dane's method.","cell_type":"markdown"},{"metadata":{"_uuid":"a6ace70ddf2f0d017efc7435510d9e26a7f80836","collapsed":true,"_cell_guid":"47bcc5c9-5798-443b-a7ce-20c7e87e01c2"},"source":"# Columns that existed in the IMDB version of the dataset and are gone.\nLOST_COLUMNS = [\n    'actor_1_facebook_likes',\n    'actor_2_facebook_likes',\n    'actor_3_facebook_likes',\n    'aspect_ratio',\n    'cast_total_facebook_likes',\n    'color',\n    'content_rating',\n    'director_facebook_likes',\n    'facenumber_in_poster',\n    'movie_facebook_likes',\n    'movie_imdb_link',\n    'num_critic_for_reviews',\n    'num_user_for_reviews'\n                ]\n\n# Columns in TMDb that had direct equivalents in the IMDB version. \n# These columns can be used with old kernels just by changing the names\nTMDB_TO_IMDB_SIMPLE_EQUIVALENCIES = {\n    'budget': 'budget',\n    'genres': 'genres',\n    'revenue': 'gross',\n    'title': 'movie_title',\n    'runtime': 'duration',\n    'original_language': 'language',  # it's possible that spoken_languages would be a better match\n    'keywords': 'plot_keywords',\n    'vote_count': 'num_voted_users',\n                                         }\n\nIMDB_COLUMNS_TO_REMAP = {'imdb_score': 'vote_average'}\n\n\ndef safe_access(container, index_values):\n    # return a missing value rather than an error upon indexing/key failure\n    result = container\n    try:\n        for idx in index_values:\n            result = result[idx]\n        return result\n    except IndexError or KeyError:\n        return pd.np.nan\n\n\ndef get_director(crew_data):\n    directors = [x['name'] for x in crew_data if x['job'] == 'Director']\n    return safe_access(directors, [0])\n\n\ndef pipe_flatten_names(keywords):\n    return '|'.join([x['name'] for x in keywords])\n\n\ndef convert_to_original_format(movies, credits):\n    # Converts TMDb data to make it as compatible as possible with kernels built on the original version of the data.\n    tmdb_movies = movies.copy()\n    tmdb_movies.rename(columns=TMDB_TO_IMDB_SIMPLE_EQUIVALENCIES, inplace=True)\n    tmdb_movies['title_year'] = pd.to_datetime(tmdb_movies['release_date']).apply(lambda x: x.year)\n    # I'm assuming that the first production country is equivalent, but have not been able to validate this\n    tmdb_movies['country'] = tmdb_movies['production_countries'].apply(lambda x: safe_access(x, [0, 'name']))\n    tmdb_movies['language'] = tmdb_movies['spoken_languages'].apply(lambda x: safe_access(x, [0, 'name']))\n    tmdb_movies['director_name'] = credits['crew'].apply(get_director)\n    tmdb_movies['actor_1_name'] = credits['cast'].apply(lambda x: safe_access(x, [1, 'name']))\n    tmdb_movies['actor_2_name'] = credits['cast'].apply(lambda x: safe_access(x, [2, 'name']))\n    tmdb_movies['actor_3_name'] = credits['cast'].apply(lambda x: safe_access(x, [3, 'name']))\n    tmdb_movies['genres'] = tmdb_movies['genres'].apply(pipe_flatten_names)\n    tmdb_movies['plot_keywords'] = tmdb_movies['plot_keywords'].apply(pipe_flatten_names)\n    return tmdb_movies","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"730238b8f08e1b63432d7fd186ddf96dcf0d665e","collapsed":true,"_cell_guid":"94c898d6-4296-45d3-a205-5c5c94c9286a"},"source":"credits = load_tmdb_credits(\"../input/tmdb_5000_credits.csv\")\nmovies = load_tmdb_movies(\"../input/tmdb_5000_movies.csv\")\ndf = convert_to_original_format(movies, credits)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"c08c05bbae26852a3116c6ec354023e607e50aee","_cell_guid":"7bbc7ca7-17b1-46d6-b3d1-3aca45c0f3bf"},"source":"We can now see that the dataframe is simplified.","cell_type":"markdown"},{"metadata":{"_uuid":"dcd319c68c28ed4d16e18b0bd685fefd15f7a252","_cell_guid":"eddae0b8-a33d-424a-a53e-3497b7a5f798"},"source":"df.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"d3fba4e36f83e1cb774cba499f6f26bc76388b45","collapsed":true,"_cell_guid":"2b248512-413d-478b-870f-28d66ed76424"},"source":"df3 = df # We store a copy of the dataframe for later use","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"0da97396c6d6dd2ba8c52e6a5eacab0b9327ad5b","_cell_guid":"8fc93727-1a18-4958-9a94-94deabb29656"},"source":"Next, we delete all the columns we won't be needing for this analysis.","cell_type":"markdown"},{"metadata":{"_uuid":"3dae2e43a4963cc49c6da59cde6f111e05b039e2","collapsed":true,"_cell_guid":"91c055e1-9b74-452b-9a1d-33b25e5de3fd"},"source":"columns = ['homepage', 'plot_keywords', 'language', 'overview', 'popularity', 'tagline',\n           'original_title', 'num_voted_users', 'country', 'spoken_languages', 'duration',\n          'production_companies', 'production_countries', 'status']\n\ndf = df.drop(columns, axis=1)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"9bf6d3cdc1ec1cc8fab2ac5b57d3d7a08c53da27","_cell_guid":"aec21b45-f83e-4596-97c0-5c7c2d4bddfb"},"source":"We are interested in the same descriptives for the actors, as we were for keywords and the genres. To do that, we first have to, once again, restructure the dataframe.\n\nWe first create a seperate dataframe for each of the three actors, after which we can combine them to get one dataframe with all three types of actor.","cell_type":"markdown"},{"metadata":{"_uuid":"c6004649c9d6718982e88371cb0771f2e8248d72","collapsed":true,"_cell_guid":"7d8ff5fa-0a9c-4897-b6a3-32af6448d4b5"},"source":"liste_genres = set()\nfor s in df['genres'].str.split('|'):\n    liste_genres = set().union(s, liste_genres)\nliste_genres = list(liste_genres)\nliste_genres.remove('')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"7779cafa966a65df0cb17e5ff5527e351fa896e2","collapsed":true,"_cell_guid":"12618724-7d7f-4cd9-8f74-fee184e070b0"},"source":"df_reduced = df[['actor_1_name', 'vote_average',\n                 'title_year', 'movie_title', 'gross', 'budget']].reset_index(drop = True)\nfor genre in liste_genres:\n    df_reduced[genre] = df['genres'].str.contains(genre).apply(lambda x:1 if x else 0)\n\ndf_reduced2 = df[['actor_2_name', 'vote_average',\n                 'title_year', 'movie_title', 'gross', 'budget']].reset_index(drop = True)\nfor genre in liste_genres:\n    df_reduced2[genre] = df['genres'].str.contains(genre).apply(lambda x:1 if x else 0)\n\ndf_reduced3 = df[['actor_3_name', 'vote_average',\n                 'title_year', 'movie_title', 'gross', 'budget']].reset_index(drop = True)\nfor genre in liste_genres:\n    df_reduced3[genre] = df['genres'].str.contains(genre).apply(lambda x:1 if x else 0)\n","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"250a05c45d2f10ece9a41614b39db22cfab1fcb8","_cell_guid":"e30d8796-a0ee-45c8-9cce-482e25306fa1"},"source":"Next, we combine the three dataframes.","cell_type":"markdown"},{"metadata":{"_uuid":"77e2d8bf764c2f6e736a26cd00475a96d2692570","_cell_guid":"8bf77ec8-cb1b-4648-ac24-d2596c6691e8"},"source":"df_reduced = df_reduced.rename(columns={'actor_1_name': 'actor'})\ndf_reduced2 = df_reduced2.rename(columns={'actor_2_name': 'actor'})\ndf_reduced3 = df_reduced3.rename(columns={'actor_3_name': 'actor'})\n\ntotal = [df_reduced, df_reduced2, df_reduced3]\ndf_total = pd.concat(total)\ndf_total.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"2b8a5f267e754c480fd8d0c09090186a1bbd52a9","_cell_guid":"0bbf867f-104d-42fa-86c0-e38a3308fd1a"},"source":"We compute averages for all actors in two categories: vote_average and title_year. We also compute an actors favorite genre.","cell_type":"markdown"},{"metadata":{"_uuid":"c4b5ace836f0a704bf9bc82735f4ba1d7de219dd","collapsed":true,"_cell_guid":"f95998f7-0189-406c-8404-e9270087be9f"},"source":"df_actors = df_total.groupby('actor').mean()\ndf_actors.loc[:, 'favored_genre'] = df_actors[liste_genres].idxmax(axis = 1)\ndf_actors.drop(liste_genres, axis = 1, inplace = True)\ndf_actors = df_actors.reset_index()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"2fdb1da4d5e9c01733fa1b19f889d2a4f6c0a418","_cell_guid":"0092fe4c-2341-4045-baf0-2c18d0980edb"},"source":"#df_actors.loc[df_actors['actor'] == \"Gary Oldman\"]\ndf_total.loc[df_total['actor'] == \"Gary Oldman\"].sort_values('vote_average')\n#df.loc[df['column_name'] == some_value]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"115b64f5b0defc51eaef0891ddb787257cc4a850","_cell_guid":"a4e5cdb4-085e-4144-8134-6969b891c250"},"source":"df_actors.loc[df_actors['actor'] == \"Gary Oldman\"]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"f5920c65acdda9d48e9dcaac8f8b980e2d848c1e","_cell_guid":"c965f73e-961e-40bf-bb6f-1eac4b9af06d"},"source":"We expect the dataframe to contain a lot of actors that have only a single observation. These observation are likely to cause outliers if these observations are very extreme. We delete all actors that are linked to less than 10 movies in our dataframe.","cell_type":"markdown"},{"metadata":{"_uuid":"12d27822113362d4b862692ecd33fd521e9fe968","collapsed":true,"_cell_guid":"acf8c9cf-3fbd-4ed9-85d1-1b152fe958e2"},"source":"df_appearance = df_total[['actor', 'title_year']].groupby('actor').count()\ndf_appearance = df_appearance.reset_index(drop = True)\nselection = df_appearance['title_year'] > 9\nselection = selection.reset_index(drop = True)\nmost_prolific = df_actors[selection]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"213218e01e05303804beb722026d7ed86f2d41f6","_cell_guid":"4f5dc608-3c61-42b4-8749-ff41be111a66"},"source":"Now that we have a clear dataframe, let us show some descriptive statistics. We first sort the dataframe on all the different attributes from highest to lowest.","cell_type":"markdown"},{"metadata":{"_uuid":"10b26127f8c94dffcce751e80b4982842e7455eb","_cell_guid":"8e4de64f-f28e-49a4-aead-9b02c6d89214"},"source":"most_prolific.sort_values('vote_average', ascending=False).head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"6f3da9eb00082b1f983f24c82bb9d6d8c270de05","_cell_guid":"47cef9a1-d25f-4540-81ef-b2e5251c2718"},"source":"most_prolific.sort_values('gross', ascending=False).head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"a2428eca4a3499540491476e1820877f678d722a","_cell_guid":"25fc801a-da82-47ce-b29b-b4fb85796790"},"source":"most_prolific.sort_values('budget', ascending=False).head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"d312bfd5bb1a3dfc2b5a4a34263f551b361927bf","_cell_guid":"81006e87-c497-41a2-b674-d605224df89b"},"source":"Looks like Sir Ian McKellen has had quite a career. He came out on top on all three of our attributes. He plays in the movies with the highsest budget, but returns this with the highest average revenues. It makes sense that these enormous budgets lead to good movies. This is reflected by him having the highest average score on IMDB.\n\nWe can now develop several plots to analyze our actors. Let us start by plotting the average budget per actor and the average revenue per actor.","cell_type":"markdown"},{"metadata":{"_uuid":"551f0233c3a8522a04c107d815bf6e3a560238ad","_cell_guid":"89ec1557-afd0-4408-9f44-8588a6d04e8a"},"source":"genre_count = []\nfor genre in liste_genres:\n    genre_count.append([genre, df_reduced[genre].values.sum()])\ngenre_count.sort(key = lambda x:x[1], reverse = True)\nlabels, sizes = zip(*genre_count)\nlabels_selected = [n if v > sum(sizes) * 0.01 else '' for n, v in genre_count]\nreduced_genre_list = labels[:19]\ntrace=[]\nfor genre in reduced_genre_list:\n    trace.append({'type':'scatter',\n                  'mode':'markers',\n                  'y':most_prolific.loc[most_prolific['favored_genre']==genre,'gross'],\n                  'x':most_prolific.loc[most_prolific['favored_genre']==genre,'budget'],\n                  'name':genre,\n                  'text': most_prolific.loc[most_prolific['favored_genre']==genre,'actor'],\n                  'marker':{'size':10,'opacity':0.7,\n                            'line':{'width':1.25,'color':'black'}}})\nlayout={'title':'Actors favored genres',\n       'xaxis':{'title':'mean year of activity'},\n       'yaxis':{'title':'mean score'}}\nfig=Figure(data=trace,layout=layout)\npyo.iplot(fig)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"636f68737f741fcee0ba62fa4b4721a44aade97a","_cell_guid":"5eb0569c-8f65-4052-aad4-7a0fdee2dd54"},"source":"We can also use this data to highlight single actors. Let us take a look at actors for who we have data of more than 20 movies.","cell_type":"markdown"},{"metadata":{"_uuid":"c38e36c048697f79330c76c219236c2a2eb05b65","_cell_guid":"7aa7a27d-95f7-47b0-a60b-94786dcc8d3d"},"source":"selection = df_appearance['title_year'] > 20\nmost_prolific = df_actors[selection]\nmost_prolific","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"ddd3cb64c937cfacf0b71e8e725da40c55039ed1","_cell_guid":"5ee5d406-8ec9-4e9a-9043-3e7a0ea256f6"},"source":"So let's have a look at Morgan Freeman. We would like to have a clear overview of all the movies he played in and what his movies scored on IMDB. We can do this using a polar chart.","cell_type":"markdown"},{"metadata":{"_uuid":"047275aef0f5dba0bd57a39bf14e029292345bd8","collapsed":true,"_cell_guid":"b2782dcb-f81f-4dbe-b67d-ab8b466f3060"},"source":"class Trace():\n    #____________________\n    def __init__(self, color):\n        self.mode = 'markers'\n        self.name = 'default'\n        self.title = 'default title'\n        self.marker = dict(color=color, size=110,\n                           line=dict(color='white'), opacity=0.7)\n        self.r = []\n        self.t = []\n    #______________________________\n    def set_color(self, color):\n        self.marker = dict(color = color, size=110,\n                           line=dict(color='white'), opacity=0.7)\n    #____________________________\n    def set_name(self, name):\n        self.name = name\n    #____________________________\n    def set_title(self, title):\n        self.na = title\n    #___________________________\n    def set_actor(self, actor):\n        self.actor = actor\n    \n    #__________________________\n    def set_values(self, r, t):\n        self.r = np.array(r)\n        self.t = np.array(t)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"0b444f97c1af537b44a6685a554dfce345ca3bf5","collapsed":true,"_cell_guid":"9ecb3acb-70e0-40cd-8f87-82ceb7974e84"},"source":"names =['Morgan Freeman']\ndf2 = df_reduced[df_reduced['actor'] == 'Morgan Freeman']\ntotal_count  = 0\nyears = []\nimdb_score = []\ngenre = []\ntitles = []\nactor = []\nfor s in liste_genres:\n    icount = df2[s].sum()\n    #__________________________________________________________________\n    # Here, we set the limit to 3 because of a bug in plotly's package\n    if icount > 3: \n        total_count += 1\n        genre.append(s)\n        actor.append(list(df2[df2[s] ==1 ]['actor']))\n        years.append(list(df2[df2[s] == 1]['title_year']))\n        imdb_score.append(list(df2[df2[s] == 1]['vote_average'])) \n        titles.append(list(df2[df2[s] == 1]['movie_title']))\nmax_y = max([max(s) for s in years])\nmin_y = min([min(s) for s in years])\nyear_range = max_y - min_y\n\nyears_normed = []\nfor i in range(total_count):\n    years_normed.append( [360/total_count*((an-min_y)/year_range+i) for an in years[i]])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"90ce019b44efea429617b4be95bee7bfda3b2984","collapsed":true,"_cell_guid":"20c9f2c2-2196-47df-a474-2464a6a07405"},"source":"color = ('royalblue', 'grey', 'wheat', 'c', 'firebrick', 'seagreen', 'lightskyblue',\n          'lightcoral', 'yellowgreen', 'gold', 'tomato', 'violet', 'aquamarine', 'chartreuse', 'red')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"9a976cff5292ac881a2b8ce0ecdaf44aa5ab2be9","_cell_guid":"bcd24d9d-f33b-4adc-a8e4-332ddc129373"},"source":"trace = [Trace(color[i]) for i in range(total_count)]\ntr    = []\nfor i in range(total_count):\n    trace[i].set_name(genre[i])\n    trace[i].set_title(titles[i])\n    trace[i].set_values(np.array(imdb_score[i]),\n                        np.array(years_normed[i]))\n    tr.append(go.Scatter(r      = trace[i].r,\n                         t      = trace[i].t,\n                         mode   = trace[i].mode,\n                         name   = trace[i].name,\n                         marker = trace[i].marker,\n#                         text   = ['default title' for j in range(len(trace[i].r))], \n                         hoverinfo = 'all'\n                        ))        \nlayout = go.Layout(\n    title='Morgan Freeman',\n    font=dict(\n        size=15\n    ),\n    plot_bgcolor='rgb(223, 223, 223)',\n    angularaxis=dict(        \n        tickcolor='rgb(253,253,253)'\n    ),\n    hovermode='Closest',\n)\nfig = go.Figure(data = tr, layout=layout)\npyo.iplot(fig)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"6accced0f3b5f648663fc11d95fc89707f3d8305","_cell_guid":"212bcd50-dc63-4569-be1d-206db54cb1f8"},"source":"Unfortunately, plotly doesn't allow us to put hover text on the different notes. This means we can't add the movie name and year of release to all the different nodes.","cell_type":"markdown"},{"metadata":{"_uuid":"78d6b99da5015704b15db176df14714a233f7f9c","_cell_guid":"4dcd650a-b4d5-4bc0-bbe4-a18fec1af724"},"source":"# Director Analysis","cell_type":"markdown"},{"metadata":{"_uuid":"ec2ced58c00c6aef08438cb075e15b240fb4d3d9","_cell_guid":"49d16cca-fa4e-45c2-a6e7-6ee9284dfbc6"},"source":"We start by retrieving the copy of the database we created earlier.","cell_type":"markdown"},{"metadata":{"_uuid":"d9c602ee1941e33d10f873b185a2834bcb6d8d30","collapsed":true,"_cell_guid":"6fec6cf6-e8cd-4874-8138-072a29f614a3"},"source":"df = df3","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"efc3ca4cecd8234853bffb73df0757dff8aa6937","_cell_guid":"74c975d7-9f61-4600-a042-807e11985600"},"source":"We start the actual analysis by computing the average per movie and total gross of the directors. We only took into account the directs for which we have at least 4 movies as observations, to exclude extreme outliers. Not surprisingly, the top rated directors are probably directors you have heard about.","cell_type":"markdown"},{"metadata":{"_uuid":"896aa69608cc5d849120717b6fc5994deba781b5","collapsed":true,"_cell_guid":"32ca8e07-8f09-41ea-82f6-73dfc99a0b2d"},"source":"def create_comparison_database(name, value, x, no_films):\n    \n    comparison_df = df3.groupby(name, as_index=False)\n    \n    if x == 'mean':\n        comparison_df = comparison_df.mean()\n    elif x == 'median':\n        comparison_df = comparison_df.median()\n    elif x == 'sum':\n        comparison_df = comparison_df.sum() \n    \n    # Create database with either name of directors or actors, the value being compared i.e. 'gross',\n    # and number of films they're listed with. Then sort by value being compared.\n    name_count_key = df[name].value_counts().to_dict()\n    comparison_df['films'] = comparison_df[name].map(name_count_key)\n    comparison_df.sort_values(value, ascending=False, inplace=True)\n    comparison_df[name] = comparison_df[name].map(str) + \" (\" + comparison_df['films'].astype(str) + \")\"\n   # create a Series with the name as the index so it can be plotted to a subgrid\n    comp_series = comparison_df[comparison_df['films'] >= no_films][[name, value]][10::-1].set_index(name).ix[:,0]\n    \n    return comp_series","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"077e27202edb2af0b5625f839f9db90794d2e1ac","_cell_guid":"3865d1af-c8a0-49b1-987c-03fe38505858"},"source":"fig = plt.figure(figsize=(18,6))\n\n# Director_name\nplt.subplot2grid((2,3),(0,0), rowspan = 2)\ncreate_comparison_database('director_name','gross','sum', 4).plot(kind='barh', color='#006600')\nplt.legend().set_visible(False)\nplt.title(\"Total Gross for Directors with 4+ Films\")\nplt.ylabel(\"Director (no. films)\")\nplt.xlabel(\"Gross (in billons)\")\n\nplt.subplot2grid((2,3),(0,1), rowspan = 2)\ncreate_comparison_database('director_name','gross','mean', 4).plot(kind='barh', color='#ffff00')\nplt.legend().set_visible(False)\nplt.title('Average revenue for Directors with 4+ Films')\nplt.ylabel(\"Director (no. films)\")\nplt.xlabel(\"Gross (in billons)\")\n\nplt.tight_layout()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"df19fd9cf119213728983e92a9c7e461e9d9a128","_cell_guid":"649780a2-7bde-41ae-bc3d-3b11b2475e92"},"source":"Next, we plot the average IMDB score and average year of activity for the directors.","cell_type":"markdown"},{"metadata":{"_uuid":"68891b44e031bdc5377b4e31ea62ac2abcdc42b9","_cell_guid":"432c7b5c-f894-435a-84ba-a625d5639c5e"},"source":"fig = plt.figure(figsize=(18,6))\n\n# Director_name\nplt.subplot2grid((2,3),(0,0), rowspan = 2)\ncreate_comparison_database('director_name','budget','mean', 4).plot(kind='barh', color='#006600')\nplt.legend().set_visible(False)\nplt.title(\"Average budget for Directors with 4+ Filmss\")\nplt.ylabel(\"Director (no. films)\")\nplt.xlabel(\"Budget (in billons)\")\n\nplt.subplot2grid((2,3),(0,1), rowspan = 2)\ncreate_comparison_database('director_name','vote_average','mean', 4).plot(kind='barh', color='#ffff00')\nplt.legend().set_visible(False)\nplt.title('Mean IMDB Score for Directors with 4+ Films')\nplt.ylabel(\"Director (no. films)\")\nplt.xlabel(\"IMDB Score\")\nplt.xlim(0,10)\n\nplt.tight_layout()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"2a37f1e7d85bd9c39613eed6081888facc788aaf","_cell_guid":"e5e557ed-79fc-4aba-a00c-ce719e3fb089"},"source":"Notice how many of the directors that have a very high average budget per movie were nowhere to be seen in the revenue plot. Implying that, although they make expensive movies, they don't make the most grossing movies. Also note that a lot of high scoring directors are not found in the top ten highest budgeted directors. This implies that a big budget doesn't necessarily lead to a good, or well-received, movie. On the other hand, it shows that some directors, for instance Hayao Miyazaki, is capable of creating excellent movies with needing a very high budget.\n\nNow, all of this is of course only true for directors with 4+ movies. It is possible that directors with few movies were lucky. A question to ask the dataset could be whether there exist any directors that are capable of consistently creating well-received movies, without the need for big budgets. To answer this question we plot the average budget next to the average score per director, for directors with at least 15 movies.","cell_type":"markdown"},{"metadata":{"_uuid":"82f69f433dcfa060661b97666e7b07003a59948f","_cell_guid":"a6d3470f-f088-40a3-a398-11137b704653"},"source":"fig = plt.figure(figsize=(18,6))\n\n# Director_name\nplt.subplot2grid((2,3),(0,0), rowspan = 2)\ncreate_comparison_database('director_name','budget','mean', 10).plot(kind='barh', color='#006600')\nplt.legend().set_visible(False)\nplt.title(\"Average budget for Directors with 15+ Filmss\")\nplt.ylabel(\"Director (no. films)\")\nplt.xlabel(\"Budget (in billons)\")\n\nplt.subplot2grid((2,3),(0,1), rowspan = 2)\ncreate_comparison_database('director_name','vote_average','mean', 10).plot(kind='barh', color='#ffff00')\nplt.legend().set_visible(False)\nplt.title('Mean IMDB Score for Directors with 15+ Films')\nplt.ylabel(\"Director (no. films)\")\nplt.xlabel(\"IMDB Score\")\nplt.xlim(0,10)\n\nplt.tight_layout()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"0b1405cbf0c99e8999150345339e79ee929dcd96","_cell_guid":"af8c14ef-5316-4a50-9ffd-cf177b7c828a"},"source":"Now, we easily see that the two bar plots have more directors in common. Still, there are some directors who manage to create excellent movies without the need for a big budget. A funny observation is Michael Bay. While he is easily the king of budget, he is nowhere to be found in the top ten highest scoring directors.","cell_type":"markdown"},{"metadata":{"_uuid":"04ccd6c62a45543f645edaedc41885d627a672f4","_cell_guid":"1c3649b0-840e-4b65-bb1e-14d54672ac79"},"source":"# Numerical Analysis","cell_type":"markdown"},{"metadata":{"_uuid":"79e868e165762027b21417614faa573b18f0415d","_cell_guid":"40b87af8-9feb-4f80-9390-54149b9b2bcb"},"source":"So let's take a closer look at our the numerical columns in our data frame.  Let's start by creating a data frame containing only numbered columns.","cell_type":"markdown"},{"metadata":{"_uuid":"b43552318df71dd7f9a01bae0b7b0eed1c46594c","collapsed":true,"_cell_guid":"6e33c569-b149-44f8-b8c5-4765d15abe97"},"source":"df2 = Early_df","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"e4e02b9d70dae3e4b64cf47bfda4c29fd551bbed","collapsed":true,"_cell_guid":"d422128b-813e-41b1-adaf-e0525bc5546c"},"source":"df2['log_budget'] = np.log(df2['budget'])\ndf2['log_popularity'] = np.log(df2['popularity'])\ndf2['log_revenue']= np.log(df2['revenue'])\ndf2['log_runtime']= np.log(df2['runtime'])\ndf2['log_vote_average'] = np.log(df2['vote_average'])\ndf2['log_vote_count'] = np.log(df2['vote_count'])\n\ndf3=df2[df2.columns[-6:]]\n\ndf3=df3[df3.replace([np.inf, -np.inf], np.nan).notnull().all(axis=1)]\ndf3=df3.dropna(axis=1)\n\ncolumn_order = ['log_budget', 'log_popularity','log_revenue','log_runtime',\n              'log_vote_average','log_vote_count']\ndf3 = df3[column_order]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"df3.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"576695fd153903fe07abd8a14c6c47d28fefbcf2","_cell_guid":"8b0352b8-73d6-4e68-850b-666631e71c2f"},"source":"num_list = ['budget','popularity','revenue','runtime','vote_average','vote_count']\nmovie_num = df2[num_list]\nmovie_num.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"df2.columns","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"There are several machine learning models we want to try on our data frame to predict the revenue and vote_average of a movie. Despite we might have found some relations between certain genres or keywords and the movie revenue or budget, we won't apply machine learning tools on these variables because of the massive amount of possibilities there are. Of course there are only limited amount of different genres, but every movie consists of a combination of genres and no clear preferences for a certain genre are given.","cell_type":"markdown"},{"metadata":{"_uuid":"226ed5a53e351db495ff5ac2103a9762d96067be","_cell_guid":"6f77ce8d-c2b2-4912-bdd2-84074e068b13"},"source":"Let's take a look at how everything is correlated:","cell_type":"markdown"},{"metadata":{},"source":"f, ax = plt.subplots(figsize=(12,10))\nplt.title('Pearson Correlation of Log Movie Features')\nsns.heatmap(df3.astype(float).corr(), linewidths=0.25, vmax=1.0, square=True,\n           cmap=\"YlGnBu\", linecolor='black', annot=True)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"42ce321f8ef81985c12abda83e9d2fc3a342c6fd","_cell_guid":"62319b61-a9d2-49ff-bae0-1b3aee252c81"},"source":"f, ax = plt.subplots(figsize=(12,10))\nplt.title('Pearson Correlation of Movie Features')\nsns.heatmap(movie_num.astype(float).corr(), linewidths=0.25, vmax=1.0, square=True,\n           cmap=\"YlGnBu\", linecolor='black', annot=True)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"d7550a9c5ba83663441285e84fc367e96be3b365","_cell_guid":"fc0e9f56-bf27-4ade-8f79-8d56368e0798"},"source":"We see quite a few dark/blue squares. These are the higher correlated variables. To be able to make predictions about certain movies later on, this might be some important knowledge.","cell_type":"markdown"},{"metadata":{},"source":"We start by splitting the data in a training and a test set, we start with our regular data and try predicting the vote average.","cell_type":"markdown"},{"metadata":{},"source":"from sklearn.model_selection import train_test_split\n\ntraining_list = ['popularity','runtime','vote_count']\ntraining = movie_num[training_list]\ntarget = movie_num['vote_average']\n\nX = training.values\nY = target.values\n\nX_train, X_test, Y_train, Y_test = train_test_split(\nX, Y, test_size=0.33, random_state=42)\n\n'''\ntraining_list = ['log_budget','log_popularity','log_revenue','log_runtime','log_vote_count']\ntraining = df3[training_list]\ntarget = df3['log_vote_average']\n\nX = training.values\nY = target.values\n\nX_train, X_test, Y_train, Y_test = train_test_split(\nX, Y, test_size=0.33, random_state=42)\n'''","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"y_test consists of continuous variables. However, many classifiers expect categorical values as the target vector, so we convert our training scores with a little help of scikit's labelencoder function.","cell_type":"markdown"},{"metadata":{"collapsed":true},"source":"# We're going to need some classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true},"source":"#y_test are continuous variables\nfrom sklearn import preprocessing\nfrom sklearn import utils\n\nlab_enc = preprocessing.LabelEncoder()\nencoded = lab_enc.fit_transform(Y_train)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"X_train.shape, Y_train.shape, X_test.shape, Y_test.shape, encoded.shape","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"#Logistic regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, encoded)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, encoded) * 100, 2)\nprint('logistic regression:', acc_log)\n\n#SVM\nsvc = SVC()\nsvc.fit(X_train, encoded)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, encoded)*100,2)\nprint('Support Vector Machine:', acc_svc)\n\n#Knearestneighbors\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, encoded)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, encoded) * 100, 2)\nprint('KNN:', acc_knn)\n\n# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, encoded)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, encoded) * 100, 2)\nprint('Gaussian Naive Bayes:', acc_gaussian)\n\n# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, encoded)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, encoded) * 100, 2)\nprint('Perceptron:', acc_perceptron)\n\n# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, encoded)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, encoded) * 100, 2)\nprint('linear SVC:', acc_linear_svc)\n\n# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, encoded)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, encoded) * 100, 2)\nprint('Stochastic Gradient Descent:', acc_sgd)\n\n# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, encoded)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, encoded) * 100, 2)\nprint(\"Decision Tree:\", acc_decision_tree)\n\n# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, encoded)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, encoded)\nacc_random_forest = round(random_forest.score(X_train, encoded) * 100, 2)\nprint(\"Random forest:\", acc_random_forest)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"movie_num.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"'''\nfrom matplotlib import pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, KFold\nimport numpy as np\n\nprint(__doc__)\n\n# Number of random trials\nNUM_TRIALS = 30\n\n# Load the dataset\ntraining_list = ['budget','popularity','revenue','runtime','vote_count']\ntraining = movie_num[training_list]\ntarget = movie_num['vote_average']\n\nX = training.values\nY = target.values\n\nX_train, X_test, Y_train, Y_test = train_test_split(\nX, Y, test_size=0.33, random_state=42)\n\nlab_enc = preprocessing.LabelEncoder()\nencoded = lab_enc.fit_transform(Y_train)\n\n#X_iris = iris.data X_iris -> X_train\n#y_iris = iris.target Y_iris _> Y_train\n\n# Set up possible values of parameters to optimize over\np_grid = {\"C\": [1, 10, 100],\n          \"gamma\": [.01, .1]}\n\n# We will use a Support Vector Classifier with \"rbf\" kernel\nsvm = SVC(kernel=\"rbf\")\n\n# Arrays to store scores\nnon_nested_scores = np.zeros(NUM_TRIALS)\nnested_scores = np.zeros(NUM_TRIALS)\n'''","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"'''\n# Loop for each trial\nfor i in range(NUM_TRIALS):\n\n    # Choose cross-validation techniques for the inner and outer loops,\n    # independently of the dataset.\n    # E.g \"LabelKFold\", \"LeaveOneOut\", \"LeaveOneLabelOut\", etc.\n    inner_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n    outer_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n\n    # Non_nested parameter search and scoring\n    clf = GridSearchCV(estimator=svm, param_grid=p_grid, cv=inner_cv)\n    clf.fit(X_train, encoded)\n    non_nested_scores[i] = clf.best_score_\n\n    # Nested CV with parameter optimization\n    nested_score = cross_val_score(clf, X=X_train, y=encoded, cv=outer_cv)\n    nested_scores[i] = nested_score.mean()\n\nscore_difference = non_nested_scores - nested_scores\n\nprint(\"Average difference of {0:6f} with std. dev. of {1:6f}.\"\n      .format(score_difference.mean(), score_difference.std()))\n'''","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"'''\n# Plot scores on each trial for nested and non-nested CV\nplt.figure()\nplt.subplot(211)\nnon_nested_scores_line, = plt.plot(non_nested_scores, color='r')\nnested_line, = plt.plot(nested_scores, color='b')\nplt.ylabel(\"score\", fontsize=\"14\")\nplt.legend([non_nested_scores_line, nested_line],\n           [\"Non-Nested CV\", \"Nested CV\"],\n           bbox_to_anchor=(0, .4, .5, 0))\nplt.title(\"Non-Nested and Nested Cross Validation on TMDB\",\n          x=.5, y=1.1, fontsize=\"15\")\n\n# Plot bar chart of the difference.\nplt.subplot(212)\ndifference_plot = plt.bar(range(NUM_TRIALS), score_difference)\nplt.xlabel(\"Individual Trial #\")\nplt.legend([difference_plot],\n           [\"Non-Nested CV - Nested CV Score\"],\n           bbox_to_anchor=(0, 1, .8, 0))\nplt.ylabel(\"score difference\", fontsize=\"14\")\n\nplt.show()\n'''","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"from sklearn import metrics\n# testing score\nscore = metrics.f1_score(Y_test, Y_pred, pos_label=list(set(Y_test)))\n# training score\nscore_train = metrics.f1_score(Y_train, pred_train, pos_label=list(set(Y_train)))","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"What if we cut Y_train into bins instead of encoding it?","cell_type":"markdown"},{"metadata":{"collapsed":true},"source":"from sklearn.model_selection import train_test_split\n\ntraining_list = ['popularity','runtime','vote_count']\ntraining = movie_num[training_list]\ntarget = movie_num['vote_average']\n\nX = training.values\nY = target.values\n\nX_train, X_test, Y_train, Y_test = train_test_split(\nX, Y, test_size=0.33, random_state=42)\n\nY_train = pd.cut(Y_train,10, labels=[\"1\", \"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"#Logistic regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint('logistic regression:', acc_log)\n\n#SVM\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train)*100,2)\nprint('Support Vector Machine:', acc_svc)\n\n#Knearestneighbors\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nprint('KNN:', acc_knn)\n\n# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nprint('Gaussian Naive Bayes:', acc_gaussian)\n\n# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nprint('Perceptron:', acc_perceptron)\n\n# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nprint('linear SVC:', acc_linear_svc)\n\n# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nprint('Stochastic Gradient Descent:', acc_sgd)\n\n# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nprint(\"Decision Tree:\", acc_decision_tree)\n\n# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(\"Random forest:\", acc_random_forest)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"BinModels = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nBinModels.sort_values(by='Score', ascending=False)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"models.sort_values(by='Score', ascending=False)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"# A different approach","cell_type":"markdown"},{"metadata":{"collapsed":true},"source":"# Importing packages\n\nimport os\nimport pandas as pd\nfrom pandas import DataFrame,Series\nfrom sklearn import tree\nimport matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\nfrom sklearn import neighbors\nfrom sklearn import linear_model\n%matplotlib inline","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"movie_num.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"correlation = []\nfor i in range(0,6):\n    correlation.append(movie_num.ix[:,i].corr(movie_num['vote_average']))","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"correlation","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"We use the highest three correlation variables, since more would only increase our error. This means we use variables 1,3,5","cell_type":"markdown"},{"metadata":{"collapsed":true},"source":"from sklearn.model_selection import train_test_split\n\ntraining_list = ['popularity','runtime','vote_count']\ntraining = movie_num[training_list]\ntarget = movie_num['vote_average']\n\nX = training.values\nY = target.values\n\nX_train, X_test, Y_train, Y_test = train_test_split(\nX, Y, test_size=0.33, random_state=42)\n\n#Y_train = pd.cut(Y_train,10, labels=[\"1\", \"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true},"source":"","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"X_train","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"# Revenue","cell_type":"markdown"},{"metadata":{},"source":"from sklearn.model_selection import train_test_split\n\ntraining_list = ['budget','popularity','vote_average','runtime','vote_count']\ntraining = movie_num[training_list]\ntarget = movie_num['revenue']\n\nX = training.values\nY = target.values\n\nX_train, X_test, Y_train, Y_test = train_test_split(\nX, Y, test_size=0.33, random_state=42)\n\n'''\ntraining_list = ['log_budget','log_popularity','log_vote_average','log_runtime','log_vote_count']\ntraining = df3[training_list]\ntarget = df3['log_revenue']\n\nX = training.values\nY = target.values\n\nX_train, X_test, Y_train, Y_test = train_test_split(\nX, Y, test_size=0.33, random_state=42)\n'''","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true},"source":"#y_test are continuous variables\nfrom sklearn import preprocessing\nfrom sklearn import utils\n\nlab_enc = preprocessing.LabelEncoder()\nencoded = lab_enc.fit_transform(Y_train)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"#Logistic regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, encoded)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, encoded) * 100, 2)\nprint('logistic regression:', acc_log)\n\n#SVM\nsvc = SVC()\nsvc.fit(X_train, encoded)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, encoded)*100,2)\nprint('Support Vector Machine:', acc_svc)\n\n#Knearestneighbors\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, encoded)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, encoded) * 100, 2)\nprint('KNN:', acc_knn)\n\n# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, encoded)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, encoded) * 100, 2)\nprint('Gaussian Naive Bayes:', acc_gaussian)\n\n# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, encoded)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, encoded) * 100, 2)\nprint('Perceptron:', acc_perceptron)\n\n# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, encoded)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, encoded) * 100, 2)\nprint('linear SVC:', acc_linear_svc)\n\n# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, encoded)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, encoded) * 100, 2)\nprint('Stochastic Gradient Descent:', acc_sgd)\n\n# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, encoded)c\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, encoded) * 100, 2)\nprint(\"Decision Tree:\", acc_decision_tree)\n\n# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, encoded)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, encoded)\nacc_random_forest = round(random_forest.score(X_train, encoded) * 100, 2)\nprint(\"Random forest:\", acc_random_forest)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"We actually want to keepY_train as a continuous variable","cell_type":"markdown"},{"metadata":{"collapsed":true},"source":"from sklearn.model_selection import train_test_split\n\ntraining_list = ['budget','popularity','revenue','runtime','vote_count']\ntraining = movie_num[training_list]\ntarget = movie_num['vote_average']\n\nX = training.values\nY = target.values\n\nX_train, X_test, Y_train, Y_test = train_test_split(\nX, Y, test_size=0.33, random_state=42)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"from sklearn import tree\nclf = tree.DecisionTreeRegressor()\nclf = clf.fit(X_train, Y_train)\nY_pred = clf.predict(X_test)\nacc_decision_tree = round(clf.score(X_train, Y_train) * 100, 2)\nprint(\"Decision Tree:\", acc_decision_tree)\n\n#Linear regression\nfrom sklearn import linear_model\nfrom sklearn import metrics\n\nlin = linear_model.LinearRegression()\nlin.fit(X_train, Y_train)\nY_pred = lin.predict(X_test)\nacc_linReg = round(lin.score(X_train, Y_train)*100,2)\nprint(\"Linear Regression:\", acc_linReg)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"# Random Forest","cell_type":"markdown"},{"metadata":{},"source":"#Set a random seed\nset.seed(754)\n\n#Build the model\n#rf_model = ","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"969817e6cdadca82eba8c11589d2f4b32c258a41","_cell_guid":"e6d3250a-f2a3-4286-8efb-e75fa2932303"},"source":"# Comparing different regression techniques\n\nWe want to compare a few regression techniques to help us in making predictions. We'll use linear regression and random forest, as treated in the lectures.\nWe start by recreating our numerical data frame.","cell_type":"markdown"},{"metadata":{"_uuid":"15695ca4fda786de92c1c941e0184027a79e1bc1","_cell_guid":"597cf1a9-bfb8-450e-bda0-1e286e1b0b52"},"source":"num_list = ['budget','popularity','revenue','runtime','vote_average','vote_count']\nmovie_num = df2[num_list]\nmovie_num.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"correlation = []\nfor i in range(0,6):\n    correlation.append(movie_num.ix[:,i].corr(movie_num['vote_average']))\ncorrelation","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"We will use only the columns with the highest correlations.","cell_type":"markdown"},{"metadata":{},"source":"training_list = ['popularity','runtime','vote_count']\ntraining = movie_num[training_list]\ntarget = movie_num['vote_average']","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true},"source":"X = training.values\ny = target.values","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true},"source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"f = plt.figure(figsize=(10,5))\nplt.scatter(X_test[:,1], y_test, s=50,label=\"Real vote_average\");\nplt.scatter(X_test[:,1], y_pred_lr,s=100, c='r',label=\"Predicted vote_average\");\nplt.ylabel(\"vote_average\");\nplt.legend(loc=2);","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"68e98ba57763e9068dcb1c196d71beccc5c1193a","_cell_guid":"b5dd4215-1935-4822-b541-7f8d8cf949eb"},"source":"We want the vote_average to be our target values, budget, popularity, revenue, runtime and vote_count are trainng values.","cell_type":"markdown"},{"metadata":{"_uuid":"1dc487caeec25cbe85762877a41e4a53d659496b","collapsed":true,"_cell_guid":"c4fecb54-86d1-4400-be15-4ed986f8dd8c"},"source":"training_list = ['budget','popularity','revenue','runtime','vote_count']\ntraining = movie_num[training_list]\ntarget = movie_num['vote_average']","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true},"source":"from sklearn import linear_model\n# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_pred_lr = regr.predict(X_test)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"e37b9ef3726f1c7c459c9ea8515ac2d457cefd59","collapsed":true,"_cell_guid":"bc571ae4-8050-443d-9adb-b80266d35dcd"},"source":"X = training.values\ny = target.values","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"6b7152f81a6313f3ec577052db7d869081c8cade","_cell_guid":"d0da5d56-9087-42a5-9def-b6a5c747efbd"},"source":"We split our data in a train and a test frame.","cell_type":"markdown"},{"metadata":{"_uuid":"8a85cd5bab9a10d364a701042f7c672b82bd6870","collapsed":true,"_cell_guid":"6b653ee0-fd89-4759-a7e0-ba531995a9c7"},"source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"d0c7610b88612d8b9313b2cec348c492cf9a8675","_cell_guid":"f6d22aed-de64-4180-81f1-956873f7e395"},"source":"Now let's train a linear regression model and plot the results: \\***","cell_type":"markdown"},{"metadata":{"_uuid":"fde481a3fd940fc8e2b0709c0876fb0588d48726","collapsed":true,"_cell_guid":"c3560a54-1b4a-469c-8a45-4bb8f86b826d"},"source":"from sklearn import linear_model\n# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_pred_lr = regr.predict(X_test)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"49c84f85029d56466c0f251b1ff72cb337d61bb4","_cell_guid":"07666f6b-e968-41bc-ba44-77784aa2e925"},"source":"f = plt.figure(figsize=(10,5))\nplt.scatter(X_test[:,1], y_test, s=50,label=\"Real vote_average\");\nplt.scatter(X_test[:,1], y_pred_lr,s=100, c='r',label=\"Predicted vote_average\");\nplt.ylabel(\"vote_average\");\nplt.legend(loc=2);","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"9cb65257aedfaa6b2d94cd24aeb8d18a26ee90d6","_cell_guid":"3fb01fc3-d3bb-4dbe-ac37-09989ce2c40d"},"source":"Now let's see what happens if we use a random forest regression model:","cell_type":"markdown"},{"metadata":{"_uuid":"87ee0d2142eba0336bf7d4ec6bb5b5c33f86bccc","collapsed":true,"_cell_guid":"8da35a43-733e-44f8-af70-55e2cef4dd9c"},"source":"from sklearn.ensemble import RandomForestRegressor\n# Create linear regression object\nrf = RandomForestRegressor(1)\n\n# Train the model using the training sets\nrf.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_pred_rf = rf.predict(X_test)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"5336e7a59a4e759c9c392055fce5435fbdcd4d94","_cell_guid":"888edfd3-b52f-4593-aa74-1df6e1f3c448"},"source":"f = plt.figure(figsize=(10,5))\nplt.scatter(X_test[:,1], y_test, s=50,label=\"Real vote_average\");\nplt.scatter(X_test[:,1], y_pred_rf,s=100, c='r',label=\"Predited vote_average\");\nplt.ylabel(\"vote_average\");\nplt.legend(loc=2);","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"25c914c5c7c5ba9f394df2e62593d0d717e6f53c","_cell_guid":"e65b692e-4a32-41da-8dc8-e0df742ecf8c"},"source":"And let's compare them:","cell_type":"markdown"},{"metadata":{"_uuid":"83df0085bd433d1142b3b7e11b2835819137f531","_cell_guid":"948e08f7-e4c0-4cd1-8bb9-efaac89955f0"},"source":"from sklearn.metrics import mean_squared_error\n\nerror_lr = mean_squared_error(y_test,y_pred_lr)\nerror_rf = mean_squared_error(y_test,y_pred_rf)\n\nprint(error_lr)\nprint(error_rf)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"f8f8c5fdf808977fa9f7570fd9013b6f9d8771ee","_cell_guid":"643637d3-468a-4161-bf77-2ca79a9ce1e2"},"source":"f = plt.figure(figsize=(10,5))\nplt.bar(range(2),[error_lr,error_rf], yerr=np.std(a))\nplt.xlabel(\"Classifiers\");\nplt.ylabel(\"Mean Squared Error of the vote_average\");\nplt.xticks(range(2),['Linear Regression','Random Forest'])\nplt.legend(loc=2);","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"np.std(error_rf)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"92d7af5fbef7109c95da5046e858a809da0ff356","_cell_guid":"d0083beb-abf2-485c-b9d1-b647c2658974"},"source":"So the mean squared error for the random forest regression is a little higher than for the linear regression, but both estimators seem to be very decent.","cell_type":"markdown"},{"metadata":{"_uuid":"d7c2f6f0b6d45ad72fabaab774e03c5ecd7320f6","_cell_guid":"e0fe96bc-a8b5-463e-9b90-f95c423159d5"},"source":"\\* https://www.kaggle.com/fabiendaniel/categorizing-actors-hands-on-plotly <br>\n\\** https://www.kaggle.com/diegoinacio/imdb-genre-based-analysis <br>\n\\*** introduction to data science, week 4, Comparison of Regression Techniques on House prediction prices.ipynb\n\nResources:\n\nhttps://www.kaggle.com/fabiendaniel/film-recommendation-engine\n\nhttps://www.kaggle.com/fabiendaniel/categorizing-actors-hands-on-plotly\n\nhttps://www.kaggle.com/willacy/director-and-actor-s-total-gross-and-imdb-score","cell_type":"markdown"},{"metadata":{},"source":"movie_num.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"from sklearn.preprocessing import StandardScaler\n\nmovie_num['normBudget'] = StandardScaler().fit_transform(movie_num['budget'].reshape(-1, 1))\nmovie_num['normPopularity'] = StandardScaler().fit_transform(movie_num['popularity'].reshape(-1, 1))\nmovie_num['normRevenue'] = StandardScaler().fit_transform(movie_num['revenue'].reshape(-1, 1))\nmovie_num['normVoteCount'] = StandardScaler().fit_transform(movie_num['vote_count'].reshape(-1, 1))\nmovie_num['normRuntime'] = StandardScaler().fit_transform(movie_num['runtime'].reshape(-1, 1))\nmovie_num['vote_classes'] = pd.cut(movie_num['vote_average'],2, labels=[0,1])\n\nmovie_test = movie_num.drop(['budget','popularity','vote_count','revenue','runtime','vote_average'],axis=1)\nmovie_test.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true},"source":"X = movie_test.ix[:, movie_test.columns != 'vote_classes']\ny = movie_test.ix[:, movie_test.columns == 'vote_classes']","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"number_high_scoring = len(movie_test[movie_test.vote_classes == 4])\nnumber_high_scoring","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"# Number of data points in the minority class\nnumber_high_scoring = len(movie_test[movie_test.vote_classes == 1])\nhigh_indices = np.array(movie_test[movie_test.vote_classes == 1].index)\n\n# Picking the indices of the normal classes\nlow_indices = movie_test[movie_test.vote_classes == 0].index\n\n# Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\nrandom_normal_indices = np.random.choice( number_high_scoring, low_indicesreplace = False)\nrandom_normal_indices = np.array(random_normal_indices)\n\n# Appending the 2 indices\nunder_sample_indices = np.concatenate([high_indices,random_normal_indices])\n\n# Under sample dataset\nunder_sample_data = movie_test.iloc[under_sample_indices,:]\n\nX_undersample = under_sample_data.ix[:, under_sample_data.columns != 'vote_classes']\ny_undersample = under_sample_data.ix[:, under_sample_data.columns == 'vote_classes']\n\n# Showing ratio\nprint(\"Percentage of normal transactions: \", len(under_sample_data[under_sample_data.vote_classes == 0])/len(under_sample_data))\nprint(\"Percentage of fraud transactions: \", len(under_sample_data[under_sample_data.vote_classes == 1])/len(under_sample_data))\nprint(\"Total number of transactions in resampled data: \", len(under_sample_data))","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":" import matplotlib.pyplot \nmovie_num.hist()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true},"source":"from sklearn.model_selection import train_test_split\n\ntraining_list = ['popularity','runtime','vote_count']\ntraining = movie_num[training_list]\ntarget = movie_num['vote_average']\n\nX = training.values\nY = target.values\n\nX_train, X_test, Y_train, Y_test = train_test_split(\nX, Y, test_size=0.33, random_state=42)","outputs":[],"execution_count":null,"cell_type":"code"}],"nbformat":4}