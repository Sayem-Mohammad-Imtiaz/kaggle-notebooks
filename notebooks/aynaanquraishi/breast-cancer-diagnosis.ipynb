{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import axes3d\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, roc_auc_score\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-07-25T08:28:48.549173Z","iopub.execute_input":"2021-07-25T08:28:48.549729Z","iopub.status.idle":"2021-07-25T08:28:49.711174Z","shell.execute_reply.started":"2021-07-25T08:28:48.549615Z","shell.execute_reply":"2021-07-25T08:28:49.710461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importing data and getting shape\ndf = pd.read_csv(\"../input/breast-cancer-detection/data.csv\")\ndf.shape","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-25T08:28:49.712282Z","iopub.execute_input":"2021-07-25T08:28:49.71264Z","iopub.status.idle":"2021-07-25T08:28:49.740462Z","shell.execute_reply.started":"2021-07-25T08:28:49.712613Z","shell.execute_reply":"2021-07-25T08:28:49.739572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()\n# we can see here that last column is having NaN values so we have to resolve it","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-25T08:28:49.742356Z","iopub.execute_input":"2021-07-25T08:28:49.74264Z","iopub.status.idle":"2021-07-25T08:28:49.788715Z","shell.execute_reply.started":"2021-07-25T08:28:49.742612Z","shell.execute_reply":"2021-07-25T08:28:49.787759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finding missing values\ndf.isnull().sum()\n# we noticed here that in 'unnamed: 32' feature we have zero non-null data so its useless for our problem","metadata":{"execution":{"iopub.status.busy":"2021-07-25T08:28:49.790415Z","iopub.execute_input":"2021-07-25T08:28:49.790719Z","iopub.status.idle":"2021-07-25T08:28:49.800732Z","shell.execute_reply.started":"2021-07-25T08:28:49.790689Z","shell.execute_reply":"2021-07-25T08:28:49.79981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dropping irrelevant features \ndf.drop(['id', 'Unnamed: 32'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T08:28:49.802133Z","iopub.execute_input":"2021-07-25T08:28:49.802517Z","iopub.status.idle":"2021-07-25T08:28:49.811569Z","shell.execute_reply.started":"2021-07-25T08:28:49.802486Z","shell.execute_reply":"2021-07-25T08:28:49.81052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# here we get some more insight about our data set\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T08:28:49.813004Z","iopub.execute_input":"2021-07-25T08:28:49.813568Z","iopub.status.idle":"2021-07-25T08:28:49.91431Z","shell.execute_reply.started":"2021-07-25T08:28:49.813522Z","shell.execute_reply":"2021-07-25T08:28:49.913251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking Malignant (M) and Benign (B) class observations in dataset\ndf['diagnosis'].value_counts()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-25T08:28:49.915786Z","iopub.execute_input":"2021-07-25T08:28:49.91615Z","iopub.status.idle":"2021-07-25T08:28:49.924635Z","shell.execute_reply.started":"2021-07-25T08:28:49.91612Z","shell.execute_reply":"2021-07-25T08:28:49.923601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualizing the same using bar graphs\ndf.diagnosis.value_counts().plot(kind = \"bar\")\nplt.title(\"People Diagnosed Benign and Malignant\")","metadata":{"execution":{"iopub.status.busy":"2021-07-25T08:28:49.927147Z","iopub.execute_input":"2021-07-25T08:28:49.927424Z","iopub.status.idle":"2021-07-25T08:28:50.134037Z","shell.execute_reply.started":"2021-07-25T08:28:49.9274Z","shell.execute_reply":"2021-07-25T08:28:50.133238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing corelation between features which will help in further process using heatmap\ndf['diagnosis'] = df['diagnosis'].map({'M': 1, 'B':0})\ncorr = df.corr()\ncmap = sns.diverging_palette(220, 10, as_cmap = True)\n\nf, ax = plt.subplots(figsize = (21, 19))\nsns.heatmap(corr, cmap = cmap, center = 0, annot = True, square = True, linewidths = .5, cbar_kws = {\"shrink\": .5})","metadata":{"execution":{"iopub.status.busy":"2021-07-25T08:28:50.135845Z","iopub.execute_input":"2021-07-25T08:28:50.136121Z","iopub.status.idle":"2021-07-25T08:28:56.160107Z","shell.execute_reply.started":"2021-07-25T08:28:50.136094Z","shell.execute_reply":"2021-07-25T08:28:56.159193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df, hue = 'diagnosis')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-25T08:28:56.161307Z","iopub.execute_input":"2021-07-25T08:28:56.161608Z","iopub.status.idle":"2021-07-25T08:37:19.356686Z","shell.execute_reply.started":"2021-07-25T08:28:56.161578Z","shell.execute_reply":"2021-07-25T08:37:19.355187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting the data\nX = df.iloc[:, 1:]\ny = df.iloc[:, 0]\n\n# encoding the diagnosis column\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y) # 1 = M, 0 = B\n\n# splitting data into test and train sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T08:37:19.358307Z","iopub.execute_input":"2021-07-25T08:37:19.35878Z","iopub.status.idle":"2021-07-25T08:37:19.368181Z","shell.execute_reply.started":"2021-07-25T08:37:19.358696Z","shell.execute_reply":"2021-07-25T08:37:19.367232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dimensionality Reduction using PCA","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T08:37:19.369058Z","iopub.execute_input":"2021-07-25T08:37:19.369326Z","iopub.status.idle":"2021-07-25T08:37:19.394032Z","shell.execute_reply.started":"2021-07-25T08:37:19.3693Z","shell.execute_reply":"2021-07-25T08:37:19.393043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA() #This graph gives us the distribution of maxmium information at each component\npca.fit_transform(X_train) \npca_var = pca.explained_variance_\nplt.figure(figsize = (8, 6))\nplt.bar(range(30), pca_var, alpha = 0.5, align = 'center', label = 'Variance retrieved by component')\nplt.legend()\nplt.ylabel('Variance ratio')\nplt.xlabel('Principal components')\nplt.show() ","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-25T08:37:19.395506Z","iopub.execute_input":"2021-07-25T08:37:19.395847Z","iopub.status.idle":"2021-07-25T08:37:19.822018Z","shell.execute_reply.started":"2021-07-25T08:37:19.39579Z","shell.execute_reply":"2021-07-25T08:37:19.821052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components = 10) #This is used to decompose or reduce the dimension into the specified dimensions given in the components\npca.fit(X_train)\nX_train_pca = pca.transform(X_train) \npca.explained_variance_ratio_ ","metadata":{"execution":{"iopub.status.busy":"2021-07-25T08:37:19.825129Z","iopub.execute_input":"2021-07-25T08:37:19.825434Z","iopub.status.idle":"2021-07-25T08:37:19.836824Z","shell.execute_reply.started":"2021-07-25T08:37:19.825403Z","shell.execute_reply":"2021-07-25T08:37:19.835548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.sum(pca.explained_variance_ratio_)*100 #The percentage of the values imply that the 10 components retrieves close to 95% percent of the information from the original dataset that had 30 features","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-25T08:37:19.838644Z","iopub.execute_input":"2021-07-25T08:37:19.83904Z","iopub.status.idle":"2021-07-25T08:37:19.846276Z","shell.execute_reply.started":"2021-07-25T08:37:19.838989Z","shell.execute_reply":"2021-07-25T08:37:19.845386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape # Before Pca","metadata":{"execution":{"iopub.status.busy":"2021-07-25T08:37:19.847724Z","iopub.execute_input":"2021-07-25T08:37:19.848261Z","iopub.status.idle":"2021-07-25T08:37:19.857335Z","shell.execute_reply.started":"2021-07-25T08:37:19.84823Z","shell.execute_reply":"2021-07-25T08:37:19.856493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_pca.shape #After Pca As you see the features have been readuced from 30 to 10","metadata":{"execution":{"iopub.status.busy":"2021-07-25T08:37:19.858555Z","iopub.execute_input":"2021-07-25T08:37:19.858927Z","iopub.status.idle":"2021-07-25T08:37:19.868646Z","shell.execute_reply.started":"2021-07-25T08:37:19.8589Z","shell.execute_reply":"2021-07-25T08:37:19.867783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X1 = pd.DataFrame(data = X_train_pca, columns = [\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"PC5\",\"PC6\",\"PC7\",\"PC8\",\"PC9\",\"PC10\"])\nX1.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-25T08:37:19.869731Z","iopub.execute_input":"2021-07-25T08:37:19.87013Z","iopub.status.idle":"2021-07-25T08:37:19.893865Z","shell.execute_reply.started":"2021-07-25T08:37:19.870093Z","shell.execute_reply":"2021-07-25T08:37:19.892755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (7, 7))\nsns.heatmap(X1.corr(), annot= True, fmt = '.1f')\nplt.show() #Thus the extracted faetures show zero correlation with each other","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-25T08:37:19.895061Z","iopub.execute_input":"2021-07-25T08:37:19.895385Z","iopub.status.idle":"2021-07-25T08:37:20.6767Z","shell.execute_reply.started":"2021-07-25T08:37:19.895343Z","shell.execute_reply":"2021-07-25T08:37:20.67586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y1 = pd.DataFrame(y_train, columns = [\"diagnosis\"])\ny1.head()\nprint(y1.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T08:37:20.677923Z","iopub.execute_input":"2021-07-25T08:37:20.678244Z","iopub.status.idle":"2021-07-25T08:37:20.684182Z","shell.execute_reply.started":"2021-07-25T08:37:20.678197Z","shell.execute_reply":"2021-07-25T08:37:20.683248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X1[\"common\"] = range(398)\ny1[\"common\"] = range(398)\ndataset = pd.merge(X1, y1, on = [\"common\"])\ndataset = dataset.drop('common', axis=1)\ndataset.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-25T08:37:20.685255Z","iopub.execute_input":"2021-07-25T08:37:20.685513Z","iopub.status.idle":"2021-07-25T08:37:20.707725Z","shell.execute_reply.started":"2021-07-25T08:37:20.685489Z","shell.execute_reply":"2021-07-25T08:37:20.706985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(dataset,hue='diagnosis')","metadata":{"execution":{"iopub.status.busy":"2021-07-25T08:37:20.708714Z","iopub.execute_input":"2021-07-25T08:37:20.70898Z","iopub.status.idle":"2021-07-25T08:38:10.864696Z","shell.execute_reply.started":"2021-07-25T08:37:20.708955Z","shell.execute_reply":"2021-07-25T08:38:10.863518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%matplotlib notebook\nfig = plt.figure()\nax = fig.add_subplot(111, projection = '3d')\n\nfor x in dataset.diagnosis.unique():\n    ax.scatter(dataset.PC1[dataset.diagnosis==x], dataset.PC2[dataset.diagnosis==x], dataset.PC3[dataset.diagnosis==x], label=x)\n    \nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.set_zlabel(\"PC3\")\n\nfor angle in range(0, 360):\n    ax.view_init(30, angle)\n    plt.draw()\n    plt.pause(.001)\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T08:38:10.869472Z","iopub.execute_input":"2021-07-25T08:38:10.86986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above 3D visualization shows that derived principal components have separability in target class.","metadata":{}},{"cell_type":"code","source":"# standard scaling test data\nX_test = scaler.transform(X_test) \n# transforming test data with pca from above obtained parameters\nX_test_pca = pca.transform(X_test)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training and Validating Models","metadata":{}},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n\nlogreg.fit(X_train_pca, y_train)\n\nyl_pred = logreg.predict(X_test_pca)\n\nprint('Confusion Matrix:\\n', confusion_matrix(y_test, yl_pred))\nprint(\"Accuracy score:\", accuracy_score(y_test, yl_pred))\nprint('Classification Report:\\n', classification_report(y_test, yl_pred))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"param_grid=[{'penalty':['l2',],\n            'C':[0.01,0.1,1,10,100],\n            'solver':['liblinear','sag','newton-cg','lbfgs'],\n            'max_iter':[100,1000,2500,5000]}]\n\nfrom sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(logreg,param_grid=param_grid,cv=10,scoring='accuracy',verbose=True)\ngrid.fit(X_train_pca,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(grid.best_params_)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### After Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"logreg = LogisticRegression(C=0.1, max_iter=100, penalty='l2', solver='liblinear' )\n\nlogreg.fit(X_train_pca, y_train)\n\nyl_pred = logreg.predict(X_test_pca)\n\nprint('Confusion Matrix:\\n', confusion_matrix(y_test, yl_pred))\nprint(\"Accuracy score:\", accuracy_score(y_test, yl_pred))\nprint('Classification Report:\\n', classification_report(y_test, yl_pred))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import learning_curve\ntrain_sizes, train_scores, test_scores = learning_curve(logreg, X_train_pca, y_train, cv=10, scoring='accuracy', n_jobs=-1, train_sizes=np.linspace(0.01, 1.0, 50))\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nplt.subplots(1, figsize=(10,10))\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Stratified KFold\n\nX1 = pd.DataFrame(X)\ny1 = pd.DataFrame(y)\n\naccuracyl = []\n\nskf = StratifiedKFold(n_splits = 10, random_state = None)\nskf.get_n_splits(X1, y1)\nfor train_index, test_index in skf.split(X1, y1):\n    X1_train, X1_test = X1.iloc[train_index], X1.iloc[test_index]\n    y1_train, y1_test = y1.iloc[train_index], y1.iloc[test_index]\n    \n    # applying PCA\n    scaler.fit(X1_train)\n    X1_train = scaler.transform(X1_train)\n    pca = PCA(n_components = 10)\n    pca.fit(X1_train)\n    X1_train = pca.transform(X1_train)\n\n    X1_test = scaler.transform(X1_test)\n    X1_test = pca.transform(X1_test)\n\n    logreg.fit(X1_train, y1_train)\n    yl_pred = logreg.predict(X1_test)\n    accuracyl.append(accuracy_score(yl_pred, y1_test))\n    \n    \nprint(np.array(accuracyl).mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVM ","metadata":{}},{"cell_type":"code","source":"from sklearn import svm\nsclf = svm.SVC()\nsclf.fit(X_train_pca, y_train)\n\nys_pred = sclf.predict(X_test_pca)\n\nprint('Confusion Matrix:\\n', confusion_matrix(y_test, ys_pred))\nprint(\"Accuracy score:\", accuracy_score(y_test, ys_pred))\nprint('Classification Report:\\n', classification_report(y_test, ys_pred))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"param_grid=[{'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf','linear','poly'],\n            }]\n\ngrid=GridSearchCV(sclf,param_grid=param_grid,cv=10,scoring='accuracy',verbose=True)\ngrid.fit(X_train_pca,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(grid.best_params_)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### After Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"sclf = svm.SVC(C=1000, gamma= 0.0001, kernel='rbf', probability = True)\nsclf.fit(X_train_pca, y_train)\n\nys_pred = sclf.predict(X_test_pca)\n\nprint('Confusion Matrix:\\n', confusion_matrix(y_test, ys_pred))\nprint(\"Accuracy score:\", accuracy_score(y_test, ys_pred))\nprint('Classification Report:\\n', classification_report(y_test, ys_pred))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import learning_curve\ntrain_sizes, train_scores, test_scores = learning_curve(sclf, X_train_pca, y_train, cv=10, scoring='accuracy', n_jobs=-1, train_sizes = np.linspace(0.01, 1.0, 50))\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nplt.subplots(1, figsize=(10,10))\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Stratified KFold\naccuracys = []\n\nskf = StratifiedKFold(n_splits = 10, random_state = None)\nskf.get_n_splits(X1, y1)\nfor train_index, test_index in skf.split(X1, y1):\n    X1_train, X1_test = X1.iloc[train_index], X1.iloc[test_index]\n    y1_train, y1_test = y1.iloc[train_index], y1.iloc[test_index]\n\n    # applying PCA\n    scaler.fit(X1_train)\n    X1_train = scaler.transform(X1_train)\n    pca = PCA(n_components = 10)\n    pca.fit(X1_train)\n    X1_train = pca.transform(X1_train)\n\n    X1_test = scaler.transform(X1_test)\n    X1_test = pca.transform(X1_test)\n    \n    sclf.fit(X1_train, y1_train)\n    ys_pred = sclf.predict(X1_test)\n    accuracys.append(accuracy_score(ys_pred, y1_test))\n    \n    \nprint(np.array(accuracys).mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Naive Bayes ","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB\nBernNb = BernoulliNB()\nBernNb.fit(X_train_pca, y_train)\n\nyn_pred = BernNb.predict(X_test_pca)\n\nprint('Confusion Matrix:\\n', confusion_matrix(y_test, yn_pred))\nprint(\"Accuracy score:\", accuracy_score(y_test, yn_pred))\nprint('Classification Report:\\n', classification_report(y_test, yn_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"param_grid = [{'binarize':[0.0,0.001,0.01,0.1,1,10,100], \n               'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0], \n               'fit_prior' : [True, False],\n               'class_prior': [None, [.1,.9],[.2, .8]]\n             }]\n\ngrid = GridSearchCV(BernNb,param_grid=param_grid,cv=10,scoring='accuracy',verbose=True)\ngrid.fit(X_train_pca,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(grid.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### After Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"BernNb=BernoulliNB(alpha=10,binarize=0.0,fit_prior=True)\nBernNb.fit(X_train_pca, y_train)\n\nyn_pred = BernNb.predict(X_test_pca)\n\nprint('Confusion Matrix:\\n', confusion_matrix(y_test, yn_pred))\nprint(\"Accuracy score:\", accuracy_score(y_test, yn_pred))\nprint('Classification Report:\\n', classification_report(y_test, yn_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import learning_curve\ntrain_sizes, train_scores, test_scores = learning_curve(BernNb, X_train_pca, y_train, cv=10, scoring='accuracy', n_jobs=-1, train_sizes=np.linspace(0.01, 1.0, 50))\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nplt.subplots(1, figsize=(10,10))\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Stratified KFold\naccuracyn = []\n\nskf = StratifiedKFold(n_splits = 10, random_state = None)\nskf.get_n_splits(X1, y1)\nfor train_index, test_index in skf.split(X1, y1):\n    X1_train, X1_test = X1.iloc[train_index], X1.iloc[test_index]\n    y1_train, y1_test = y1.iloc[train_index], y1.iloc[test_index]\n    \n    # applying PCA\n    scaler.fit(X1_train)\n    X1_train = scaler.transform(X1_train)\n    pca = PCA(n_components = 10)\n    pca.fit(X1_train)\n    X1_train = pca.transform(X1_train)\n\n    X1_test = scaler.transform(X1_test)\n    X1_test = pca.transform(X1_test)\n\n    BernNb.fit(X1_train, y1_train)\n    yn_pred = BernNb.predict(X1_test)\n    accuracyn.append(accuracy_score(yn_pred, y1_test))\n    \n    \nprint(np.array(accuracyn).mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Box Plot of Accuracies achieved from 10 Fold Stratified K Fold","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame({'Logistic Regression': accuracyl,\n                   'SVM': accuracys,\n                   'Naive Bayes': accuracyn\n                    })\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.boxplot([df['Logistic Regression'], df['SVM'], df['Naive Bayes']]);\nplt.xticks([1, 2, 3], ['Logistic Regression', 'SVM', 'Naive Bayes'])\nplt.xlabel('Classifier');\nplt.ylabel('Mean Accuracy score')\nplt.title('Comparing 10 fold CV accuracies of classifiers')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation of Models using ROC","metadata":{}},{"cell_type":"code","source":"# calculation of probabilities\nlr_probs = logreg.predict_proba(X_test_pca)\nsvc_probs = sclf.predict_proba(X_test_pca)\nnb_probs = BernNb.predict_proba(X_test_pca)\n\n# Keeping only True Positive and False Positive \nlr_probs = lr_probs[:, 1]\nsvc_probs = svc_probs[:, 1]\nnb_probs = nb_probs[:, 1]\n\n# calculating roc auc score to evaluate each ones performance\nlr_auc = roc_auc_score(y_test, lr_probs)\nsvc_auc = roc_auc_score(y_test, svc_probs)\nnb_auc = roc_auc_score(y_test, nb_probs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Logistic Regression: \", lr_auc)\nprint(\"SVM: \", svc_auc)\nprint(\"Naive Bayes: \", nb_auc)\n# so from below we see that auc roc score of Logistic Regresion is highest","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating True Positive Rate and False Positive Rate for Each Models\nlr_fpr, lr_tpr, thr_l = roc_curve(y_test, lr_probs)\nsvc_fpr, svc_tpr, thr_s = roc_curve(y_test, svc_probs)\nnb_fpr, nb_tpr, thr_n = roc_curve(y_test, nb_probs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting ROC Curve\nplt.figure(figsize = (10, 10))\n\nplt.plot(lr_fpr, lr_tpr, marker = \".\", label = 'Logistic Regression', color = 'green')\nplt.plot(svc_fpr, svc_tpr, marker = \".\", label = 'SVM', color = 'blue')\nplt.plot(nb_fpr, nb_tpr, marker = \".\", label = 'Naive Bayes', color = 'red')\n\nplt.title('ROC Plot')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}