{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID19 Tweet Analysis\n<hr />\n<hr />"},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\n\"*Fake news spreads 6 times faster on Social Media*\". A line from the recent Netflix special \"*The Social Dilemma*\" that caught my attention. The social media, the likes of Twitter,Facebook etc, has a fair share of opinionated, silent consumers, trolls, objective individuals, hate mongerers and the list never ends. The recent outbreak of coronovirus (COVID19) has led us to witness the most unprecedented of times. While, the world was grappling with these turbulent times of a pandemic, Twitterati tweeted away with news, facts, myths, opinions,etc. This notebook is an attempt to understand the various aspects of these tweets. The Notebook also attempts to explore the hypothesis of genuinity and the legitimacy of these tweets by understanding various parameters like User location, Verified users, number of followers, friends, favorites, number of retweets and several other parameters.\n\n<hr />"},{"metadata":{},"cell_type":"markdown","source":"# Methodology\n\nThe Noteook is segmented broadly into 2 sections of Exploratory Data Analysis (EDA):\n1. Univariate Exploratory Analysis\n2. Mutlivariate Exploratory Analysis\n\n<hr />"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #base plotting library for plotly\nfrom matplotlib.pyplot import figure # to set the figure size\nimport plotly.express as px #plotly library to produce plots\nfrom wordcloud import WordCloud, ImageColorGenerator #wordcloud library\nfrom nltk.tokenize import word_tokenize #word tokenizer\nfrom nltk.probability import FreqDist # Frequency Distributor\nfrom nltk.corpus import stopwords #stop words for data cleaning\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's begin by reading the data into a dataframe by summoning our good old friend Pandas(Ofcourse :))"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/covid19-tweets/covid19_tweets.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see how the data looks like and the shape of the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The dataset has {} rows and {} columns\".format(data.shape[0],data.shape[1]))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Data Type of each column using the `dtype` utility method pandas. This will give us a clear indication of the kind of data present in the dataset and that which could potentially lead to type casting to the underlying datatype value for further processing."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"------------------------------------------------------------------------------------------------------------\")\nprint(\"The Datatype of each column in the dataset.\\n\\n\")\nprint(data.dtypes)\nprint(\"------------------------------------------------------------------------------------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the list of datatype of each column, there are 3 columns which are of integer type. As part of Preliminary Analysis, meaningful insights can be drawn on the integer valued columns by implementing a **Generative Descriptive statistics** on them.\n\nThe Descriptive statistics exhibits the central tendency, dispersion and the shape of dataset distribution. For further information read [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html)."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"------------------------------------------------------------------------------------------------------------\")\nprint(\"The Descriptive Statistics of the Dataset.\\n\\n\")\nprint(data.describe())\nprint(\"------------------------------------------------------------------------------------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Univariate Analysis\n\nLets Get started with Univariate Analysis of the Dataset.\n\nIn this section, we will go through each of the variables/columns in the dataset and understand some basic information like:\n\n* Distribution of column values across the dataset.\n* Outlier Detection\n* Number of Null values.\n* Number of Unique values.\n* Aggregation(wherever necessary)"},{"metadata":{},"cell_type":"markdown","source":"### 1. user_location\n\nLet us start with the number of unique places in the dataset."},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"print(\"Total number of Unique locations: \",data[\"user_location\"].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The tweets were made from 26920 unique places. Could that really be the place ? Seems unlikely. Considering the internet connectivity and access to social media platforms like twitter, this number looks fairly exaggerated. Assuming, each location only mentions the COUNTRY the tweet was made from, having these many countries doesn't make sense. This means that, this dataset would need some fair amount of data cleaning/wrangling wherein, we would generalize the country names. \n"},{"metadata":{},"cell_type":"markdown","source":"**NOTE: This notebook will only explore the variables and will not exclusively cater to data cleaning/wrangling. There will be a follow up kernel which will only cater to data cleaning.**"},{"metadata":{},"cell_type":"markdown","source":"Let us now see the distribution of locations in all the tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"------------------------------------------------------------------------------------------------------------\")\nprint(\"Number of tweets for each of the unique location in the dataset.\\n\\n\")\nprint(data[\"user_location\"].value_counts())\nprint(\"------------------------------------------------------------------------------------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that the `user_location` column is fairly polluted. It has website URL's, country names, state names belonging to the same country, which otherwise would not be considered a unique location if we are only considering the country as the unique location. However, we will only be using the top few locations to understand the distribution of tweet `user_location`."},{"metadata":{},"cell_type":"markdown","source":"Let us see if there are null values in this particular column"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The number of tweets where the user location is unkown: \", data[\"user_location\"].isna().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Storing the value counts into a dataframe for plotting the distribution of the user location counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"user_location_df = data[\"user_location\"].value_counts().rename_axis(\"place\").reset_index(name=\"counts\")\nuser_location_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_location_threshold_data = user_location_df[user_location_df[\"counts\"]>25].head(50)\n\nfig = px.bar(user_location_threshold_data,x=\"place\",y=\"counts\", title=\"Top 50 Locations tweets originate from\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"INDIA has been tweeting away during the pandemic. UNITED STATES comes a close second. \n\nWe can see from the list of user locations, there are a lot of repetitive locations that will have to be grouped/aggregated under the country name."},{"metadata":{},"cell_type":"markdown","source":"We could record the steps carried out for univariate preliminary analysis into a function. The `inspect_column` method will gather the basic information of a particular column and display the graphical findings."},{"metadata":{"trusted":true},"cell_type":"code","source":"def inspect_column(data,column):\n    print(\"------------------------------------------------------------------------------------------------------------\")\n    print(\"Basic Preliminary Information of column '{}'\\n\\n\".format(column))\n    print(\"Total number of Unique \",column,\"values: \",data[column].nunique())\n    print(\"----Quick overview of the distribution of the variable------\")\n    print(data[column].value_counts())\n    print(\"The number of tweets where the \", column ,\"specific data is unkown : \", data[column].isna().sum())\n    sub_data_df = data[column].value_counts().rename_axis(column).reset_index(name=\"counts\")\n    sub_data_threshold_df = sub_data_df[sub_data_df[\"counts\"]>25].head(100)\n    fig = px.bar(sub_data_threshold_df,x=column,y=\"counts\", title=\"Distribution of values of column '{}'\".format(column))\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. user_verified"},{"metadata":{"trusted":true},"cell_type":"code","source":"inspect_column(data,\"user_verified\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there are no null values in the `user_verified` column. Also what's interesting to note is that there are 7 times as many **unverified** accounts as verified accounts putting out information/opinion during the pandemic season. This should potentially raise concerns on the legitimacy of the information contained in these tweets as most of the information is emanating from unverified sources."},{"metadata":{},"cell_type":"markdown","source":"### 3. hashtags"},{"metadata":{"trusted":true},"cell_type":"code","source":"inspect_column(data,\"hashtags\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the output of the inspect_column method, the hashtags for each tweet, is present as a string value represented as a list of string values. To analyse the hashtags further, we need to clean the data in this column. Assuming most of the tweet would be made in relation to the present COVID situation, we will be focussing on cleaning most of the COVID related hashtags first."},{"metadata":{"trusted":true},"cell_type":"code","source":"# A small method that cleans up the hashtags and collates all the hashtags into a list.\nall_hashtag_list = []\n\n# Itertuples is much faster than iterrows\nfor each_row in data.itertuples():\n    if not str(each_row.hashtags).lower() == \"nan\":\n        each_hashtag = str(each_row.hashtags)\n        each_hashtag = each_hashtag.strip('[]').replace(\"'\",\"\")\n        all_hashtag_list += each_hashtag.split(\",\")\n        \nprint(\"Total number of hashtags\",len(all_hashtag_list))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting the list of values into a Dataframe to leverage the dataframe utility methods to carry out further analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"hashtag_df = pd.DataFrame(all_hashtag_list,columns=[\"hashtags\"])\nhashtag_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us use the `value_counts` to get the individual counts of each of the hastags"},{"metadata":{"trusted":true},"cell_type":"code","source":"count_df = hashtag_df[\"hashtags\"].value_counts().rename_axis(\"hashtags\").reset_index(name=\"counts\")\ncount_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But, is it totally cleaned ? If not all the hastags, atleast the COVID related hastags ? The answer is NO. we can still see duplicate counts and entries for the same covid19 hashtag. We need to minimise this repition and bring it down to a smaller number of varying versions of the COVID hashtag. The below code cell, simply adds the count values of the COVID related hastags."},{"metadata":{"trusted":true},"cell_type":"code","source":"hashtag_final_count_dic = {}\nfor each_row in count_df.itertuples():\n    if str(each_row.hashtags).strip().lower() == \"covid19\":\n        if \"covid19\" not in hashtag_final_count_dic:\n            hashtag_final_count_dic[\"covid19\"] = each_row.counts\n        else:\n            hashtag_final_count_dic[\"covid19\"] += each_row.counts\n    else:\n        hashtag_final_count_dic[str(each_row.hashtags).strip()] = each_row.counts\n        \nprint(\"The aggregated hashtags count has {} hashtags\".format(len(hashtag_final_count_dic)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets get this into a dataframe as well. "},{"metadata":{"trusted":true},"cell_type":"code","source":"final_hashtag_count_df = pd.DataFrame(hashtag_final_count_dic.items(),columns=['hashtag','count'])\nfinal_hashtag_count_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We were able to drill down and aggregate most of the covid19 hashtags and we now have the cummulated sum of the number of tweets with covid19 hashtags. Let us now see the top 10 tweets with highest number of hashtags."},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df = final_hashtag_count_df.sort_values(by='count',ascending=False).head(10)\nfig = px.bar(final_df,\"hashtag\",\"count\",title=\"Top 10 used hashtags during the pandemic\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"No surprises here the hashtag `#covid19` has been used over 100K times.  "},{"metadata":{},"cell_type":"markdown","source":"### Analyzing numerical columns in the dataset\n\n"},{"metadata":{},"cell_type":"markdown","source":"## 4. user_followers"},{"metadata":{},"cell_type":"markdown","source":"Let us perform analysis on the distribution of user followers for each twitter handle."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.box(data,y=\"user_followers\", title=\"The overall distribution of user followers\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Looks like the distribution of the users is fairly right skewed with IQR(Inter Quartile Range) being between 172 to 5K user followes. Closing down on the upper and lower limit whiskers will give us a better sense of the distribution of the users. We can alse see that there are significant outliers in terms of followers to the overall set of users. The median number of followers however, is around `992` users. "},{"metadata":{},"cell_type":"markdown","source":"Let us cut the whiskers on either end to see how the distribution looks like."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.box(data[(data[\"user_followers\"]>0) & (data[\"user_followers\"]<=15000)],y=\"user_followers\",title=\"The distribution of User followers within 15000 user followers\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The IQR does shift a bit more towards the right. However, the overall distribution still looks heavily right skewed with many outliers considering the fence values and IQR values. However in this case, there are too many values outside the deemed area to be termed as outliers. These values can add extra value to the analysis. We keep them as it is for now. "},{"metadata":{},"cell_type":"markdown","source":"Let us see how the distribtion looks like on a histogram."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(data[(data[\"user_followers\"]>0) & (data[\"user_followers\"]<=15000)],x=\"user_followers\", nbins=10, color_discrete_sequence=[\"red\"],\n                   title=\"Distribution of user followers with user followers less than 15000\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the highest number of followers for a twitter handle/user, lie within the range of 0 to 2000. The number of followers keep descreasing with the increase in the number of followers. So to put it simply, majority of the twitter users have followers in the range upto 2000."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We perform the same actions for the remaining 2 numerical columns `user_friends` and `user_favourites`"},{"metadata":{},"cell_type":"markdown","source":"## 5. user_friends "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.box(data,y=\"user_friends\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The boxplot for `user_friends` shows similar properties to that of `user_followers`. The distribution looks extremely right skewed with IQR roughly ranging between 100+ to 1.7K. let us do the same operations to this column as well to see the effects of limiting the number of user friends."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.box(data[(data[\"user_friends\"]>0) & (data[\"user_friends\"]<=5000)],y=\"user_friends\", title=\"Distribution of user friends with user friends less than 5k\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(data[(data[\"user_friends\"]>0) & (data[\"user_friends\"]<=5000)],x=\"user_friends\", nbins=10, color_discrete_sequence=[\"red\"],\n                  title=\"Distribution of user friends less than 5K\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the majority of users have a maximum of 500 followers. Let us now explore user_favorites and see if it has similar characteristics"},{"metadata":{},"cell_type":"markdown","source":"## 6. user_favourites"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.box(data,y=\"user_favourites\",title=\"Distribution of user favourites\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The traits are very similar to the above 2 columns with identical distribution. Let us reduce the scope and try and understand it better."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.box(data[(data[\"user_favourites\"]>0) & (data[\"user_favourites\"]<=25000)],y=\"user_favourites\",title=\"Distribution of user favourites with a maximum of 25K\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(data[(data[\"user_favourites\"]>0) & (data[\"user_favourites\"]<=25000)],x=\"user_favourites\", nbins=10, color_discrete_sequence=[\"red\"],\n                  title=\"Distribution of user favorites with a maximum of 25k user favorites\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the highest number of user_favourites are for the users with user_favorites between the range of 0-5000."},{"metadata":{},"cell_type":"markdown","source":"## 7. source"},{"metadata":{},"cell_type":"markdown","source":"Let us now understand the different types of sources twitterati made use to make them tweets. We will later examine the variation in the usage of different kinds of sources with other variables like user_location and such."},{"metadata":{"trusted":true},"cell_type":"code","source":"count_df = data[\"source\"].value_counts().rename_axis(\"source\").reset_index(name=\"counts\")\ncount_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that sources across the spectrum are made use to desseminate information and opinion. `Twitter Web App`, `Twitter For Android`, `Twitter for iPhone` and `TweetDeck` are some of the most commonly used sources to make tweets from all over the world."},{"metadata":{},"cell_type":"markdown","source":"We will just be seeing the variation and distribution of the top 10 sources for better understanding and convenience."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(count_df.head(10), x='source', y='counts',title=\"Top 10 Sources to make tweets\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. is_retweet"},{"metadata":{},"cell_type":"markdown","source":"This variable indicates whether a particular tweet was retweeted or not. Let us continue the exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"count_df = data[\"is_retweet\"].value_counts()\ncount_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Intersting! Looks like none of the tweets were retweeted. let us confirm this by finding unique values in this variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There are {} unique values in the column 'is_retweet'\".format(data[\"is_retweet\"].nunique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes. None of the tweets have been retweeted. This is really interesting considering how rampantly tweets are shared and retweeted."},{"metadata":{},"cell_type":"markdown","source":"## 9. date"},{"metadata":{},"cell_type":"markdown","source":"Let us perform time series analysis and understand the trends with respect to the duration of the tweets made.\n\nThe `date` column is of object datatype. We need to type cast into Date type for further analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting to Date type\ndata[\"date\"] = pd.to_datetime(data[\"date\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Date column is of '{}' type\".format(data[\"date\"].dtype))\ndata[\"date\"].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us analyse the distribution of tweets per day. Let us start by adding a new column `day_of_tweet` which will only have the date part of the tweet made. This will prove convenient while aggregating values with respect to dates."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"day_of_tweet\"] = pd.to_datetime(data['date']).dt.date\ndata[\"day_of_tweet\"].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once we have only the dates without the timestamps, let us perform aggregation and record the number of tweets made per day."},{"metadata":{"trusted":true},"cell_type":"code","source":"date_time_series = data.groupby(\"day_of_tweet\").size().rename_axis(\"day_of_tweet\").reset_index(name=\"number_of_tweets\")\ndate_time_series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We now have the number of tweets made per day from 24th July to 30th August. Its time plot a time series of this data."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(date_time_series, x='day_of_tweet', y=\"number_of_tweets\", title=\"Time series for number of tweets made per day\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe some periodicity in terms of peaks and troughs of interest shown by people in terms of number of tweets made. Given the bulk of hashtags made in relation to COVID19, it would be safe to assume that majority of the tweets would have been made in relation the same topic. However, we can confirm this hypothesis during multivariate analysis. We can see that 25th July has the highest number of tweets and August 7th has the least number of tweets. This was the period where WHO and other major health institutions were discovering new symptoms and behviour of this deadly virus. Follwed by new preventive measures etc. This would have subsequently spiked people's interests and hence the spikes in the number of tweets. "},{"metadata":{},"cell_type":"markdown","source":"## 10. text"},{"metadata":{},"cell_type":"markdown","source":"Finally its time to analyse and explore the tweet itself. We will be performing primitive NLP techniques in the following order:\n\n- Tokenization\n- Removal of Stop Words\n- Finding the highest occuring words across the corpus\n- Graphical representation of the same using a wordcloud and Frequency distribution.\n\nLets get Started !\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting a really simple wordcloud of only the first tweet\nwordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(data.text[0])\nplt.figure(figsize=(30,30))\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets gather all the tweet data into a variable.\nfinal_text = \"\".join(each_text for each_text in data.text)\nprint (\"There are {} words overall in tweets\".format(len(final_text)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets do some applied NLP analytics on the entire corpus."},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets first tokenize the entire corpus\ntokenized_words = word_tokenize(final_text)\nprint(\"The size of the tokenized words in the corpus is of size {}\".format(len(tokenized_words)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see some Frequency Distribution of the words"},{"metadata":{"trusted":true},"cell_type":"code","source":"fdist = FreqDist(tokenized_words)\nprint(fdist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets see the top 50 most frequently used words\nfdist.most_common(50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see a lot of garbage characters and stop words. Let us see how these are distrubuted anyway"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting the top 50 words in the frequency distribution\nplt.figure(figsize=(20,20))\nfdist.plot(50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see that there are a lot of punctuation words in the corpus. Lets first get rid of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\ntokenized_words = tokenizer.tokenize(final_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that a lot of words do not show substantail meaning. These are Stop words. Which are basically considered as noise. Words like is, am, are, this, a, an, the, etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words(\"english\"))\nprint(stop_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing stop words from our tweets\nfiltered_tokens = []\nfor each in tokenized_words:\n    if (each not in stop_words) and (len(each) > 3):\n        filtered_tokens.append(each)\nprint(\"Tokenised words now has {} words after removing stop words\".format(len(filtered_tokens)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets replot the frequency distribution and see the most frequently used words in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"fdist = FreqDist(filtered_tokens)\nprint(fdist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,30))\nfdist.plot(50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Barring the most frequently used token `https` which on the first impression may seem a fairly non-consequential token, it does show that majority of the tweets do quote some sort of source that led to the information contained in the tweet. The other most commonly used words across the entire dataset are pretty indicative of the current situation. Some of the most commonly used words are : `COVID19`, `cases`, `people`, `pandemic`, `2020` and several others."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Combining the top 200 frequently used words in the dataset\nanalyse_str = \" \".join([each[0] for each in fdist.most_common(200)])\nprint(analyse_str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# WordCloud for the most frequently used words across the dataset\nwordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(analyse_str)\nplt.figure(figsize=(20,20))\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Amidst all the commotion and clamour on COVID, we see some politically atrributed and/or driven tweets as well. The words like `realDonaldTrump` and `trump` shows that twitter almost never runs out of gas when it comes to discussing politics. "},{"metadata":{},"cell_type":"markdown","source":"-----------------------------------------------------------------------------------------\n-----------------------------------------------------------------------------------------\n"},{"metadata":{},"cell_type":"markdown","source":"## Multi-Variate Exploratory Analysis"},{"metadata":{},"cell_type":"markdown","source":"In this section, we will explore and understand the change caused by one of the variables on other variables. Mutlivariate analysis is a prominent technique to draw concrete insights into the behaviour of data. \n\n\nLets get started."},{"metadata":{},"cell_type":"markdown","source":"## 1. user location v/s account verification status."},{"metadata":{},"cell_type":"markdown","source":"In this section, we will see how the account verification status of the user making the tweet relates to the location. This will help us to understand what country/location has been making most of the tweets and how many of them are verified. Going back to our hypothesis of verification determining the genuinity of the information, we are trying to formulate which location potentially has high unverified news v/s verified information."},{"metadata":{"trusted":true},"cell_type":"code","source":"# grouping by user_location and user_verified and gathering top 50 entries\nuser_loc_df = data.groupby([\"user_location\",\"user_verified\"])[\"user_verified\"].count().reset_index(name=\"count\").sort_values(by=['count'], ascending=False).head(50)\nuser_loc_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bar plot to show the user_location and count by user_verified\nfig = px.bar(user_loc_df, x='user_location',y='count',color='user_verified',barmode=\"group\",\n            title=\"Relationship between the user locations v/s user verified\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that United States has the most amount of unverified users. India on the other hand has fair amount of verified users. Does this say anything about the fake news narrative which often propogates media these days especially in the US? *Food for thought*"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.pie(user_loc_df, values='count', names='user_verified', title='Ratio of Verified acounts v/s Unverified accounts')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From an overall perspective, approximately 86% of user accounts are unverified and only about 14% of the user accounts verified. This is fairly a large skew leaning towards unverified accounts."},{"metadata":{},"cell_type":"markdown","source":"## 2. user_location v/s Hashtags"},{"metadata":{},"cell_type":"markdown","source":"Analysing user location and hashtags will give us the insight into several different topics that are being talked about with different locations. This also, in a way, presents the overall mood of a particular location in relation to the topic at hand."},{"metadata":{"trusted":true},"cell_type":"code","source":"# extracting the user location and their respective hashtags\nuser_loc_hastag_data = data[[\"user_location\",\"hashtags\"]]\nuser_loc_hastag_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting the dataframe to dictionary to aggregate by location\nuser_loc_hastag_data_dic = user_loc_hastag_data.to_dict(orient='records')\nprint(\"There are a total of {} records in the dictionary\".format(len(user_loc_hastag_data_dic)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# code block to perform string manipulation and extract location keys and aggregated values\ncleaned_dic_container = []\nfor each in user_loc_hastag_data_dic:\n    if str(each[\"user_location\"]).lower() != 'nan' and str(each[\"hashtags\"]).lower() != 'nan':\n        cleaned_dic = {}\n        each[\"hashtags\"] = str(each[\"hashtags\"]).strip('[]').replace(\"'\",\"\").split(\",\")\n        cleaned_dic[\"user_location\"] = str(each[\"user_location\"])\n        cleaned_dic[\"hashtags\"] = each[\"hashtags\"]\n        cleaned_dic_container.append(cleaned_dic)\ncleaned_dic_container[0:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting the processed list of dictionaries to a dataframe by using the 'explode' method of pandas to spread each of the 'hashtag' column entries vertically\nuser_loc_hashtags_df = pd.DataFrame(cleaned_dic_container)\nuser_loc_hashtags_df = user_loc_hashtags_df.explode('hashtags')\nuser_loc_hashtags_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# applying final manipulations using lambda functions\nhashtag_loc_df = user_loc_hashtags_df.groupby(['user_location',\"hashtags\"])[\"hashtags\"].count().reset_index(name=\"count\").sort_values(by=['count'], ascending=False).head(100)\nhashtag_loc_df[\"user_location\"] = hashtag_loc_df[\"user_location\"].apply(lambda x : x.strip())\nhashtag_loc_df[\"hashtags\"] = hashtag_loc_df[\"hashtags\"].apply(lambda x : x.strip())\nhashtag_loc_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us see the most popular hastags aggregated by the user location."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(hashtag_loc_df,x = \"user_location\",y=\"count\",color=\"hashtags\",title=\"What are these countries talking about the most ?\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No surprises here. Majority of the users have been talking about COVID. However, we do get to see some some other hashtags as well. We can see `#auspoll` trending in the Canberra, Australia. Goes to show that despite the showstopper 'COVID', there are other pressing issues like elections, world is talking about."},{"metadata":{},"cell_type":"markdown","source":"## 3. user_location v/s tweet source"},{"metadata":{},"cell_type":"markdown","source":"It would also be interesting to know the sources made use of to make tweets with the diverse demographics in the world. Lets give it a look. Also, seeing this with conjuction of the user_verified status should be really interesting to note. We can understand what source is predominantly used and how many of them are verified. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# grouping by user location , user verified and the source. extracting the top 50 most commonly used sources where the users are verified.\nuser_loc_source_df = data.groupby([\"user_location\",\"user_verified\",\"source\"])[\"source\"].count().reset_index(name=\"count\").sort_values(by=['count'], ascending=False).head(50)\nuser_loc_source_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(user_loc_source_df,x=\"user_location\", y=\"count\",color=\"source\", facet_col=\"user_verified\",title=\"Exploring the relationship between the user location v/s source of the tweet v/s user verification status\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that most of the verified users make use of Twitter Web App and Tweet Deck(which is generally made use by users having multiple twitter accounts. eg. Media houses, etc.). Additionally, Twitter for Android is used as the source by most of the unverified accounts. "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 4. user verified v/s user followers"},{"metadata":{},"cell_type":"markdown","source":"Given that we have established the already proven research results of the spread of fake news. And assuming for a minute, that verified accounts are less likely to propogate fake news, it would be interesting to find the median user followers against the verification status of an account. Ideally, we should have more followers for the verified account than the unverified accounts. But is that the case in reality ? Let us find out. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# grouping by user verified status and finding the median user followers\nuser_ver_followers_df = data.groupby(\"user_verified\")[\"user_followers\"].median().reset_index(name=\"median_followers\").sort_values(by=[\"median_followers\"],ascending=False)\nuser_ver_followers_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.pie(user_ver_followers_df, values=\"median_followers\",names=\"user_verified\",hole=.5, title=\"The proportion of Median number of followers between user verified accounts v/s unverified accounts\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see some skewed results favouring the user verified status. Perhaps, setting a particular threshold on the number of followers should give us a clearer picture. This is classic case of including outliers in the overall analysis."},{"metadata":{},"cell_type":"markdown","source":"## 5. user_verified v/s user friends"},{"metadata":{},"cell_type":"markdown","source":"Following the same hypothesis of verified accounts and user followers. Let us find out if the same pattern holds for the median user friends for verified user."},{"metadata":{"trusted":true},"cell_type":"code","source":"user_ver_friends_df = data.groupby(\"user_verified\")[\"user_friends\"].median().reset_index(name=\"median_friends\").sort_values(by=[\"median_friends\"],ascending=False)\nuser_ver_friends_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.pie(user_ver_friends_df, values=\"median_friends\",names=\"user_verified\",hole=.5, title=\"The proportion of Median number of friends between user verified accounts v/s unverified accounts\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the both the verified accounts have almost the same amount of friends with unverified users slightly having more median friends than verified users."},{"metadata":{},"cell_type":"markdown","source":"## 6. users_verified vs number of favorites vs retweets"},{"metadata":{},"cell_type":"markdown","source":"In this case, we are going to see 3 varibles in conjunction. we are going to see how median user favorites and the number of times a retweet has been made when aggregated by user_verified. "},{"metadata":{"trusted":true},"cell_type":"code","source":"user_ver_fav_retweet_df = data.groupby(\"user_verified\",as_index=False).agg({\"user_favourites\":\"median\",\"is_retweet\":\"count\"})\nuser_ver_fav_retweet_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(user_ver_fav_retweet_df, x=\"is_retweet\",y=\"user_favourites\",color=\"user_verified\",orientation='h', \n             title = \"Relationship between user_favorites, number of retweets made grouped on user_verified status\" )\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that unverified users have higher retweets and higher user favorites in comparison to verified users. Unverified users have around 1800 user favorites with a total of 156k retweets. However, the verified users have 1500 user favorites with a meagre 23K retweets only. we can see why fake information tends to propogate faster(this is totally made under the assumption that unverified accounts generally make disinformed posts). "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 7. Exploring Relationship between user followers for verified and unverified users with Time"},{"metadata":{},"cell_type":"markdown","source":"In this section we will see how the relationship between the verified user and user followers has changed with time. "},{"metadata":{"trusted":true},"cell_type":"code","source":"time_series_data = data.groupby('day_of_tweet',as_index=False).agg({'user_followers':'median','user_verified':'count'})\ntime_series_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=time_series_data[\"day_of_tweet\"], y=time_series_data[\"user_followers\"],\n                    mode='lines+markers',\n                    name='number of followers'))\nfig.add_trace(go.Scatter(x=time_series_data[\"day_of_tweet\"], y=time_series_data[\"user_verified\"],\n                    mode='lines+markers',\n                    name='number of verified users'))\n\nfig.update_layout(title='Time Series data for change in followers with number of verified users',\n                   xaxis_title='Day',\n                   yaxis_title='Number of users')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We can see that the median followers has remained pretty much constant with time. However, we can see that the number of tweets made has lots of peaks and troughs with time. The highest peak during towards the end of July and the lowest during the start of August. Let us only see how the verified users and the number of retweets has changed with time."},{"metadata":{"trusted":true},"cell_type":"code","source":"time_series_retweet_df = data.groupby(['day_of_tweet','user_verified'],as_index=False).agg({'is_retweet':'count'})\ntime_series_retweet_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(time_series_retweet_df, x=\"day_of_tweet\", y=\"is_retweet\", color='user_verified',title=\"Time Series representation for change of retweets made on verified and unverified users\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see a similar pattern here as well. The number of retweets made on verified user accounts seemed to stay consistent. However, the retweets made on unverified accounts has a varying pattern almost like a chaotic pattern. Isnt it metaphorical? "},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"With this, the EDA of the tweets made during the pandemic comes to an end. We have explored fairly the relationships and discussed various aspects. we have seen how information spreads and how the different variables play their part in spreading the information. This wasnt the cleanest of data. We have done very basic and preliminary cleaning for the sake of EDA and a more detailed cleaning exercise needs to be carried out on the dataset in order to perform unsupervised learning on them in order to find more patterns and trends."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}