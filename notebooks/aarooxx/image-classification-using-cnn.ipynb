{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Libraries \nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport cv2\nfrom PIL import Image\nimport os\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing training dataset\ntrain=pd.read_csv('../input/gtsrb-german-traffic-sign/Train.csv')\nX_train=train['Path']\ny_train=train.ClassId\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = \"../input/gtsrb-german-traffic-sign\"\ntrain_imgpath= list((data_dir + '/' + str(train.Path[i])) for i in range(len(train.Path)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,9):\n    plt.subplot(331+i)\n    seed=np.random.randint(0,39210)\n    im = Image.open(train_imgpath[seed])  \n    plt.imshow(im)\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Preprocessing image-**\nconverting images into arrays of the form (28,28,3)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=[]\ntrain_labels=[]\n\n\npath = \"../input/gtsrb-german-traffic-sign/\"\nfor i in range(len(train.Path)):\n    image=cv2.imread(train_imgpath[i])\n    image_from_array = Image.fromarray(image, 'RGB')\n    size_image = image_from_array.resize((28,28))\n    train_data.append(np.array(size_image))\n    train_labels.append(train.ClassId[i])\n\n\nX=np.array(train_data)\ny=np.array(train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split( X, y, test_size=0.20, random_state=7777)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Spliting the images into train and validation sets\n\nX_train = X_train.astype('float32')/255 \nX_val = X_val.astype('float32')/255\n\n#Using one hote encoding for the train and validation labels\nfrom keras.utils import to_categorical\ny_train = to_categorical(y_train, 43)\ny_val = to_categorical(y_val, 43)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CNN Model-\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.optimizers import Adam\nfrom keras.layers import Dropout","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Grid Search to determine the layers and neurons in each layer in the sequential model.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(layers):\n    cnn = tf.keras.models.Sequential()\n    cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=[28, 28, 3]))\n    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'))\n    cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'))\n    cnn.add(tf.keras.layers.Flatten())\n    \n    for i, nodes in enumerate(layers):\n        cnn.add(tf.keras.layers.Dense(units=nodes, activation='relu'))\n            \n    cnn.add(tf.keras.layers.Dense(units=43, activation='softmax'))\n    \n    cnn.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return cnn\n\nmodel = KerasClassifier(build_fn=create_model, verbose=1)\nlayers = [[128],(256, 128),(200, 150, 120)]\nparam_grid = dict(layers=layers)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, verbose=1)\ngrid_results = grid.fit(X_train,y_train, validation_data=(X_val, y_val))\nprint(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\nmeans = grid_results.cv_results_['mean_test_score']\nstds = grid_results.cv_results_['std_test_score']\nparams = grid_results.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{0} ({1}) with: {2}'.format(mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Grid Search to determine the batch size","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model1():\n    # create model\n    cnn = tf.keras.models.Sequential()\n    cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=[28, 28, 3]))\n    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'))\n    cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'))\n    cnn.add(tf.keras.layers.Flatten())\n    cnn.add(tf.keras.layers.Dense(units=256, activation='relu'))\n    cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n    cnn.add(tf.keras.layers.Dense(units=43, activation='softmax'))\n    \n    # compile the model\n    cnn.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return cnn\n\n# create the model\nmodel = KerasClassifier(build_fn = create_model1, verbose = 1)\n\n# define the grid search parameters\nbatch_size = [20,40]\n\n# make a dictionary of the grid search parameters\nparam_grid = dict(batch_size=batch_size)\n\n# build and fit the GridSearchCV\ngrid = GridSearchCV(estimator = model, param_grid = param_grid, verbose = 1)\ngrid_results = grid.fit(X_train,y_train, validation_data=(X_val, y_val))\n\n# summarize the results\nprint(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\nmeans = grid_results.cv_results_['mean_test_score']\nstds = grid_results.cv_results_['std_test_score']\nparams = grid_results.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{0} ({1}) with: {2}'.format(mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Grid Search to determine the dropout rate\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model2(dropout):\n    # create model\n    cnn = tf.keras.models.Sequential()\n    cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=[28, 28, 3]))\n    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'))\n    cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'))\n    cnn.add(tf.keras.layers.Flatten())\n    cnn.add(tf.keras.layers.Dense(units=256, activation='relu'))\n    cnn.add(Dropout(dropout))\n    cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n    cnn.add(Dropout(dropout))\n    cnn.add(tf.keras.layers.Dense(units=43, activation='softmax'))\n    \n    # compile the model\n    cnn.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return cnn\n\n# create the model\nmodel = KerasClassifier(build_fn = create_model2, verbose = 1,epochs=10, batch_size=20)\n\n# define the grid search parameters\ndropout = [0.0, 0.1, 0.2]\n\n# make a dictionary of the grid search parameters\nparam_grid = dict(dropout=dropout)\n\n# build and fit the GridSearchCV\ngrid = GridSearchCV(estimator = model, param_grid = param_grid, verbose = 1)\ngrid_results = grid.fit(X_train,y_train, validation_data=(X_val, y_val))\n\n# summarize the results\nprint(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\nmeans = grid_results.cv_results_['mean_test_score']\nstds = grid_results.cv_results_['std_test_score']\nparams = grid_results.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{0} ({1}) with: {2}'.format(mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"inputing the parameters in the final model \n(for better accuracy, you can run grid search multiple times zooming into each range used before)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Definition of the DNN model\n\ncnn = tf.keras.models.Sequential()\ncnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=[28, 28, 3]))\ncnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'))\ncnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\ncnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'))\ncnn.add(tf.keras.layers.Flatten())\ncnn.add(tf.keras.layers.Dense(units=256, activation='relu'))\ncnn.add(Dropout(0.2))\ncnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\ncnn.add(Dropout(0.2))\ncnn.add(tf.keras.layers.Dense(units=43, activation='softmax'))\n\n# compile the model\ncnn.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\nhistory = cnn.fit(X_train, y_train, batch_size=20, epochs=20,validation_data=(X_val, y_val))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the values of accuracy and loss vs epoch to visually determine the suitable number of epochs required","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(0)\nplt.plot(history.history['accuracy'], label='training accuracy')\nplt.plot(history.history['val_accuracy'], label='val accuracy')\nplt.title('Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.figure(1)\nplt.plot(history.history['loss'], label='training loss')\nplt.plot(history.history['val_loss'], label='val loss')\nplt.title('Loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# preparing test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test=pd.read_csv('../input/gtsrb-german-traffic-sign/Test.csv')\nX_test=train['Path']\ny_test=train.ClassId","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = \"../input/gtsrb-german-traffic-sign\"\ntest_imgpath= list((data_dir + '/' + str(test.Path[i])) for i in range(len(test.Path)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data=[]\ntest_labels=[]\n\n\npath = \"../input/gtsrb-german-traffic-sign/\"\nfor i in range(len(test.Path)):\n    image=cv2.imread(test_imgpath[i])\n    image_from_array = Image.fromarray(image, 'RGB')\n    size_image = image_from_array.resize((28,28))\n    test_data.append(np.array(size_image))\n    test_labels.append(test.ClassId[i])\n\n\nX_test=np.array(test_data)\ny_test=np.array(test_labels)\n\nX_test = X_test.astype('float32')/255 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predictions-\npred = cnn.predict_classes(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test, pred)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}