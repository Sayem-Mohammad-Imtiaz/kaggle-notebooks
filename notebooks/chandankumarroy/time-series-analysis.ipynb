{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.style.use('ggplot')\nplt.rcParams['lines.linewidth'] = 2\nplt.rcParams['figure.figsize'] = (10,8)\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_df = pd.read_csv(\"../input/nse-stocks-data/FINAL_FROM_DF.csv\")\nstock_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"stock_df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert TIMESTAMP to datetime instance\nstock_df['TIMESTAMP'] = pd.to_datetime(stock_df.TIMESTAMP, dayfirst = True)\nstock_df.set_index('TIMESTAMP', inplace = True)\nstock_df['PER_DAILY_CHANGE'] = (stock_df.CLOSE - stock_df.OPEN)*100/stock_df.OPEN\nstock_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets check how many different symbols are present in the dataset\nunique_symb = stock_df['SYMBOL'].unique()\nprint(\"total number of unique symbols is \", len(unique_symb))\n\n#let's print top 10 different symbols\nunique_symb[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# total number of datapoints\nstock_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#say if we are interested in predicting prices of Banks only\nbank_stock = stock_df[stock_df['SYMBOL'].str.contains(\"BANK\")]\nbank_stock.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We need to define the columns data we are interested in predicting,\n# say we want to predict the closing price of all the companies\n#lets plot a pivot table\n\nstock_pivot = bank_stock.pivot_table(values = 'CLOSE', columns = 'SYMBOL', index = 'TIMESTAMP')\nstock_pivot.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_pivot.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's drop those columns in which there are missing values\nstock_pivot.dropna(axis = 1, how = 'any', inplace = True)\nstock_pivot.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_pivot.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Randomly choose 5 columns and plot its value\n# col = stock_pivot.columns[np.random.randint(0, len(stock_pivot.columns)+1, size = 5)]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#most important thing in time series analysis is to have correlation between the values at different timestamp\n#choose one bank at random and look for correlation using lag plot with lag value 1\nnp.random.seed(1)\nbank = stock_pivot.columns.values[np.random.randint(0, len(stock_pivot.columns)+1)]\nprint(\"Bank is = \", bank)\npd.plotting.lag_plot(stock_pivot[bank])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see in the above plot that there is strong correlation bewteen the datapoints. So, we are good to go with time series analysis on the above data","metadata":{}},{"cell_type":"code","source":"stock_pivot.plot(kind ='line', y = bank)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #check for correlation using heatmap\n# plt.figure(figsize = (14,14))\n# sns.heatmap(stock_pivot.corr(method='pearson'),annot=True, linewidth = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The most important thing in autocorrelation analysis is to know the appropriate lag value. we can manually do that. Let's do that.","metadata":{}},{"cell_type":"code","source":"# find appropriate lag value for the autocorrelation analysis\nMAX_LAG = 30 #maximum lag value\ncorr = 0\nlag = 1\nfor i in range(1, MAX_LAG):\n    temp = np.abs(stock_pivot[bank].corr(stock_pivot[bank].shift(i)))\n    if temp > corr:\n        corr = temp\n        lag = i\n\nprint(\"apprpriate lag value is \", lag)\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's cross check our value using pandas autocorrelation plot\npd.plotting.autocorrelation_plot(stock_pivot[bank])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see in the above plot that the autocorrelation corresponding to lag value 1 is maximum","metadata":{}},{"cell_type":"markdown","source":"## Check for Trend and seasonality","metadata":{}},{"cell_type":"markdown","source":"__Note__ that we have assumed our model to be additive in general","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#seasonal decomposition using period of 1\ndecomposed = seasonal_decompose(stock_pivot[bank], model='additive', period = 1)\n\nx = decomposed.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Though our dataset does not exhibit trend and seasonality, But it's still a good practice to make model stationary by differencing","metadata":{}},{"cell_type":"code","source":"# remove seasonality and trend\ndf = stock_pivot.copy()\ndf['STATIONARY'] = df[bank].diff()\ndf.STATIONARY","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Recheck for seasonality and trend\ndf.dropna(axis = 0, inplace = True)\ndecomposed = seasonal_decompose(df['STATIONARY'], model='additive', period = 1)\nx = decomposed.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## AR(1) model","metadata":{}},{"cell_type":"code","source":"#divide data in train and test\nfreq = 7\nX = df['STATIONARY']\nY =  X.shift(1).dropna()\nY, X\ntrain_size = int(len(X)*0.8)\n# train, test = X[1: len(X)-freq], X[len(X)-freq : ]\n# train.shape, test.shape\nx_train, x_test = X[0:train_size], X[train_size:len(X)-1]\ny_train, y_test = Y[0:train_size], Y[train_size:]\nx_train.shape, y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#AR1 equation is given by\n#x_t = ϕ_{t-1} + ϵ\n\n#find ϕ using OLS (ordinary least square)\nphi = np.dot(x_train.values,y_train.values.T)/np.dot(x_train, x_train.T)\nprint(phi)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check the prediction over test data\npred = phi*x_test\npred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#finding mean square error\nimport sklearn.metrics as m\nr2 =m.r2_score(y_test, pred)\nprint(\"R square is \", r2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's plot the predicted vs test value\nplt.plot(y_test)\nplt.plot(pred)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As it can be seen in the above plot that we are nearly predicting the exact value as it should. it makes sense as we can see in the correlation plot that all the data points are perfectly cluttered along the diagonal.","metadata":{}}]}