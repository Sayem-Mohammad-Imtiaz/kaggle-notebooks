{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Breast Cancer Diagnostic - Automatically choosing the best algorithm\n\nThe goal here is to predict the diagnosis of a breast cancer, whether it is malignant or benign, depending on the values of several observations performed in cells."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## Understanding the dataset\n\n### Attribute information\n\n#### 1) ID number\n\n#### 2) Diagnosis\n\n- M = malignant\n- B = benign\n\n#### 3-32) Features (Mean, Std Err, Worst/Largest)\n\nTen real-valued features are computed for each cell nucleus:\n\n- a) **radius** (mean of distances from center to points on the perimeter)\n- b) **texture** (standard deviation of gray-scale values)\n- c) **perimeter**\n- d) **area**\n- e) **smoothness** (local variation in radius lengths)\n- f) **compactness** (perimeter^2 / area - 1.0)\n- g) **concavity** (severity of concave portions of the contour)\n- h) **concave points** (number of concave portions of the contour)\n- i) **symmetry**\n- j) **fractal dimension** (\"coastline approximation\" - 1)\n\nThe **mean**, **standard error** and **\"worst\" or largest** (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n\n### Observations\n\n- All feature values are recoded with four significant digits.\n- Missing attribute values: none.\n- Class distribution: 357 benign, 212 malignant."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import standard libraries for linear algebra, handling data and plotting\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npd.set_option('precision', 2)\nsns.set(style=\"white\", color_codes=True)\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read the data into a Pandas DataFrame\ndata = pd.read_csv('../input/data.csv')\n\n# show some samples\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove unnamed 32th column\ndata.drop(data.columns[32], axis=1, inplace=True)\n\n# save identifications\nids = data['id']\n\n# remove unnecessary id column\ndata.drop(['id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# how many rows and columns are there in the data?\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# which are the names of the columns and their datatypes?\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# is there any column with null values?\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# describing numerical features\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# describing categorical features\ndata.describe(include=['O'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# which are the possible values for the categorical attribute?\ndata['diagnosis'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalizing numeric values in order to avoid distortions\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndata.iloc[:,1:] = scaler.fit_transform(data.iloc[:,1:])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# analyzing correlation between features\ndata.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting this correlation between features\nfig, ax = plt.subplots(figsize=(17, 17))\nsns.heatmap(data=data.corr().round(2), annot=True, cmap=\"PiYG\", ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are so many features. Therefore, pair plotting all of them isn't a good idea. Let's do it by each 10 sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"# pair plot diagnosis + 10 features (mean)\nsns.pairplot(data=data.iloc[:,0:11], hue='diagnosis', diag_kind='kde')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pair plot diagnosis + 10 features (std error)\nsns.pairplot(data=data.iloc[:,np.append([0],np.arange(11,21))], hue='diagnosis', diag_kind='kde')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pair plot diagnosis + 10 features (worst/largest)\nsns.pairplot(data=data.iloc[:,np.append([0],np.arange(21,31))], hue='diagnosis', diag_kind='kde')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From these 3 last plottings, we can't realize a clear distinction in the class (diagnosis) against a given pair of numerical attributes. Let's try scatter plotting a given pair of attributes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# scatter plotting radius (mean) versus concave points (mean)\nsns.FacetGrid(data, hue='diagnosis', height=5) \\\n   .map(plt.scatter, 'radius_mean', 'concave points_mean') \\\n   .add_legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We still cannot define the distinction. Let's try box plotting another feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"# box plotting diagnosis against radius (mean)\nsns.boxplot(x='diagnosis', y='radius_mean', data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Still confusing. Let's try an histogram based on these same attributes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an histogram on radius (mean)\nsns.FacetGrid(data, hue='diagnosis')\\\n   .map(plt.hist, 'radius_mean', alpha=.5, bins=20)\\\n   .add_legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's a region in the middle where we can't define perfectly the class. Let's try it with another feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an histogram on concave points (mean)\nsns.FacetGrid(data, hue='diagnosis')\\\n   .map(plt.hist, 'concave points_mean', alpha=.5, bins=20)\\\n   .add_legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The features alone definitely can't split the class (i.e., give a clue to the diagnosis). Therefore, we'll include every numeric feature in the model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# select the features\n#X = data[data.columns[[1, 3, 4, 7, 8]]] # use only selected 5 features\nX = data.iloc[:,1:] # use all numeric features\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# select the class column\ny = data.diagnosis\ny.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's start creating a model for the problem based on the data!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing packages used in model selection and metrics evaluation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# importing all the necessary packages to use the various classification algorithms\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First of all, let's split the data we handled so far in two sets: training and testing. The latter must contain fewer rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"# separate data for training (70%) and testing (30%)\n\nprint('original data shapes:', X.shape, y.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint('splitted data shapes:', X_train.shape, X_test.shape, y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we'll instantiate each algorithm to be checked. We'll insert them in a single list."},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate checking algorithms\nmodels = []\nmodels.append(('Support Vector Machines (SVM)', SVC()))\nmodels.append(('Logistic Regression', LogisticRegression()))\nmodels.append(('Decision Tree', DecisionTreeClassifier()))\nmodels.append(('K-Nearest Neighbours (3)', KNeighborsClassifier(n_neighbors=3)))\nmodels.append(('K-Nearest Neighbours (7)', KNeighborsClassifier(n_neighbors=7)))\nmodels.append(('K-Nearest Neighbours (11)', KNeighborsClassifier(n_neighbors=11)))\nmodels.append(('Random Forest', RandomForestClassifier()))\nmodels.append(('Random Forest (10)', RandomForestClassifier(n_estimators=10)))\nmodels.append(('Random Forest (100)', RandomForestClassifier(n_estimators=100)))\nmodels.append(('Gaussian Na√Øve Bayes', GaussianNB()))\nmodels.append(('Perceptron (5)', Perceptron(max_iter=5)))\nmodels.append(('Perceptron (10)', Perceptron(max_iter=10)))\nmodels.append(('Perceptron (50)', Perceptron(max_iter=50)))\nmodels.append(('Stochastic Gradient Decent (SGD)', SGDClassifier(max_iter=50)))\nmodels.append(('Linear SVC', LinearSVC()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For each algorithm, let's perform the training, try predicting values, and then measure the model accuracy. A confusion matrix is to be calculated, as well as the number of False Negatives found."},{"metadata":{"trusted":true},"cell_type":"code","source":"names = []\nscores = []\nfalnegs = []\n\nbest_model = None\nhighest_score = 0.0\nfalse_negatives = None\n\nfor name, model in models:\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test)\n    \n    y_pred = model.predict(X_test)\n\n    cm = confusion_matrix(y_pred, y_test)\n    tn, fp, fn, tp = cm.ravel()\n    print(name, '\\n', cm, '\\n')\n\n    names.append(name)\n    scores.append(score)\n    falnegs.append(fn)\n\n    if ((score > highest_score) or (score == highest_score and fn < false_negatives)):\n        best_model = model\n        highest_score = score\n        false_negatives = fn\n        \nprint('Best model:', best_model, '\\n[Score: %.3f, False Negatives: %d]' % (highest_score, false_negatives))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the end, this section will give us the best model found. We consider the higher accuracy and the fewer number of false negatives to chosse this model."},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame({'Model': names, 'Score': scores, 'FN': falnegs})\nresults.sort_values(by=['Score', 'FN'], ascending=[False, True])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the best model found, let's predict the values for the entire dataset and then print the score and the confusion matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"# consider the best algorithm found\nmodel = best_model\n\n# train the model with the training dataset\nmodel.fit(X_train, y_train)\n\n# calculate the score against the whole dataset\nscore = model.score(X, y)\nprint('Final score:', score)\n\n# produce the confusion matrix\ny_pred = model.predict(X)\nprint('Confusion matrix:\\n', confusion_matrix(y_pred, y), '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, let's create a DataFrame containing a possible submission to the competition."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n  \"ID\": ids,\n  \"Diagnosis\": y,\n  \"Predicted\": y_pred,\n  \"Correct\": (y == y_pred).map({True: 1, False: 0})\n})\nsubmission.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If there is any incorrect classification, which were them?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# show the incorrectly classified cases\n\nincorrectly = submission[submission[\"Correct\"] == False]\n\nincorrect = len(incorrectly.index)\ntotal_cases = len(submission)\nprint('Incorrectly classified cases:', incorrect, \\\n      'of', total_cases, '(%.3f%%)' % (incorrect / total_cases))\n\nincorrectly","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"False Negatives are unforgiven incorrect classifications for the given study. Which are they?"},{"metadata":{"trusted":true},"cell_type":"code","source":"unforgiven_incorrectly = submission.query(\"Diagnosis == 'M' & Predicted == 'B'\")\nunforgiven_incorrectly.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The last thing: submitting the final file."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"predicted.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}