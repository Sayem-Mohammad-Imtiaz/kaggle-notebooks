{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# 786\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport plotly.express as px\nimport plotly.graph_objs as go\n\nimport plotly as py\nfrom plotly import tools\nfrom plotly.offline import iplot\npy.offline.init_notebook_mode(connected = True)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Loading & Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = pd.read_csv(\"../input/pakistans-largest-ecommerce-dataset/Pakistan Largest Ecommerce Dataset.csv\", parse_dates=[\"created_at\", \"Working Date\"], low_memory=False)\nprint(\"Data Dimensions are: \", dt.shape)\nprint(\"Columns: \", dt.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dt.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data contains 1048574 rows but maximum columns contain 584524 records. \n\nHalf of row are completely empty, so we will drop them. The tricky part is we can't drop all na rows as actual data set  also contain few NA entries. We need to keep them.\nWe will drop NA values where all entries are Null. \n\nAlso, we will drop last 5 empty columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = dt.iloc[:, :-5]\ndt = dt.dropna(how = 'all') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The column MV contains leading and trailing space that might cause problem. We will rename it first."},{"metadata":{"trusted":true},"cell_type":"code","source":"dt.rename(columns = {' MV ':'MV'}, inplace = True)\ndt.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see above, few columns are not in correct data type. We need to perform casting."},{"metadata":{"trusted":true},"cell_type":"code","source":"dt['Customer ID'] = dt['Customer ID'].astype(str)\ndt['item_id'] = dt['item_id'].astype(str)\ndt['qty_ordered'] = dt['qty_ordered'].astype(int)  \ndt['Year'] = dt['Year'].astype(int)  \ndt['Month'] = dt['Month'].astype(int)  \n# dt['MV'] = dt['MV'].astype(float, errors = 'raise')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's look into summary of data\nData Summary of non-numeric data"},{"metadata":{"trusted":true},"cell_type":"code","source":"dt.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Summary of non-numeric data"},{"metadata":{"trusted":true},"cell_type":"code","source":"dt.describe(include=['object', 'bool'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Analysis to Understand Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = dt.sort_values('created_at')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Few new features extracted"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtg = dt.groupby('created_at')['grand_total'].sum().reset_index()\ndtq = dt.groupby('created_at')['qty_ordered'].sum().reset_index()\ndtd = dt.groupby('created_at')['discount_amount'].sum().reset_index()\n# comput count for non numeric values\ndts = dt.groupby('created_at')['sku'].count().reset_index() \ndtst = dt.groupby('created_at')['status'].count().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# new data set\np = pd.DataFrame(dtg) \np['qty_ordered'] = dtq['qty_ordered']\np['discount_amount'] = dtd['discount_amount']\np['sku'] = dts['sku']\np['status'] = dtst['status']\n#Cumulative Sum\np['cum_grand_total'] = p['grand_total'].cumsum()\np['cum_qty_ordered'] = p['qty_ordered'].cumsum()\np['cum_discount_amount'] = p['discount_amount'].cumsum()\np['cum_sku_cnt'] = p['sku'].cumsum()\np['cum_status_cnt'] = p['status'].cumsum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Date features\np['Dateofmonth'] = p['created_at'].dt.day\np['Month'] = p['created_at'].dt.month\np['Week'] = p['created_at'].dt.week\np['Dayofweek'] = p['created_at'].dt.dayofweek # 0 = monday.\np['Weekdayflg'] = (p['Dayofweek'] // 5 != 1).astype(float)\np['Month'] = p['created_at'].dt.month\np['Quarter'] = p['created_at'].dt.quarter\np['Dayofyear'] = p['created_at'].dt.dayofyear","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Daily Sales vs. Discount"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\n# Add traces\nfig.add_trace(go.Scatter(x=p['created_at'], y=p['grand_total'],\n                    mode='lines+markers',\n                    name='grand_total'))\nfig.add_trace(go.Scatter(x=p['created_at'], y=p['discount_amount'],\n                    mode='lines+markers',\n                    name='discount_amount'))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cumulative Sums of Grand_Total and discount_amount"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\n# Add traces\nfig.add_trace(go.Scatter(x=p['created_at'], y=p['cum_grand_total'],\n                    mode='lines+markers',\n                    name='xcum_grand_total'))\nfig.add_trace(go.Scatter(x=p['created_at'], y=p['cum_discount_amount'],\n                    mode='lines+markers',\n                    name='cum_discount_amount'))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**In above graphs we can observe that sales boosted when discount offer initiated.**\n\nBut this can we tempting without looking into item status."},{"metadata":{"trusted":true},"cell_type":"code","source":"n = dt.groupby(['Year' ,'status'])['grand_total'].sum().reset_index()\nfig = px.bar(n, x=\"Year\", y=\"grand_total\", color=\"status\", title=\"Long-Form Input\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**In each year order cancellation is high. We need to drop Cancelled items and recheck sales growth**\n\nNote: We will do this after looking into other data points. "},{"metadata":{"trusted":true},"cell_type":"code","source":"n = dt.groupby(['Year' ,'payment_method'])['grand_total'].sum().reset_index()\nfig = px.bar(n, x=\"Year\", y=\"grand_total\", color=\"payment_method\", title=\"Long-Form Input\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Order Status"},{"metadata":{"trusted":true},"cell_type":"code","source":"n = dt.groupby(['status'])['grand_total'].sum().reset_index()\nfig = px.bar(n, y='grand_total', x='status', text='grand_total')\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = dt.groupby(['created_at' ,'status'])['grand_total'].sum().reset_index()\npx.box(n, y=\"grand_total\", color = \"status\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Category Type"},{"metadata":{"trusted":true},"cell_type":"code","source":"n = dt.groupby(['category_name_1'])['grand_total'].sum().reset_index()\nfig = px.bar(n, y='grand_total', x='category_name_1', text='grand_total')\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = dt.groupby(['category_name_1','status'])['grand_total'].sum().reset_index()\nfig = px.bar(n, x=\"category_name_1\", y=\"grand_total\",\n             color='status', barmode='group')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Payment Methods\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"n = dt.groupby(['payment_method'])['grand_total'].sum().reset_index()\n\nfig = px.bar(n, y='grand_total', x='payment_method', text='grand_total')\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Growth Analysis"},{"metadata":{},"cell_type":"markdown","source":"As we analysed above, we need to drop cancelled orders\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"ord_cncl_ind = dt[dt['status'] == 'canceled' ].index\ndt.drop(ord_cncl_ind , inplace=True)\ndt.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Recomputing daily figures"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtg = dt.groupby('created_at')['grand_total'].sum().reset_index()\ndtq = dt.groupby('created_at')['qty_ordered'].sum().reset_index()\ndtd = dt.groupby('created_at')['discount_amount'].sum().reset_index()\n# comput count for non numeric values\ndts = dt.groupby('created_at')['sku'].count().reset_index() \ndtst = dt.groupby('created_at')['status'].count().reset_index()\n\n# new data set\np = pd.DataFrame(dtg) \np['qty_ordered'] = dtq['qty_ordered']\np['discount_amount'] = dtd['discount_amount']\np['sku'] = dts['sku']\np['status'] = dtst['status']\n#Cumulative Sum\np['cum_grand_total'] = p['grand_total'].cumsum()\np['cum_qty_ordered'] = p['qty_ordered'].cumsum()\np['cum_discount_amount'] = p['discount_amount'].cumsum()\np['cum_sku_cnt'] = p['sku'].cumsum()\np['cum_status_cnt'] = p['status'].cumsum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\n# Add traces\nfig.add_trace(go.Scatter(x=p['created_at'], y=p['grand_total'],\n                    mode='lines+markers',\n                    name='grand_total'))\nfig.add_trace(go.Scatter(x=p['created_at'], y=p['discount_amount'],\n                    mode='lines+markers',\n                    name='discount_amount'))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A quick view of Regession model (OLS)"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(p, x= 'created_at', y = 'grand_total', trendline = \"ols\")\nfig.show()\nresults = px.get_trendline_results(fig)\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Density Graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"n = dt.groupby('created_at')['grand_total'].sum().reset_index()\npx.density_contour(n,x=\"created_at\",y=\"grand_total\",marginal_x=\"histogram\",marginal_y=\"histogram\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Graph for quantity\nn = dt.groupby('created_at')['qty_ordered'].sum().reset_index()\npx.density_contour(n,x=\"created_at\",y=\"qty_ordered\",marginal_x=\"histogram\",marginal_y=\"histogram\", title=\"no of orders\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = dt.groupby(['created_at' ,'category_name_1', 'status'])['qty_ordered'].sum().reset_index()\npx.scatter(n, x=\"created_at\", y=\"qty_ordered\", color=\"status\", size=\"qty_ordered\", hover_data=['category_name_1','status'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = dt.groupby(['created_at' ,'status'])['qty_ordered'].sum().reset_index()\npx.line(n, x=\"created_at\", y=\"qty_ordered\", color=\"status\", )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To be Continue...\n\n**You can fork this kernel and continue your analysis.**\n\n**Way Forward**\n* Data Cleansing at SKU and Status columns\n* Segregate analysis by dropping Cancel status orders. \n* Quarterly, Monthly, Weekday and Weekend Analysis\n* Seasonality Analysis\n* What are the Trends in Top 10 Categories\n* Weekly Moving Average Analysis"},{"metadata":{},"cell_type":"markdown","source":"# Quarterly, Monthly, Weekday and Weekend Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"pa = pd.DataFrame(dt)\n# pa.reset_index(inplace=True)\npa.drop(pa[pa.status==\"canceled\"].index,inplace=True) #dropping cancelled orders\ndf = pd.DataFrame(pa)\n#adding few more date features\ndf[\"month_name\"] = df[\"created_at\"].dt.month_name()\ndf[\"week_day_name\"] = df[\"created_at\"].dt.day_name()\ndf[\"week_day\"] = df[\"created_at\"].dt.weekday\ndf[\"week\"] = df[\"created_at\"].dt.isocalendar().week\ndf[\"month_start\"] = df[\"created_at\"].dt.is_month_start\ndf[\"month_end\"]= df[\"created_at\"].dt.is_month_end\ndf[\"quarter\"] = df[\"created_at\"].dt.quarter\ndf[\"quarter_start\"] = df[\"created_at\"].dt.is_quarter_end\ndf[\"quarter_end\"]= df[\"created_at\"].dt.is_quarter_start\n\ndf[\"year_start\"] = df[\"created_at\"].dt.is_year_start\ndf[\"year_end\"] = df[\"created_at\"].dt.is_year_end\ndf[\"month\"] = df[\"created_at\"].dt.month\ndf.columns = df.columns.str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quarterly = df[[\"grand_total\",\"discount_amount\"]].groupby(df.quarter).sum() # Extracting quarterly turnover \nfig = px.bar(quarterly,x=quarterly.index,y=[\"grand_total\",\"discount_amount\"], title=\"Quaterly Turnover\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly = df[[\"grand_total\",\"discount_amount\"]].groupby(df.month_name).sum() #Extracting monthly turnover \nfig = px.bar(monthly,x=monthly.index,y=[\"grand_total\",\"discount_amount\"], title=\"Monthly Turnover\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weekday = df[[\"grand_total\",\"discount_amount\"]].groupby(df.week_day_name).sum() # Extracting day wise \nfig = px.bar(weekday,x=weekday.index,y=[\"grand_total\",\"discount_amount\"], title=\"Day-wise Turnover\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"month_end = df[[\"grand_total\",\"discount_amount\"]].groupby(df.month_end).sum() \nfig = px.bar(month_end,x=month_end.index,y=[\"grand_total\",\"discount_amount\"], title=\"Month end days vs otherdays\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"year = df[[\"grand_total\",\"discount_amount\"]].groupby(df.year).sum() # Extracting year wise \nfig = px.bar(year,x=year.index,y=[\"grand_total\",\"discount_amount\"], title=\"Yearly Turnover\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Top Ten Categories in each Year"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_2016 = df.groupby([df.year,df.category_name_1]).grand_total.sum().loc[2016].nlargest(10)\ny_2017 = df.groupby([df.year,df.category_name_1]).grand_total.sum().loc[2017].nlargest(10)\ny_2018 = df.groupby([df.year,df.category_name_1]).grand_total.sum().loc[2018].nlargest(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from plotly.subplots import make_subplots\n\n# Create subplots, using 'domain' type for pie charts\nspecs = [[{'type':'domain'}, {'type':'domain'}], [{'type':'domain'}, {'type':'domain'}]] #adopted from https://plotly.com/python/pie-charts/\nfig = make_subplots(rows=2, cols=2, specs=specs) \n# Define pie charts\nfig.add_trace(go.Pie(labels=y_2016.index, values=y_2016, title='2016'), 1, 1)\nfig.add_trace(go.Pie(labels=y_2017.index, values=y_2017, title='2017'), 1, 2)\nfig.add_trace(go.Pie(labels=y_2018.index, values=y_2018, title='2018'), 2, 1)\n# Tune layout and hover info\n# fig.update_traces(hoverinfo='label+percent+name', textinfo='none')\nfig.update(layout_title_text= \"Yearly share of Top Ten Categories by Grand Total\",\n           layout_showlegend=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.payment_method =  df.payment_method.str.lower()\ny_2016pm = df.groupby([df.year,df.payment_method]).grand_total.sum().loc[2016].nlargest(10)\ny_2017pm = df.groupby([df.year,df.payment_method]).grand_total.sum().loc[2017].nlargest(10)\ny_2018pm = df.groupby([df.year,df.payment_method]).grand_total.sum().loc[2018].nlargest(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Top (10)  most preferable payment methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create subplots, using 'domain' type for pie charts\nspecs = [[{'type':'domain'}, {'type':'domain'}], [{'type':'domain'}, {'type':'domain'}]] #adopted from https://plotly.com/python/pie-charts/\nfig = make_subplots(rows=2, cols=2, specs=specs) \n# Define pie charts\n\nfig.add_trace(go.Pie(labels=y_2016pm.index, values=y_2016pm, title='2016'), 1, 1)\nfig.add_trace(go.Pie(labels=y_2017pm.index, values=y_2017pm, title='2017'), 1, 2)\nfig.add_trace(go.Pie(labels=y_2018pm.index, values=y_2018pm, title='2018'), 2, 1)\n# Tune layout and hover info\nfig.update_traces(hole=.4, hoverinfo='label+percent+name',)\nfig.update(layout_title_text= \"Most used payments methods for each year\",\n           layout_showlegend=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#  Unveiling Mean reverting behaviour \n\nThe Growth analysis unravel some interesting patterns in daily order count.However, one may ask does \norder count keep on growing persistently. Mainly, is there any mean reverting behavior. In simple words,if a time series exibhit mean reversion it plunges to its long-run or shor run average value. \nThe average value act as a magnetic force, pulling the series towards it. In order to unveil mean reverting behaviour we will carry on some moving average analysis."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Moving Average Analysis for daily orders count"},{"metadata":{"trusted":true},"cell_type":"code","source":"dto = df.groupby(\"created_at\").sum()[[\"grand_total\",\"discount_amount\"]] # aggregating sum day-wise\ndto[\"opd\"] = df.groupby(\"created_at\").size() # extracting daily count for orders\n# Simple Moving Average for Grand total \ndto[\"3 Days Moving Average\"] = dto.opd.rolling(3).mean() # Window = 3 days\ndto[\"7 Days Moving Average\"] = dto.opd.rolling(7).mean() # Window = 3 days\ndto[\"twenty_1_sma\"] = dto.opd.rolling(window=21).mean() # Window = 3 days\n# Exponentialy weighted moving avg\nalpha=0.2\ndto[\"EWM_Avg\"] = dto.opd.ewm(alpha=alpha).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from plotly.subplots import make_subplots\nsubplot_titles = [\"3 days Simple Moving Average\",\"7 days Simple Moving Average\", \n                  \"21 days Simple Moving Average\",\n                 f\"Exponential weighted moving average with Alpha = {alpha}\"]\nfig = make_subplots(rows=4, cols=1,shared_yaxes=False,shared_xaxes=True,vertical_spacing=0.1,\n                    subplot_titles=subplot_titles)\nfig.add_scatter(x=dto.index, y=dto.opd, row=1, col=1, name=\"Orders Per Day\")\nfig.add_scatter(x=dto.index, y=dto[\"3 Days Moving Average\"], name=\"3 Days MA\",row=1, col=1)\nfig.add_scatter(x=dto.index, y=dto.opd, row=2, col=1, name=\"Orders Per Day\")\nfig.add_scatter(x=dto.index, y=dto[\"7 Days Moving Average\"], name=\"7 Days MA\",row=2, col=1)\nfig.add_scatter(x=dto.index, y=dto.opd,name=\"Orders Per Day\", row=3, col=1)\nfig.add_scatter(x=dto.index, y=dto.twenty_1_sma,name=\"21 Days MA\", row=3, col=1)\nfig.add_scatter(x=dto.index, y=dto.opd,name=\"Orders Per Day\", row=4, col=1)\nfig.add_scatter(x=dto.index, y=dto.EWM_Avg,name=\"EMW Average\", row=4, col=1)\nfig.update_layout(height=900,width=850, showlegend=True,\n                  title_text=\"Moving Average Analysis for Order Count\",\n                 legend=dict( orientation=\"v\"))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Moving Average Analysis for daily Grand total (Turnover)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simple Moving Average for Grand total \ndto[\"tdayma_gt\"] = dto.grand_total.rolling(window=3).mean()  # window = 3 days\ndto[\"sdayma_gt\"] = dto.grand_total.rolling(window=7).mean() # window = 7 days\ndto[\"twenty_1_sma_gt\"] = dto.grand_total.rolling(window=21).mean() # Window = 21 days\n# Exponentialy weighted moving avg\nalpha=0.2 # setting aplha equal to 0.2 \ndto[\"EWM_Grand_Total\"] = dto.grand_total.ewm(alpha=alpha).mean() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=4, cols=1,shared_yaxes=False,shared_xaxes=True,vertical_spacing=0.1,\n                   subplot_titles=subplot_titles)\nfig.add_scatter(x=dto.index, y=dto.grand_total, row=1, col=1, name=\"Daily Grand Total\")\nfig.add_scatter(x=dto.index, y=dto.tdayma_gt, name=\"3 Days MA for Grand Total\",row=1, col=1)\nfig.add_scatter(x=dto.index, y=dto.grand_total, row=2, col=1, name=\"Daily Grand Total\")\nfig.add_scatter(x=dto.index, y=dto.sdayma_gt, name=\"7 Days MA for Grand Total\",row=2, col=1)\nfig.add_scatter(x=dto.index, y=dto.grand_total,name=\"Daily Grand Total\", row=3, col=1)\nfig.add_scatter(x=dto.index, y=dto.twenty_1_sma_gt,name=\"21 Days Moving Average\", row=3, col=1)\nfig.add_scatter(x=dto.index, y=dto.grand_total,name=\"Daily Grand Total\", row=4, col=1)\nfig.add_scatter(x=dto.index, y=dto.EWM_Grand_Total,name=\"EMW Average\", row=4, col=1)\nfig.update_layout(height=900,width=850, showlegend=True,\n                  title_text=\"Moving Average Analysis for Grand Total\",\n                 legend=dict( orientation=\"v\",yanchor='top',xanchor=\"left\"))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Moving Average analysis clearly indicates that both series  daily order counts  and daily grand total kept on hovering around average value hence exbiting mean reversion. "},{"metadata":{},"cell_type":"markdown","source":"# Work in progress..... more to come"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}