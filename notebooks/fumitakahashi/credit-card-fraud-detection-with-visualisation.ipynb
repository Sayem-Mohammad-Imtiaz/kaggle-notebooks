{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center>Kaggle Credit Card Fraud Detection</center>\n\n## Table of contents\n> ### 1. [Understanding the overall data](#1) \n> ### 2. [Understanding the individual feature](#2)\n> ### 3. [Data cleaning](#3)\n> ### 4. [Model with row data](#4)\n> ### 5. [Feature procesing](#5)\n> ### 6. [Feature selection](#6)\n> ### 7. [Train and predict](#7)\n"},{"metadata":{},"cell_type":"markdown","source":"<a id='1'></a>\n# 1. Understanding the overall data\n"},{"metadata":{},"cell_type":"markdown","source":"> #### 1.1 [Load necesarry libraries](#1.1)\n> #### 1.2 [Load data](#1.2)\n> #### 1.3 [Check data in row format](1.3)\n> #### 1.4 [Check missing data](#1.4)\n> #### 1.5 [Meaning of each feature](#1.5)"},{"metadata":{},"cell_type":"markdown","source":"<a id='1.1'></a>\n### 1.1 Load necesarry libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, recall_score, f1_score, accuracy_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='1.2'></a>\n### 1.2 Load data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='1.3'></a>\n### 1.3  Check data in row format\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iloc[-5:,:].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note\n* 25 columns in total (**default.payment.next.month** is the target)\n* 30000 samples\n* ID is given to each card"},{"metadata":{},"cell_type":"markdown","source":"<a id='1.4'></a>\n### 1.4 Check missing data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note\n* No missing data\n"},{"metadata":{},"cell_type":"markdown","source":"<a id='1.5'></a>\n### 1.5 Meaning of each feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  "},{"metadata":{},"cell_type":"markdown","source":"#### Note\nI categorized the features into 3 categories: User's card information, User's demograohic information, Client's payment information in the last 6 months.\n\n\n\n#### Client's card information \n* LIMIT_BAL\n    * Upper limit of credit card\n    * 10000.0 ~ 1000000.0 (in Hong Kong Dollars)\n    \n#### Client's demographic information\n\n* SEX\n    * Sex of the card user\n    * male or female\n* EDUCATION\n    * Final education level of the card user\n    * Graduate school, Undergraduate school, High school, Others, Unknown\n* MARRIAGE\n    * Marriage status of the card user\n    * Married, Single, Others, Unknown\n* AGE\n    * Age of the card user\n    * 21 ~ 79\n    \n#### Client's payment information in last 6 months\n\n* PAY\n    * Payment status of last 6 months\n    * No payment, Duly payment, Revolving Payment, Delay (for 1~8 months + more than 9 months)\n* BILL_AMT\n    * Amounts of bill in last 6 months\n    * -339603.0 ~ 1664089.0 (in Hong Kong Dollars)\n* PAY_AMT\n    * Amounts paid in the previous month in last 6 months\n    * 0.0 ~ 1684259.0 (in Hong Kong Dollars)\n    \n"},{"metadata":{},"cell_type":"markdown","source":"<a id='2'></a>\n# 2. Understanding the individual feature\n"},{"metadata":{},"cell_type":"markdown","source":"I will take a look at the distribution of individual feature."},{"metadata":{},"cell_type":"markdown","source":"> #### 2.1 [default.payment.next.month](#2.1)\n> #### 2.2 [LIMIT_BAL](#2.2)\n> #### 2.3 [SEX](#2.3)\n> #### 2.4 [EDUCATION](#2.4)\n> #### 2.5 [MARRIAGE](#2.5)\n> #### 2.6 [AGE](#2.6)\n> #### 2.7 [PAY](#2.7)\n> #### 2.8 [PAY_AMT](#2.8)\n> #### 2.9 [BILL_AMT](#2.9)"},{"metadata":{},"cell_type":"markdown","source":"<a id='2.1'></a>\n## 2.1 default.payment.next.month"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df.copy()\nclasses = pd.value_counts(df2['default.payment.next.month'], sort = True).sort_index()\nclasses.plot(kind = 'bar')\nplt.title(\"default.payment.next.month histogram\")\nplt.show()\nprint('Number of frauds clients: ', len(df2[df2['default.payment.next.month'] == 1]))\nprint('Fraud percentage:', round(df2['default.payment.next.month'].value_counts()[1]/len(df2) * 100,2), \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note\n* 22.12 % of the clients are fraud. \n* The data is a little bit imbalanced."},{"metadata":{},"cell_type":"markdown","source":"<a id='2.1'></a>\n## 2.2   LIMIT_BAL"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nfig = plt.figure(figsize = (7,5))\nax = plt.subplot()\n\nsns.distplot(df2[\"LIMIT_BAL\"][df2['default.payment.next.month']==0], bins = 40, label = 'non-Fraud',kde = False)\nsns.distplot(df2[\"LIMIT_BAL\"][df2['default.payment.next.month']==1], bins = 40, label = 'Fraud',kde = False)\n\nplt.legend(loc = 'upper right')\nplt.title(\"LIMIT_BAL Histogram\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='default.payment.next.month',y='LIMIT_BAL',data=df2,palette='Set2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note\n* Extremelly large values in upper range.\n* Higher concentration at certain values (e.g: 500000). -> Possibly because there are specific card types like Family credit card. \n* At extremelly high values, no fraud is observed. "},{"metadata":{},"cell_type":"markdown","source":"<a id='2.3'></a>\n## 2.3 SEX"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df.copy()\nSEX_dict = {1:\"male\", 2:\"female\"}\ndf2['SEX'] = df2['SEX'].map(SEX_dict)\n\ndf_sex = df2.groupby(['SEX', 'default.payment.next.month']).size().unstack(1)\ndf_sex.plot(kind='bar', stacked = True)\nplt.legend(loc = 'upper left')\nplt.title(\"SEX Histogram\")\nfig.show()\n\ndf_sex['Fraud_rate'] = (df_sex[df_sex.columns[1]]/(df_sex[df_sex.columns[0]] + df_sex[df_sex.columns[1]]))\nprint(df_sex)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note\n* There is a greater number of female clients than male.\n* Fraud rate is lower for female. "},{"metadata":{},"cell_type":"markdown","source":"<a id='2.4'></a>\n## 2.4 EDUCATION\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nEDUCATION_dict = {0:\"error\", 1:\"graduate school\", 2:\"undergraduate\", 3:\"high school\", 4:\"others\", 5:\"unknown\", 6:\"unknown\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df.copy()\n\nEDUCATION_dict = {0:\"error\", 1:\"graduate school\", 2:\"undergraduate\", 3:\"high school\", 4:\"others\", 5:\"unknown\", 6:\"unknown\"}\ndf2['EDUCATION'] = df2['EDUCATION'].map(EDUCATION_dict)\ndf_ed = df2.groupby(['EDUCATION', 'default.payment.next.month']).size().unstack(1)\ndf_ed.plot(kind='bar', stacked = True)\nplt.legend(loc = 'upper left')\nplt.title(\"EDUCATION Histogram\")\n\nfig.show()\n\ndf_ed['Fraud_rate'] = (df_ed[df_ed.columns[1]]/(df_ed[df_ed.columns[0]] + df_ed[df_ed.columns[1]]))\nprint(df_ed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note\n* Most of clients are distributed in graduate school, high school and undergraduate. \n* Fraud rates of the minority classes are low. I will combine the three classes together later. "},{"metadata":{},"cell_type":"markdown","source":"<a id='2.5'></a>\n## 2.5 MARRIAGE"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df.copy()\n\nMARRIAGE_dict  = {0: 'unknown', 1:\"married\", 2:\"single\", 3:\"others\"}\ndf2['MARRIAGE'] = df2['MARRIAGE'].map(MARRIAGE_dict)\n\ndf_mar = df2.groupby(['MARRIAGE', 'default.payment.next.month']).size().unstack(1)\ndf_mar.plot(kind='bar', stacked = True)\nplt.legend(loc = 'upper left')\nplt.title(\"MARRIAGE Histogram\")\nfig.show()\n\ndf_mar['Fraud_rate'] = (df_mar[df_mar.columns[1]]/(df_mar[df_mar.columns[0]] + df_mar[df_mar.columns[1]]))\nprint(df_mar)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note\n* Others have small samples, but the fraud rate is higher than married or single people. \n* Single people have the least fraud rate. "},{"metadata":{},"cell_type":"markdown","source":"<a id='2.6'></a>\n## 2.6 AGE"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nfig = plt.figure(figsize = (7,5))\nax = plt.subplot()\n\nsns.distplot(df2[\"AGE\"][df2['default.payment.next.month']==0], bins = 40, label = 'non-Fraud',kde = False)\nsns.distplot(df2[\"AGE\"][df2['default.payment.next.month']==1], bins = 40, label = 'Fraud',kde = False)\n\nplt.legend(loc = 'upper right')\nplt.title(\"AGE Histogram\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It is difficult to see the trend, so I will categorize into 20s, 30s, 40s, 50s, 60s, 70s and 80s."},{"metadata":{"trusted":true},"cell_type":"code","source":"AGE_bin = [20, 30, 40, 50, 60, 70, 80]\nAGE_labels  = [ \"20s\", \"30s\", \"40s\", \"50s\", \"60s\", \"70s\"]\ndf2[\"AGE\"] = pd.cut(df[\"AGE\"], AGE_bin,right=False, labels=AGE_labels)\n\ndf_age = df2.groupby(['AGE', 'default.payment.next.month']).size().unstack(1)\ndf_age.plot(kind='bar', stacked = True)\nplt.legend(loc = 'upper left')\nplt.title(\"AGE Bin Histogram\")\nfig.show()\n\ndf_age['Fraud_rate'] = (df_age[df_age.columns[1]]/(df_age[df_age.columns[0]] + df_age[df_age.columns[1]]))\nprint(df_age)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note\n* Fraud rate is the lowest at 30s.\n* From 30s, the older the client, the higher rate of fraud."},{"metadata":{},"cell_type":"markdown","source":"<a id='2.7'></a>\n## 2.7 PAY"},{"metadata":{"trusted":true},"cell_type":"code","source":"tab = pd.DataFrame(df2[\"PAY_0\"].value_counts().sort_index(ascending=False))\n\nfor i in range(2,7):\n    tab[\"PAY_\" + str(i)] = df2[\"PAY_\" + str(i)].value_counts().sort_index(ascending=False)\ntab = tab.T\n \nfig = plt.figure(figsize=(12,6))\nax = fig.add_subplot(111)\ncolors = sns.color_palette(\"hls\", 15)\n\nrows, cols = len(tab), len(tab.columns)\nx = range(rows)\n\n    \nfor i, t in enumerate(tab.columns):\n    y = tab.iloc[:, i:cols].sum(axis=1)\n    ax.bar(x, y, label=t, color = colors[i])\n    \nax.set_xticks(range(rows + 2))\nax.set_xticklabels(tab.index)\nax.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note\n* The share of revolving payment is large.\n* There are few people dalayed for few months. "},{"metadata":{},"cell_type":"markdown","source":"I will take look at composition of payments for dafalt and non-default clients separately."},{"metadata":{"trusted":true},"cell_type":"code","source":"PAY_dict  = {-2:\"no pay\", -1:\"dul pay\", 0:\"rev pay\", 1:\"1m del\", 2:\"2m del\",\n                                3:\"3m del\", 4:\"4m del\", 5:\"5m del\", 6:\"6m del\", 7:\"8m del\", 8:\"9m del\"}\ndf2 = df.copy()\nfrom itertools import chain\ntab_list = [\"PAY_\" + str(i) for i in chain(range(0,1),range(2, 7))]\nk = 1\nfig = plt.figure(figsize=(13,10))\nfor i in tab_list:\n    df2[i] = df2[i].map(PAY_dict)\n    tab = pd.crosstab(df2['default.payment.next.month'], df2[i], normalize='index')\n    \n    ax = fig.add_subplot(2,3, k)\n    k = k+1\n\n    rows, cols = len(tab), len(tab.columns)\n    x = range(rows)\n\n    for j, t in enumerate(tab.columns):\n        y = tab.iloc[:, j:cols].sum(axis=1)\n        if((i=='PAY_5')|(i=='PAY_6')):\n            ax.bar(x, y, label=t, color = colors[j+1])\n        else:\n            ax.bar(x, y, label=t, color = colors[j])\n    ax.set_xticks(range(rows + 1))\n    ax.set_xticklabels(tab.index)\n    ax.set_title(i)\n    ax.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note\n* Obviously, clients with delayed payment are more likely to fall into default.\n* But it is still true that there are some people who paid late but still get non-default and vice versa. "},{"metadata":{},"cell_type":"markdown","source":"<a id='2.8'></a>\n## 2.8 PAY_AMT"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn-darkgrid')\npalette = plt.get_cmap('Set1')\nplt.figure(figsize=(13, 9), dpi=100)\n\n\ncols = ['PAY_AMT1','PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\ni = 1\nfor c in cols:\n    plt.subplot(2,3, i)\n    plt.hist(df2[c][(df2['default.payment.next.month'] == 0)], bins=300, label='non-defalut', color = 'steelblue')\n    plt.hist(df2[c][(df2['default.payment.next.month'] == 1)], bins=300, label='default', color = 'orange')\n    plt.legend(loc = 'upper right')\n    i = i+1\n    plt.xlim(-100,50000)\n    plt.ylim(0, 20000)\n \n    plt.title(c, fontsize=12, fontweight=0 )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note\n* Left skewed distribution due to the small number of people with extremelly high pay_amt\n* The extreme values are also seen from the box plots below"},{"metadata":{"trusted":true},"cell_type":"code","source":"#li = [\"PAY_AMT\" + str(i) for i in range(1, 7)]\n\nfig = plt.figure(figsize=(13,5))\nax  = fig.add_subplot(111)\nax.set_title(\"PAY_AMT\")\nax.boxplot(df[cols].values, labels=cols, patch_artist=True)\nax.set_facecolor\nax.set_ylim(0,600000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the boxes closely for default and non default cases."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(13,7))\nax  = fig.add_subplot(211)\nax2 = fig.add_subplot(212)\nax.set_title(\"PAY_AMT\")\nax.boxplot(df2[cols][df2['default.payment.next.month'] == 0].values, labels=cols, patch_artist=True)\nax2.boxplot(df2[cols][df2['default.payment.next.month'] == 1].values, labels=cols, patch_artist=True)\nax.set_ylim(0,20000)\nax2.set_ylim(0,20000)\nax.set_title('Non-default')\nax2.set_title('Default')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note\n* Default clients are distributed at lower PAY_AMT than non-default clients."},{"metadata":{},"cell_type":"markdown","source":"<a id='2.9'></a>\n## 2.9 BILL_AMT"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn-darkgrid')\npalette = plt.get_cmap('Set1')\nplt.figure(figsize=(13, 9), dpi=100)\n\n\ncols2 = ['BILL_AMT1', 'BILL_AMT2','BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']\ni = 1\nfor c in cols2:\n    plt.subplot(2,3, i)\n    plt.hist(df2[c][(df2['default.payment.next.month'] == 0)], bins=300, label='non-defalut', color = 'steelblue')\n    plt.hist(df2[c][(df2['default.payment.next.month'] == 1)], bins=300, label='default', color = 'orange')\n    plt.legend(loc = 'upper right')\n    i = i+1\n    plt.xlim(-100,200000)\n    plt.ylim(0, 8000)\n    \n    plt.title(c, fontsize=12, fontweight=0 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(13,5))\nax  = fig.add_subplot(111)\nax.set_title(\"BILL_AMT\")\nax.boxplot(df2[cols2].values, labels=cols2, patch_artist=True)\nax.set_facecolor\nax.set_ylim(0,2000000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(13,7))\nax  = fig.add_subplot(211)\nax2 = fig.add_subplot(212)\nax.set_title(\"PAY_AMT\")\nax.boxplot(df2[cols2][df2['default.payment.next.month'] == 0].values, labels=cols2, patch_artist=True)\nax2.boxplot(df2[cols2][df2['default.payment.next.month'] == 1].values, labels=cols2, patch_artist=True)\nax.set_ylim(0,100000)\nax2.set_ylim(0,100000)\nax.set_title('Non-default')\nax2.set_title('Default')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note\n* Like PAY_AMT, there are extremelly high values for BILL_AMT.\n* There is not obvious difference in distribtiuon of defalt and non-default, but upper quartile for Non-default seem slighly higher in general. "},{"metadata":{},"cell_type":"markdown","source":"Now, let's take a look how all the 6 months quantitative payment values (PAY_AMT, BILL_AMT) and LIMIT_BAL are related. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bill_amt = df2[['BILL_AMT1', 'BILL_AMT2','BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'LIMIT_BAL']]\ndf_bill_amt[cols] = df2[cols]\n\nplt.figure(figsize=(14, 7), dpi=100)\ncm = np.corrcoef(df_bill_amt.values.T) \ncorr = df_bill_amt.corr()\n\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True,\n    annot = True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n)\nax.set_title('BILL_AMT Correlation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note:\n* BILL_AMT have quite strong correlation among the 6 months. \n* We can also see the correlation between LIMIT_BAL and BILL_AMT\n* There is also some correlation between BILL_AMT and PAY_AMT **from one month before**."},{"metadata":{},"cell_type":"markdown","source":"\n\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"<a id='3'></a>\n# 3. Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"As we saw in the previous section, there were some features that had classes with too small number of samples. Thus, I will combine these classes together as much as possible. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# EDUCATION\n# Merge -4:others to 0:unknown\ndf3[\"EDUCATION\"] = df[\"EDUCATION\"].apply( lambda x: (x+1) if ((x>0) and (x<4)) else 1 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MARRIAGE \n# Mergee unknown:0 to others 3\ndf3[\"MARRIAGE\"] = df3[\"MARRIAGE\"].apply(lambda x: x if x>0 else 3 )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" \n \n \n \n "},{"metadata":{},"cell_type":"markdown","source":"<a id='4'></a>\n# 4. Model with row data"},{"metadata":{},"cell_type":"markdown","source":"> #### 4.1[Separate into train and test set](#4,1)\n> #### 4.2[Train and predict](#4.2)\n> #### 4.3[Check the result](#4.3)"},{"metadata":{},"cell_type":"markdown","source":"<a id='4.1'></a>\n### 4.1 Separate into train and test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"dffin = df3.copy()\ndffin.drop(\"ID\", axis = 1,  inplace = True)\nX = dffin.drop(\"default.payment.next.month\", axis = 1,  inplace = False)\ny = dffin.loc[:,\"default.payment.next.month\"]\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='4.2'></a>\n### 4.2 Train and Predict"},{"metadata":{},"cell_type":"markdown","source":"I will use Random Forest just for now."},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.ensemble\nrf = sklearn.ensemble.RandomForestClassifier()\nrf.fit(X_train, Y_train)\nY_predrf = rf.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n\nplt.figure(figsize=(14, 7), dpi=100)\nfeat_importances.plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The correlation of each feature to the target values are:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cori = []\nfor col in df3.columns:\n    print(col,'     ', df3[col].dtypes)\n    cori.append([col, df3[col].corr(df3['default.payment.next.month'])])\ndf_cor = pd.DataFrame(sorted(cori, key = lambda x: abs(x[1]), reverse = True), columns = ['feature', 'corr'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 7))\nplt.barh(df_cor['feature'], np.abs(df_cor['corr']))\nplt.title(\"Correlation with default.payment.next.month\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='4.3'></a>\n### 4.3 Check the result"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc(fpr, tpr, y_target, y_predicted):\n    auc = roc_auc_score(y_target, y_predicted)\n    gini = 2*auc-1\n    plt.plot(fpr, tpr, color='red', label='ROC')\n    plt.plot([0, 1], [0, 1], color='blue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC (AUC: {:.3f}, GINI: {:.3f})'.format(auc,gini) )\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(Y_test, Y_predrf)\nplot_roc(fpr, tpr, Y_test, Y_predrf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note\n* GINI with row data was 0.44, which is not very bad, but still has space for improve. \n* In terms of feature importance, AGE has PAY_1(the payment status of 1 month ago) has the biggest impact on the model.\n* BILL_AMT comes top as well, but as we saw in section 2, they are correlated, and so is LIMIT_BAL.\n* SEX and MARRIAGE has low impact.\n* PAY have strong correlation with the target value.\n* BILL on the other hand have relatively weak correlation.\n* AGE is also ranked low."},{"metadata":{},"cell_type":"markdown","source":"\n\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"<a id='5'></a>\n# 5. Feature Processing"},{"metadata":{},"cell_type":"markdown","source":"Considering the observations from Section 2 and Section 4, I will further process the features in order to improve my model."},{"metadata":{"trusted":true},"cell_type":"code","source":"df5 = df3.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check how many times clients paid late\n# Check whether the client's payment status is constant but delay\n\ndft = df5[cols]\ndf5['delay_count'] = (dft>=1).sum(axis = 1)\n\ndf5['constant_payer'] = (df5[cols].std(axis=1)== 0).astype(int)\ndf5[df5['delay_count']>0]['constant_payer'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ratio of BILL and LIMIT_BAL\ndf5['UsedRate6'] = df5.BILL_AMT6 / df5.LIMIT_BAL\ndf5['UsedRate5'] = df5.BILL_AMT5 / df5.LIMIT_BAL\ndf5['UsedRate4'] = df5.BILL_AMT4 / df5.LIMIT_BAL\ndf5['UsedRate3'] = df5.BILL_AMT3 / df5.LIMIT_BAL\ndf5['UsedRate2'] = df5.BILL_AMT2 / df5.LIMIT_BAL\ndf5['UsedRate1'] = df5.BILL_AMT1 / df5.LIMIT_BAL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cut the upper outliers for BILL_AMT and PAY_AMT\np0 = df5[cols].min()\np98 = df5[cols].quantile(0.98)\ndf5[cols] = df5[cols].clip(p0, p98, axis = 1)\n\np0 = df5[cols2].quantile(0.01)\np98 = df5[cols2].quantile(0.98)\ndf5[cols] = df5[cols2].clip(p0, p98, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p98","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='6'></a>\n# 6. Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"First, I will build Random Forest model with all features. Then, I will select some important features and use XGBoost to predict."},{"metadata":{"trusted":true},"cell_type":"code","source":"dffin = df5.copy()\ndffin.drop(\"ID\", axis = 1,  inplace = True)\nX = dffin.drop(\"default.payment.next.month\", axis = 1,  inplace = False)\ny = dffin.loc[:,\"default.payment.next.month\"]\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize the data\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = sklearn.ensemble.RandomForestClassifier()\nrf.fit(X_train, Y_train)\nY_predrf = rf.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n\nplt.figure(figsize=(14, 7), dpi=100)\nfeat_importances.plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cori = []\nfor col in dffin.columns:\n    print(col,'     ', dffin[col].dtypes)\n    cori.append([col, dffin[col].corr(dffin['default.payment.next.month'])])\ndf_cor = pd.DataFrame(sorted(cori, key = lambda x: abs(x[1]), reverse = True), columns = ['feature', 'corr'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13, 9))\nplt.barh(df_cor['feature'], np.abs(df_cor['corr']))\nplt.title(\"Correlation with default.payment.next.month\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(Y_test, Y_predrf)\nplot_roc(fpr, tpr, Y_test, Y_predrf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n#### Let's select the features.\n* I will just pick the features with high correlations and high importances."},{"metadata":{"trusted":true},"cell_type":"code","source":"list(df_cor['feature'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_features = [\n 'PAY_0',\n 'PAY_2',\n 'PAY_3',\n 'PAY_4',\n 'PAY_5',\n 'PAY_6',\n 'delay_count',\n 'LIMIT_BAL',\n 'UsedRate6',\n 'UsedRate5',\n 'UsedRate4',\n 'UsedRate3',\n 'UsedRate2',\n 'UsedRate1',\n 'constant_payer',\n 'PAY_AMT1',\n 'EDUCATION',\n 'PAY_AMT2',\n 'PAY_AMT4',\n 'PAY_AMT3',\n 'PAY_AMT5',\n 'PAY_AMT6',\n 'SEX',\n 'MARRIAGE',\n 'AGE'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX = dffin.loc[:,final_features]\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, random_state=42)\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = sklearn.ensemble.RandomForestClassifier()\nrf.fit(X_train, Y_train)\nY_predrf = rf.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(Y_test, Y_predrf)\nplot_roc(fpr, tpr, Y_test, Y_predrf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Improvement!\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n\nplt.figure(figsize=(14, 7), dpi=100)\nfeat_importances.plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='7'></a>\n# 7. Train and Predict"},{"metadata":{},"cell_type":"markdown","source":"This is the last part of my work! \n- I will use XGBoost for the classification. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom xgboost import XGBClassifier\nfrom sklearn import model_selection\nimport math\nfrom sklearn.model_selection import GridSearchCV\n# grid_param = {\n#     'gamma':[7, 9],\n#     'eta':[0.6, 0.8],\n#     'n_estimators':[100, 1000],\n#     'max_depth':[4, 7],\n#     'learning_rate':[0.1, 0.01],\n#     'eval_metric':['auc'],\n#     'object':['binary:logistic'],\n#     'subsample': [0.7, 0,9],\n    \n#     }\n# Xgb = XGBClassifier()\n# cv = GridSearchCV(Xgb, grid_param, cv = 5, n_jobs =-1,verbose=True)\n# cv.fit(X_train, Y_train)\n# print(cv.best_params_, cv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nXgbcv = XGBClassifier(gamma = 9, eta= 0.8, learning_rate= 0.01, max_depth=4, n_estimators=1000, subsample= 0.7)\nXgbcv.fit(X_train, Y_train)\nY_pred = Xgbcv.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(Y_test, Y_pred))\nprint(classification_report(Y_test, Y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- f1-score is still very low.\n- Maybe over-sampling would improve more. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}