{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"bdb488d8-7a21-e939-c31a-cb5de9841b01"},"source":"# Classification of Mushrooms"},{"cell_type":"markdown","metadata":{"_cell_guid":"bdf3ccd2-e7cb-9dca-ea1c-93a719177847"},"source":"This notebook shows how to classify mushrooms as edible or poisonous by using various Machine Learning Methods. All the code in this notebook is written Python 3.6.0 . Also library used in this notebook containing Machine Learning methods is sklearn which is an open source library. "},{"cell_type":"markdown","metadata":{"_cell_guid":"20a109d9-a464-59f0-71cc-54e20b5c6ce7"},"source":"This dataset is taken from Kaggle (https://www.kaggle.com )\nThe link for the dataset is as follows:\nhttps://www.kaggle.com/uciml/mushroom-classification\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of\ngilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon\nSociety Field Guide to North American Mushrooms (1981). Each species is identified as definitely\nedible, definitely poisonous, or of unknown edibility and not recommended. This latter class was\ncombined with the poisonous one. The dataset has features which are entirely categorical in\nnature. The dataset will be used after being transformed by LabelEncoder or OneHotEncoding.\nThe following describes the columns and its categorical values and what they represent.\n\n**Attribute Information**: (**classes**: edible=e, poisonous=p)\n**cap-shape**: bell=b, conical=c, convex=x ,flat=f, knobbed=k, sunken=s\n**cap-surface**: fibrous=f, grooves=g, scaly=y, smooth=s\n**cap-color**: brown=n, buff=b, cinnamon=c, gray=g, green=r, pink=p, purple=u, red=e, white=w,\nyellow=y\n**bruises**: bruises=t, no=f\n**odor**: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n**gill-attachment**: attached=a, descending=d, free=f, notched=n\n**gill-spacing**: close=c, crowded=w, distant=d\n**gill-size**: broad=b, narrow=n\n**gill-color**: black=k, brown=n, buff=b, chocolate=h, gray=g, green=r, orange=o ,pink=p,\npurple=u, red=e, white=w, yellow=y\n**stalk-shape**: enlarging=e, tapering=t\n**stalk-root**: bulbous=b, club=c, cup=u, equal=e, rhizomorphs=z, rooted=r, missing=?\n**stalk-surface-above-ring**: fibrous=f, scaly=y, silky=k, smooth=s \n**stalk-surface-below-ring**: fibrous=f, scaly=y, silky=k, smooth=s\n**stalk-color-above-ring**: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e,\nwhite=w, yellow=y\n**stalk-color-below-ring**: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e,\nwhite=w, yellow=y\n**veil-type**: partial=p, universal=u\n**veil-color**: brown=n, orange=o, white=w, yellow=y\n**ring-number**: none=n ,one=o, two=t\n**ring-type**: cobwebby=c, evanescent=e, flaring=f, large=l, none=n, pendant=p, sheathing=s,\nzone=z\n**spore-print-color**: black=k, brown=n, buff=b, chocolate=h, green=r, orange=o, purple=u,\nwhite=w, yellow=y\n**population**: abundant=a, clustered=c, numerous=n, scattered=s, several=v, solitary=y\n**habitat**: grasses=g, leaves=l, meadows=m, paths=p, urban=u, waste=w, woods=d"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7c2d1991-7750-91f7-9ffc-e6be65b3d3f3"},"outputs":[],"source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import preprocessing # Preprocessing\nfrom sklearn import metrics  # For Evaluation\nimport matplotlib.pyplot as plt #For Plots\n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"31296b7b-1fcb-2305-8764-e3d97f0c599d"},"outputs":[],"source":"#Importing the dataset \nraw_data=pd.read_csv('../input/mushrooms.csv')\n\n#Display top few rows in the dataset\nraw_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dc46c631-2aae-8a5d-f3c9-c4ebe1b712e7"},"outputs":[],"source":"#See the description of different columns\nfor col in raw_data :\n    print()\n    print ('Column Name: ',col)\n    print(raw_data[col].describe())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7090a20d-3493-0b6b-d331-355ea46c5cb2"},"outputs":[],"source":"# Different values in the columns and its count\nfor col in raw_data :\n    print()\n    print ('Column Name: ',col)\n    print(raw_data[col].value_counts())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"535ee0cc-1951-72d9-ef0d-6f6ee4c0a7ac"},"outputs":[],"source":"#Split the file into samples and labels\nsamples = raw_data.drop('class',1)\nlabels = raw_data['class']\nprint('Samples')\nprint(samples[:5])\nprint()\nprint('Labels')\nprint(labels[:5])"},{"cell_type":"markdown","metadata":{"_cell_guid":"dee10afb-6c22-dd9c-d522-3f9fc5e8f2ec"},"source":"From the description of data it can be seen that all of the values are categorical values. Thus before initiating training we have to convert them into numerical values. This is done using Label Encoder as shown below"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4b6cc965-6743-448a-562c-85adc87d3eef"},"outputs":[],"source":"\nlb=preprocessing.LabelEncoder() #Initating the encoder\n\n#Encoding the features(columns of samples)\nfor cols in samples.columns:\n    samples[cols] = lb.fit_transform(samples[cols])\n\nprint('Samples')\nprint(samples[:5])\nprint()\n\n#Encoding the labels\nlabels=lb.fit_transform(labels)\nprint('Labels')\nprint(labels[:5])"},{"cell_type":"markdown","metadata":{"_cell_guid":"1f4b68bc-ef53-d7a4-b3a0-ef42beaca2e9"},"source":"Now that ecoding is done it is time to split the data for training , validation and testing purposes. This is done so that a model does not overfit to the training data. Validation data is done so as to select the type of models which perform good in validation set after having trained in training set. But still there is one more problem which arises that is bleeding of the validation data into the training data. This happens when one tries to maximize the performance in validation set by tweaking the parameters in training set. Thus the model tries to indirectly fit the validation set.\nThus the need for a set which can be used for testing and which is isolated from training is required. Performance on this set will be the final deciding factor for selection of the best model."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ce96355c-5c73-1add-b7a6-e4f2b9943b64"},"outputs":[],"source":"from sklearn.model_selection import train_test_split # for splitting the data\n\n# Normally data is split into 70,15,15 % for training , validation and testing respectivlely\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(samples, labels, test_size=0.30, random_state=42)\nX_valid, X_test, Y_valid, Y_test = train_test_split(X_valid, Y_valid, test_size=0.50, random_state=42)\nprint('Training data count : {}'.format(X_train.count()[0]))\nprint('Validation data count : {}'.format(X_valid.count()[0]))\nprint('Testing data count : {}'.format(X_test.count()[0]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"d43f7eed-577e-ac20-e2d7-07e950948b8c"},"source":"Now lets set a benchmark model which we can compare with other models. Since this is a binary classification problem the simplest model would be when the model classifies each sample as poisonous i.e. 1. This will cause roughly half of the samples to be classified correctly."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6a68815c-86c3-4c1e-a7b5-0f842028af09"},"outputs":[],"source":"# Making predictions as 1(poisonous)\nY_pred =np.ones_like(Y_test)\nprint('Precision: {0:2f}'.format(metrics.precision_score(Y_test,Y_pred)))\nprint('Accuracy: {0:2f}'.format(metrics.accuracy_score(Y_test,Y_pred)))\nprint('Recall: {0:2f}'.format(metrics.recall_score(Y_test,Y_pred)))\nprint('F1 score: {0:2f}'.format(metrics.f1_score(Y_test,Y_pred)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"d3bf1c83-1613-20ff-541a-c9d5ee769f45"},"source":"Thus from above it is certain that the worst a model can perform would have these values. For our model it is important to absolutely classify poisonous correctly. Thus importance will be given to precision and F1 score rather then accuracy."},{"cell_type":"markdown","metadata":{"_cell_guid":"752306e5-48d7-abc3-f705-a3064cb50500"},"source":"For training following models are considered : 1) Logistic Regression 2) Decision Tree Classifier 3) Random Forest 4) Support Vector Machines 5) AdaBoost Classifier 6) xgBoost Classifier 7) Stochastic Gradient Descent"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"802e4636-8e07-5203-3247-5e12ff4019e4"},"outputs":[],"source":"from sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom xgboost import XGBClassifier\n\n\n\nmodels ={'Logistic Regression':LogisticRegression(),'Decision Tree Classifier':DecisionTreeClassifier(),\n         'Random Forest':RandomForestClassifier(),'Support Vector Machines':SVC(),'AdaBoost Classifier':AdaBoostClassifier(),\n         'Stochastic Gradient Descent':SGDClassifier(),'xgBoost Classifier':XGBClassifier()}\n\n\n\n#Training models on training set and performance evluation on validation set\nscores_precision=[]\nscores_acc=[]\nscores_recall=[]\nscores_f1score=[]\nnames=[]\nfor name,model in models.items():\n    model.fit(X_train,Y_train)\n    names.append(name)\n    precision=metrics.precision_score(Y_valid,model.predict(X_valid))\n    scores_precision.append(precision)\n    acc=metrics.accuracy_score(Y_valid,model.predict(X_valid))\n    scores_acc.append(acc)\n    recall=metrics.recall_score(Y_valid,model.predict(X_valid))\n    scores_recall.append(recall)\n    f1score=metrics.f1_score(Y_valid,model.predict(X_valid))\n    scores_f1score.append(f1score)\n   \n    dataframe = pd.DataFrame({'Models':names,'Precision':scores_precision,'F1-score':scores_f1score,\n                              'Accuracy':scores_acc,'Recall':scores_recall})\n    \ncols = list(dataframe)\n# move the column to head of list using index, pop and insert\ncols.insert(0, cols.pop(cols.index('Models')))\ndataframe = dataframe.ix[:, cols]\ndataframe"},{"cell_type":"markdown","metadata":{"_cell_guid":"bfcc0d32-3480-2a9a-0719-577982fe460c"},"source":"From the result above it can be concluded that Stochastic Gradient Descent and Logistic regression performs worst in this dataset classification. And rest of the classifiers perform exceptionally well in classification. As previously specified the model will be chosen based on the Precision score and F1 score. Thus any of the following models could be used : Decision Tree Classifier, AdaBoost Classifier, Support Vector Machines, Random Forest, xgBoost Classifier. This result was based on not using cross-validation to optimize the results. "},{"cell_type":"markdown","metadata":{"_cell_guid":"b29a30ad-3cb4-1c37-abdf-ff5c7e16cb68"},"source":"The models train on a large number of features. And may be not all features contribute to the classification. In this dataset and classification the number of features used are 22. Thus to check weather these features actually conrtibute to the classification we will use PCA(Principle Component Analysis) available from sklearn.decomposition . We shall first import the PCA.Then run it with all the components, plot a bar graph. Calculate how many features are required to maintain 98% of data variance."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5c5920fb-548f-cb90-857a-964da0ceaaf8"},"outputs":[],"source":"from sklearn.decomposition import PCA #Importing PCA\npca=PCA(n_components=22) # Initializing PCA\npca.fit(X_train) \nvar_ratio=pca.explained_variance_ratio_\nprint('First 5 features variance ratio is :',var_ratio[:5])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"31f96cf8-e2b8-b72c-61cd-f9a0fe31fcf9"},"outputs":[],"source":"#Plot to show fetures contribution to variance\nwith plt.style.context('dark_background'):\n    plt.figure(figsize=(10, 8))\n    \n    plt.bar(range(22), var_ratio, alpha=0.5, align='center',\n            label='Individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5aa57c6f-38db-6da0-b575-a9dd983c2ef4"},"outputs":[],"source":"from xgboost import plot_importance\nplot_importance(models['xgBoost Classifier'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"4810d3e7-e4b6-4f67-8d2c-a4a19a994adf"},"source":"Thus it can be concluded that odor is the most important feature used for classifying"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b8322d0-bf1a-f734-216b-b9ba3a4fd357"},"outputs":[],"source":"#Now calculating the number of features required to contain 98% of the data's variance\nadd =0.0\ncount =0\nfor i in range(22):\n    count+= 1\n    add+= var_ratio[i]\n    if ((add/1.0)*100 >99.5) :\n        break\n        \nprint('{} features are required to contain {} % of variance in data'.format(count,add*100))  "},{"cell_type":"markdown","metadata":{"_cell_guid":"dca86244-f595-c231-eca2-4e158fbd6129"},"source":"Now that we have determined that 16 features are enough to contain 99.5% of variance, we will transform the data into 16 principle components and retrain the models on these features."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"76f551cd-20c5-f6e5-3248-5d5fbc904bc8"},"outputs":[],"source":"#Transforming \npca=PCA(n_components=16) # Initializing PCA\npca.fit(X_train)\nX_train_pca= pca.transform(X_train)\nX_valid_pca= pca.transform(X_valid)# for validation purposes\nprint(X_train_pca[:5])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"19d841d1-b4cd-c3df-6aad-34a78bcb7e41"},"outputs":[],"source":"#Retraining the models on X_train_pca features and validating on X_valid_pca\nscores_precision=[]\nscores_acc=[]\nscores_recall=[]\nscores_f1score=[]\nnames=[]\nfor name,model in models.items():\n    model.fit(X_train_pca,Y_train)\n    names.append(name)\n    precision=metrics.precision_score(Y_valid,model.predict(X_valid_pca))\n    scores_precision.append(precision)\n    acc=metrics.accuracy_score(Y_valid,model.predict(X_valid_pca))\n    scores_acc.append(acc)\n    recall=metrics.recall_score(Y_valid,model.predict(X_valid_pca))\n    scores_recall.append(recall)\n    f1score=metrics.f1_score(Y_valid,model.predict(X_valid_pca))\n    scores_f1score.append(f1score)\n    dataframe = pd.DataFrame({'Models':names,'Precision':scores_precision,'F1-score':scores_f1score,\n                              'Accuracy':scores_acc,'Recall':scores_recall})\n    \ncols = list(dataframe)\n# move the column to head of list using index, pop and insert\ncols.insert(0, cols.pop(cols.index('Models')))\ndataframe = dataframe.ix[:, cols]\ndataframe"},{"cell_type":"markdown","metadata":{"_cell_guid":"9799a96c-c6db-b4cc-1e89-3d30223ff07d"},"source":"From the results above it clearly shows that without any optimization the best models for this scenario are random forest and Support Vector Machines followed by XGBoost,Decision Tree Classifier and AdaBoost Classifier. Thus we will select them for final testing in the test data set. ALso we will try to optimize them with the help of Grid Search CV."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aef8e869-3ef7-2602-835d-45c448f11207"},"outputs":[],"source":"#Importing Grid Search CV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer,precision_score\n\n#Initializing few models for Grid Search CV\nsvr =SVC()\nran=RandomForestClassifier()\nxgb=XGBClassifier()\nada=AdaBoostClassifier()\ndt=DecisionTreeClassifier()\n\n#parameters for Support Vector Machine\nparameters_svr = {'C':[0.1,0.3,0.9,1.0,3.0,9.0,10.0],\n                  'kernel':('linear','rbf'),\n                  'random_state':[1,42,56,32,15]}\n#parameters for Random_forest\nparameters_ran = {'n_estimators':[2,5,7,10,12,15,18,20],'random_state':[1,42,56,32,15]}\n\n#parameters for XG Boost Classifier\nparameters_xgb = {'learning_rate':[0.01,0.03,0.09,0.1,0.3,0.9,1],'booster':('gbtree','gblinear','dart'),\n                  'random_state':[1,15,2,3,48,42]}\n\n#parameters for Decision Tree Classifier\nparameters_dt = {'min_samples_split':[2,7,10], 'random_state':[1,42,56,32,15]}\n\n#parameters for AdaBoost Classifier\nparameters_ada= {'learning_rate':[0.01,0.03,0.09,0.1,0.3,0.9,1],'n_estimators':[10,30,50,70],\n                'random_state':[1,42,56,32,15]}\n\n#Defining precision as the factor for comparision\nscorer = make_scorer(precision_score)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ce24d1de-da19-eeda-081a-7809b45b2190"},"outputs":[],"source":"#Calculating best parameters for SVM\nclf = GridSearchCV(svr,parameters_svr,scoring=scorer)\nclf.fit(X_train_pca,Y_train)\nprint('SVM best Parameters: ')\nprint(clf.best_params_)\nprint('')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"00fb612c-9732-65e3-5170-7ec2937ed78b"},"outputs":[],"source":"#Calculating best parameters for Random Forest\nclf = GridSearchCV(ran,parameters_ran,scoring=scorer)\nclf.fit(X_train_pca,Y_train)\nprint('Random Forest best Parameters: ')\nprint(clf.best_params_)\nprint('')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"15ad8c16-7074-4983-614e-f050a31e34a8"},"outputs":[],"source":"#Calculating best parameters for xgBoost Classifier\nclf = GridSearchCV(xgb,parameters_xgb,scoring=scorer)\nclf.fit(X_train_pca,Y_train)\nprint('XG Boost best Parameters: ')\nprint(clf.best_params_)\nprint('')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f5ae9d03-f0dc-25d5-f170-ca28d9ec24c2"},"outputs":[],"source":"#Calculating best parameters for DecisionTree Classifier\nclf = GridSearchCV(dt,parameters_dt,scoring=scorer)\nclf.fit(X_train_pca,Y_train)\nprint('DecisionTree best Parameters: ')\nprint(clf.best_params_)\nprint('')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"888adea8-9805-f5bd-d9ab-8a5aeb694d2a"},"outputs":[],"source":"#Calculating best parameters for AdaBoost Classifier\nclf = GridSearchCV(ada,parameters_ada,scoring=scorer)\nclf.fit(X_train_pca,Y_train)\nprint('AdaBoost best Parameters: ')\nprint(clf.best_params_)\nprint('')"},{"cell_type":"markdown","metadata":{"_cell_guid":"107cff0c-a2e5-2a32-0f68-95970af50107"},"source":"Now that we have optimal parameters from the set of parameters that we defined, let us re-train these models on the opitmal parameters and then test them with test set which we have previously kept aside."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"66eefeae-269b-3102-69e4-5dc58cf36ba0"},"outputs":[],"source":"model_optimal={'Random Forest':RandomForestClassifier(n_estimators=7,random_state=1),\n               'Support Vector Machines':SVC(kernel='rbf',random_state=1,C=3),\n               'XG Boost Classifier':XGBClassifier(random_state=1, booster='gbtree', learning_rate= 0.9),\n               'Decision Tree Classifier': DecisionTreeClassifier(min_samples_split= 2, random_state= 42),\n               'AdaBoost Classifier': AdaBoostClassifier(random_state= 1, n_estimators= 70, learning_rate= 1)}\nscores_precision=[]\nscores_acc=[]\nscores_recall=[]\nscores_f1score=[]\nnames=[]\nfor name,model in model_optimal.items():\n    model.fit(X_train_pca,Y_train)\n    names.append(name)\n    precision=metrics.precision_score(Y_valid,model.predict(X_valid_pca))\n    scores_precision.append(precision)\n    acc=metrics.accuracy_score(Y_valid,model.predict(X_valid_pca))\n    scores_acc.append(acc)\n    recall=metrics.recall_score(Y_valid,model.predict(X_valid_pca))\n    scores_recall.append(recall)\n    f1score=metrics.f1_score(Y_valid,model.predict(X_valid_pca))\n    scores_f1score.append(f1score)\n    dataframe = pd.DataFrame({'Models':names,'Precision':scores_precision,'F1-score':scores_f1score,\n                              'Accuracy':scores_acc,'Recall':scores_recall})\n    \ncols = list(dataframe)\n# move the column to head of list using index, pop and insert\ncols.insert(0, cols.pop(cols.index('Models')))\ndataframe = dataframe.ix[:, cols]\ndataframe"},{"cell_type":"markdown","metadata":{"_cell_guid":"dc678983-ca72-a5d9-759c-2c4aeb0dd41d"},"source":"Now lets test the models on test data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e0f70439-fadf-847f-0660-6d8a6e66f20e"},"outputs":[],"source":"#Transforming X_test to PCA components\nX_test_pca = pca.transform(X_test)\n\n\nscores_precision=[]\nscores_acc=[]\nscores_recall=[]\nscores_f1score=[]\nnames=[]\nfor name,model in model_optimal.items():\n    model.fit(X_train_pca,Y_train)\n    names.append(name)\n    precision=metrics.precision_score(Y_test,model.predict(X_test_pca))\n    scores_precision.append(precision)\n    acc=metrics.accuracy_score(Y_test,model.predict(X_test_pca))\n    scores_acc.append(acc)\n    recall=metrics.recall_score(Y_test,model.predict(X_test_pca))\n    scores_recall.append(recall)\n    f1score=metrics.f1_score(Y_test,model.predict(X_test_pca))\n    scores_f1score.append(f1score)\n    dataframe = pd.DataFrame({'Models':names,'Precision':scores_precision,'F1-score':scores_f1score,\n                              'Accuracy':scores_acc,'Recall':scores_recall})\n    \ncols = list(dataframe)\n# move the column to head of list using index, pop and insert\ncols.insert(0, cols.pop(cols.index('Models')))\ndataframe = dataframe.ix[:, cols]\ndataframe\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"e2f5d951-33c5-2000-c73a-1e690e6884e7"},"source":"Thus it can be concluded that best models for classification are : Support Vector Machines, Random Forest and xgBoost for when we have applied PCA. But training time wise , it can be said that Decision tree is the fastest and SVM takes the most time for training. But overall if training time is not a constraint then SVM could be said to be the best model for classification for this type of problem. SVM has been consistently giving outstanding performance in this problem. It has perfect results in all areas in all types of tesing (validation , testing). Thus it could be said to be the best model If training time is a constraint then Random Forest should be the next best classifier.\n\n\nCode Reference :\nhttps://www.kaggle.com/nirajvermafcb/comparing-various-ml-models-roc-curve-comparison\nhttps://www.kaggle.com/monkeydunkey/a-comparison-of-few-ml-models"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eb16891b-ce77-e420-b75d-a37c05ddcb76"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}