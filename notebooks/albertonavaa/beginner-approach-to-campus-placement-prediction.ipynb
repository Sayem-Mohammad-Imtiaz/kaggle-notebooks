{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Beginner approach to campus palcement prediction","metadata":{}},{"cell_type":"code","source":"#Import all relevant libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read the data and show a sample\nurl = '../input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv'\ndf = pd.read_csv(url, index_col = 'sl_no')\ndict = {'Placed': 1, 'Not Placed': 0}\n#Here I map the status feature to 0s and 1s\ndf['status'] = df['status'].map(dict)\ndf.head(10)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check missing values in df\ndf.isnull().any()\n#We see that missing values are not present except in salary which I drop later, so there is no need to replace NaNs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basic EDA\nHere we see that gender, ssc_p, hsc_p, degree_p, workex, specialization and mba_p are the more relevant features","metadata":{}},{"cell_type":"code","source":"sns.countplot(x='status' , hue = 'gender' , data =df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='status' , hue = 'degree_t' , data =df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='status' , hue = 'specialisation' , data =df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='status' , hue = 'workex' , data =df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='status' , hue = 'hsc_b' , data =df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='status' , hue = 'ssc_b' , data =df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='status' , hue = 'hsc_s' , data =df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x=df.mba_p, y=df.status)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x=df.etest_p, y=df.status)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x=df.degree_p, y=df.status)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x=df.hsc_p, y=df.status)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x=df.ssc_p, y=df.status)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.status.value_counts()\n#We can see that the data is not balanced, we can attempt to balance the data but I didn't see any significant improvement in the predictions when doing that","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"#Split the data in X and y, I'm dropping the salary feature since we are trying to predict the status so it doesn't make sense to consider the salary as a predictor.\ny = df.status\nX = df.drop(['status', 'salary'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Selecting categorical colums\ncat_cols = [cname for cname in X_train.columns if X_train[cname].dtype == \"object\"]\n\n# Select numerical columns\nnum_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[cat_cols]), index = X_train.index)\nOH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[cat_cols]), index = X_test.index)\n\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([X_train[num_cols], OH_cols_train], axis=1)\nOH_X_test = pd.concat([X_test[num_cols], OH_cols_test], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#scaling\nfrom sklearn.preprocessing import scale\nscaled_X_train = pd.DataFrame(scale(OH_X_train[num_cols]), index=OH_X_train.index)\nscaled_X_test = pd.DataFrame(scale(OH_X_test[num_cols]), index=OH_X_test.index)\n\nscaled_X_train.columns = OH_X_train[num_cols].columns\nscaled_X_test.columns = OH_X_test[num_cols].columns\n\nOH_X_train[num_cols] = scaled_X_train[num_cols]\nOH_X_test[num_cols] = scaled_X_test[num_cols]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OH_X_train.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 different models for predicting placement status","metadata":{}},{"cell_type":"code","source":"#KNN neighbors classifier model\nknn_model = KNeighborsClassifier(n_neighbors=10)\nknn_model.fit(OH_X_train, y_train)\npreds = knn_model.predict(OH_X_test)\nprint(accuracy_score(y_test, preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Random forestclassifier model\nrf_model = RandomForestClassifier(n_estimators = 100, random_state = 0)\nrf_model.fit(OH_X_train, y_train)\npreds = rf_model.predict(OH_X_test)\nprint(accuracy_score(y_test, preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#logistic regression model\nlog_model = LogisticRegression(max_iter=100)\nlog_model.fit(OH_X_train, y_train)\npreds = log_model.predict(OH_X_test)\nprint(accuracy_score(y_test, preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model that showed the best performance was logistic regression.","metadata":{}},{"cell_type":"markdown","source":"# Thanks for checking my notebook out, I'm still a beginner so any comment or suggestion is welcome, thanks!","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}