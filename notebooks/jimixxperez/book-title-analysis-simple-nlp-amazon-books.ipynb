{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }</style>\"))\n\nimport holoviews as hv\nfrom holoviews import opts\nhv.extension('bokeh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fname = '/kaggle/input/amazon-top-50-bestselling-books-2009-2019/bestsellers with categories.csv'\ndf = pd.read_csv(fname)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# User Review (90 percentile-distribution over years) - just out of interest\n\n- time series seems (visually) quiet stationary - further tests required"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax  = plt.subplots(1,1,figsize=(10,8))\n\nuser_rating_year = (\n    df.groupby('Year')\n        .agg({\n            'User Rating': [\n                lambda x: x.quantile(0.05),\n                lambda x: x.quantile(0.95),\n                'median',\n                'mean',\n            ]\n        })\n        .rename(columns={\n            '<lambda_0>': 'min',\n            '<lambda_1>': 'max',\n        })\n)\nax.set_title('User Rating vs year')\nax.fill_between(user_rating_year.index, user_rating_year['User Rating', 'min'], user_rating_year['User Rating', 'max'], label='90%')\nsns.lineplot(x=user_rating_year.index, y=user_rating_year['User Rating','median'], ax=ax, color='r', label='median')\nsns.lineplot(x=user_rating_year.index, y=user_rating_year['User Rating','mean'], ax=ax, color='y', label='mean')\nplt.legend()\nax.set_xlim((2009, 2019))\nax.set_ylim((0, 5))\nax.set_ylabel('User Rating')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Book title analysis - unsupervised attempts via rudimentary nlp\n\n### is it all possible - without any further info, such as e.g. book summaries - to extract insights/pattern from the top50 books (disregarding the time component)?"},{"metadata":{},"cell_type":"markdown","source":"# 1. tfidf"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.corpora.dictionary import Dictionary\nfrom gensim.models import LdaModel, TfidfModel\nfrom gensim import similarities\nfrom gensim.parsing.preprocessing import (\n    remove_stopwords,\n    preprocess_string,\n    strip_punctuation,\n    strip_numeric,\n    strip_non_alphanum,\n    strip_short,\n)\n\ncustom_filters = (\n    lambda s: s.lower(),\n    strip_numeric,\n    remove_stopwords,\n    strip_punctuation,\n    lambda s: strip_short(s, minsize=3),\n)\ndf['pre_name'] = df.Name.apply(\n    lambda n: preprocess_string(n, custom_filters)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dct = Dictionary(df.pre_name)\ncorpus = [dct.doc2bow(n) for n in df.pre_name]\ntfidf = TfidfModel(corpus)\nindex = similarities.MatrixSimilarity(tfidf[corpus])\n#str_check = (\n#    df.Name\n#    .apply(strip_punctuation)\n#    .apply(strip_non_alphanum)\n#    .str\n#    .replace(' ', '')\n#    .str\n#)\ndf['name_isalpha'] = str_check.isalpha().astype(int)\ndf['name_isalphanumeric'] = str_check.isalnum().astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import umap\ndata = umap.UMAP().fit_transform(\n    index[tfidf[corpus]]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pts = hv.Points(data)\nlabels = hv.Labels({\n    ('x', 'y'): data,\n    'text': df.Name,\n}, ['x', 'y'], 'text')\noverlay = (pts * labels)\noverlay.opts(\n    opts.Labels(text_font_size='8pt', xoffset=0.08),\n)\noverlay.options(\n    title='Dim. reduced similarity matrix based on tf-idf',\n    width=1000,\n    height=800\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### feel free to explore the plot:\n#### it is interesting to see that such simple approach based solely term and inverse document frequency (without any futher magic such as word2vec) with minimal preprocessing can already give some clues about cluster:\n#### - it graps the idea that for instance \"Laugh-Out-Loud Jokes for Kids\" and \"Knock-Knock Jokes for Kids\" are very similar and significantly different from other titles\n#### - multiple cookbooks and diets are often clustered together, same goes with coloring books (\"Unicorn coloring book\" and \"Adult Coloring Books\") - did not know they were this popular\n #### - otherwise kind of mixed bag, requires further processing, e.g. books that contain suffixes such \": A Memoir\" or \": A Novel\" form cluster and should have removed duplicate titles\n"},{"metadata":{},"cell_type":"markdown","source":"# word2vec"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim.downloader\nprint(list(gensim.downloader.info()['models'].keys()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fasttext_vectors = gensim.downloader.load('fasttext-wiki-news-subwords-300')#","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# compute word distance based distance map (earth mover distance/wasserstein distance)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#N = df.shape[0]\n#dmap = np.zeros((N,N))\n#for i in range(N):\n#    for j in range(N):\n#        dmap[i,j] = fasttext_vectors.wmdistance(\n#            df.pre_name.iloc[i], \n#            df.pre_name.iloc[j],\n#        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dmap[dmap == np.inf] = np.median(dmap) # impute inf val","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# visualize - MDS would have probably been more suitable for dmap"},{"metadata":{"trusted":true},"cell_type":"code","source":"#data = umap.UMAP().fit_transform(dmap)\n#pts = hv.Points(data)\n#labels = hv.Labels({\n#    ('x', 'y'): data,\n#    'text': df.Name,\n#}, ['x', 'y'], 'text')\n#overlay = (pts * labels)\n#overlay.opts(\n#    opts.Labels(text_font_size='8pt'),\n#)\n#overlay.options(\n#    width=1000,\n#    height=800\n#)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}