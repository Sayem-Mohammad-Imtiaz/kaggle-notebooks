{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"14e40f6d-5b07-37ab-5534-118033dedcbe"},"source":"This tries to estimate hard drive failure probability, from the data on last day. Set up one of many prediction problems, clean the data a bit, notice some areas for dataset improvement and likely *introduce a leak* into my prediction task(some eda to find it would be a good Kernel for someone to write). There are a few data processing improvements for this Dataset, to make it more usable.\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"be6d9fc3-c24b-b312-6842-c53bd0a1fc59"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import ensemble, metrics \nimport gc"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c72155e1-b64a-3bc8-1dc2-7cc345e6c157"},"outputs":[],"source":"hdd = pd.read_csv('../input/harddrive.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9d2ebc5f-019a-0a65-b859-d14c5c648aa6"},"outputs":[],"source":"hdd.head()\nhdd.serial_number.unique().shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5f4b2c16-ba4d-578e-0d4e-c95edcaf59b7"},"outputs":[],"source":"np.int64(hdd['capacity_bytes'].at[5])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0b6c9e70-e59f-bd8b-5167-8bc1f86435d2"},"outputs":[],"source":"print(hdd['capacity_bytes'].at[5])\nprint(hdd['capacity_bytes'].at[5] * 10**10)"},{"cell_type":"markdown","metadata":{"_cell_guid":"e8906aff-9389-16ce-130a-6fd5536d15e8"},"source":"**note** capacity is in bytes has been loaded as a very small number(very close to zero)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9579fdd2-8543-23b3-78c9-bbc89fe4c7e3"},"outputs":[],"source":"hdd.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6a08f614-3b12-329d-b993-7d5feb955307"},"outputs":[],"source":"# number of hdd\nhdd['serial_number'].value_counts().shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"49d1f7b7-a704-e0fb-32e1-03843cee8816"},"outputs":[],"source":"# drop constant columns\nhdd = hdd.loc[:, ~hdd.isnull().all()]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dbdeb85f-a836-c888-1a05-bf2371afb245"},"outputs":[],"source":"# number of different types of harddrives\nhdd['model'].value_counts().shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa3c2f91-6b44-2375-9b9b-1c9d93a60632"},"outputs":[],"source":"print(hdd.groupby('model')['failure'].sum().sort_values(ascending=False).iloc[:30])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"857c6390-f8e4-902a-ced0-fad2a47c822d"},"outputs":[],"source":"# ST4000DM000 refers to Seagate SATA 6Gb/s 3.5-Inch 4TB Desktop HDD \n# Since it makes up a majority of the data, and the features should be the same for \n# drives of the same model, we will start by predicting failure in these drives. \n# combining them with other similar seagate drives might be an easy way to use more of the data\n\nhdd_st4 = hdd.query('model == \"ST4000DM000\"')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b3de2ea7-1e6f-11aa-22eb-9c9ee95c8005"},"outputs":[],"source":"del hdd\ngc.collect()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e476f477-41ed-0b0c-3809-0e4efb0d1bc1"},"outputs":[],"source":"# number of drives in the reduced data\nhdd_st4['serial_number'].value_counts().shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bd01b3f0-ccec-2ea9-768c-21ea9969c47a"},"outputs":[],"source":"# out of the 35k drives there are 131 failures, so this is definitly an imbalanced dataset. \n# note the output says 139 1 labeled but this is incorrect as 8 are duplicates. I drop them later\n# because dropping at the begginning crashed the Kernel\nhdd_st4['failure'].value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a4f4329b-865f-4592-edc8-212471f24e09"},"outputs":[],"source":"# more constant columns\nhdd_st4['capacity_bytes'].value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7570b537-4d2e-27ba-1994-4267ab7d0f80"},"outputs":[],"source":"# drop them \nhdd_st4 = hdd_st4.loc[:, ~hdd_st4.isnull().all()]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"43108e38-ceb9-bbd8-2662-1f99144e869c"},"outputs":[],"source":"hdd_st4.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"afed36ed-6ce9-89b9-52f3-62c363d9d2ba"},"outputs":[],"source":"# these have similar exponents as the size of harddrive. \n# I am also pretty sure these variables are something like total read or total write. \n# The scientific format is interprating the exponents as negative when they should likely be positive\n# A fractional byte does not make sense. \nhdd_st4.iloc[:5,13:15] "},{"cell_type":"markdown","metadata":{"_cell_guid":"4c14fad9-7776-e142-3996-a00fdb9f16bc"},"source":"The normalized values are not correcting for the format issue. That is 1.2*10^317, should be larger then 1.01*10^315\nAnd as can be seen above it is not. **NOTE** I only process four columns that have very small values where there should be large values. The others should be found and corrected(though reversing order is probably not horrible and training on these would probably work but probably better to have them corrected) "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"74439c70-d882-dd83-3610-be08acc7e340"},"outputs":[],"source":"# removed normalized values, and model, and capacity, since they are constants\nhdd_st4 = hdd_st4.select(lambda x: x[-10:] != 'normalized', axis=1)\nhdd_st4 = hdd_st4.drop(['model', 'capacity_bytes'], axis=1)\ngc.collect()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2f4966c3-8a85-2aae-f605-6523b9fb6249"},"outputs":[],"source":"# no null values left. \n# thanks to the scripts by Alan Pryor(https://www.kaggle.com/apryor6) foor this and other nice ways of doing things \nhdd_st4.isnull().any()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fd3d1309-8fd1-bf64-8a43-bffcda00f1fc"},"outputs":[],"source":"# there are more constant columms but skipping this for run time reasons\n# hdd_st4['smart_3_raw'].value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"30d2f3d8-a51c-e39f-e441-5dc2e5540536"},"outputs":[],"source":"# remove more constant columns(anyone have a fast one liner for this?)\n# for i in hdd_st4.columns:\n#    if len(hdd_st4.loc[:,i].unique()) == 1:\n#        hdd_st4.drop(i, axis=1, inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a6f7174b-94f8-05e7-ae03-debc5969d71e"},"outputs":[],"source":"# hdd_st4.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"32e71f56-b6e2-33b1-e33d-73008b96b8ea"},"outputs":[],"source":"# now to deal with the issue of really small byte numbers. If there is an easy way to do this please\n# feel free to point it out, after a quick search turned up nothing this was the first solution that \n# I came up with. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db164935-083d-026b-cb69-6f7b74732597"},"outputs":[],"source":"# turns number into a string, then extracts, base and exponent\ndef convert_large_number(large_num, min_exponent):\n    str_num = str(large_num)\n    \n    base_end = str_num.find('e')\n    base  = np.float64((str_num[:base_end]))\n    \n    # if i remember correctly this is equivelent to dividing by a constant\n    exponent = np.int64(str_num[base_end+2:]) - (min_exponent-1)\n    return base*10**exponent\n\n# just fetches the exponent\ndef get_exp(large_num):\n    str_num = str(large_num)\n    base_end = str_num.find('e')\n    \n    exponent = np.int64(str_num[base_end+2:])\n        \n    return exponent\n\n# finds the minimum exponenet for a series\ndef min_exp(series_of_large_num):\n    exps = series_of_large_num.apply(get_exp)\n    return exps.min()\n\n# scales a series down but subtracting the min observed exponent from exponent.  \ndef scale_large_num_col(series, min_exponent):\n    return series.apply(convert_large_number, min_exponent=min_exponent)\n    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"64edbe5e-945b-97df-16e5-f23b0aa79a8a"},"outputs":[],"source":"# smart_241_raw contains a single 0 which messes up my method of conversion\ns241_mean = hdd_st4['smart_241_raw'].mean()\nhdd_st4['smart_241_raw'].replace(0.0, s241_mean, inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9af3c9d8-1da8-12f3-a69d-6d4473bc1d34"},"outputs":[],"source":"# just find the min exponent of the whole of all the columns so I can adjust all the data together\n# or you can just look at the columns\n# mins = []\n# for i in range(3, len(hdd_st4.columns)):\n#     if hdd_st4.iloc[0,i] < 10**-10 and hdd_st4.iloc[0,i] > 0:\n#             mins.append(min_exp(hdd_st4.iloc[:,i]))\n\n# this takes a little to long to run\n\n# minimum exponent for these is 309\n        "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9d3e1718-77e1-db7d-f842-509b6422397b"},"outputs":[],"source":"# transform data so it is a more managable size\n# alternativly they could be stored as full length integers\nfor i in range(3, len(hdd_st4.columns)):\n    if hdd_st4.iloc[0,i] < 10**-10 and hdd_st4.iloc[0,i] > 0:\n       hdd_st4.iloc[:,i] =  scale_large_num_col(hdd_st4.iloc[:,i], 308)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3bde4433-0aea-08c8-9cea-0c9fd6102e9a"},"outputs":[],"source":"gc.collect()\nhdd_st4.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0887bc9f-9cb5-0413-1b47-645b5865c5a1"},"outputs":[],"source":"# Since we are trying to predict drive failure, we randomly select a set of drives. \n# note that if there is some relationship between the drives, say a large group are in the same building. Then failure \n# between drives won't be indepentent\n\nhdd_st4.loc[:, 'date'] = pd.to_datetime(hdd_st4.loc[:,'date'])\nhdd_st4['day_of_year'] = hdd_st4['date'].dt.dayofyear\n\nhdd_st4.plot(kind='scatter', x='day_of_year', y='failure', title='Hard drive failures over time')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"65deeb23-1a34-8575-b726-96bc2e51547c"},"source":"looks like there is a gap in the data, and the failure distrubtion looks roughly uniform\nbut this should be more carefully checked."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1c07e419-006c-be6e-1fe0-0940116a3586"},"outputs":[],"source":"# note this could be done earlier but it doesn't work on Kernels because of memory limitations\nhdd_st4 = hdd_st4.drop_duplicates()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d0720268-ce5f-73e3-b04b-34e27c8ebfa3"},"outputs":[],"source":"# lets try to predict the probability of failure from data only on the day of failure\n# it would be good to see how this probability relates to the probability of failure using previous days data only\nhdd_group = hdd_st4.groupby('serial_number')\nhdd_last_day = hdd_group.nth(-1) # take the last row from each group\ndel hdd_st4\ngc.collect()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"26603b64-f739-4040-a73f-1d7d8fed74b2"},"outputs":[],"source":"# the number of drives in the dataset\nuniq_serial_num = pd.Series(hdd_last_day.index.unique())\nuniq_serial_num.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9f1b4c92-ced2-bc29-0beb-ed315deda6bc"},"outputs":[],"source":"# hold out 25% of data for testing\ntest_ids = uniq_serial_num.sample(frac=0.25)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"69e684ff-eea5-3571-f22b-19abce9f4906"},"outputs":[],"source":"train = hdd_last_day.query('index not in @test_ids')\ntest = hdd_last_day.query('index in @test_ids')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a8b913af-3c60-3c74-693b-f2e8aa9dab3c"},"outputs":[],"source":"test['failure'].value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9c06842a-7800-f4ab-873e-135bfc19b822"},"outputs":[],"source":"train['failure'].value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"48c40274-db3b-82f5-2219-bc04bb75e405"},"outputs":[],"source":"# close enough to stratified sampling for me. \n131/4"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8e9b4e32-f862-3f78-3627-7c798357e2ae"},"outputs":[],"source":"train_labels = train['failure']\ntest_labels = test['failure']\ntrain = train.drop('failure', axis=1)\ntest = test.drop('failure', axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1fc69401-5525-144e-6c87-0595bfa6b659"},"outputs":[],"source":"train['day_of_year'].value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0fa21938-bf7e-7a7e-7f37-6f7327ec49eb"},"outputs":[],"source":"# the last day has most of the data without failures. This makes sense because I chose\n# to use the last day as a feature and most drives are still working on last day.\n# looks like I wasn't careful enough about possible leaks. \n# in this dataset date, and by extension number of samples will be a leak, as those \n# harddrives which failed in the dataset will likely have less days available. \nprint(train_labels.reindex(train.query('day_of_year == 120').index).shape[0],\n      train_labels.reindex(train.query('day_of_year == 120').index).sum())\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"94d2cfb7-6947-9e36-a6f7-a8c8c5800132"},"outputs":[],"source":"# \nprint(train_labels.reindex(train.query('day_of_year != 120').index).shape[0],\n      train_labels.reindex(train.query('day_of_year != 120').index).sum())\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2d53f18-304c-7c03-05d6-b8a3c0116f02"},"outputs":[],"source":"#drop date related features maybe this will prevent leakage;)\ntrain = train.drop(['day_of_year', 'date'], axis=1)\ntest = test.drop(['day_of_year', 'date'] , axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"272c10f3-2e36-6dc6-830b-044de04bd6a5"},"outputs":[],"source":"# remove more constant columns(anyone have a fast one liner for this?)\n# could have done this earlier\nfor i in train.columns:\n    if len(train.loc[:,i].unique()) == 1:\n        train.drop(i, axis=1, inplace=True)\n        test.drop(i, axis=1, inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3b654042-c675-2246-4ab7-e46bf91fbe22"},"outputs":[],"source":"train.head().columns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7655a7fb-8a11-ed01-8389-c6a8ab5ebf02"},"outputs":[],"source":"rf = ensemble.RandomForestClassifier()\nrf.fit(train, train_labels)\npreds = rf.predict_proba(test)\n\nprint('logloss', metrics.log_loss(y_true=test_labels, y_pred=preds[:,1]))\nprint('roc_auc', metrics.roc_auc_score(y_true=test_labels, y_score=preds[:,1]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"baed71f0-35a8-2c88-9075-d595e8fe11b3"},"source":"This looks good but I am immediately skeptical about this result given that the last day has almost all the negative examples. There may be a another variable which is a proxy for time. It would be good to **prove there is a leak** or exhaust as many possibilities as possible. This is where eda is very useful. I have gone over the amount of time I wanted to spend on this Kernel so some one else will need to follow up on data quality stuff.  \n\nA better next step might be to use the first day instead of the last day(if i have time I will try this otherwise someone else give it a try) "}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}