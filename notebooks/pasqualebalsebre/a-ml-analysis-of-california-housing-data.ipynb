{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nimport math\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom scipy.cluster import hierarchy as hc\n\nimport os, os.path\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import learning_curve\nfrom sklearn import linear_model\nfrom sklearn import svm\nfrom sklearn.svm import SVR\nfrom sklearn import metrics\nfrom sklearn import neighbors\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import r2_score\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npRand = 5\nplt.style.use('seaborn-whitegrid')\nplt.style.use('seaborn-deep')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Let's read the data and introduce it\n\npath = \"/kaggle/input/housing/housing.csv\"\ndata = pd.read_csv(path)\n\nprint('\\nCalifornia housing Dataset is composed of %d rows (samples) ' %data.shape[0] + 'and %d columns (features):\\n' %data.shape[1])\nprint('1. Longitude of the position of the neighborhood' + (' NULLABLE]' if data.isnull().longitude.any() else ''))\nprint('2. Latitude of the position of the neighborhood' + (' [NULLABLE]' if data.isnull().latitude.any() else ''))\nprint('3. Median age of the houses' + (' [NULLABLE]' if data.isnull().housing_median_age.any() else ''))\nprint('4. Total rooms in the district' + (' [NULLABLE]' if data.isnull().total_rooms.any() else ''))\nprint('5. Total bedrooms in the district' + (' [NULLABLE]' if data.isnull().total_bedrooms.any() else ''))\nprint('6. People who live in the district' + (' [NULLABLE]' if data.isnull().population.any() else ''))\nprint('7. Families in the district' + (' [NULLABLE]' if data.isnull().households.any() else ''))\nprint('8. Median income of people in the district' + (' [NULLABLE]' if data.isnull().median_income.any() else ''))\nprint('9. Median house value' + (' [NULLABLE]' if data.isnull().median_house_value.any() else ''))\nprint('10. Ocean proximity' + (' [NULLABLE]' if data.isnull().ocean_proximity.any() else ''))\nprint ('\\n')\nprint('Notes:')\nprint('The median income is scaled in the range [0.4999, 15.0001]')\nprint('The median house value is the one I am willing to predict')\nprint('The ocean proximity is the only non-numerical feature, we\\'ll see it in deatil later')\nprint ('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nFirst 5 samples of the dataset:')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Ocean_proximity feature in detail\n\nprint('\\nHere we have the different values that \\'ocean proximity\\' feature can assume,\\nalong with their occurrencies:\\n')\nprint(data['ocean_proximity'].value_counts())\nprint('\\n')\n_ = sns.countplot(data.ocean_proximity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical data handling\n\nprint('\\nI dropped the rows which \\'ocean_proximity\\' value is \\'ISLAND\\', because they are too few!\\n')\ndata_noisland = data[data.ocean_proximity != 'ISLAND']\n\nprint('To manage categorical data, two general approaches are possible:')\nprint('1. LabelEncoder + OneHotEncoder')\nprint('2. Pandas getDummies')\nprint('\\n')\nprint('Label encoder alone is not a good choice as long as the categorical values are more than 2.\\nIn this case in fact the model would consider a value \"greater\" than the other,\\nthus leading to poor performance\\n')\nprint('I chose pandas get dummies because it\\'s easier to understand and outputs a dataframe,\\nso it requires little to no effort\\n')\n\ndummies = pd.get_dummies(data_noisland.ocean_proximity)\ndata_noisland[dummies.columns] = dummies\n\nprint('I deleted the column \\'ocean_proximity\\' and put the label we are willing to predict as last\\n')\npr_data = data_noisland[['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n       'total_bedrooms', 'population', 'households', 'median_income'\n       , '<1H OCEAN', 'INLAND', 'NEAR BAY', 'NEAR OCEAN','median_house_value']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling nullable values\n\nprint('\\nAs seen before, the \\'total_bedrooms\\' feature assumes NULL value sometimes,\\nlet\\'s see it more in detail\\n')\nprint(pr_data.isna().sum())\n\nprint('\\n')\n\nprint('%d rows are too many to be dropped, so I will fill them with the mean value\\n' %pr_data.total_bedrooms.isna().sum())\nproc_data = pr_data.fillna(pr_data.mean())\n\nprint('Number of null values after \\'fillna\\': %d\\n' %proc_data.total_bedrooms.isna().sum())\n\nprint('\\n')\n\nprint('Let\\'s have a look on the data after these modifications\\n')\nproc_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation between features\nprint('\\nFirst of all I am going to show general correlations between features\\n')\nprint('Let\\'s say that a positive correlation starts from 0.5 to 0.7 and a strong/perfect correlation\\napproaches 0.9/1.0, same applies for negative correlation with negative values\\n')\nplt.figure(figsize=(12,10))\nplt.title('Correlation between features', fontsize='18').set_position([.5, 1.05])\n_ = sns.heatmap(proc_data.corr(), annot=True)\n_ = plt.xticks(rotation=-50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nAs shown in the Heatmap there is a strong correlation between the following features:\\n')\nprint('- households')\nprint('- total_bedrooms')\nprint('- total_rooms')\nprint('- population')\n\nprint('\\n')\n\nprint('This makes sense to me, the number of bedrooms in a district is obviously correlated\\nwith the number of rooms in the district, the same is true for the number of families\\nand the total population living in a district, finally number  of rooms is correlated\\nwith the people\\n')\n\nprint('In the following dendrogram is spotted also the obvious correlation\\nbetween longitude and the nearness to the ocean\\n')\nprint('Reader attention will finally be drawn by the relationship between\\nmedian house value and median income, which is high enough and we are going to inspect\\n')\ncorr = 1 - proc_data.corr() \n\ncorr_condensed = hc.distance.squareform(corr)\nz = hc.linkage(corr_condensed, method='average')\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\n_ = hc.dendrogram(z, ax = ax, labels=corr.columns, orientation=\"right\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nHere are listed features correlation with median_house_value (sorted by correlation)\\n')\nproc_data.corr().sort_values(ascending=False, by = 'median_house_value').median_house_value","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Correlation between median_house_value and median_income\n\nprint('\\nEvidence shows a correlation between the target feature and the \\'median_income\\'');\nprint('In the next graphs we see more in detail the relationship between these two features\\n')\n\nfig = plt.figure(figsize=(15, 10))\n\ngs = gridspec.GridSpec(2, 2, figure=fig)\n\nax = plt.subplot(gs[0, 0]) \nax.set_title('Median house value distribution').set_position([.5, 1.04])\nax.set(xlabel='Median house value', ylabel='Count')\n_ = ax.hist(proc_data.median_house_value,bins=200)\n\nax = plt.subplot(gs[0, 1]) \nax.set_title('Median house value per median income').set_position([.5, 1.04])\nax.set(xlabel='Median income', ylabel='Median house value')\n_ = ax.plot(proc_data.median_income, proc_data.median_house_value, 'ro', ms=1)\n\nax = plt.subplot(gs[1, :]) \nax = sns.distplot(proc_data.median_house_value) #distribution of values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features/Target splitting of the dataset\n\ndata_x = proc_data.drop(['median_house_value'], axis=1)\ndata_y = proc_data['median_house_value']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Principal component analysis\n\nprint('\\nPrincipal Component Analysis (PCA) aims to convert a set of possibly correlated features\\ninto linearly uncorrelated values, called principal components.', \\\n     'This transformation is defined\\nsuch that the first component has highest variance, and each succeding component has\\nhighest variance ', \\\n     'possible, under the constraint that is orthogonal to the preceding ones\\n')\n\nprint('\\nThree main approaches are possible to compute the Principal Components:\\n');\nprint('-Algorithm 1 (Sequential): Select the component that maximizes the\\naverage sum of squares of that component.')\nprint(' For each successive component, the PCA reconstructions relative to previous PCs\\nare subtracted\\n')\nprint('-Algorithm 2 (Sample Covariance Matrix): Computes the covariance matrix as follow:\\n(1/m)*Σ(xi - x̄)(xi - x̄)T')\nprint(' The eigenvectors of the matrix are the PCA basis vectors,\\nthe larger the eigenvalue the more important the PC\\n')\nprint('-Algorithm 3 (Singular Value Decomposition of centered data matrix X)\\n')\n\n# Seeking for a relationship between data, better to work on scaled data\n# Standard Scaler returns a numpy array\n\nscaled_data_x = (data_x - data_x.mean()) / data_x.std()\npca = PCA(random_state=pRand, n_components=12)\nx_pca = pca.fit_transform(scaled_data_x)\n\ntotalvar = 0\nfor var in pca.explained_variance_:\n    totalvar += var\n\n# perc_var is the variance (%) for each feature\nperc_var = pca.explained_variance_/totalvar*100\n\n\n# here i build the cumulative var\n\ncum_var = []\n\nfor i in range(0,len(perc_var)):\n    cum_var.append(perc_var[i])\n\nfor i in range(0,len(perc_var)):\n    for j in range(0,i):\n        cum_var[i] = cum_var[i] + perc_var[j]\n    \nprint('\\nIn the following graphs are exposed the Individual Explained Variance and\\nthe Cumulative Explained Variance for the  features, to make easier the job of choosing\\nthe number of Principal Components we want to keep\\n')\n\nfig, axs = plt.subplots(1, 2, figsize=(15,5))\naxs[0].set_xticks(range(1,len(perc_var)+1))\naxs[1].set_xticks(range(1,len(cum_var)+1))\n#axs[0].set_ylim(ymin=-5,ymax=105)\naxs[1].set_ylim(ymin=-5,ymax=105)\naxs[0].set_title('Individual Explained Variance').set_position([.5, 1.04])\naxs[0].set(xlabel='', ylabel='Percentage')\naxs[1].set_title('Cumulative Explained Variance').set_position([.5, 1.04])\naxs[1].set(xlabel='', ylabel='Percentage')\nplt.subplots_adjust(wspace = 0.3)\n\n_ = axs[0].plot(range(1,len(perc_var)+1),perc_var,linewidth=4)\n_ = axs[0].plot([7,7],[0,35],'--',linewidth=2)\n_ = axs[1].plot(range(1,len(cum_var)+1),cum_var,linewidth=4)\n_ = axs[1].plot([1,12],[97.9,97.9],'--',linewidth=2)\n\nprint('5 Components: %d%% of variance\\n'%cum_var[4])\nprint('6 Components: %d%% of variance\\n'%cum_var[5])\nprint('7 Components: %d%% of variance\\n'%cum_var[6]) # %f mostra 97.89 ~ 97.9\nprint('8 Components: %d%% of variance\\n'%cum_var[7])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\n7 components are enough to explain 97% of the total variance\\n')\nn_components = 7\n\npca = PCA(n_components)\n\npcs = pd.DataFrame(pca.fit_transform(scaled_data_x)) #principal components\n\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Comparison between PCs and entire DataSet\n\n#Split datasets with proportion 4:1 for train and test\n\nx_train, x_test, y_train, y_test = train_test_split(scaled_data_x, data_y, test_size=0.2, random_state=pRand)\nx_train_pca, x_test_pca, y_train_pca, y_test_pca = train_test_split(pcs, data_y, test_size=0.2, random_state=pRand)\n\nprint('\\nIn order to compare the original dataset after pre-processing,\\nwith the Principal Components dataset, ', \\\n      'I am going to evaluate R² score\\nusing Cross Validation with K-Folds (K=10)\\n')\n\nprint('R² (R-Squared) indicates the proportion of variation in the outcome\\nthat is explained by the predictor variables\\n')\n\nprint('R² ≡ 1 - RSS/TSS\\nTSS (Total Sum of Squares) = Σ(yi-ȳ)²\\nRSS (Residual Sum of Squares) = Σ(yi-fi)²\\n')\n\n\ndef test_model(clf,train,labels):\n        \n        cv = KFold(n_splits=10,shuffle=True,random_state=pRand)\n        r2 = make_scorer(r2_score)\n        r2_val_score = cross_val_score(clf, train, labels, cv=cv,scoring=r2)\n        return [r2_val_score.mean()]\n\n\n\nprint('Starting with the original dataset:\\n')\n\nres = {}\n\nnow = time.time()\n\nres['Linear'] = test_model(linear_model.LinearRegression(),x_train,y_train)\nres['Ridge'] = test_model(linear_model.Ridge(),x_train,y_train) #slightly worse fit, for better long term predictions\nres['Lasso'] = test_model(linear_model.Lasso(alpha=1e-4),x_train,y_train)\nres['Decision Tree'] = test_model(DecisionTreeRegressor(),x_train,y_train)\nres['Random Forest'] = test_model(RandomForestRegressor(),x_train,y_train)\n\nt = time.time()-now\nprint('Time: %.3f s\\n' %t)\n\nresults = pd.DataFrame.from_dict(res,orient='index')\nresults.columns=[\"R Square Score\"]\nresults = results.sort_values(by=\"R Square Score\", ascending=False)\nfig, axs = plt.subplots(1, 2, figsize=(15,5), gridspec_kw={'width_ratios': [5, 2]})\n\nresults.plot(kind=\"bar\",title=\"Model Scores\",ax=axs[0])\naxs[0].set_ylim(ymin=0,ymax=1)\n\nplt.subplots_adjust(wspace = 0.4)\n\nbbox=[0, 0, 1, 1]\naxs[1].axis('off')\nmpl_table = axs[1].table(cellText = results.values, rowLabels = results.index, bbox=bbox, colLabels=results.columns)\nmpl_table.auto_set_font_size(False)\nmpl_table.set_fontsize(12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nProceeding with PCA-Reduced dataset\\n')\n\nres = {}\n\nnow = time.time()\n\nres['Linear'] = test_model(linear_model.LinearRegression(),x_train_pca,y_train_pca)\nres['Ridge'] = test_model(linear_model.Ridge(),x_train_pca,y_train_pca) #slightly worse fit, for better long term predictions\nres['Lasso'] = test_model(linear_model.Lasso(alpha=1e-4),x_train_pca,y_train_pca)\nres['Decision Tree'] = test_model(DecisionTreeRegressor(),x_train_pca,y_train_pca)\nres['Random Forest'] = test_model(RandomForestRegressor(),x_train_pca,y_train_pca)\n\nt = time.time()-now\nprint('Time: %.2f s\\n' %t)\n\nresults = pd.DataFrame.from_dict(res,orient='index')\nresults.columns=[\"R Square Score\"]\nresults = results.sort_values(by=\"R Square Score\", ascending=False)\nfig, axs = plt.subplots(1, 2, figsize=(15,5), gridspec_kw={'width_ratios': [5, 2]})\n\nresults.plot(kind=\"bar\",title=\"Model Scores\",ax=axs[0])\naxs[0].set_ylim(ymin=0,ymax=1)\n\nplt.subplots_adjust(wspace = 0.4)\n\nbbox=[0, 0, 1, 1]\naxs[1].axis('off')\nmpl_table = axs[1].table(cellText = results.values, rowLabels = results.index, bbox=bbox, colLabels=results.columns)\nmpl_table.auto_set_font_size(False)\nmpl_table.set_fontsize(12)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nAltough the PCA-Reduced dataset was faster than the original dataset,\\nthe loss in the R-Square score says it all. I am going to proceed using\\nthe original preprocessed dataset\\n')\n\nprint('Let\\'s start with the training phase and prediction of the target value\\nto check how does the model behave!\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = KFold(n_splits=5,shuffle=True,random_state=pRand)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#LINEAR REGRESSION\n\nprint('\\nLinear Regression\\n')\n\nprint('Linear Regression uses least squares to fit a line to the data')\nprint('It tries to minimize over w the following sum: Σ(yi - w*xi)² where \\'yi\\' is the target value,\\n\\'xi\\' is the features array', \\\n      'and w is the slope\\n')\nprint('It ends up with an equation looking like this: y = w0 + w1*x + ε\\n')\n\n\nnow = time.time()\nLRModel = LinearRegression().fit(x_train, y_train)\nt = time.time()-now\n\ny_pred = LRModel.predict(x_test)\n\nres = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred.flatten()})\n\nprint('Training time: %.2f ms\\n' %(t*1000))\nprint('Mean Absolute Error: %d\\n' %metrics.mean_absolute_error(y_test, y_pred))  \n#print('Mean Squared Error: %d' %metrics.mean_squared_error(y_test, y_pred))  \nLRScore = LRModel.score(x_train,y_train)\nprint('R²: %f' %LRScore)\nprint('\\n')\n#res.head(10)\ndf1 = res.head(25)\ndf1.plot(kind='bar',figsize=(15,6))\nplt.title('Comparison of Actual and Predicted values', fontsize=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"print('\\nPolynomial Regression\\n')\n\nprint('Polynomial Regression is very similar to Linear Regression\\nand is also known as Polynomial Linear Regression\\n')\nprint('The equation we are looking for is something like: y = b0 + b1*x + b2*x² + ... + bn*x^n\\n')\nprint('This Regression is still linear because even though the relationship between x and y\\nis not linear anymore,', \\\n     'we are talking about the coefficients and that\\'s what we are\\nlooking for, and that function can be expressed as a linear combination of that coefficients\\n')\n\nprint('Let\\'s now have a look on how this Regression works!\\n')\n\ndegree=4;\ntimes = {}\nnptimes = []\nrScores = {}\nnprScores = []\nMAEs = {}\nnpMAEs = []\n\n\nfor i in range(1,degree+1):\n    \n    transformer = PolynomialFeatures(degree=i, include_bias=False)\n    _ = transformer.fit(x_train)\n    \n    x_train_transformed = transformer.transform(x_train)\n    x_test_transformed = transformer.transform(x_test)\n    \n    now = time.time()\n    PRModel = LinearRegression().fit(x_train_transformed, y_train)\n    t = time.time() - now\n    times['Degree: %d' %i] = math.ceil(t*10000)/10000\n    nptimes.append(t)\n    \n    y_pred = PRModel.predict(x_test_transformed)\n    PRScore = PRModel.score(x_train_transformed,y_train)\n    rScores['Degree: %d' %i] = math.ceil(PRScore*100)/100\n    nprScores.append(PRScore)\n    npMAEs.append(metrics.mean_absolute_error(y_test, y_pred))\n    MAEs['Degree: %d' %i] = math.ceil(metrics.mean_absolute_error(y_test, y_pred))\n\n\nprint('\\nTraining times for different polynomial degrees\\n')    \n\n\nfig, axs = plt.subplots(1, 2, figsize=(15,5), gridspec_kw={'width_ratios': [5, 2]})\nplt.subplots_adjust(wspace = 0.4)\n\naxs[0].plot(range(1,degree+1),nptimes,linewidth=4)\naxs[0].set_xticks(range(1,degree+1))\naxs[0].set_ylim(ymin=0,ymax=int(np.amax(nptimes)+1))\n\nbbox=[0, 0, 1, 1]\naxs[1].axis('off')\ntimesdf = pd.DataFrame.from_dict(times,orient='index')\ntimesdf.columns=[\"Training time in seconds\"]\n\nmpl_table = axs[1].table(cellText = timesdf.values, rowLabels = timesdf.index, bbox=bbox, colLabels=timesdf.columns)\nmpl_table.auto_set_font_size(False)\nmpl_table.set_fontsize(12)\n\nplt.show()\n\n\nprint('\\nR-Squared for different polynomial degrees\\n')\n\n\nfig, axs = plt.subplots(1, 2, figsize=(15,5), gridspec_kw={'width_ratios': [5, 2]})\nplt.subplots_adjust(wspace = 0.4)\n\naxs[0].plot(range(1,degree+1),nprScores,linewidth=4)\naxs[0].set_xticks(range(1,degree+1))\naxs[0].set_ylim(ymin=0.5,ymax=1)\n\nbbox=[0, 0, 1, 1]\naxs[1].axis('off')\nrscoresdf = pd.DataFrame.from_dict(rScores,orient='index')\nrscoresdf.columns=[\"R-Squared\"]\n\nmpl_table = axs[1].table(cellText = rscoresdf.values, rowLabels = rscoresdf.index, bbox=bbox, colLabels=rscoresdf.columns)\nmpl_table.auto_set_font_size(False)\nmpl_table.set_fontsize(12)\n\nplt.show()\n\n\nprint('\\nMean Absolute Error for different polynomial degrees\\n')\n\n\nfig, axs = plt.subplots(1, 2, figsize=(15,5), gridspec_kw={'width_ratios': [5, 2]})\nplt.subplots_adjust(wspace = 0.4)\n\naxs[0].plot(range(1,degree+1),npMAEs,linewidth=4)\naxs[0].set_xticks(range(1,degree+1))\n#axs[0].set_ylim(ymin=0.5,ymax=1)\n\nbbox=[0, 0, 1, 1]\naxs[1].axis('off')\nMAEsdf = pd.DataFrame.from_dict(MAEs,orient='index')\nMAEsdf.columns=[\"Mean Absolute Error\"]\n\nmpl_table = axs[1].table(cellText = MAEsdf.values, rowLabels = MAEsdf.index, bbox=bbox, colLabels=MAEsdf.columns)\nmpl_table.auto_set_font_size(False)\nmpl_table.set_fontsize(12)\n\nplt.show()\n\nprint('\\nAs we can see despite R-Squared gets better with higher degree of the polynomial,\\nwe have a huge overfitting on training data', \\\n     'which causes poor results on test set.\\nIn this case, as the degree increases, the model stops describing the relationship\\nbetween variables,' \\\n     'and starts describing the random noise affecting data.\\n\\nThe polynomial degree which performed best on MAE was 2!\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nK-Nearest Neighbors\\n')\nprint('K-Nearest Neighbors is a typical classification algorithm which classifies a data point\\nbased on similarity (\"distance\") with', \\\n     'other points. It can be used in regression problems:\\ntarget value is predicted computing the average of the K nearest neighbors\\n')\n\nprint('Choosing a value for K can be tough. In fact, if K is too small, noise points may have\\na high influence on the result.', \\\n     'On the other hand, if K is too high, bias will increase much\\nas long as computational cost\\n')\nprint('To solve the problem of finding a value for K, I\\'m going to use the elbow approach:\\nI train and test the model with many values of K', \\\n     'and plot the MAE to find a minimum\\n')\nnpMAEs = []\n\nfor K in range(20):\n    K = K+1\n    model = neighbors.KNeighborsRegressor(n_neighbors = K)\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    npMAEs.append(metrics.mean_absolute_error(y_test,y_pred))\n    \n    \ncurve = pd.DataFrame(npMAEs)\nfig, ax = plt.subplots(1, 1, figsize=(8,5))\n_ = ax.plot(range(1,21),curve, linewidth = 4)\n_ = ax.set_xticks(range(1,21))\n_ = ax.set_title('Elbow curve', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find best K according to MAE\n\nbestK=0\nbestMAE=npMAEs[0]+1\n\n\nfor i in range(len(npMAEs)):\n    if(npMAEs[i]<bestMAE):\n        bestK=i+1\n        bestMAE=npMAEs[i]\n    \nprint('\\nBest number of Neighbors is: %d, '%bestK + 'with a Mean Absolute Error of: %d\\n' %bestMAE )\n\nmodel = neighbors.KNeighborsRegressor(n_neighbors = bestK)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nres = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred.flatten()})\ndf1 = res.head(25)\ndf1.plot(kind='bar',figsize=(15,6))\nplt.title('Comparison of Actual and Predicted values', fontsize=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nSupport Vector Regression\\n')\n\nprint('In general SVM when used for classification purposes, finds a hyperplane that\\nmaximizes the distance to the nearest', \\\n     'training data of any class.\\nThe main feature of this method is that it relies only on some data points,\\ninstead of all training data,'\\\n     'the so-called support vectors, that are the\\ndata points closest to the hyperplane. Analogously the SVR builds a regression model', \\\n     'that\\ndepends only on a subset of the training data, ignoring data close to the hyperplane\\n')\n\nprint('A very important parameter to set, for correct SVM/SVR functioning is C.\\nThis is the penalty parameter of the error term and defines how large', \\\n     'will be the margin.\\nWith a small value for C, we will have a larger margin that may perform better on the\\ntraining set,' \\\n     'but could end up performing poorly on test set. Since there is no rule\\nof thumb in choosing C, I am going to find best value for it using a grid search\\n')\n\nR2scores = []\n\nC_Range = np.logspace(-1, 5,num = 7)\n\nfor C in C_Range:\n    clf = SVR(C=C, epsilon=0.2)\n    _ = clf.fit(x_train,y_train)\n    R2scores.append(clf.score(x_train,y_train))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = 0;\nfor score in R2scores:\n    if (score==np.amax(R2scores)):\n        break\n    index += 1\n    \nprint('\\nBest value for C: %d\\n' %C_Range[index])    \n\nclf = SVR(C=C_Range[index], epsilon=0.2)\n_ = clf.fit(x_train,y_train)\ny_pred = clf.predict(x_test)\n\nprint('\\nR-Squared score for SVR with selected parameters: %f\\n' %np.amax(R2scores))\nprint('\\nMean Absolute Error: %d\\n' %metrics.mean_absolute_error(y_test,y_pred))\n\nres = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred.flatten()})\ndf1 = res.head(25)\ndf1.plot(kind='bar',figsize=(15,6))\nplt.title('Comparison of Actual and Predicted values', fontsize=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nDecision Tree Regression\\n')\n\nprint('Decision Trees can be used for regression scope. The tree looks for a best split\\nto use as a root node and predicts', \\\n     'a new data point as the average of data points\\nbelonging to the cluster of the leaf it belongs.\\nSplitting too much could', \\\n     'of course lead to a low bias and high variance.\\nTo prevent overfitting it can decide to split a node only if a cluster\\ncontains', \\\n     'at least a number of data points greater than a threashold.\\nAlso we can set the maximum depth of the tree.\\n')\nprint('This last parameter is going to be found using a grid search\\n')\n\ndepths = range(1,31)\ncriterions = ['mse','mae']\n\nbestMAE = np.inf #set infinite as starting value\nbestDepth = 0\nbestCriterion = ''\nmaeR2Scores = []\nmseR2Scores = []\nmaeMAEs = []\nmseMAEs = []\n\nfor criterion in criterions:\n    \n    for depth in depths:\n        \n        DTregressor = DecisionTreeRegressor(random_state = pRand, max_depth=depth, criterion=criterion)\n        _ = DTregressor.fit(x_train, y_train)\n        y_pred = DTregressor.predict(x_test)\n        \n        if(criterion == 'mae'):\n            maeR2Scores.append(DTregressor.score(x_train,y_train))\n            maeMAEs.append(metrics.mean_absolute_error(y_test,y_pred))\n            \n        if(criterion == 'mse'):\n            mseR2Scores.append(DTregressor.score(x_train,y_train))\n            mseMAEs.append(metrics.mean_absolute_error(y_test,y_pred))\n            \n        if(metrics.mean_absolute_error(y_test,y_pred)<bestMAE):\n            bestMAE = metrics.mean_absolute_error(y_test,y_pred)\n            bestDepth = depth\n            bestCriterion = criterion\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"print('\\nBest parameters for classifier:\\n')\nprint('- Criterion: %s' %bestCriterion)\nprint('- Max depth: %s' %bestDepth)\nprint('- Mean Absolute Error of classifier with best parameters: %d\\n' %bestMAE)\n\nprint('\\nNow let\\'s have a look on the behavior of the R-Squared score with respect to the MAE\\n')\n\nprint('In the following graph I analyze the trend of the R-Squared score and MAE for\\ndifferent depths using the criterion that performed better (%s)\\n' %bestCriterion)\n\nR2Scores = []\nMAEs = []\n\nif(bestCriterion=='mae'):\n    R2Scores = maeR2Scores\n    MAEs = maeMAEs\nelse:\n    R2Scores = mseR2Scores\n    MAEs = mseMAEs\n\n\nfig, ax = plt.subplots(1, 1, figsize=(10,6))\n    \n_ = ax.plot(range(1,31),R2Scores,linewidth=3, label='R-Squared score')\n_ = ax.plot(range(1,31),MAEs/np.amax(MAEs),linewidth=3, label='Normalized MAE')\n_ = ax.legend(fontsize='medium')\nplt.show()\n\nprint('\\nNote that the MAE curve is normalized between 0 and 1, where 1 is the highest MAE occurred\\nin the gridsearch; in this way we can spot the overfitting of the model at increasing tree\\ndepth, which leads to a very high R2-score but also to a worse performance on the test set\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nDecision Tree feature importance\\n')\n\nDTregressor = DecisionTreeRegressor(random_state = pRand, max_depth=bestDepth, criterion=bestCriterion)\n_ = DTregressor.fit(x_train, y_train)\n\ndic = {}\ni = 0\nfor column in x_train.columns:\n    dic[column] = DTregressor.feature_importances_[i]\n    i += 1\n\ndfp = pd.DataFrame.from_dict(dic, orient='index')\ndfp.sort_values(by=0, ascending=False).plot(kind='bar',figsize=(16,6))\nplt.title('Importance of the features in the Decision Tree', fontsize=10)\nplt.xticks(rotation=-50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nRandom Forest Regression\\n')\n\nprint('Random Forests are built creating each time a tree, using just a subset of the available\\nvariables. In this way we will end up', \\\n     'with a great variety of trees that will all be used\\nto predict target value for new data points; in fact a new point will', \\\n     'go through each\\ntree of the forest and get as predicted value the average of the values predicted by each tree\\n')\n\nprint('The number of trees for the forest may increase a lot the training time:\\nI am going to find the best number of trees in a small subset of values\\n')\n\n\nn_estimators = [10,50,100]\n\n\nbestMAE = np.inf #set infinite as starting value\nbestN_estimators = 0 #number of trees\nR2Scores = []\nMAEs = []\n\nfor n_estimator in n_estimators:\n            \n    RFregressor = RandomForestRegressor(random_state = pRand, max_depth=bestDepth, criterion=bestCriterion, n_estimators=n_estimator)\n    _ = RFregressor.fit(x_train, y_train)\n    y_pred = RFregressor.predict(x_test)\n        \n    R2Scores.append(RFregressor.score(x_train,y_train))\n    MAEs.append(metrics.mean_absolute_error(y_test,y_pred))\n            \n    if(metrics.mean_absolute_error(y_test,y_pred)<bestMAE):\n        bestMAE = metrics.mean_absolute_error(y_test,y_pred)\n        bestN_estimators = n_estimator\n           \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nBest parameters for classifier:\\n')\nprint('- Criterion: %s' %bestCriterion)\nprint('- Max depth: %s' %bestDepth)\nprint('- Best number of estimators: %s' %bestN_estimators)\nprint('- Mean Absolute Error of classifier with best parameters: %d\\n' %bestMAE)\n\nprint('\\nAs shown below, there\\'s a little improvement in the Mean Absolute Error\\nfor increasing number of estimators:\\n')\n\nfor i in range(3):\n    print('%d estimators - MAE: ' %n_estimators[i] + '%d' %MAEs[i])\n\nprint('\\n')\n\nRFregressor = RandomForestRegressor(random_state = pRand, max_depth=bestDepth, criterion=bestCriterion, n_estimators=bestN_estimators)\n_ = RFregressor.fit(x_train, y_train)\ny_pred = RFregressor.predict(x_test)\n\nres = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred.flatten()})\ndf1 = res.head(25)\ndf1.plot(kind='bar',figsize=(15,6))\nplt.title('Comparison of Actual and Predicted values', fontsize=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nRandom Forest feature importance\\n')\n\ndic = {}\ni = 0\nfor column in x_train.columns:\n    dic[column] = RFregressor.feature_importances_[i]\n    i += 1\n\ndfp = pd.DataFrame.from_dict(dic, orient='index')\ndfp.sort_values(by=0, ascending=False).plot(kind='bar',figsize=(16,6))\nplt.title('Importance of the features in the Random Forest', fontsize=10)\nplt.xticks(rotation=-50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nConclusions\\n')\n\nprint('Models that perfomed the best were Support Vector Regressor and Random Forest.\\nDecision Tree and Random Forest results', \\\n     'are easily interpretable thanks to the\\nfeature importance. They relied on very similar features for the decision making\\n')\n\nprint('As also seen in the start of this study, \\'median_income\\' had a key role\\nin the prediction of the median house value in a district', \\\n     'and this makes a lot of\\nsense, because in many places rich people live in different districts with respect\\nto poorer people, and of course', \\\n     'in more expensive houses\\n')\n\nprint('Secondly, the geographical position of the district was an important\\nfactor in the equation. In fact the latitude and longitude,', \\\n     'directly\\nlinked to the position with respect to the ocean gave a good hint to understand\\nhow much a house can cost in a certain district\\n\\n')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":1}