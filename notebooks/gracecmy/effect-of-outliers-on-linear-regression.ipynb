{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Diamonds** are formed naturally from extreme heat and pressure beneath the earth's surface millions of years ago. They are then only brought to the Earth's surface through volcanic eruptions. And because they are natural, they can come in many different shapes, sizes, colours and clarities.\n\nOf course, the bigger the better, right?? But at what point does big become fake??\n\nWhen modelling the price of diamonds, these *outliers* can skew predictions and therefore lessen the model's accuracy. Let's explore the diamond dataset and see the effect of outliers on linear regression:","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport seaborn as sns\nplt.rcParams[\"figure.figsize\"]=12,6","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the data, there are 53940 diamonds which can be described in 10 attributes:\n1. carat (i.e. weight)\n1. quality of the cut (5 categories in ascending order of Fair, Good, Very Good, Premium, Ideal)\n1. colour (7 categories with J being the worst to D being the best)\n1. clarity (8 categories in ascending order of I1, SI2, SI1, VS2, VS1, VVS2, VVS1, IF)\n1. depth out of 100 (i.e. how deep the diamond is relative to its width)\n1. table out of 100 (i.e. the ratio between the flat surface at the top of the diamond to its average width)\n1. price in USD\n1. length (x) in mm\n1. width (y) in mm \n1. depth (z) in mm","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/diamonds/diamonds.csv\",index_col=0)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although the cut, colour and clarity are categorical variables, there is an order/ranking and so can therefore be transformed into numeric values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#CUT\ndef cut_codes(cut):\n    if cut==\"Ideal\":\n        return 5\n    elif cut==\"Premium\":\n        return 4\n    elif cut==\"Very Good\":\n        return 3\n    elif cut==\"Good\":\n        return 2\n    else:\n        return 1\ndf[\"cut\"]=df[\"cut\"].apply(lambda x:cut_codes(x))\n\n#COLOUR\ndef color_codes(color):\n    if color==\"D\":\n        return 7\n    elif color==\"E\":\n        return 6\n    elif color==\"F\":\n        return 5\n    elif color==\"G\":\n        return 4\n    elif color==\"H\":\n        return 3\n    elif color==\"I\":\n        return 2\n    else:\n        return 1\ndf[\"color\"]=df[\"color\"].apply(lambda x:color_codes(x))\n\n#CLARITY\ndef clarity_codes(clarity):\n    if clarity==\"I1\":\n        return 8\n    elif clarity==\"SI2\":\n        return 7\n    elif clarity==\"SI1\":\n        return 6\n    elif clarity==\"VS2\":\n        return 5\n    elif clarity==\"VS1\":\n        return 4\n    elif clarity==\"VVS2\":\n        return 3\n    elif clarity==\"VVS1\":\n        return 2\n    else:\n        return 1\ndf[\"clarity\"]=df[\"clarity\"].apply(lambda x:clarity_codes(x))\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no null values but there are some 0s for the x, y and z which should not be mistaken as a measurement of 0 mm because without a length, width or depth a diamond cannot be a 3-dimensional object. Instead this 0 value should be treated as missing data. Our options to deal with missing data are to either:\n\n1. drop the rows; or\n1. replace with 0s with a new value such as the mean","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"The number of rows with a value of 0 for x are \",(df[\"x\"]==0).sum(),\".\")\nprint(\"The number of rows with a value of 0 for y are \",(df[\"y\"]==0).sum(),\".\")\nprint(\"The number of rows with a value of 0 for z are \",(df[\"z\"]==0).sum(),\".\")\nprint(\"The total number of rows with a value of 0 are \",((df[\"x\"]==0)|(df[\"y\"]==0)|(df[\"z\"]==0)).sum(),\".\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the number of rows with a value of 0 is only 20 (0.04%) out of the total 53940, we shall just drop said rows.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(df[(df[\"x\"]==0)|(df[\"y\"]==0)|(df[\"z\"]==0)].index,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But now when we take a look at the data, there are some obvious outliers (especially in y, z, depth and table).","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig=plt.figure(figsize=(16,16))\nax1=plt.subplot2grid((4,3),(0,0),colspan=2,rowspan=2)\nax2=plt.subplot2grid((4,3),(0,2))\nax3=plt.subplot2grid((4,3),(1,2))\nax4=plt.subplot2grid((4,3),(2,0))\nax5=plt.subplot2grid((4,3),(2,1))\nax6=plt.subplot2grid((4,3),(2,2))\nax7=plt.subplot2grid((4,3),(3,0))\nax8=plt.subplot2grid((4,3),(3,1))\nax9=plt.subplot2grid((4,3),(3,2))\n\nsns.scatterplot(x=df[\"carat\"],y=df[\"price\"],color=\"lavender\",ax=ax1)\nsns.scatterplot(x=df[\"x\"],y=df[\"price\"],color=\"powderblue\",ax=ax2)\nsns.scatterplot(x=df[\"y\"],y=df[\"price\"],color=\"lightblue\",ax=ax3)\nsns.scatterplot(x=df[\"depth\"],y=df[\"price\"],color=\"palegreen\",ax=ax4)\nsns.scatterplot(x=df[\"table\"],y=df[\"price\"],color=\"lightgreen\",ax=ax5)\nsns.scatterplot(x=df[\"z\"],y=df[\"price\"],color=\"skyblue\",ax=ax6)\nsns.scatterplot(x=df[\"cut\"],y=df[\"price\"],color=\"lightsalmon\",ax=ax7)\nsns.scatterplot(x=df[\"color\"],y=df[\"price\"],color=\"palevioletred\",ax=ax8)\nsns.scatterplot(x=df[\"clarity\"],y=df[\"price\"],color=\"gold\",ax=ax9)\n\nplt.tight_layout(pad=1,h_pad=1,w_pad=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now what to do with these outliers? Resume the analysis with the outliers, or filter them out? And if we filter them out, should we remove the outliers visually based on their position on the graph or by calculating 1.5 times the interquartile range above the third quartile or below the first quartile?\nLet's try all three methods and compare their model accuracy!\n\n1. Including the outliers\n1. Removing the visual outliers\n1. Removing the calculated outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\noutliers=[\"Including the Outliers\",\"Removing the Visual Outliers\",\"Removing the Calculated Outliers\"]\nrmse=[]\nr2score=[]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Including the Outliers","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df1=df.copy()\n\nprint(\"The number of rows include all the outliers are\",df1.shape[0],\".\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"x1=df1.drop([\"price\"],axis=1)\ny1=df1[\"price\"]\nx1_train,x1_test,y1_train,y1_test=train_test_split(x1,y1,test_size=0.3,random_state=5)\n\nlr1=LinearRegression()\nlr1.fit(x1_train,y1_train)\ny1_predict=lr1.predict(x1_test)\n\nprint(\"MAE: %.2f\"%mean_absolute_error(y1_test,y1_predict))\nprint(\"MSE: %.2f\"%mean_squared_error(y1_test,y1_predict))\nprint(\"RMSE: %.2f\"%np.sqrt(mean_absolute_error(y1_test,y1_predict)))\nprint(\"R2: %.2f\"%r2_score(y1_test,y1_predict))\n\nrmse.append(np.sqrt(mean_absolute_error(y1_test,y1_predict)))\nr2score.append(r2_score(y1_test,y1_predict))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.distplot(y1_test,hist=True,color=\"lightskyblue\",label=\"Actual Values\")\nsns.distplot(y1_predict,hist=True,color=\"plum\",label=\"Predicted Values\")\nplt.legend()\nplt.xlabel(\"Price\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f,axes=plt.subplots(2,1,sharex=True)\nsns.boxplot(y1_test,color=\"lightskyblue\",whis=4,ax=axes[0])\nsns.boxplot(y1_predict,color=\"plum\",whis=7,ax=axes[1])\n\naxes[0].set_xlabel(\"\")\nplt.xlabel(\"Price\")\n\naxes[0].set_ylabel(\"Actual Price\")\naxes[1].set_ylabel(\"Predicted Price\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the entire dataset with all the outliers included did not do too bad a job with a R Squared Score of 0.91. The predicted prices followed the general trend and distribution of the acutal prices, but it predicted a much much larger price range and even predicted negative prices which are impossible! So maybe not so good after all..","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Removing the Visual Outliers","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df2=df.copy()\n\nvisual=df2[(df2[\"y\"]>30)|(df2[\"z\"]>10)|(df2[\"depth\"]<50)|(df2[\"depth\"]>75)|(df2[\"table\"]<45)|(df2[\"table\"]>75)]\nprint(\"There are {num} rows of visual outliers where y >30, z >10, depth <50 and >75, and table <45 and >75.\".format(num=visual.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df2.drop(visual.index,inplace=True)\n\nprint(\"After dropping the Visual Outliers, the number of rows are now\",df2.shape[0],\".\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"x2=df2.drop([\"price\"],axis=1)\ny2=df2[\"price\"]\nx2_train,x2_test,y2_train,y2_test=train_test_split(x2,y2,test_size=0.3,random_state=5)\n\nlr2=LinearRegression()\nlr2.fit(x2_train,y2_train)\ny2_predict=lr2.predict(x2_test)\n\nprint(\"MAE: %.2f\"%mean_absolute_error(y2_test,y2_predict))\nprint(\"MSE: %.2f\"%mean_squared_error(y2_test,y2_predict))\nprint(\"RMSE: %.2f\"%np.sqrt(mean_absolute_error(y2_test,y2_predict)))\nprint(\"R2: %.2f\"%r2_score(y2_test,y2_predict))\n\nrmse.append(np.sqrt(mean_absolute_error(y2_test,y2_predict)))\nr2score.append(r2_score(y2_test,y2_predict))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.distplot(y2_test,hist=True,color=\"lightskyblue\",label=\"Actual Values\")\nsns.distplot(y2_predict,hist=True,color=\"plum\",label=\"Predicted Values\")\nplt.legend()\nplt.xlabel(\"Price\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f,axes=plt.subplots(2,1,sharex=True)\nsns.boxplot(y2_test,color=\"lightskyblue\",whis=4,ax=axes[0])\nsns.boxplot(y2_predict,color=\"plum\",whis=7,ax=axes[1])\n\naxes[0].set_xlabel(\"\")\nplt.xlabel(\"Price\")\n\naxes[0].set_ylabel(\"Actual Price\")\naxes[1].set_ylabel(\"Predicted Price\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing the visual outliers did ever so slightly better than including all the outliers. The R Squared Score was the same at 0.91, and the Root Mean Squared Error only improved by 0.04 - almost negligible. This could be because there was only a difference 14 data points out of the original 50,000+ data points. Once again, even though the model predicted the same price distribution, the model still predicted a larger price range and even negative prices.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Removing the Calculated Outliers","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df3=df.copy()\n\nQ1=df3.quantile(0.25)\nQ3=df3.quantile(0.75)\nIQR=Q3-Q1\n\ncol=list(df3.columns)\n\nprint(\"Number of Calculated Outliers\")\nprint(df3[(df3[col]<(Q1[col]-1.5*IQR[col]))|(df3[col]>(Q3[col]+1.5*IQR[col]))].count())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def c_outliers(col):\n    return df3[(df3[col]<(Q1[col]-1.5*IQR[col]))|(df3[col]>(Q3[col]+1.5*IQR[col]))]\n\nfor col in df3:\n    df3.drop(c_outliers(col).index,inplace=True)\n    \nprint(\"After dropping the Calculated Outliers, the number of rows are now\",df3.shape[0],\".\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"x3=df3.drop([\"price\"],axis=1)\ny3=df3[\"price\"]\nx3_train,x3_test,y3_train,y3_test=train_test_split(x3,y3,test_size=0.3,random_state=5)\n\nlr3=LinearRegression()\nlr3.fit(x3_train,y3_train)\ny3_predict=lr3.predict(x3_test)\n\nprint(\"MAE: %.2f\"%mean_absolute_error(y3_test,y3_predict))\nprint(\"MSE: %.2f\"%mean_squared_error(y3_test,y3_predict))\nprint(\"RMSE: %.2f\"%np.sqrt(mean_absolute_error(y3_test,y3_predict)))\nprint(\"R2: %.2f\"%r2_score(y3_test,y3_predict))\n\nrmse.append(np.sqrt(mean_absolute_error(y3_test,y3_predict)))\nr2score.append(r2_score(y3_test,y3_predict))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.distplot(y3_test,hist=True,color=\"lightskyblue\",label=\"Actual Values\")\nsns.distplot(y3_predict,hist=True,color=\"plum\",label=\"Predicted Values\")\nplt.legend()\nplt.xlabel(\"Price\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f,axes=plt.subplots(2,1,sharex=True)\nsns.boxplot(y3_test,color=\"lightskyblue\",whis=4,ax=axes[0])\nsns.boxplot(y3_predict,color=\"plum\",whis=7,ax=axes[1])\n\naxes[0].set_xlabel(\"\")\nplt.xlabel(\"Price\")\n\naxes[0].set_ylabel(\"Actual Price\")\naxes[1].set_ylabel(\"Predicted Price\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most accurate model out of the three! The R Squared Score did not improve all that much from 0.91 to 0.92, and there negative prices were still predicted but at least the predicted prices were not off by 10,000s.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### So did the outliers do anything?","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"scores=pd.DataFrame({\"Data\":outliers,\"RMSE\":rmse,\"R2-Scores\":r2score})\nscores","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig,ax1=plt.subplots()\n\nax1.plot(scores[\"Data\"],scores[\"RMSE\"],color=\"sandybrown\",marker=\"o\")\nax1.set_ylabel(\"Root Mean Square Error\",fontsize=12,color=\"sandybrown\")\nfor label in ax1.get_yticklabels():\n    label.set_color(\"sandybrown\")\n    \nax2=ax1.twinx()\nax2.plot(scores[\"Data\"],scores[\"R2-Scores\"],color=\"yellowgreen\",marker=\"^\")\nax2.set_ylabel(\"R Squared Score\",fontsize=12,color=\"yellowgreen\")\nfor label in ax2.get_yticklabels():\n    label.set_color(\"yellowgreen\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Removing the calculated outliers* gave the most accurate model because it decreased variability as almost 12% (6412) of the dataset was filtered out to create the ideal dataset. This would make an unreliable model as real datasets are not always so consistent. \n\n*Removing the visual outliers* did not improve much from *including the outliers*. This may be because of the boundaries I set for the outliers - I am no diamond expert and so these numbers were just based off what I thought were suitable (i.e. y >30; z >10; depth <50 and >75; table <45 and >75; I also did not list any outlier boundaries for carat).\n\nNow this is where domain knowledge would be very useful in identifying appropriate boundaries for outliers. So if any diamond extraordinaire is reading this, please do let me know!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}