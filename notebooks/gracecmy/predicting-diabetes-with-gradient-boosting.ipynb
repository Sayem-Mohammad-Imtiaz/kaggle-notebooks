{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Characterized by high levels of sugar in the blood, Type 2 diabetes can be prevent or delayed with lifestyle changes. By modelling diabetes in patients, individuals can be better informed about their risks of developing the disease.**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option(\"display.max_columns\",100)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nparams={\"figure.facecolor\":(0.0,0.0,0.0,0),\n        \"axes.facecolor\":(1.0,1.0,1.0,1),\n        \"savefig.facecolor\":(0.0,0.0,0.0,0)}\nplt.rcParams.update(params)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/pima-indians-diabetes-database/diabetes.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset is of only Pima Indian females aged 21 and above."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data has 768 entries, and can be described in 9 columns."},{"metadata":{},"cell_type":"markdown","source":"The columns are:\n* Pregnancies: the number of times the patient has been pregnant.\n* Glucose: the plasma glucose concentration after 2 hours in an oral glucose tolerance test.\n* BloodPressure: the aiastolic blood pressure (mm Hg).\n* SkinThickness: the triceps skin fold thickness (mm).\n* Insulin: 2-Hour serum insulin (mu U/ml).\n* BMI: the body mass index.\n* DiabetesPedigreeFunction: a function which scores the likelihood of diabetes based on family history.\n* Age: the age (years)\n* Outcome: \"0\" as no diabetes, \"1\" as with diabetes."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good, no null values and all the columns are numeric."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But is it possible to have 0 levels of *Glucose*, *BloodPressure*, *SkinThickness*, *Insulin*, *BMI*? Let's take a closer look:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df[[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]]:\n    plt.figure()\n    sns.distplot(df[i],kde=False,color=\"#AE9CCD\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For *Glucose*, *BloodPressure* and *BMI* it is obvious that the 0s must be addressed. While not so obvious in *SkinThickness* and *Insulin* these 0s should also be addressed as it is not possible to have 0mm of skin or 0 mu U/ml of insulin."},{"metadata":{},"cell_type":"markdown","source":"As such the 0s should be treated as missing data, and changed to NaN. So then we do have null values..."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]]=df[[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]].replace(0,np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.isnull(),cmap=\"RdPu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we drop the null values, we will loose too much information. So let's replace them, but replace them with what? Do we choose the mean, median, mode or some other arbituary number?\n\nIf we choose either the mean, median or mode, we must split the data into the training and testing set to ensure the value (i.e. mean, median or mode) is not leaked over from the testing set."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=df.drop([\"Outcome\"],axis=1)\ny=df[\"Outcome\"]\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.33,random_state=7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training set x shape:\",x_train.shape,\"and y shape:\",y_train.shape)\nprint(\"Testing set shape:\",x_test.shape,\"and y shape:\",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can impute the replacing value (i.e. mean, median or mode) from only the training set.\n\nLet's take a look at each column to decide which metric to use:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in x_train[[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]]:\n    plt.figure()\n    sns.distplot(df[i],kde=False,color=\"#AE9CCD\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For *Glucose* and *BloodPressure* I will use the mean; while for the *SkinThickness*, *Insulin* and *BMI* I will use the median. This is based on their distribution (*Glucose* and *BloodPressure* have a more normal distribution whereas *SkinThickness*, *Insulin* and *BMI* are more skewed)."},{"metadata":{},"cell_type":"markdown","source":"First fill in the missing values in the training set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The number of null values:\")\nprint(x_train.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train[\"Glucose\"].fillna(x_train[\"Glucose\"].mean(),inplace=True)\nx_train[\"BloodPressure\"].fillna(x_train[\"BloodPressure\"].mean(),inplace=True)\nx_train[\"SkinThickness\"].fillna(x_train[\"SkinThickness\"].median(),inplace=True)\nx_train[\"Insulin\"].fillna(x_train[\"Insulin\"].median(),inplace=True)\nx_train[\"BMI\"].fillna(x_train[\"BMI\"].median(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Check there are no more null values:\")\nprint(x_train.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now fill in the missing values in the testing set using the training set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The number of null values:\")\nprint(x_test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test[\"Glucose\"].fillna(x_train[\"Glucose\"].mean(),inplace=True)\nx_test[\"BloodPressure\"].fillna(x_train[\"BloodPressure\"].mean(),inplace=True)\nx_test[\"SkinThickness\"].fillna(x_train[\"SkinThickness\"].median(),inplace=True)\nx_test[\"Insulin\"].fillna(x_train[\"Insulin\"].median(),inplace=True)\nx_test[\"BMI\"].fillna(x_train[\"BMI\"].median(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Check there are no more null values:\")\nprint(x_test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is now nice and clean, but will need to be scaled to ensure columns with higher values do not have a higher weighting.\n\nBut as before, to avoid any data leakage, we will only fit the scaler to the training set and not the testing set (i.e. fit and transform the training set, but only transform the testing set)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler=StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train=scaler.fit_transform(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test=scaler.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is finally time to model the data!"},{"metadata":{},"cell_type":"markdown","source":"Let's try as many algorithms as we can."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The algorithms will be evaluated based on the number of False Negatives and accuracy.\n\nThe accuracy score will tell us how accurately the model predicts the True Positives and True Negatives, which is still essential, but there is a trade-off between having a model that predicts more False Positives (predicting diabetes in a healthy person) vs False Negatives (predicting no diabetes in a person that does have diabetes) - i.e. type 1 error vs type 2 error respectively. I think it is more important to prevent type 2 errors to ensure that patients are not overlooked."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix,roc_curve,roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"name=[\"Logistic\",\"kNN\",\"DecisionTree\",\"RandomForest\",\"GradientBoost\",\"AdaBoost\",\"SVM\",\"LGBM\",\"XGB\"]\nmodels=[LogisticRegression(random_state=7),KNeighborsClassifier(),DecisionTreeClassifier(random_state=7),RandomForestClassifier(random_state=7),GradientBoostingClassifier(random_state=7),AdaBoostClassifier(random_state=7),SVC(random_state=7),LGBMClassifier(random_state=7),XGBClassifier(random_state=7)]\nscore=[]\nfalsenegative=[]\n\nfor model in models:\n    model.fit(x_train,y_train)\n    score.append(model.score(x_test,y_test))\n    y_predict=model.predict(x_test)\n    tn,fp,fn,tp=confusion_matrix(y_test,y_predict).ravel()\n    falsenegative.append(fn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=pd.DataFrame({\"name\":name,\"models\":models,\"score\":score,\"fn\":falsenegative})\nresults.sort_values([\"fn\",\"score\"],ascending=[True,False])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus the best model is the Gradient Boosting Classifier with the lowest number of False Negatives and the third highest accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"model=GradientBoostingClassifier(random_state=7)\nmodel.fit(x_train,y_train)\ny_predict=model.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,y_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_score=model.predict_proba(x_test)[:,1]\n\nfalse_positive_rate,true_positive_rate,threshold=roc_curve(y_test,y_score)\nprint(\"roc_auc_score: \",roc_auc_score(y_test,y_score))\n\nplt.plot(false_positive_rate,true_positive_rate)\nplt.plot([0,1],ls=\"--\")\nplt.plot([0,0],[1,0],c=\".7\")\nplt.plot([1,1],c=\".7\")\nplt.title(\"Receiver Operating Characteristic\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model is based on its default parameters:\n\n* loss=\"deviance\"\n* learning_rate=0.1\n* n_estimators=100\n* subsample=1.0\n* criterion=\"friedman_mse\"\n* min_samples_split=2\n* min_samples_leaf=1\n* min_weight_fraction_leaf=0.0\n* max_depth=3\n* min_impurity_decrease=0.0\n* initestimator=None\n* random_state=None\n* max_features=None\n* verbose=0\n* max_leaf_nodes=None\n* warm_start=False\n* validation_fraction=0.1\n* n_iter_no_change=None\n* tol=1e-4\n* ccp_alpha=0.0"},{"metadata":{},"cell_type":"markdown","source":" Perhaps the model can be improved by tuning its parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params={\n    \"loss\":[\"exponential\"],\n    \"learning_rate\":[0.01,0.05,0.1,0.15,0.2],\n    #\"n_estimators\":[],\n    \"subsample\":[0.8,0.9,1.0],\n    \"criterion\":[\"friedman_mse\"],\n    #\"min_samples_split\":[],\n    #\"min_samples_leaf\":[],\n    \"max_depth\":[3,5,8],\n    \"max_features\":[\"log2\",\"sqrt\"]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid=GridSearchCV(GradientBoostingClassifier(random_state=7),params,refit=True,verbose=1)\ngrid.fit(x_train,y_train)\ny_predict=grid.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,y_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_score=grid.predict_proba(x_test)[:,1]\n\nfalse_positive_rate,true_positive_rate,threshold=roc_curve(y_test,y_score)\nprint(\"roc_auc_score: \",roc_auc_score(y_test,y_score))\n\nplt.plot(false_positive_rate,true_positive_rate)\nplt.plot([0,1],ls=\"--\")\nplt.plot([0,0],[1,0],c=\".7\")\nplt.plot([1,1],c=\".7\")\nplt.title(\"Receiver Operating Characteristic\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although only ever so slightly, the model has improved - precision, recall, f1-score, accuracy and ROC AUC score all increased; and the number of False Negatives and False Positives both decreased!\n\nThe model may be further improved by - for example - adjusting its parameters (as above); collecting more data, and data with minimal missing information; and conducting feature engineering and selection."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}