{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"This project focuses on imbalanced dataset and understanding the importance of performance metrics. Also, using real-time knowledge for better data pre-processing, understanding the distribution attributes using various visualization tools and using Gausian Mixture Model(GMM) in processing bimodal data distribution. In addition, two models of Random Forest Classifiers are used for predicting stroke.","metadata":{}},{"cell_type":"markdown","source":"# Importing libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\npd.options.mode.chained_assignment = None\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:40.055929Z","iopub.execute_input":"2021-06-30T19:42:40.056302Z","iopub.status.idle":"2021-06-30T19:42:40.070682Z","shell.execute_reply.started":"2021-06-30T19:42:40.056277Z","shell.execute_reply":"2021-06-30T19:42:40.069131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Analysis","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:40.075305Z","iopub.execute_input":"2021-06-30T19:42:40.075657Z","iopub.status.idle":"2021-06-30T19:42:40.114675Z","shell.execute_reply.started":"2021-06-30T19:42:40.075627Z","shell.execute_reply":"2021-06-30T19:42:40.113619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To predict the stroke, we have 3 continuous variables - *age, avg_glucose_level, bmi* and 7 categorical variables - *gender,hypertension, heart_disease, ever_married, Work_type, residence_type, smoking_status*. *id* is just a unique number without any value towards stroke prediction and can be dropped ","metadata":{}},{"cell_type":"code","source":"df.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:40.116273Z","iopub.execute_input":"2021-06-30T19:42:40.116543Z","iopub.status.idle":"2021-06-30T19:42:40.178703Z","shell.execute_reply.started":"2021-06-30T19:42:40.116516Z","shell.execute_reply":"2021-06-30T19:42:40.177833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the brief we can report two things.\n1. *bmi* holds a few null values\n2. *bmi* ranges from 10 to 97. [We know that, bmi <18.5 is underweight, 18.5 - 25 healthy, 25 - 30 overweight, >30 Obese, >40 morbidly obese (very rare and almost impossible)]. Hence, very high bmi values can be considered as noise/outliers.\n","metadata":{}},{"cell_type":"code","source":"df_no_mv = df.dropna(axis=0, how='any')\nx_opt_bmi = df_no_mv[df_no_mv['bmi']<=45]\n\n#dropping id\ndata = x_opt_bmi.drop(['id'], axis =1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:40.180502Z","iopub.execute_input":"2021-06-30T19:42:40.180797Z","iopub.status.idle":"2021-06-30T19:42:40.194375Z","shell.execute_reply.started":"2021-06-30T19:42:40.180768Z","shell.execute_reply":"2021-06-30T19:42:40.193535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_stroke_count(y_count):\n    ax = sns.countplot(x=y_count, label = 'stroke count')\n    N , Y = y_count.value_counts()\n    print('Number of people without stroke attack - ', N)\n    print('Number of people with stroke attack - ', Y)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:40.195367Z","iopub.execute_input":"2021-06-30T19:42:40.195676Z","iopub.status.idle":"2021-06-30T19:42:40.207134Z","shell.execute_reply.started":"2021-06-30T19:42:40.195642Z","shell.execute_reply":"2021-06-30T19:42:40.206541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = data.stroke\nx = data.drop(['stroke'], axis=1)\nplot_stroke_count(y)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:40.20793Z","iopub.execute_input":"2021-06-30T19:42:40.208227Z","iopub.status.idle":"2021-06-30T19:42:40.309614Z","shell.execute_reply.started":"2021-06-30T19:42:40.208203Z","shell.execute_reply":"2021-06-30T19:42:40.30905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The stroke to no-stroke-attack ratio is 1:20, indicating that the dataset is highly imbalanced. ","metadata":{}},{"cell_type":"markdown","source":"# Random Forest Classification with Univariate Feature Selection","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix, classification_report, accuracy_score\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:40.310739Z","iopub.execute_input":"2021-06-30T19:42:40.311039Z","iopub.status.idle":"2021-06-30T19:42:40.314813Z","shell.execute_reply.started":"2021-06-30T19:42:40.311004Z","shell.execute_reply":"2021-06-30T19:42:40.313826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def kBest_model(x_pp, y, kv):\n    x_train_k, x_test_k, y_train_k, y_test_k = train_test_split(x_pp, y, test_size=0.3, random_state=42)\n    select_feature = SelectKBest(chi2, k=kv).fit(x_train_k, y_train_k)\n    rec_result = pd.DataFrame({'features':x_train_k.columns,'score':select_feature.scores_}).sort_values(\n        by=['score'], ascending=False)\n    x_train_rfc = select_feature.transform(x_train_k)\n    x_test_rfc = select_feature.transform(x_test_k)\n    clf_rf = RandomForestClassifier()      \n    clr_rf = clf_rf.fit(x_train_rfc,y_train_k)\n    y_pred_k = clf_rf.predict(x_test_rfc)\n    ac = accuracy_score(y_test_k,y_pred_k)\n    print('Accuracy is: ',ac)\n    cm = confusion_matrix(y_test_k,y_pred_k)\n    sns.heatmap(cm,annot=True,fmt=\"d\")\n    return y_test_k,y_pred_k\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:40.315915Z","iopub.execute_input":"2021-06-30T19:42:40.316122Z","iopub.status.idle":"2021-06-30T19:42:40.337229Z","shell.execute_reply.started":"2021-06-30T19:42:40.316102Z","shell.execute_reply":"2021-06-30T19:42:40.33611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Encoding categarical variables\nfrom sklearn.preprocessing import LabelEncoder\n\ndata_rfe = pd.DataFrame(x)\nlabelEncoder = LabelEncoder()\ndata_rfe['gender'] = labelEncoder.fit_transform(data_rfe['gender'])\ndata_rfe[\"ever_married\"] = labelEncoder.fit_transform(data_rfe['ever_married'])\ndata_rfe['work_type'] = labelEncoder.fit_transform(data_rfe['work_type'])\ndata_rfe['Residence_type'] = labelEncoder.fit_transform(data_rfe['Residence_type'])\ndata_rfe['smoking_status'] = labelEncoder.fit_transform(data_rfe['smoking_status'])\ndata_rfe['heart_disease'] = labelEncoder.fit_transform(data_rfe['heart_disease'])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:40.339364Z","iopub.execute_input":"2021-06-30T19:42:40.339765Z","iopub.status.idle":"2021-06-30T19:42:40.370946Z","shell.execute_reply.started":"2021-06-30T19:42:40.339725Z","shell.execute_reply":"2021-06-30T19:42:40.370228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_rfe,y_pred_rfe = kBest_model(data_rfe, y, 6)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:40.372204Z","iopub.execute_input":"2021-06-30T19:42:40.372596Z","iopub.status.idle":"2021-06-30T19:42:40.927107Z","shell.execute_reply.started":"2021-06-30T19:42:40.372572Z","shell.execute_reply":"2021-06-30T19:42:40.925879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on accuracy, the model is making a correct prediction 95% of the time, indicating that the model is doing a great job.\n\nHowever, the confusion matrix reveals that stroke was correctly predicted ZERO times, unlike its couterpart. The model has just predicted no-stroke in almost all cases.","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test_rfe,y_pred_rfe))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:40.928635Z","iopub.execute_input":"2021-06-30T19:42:40.928917Z","iopub.status.idle":"2021-06-30T19:42:40.940963Z","shell.execute_reply.started":"2021-06-30T19:42:40.928887Z","shell.execute_reply":"2021-06-30T19:42:40.939949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We get recall = 0.01, which means that the model is broken and it can not correctly classify even a single entry when the actual value is positive. \n\nFurther, precision also expresses an irregularity in the proportion of the data points in our model\n\n**Thus, accuracy alone doesn't tell the full story**","metadata":{}},{"cell_type":"markdown","source":"# AUC and ROC","metadata":{}},{"cell_type":"markdown","source":"ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0 classes as 0 and 1 classes as 1.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, roc_curve, auc\nfrom sklearn.metrics import precision_recall_curve, average_precision_score","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:40.942296Z","iopub.execute_input":"2021-06-30T19:42:40.942528Z","iopub.status.idle":"2021-06-30T19:42:40.952317Z","shell.execute_reply.started":"2021-06-30T19:42:40.942503Z","shell.execute_reply":"2021-06-30T19:42:40.951007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def roc_auc_model(y_test_rfe,y_pred_rfe):\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test_rfe,y_pred_rfe)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    plt.figure(figsize=(5,5))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],linestyle='--')\n    plt.axis('tight')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:40.953597Z","iopub.execute_input":"2021-06-30T19:42:40.953932Z","iopub.status.idle":"2021-06-30T19:42:40.970326Z","shell.execute_reply.started":"2021-06-30T19:42:40.953894Z","shell.execute_reply":"2021-06-30T19:42:40.969326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc_auc_model(y_test_rfe,y_pred_rfe)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:40.971361Z","iopub.execute_input":"2021-06-30T19:42:40.971638Z","iopub.status.idle":"2021-06-30T19:42:41.153237Z","shell.execute_reply.started":"2021-06-30T19:42:40.971609Z","shell.execute_reply":"2021-06-30T19:42:41.152365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Further, AUC 0.5 indicates that the classifier is not able to distinguish between Positive and Negative class points. Either the classifier is predicting random class or constant class for all the data points","metadata":{}},{"cell_type":"markdown","source":"Most ML algorithms used for classification are designed around the assumption of an equal number of examples in each class. \n\nAppropriate sampling is one of the techniques to handle imbalanced datasets","metadata":{}},{"cell_type":"markdown","source":"# SAMPLING","metadata":{}},{"cell_type":"markdown","source":"# Random Under-Sampling","metadata":{}},{"cell_type":"markdown","source":"Reducing the size of data so as to increase the ratio between minority and majority classes","metadata":{}},{"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler\nundersample = RandomUnderSampler(sampling_strategy='majority')\nX_us, y_us = undersample.fit_resample(data_rfe, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:41.156111Z","iopub.execute_input":"2021-06-30T19:42:41.156429Z","iopub.status.idle":"2021-06-30T19:42:41.171016Z","shell.execute_reply.started":"2021-06-30T19:42:41.156398Z","shell.execute_reply":"2021-06-30T19:42:41.169781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_stroke_count(y_us)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:41.173781Z","iopub.execute_input":"2021-06-30T19:42:41.174187Z","iopub.status.idle":"2021-06-30T19:42:41.306768Z","shell.execute_reply.started":"2021-06-30T19:42:41.174126Z","shell.execute_reply":"2021-06-30T19:42:41.305651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Though undersampling helps in reducing the disparity between majority and minority classes, it often leads to the loss of critical data.","metadata":{}},{"cell_type":"markdown","source":"# Random Over-sampling","metadata":{}},{"cell_type":"code","source":"data_ros = pd.concat([data_rfe, y], axis =1)\ni=0\nwhile i <=3:\n    temp = data_ros.query('stroke ==1')\n    data_ros = pd.concat( [data_ros,data_ros.query('stroke ==1')], axis=0)\n    i+=1\n    \nX_ros = data_ros.drop(['stroke'], axis =1)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:41.308875Z","iopub.execute_input":"2021-06-30T19:42:41.309334Z","iopub.status.idle":"2021-06-30T19:42:41.351431Z","shell.execute_reply.started":"2021-06-30T19:42:41.309294Z","shell.execute_reply":"2021-06-30T19:42:41.350286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data_ros.columns)\nplot_stroke_count(data_ros.stroke)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:41.35489Z","iopub.execute_input":"2021-06-30T19:42:41.3551Z","iopub.status.idle":"2021-06-30T19:42:41.466645Z","shell.execute_reply.started":"2021-06-30T19:42:41.355079Z","shell.execute_reply":"2021-06-30T19:42:41.465693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_ros,y_pred_ros = kBest_model(X_ros, data_ros.stroke, 5)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:41.467791Z","iopub.execute_input":"2021-06-30T19:42:41.468075Z","iopub.status.idle":"2021-06-30T19:42:42.374417Z","shell.execute_reply.started":"2021-06-30T19:42:41.468048Z","shell.execute_reply":"2021-06-30T19:42:42.373312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a result of over-sampling, prominent number of cases in both minority and majority classes are predicted correctly. ","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test_ros,y_pred_ros))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:42.37574Z","iopub.execute_input":"2021-06-30T19:42:42.376042Z","iopub.status.idle":"2021-06-30T19:42:42.391106Z","shell.execute_reply.started":"2021-06-30T19:42:42.376014Z","shell.execute_reply":"2021-06-30T19:42:42.390327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both precision and recal reveals promising values. However, over-sampling often leads to **over-fitting** of data, since multiple duplicate values of a particular values is appended to the dataset.","metadata":{}},{"cell_type":"markdown","source":"# Synthetic minority oversampling technique (SMOTE)","metadata":{}},{"cell_type":"markdown","source":"SMOTE is an over-sampling method that also addressed the issue of overfitting. \nFirstly, take the difference between a feature vector (minority class sample) and one of its k nearest neighbors (minority class samples).Then, multiply this difference by a random number between 0 and 1. Finally, add this difference to the feature value of the original feature vector, thus a new feature vector is created","metadata":{}},{"cell_type":"markdown","source":"Before applying SMOTE, lets analyze the data.","metadata":{}},{"cell_type":"markdown","source":"# Data Pre-Processing","metadata":{}},{"cell_type":"markdown","source":"# Analyzing variables","metadata":{}},{"cell_type":"code","source":"#Understanding continuous variables using a box plot\n\nf, axes = plt.subplots(1,3, figsize=(15,7))\n\ndata_age = pd.concat([data.stroke, data.age], axis=1)\ndata_plot_age = pd.melt(data_age, id_vars = 'stroke', var_name = 'age')\nsns.boxplot(x='age', y='value', hue='stroke', data=data_plot_age,ax=axes[0])\n\ndata_bmi = pd.concat([data.stroke, data.bmi], axis=1)\ndata_plot_bmi = pd.melt(data_bmi, id_vars = 'stroke', var_name = 'bmi')\nsns.boxplot(x='bmi', y='value', hue='stroke', data=data_plot_bmi,ax=axes[1])\n\ndata_agl = pd.concat([data.stroke, data.avg_glucose_level], axis=1)\ndata_plot_agl = pd.melt(data_agl, id_vars = 'stroke', var_name = 'avg_glucose_level')\nsns.boxplot(x='avg_glucose_level', y='value', hue='stroke', data=data_plot_agl,ax=axes[2])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:42.392349Z","iopub.execute_input":"2021-06-30T19:42:42.392738Z","iopub.status.idle":"2021-06-30T19:42:42.898604Z","shell.execute_reply.started":"2021-06-30T19:42:42.392709Z","shell.execute_reply":"2021-06-30T19:42:42.897554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plot above, we see that, there is a clear distinction in age, indicating that old people are more prone to stroke. \n\n","metadata":{}},{"cell_type":"code","source":"# removing outliers for age\ndata = data.query('(age>25 & stroke ==1) or stroke==0' )","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:42.89968Z","iopub.execute_input":"2021-06-30T19:42:42.899895Z","iopub.status.idle":"2021-06-30T19:42:42.909271Z","shell.execute_reply.started":"2021-06-30T19:42:42.899872Z","shell.execute_reply":"2021-06-30T19:42:42.907585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly, the box plot of avg_glucose_level displays multiple datapoints as outliers, mandating the need for further analysis","metadata":{}},{"cell_type":"code","source":"#Analysing avg_glucose_level attribute\n\nstroke1 =data_agl[data_agl['stroke']==1]\nstroke0 =data_agl[data_agl['stroke']==0]\n\nsns.kdeplot(stroke1['avg_glucose_level'], shade=True, color=\"r\")\nsns.kdeplot(stroke0['avg_glucose_level'], shade=True, color=\"g\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:42.910742Z","iopub.execute_input":"2021-06-30T19:42:42.911091Z","iopub.status.idle":"2021-06-30T19:42:43.09731Z","shell.execute_reply.started":"2021-06-30T19:42:42.911058Z","shell.execute_reply":"2021-06-30T19:42:43.096497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Green and red denotes those without stroke and with stroke respectively.\n\nThus, the anomaly observed in box plot is justified by the bimodal distribution of data. ","metadata":{}},{"cell_type":"code","source":"# Using Gausian Mixture Model(GMM)\n\nfrom sklearn.mixture import GaussianMixture\n\ngmm = GaussianMixture(n_components=2, random_state=42)\ngmm.fit(data_agl['avg_glucose_level'].values.reshape(-1, 1))\ndata_agl['gmm']= gmm.predict(data_agl['avg_glucose_level'].values.reshape(-1, 1))\n\n#Visualization\nf, ax = plt.subplots(1,2, figsize=(18, 6))\nsns.kdeplot(data=data_agl['avg_glucose_level'], ax=ax[0])\nax[0].set_title('Before GMM', fontsize=16)\nsns.kdeplot(data=data_agl[data_agl['gmm']==0].avg_glucose_level, label='Component 1', ax=ax[1])\nsns.kdeplot(data=data_agl[data_agl['gmm']==1].avg_glucose_level, label='Component 2', ax=ax[1])\nax[1].set_title('After GMM', fontsize=16)\nplt.show()\n\n# Adding gmm to the input dataset\ndata['gmm'] = data_agl['gmm']\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:43.098351Z","iopub.execute_input":"2021-06-30T19:42:43.09866Z","iopub.status.idle":"2021-06-30T19:42:43.498664Z","shell.execute_reply.started":"2021-06-30T19:42:43.098629Z","shell.execute_reply":"2021-06-30T19:42:43.497851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"GMM model helps in identifying and splitting the subpopulations in a normally distributed data. In this case, we are taking datapoints with low average_glucose_level as 0 and the higher value as 1. Since, the healthy population has agl < 140, it understandable that maximum datapoints fall in 0.","metadata":{}},{"cell_type":"code","source":"# Using bar plot to understand categorical variables\n\nf, axes = plt.subplots(3, 2, sharey = True, figsize =(10,10))\ndata_cat = pd.concat([y, x_cat], axis=1)\n                          \n#gender\ndata_g = data[['stroke','gender']].value_counts().reset_index()\nsns.barplot(x=\"gender\", y=0, hue=\"stroke\", data=data_g, ax=axes[0, 0])\n       \n#hypertension\ndata_h = data[['stroke','hypertension']].value_counts().reset_index()\nsns.barplot(x=\"hypertension\", y=0, hue=\"stroke\", data=data_h,ax=axes[0, 1])\n                          \n#heart_disease\ndata_hd = data[['stroke','heart_disease']].value_counts().reset_index()\nsns.barplot(x=\"heart_disease\", y=0, hue=\"stroke\", data=data_hd, ax=axes[1, 0])\n\n#ever_married\ndata_em = data[['stroke','ever_married']].value_counts().reset_index()\nsns.barplot(x=\"ever_married\", y=0, hue=\"stroke\", data=data_em, ax=axes[1, 1])\n       \n#work_type\ndata_wt = data[['stroke','work_type']].value_counts().reset_index()\nsns.barplot(x=\"work_type\", y=0, hue=\"stroke\", data=data_wt,ax=axes[2, 0])\n                          \n#smoking_status\ndata_ss = data[['stroke','smoking_status']].value_counts().reset_index()\nsns.barplot(x=\"smoking_status\", y=0, hue=\"stroke\", data=data_ss, ax=axes[2, 1])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:43.499641Z","iopub.execute_input":"2021-06-30T19:42:43.499871Z","iopub.status.idle":"2021-06-30T19:42:44.313782Z","shell.execute_reply.started":"2021-06-30T19:42:43.499845Z","shell.execute_reply":"2021-06-30T19:42:44.312603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = data.stroke\nx_pp = data.drop(['stroke'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:44.315664Z","iopub.execute_input":"2021-06-30T19:42:44.316003Z","iopub.status.idle":"2021-06-30T19:42:44.323032Z","shell.execute_reply.started":"2021-06-30T19:42:44.31597Z","shell.execute_reply":"2021-06-30T19:42:44.321616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Encoding categarical variables\nfrom sklearn.preprocessing import LabelEncoder\n\nlabelEncoder = LabelEncoder()\nx_pp['gender'] = labelEncoder.fit_transform(x_pp['gender'])\nx_pp[\"ever_married\"] = labelEncoder.fit_transform(x_pp['ever_married'])\nx_pp['work_type'] = labelEncoder.fit_transform(x_pp['work_type'])\nx_pp['Residence_type'] = labelEncoder.fit_transform(x_pp['Residence_type'])\nx_pp['smoking_status'] = labelEncoder.fit_transform(x_pp['smoking_status'])\nx_pp['heart_disease'] = labelEncoder.fit_transform(x_pp['heart_disease'])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:44.324679Z","iopub.execute_input":"2021-06-30T19:42:44.325013Z","iopub.status.idle":"2021-06-30T19:42:44.350489Z","shell.execute_reply.started":"2021-06-30T19:42:44.324981Z","shell.execute_reply":"2021-06-30T19:42:44.34951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Collinearity","metadata":{}},{"cell_type":"code","source":"#x_pp = pd.concat([x_cont, x_cat], axis=1)\n\n\n#Dropping avg_glucose_level, since gmm attribute gives a better understanding of avg_glucose_level\nx_pp = x_pp.drop(['avg_glucose_level'], axis=1)\n\nx_pp.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:44.351599Z","iopub.execute_input":"2021-06-30T19:42:44.351879Z","iopub.status.idle":"2021-06-30T19:42:44.407462Z","shell.execute_reply.started":"2021-06-30T19:42:44.351852Z","shell.execute_reply":"2021-06-30T19:42:44.40628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking collinearity using heatmap\nf,ax = plt.subplots(figsize=(8,8))\nsns.heatmap(x_pp.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:44.408712Z","iopub.execute_input":"2021-06-30T19:42:44.409043Z","iopub.status.idle":"2021-06-30T19:42:44.979373Z","shell.execute_reply.started":"2021-06-30T19:42:44.409015Z","shell.execute_reply":"2021-06-30T19:42:44.978244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that age and marriage status are correlated, since, legally only those above age 18 can marry.","metadata":{}},{"cell_type":"code","source":"x_pp = x_pp.drop(['ever_married'], axis =1)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:44.982189Z","iopub.execute_input":"2021-06-30T19:42:44.98242Z","iopub.status.idle":"2021-06-30T19:42:44.987792Z","shell.execute_reply.started":"2021-06-30T19:42:44.982396Z","shell.execute_reply":"2021-06-30T19:42:44.986795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Performing SMOTE sampling","metadata":{}},{"cell_type":"code","source":"import smote_variants as sv","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:42:44.989771Z","iopub.execute_input":"2021-06-30T19:42:44.990063Z","iopub.status.idle":"2021-06-30T19:42:45.006844Z","shell.execute_reply.started":"2021-06-30T19:42:44.990038Z","shell.execute_reply":"2021-06-30T19:42:45.005358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(y_sm)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:44:30.110392Z","iopub.execute_input":"2021-06-30T19:44:30.110738Z","iopub.status.idle":"2021-06-30T19:44:30.117529Z","shell.execute_reply.started":"2021-06-30T19:44:30.110709Z","shell.execute_reply":"2021-06-30T19:44:30.116641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_smote, y_smote= x_pp.values, y.values\noversampler= sv.MulticlassOversampling(sv.distance_SMOTE())\nX_samp, y_samp= oversampler.sample(X_smote, y_smote)\nX_df_smote = pd.DataFrame(X_samp, columns = x_pp.columns)\ny_sm = pd.Series(y_samp)\ny_sm = y_sm.rename('stroke')\nplot_stroke_count(y_sm)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:44:40.783531Z","iopub.execute_input":"2021-06-30T19:44:40.783907Z","iopub.status.idle":"2021-06-30T19:44:41.053628Z","shell.execute_reply.started":"2021-06-30T19:44:40.783866Z","shell.execute_reply":"2021-06-30T19:44:41.052354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using SMOTE, we have equal levels of majority and minority classes.","metadata":{}},{"cell_type":"markdown","source":"# Random Forest Classification with Cross Validation","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFECV","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:46:59.900977Z","iopub.execute_input":"2021-06-30T19:46:59.901625Z","iopub.status.idle":"2021-06-30T19:46:59.906256Z","shell.execute_reply.started":"2021-06-30T19:46:59.901551Z","shell.execute_reply":"2021-06-30T19:46:59.905453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rfecv_model(x_pp, y):\n    x_train, x_test, y_train, y_test = train_test_split(x_pp, y, \n                                                        test_size=0.3, random_state=42)\n    clf_rfecv = RandomForestClassifier() \n    rfecv = RFECV(estimator=clf_rfecv, step=1, cv=5,scoring='accuracy') \n    rfecv = rfecv.fit(x_train, y_train)\n\n    print('Optimal number of features :', rfecv.n_features_)\n    print('Best features :', x_train.columns[rfecv.support_])\n\n    plt.figure()\n    plt.xlabel(\"Number of features selected\")\n    plt.ylabel(\"Cross validation score of number of selected features\")\n    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n    plt.show()\n    \n    x_train_rfecv = rfecv.transform(x_train)\n    x_test_rfecv = rfecv.transform(x_test)\n\n    rfecv_1 = rfecv.fit(x_train_rfecv,y_train)\n    y_pred_rfecv = rfecv_1.predict(x_test_rfecv)\n    ac = accuracy_score(y_test,y_pred_rfecv)\n    print('Accuracy is: ',ac)\n    cm = confusion_matrix(y_test,y_pred_rfecv)\n    sns.heatmap(cm,annot=True,fmt=\"d\")\n    print(classification_report(y_test, y_pred_rfecv))\n    return y_test,y_pred_rfecv","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:47:01.293777Z","iopub.execute_input":"2021-06-30T19:47:01.294192Z","iopub.status.idle":"2021-06-30T19:47:01.301113Z","shell.execute_reply.started":"2021-06-30T19:47:01.294152Z","shell.execute_reply":"2021-06-30T19:47:01.300325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ytest, ypred = rfecv_model(X_train_res, y_train_res)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:47:03.880511Z","iopub.execute_input":"2021-06-30T19:47:03.880927Z","iopub.status.idle":"2021-06-30T19:47:57.764442Z","shell.execute_reply.started":"2021-06-30T19:47:03.880902Z","shell.execute_reply":"2021-06-30T19:47:57.76352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc_auc_model(ytest, ypred)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T19:47:57.76573Z","iopub.execute_input":"2021-06-30T19:47:57.766022Z","iopub.status.idle":"2021-06-30T19:47:57.895335Z","shell.execute_reply.started":"2021-06-30T19:47:57.765993Z","shell.execute_reply":"2021-06-30T19:47:57.894358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"AUC score > 0.9 indicates that the model is accurate in prediction of values. Overall, comparing the accuracy, prediction, recall and AUC score, the model's performance can be categorized as outstanding.\n\nHowever, on the other hand, it is important to note that such metrics can also be due to over-fitting.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"Various sampling methods are available and its applications depend on the type and severity of imbalanced dataset. Thus, its always important to analyse the dataset prior to choosing an appropriate sampling method.\n\nMost importantly, accuray of a model **doesnt** indicate a model's excellence. It can only be determined from a consolidated analysis of all evaluation parameters. ","metadata":{}},{"cell_type":"markdown","source":"**Your advise/feedback is highly appreciated. Do upvote and support!!**","metadata":{}}]}