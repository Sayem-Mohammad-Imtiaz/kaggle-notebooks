{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hello! My name is Laith. We are going to work on a machine learning (ML) project from beginning to end :) We will be using Pandas and Sklearn for the most part, both are very useful for ML/data analysis. Since we are using Pandas and Sklearn, yes you have guessed it, we will by coding in Python! "},{"metadata":{},"cell_type":"markdown","source":"Steps we will go through:\n\n- Looking at the big picture. What are we trying to accomplish? \n- Get the data \n- Play around with the data to get a better understanding of it \n- Clean the data \n- Selecting our model to train \n- evaluate our models performance \n- present our solution \n\nOkay, let's begin! "},{"metadata":{},"cell_type":"markdown","source":"We are hired by a university that wants us to build a model that will help predict if a student will get work placement or not. For the sake of simplicity, they dont give us any further information, so we are free to approach this in any way we want. "},{"metadata":{},"cell_type":"markdown","source":"Since we will be dealing with labeled training examples where every instance comes with expected output, this will be a supervised learning task. It is a classification task as well since we aiming to classify if a studnet will get placement or not. \n\nOkay, let's get the data!"},{"metadata":{},"cell_type":"markdown","source":"The data has been downloaded from Kaggle (https://www.kaggle.com/benroshan/factors-affecting-campus-placement?select=Placement_Data_Full_Class.csv). I have it in my directory so we will start by using Pandas to load the data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# ALL OF OUR IMPORTS WILL GO RIGHT HERE IN THIS CELL \nimport os \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FUNCTION TO LOAD OUR DATA \n\ndef load_dataset():\n    csv_path = os.path.join(\"../input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv\")\n    return pd.read_csv(csv_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LOADING THE DATASET \nplacement = load_dataset()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OKAY! So, we looking at the big picture and now we have the data. \n\nNext step? Lets get a better understanding of the data. We will start of by looking at the dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# looking at the database placement \nplacement","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What are we looking at? \n\nThis table is 215x15. Meaning that there are 215 students (instances) and 15 different columns (attributes). \n\nThe columns: \n- sl_no: serial number \n- gender: gender \n- ssc_p: secondary school percentage (grade 10)\n- ssc_b: secondary school board \n- hsc_p: higher secondary school percentage (grade 11&12)\n- hsc_b: higher secondary school board \n- hsc_s: specialization in higher secondary school \n- degree_p: degree percentage \n- degree_t: undergrad degree \n- workex: work experience \n- etest_p: employability test percentage\n- specialisation: Postgrad degree \n- mba_p: MBA percentage \n- status: if they are placed or not \n- salary: salary \n\nWe can get more info by simply calling .info() to get a better understanding of the attributes. And .describe() to see more numarical summary of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# database info\nplacement.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# database decription \nplacement.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What does .info() and .describe() tell us? \n\nWe can see from .info() that the salary attribute has 148 instances which is less that 215. Thats not okay so we will need to take care of that later. Just something to notice. \n\n.describe(), as you can see gives us a couple things. We wont go through all of them as they are self explanatory. 25%, 50% and 75% correspond to the percentiles. For example, 25% of students have degree_p lower than 61. "},{"metadata":{},"cell_type":"markdown","source":"Lets use the matplotlib to now get a better understanding of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# histograms of each numeric attribute \nplacement.hist(bins=50, figsize=(20,15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that ssc_p, hsc_p, degree_p, and mba_p are fairly bell shaped. Meaning that they have a normal distribution where points are as likely to happen on one side of the average as on the other side. The salary attribute is clearly a right-skewed histogram. This is expected as most salaries tend to be close to be the same or similar and only a few will end up having a salary that is very high. "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nc=placement['status'].values.copy()\nc[c==\"Not Placed\"]=0\nc[c==\"Placed\"]=1\nplacement.plot(kind='scatter',y='hsc_p',x='degree_p',s='mba_p',c=c,cmap=plt.get_cmap(\"jet\"), colorbar=True)\nplt.legend([\"Not Placed\",\"Placed\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # confusion matrix -- focusing on the true negatives \ndef score(y, y_pred):\n    return precision_score(y, y_pred), recall_score(y, y_pred), f1_score(y, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets split the dataset into training and testing sets. There a multiple ways to do this. \n\nWe will use the sklearn stratify method. \n\ndocumentation for StratifiedShuffleSplit -- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html "},{"metadata":{"trusted":true},"cell_type":"code","source":"strat = StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for train_index, test_index in strat.split(placement, placement['status']):\n    strat_train = placement.loc[train_index]\n    strat_test = placement.loc[test_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = strat_train.drop(\"status\", axis=1)\ny_train = strat_train[\"status\"]\nx_test = strat_test.drop(\"status\", axis=1)\ny_test = strat_test[\"status\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are now going to use Transformation Piplines, comes from sklearn. \n\nWe need to fill in the missing salary values (we saw this earlier), and change all categorical attributes to numarical since models learn best that way. \n\nEssentially what Piplines are, is that they hand the Transformation of each attribute to the right values. \n\nDocumentation -- https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html "},{"metadata":{"trusted":true},"cell_type":"code","source":"num_attributes = [\n    \"sl_no\",\n    \"ssc_p\",\n    \"hsc_p\",\n    \"degree_p\",\n    \"etest_p\",\n    \"mba_p\",\n    \"salary\"\n]\n\ncat_attributes = [\n    \"gender\",\n    \"ssc_b\",\n    \"hsc_b\",\n    \"hsc_s\",\n    \"degree_t\",\n    \"workex\",\n    \"specialisation\"\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_pipline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    ('std_scalar', StandardScaler())\n])\n\npipline = ColumnTransformer([\n    (\"num\", num_pipline, num_attributes),\n    (\"cat\", OneHotEncoder(), cat_attributes)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"proccessed_train_x = pipline.fit_transform(x_train)\nproccessed_test_x = pipline.fit_transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_text_to_num = {\n    \"status\": {\"Placed\": 0, \"Not Placed\": 1}\n}\n\ny_train = y_train.to_frame()\ny_test = y_test.to_frame()\n\n\nproccessed_train_y = y_train.replace(y_text_to_num)\nproccessed_test_y = y_test.replace(y_text_to_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"proccessed_train_y = proccessed_train_y[\"status\"].values\nproccessed_test_y = proccessed_test_y[\"status\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FINAL_X = pipline.fit_transform(placement.drop(\"status\", axis=1))\nFINAL_Y = placement[\"status\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split = StratifiedKFold(n_splits=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The models we will use: \n\n- Logistic Regression \n- Decision Tree \n- Gaussian Naive Bayes\n- Random Forest Classifier \n- K Nearest Neighbors Classifier\n- Support Vector Machine Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg = LogisticRegression()\nlog_reg.fit(proccessed_train_x, proccessed_train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_pred_y = log_reg.predict(proccessed_test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_reg = DecisionTreeClassifier()\ntree_reg.fit(proccessed_train_x, proccessed_train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_pred_y = tree_reg.predict(proccessed_test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gau_naiv_bay = GaussianNB()\ngau_naiv_bay.fit(proccessed_train_x, proccessed_train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb_pred_y = gau_naiv_bay.predict(proccessed_test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ran_for_cla = RandomForestClassifier()\nran_for_cla.fit(proccessed_train_x, proccessed_train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_pred_y = ran_for_cla.predict(proccessed_test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_near_nei = KNeighborsClassifier()\nk_near_nei.fit(proccessed_train_x, proccessed_train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_pred_y = k_near_nei.predict(proccessed_test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sup_vec_mac = SVC()\nsup_vec_mac.fit(proccessed_train_x, proccessed_train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_pred_y = sup_vec_mac.predict(proccessed_test_x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets show the scores for each model using the score() we built earlier. "},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [\n    (\"Logistic Regression\", lr_pred_y),\n    (\"Decision Tree Classifier\", tr_pred_y),\n    (\"Gaussian Naive Bayes\", gnb_pred_y),\n    (\"Random Forest Classifier\", rfc_pred_y),\n    (\"K Nearest Neighbors Classifier\", knn_pred_y),\n    (\"Support Vector Machine Classifier\", svm_pred_y)\n]\n\nfor info in models:\n    model = info[0]\n    y_pred = info[1]\n    print(model)\n    print(\"These are the predicted values from the model: \", y_pred)\n    print(\"These are the correct output values:           \", proccessed_test_y)\n    print(\"The score for this model (precision, recall, f1_score): \", score(proccessed_test_y, y_pred))\n    print(\"\\n\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This part is just to show how a confusion_matrix works.\n\n\nNow, for our data science metrics, we will be using the confusion_matrix. It does take a bit of time to wrap your head around it. So make sure you visit this line (https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html) to get a better understanding. Or just YouTube it. YouTube + Google is everything!\n\nEssentially, the confusion_matrix tells us the true positives, false positives, true negatives, and false negatives. "},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [\n    (\"Logistic Regression\", lr_pred_y),\n    (\"Decision Tree Classifier\", tr_pred_y),\n    (\"Gaussian Naive Bayes\", gnb_pred_y),\n    (\"Random Forest Classifier\", rfc_pred_y),\n    (\"K Nearest Neighbors Classifier\", knn_pred_y),\n    (\"Support Vector Machine Classifier\", svm_pred_y)\n]\n\nfor info in models:\n    model = info[0]\n    y_pred = info[1]\n    print(model)\n    print(\"The confusion matrix is:  \\n\", confusion_matrix(proccessed_test_y, y_pred))\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we take Random Forest Classifier for example. Its confusion matrix has the least amount of false positives and false negatives, and highest amount of true positives and true negatives. Hence when you look at the scoring, RFC has the highest score at about 0.95. "},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}