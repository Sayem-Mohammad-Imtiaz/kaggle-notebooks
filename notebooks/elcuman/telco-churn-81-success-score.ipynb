{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this project, a study will be conducted on classification from Data Mining predictive methods. Our application will be on a telecommunications company"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndf=pd.read_csv(\"../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\",sep=',',decimal='.')\ndf.head(10000)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our data set consists of 7043 rows and 21 columns."},{"metadata":{},"cell_type":"markdown","source":"Let's remove the CostumerID (Customer ID) part from our data set because it is unnecessary."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('customerID', axis=1, inplace=True)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's categorize the data of the elder column."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"SeniorCitizen\"]= df[\"SeniorCitizen\"].replace(0, \"No\") \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The \"Total Payout\" part is specified as object when it should be float. Let's fix that."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"SeniorCitizen\"]= df[\"SeniorCitizen\"].replace(1, \"Yes\") \ndf['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\ndf['TotalCharges'] = df['TotalCharges'].fillna(value=0)\ndf['SeniorCitizen'] = df['SeniorCitizen'].astype('object')\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"df['SeniorCitizen'] = df['SeniorCitizen'].astype('object')\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of lost and missing customers was visualized by creating a box chart of lost customers."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = \"Churn\", data = df)\ndf.loc[:, 'Churn'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no missing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data are categorically and numerically classified. Our target variable, Loss, will not be included in categorical fields."},{"metadata":{"trusted":true},"cell_type":"code","source":"Categorical = df.select_dtypes(include='object').drop('Churn', axis=1).columns.tolist()\nnumerical = df.select_dtypes(exclude='object').columns.tolist()\nfor c in Categorical:\n    print('Column {} unique values: {}'.format(c, len(df[c].unique())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's observe outlier data"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsns.boxplot(x=df['tenure'],y=df['Churn'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" sns.boxplot(x=df['TotalCharges'],y=df['Churn'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We often see contrary data in the part of the total payment where customer loss occurs. In fact, even this situation shows us why customer losses occur, but let's do a more detailed analysis. Apart from this, some outlier data appear during the subscription period. Now, let's clear my outlier data. First of all, the data will be completely digitized with labelencoder."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nencoded = df.apply(lambda x: LabelEncoder().fit_transform(x) if x.dtype == 'object' else x)\nencoded.head(8000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"customerlost=encoded.loc[encoded['Churn'].abs()>0]\ncustomerlost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1 = customerlost['TotalCharges'].quantile(0.25)\nQ3 = customerlost['TotalCharges'].quantile(0.75)\nIQR = Q3 - Q1\nIQR","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"Q=Q3+(1.5*IQR)\nQ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"encoded_out = encoded[~((encoded['TotalCharges'] < (Q3 + 1.5 * IQR)))&(encoded['Churn']>0)]\nencoded_out.head(8000)\nencoded.drop(encoded[~((encoded['TotalCharges'] < (Q3 + 1.5 * IQR)))&(encoded['Churn']>0)].index, inplace=True)\nencoded.head(5000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Outlier data in the total payment section has been deleted. During Subscription Period."},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1_A = customerlost['tenure'].quantile(0.25)\nQ3_A = customerlost['tenure'].quantile(0.75)\nIQR_A = Q3_A - Q1_A\nIQR_A","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q_A=Q3_A+(1.5*IQR_A)\nQ_A\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data contrary to the subscription period was brought."},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_A_out = encoded[~((encoded['tenure'] < (Q3_A + 1.5 * IQR_A)))&(encoded['Churn']>0)]\nencoded_A_out.head(8000)\nencoded.drop(encoded[~((encoded['tenure'] < (Q3_A + 1.5 * IQR_A)))&(encoded['Churn']>0)].index, inplace=True)\nencoded.head(8000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preparation of Test and Training DataÂ¶\nAt this stage, it is the process of dividing the data, whose target variable is defined and which takes its final form before the algorithm, into test and training. The separation threshold suitable for this process was found to be 85-15.\n\nFor the sake of a general demonstration, I will show the separation of test and training data below. These steps will be applied after performing operations according to the test before applying the test algorithms."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.drop('Churn', axis = 1)              \ny = df['Churn'] \nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.85, random_state = 400)\nx_test.head(8000)\nx_train.head(8000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.head(5000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.head(500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Application of Classification Algorithms and Performance Analysis**\nWe will predict customer losses with classification algorithms. In our application, four classification algorithms will be applied to the data and the performance outputs will be analyzed. These performance outputs will be compared in section 5 and the classification algorithm to be used will be determined.\nThese four classification algorithms are;\nLogistic Regression\nNaive Bayes\nDecision Tree\nK-NN (Nearest Neighbor)\n\n**Logistic Regression**\nLogistic regression is valid when the output variable takes discrete values.\nSince our data has columns with more than two options, we will use multiple logistic regression.\n\nBefore starting logistic regression, let's do a little correlation analysis and throw out the columns that don't work for us."},{"metadata":{"trusted":true},"cell_type":"code","source":"x=encoded['gender']\ny=encoded['Churn']\nprint('gender:', x.corr(y)*100)\nx=encoded['SeniorCitizen']\ny=encoded['Churn']\nprint('SeniorCitizen:', x.corr(y)*100)\nx=encoded['Partner']\ny=encoded['Churn']\nprint('Partner:', x.corr(y)*100)\nx=encoded['Dependents']\ny=encoded['Churn']\nprint('Dependents:', x.corr(y)*100)\nx=encoded['PhoneService']\ny=encoded['Churn']\nprint('PhoneService:', x.corr(y)*100)\nx=encoded['MultipleLines']\ny=encoded['Churn']\nprint('MultipleLines:', x.corr(y)*100)\nx=encoded['tenure']\ny=encoded['Churn']\nprint('tenure:', x.corr(y)*100)\nx=encoded['InternetService']\ny=encoded['Churn']\nprint('InternetService:', x.corr(y)*100)\nx=encoded['OnlineSecurity']\ny=encoded['Churn']\nprint('OnlineSecurity:', x.corr(y)*100)\nx=encoded['OnlineBackup']\ny=encoded['Churn']\nprint('OnlineBackup:', x.corr(y)*100)\nx=encoded['DeviceProtection']\ny=encoded['Churn']\nprint('DeviceProtection:', x.corr(y)*100)\nx=encoded['TechSupport']\ny=encoded['Churn']\nprint('TechSupport:', x.corr(y)*100)\nx=encoded['StreamingTV']\ny=encoded['Churn']\nprint('StreamingTV:', x.corr(y)*100)\nx=encoded['StreamingMovies']\ny=encoded['Churn']\nprint('StreamingMovies:', x.corr(y)*100)\nx=encoded['Contract']\ny=encoded['Churn']\nprint('Contract:', x.corr(y)*100)\nx=encoded['PaperlessBilling']\ny=encoded['Churn']\nprint('PaperlessBilling:', x.corr(y)*100)\nx=encoded['MonthlyCharges']\ny=encoded['Churn']\nprint('MonthlyCharges:', x.corr(y)*100)\nx=encoded['MonthlyCharges']\ny=encoded['Churn']\nprint('MonthlyCharges:', x.corr(y)*100)\nx=encoded['TotalCharges']\ny=encoded['Churn']\nprint('TotalCharges:', x.corr(y)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded.drop('gender', axis=1, inplace=True)\nencoded.drop('PhoneService', axis=1, inplace=True)\nencoded.drop('MultipleLines', axis=1, inplace=True)\nencoded.drop('InternetService', axis=1, inplace=True)\nencoded.drop('StreamingTV', axis=1, inplace=True)\nencoded.drop('StreamingMovies', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We dropped the columns that were less than 10 as a result of the correlation. As a result of the correlation, we see the effect of the contract period and subscription period on customer loss."},{"metadata":{"trusted":true},"cell_type":"code","source":"x=encoded.drop('Churn',axis=1)\ny=encoded['Churn']\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.85, random_state=43)\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nsc=StandardScaler()\nx_train=sc.fit_transform(x_train)\nx_test=sc.fit_transform(x_test)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Multinomial logistic compression only supports newton-cg and lbfgs as solver. In this case, penalty = l2 is a mandatory option."},{"metadata":{"trusted":true},"cell_type":"code","source":"Logistic_Regression = LogisticRegression(C=0.5,tol=0.1,multi_class='multinomial',solver='newton-cg',penalty='l2',max_iter=100)\nLogistic_Regression.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=Logistic_Regression.predict(x_test)\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score\nclassification_report(y_true=y_test, y_pred=y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test, y_pred)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Decision Tree**\nClassification is a classification method that creates a model in the form of a tree structure consisting of decision nodes and leaf nodes according to feature and target."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import KBinsDiscretizer  \nest = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='quantile')\nencoded['tenure'] = est.fit_transform(encoded['tenure'].values.reshape(-1,1))\nfrom sklearn.preprocessing import KBinsDiscretizer  \nest = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='quantile')\nencoded['MonthlyCharges'] = est.fit_transform(encoded['MonthlyCharges'].values.reshape(-1,1))\nfrom sklearn.preprocessing import KBinsDiscretizer  \nest = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='quantile')\nencoded['TotalCharges'] = est.fit_transform(encoded['TotalCharges'].values.reshape(-1,1))\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import confusion_matrix as Confusion_Matrix\n\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.85, random_state=42)\ntree_Decision= DecisionTreeClassifier(max_depth = 4, random_state=42)\ntree_Decision.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = tree_Decision.predict(x_test)\nscore = round(accuracy_score(y_test, predictions), 2)\nConfusion_Matrix = Confusion_Matrix(y_test, predictions)\nsns.heatmap(Confusion_Matrix, annot=True, fmt=\".0f\")\nplt.xlabel('estimated value')\nplt.ylabel('real value')\nplt.title('Score: {0}'.format(score), size = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, predictions, target_names=['Not Lost customer', 'Lost customer']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\nclf = tree.DecisionTreeClassifier(max_depth = 4, random_state=42,min_weight_fraction_leaf=0.0)\nclf = clf.fit(x, y)\ntree.plot_tree(clf,fontsize=10) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**KNN (Nearest Neighbor)****\n\nIt is based on the principle of choosing k value distances, which is an observation value determined later as a parameter, and k number of observations with the smallest distance.\n\nThe method used to best assign the K value."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nerror = []\n\n# Calculating error for K values between 1 and 40\nfor i in range(1, 40):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train, y_train)\n    pred_i = knn.predict(x_test)\n    error.append(np.mean(pred_i != y_test))\nplt.figure(figsize=(12, 6))\nplt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',markerfacecolor='blue', markersize=10)\nplt.title('Fail rate for K ')\nplt.xlabel('K value')\nplt.ylabel('Average Error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = knn.predict(x_test)\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Naive Bayes**\n\nThe way the algorithm works calculates the probability of each state for an element and classifies it according to the one with the highest probability value.\nNaive Bayes is divided into 3 groups;\nGaussianNB\nMultinomialNB\nBernoulliNB"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nNBG = GaussianNB()\nNBG.fit(x_train, y_train)\ny_forcast = NBG.predict(x_test)\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.85, random_state=42)\nprint(\"x_train: \", x_train.shape)\nprint(\"x_test: \", x_test.shape)\nprint(\"y_train: \", y_train.shape)\nprint(\"y_test: \", y_test.shape)\nprint(\"Navy Bayes Gaussian Score :\",accuracy_score(y_test, y_forcast))\nprint(\"Confusion Matrix :\",confusion_matrix(y_test, y_forcast))\nprint(\"Classification Report :\",classification_report(y_test, y_forcast))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nNBM = MultinomialNB()\nNBM.fit(x_train, y_train)\ny_forcast = NBM.predict(x_test)\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.85, random_state=42)\nprint(\"x_train: \", x_train.shape)\nprint(\"x_test: \", x_test.shape)\nprint(\"y_train: \", y_train.shape)\nprint(\"y_test: \", y_test.shape)\nprint(\"Navy Bayes Multinomial Score :\",accuracy_score(y_test, y_forcast))\nprint(\"Confusion Matrix :\",confusion_matrix(y_test, y_forcast))\nprint(\"Classification Report :\",classification_report(y_test, y_forcast))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nNBB = BernoulliNB()\nNBB.fit(x_train, y_train)\ny_forcast = NBB.predict(x_test)\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.85, random_state=42)\nprint(\"x_train: \", x_train.shape)\nprint(\"x_test: \", x_test.shape)\nprint(\"y_train: \", y_train.shape)\nprint(\"y_test: \", y_test.shape)\nprint(\"Navy Bayes Bernoulli Score :\",accuracy_score(y_test, y_forcast))\nprint(\"Confusion Matrix :\",confusion_matrix(y_test, y_forcast))\nprint(\"Classification Report :\",classification_report(y_test, y_forcast))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Comparison of the Performance of Classification AlgorithmsÂ¶**\nThe Performance of the Classification Model can be measured in detail over the Confusion Matrix.\nHowever, we will only use the auc-roc chart."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nx, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\ntrainx, testx, trainy, testy = train_test_split(x, y, test_size=0.85, random_state=42)\nknn_forcast = [0 for _ in range(len(testy))]\nNBG_forcast = [0 for _ in range(len(testy))]\ntree_Decision_forcast = [0 for _ in range(len(testy))]\nLogistic_Regression_forcast = [0 for _ in range(len(testy))]\n\nmodel = NBG\nmodel.fit(trainx, trainy)\nmodel2 = knn\nmodel2.fit(trainx, trainy)\nmodel3=tree_Decision\nmodel3.fit(trainx, trainy)\nmodel4=Logistic_Regression\nmodel4.fit(trainx, trainy)\n\nNBG_forcast = model.predict_proba(testx)\nknn_forcast = model2.predict_proba(testx)\ntree_Decision_forcast = model3.predict_proba(testx)\nLogistic_Regression_forcast= model4.predict_proba(testx)\n\nNBG_forcast = NBG_forcast[:, 1]\nknn_forcast = knn_forcast[:, 1]\ntree_Decision_forcast = tree_Decision_forcast[:, 1]\nLogistic_Regression_forcast = Logistic_Regression_forcast[:, 1]\n\nKNN_sensitivity = roc_auc_score(testy, knn_forcast)\nNBG_sensitivity = roc_auc_score(testy, NBG_forcast)\ntree_Decision_sensitivity= roc_auc_score(testy, tree_Decision_forcast)\nLogistic_Regression_sensitivity = roc_auc_score(testy, Logistic_Regression_forcast)\n\nprint('KNN: ROC AUC=%.3f' % (KNN_sensitivity))\nprint('Navy Bayes Gaussian: ROC AUC=%.3f' % (NBG_sensitivity))\nprint('Desicion_tree: ROC AUC=%.3f' % (tree_Decision_sensitivity))\nprint('Logistic Regresyon: ROC AUC=%.3f' % (Logistic_Regression_sensitivity))\n\nKNN_fpr, KNN_tpr, _ = roc_curve(testy, knn_forcast)\nNBG_fpr, NBG_tpr, _ = roc_curve(testy, NBG_forcast)\ntree_Decision_fpr, tree_Decision_tpr, _ = roc_curve(testy, tree_Decision_forcast)\nLogistic_Regression_fpr, Logistic_Regression_tpr, _ = roc_curve(testy, Logistic_Regression_forcast)\n\npyplot.plot(KNN_fpr, KNN_tpr, linestyle='--', label='KNN')\npyplot.plot(NBG_fpr, NBG_tpr, marker='.', label='Navy Bayes Gaussian')\npyplot.plot(tree_Decision_fpr, tree_Decision_tpr, marker='.', label='tree Decision')\npyplot.plot(Logistic_Regression_fpr, Logistic_Regression_tpr, marker='.', label='Logistic Regresyon')\n\npyplot.xlabel('customer loss')\npyplot.ylabel('no customer loss')\n\npyplot.legend()\n\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Final Report\nIt is possible to predict customer loss using various analyzes and thus to warn the business before customer loss occurs.\nThe most suitable algorithm chosen for the database we analyze is Logistic Regression.\nAs a result of our logistic regression analysis, we achieved a successful prediction score of 81.3%.\nThe main factor that causes customer loss is the height of the total payments.\nTotal payments are followed by subscription period and monthly payments. We can say that customers who do not receive online security and technical support have a higher loss rate.****"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}