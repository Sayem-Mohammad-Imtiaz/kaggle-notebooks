{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"data =pd.read_csv(r\"../input/mushroom-classification/mushrooms.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['class'].unique()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus we have two claasification. Either the mushroom is poisonous or edible\n\n","metadata":{}},{"cell_type":"code","source":"data.shape\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus we have 22 features(1st one is label) and 8124 instances.Now let us check which features constitutes maximum information.","metadata":{}},{"cell_type":"markdown","source":"We can see that the dataset has values in strings.We need to convert all the unique values to integers. Thus we perform label encoding on the data.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabelencoder=LabelEncoder()\nfor col in data.columns:\n    data[col] = labelencoder.fit_transform(data[col])\n \ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking the encoded values","metadata":{}},{"cell_type":"code","source":"data['stalk-color-above-ring'].unique()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.groupby('class').size())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Separating features and label\n\n","metadata":{}},{"cell_type":"code","source":"X = data.iloc[:,1:23]  # all rows, all the features and no labels\ny = data.iloc[:, 0]  # all rows, label only\nX.head()\ny.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.corr()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Standardising the data\n# ","metadata":{}},{"cell_type":"code","source":"# Scale the data to be between -1 and 1\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX=scaler.fit_transform(X)\nX","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting the data into training and testing dataset\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Default Logistic Regression\n# ","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\n\nmodel_LR= LogisticRegression()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_LR.fit(X_train,y_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_prob = model_LR.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \ny_pred = np.where(y_prob > 0.5, 1, 0) # This will threshold the probabilities to give class predictions.\nmodel_LR.score(X_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix=metrics.confusion_matrix(y_test,y_pred)\nconfusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"auc_roc=metrics.roc_auc_score(y_test,y_pred)\nauc_roc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],linestyle='--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Recursive feature elimination\nresource: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html","metadata":{}},{"cell_type":"code","source":"X= data.drop(columns='class')\nY=data['class']\nY=pd.DataFrame(Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.preprocessing import MinMaxScaler\nX_norm = MinMaxScaler().fit_transform(X)\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nrfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=10, step=10, verbose=5)\nrfe_selector.fit(X_norm,Y)\nrfe_support = rfe_selector.get_support()\nrfe_feature = X.loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'selected features')\nprint(rfe_feature)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CHI2\nhttps://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import MinMaxScaler\nchi_selector = SelectKBest(chi2, k=10)\nchi_selector.fit(X_norm, Y)\nchi_support = chi_selector.get_support()\nchi_feature = X.loc[:,chi_support].columns.tolist()\nprint(str(len(chi_feature)), 'selected features')\nprint(chi_feature)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Embeded\nhttps://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\nembeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l2\"), '1.25*median')\nembeded_lr_selector.fit(X_norm, Y)\nembeded_lr_support = embeded_lr_selector.get_support()\nembeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\nprint(str(len(embeded_lr_feature)), 'selected features')\nprint(embeded_lr_feature)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVD","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=20, n_iter=50, random_state=42)\nsvd.fit(X_norm)\nexplained_variance=svd.explained_variance_ratio_\nprint(explained_variance.shape)\nprint(svd.explained_variance_ratio_.sum())\nwith plt.style.context('dark_background'):\n    plt.figure(figsize=(6, 4))\n\n    plt.bar(range(20), explained_variance, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}