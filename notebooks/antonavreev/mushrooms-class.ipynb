{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"Mushrooms.ipynb\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    https://colab.research.google.com/gist/Trilibom/01355a64ccc790d7e7643b815cfac33f/mushrooms.ipynb\n\nВсем привет! Рассмотрим данные о грибах, предскажем их съедобность, построим корреляцию и многое другое.\n\nВоспользуемся данными о грибах с Kaggle (исходный датафрейм) с  https://www.kaggle.com/uciml/mushroom-classification , 2 дополнительных датафрейма приложу к статье.\n\nВсе операции проделаны на https://colab.research.google.com/notebooks/intro.ipynb\n\"\"\"\n\n# Загружаем библиотекeу для работы с данными\nimport pandas as pd\n\n# для построения леса деревьев решений, обучения моделей и построения confusion_matrix:\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\n\n# для работы с графикой:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Загружаем наш датафрейм\nmushrooms = pd.read_csv('/content/mushrooms.csv')\n\n#Просматриваем наши данные\nmushrooms.head()\n\n#Краткая сводка данных\nmushrooms.info()\n\n#Информация о количестве строк и столбцов\nmushrooms.shape\n\n# Используем кодировщик данных LabelEncoder для преобразования наших категоральных или текстовых данных в числа (обязательно перед heatmap)\n# Если мы этого не сделаем, при обучении дерева у нас возникнет ошибка на этапе его обучения\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nfor i in mushrooms.columns:\n    mushrooms[i]=le.fit_transform(mushrooms[i])\n\n# Посмотрим как преобразовались наши данные\nmushrooms.head()\n\n# Просмотрим корреляцию наших данных с помощью heatmap\nfig = plt.figure(figsize=(18, 14))\nsns.heatmap(mushrooms.corr(), annot = True, vmin=-1, vmax=1, center= 0, cmap= 'coolwarm', linewidths=3, linecolor='black')\nfig.tight_layout()\nplt.show()\n\n\"\"\"Положительно коррелирующие значения:\nСильная корреляция (veil-color,gill-spacing) = +0.9\nСредняя корреляция (ring-type,bruises) = +0.69\nСредняя корреляция (ring-type,gill-color) = +0.63\nСредняя корреляция (spore-print-color,gill-size) = +0.62\nОтрицательно коррелирующие значения\nСредняя корреляция (stalk-root,spore-print-color) = -0.54\nСредняя корреляция (population,gill-spacing) = -0.53\nСредняя корреляция (gill-color,class) = -0.53\nЕсли в нашем иследование возьмем максимально тесно связанные коррелирующие значения, то получим максимально точные значения и точно обученную модель.\nВ нашей задаче мы будем обучать модель по классу, представляя, что аналитик не воспользовался таблицей корреляции.\n\"\"\"\n\n# Отбросим колонку, которую будем предсказывать.\nX = mushrooms.drop(['class'], axis=1)\n# Создадим переменную, которую будем предсказывать.\ny = mushrooms['class']\n\n# Создаем модель RandomForestClassifier.\nrf = RandomForestClassifier(random_state=0)\n\n# Задаем параметры модели, изначально когда мы не знаем оптимальных параметров для обучения леса задаем так\n#{'n_estimators': range(10, 51, 10), 'max_depth': range(1, 13, 2),\n#             'min_samples_leaf': range(1,8), 'min_samples_split': range(2,10,2)}\nparameters = {'n_estimators': [10], 'max_depth': [7],\n              'min_samples_leaf': [1], 'min_samples_split': [2]}\n\n# Обучение Random forest моделей GridSearchCV.\nGridSearchCV_clf = GridSearchCV(rf, parameters, cv=3, n_jobs=-1)\nGridSearchCV_clf.fit(X, y)\n\n# Определение наилучших параметров, и обучаем с ними дерево для получения лучшего уровня обучаемости\nbest_clf = GridSearchCV_clf.best_params_\n\n# Просмотр оптимальных параметров.\nbest_clf\n\n# Считываем второй файл c данными о грибах.\nmushrooms_2 = pd.read_csv('/content/testing_mush.csv')\n\n# Предсказываем съедобность грибов из второго файла, используя созданный наилучший классификатор.\npredictions = GridSearchCV_clf.predict(mushrooms_2)\n\n# выгружаем и подсчитываем грибы по типу 0 съедобные, 1 несъдобные\nmushrooms_type = pd.Series (predictions)\nmushrooms_type.groupby(mushrooms_type).count()\n\n# Создадим атрибут feature_importances и сохраним его в отдельную переменную.\nfeature_importances = GridSearchCV_clf.best_estimator_.feature_importances_\n# и сделаем DataFrame, одна колонка - имена переменных, другая - важность переменных, отсортированные по убыванию.\nfeature_importances_df = pd.DataFrame({'features': list(X), 'feature_importances': feature_importances})\\\n    .sort_values(by='feature_importances', ascending=False)\n\n# Построение графика с важностью атрибутов.\nf, ax = plt.subplots()\nsns.barplot(y=feature_importances_df.features, x=feature_importances_df.feature_importances)\nplt.xlabel('Важность атрибутов')\nplt.ylabel('Атрибуты')\nplt.title('Наиболее важные атрибуты')\nplt.show()\n\n# Создание confusion matrix (матрицу ошибок) по предсказаниям, полученным в прошлом шаге и правильным ответам с нового датасета.\ny_true = pd.read_csv ('/content/testing_y_mush.csv')\nsns.heatmap(confusion_matrix(y_true, predictions), annot=True, cmap=\"Blues\")\nplt.show()\n\n\"\"\"Данная матрица ошибок показывает, что у нас отсутствуют ошибки первого типа, но присутствуют ошибки второго типа в значении 3, что для нашей модели является очень низким показателем стремящемся к 0.\n\nДалее мы проделаем операции для определения модели наилучшей точности нашем дф\n\n\"\"\"\n\n# определим точность нашей модели \nfrom sklearn.metrics import accuracy_score\nmr = accuracy_score(y_true, predictions)\n\n#Данные для тренировки и тестировки датафрейма\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n#Логистическая регрессия\n#Тренируем модель\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(max_iter = 10000)\nlr.fit(x_train,y_train)\n\n#Строим матрицу ошибок\nfrom sklearn.metrics import confusion_matrix,classification_report\ny_pred = lr.predict(x_test)\ncm = confusion_matrix(y_test,y_pred)\n\n\n#Делаем проверку точности\nlog_reg = accuracy_score(y_test,y_pred)\n\n#K ближайших соседей\n#Тренируем модель\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski',p = 2)\nknn.fit(x_train,y_train)\n\n#Создаем матрицу ошибок\nfrom sklearn.metrics import confusion_matrix,classification_report\ny_pred = knn.predict(x_test)\ncm = confusion_matrix(y_test,y_pred)\n\n\n#Делаем проверку точности\nfrom sklearn.metrics import accuracy_score\nknn_1 = accuracy_score(y_test,y_pred)\n\n#Дерево решений\n#Тренируем модель\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(criterion = 'entropy')\ndt.fit(x_train,y_train)\n\n#Создаем матрицу ошибок\nfrom sklearn.metrics import confusion_matrix,classification_report\ny_pred = dt.predict(x_test)\ncm = confusion_matrix(y_test,y_pred)\n\n#Делаем проверку точности\nfrom sklearn.metrics import accuracy_score\ndt_1 = accuracy_score(y_test,y_pred)\n\n#Простой вероятностный классификатор\n#Тренируем модель\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n\n#Создаем матрицу ошибок\nfrom sklearn.metrics import confusion_matrix,classification_report\ny_pred = nb.predict(x_test)\ncm = confusion_matrix(y_test,y_pred)\n\n\n#Делаем проверку точности\nfrom sklearn.metrics import accuracy_score\nnb_1 = accuracy_score(y_test,y_pred)\n\n#Осущевстляем проверку точностей\nplt.figure(figsize= (16,12))\nac = [log_reg,knn_1,nb_1,dt_1,mr]\nname = ['Логистическая регрессия','К ближайших соседей','Простой вероятностный классификатор','Дерево решений', 'Случайные деревья']\nsns.barplot(x = ac,y = name,palette='colorblind')\nplt.title(\"График точностей моделей\", fontsize=20, fontweight=\"bold\")\n\n\"\"\"Мы можем сделать вывод, что наиболее точной моделью для наших предсказаний является дерево решений.\n\n\n\"\"\"","metadata":{"_uuid":"7d847a0f-3b4e-49c3-9c09-b44b624af847","_cell_guid":"9a234e4a-55b6-4b89-90dd-3c2be92a990c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}