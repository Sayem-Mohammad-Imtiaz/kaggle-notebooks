{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#Okay, I was getting ahead of myself.  Let's stick to the assignment\n#the task from day #5 in this tutorial:\n#https://machinelearningmastery.com/crash-course-deep-learning-natural-language-processing/\n\nfrom numpy import array\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Dropout\nimport pandas\n# define documents\n\ndocs = ['You are an annoying, whining man-child',\n        'I am sure the pulse setting on your shower head will be devastated!',\n        'I think your credit card statement would beg to differ.',\n        'Did Santa finally bring you that Y-chromosome you always wanted?',\n        'Should I talk slower or get a nurse who speaks fluent Moron?',\n        'Elmo is so happy to see you! Welcome to Sesame Street!',\n        'Elmo loves to stay nice and clean!',\n        'It’s time to make up a musical!',\n        'Oh look, it’s Mr. Noodle’s brother, Mr. Noodle.',\n        'Elmo loves you!'] #First five are Dr. Cox from Scrubs, last 5 are Elmo from Sesame Street\n# define class labels\nprint('doc type: ', type(docs))\nlabels = array([1,1,1,1,1,0,0,0,0,0])\n# integer encode the documents\nvocab_size = 1000\n\ntokenMaker = Tokenizer()\ntokenMaker.fit_on_texts(docs)\n\nencoded_docs = tokenMaker.texts_to_sequences(docs)\n\nmax_length = 12\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n#print(padded_docs)\n\nprint(\"shapes: \", padded_docs.shape, labels.shape)\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 12, input_length=max_length))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# summarize the model\nprint(model.summary())\n# fit the model\nmodel.fit(padded_docs, labels, epochs=10, verbose=1)\n# evaluate the model\nloss, accuracy = model.evaluate(padded_docs, labels, verbose=1)\nprint('Accuracy: %f' % (accuracy*100))\nprint('Loss/ ', loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I learned a lot from https://rajmak.in/2017/12/07/text-classification-classifying-product-titles-using-convolutional-neural-network-and-word2vec-embedding/\nfrom gensim.models import KeyedVectors\nfrom numpy import array\nimport numpy\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Dropout\n\nEMBEDDING_DIMENSIONS=300\n\ndocs = ['You are an annoying, whining man-child',\n        'I am sure the pulse setting on your shower head will be devastated!',\n        'I think your credit card statement would beg to differ.',\n        'Did Santa finally bring you that Y chromosome you always wanted?',\n        'Should I talk slower or get a nurse who speaks fluent Moron?',\n        'Elmo is so happy to see you! Welcome to Sesame Street!',\n        'Elmo loves to stay nice and clean!',\n        'It’s time to make up a musical!',\n        'Oh look, it’s Mr. Noodle’s brother, Mr. Noodle.',\n        'Elmo loves you!']\n# define class labels\nprint('doc type: ', type(docs))\nlabels = array([1,1,1,1,1,0,0,0,0,0])\n# integer encode the documents\nvocab_size = 1000\n\nword_model = KeyedVectors.load(\"../input/gensim-embeddings-dataset/GoogleNews-vectors-negative300.gensim\")\n\nprint('Model type: ', type(word_model))\nprint('Found %s word vectors of Keyed Vectors' % len(word_model.vocab))\n\n#print('Monkey: ', model['Monkey'], ' shape: ', model['Monkey'].shape)\n\nvectors = [[word for word in line.split()] for line in docs]\nfor badchar in ['?', '!', '.', ',', '’', '-']:\n    vectors = [[d.replace(badchar, '') for d in doc] for doc in vectors] \nprint(vectors)\nfor badword in ['']: #I suppose these were removed from the training set?  \n    for a in range(len(vectors)):   #They were wonky making when I made my own encoding\n        if badword in vectors[a]: \n            vectors[a].remove(badword)\n        if badword in vectors[a]: #This is not the most elegant solution, could've googled it, but remove was only removing the first instance\n            vectors[a].remove(badword)\n            print(a, badword)\nprint(vectors)\n\nmax_length = max(len(words) for words in vectors) + 1\n\nprint(\"Max length: \", max_length)\n\ntokenizer = Tokenizer(num_words = vocab_size)\ntokenizer.fit_on_texts(vectors)\n\nnumeric_sequences = tokenizer.texts_to_sequences(vectors)\n\n#print(numeric_sequences)\n\ntotal_words = len(tokenizer.word_index)\nprint(\"Total unique words: \", total_words)\n\n#encoded_vectors = [[word_model[word] for word in vector] for vector in vectors]\n\n#max_length = 12\npadded_docs = pad_sequences(numeric_sequences, maxlen=max_length, padding='post')\n#print(padded_docs)\n\nprint('shapes: ', padded_docs.shape, labels.shape)\n\nembedding_matrix = numpy.zeros((vocab_size, EMBEDDING_DIMENSIONS))\n\nfor word, i in tokenizer.word_index.items():\n    if word in word_model.vocab:\n        #print(\"word: \", word)\n        embedding_matrix[i] = word_model.word_vec(word)\nprint('Null word embeddings: %d' % numpy.sum(numpy.sum(embedding_matrix, axis=1) == 0))\n\n\nembedding_layer = Embedding(embedding_matrix.shape[0], # or len(word_index) + 1\n                            embedding_matrix.shape[1], # or EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=max_length,\n                            trainable=False)\n\n\n#print(vocab_size)\nmodel = Sequential()\n#model.add(Embedding(vocab_size, 12, input_length=max_length))\nmodel.add(embedding_layer)\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# summarize the model\nprint(model.summary())\n# fit the model\nmodel.fit(padded_docs, labels, epochs=13, verbose=1)\n# evaluate the model\n\nloss, accuracy = model.evaluate(padded_docs, labels, verbose=1)\nprint('Accuracy: %f' % (accuracy*100))\nprint('Loss/ ', loss)\n\ntest_tokens = tokenizer.texts_to_sequences(['God I would love to kick you in your dumb dumb face', 'Jesus loves you', 'lets be friends', 'get bent'])\npadded_test = pad_sequences(test_tokens, maxlen=max_length, padding='post')\ntest_labels = array([1,0,0, 1])\nloss, accuracy = model.evaluate(padded_test, test_labels, verbose=1)\nprint('Test Accuracy: %f' % (accuracy*100))\nprint('Test Loss: ', loss)\nprint('Confidence: ', model.predict(padded_test))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#A slightly modified version of the task from day #5 in this tutorial:\n#https://machinelearningmastery.com/crash-course-deep-learning-natural-language-processing/\nfrom numpy import array\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Dropout\nimport pandas\n# define documents\n\nreviews = pandas.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\nreviews.describe()\n\n\nreviews['sentiment'].replace({'negative':0, 'positive':1}, inplace=True)\n#docs = ['Well done!',\n#        'Good work',\n#        'Great effort',\n#        'nice work',\n#        'Excellent!',\n#        'Weak Sauce',\n#        'Poor effort!',\n#        'not good',\n#        'poor work',\n#        'Could have done better.']\n# define class labels\ndocs = reviews['review'].values.tolist()\n#print(docs)\nprint('doc type: ', type(docs))\nlabels = reviews['sentiment']\n#words = docs.split()\n#labels = array([1,1,1,1,0,0,0,0,0])\n# integer encode the documents\nvocab_size = 250000\n\n\ndocs = [d.replace(\"<br />\", \"  \") for d in docs] #remove line breaks, probably meaningless\ndocs = [d.replace(\"\\\\\", \"  \") for d in docs] #removespecial char, probably meaningless\n\nprint(docs[54])\n\nencoded_docs = [one_hot(d, vocab_size) for d in docs]\n#print(encoded_docs)\n\nlongest = max(len(a) for a in encoded_docs)\nprint('Longest review: ', longest)\n\nmax_length = 2500\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n#print(padded_docs)\n\nX_Train, X_Validation, Y_Train, Y_Validation = train_test_split(padded_docs, labels, test_size=0.02, random_state=1)\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 12, input_length=max_length))\n#4 = 87.7%\n#16 = 89.4%\n#64 = 88.8%\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# summarize the model\nprint(model.summary())\n# fit the model\nmodel.fit(X_Train, Y_Train, epochs=10, verbose=1)\n# evaluate the model\nloss, accuracy = model.evaluate(X_Validation, Y_Validation, verbose=1)\nprint('Accuracy: %f' % (accuracy*100))\nprint('Loss/ ', loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}