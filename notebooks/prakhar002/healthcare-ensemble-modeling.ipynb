{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL\nimport matplotlib.pyplot as plt # this is used for the plot the graph \nimport seaborn as sns # plot library of python\n%matplotlib inline\nfrom sklearn.linear_model import LogisticRegression # to apply the Logistic regression\nfrom sklearn.model_selection import train_test_split # to split the data into two partsas\nfrom sklearn.model_selection import GridSearchCV# for tuning parameter\nfrom sklearn.ensemble import RandomForestClassifier # for random forest classifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics # to check the error and accuracy of the model\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nimport plotly.figure_factory as ff\nimport plotly.offline as py","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data_df= pd.read_csv(\"../input/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"     It's good idea to drop Unnamed: 32 Column"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.drop(['Unnamed: 32'],axis = 1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df[['diagnosis','radius_worst','radius_mean','radius_se']].groupby('diagnosis').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that on an average of radius in all scenarios is less in Benign Tumor as compared to Malignant one."},{"metadata":{"trusted":true},"cell_type":"code","source":"M = data_df[(data_df['diagnosis'] == 'M')]\nB = data_df[(data_df['diagnosis'] == 'B')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def myplot(data_new,bin_size):\n    tmp1 = M[data_new]\n    tmp2 = B[data_new]\n    hist_data = [tmp1, tmp2]\n    group_labels = [\"Malignant\",\"Benign\"]\n    colors = ['#F24027', '#2CD166']\n    fig  = ff.create_distplot(hist_data,group_labels,colors = colors, show_hist = True, bin_size = bin_size,curve_type = 'kde')\n    fig['layout'].update(title = data_new)\n    py.iplot(fig, filename = 'Density Plot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"myplot('radius_mean',.5)\nmyplot('texture_mean',.5)\nmyplot('compactness_mean' , 0.005)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Segregating the features"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_mean = data_df.columns[2:11]\nfeatures_mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_se = data_df.columns[12:22]\nfeatures_se","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_worst = data_df.columns[23:]\nfeatures_worst","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df['diagnosis'] = data_df['diagnosis'].map({'M':1,'B':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"diagnosis\", kind=\"count\", palette=\"ch:.30\", data=data_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_mean = data_df[features_mean].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize= (10,10))\nsns.heatmap(corr_mean,annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ****Highly Correalted Pairs:\n>     (radius_mean <-> area_mean)\n>     (perimeter_mean <-> area_mean)\n>     (concavity_mean <-> concave points_mean)"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_mean.abs()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"select_pred_mean = ['radius_mean','texture_mean','smoothness_mean','compactness_mean','symmetry_mean']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"train , test = train_test_split(data_df,test_size = 0.2)\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = train[select_pred_mean]# taking the training data input \ntrain_y=train.diagnosis\ntest_X= test[select_pred_mean] # taking test data inputs\ntest_y =test.diagnosis   #output value of test data\nmodel_rf=RandomForestClassifier(n_estimators=100)\nmodel_rf.fit(train_X,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_value = model_rf.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.accuracy_score(predict_value,test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_logreg = LogisticRegression()\nmodel_logreg.fit(train_X,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_logreg = model_logreg.predict(test_X)\nmetrics.accuracy_score(predict_logreg,test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.f1_score(predict_logreg,test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.confusion_matrix(predict_logreg,test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.confusion_matrix(predict_value,test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.f1_score(predict_value,test_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ROC Curves\ntells how much model is capable of distinguishing between classes.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"def roc_curve(model_num,name_model):\n    probs = model_num.predict_proba(test_X)\n    preds = probs[:,1] # tpr\n    fpr, tpr, threshold = metrics.roc_curve(test_y, preds)\n    roc_auc = metrics.auc(fpr, tpr)\n    plt.title('Receiver Operating Characteristic for ' + name_model)\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_curve(model_logreg,\"LogisticRegression\")\nroc_curve(model_rf,\"Random Forest\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking with taking all features\nselect_pred_mean_full = features_mean\ntrain_X= train[select_pred_mean_full]\ntrain_y= train.diagnosis\ntest_X = test[select_pred_mean_full]\ntest_y = test.diagnosis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Default Random Forest Classifier Algorithm without any tuning\nmodel=RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X,train_y)\nprediction = model.predict(test_X)\nmetrics.accuracy_score(prediction,test_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy Increases a Bit by including all mean features but not by a significant margin. So we keep the model complexity low."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating Feature Importance\nfeatimp = pd.Series(model.feature_importances_, index=select_pred_mean_full).sort_values(ascending=False)\nprint(featimp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Xgboost\nfrom xgboost import XGBRegressor\nmodel_xgb = XGBRegressor()\n# We can Add silent=True to avoid printing out updates with each cycle\nmodel_xgb.fit(train_X, train_y, verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model_xgb.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculating Accuracy "},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = [round(value) for value in y_pred]\n# evaluate predictions\naccuracy = metrics.accuracy_score(predictions,test_y)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://stackoverflow.com/questions/36063014/what-does-kfold-in-python-exactly-do"},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold = StratifiedKFold(n_splits=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state = 2\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, train_X, y = train_y, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set2\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision Tree\nRandom Forest\nET\nGB"},{"metadata":{"trusted":true},"cell_type":"code","source":"### META MODELING  WITH ADABOOST, RF, EXTRATREES and GRADIENTBOOSTING\n\n# Adaboost\nDTC = DecisionTreeClassifier()\n\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsadaDTC.fit(train_X,train_y)\npp = gsadaDTC.predict(test_X)\n\ngsadaDTC_Acc = metrics.accuracy_score(pp,test_y)\n\nprint(gsadaDTC_Acc)\n\nada_best = gsadaDTC.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gsadaDTC.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://datascience.stackexchange.com/questions/21877/how-to-use-the-output-of-gridsearch"},{"metadata":{"trusted":true},"cell_type":"code","source":"#ExtraTrees \nExtC = ExtraTreesClassifier()\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 9],\n              \"min_samples_split\": [2, 3, 9],\n              \"min_samples_leaf\": [1, 3, 9],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsExtC.fit(train_X,train_y)\n\nExtC_best = gsExtC.best_estimator_\n\n# Best score\ngsExtC.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RFC Parameters tunning \nRFC = RandomForestClassifier()\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 9],\n              \"min_samples_split\": [2, 3, 9],\n              \"min_samples_leaf\": [1, 3, 9],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(train_X,train_y)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gradient boosting tunning\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(train_X,train_y)\n\nGBC_best = gsGBC.best_estimator_\n# Best score\ngsGBC.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://medium.com/all-things-ai/in-depth-parameter-tuning-for-gradient-boosting-3363992e9bae"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking HyperParameter Values\nRFC_best","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://medium.com/@srnghn/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3"},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = cols = 2\nfig, axes = plt.subplots(rows , cols ,figsize=(14,14))\n\nbest_classifiers = [(\"AdaBoosting\", ada_best),(\"GradientBoosting\",GBC_best),(\"ExtraTrees\",ExtC_best),(\"RandomForest\",RFC_best)]\n\nnclassifier = 0\nfor row in range(rows):\n    for col in range(cols):\n        name = best_classifiers[nclassifier][0]\n        classifier = best_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:9]\n        g = sns.barplot(y=train_X.columns[indices][:9],x = classifier.feature_importances_[indices][:9] ,\n                        orient='h',ax=axes[row][col],palette=\"rocket\")\n        g.set_xlabel(\"Relative importance\",fontsize=10)\n        g.set_ylabel(\"Features\",fontsize=10)\n        g.tick_params(labelsize=10)\n        g.set_title(name + \" feature importance\")\n        nclassifier += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Ensembled Voting**"},{"metadata":{"trusted":true},"cell_type":"code","source":"votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),\n('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\nvotingC = votingC.fit(train_X,train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hard voting is where a model is selected from an ensemble to make the final prediction by a simple majority vote for accuracy.\n\nSoft Voting can only be done when all your classifiers can calculate probabilities for the outcomes. Soft voting arrives at the best result by averaging out the probabilities calculated by individual algorithms."},{"metadata":{},"cell_type":"markdown","source":"https://towardsdatascience.com/ensemble-learning-in-machine-learning-getting-started-4ed85eb38e00****"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_voting = votingC.predict(test_X)\nmetrics.accuracy_score(pred_voting,test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.f1_score(pred_voting,test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(pred_voting,test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def roc_curve(model_num,name_model):\n    probs = model_num.predict_proba(test_X)\n    preds = probs[:,1] # True Positive Rate\n    fpr, tpr, threshold = metrics.roc_curve(test_y, preds)\n    roc_auc = metrics.auc(fpr, tpr)\n    plt.title('Receiver Operating Characteristic for ' + name_model)\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_curve(votingC,\"Vf\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_sub = pd.DataFrame({'Id': test.id, 'Ensembled_Diagnosis_Prediction': pred_voting})\nfinal_sub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thanks for your time ! This is my first Kernel. Please let me know any suggestion for improvement.\nDo Upvote if you liked the kernel it will motivate me. \n\n**Happy Learning ! **"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}