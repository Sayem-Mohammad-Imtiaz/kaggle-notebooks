{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # Linear algebra\nimport pandas as pd # Data processing.\n\nimport seaborn as sns # Visualizing (Heat Map)\nimport matplotlib.pyplot as plt # Visualizing\n\nfrom sklearn.metrics import confusion_matrix # Comparing\n\nimport warnings # For ignore warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preparing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop([\"id\",\"Unnamed: 32\"],axis = 1, inplace = True)\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\n\ny = data.diagnosis\nx_data = data.drop([\"diagnosis\"],axis = 1)\n\nx = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data)) # Normalize data\n\ncompare = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train - Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.15,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Algorithms\n- K-Nearest Neighbour (KNN) Classification\n- Logistic Regression Classification\n- Naive Bayes Classification\n- Random Forest Classification\n- Suppor Vector Machine (SVM) Classification\n- Decision Tree Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K-Nearest Neighbour (KNN) Classification"},{"metadata":{},"cell_type":"markdown","source":"**Firstly, Let's find optimal k value**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nscores = []\nfor i in range(1,len(x_test)):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train,y_train)\n    scores.append(knn.score(x_test,y_test))\nk_value = scores.index(max(scores))+1\nprint(\"Optimal n_neighbors values is :\", k_value)\n# We write max(scores)+1 because normally counting starts from 0 in software but scores list is starting with 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prepare Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn2 = KNeighborsClassifier(n_neighbors=k_value)\nknn2.fit(x_train,y_train) # Fit data\ny_predict = knn2.predict(x_test)\n\ncm = confusion_matrix(y_true,y_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizaiton"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm, annot = True, linewidths = 1, linecolor = \"red\", fmt = \".0f\", ax = ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Real\")\nplt.show()\n\nknn_correct = cm[1][1] + cm[0][0]\nknn_accuracy = knn.score(x_test,y_test)\nprint(\"Number of Correct :\",knn_correct)\nprint(\"KNN accuracy\", knn_accuracy)\ncompare.append([\"KNN\",knn_correct,knn_accuracy])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression Classification"},{"metadata":{},"cell_type":"markdown","source":"Prepare Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\n\ny_predict = lr.predict(x_test)\n\ncm = confusion_matrix(y_true,y_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm, annot = True, linewidths = 1, linecolor = \"red\", fmt = \".0f\", ax = ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Real\")\nplt.show()\n\nlr_correct = cm[1][1] + cm[0][0]\nlr_accuracy = lr.score(x_test,y_test)\nprint(\"Number of Correct :\",lr_correct)\nprint(\"LR accuracy\", lr_accuracy)\ncompare.append([\"LR\",lr_correct,lr_accuracy])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive Bayes Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\ny_predict = nb.predict(x_test)\n\ncm = confusion_matrix(y_true,y_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm, annot = True, linewidths = 1, linecolor = \"red\", fmt = \".0f\", ax = ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Real\")\nplt.show()\n\nnb_correct = cm[1][1] + cm[0][0]\nnb_accuracy = nb.score(x_test,y_test)\nprint(\"Number of Correct :\",nb_correct)\nprint(\"NB accuracy\", nb_accuracy)\ncompare.append([\"NB\",nb_correct,nb_accuracy])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classification"},{"metadata":{},"cell_type":"markdown","source":"**Firstly, Let's find optimal n_estimators value**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nscores = []\nfor i in range(1,10):\n    rf = RandomForestClassifier(n_estimators = i,random_state = 42)\n    rf.fit(x_train,y_train)\n    scores.append(rf.score(x_test,y_test))\noptimal_n = scores.index(max(scores))+1\nprint(\"Optimal n_estimator :\", optimal_n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prepare Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf2 = RandomForestClassifier(n_estimators = 8,random_state = 42)\nrf2.fit(x_train,y_train)\ny_predict = rf2.predict(x_test)\n\ncm = confusion_matrix(y_true,y_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizing"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm, annot = True, linewidths = 1, linecolor = \"red\", fmt = \".0f\", ax = ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Real\")\nplt.show()\n\nrf_correct = cm[1][1] + cm[0][0]\nrf_accuracy = nb.score(x_test,y_test)\nprint(\"Number of Correct :\",rf_correct)\nprint(\"RF accuracy\", rf_accuracy)\ncompare.append([\"RF\",rf_correct,rf_accuracy])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Suppor Vector Machine (SVM) Classification"},{"metadata":{},"cell_type":"markdown","source":"Prepare Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvm = SVC(random_state = 42)\nsvm.fit(x_train,y_train)\ny_predict = svm.predict(x_test)\n\ncm = confusion_matrix(y_true,y_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizing"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm, annot = True, linewidths = 1, linecolor = \"red\", fmt = \".0f\", ax = ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Real\")\nplt.show()\n\nsvm_correct = cm[1][1] + cm[0][0]\nsvm_accuracy = svm.score(x_test,y_test)\nprint(\"Number of Correct :\",svm_correct)\nprint(\"SVM accuracy\", svm_accuracy)\ncompare.append([\"SVM\",svm_correct,svm_accuracy])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Classification"},{"metadata":{},"cell_type":"markdown","source":"Prepare Mode"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\ny_predict = dt.predict(x_test)\n\ncm = confusion_matrix(y_true,y_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm, annot = True, linewidths = 1, linecolor = \"red\", fmt = \".0f\", ax = ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Real\")\nplt.show()\n\ndt_correct = cm[1][1] + cm[0][0]\ndt_accuracy = dt.score(x_test,y_test)\nprint(\"Number of Correct :\",dt_correct)\nprint(\"DT accuracy\", dt_accuracy)\ncompare.append([\"DT\",dt_correct,dt_accuracy])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compare"},{"metadata":{},"cell_type":"markdown","source":"- K-Nearest Neighbour (KNN) Classification -> KNN\n- Logistic Regression Classification -> LR\n- Naive Bayes Classification -> NB\n- Random Forest Classification -> RF\n- Suppor Vector Machine (SVM) Classification -> SVM\n- Decision Tree Classification -> DT"},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = []\ncorrect = []\nindex = []\nfor i in compare:\n    accuracy.append(i[2])\n    correct.append(i[1])\n    index.append(i[0])\ndata = {\"Correct\":correct,\"Accuracy\":accuracy}\n\npd.options.display.float_format = '{:,.3f}'.format # We write this code cause of to show 3 digits after the comma\ndf = pd.DataFrame(data,index = index)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(index,correct,color = \"red\")\nplt.xlabel(\"Algorithms\")\nplt.ylabel(\"Number of Correct\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(index,accuracy,color = \"blue\")\nplt.xlabel(\"Algorithms\")\nplt.ylabel(\"Accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Result"},{"metadata":{},"cell_type":"markdown","source":"- **When we look at this comparison, we see that,the most suitable model is Linear Regression. but this does not indicate that it is the best classification algorithm. Since all classification algorithms have different mathematical background, we get different results in different datasets.**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}