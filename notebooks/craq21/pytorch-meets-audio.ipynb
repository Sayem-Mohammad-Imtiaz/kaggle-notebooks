{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import librosa\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom pathlib import Path\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pwd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir spectrograms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/patient_diagnosis.csv',\n                      names=['Patient number', 'Diagnosis']\n                  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(df['Diagnosis'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_no_diagnosis = pd.read_csv('../input/respiratory-sound-database/demographic_info.txt', delimiter=' ',\n                              names=['Patient number', 'Age', 'Sex' , 'Adult BMI (kg/m2)', 'Child Weight (kg)' , 'Child Height (cm)'],\n                             )\ndf_no_diagnosis.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df =  df_no_diagnosis.join(df.set_index('Patient number'), on='Patient number', how='left')\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_path = Path('../input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[[\"Patient number\", \"Sex\", \"Diagnosis\"]]\ndf.isna().sum()\ndf.dropna(axis=0, inplace=True)\ndf.reset_index(inplace=True, drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Diagnosis.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.loc[df.Diagnosis != \"Asthma\"]\ndf = df.loc[df.Diagnosis != \"LRTI\"]\ndf = df.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import soundfile as sf\nsamplerates = []\ntargets = []\nfilenames = []\nfor file in base_path.glob('audio_and_txt_files/*.wav'):\n    _, sr = sf.read(file)\n    filenames.append(str(file).split('/')[-1])\n    targets.append(int(str(file).split('/')[-1][:3]))\n    samplerates.append(sr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique, counts = np.unique(samplerates, return_counts=True)\nunique, counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"patient_rate = []\nfor patient_id in df[\"Patient number\"]:\n    srs = True\n    for file in base_path.glob(f'audio_and_txt_files/{patient_id}_*.wav'):\n        _, sr = sf.read(file)\n        if sr < 41000:\n            srs = False\n    patient_rate.append(srs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[patient_rate]\ndf.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import IPython\ndef display_wav(patient_id, target):\n    patient_sounds = list(base_path.glob(f'audio_and_txt_files/{patient_id}*.wav'))\n    test_wav = librosa.core.load(patient_sounds[0], sr=44100)[0]\n\n    fig, ax = plt.subplots(5, 1, figsize=(20,18))\n    ax[0].plot(test_wav)\n    ax[0].set_title(f\"Full {target} waveform\")\n\n    mel = librosa.feature.melspectrogram(test_wav, sr=44100, n_fft=512, hop_length=256, win_length=512, window='hann', n_mels=100)\n    mel[mel < 1e-12] = 1e-12\n    ax[1].imshow(np.log(mel)[:,:1000], origin='lower')\n    ax[1].set_title(f\"Slice of {target} mel spectrogram 512 fft\")\n\n    mel = librosa.feature.melspectrogram(test_wav, sr=44100, n_fft=1024, hop_length=256, win_length=1024, window='hann', n_mels=100)\n    mel[mel < 1e-12] = 1e-12\n    ax[2].imshow(np.log(mel)[:,:1000], origin='lower')\n    ax[2].set_title(f\"Slice of {target} mel spectrogram 1024 fft\")\n\n    mel = librosa.feature.melspectrogram(test_wav, sr=44100, n_fft=2048, hop_length=256, win_length=1024, window='hann', n_mels=100)\n    mel[mel < 1e-12] = 1e-12\n    ax[3].imshow(np.log(mel)[:,:1000], origin='lower')\n    ax[3].set_title(f\"Slice of {target} mel spectrogram 2048 fft\")\n    \n    chroma = librosa.feature.chroma_stft(test_wav, sr=44100, n_fft=2048, hop_length=256, win_length=1024, window='hann')\n    ax[4].imshow(chroma[:,:1000], origin='lower')\n    ax[4].set_title(f\"Slice of {target} chromagram 2048 fft\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_wav(102, \"healthy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_wav(104, \"COPD\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_wav(103, \"asthma\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_files = np.array(filenames)[np.array(samplerates) > 10000]\n\nmics = []\nsound_location = []\nfor i in range(len(all_files)):\n    filename = all_files[i].split('/')[-1].split('_')\n    mics.append(filename[4].replace(\".wav\", \"\"))\n    sound_location.append(filename[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_mics = np.unique(np.array(mics))\nall_locations = np.unique(np.array(sound_location))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Unique microphones: {all_mics}\")\nprint(f\"Unique sound positions: {all_locations}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(all_mics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(all_locations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_lengths = []\nfor file in base_path.glob('audio_and_txt_files/*.wav'):\n    data, sr = sf.read(file)\n    if sr == 44100:\n        file_lengths.append(len(data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(file_lengths, kde=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test = train_test_split(df, \n                                    random_state=42, test_size=0.26, \n                                    stratify=df[\"Diagnosis\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(X_train[\"Diagnosis\"])\nplt.title(\"Train target distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(X_test[\"Diagnosis\"])\nplt.title(\"Test target distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Collater:\n    def __init__(self, sample_seconds, repeat=1):\n        self.sample_frames = int(sample_seconds * (44100 // 256))\n        self.repeat = repeat\n    \n    def __call__(self, batch):\n        new_batch = {\n            \"location\": [],\n            \"gender\": [],\n            \"mel\": [],\n            \"chroma\": [],\n            \"mic\": [],\n            \"target\": []\n        }\n        for entry in batch:\n            new_batch[\"location\"] += [entry[\"location\"] for i in range(self.repeat)]\n            new_batch[\"gender\"]   += [entry[\"gender\"] for i in range(self.repeat)]\n            new_batch[\"mel\"]      += [entry[\"mel\"] for i in range(self.repeat)]\n            new_batch[\"chroma\"]   += [entry[\"chroma\"] for i in range(self.repeat)]\n            new_batch[\"mic\"]      += [entry[\"mic\"] for i in range(self.repeat)]\n            new_batch[\"target\"]   += [entry[\"target\"] for i in range(self.repeat)]\n        lengths = [entry.shape[1] for entry in new_batch[\"mel\"]]\n        \n        start_frames = torch.tensor([np.random.randint(0, length - self.sample_frames - 5) for length in lengths])\n        \n        for i in range(len(new_batch[\"mel\"])):\n            new_batch[\"mel\"][i] = new_batch[\"mel\"][i][:, start_frames[i]:start_frames[i]+self.sample_frames]\n            new_batch[\"chroma\"][i] = new_batch[\"chroma\"][i][:, start_frames[i]:start_frames[i]+self.sample_frames]\n        \n        new_batch[\"mel\"] = torch.tensor(new_batch[\"mel\"], dtype=torch.float32)\n        new_batch[\"chroma\"] = torch.tensor(new_batch[\"chroma\"], dtype=torch.float32)\n        new_batch[\"location\"] = torch.tensor(new_batch[\"location\"], dtype=torch.long)\n        new_batch[\"gender\"] = torch.tensor(new_batch[\"gender\"], dtype=torch.long)\n        new_batch[\"mic\"] = torch.tensor(new_batch[\"mic\"], dtype=torch.long)\n        new_batch[\"target\"] = torch.tensor(new_batch[\"target\"], dtype=torch.long)\n        \n        return new_batch\n            \n\n\nclass SoundDataset(Dataset):\n    def __init__(self, df, mics, locations, genders, targets, sound_path, sample_seconds):\n        self.all_mics = mics\n        self.all_locations = locations\n        self.all_genders = genders\n        self.all_targets = targets\n        self.sample_length = sample_seconds * 44100\n        \n        data = df.reset_index(drop=True)\n        self.mics = []\n        self.locations = []\n        self.genders = []\n        self.targets = []\n        self.sound_len = []\n        self.sound_files = []\n        for i in range(len(df)):\n            entry = df.iloc[i]\n            patient_id = int(entry[\"Patient number\"])\n            for file in sound_path.glob(f\"{patient_id}_*.wav\"):\n                self.sound_files.append(file)\n                data, sr = sf.read(file)\n                if sr != 44100:\n                    print(\"Skipping \", file.stem)\n                    continue\n                self.sound_len.append(len(data))\n                meta = str(file).split('/')[-1].split('_')\n                self.mics.append(self.all_mics.index(meta[4].replace(\".wav\", \"\")))\n                self.locations.append(self.all_locations.index(meta[2]))\n                self.genders.append(self.all_genders.index(entry[\"Sex\"]))\n                self.targets.append(self.all_targets.index(entry[\"Diagnosis\"]))\n        \n    def __getitem__(self, idx):\n        sample_length = int(self.sample_length * 2.5)\n        start_ind = np.random.randint(0, int(self.sound_len[idx] - sample_length - 5))\n        waveform = sf.read(self.sound_files[idx], start=start_ind, frames=sample_length)[0]\n        mel = librosa.feature.melspectrogram(waveform, sr=44100, n_fft=2048, hop_length=256, win_length=1024, window='hann', n_mels=100)\n        mel[mel < 1e-12] = 1e-12\n\n        chroma = librosa.feature.chroma_stft(waveform, sr=44100, n_fft=2048, hop_length=256, win_length=1024, window='hann')\n        \n        return {\"mel\": mel, \n                \"chroma\": chroma, \n                \"gender\": self.genders[idx], \n                \"location\": self.locations[idx], \n                \"target\": self.targets[idx], \n                \"mic\": self.mics[idx]}\n    \n    def __len__(self):\n        return len(self.sound_files)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, hidden_dim, dropout_prob=0.1, max_len=5000):\n        super().__init__()\n        self.dropout_prob = dropout_prob\n\n        pe = torch.zeros(max_len, hidden_dim)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-math.log(10000.0) / hidden_dim))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return F.dropout(x, p=self.dropout_prob)\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, padding=(2,0), stride=1):\n        super().__init__()\n        self.triple_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels // 2, kernel_size=5, stride=stride, padding=padding),\n            nn.BatchNorm2d(out_channels // 2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(\n                out_channels // 2, out_channels // 2, kernel_size=5, stride=stride, padding=padding\n            ),\n            nn.BatchNorm2d(out_channels // 2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(\n                out_channels // 2, out_channels, kernel_size=5, stride=stride, padding=padding\n            ),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.triple_conv(x)\n    \nclass Model(nn.Module):\n    def __init__(self, mel_channels, output_mel_channels, \n                 chroma_channels, output_chroma_channels, \n                 num_mics, mic_dim, num_sound_srcs, sound_src_dim,\n                 num_genders, gender_dim, encoder_dim, \n                 output_dim, num_targets,\n                 num_heads=8, num_layers=3, dropout=0.2):\n        super().__init__()\n        self.spectro_encoder = ConvBlock(mel_channels, output_mel_channels, padding=(0,0), stride=(2,1))\n        \n        self.chroma_encoder = ConvBlock(chroma_channels, output_chroma_channels, padding=(1,0))\n        \n        self.microphone_embedding = nn.Embedding(num_mics + 1, mic_dim)\n        self.sound_source_embedding = nn.Embedding(num_sound_srcs + 1, sound_src_dim)\n        \n        self.gender_embedding = nn.Embedding(num_genders + 1, gender_dim)\n        \n        hidden_dim = 1948\n        \n        self.linear_projection = nn.Linear(hidden_dim, encoder_dim)\n        \n        self.pos_encoder = PositionalEncoding(encoder_dim, dropout)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=encoder_dim, nhead=num_heads)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        \n        self.encoder_output_layer = nn.Linear(encoder_dim, output_dim)\n        self.output_layer = nn.Linear(output_dim*2, num_targets)\n        \n    def forward(self, batch):\n        \n        spectro = self.spectro_encoder(batch[\"mel\"].unsqueeze(1))\n        \n        chroma = self.chroma_encoder(batch[\"chroma\"].unsqueeze(1))\n        n_time = spectro.size(3)\n        n_batch = spectro.size(0)\n        \n        mic_embed = self.microphone_embedding(batch[\"mic\"]).unsqueeze(-1).expand(-1, -1, n_time).transpose(1,2)\n        pos_embed = self.sound_source_embedding(batch[\"location\"]).unsqueeze(-1).expand(-1, -1, n_time).transpose(1,2)\n        gender_embed = self.gender_embedding(batch[\"gender\"]).unsqueeze(-1).expand(-1, -1, n_time).transpose(1,2)\n        \n        chroma = chroma.transpose(1,3).reshape(n_batch, n_time, -1)\n        spectro = spectro.transpose(1,3).reshape(n_batch, n_time, -1)\n        \n        x = torch.cat((chroma, spectro, mic_embed, pos_embed, gender_embed), 2)\n        x = self.linear_projection(x)\n        \n        x = F.leaky_relu(x, negative_slope=0.2)\n        \n        x = self.pos_encoder(x)\n        x = x.transpose(0, 1)\n        x = self.transformer_encoder(x)\n        x = x.transpose(0, 1)\n\n        x = self.encoder_output_layer(x)\n        x = F.leaky_relu(x, negative_slope=0.2)\n        x = torch.cat([torch.mean(x, dim=1), torch.std(x, dim=1)], dim=1)\n\n        x = self.output_layer(x)\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_cuda(batch):\n    for key in batch.keys():\n        batch[key] = batch[key].cuda()\n    return batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@torch.no_grad()\ndef validate(model, val_data, criterion, batch_size):\n    valset = SoundDataset(val_data, all_mics.tolist(), all_locations.tolist(), [\"M\", \"F\"], \n               [\"COPD\", \"Healthy\", \"URTI\", \"Bronchiectasis\", \"Bronchiolitis\", \"Pneumonia\"], base_path / \"audio_and_txt_files\", 6.0)\n    collater = Collater(5.0, repeat=2)\n    val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=5, collate_fn=collater)\n    \n    mean_ce = 0.0\n    model.eval()\n    for i, batch in enumerate(val_loader):\n        batch = to_cuda(batch)\n        output = model(batch)\n        loss = criterion(output, batch[\"target\"])\n        mean_ce += loss.item()\n        \n    mean_ce /= (i+1)\n    model.train()\n    return mean_ce","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 16\ncriterion = torch.nn.CrossEntropyLoss()\nlr = 3e-5\nweight_decay=1e-8\nepochs = 20\nverbose_iter=10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model(1, 128, 1, 128, len(all_mics), 8, len(all_locations), 16, 2, 4, 512, 128, 6, num_layers=3).cuda()\ntrain_set = SoundDataset(X_train, all_mics.tolist(), all_locations.tolist(), [\"M\", \"F\"], \n               [\"COPD\", \"Healthy\", \"URTI\", \"Bronchiectasis\", \"Bronchiolitis\", \"Pneumonia\"], base_path / \"audio_and_txt_files\", 6.0)\ncollater = Collater(5.0, repeat=2)\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False, num_workers=5, collate_fn=collater)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\niteration = 0\ntrain_loss = []\nval_loss = []\nfor epoch in range(1,epochs):\n    for batch in train_loader:\n        batch = to_cuda(batch)\n        optimizer.zero_grad()\n        \n        output = model(batch)\n        loss = criterion(output, batch[\"target\"])\n        \n        loss.backward()\n        optimizer.step()\n        \n        if iteration % verbose_iter == 0:\n            print(f\"{iteration}: Cross-entropy {loss.item()}\")\n        train_loss.append(loss.item())\n        iteration += 1\n    print(\"VALIDATING\")\n    val_ce = validate(model, X_test, criterion, batch_size)\n    val_loss.append(val_ce)\n    \n    print(f\"DONE EPOCH {epoch}: Train {train_loss[-1]} ;  Val {val_loss[-1]}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14,9))\nplt.plot(train_loss)\nplt.title(\"Train loss\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14,9))\nplt.plot(val_loss)\nplt.title(\"Validation loss\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valset = SoundDataset(X_test, all_mics.tolist(), all_locations.tolist(), [\"M\", \"F\"], \n           [\"COPD\", \"Healthy\", \"URTI\", \"Bronchiectasis\", \"Bronchiolitis\", \"Pneumonia\"], base_path / \"audio_and_txt_files\", 6.0)\ncollater = Collater(5.0, repeat=2)\nval_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=5, collate_fn=collater)\n\ntargets = []\npredictions = []\n\nmodel.eval()\nwith torch.no_grad():\n    for i, batch in enumerate(val_loader):\n        batch = to_cuda(batch)\n        output = model(batch)\n        \n        targets.append(batch[\"target\"].cpu().numpy())\n        predictions.append(output.cpu().numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_targets = []\nfor entry in targets:\n    new_targets += entry.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_preds = []\nfor pred in predictions:\n    new_preds += np.argmax(pred, axis=1).tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_classes = [\"COPD\", \"Healthy\", \"URTI\", \"Bronchiectasis\", \"Bronchiolitis\", \"Pneumonia\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfig, ax = plt.subplots(figsize=(13,10))\nsns.heatmap(confusion_matrix(new_targets, new_preds), annot=True, annot_kws={\"size\": 16}, cmap='ocean_r')\nax.set_xticklabels(target_classes)\nax.set_yticklabels(target_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import label_binarize\ny_test = label_binarize(new_targets, classes=[0, 1, 2, 3, 4, 5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_proba = []\nfor proba in predictions:\n    predicted_proba += nn.Softmax()(torch.tensor(proba)).cpu().numpy().tolist()\npredicted_proba = np.array(predicted_proba)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(len(target_classes)):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], predicted_proba[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14,9))\nfor i in range(len(target_classes)):\n    plt.plot(fpr[i], tpr[i], label=f'{i} ROC curve (area = %0.2f)' % roc_auc[i])\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic')\n    plt.legend(loc=\"lower right\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}