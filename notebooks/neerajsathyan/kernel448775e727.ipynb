{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n\n\nsns.set(style=\"whitegrid\")\nsns.set_color_codes(\"pastel\")\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/student-alcohol-consumption/student-mat.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(4, 4))\nplt.pie(data['sex'].value_counts().tolist(), \n        labels=['Female', 'Male'], colors=['#ffd1df', '#a2cffe'], \n        autopct='%1.1f%%', startangle=90)\naxis = plt.axis('equal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(5, 4))\nsns.distplot(data['age'],  \n             hist_kws={\"alpha\": 1, \"color\": \"#a2cffe\"}, \n             kde=False, bins=8)\nax = ax.set(ylabel=\"Count\", xlabel=\"Age\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(4, 4))\nplt.pie(data['studytime'].value_counts().tolist(), \n        labels=['2 to 5 hours', '<2 hours', '5 to 10 hours', '>10 hours'], \n        autopct='%1.1f%%', startangle=0)\naxis = plt.axis('equal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(4, 4))\nplt.pie(data['romantic'].value_counts().tolist(), \n        labels=['No', 'Yes'], autopct='%1.1f%%', startangle=90)\naxis = plt.axis('equal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(5, 4))\nsns.distplot(data['Walc'],  \n             hist_kws={\"alpha\": 1, \"color\": \"#a2cffe\"}, \n             kde=False, bins=4)\nax = ax.set(ylabel=\"Students\", xlabel=\"Weekend Alcohol Consumption\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot1 = sns.factorplot(x=\"Walc\", y=\"health\", hue=\"sex\", data=data)\nplot1.set(ylabel=\"Health\", xlabel=\"Weekend Alcohol Consumption\")\n\nplot2 = sns.factorplot(x=\"Dalc\", y=\"health\", hue=\"sex\", data=data)\nplot2.set(ylabel=\"Health\", xlabel=\"Workday Alcohol Consumption\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot1 = sns.factorplot(x=\"G3\", y=\"Walc\", data=data)\nplot1.set(ylabel=\"Final Grade\", xlabel=\"Weekend Alcohol Consumption\")\n\nplot2 = sns.factorplot(x=\"G3\", y=\"Dalc\", data=data)\nplot2.set(ylabel=\"Final Grade\", xlabel=\"Workday Alcohol Consumption\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot('age','G3',data = data)\nsns.factorplot('studytime','G3',data = data)\nsns.factorplot('goout','G3',data = data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"str_list = [] # empty list to contain columns with strings (words)\n\nfor colname, colvalue in data.iteritems():\n    if type(colvalue[1]) == str:\n         str_list.append(colname)\n# Get to the numeric columns by inversion            \nnum_list = data.columns.difference(str_list) \nprint(str_list)\nprint(num_list)\ndata_matnum = data[num_list]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 10))\nplt.title('Attr Correlation')\n# Draw the heatmap using seaborn\nsns.heatmap(data_matnum.astype(float).corr(),linewidths=0.25,vmax=1.0, square=True, cmap=\"YlGnBu\", linecolor='black', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data['G3']\nX = data.drop(['G3'], axis=1)\nX.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.get_dummies(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = ['DecisionTreeRegressor','LinearRegression']\nclf_list = [DecisionTreeRegressor(),LinearRegression()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X[201:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import spatial\n# import export_graphviz \nfrom sklearn.tree import export_graphviz \n# create a regressor object \nregressor = DecisionTreeRegressor(random_state = 0) \n# fit the regressor with X and Y data \nregressor.fit(X[0:200], y[0:200])\n \n  \n# export the decision tree to a tree.dot file \n# for visualizing the plot easily anywhere \nexport_graphviz(regressor, out_file ='tree.dot', \n               feature_names = X.columns)\n\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\n# Display in python\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (42, 72))\nplt.imshow(plt.imread('tree.png'))\nplt.axis('off');\nplt.show();\n\n# predicting a new value \n  \n# test the output by changing values, like 3750 \ny_pred = regressor.predict(X[201:])\nresult = 1 - spatial.distance.cosine(y_pred, y[201:])\nprint(result)\nprint(y_pred[2])\nprint(y[203])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cross_val_score(DecisionTreeRegressor(), X, y, cv=4).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for name, clf in zip(names, clf_list):\n    print(name, end=': ')\n    print(cross_val_score(clf, X, y, cv=4).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = tree.feature_importances_\nindices = np.argsort(importances)[::-1]\nfor f in range(X.shape[1]):\n    print(\"%d. Feature %s (%f)\" % (f + 1, X.columns.values[indices[f]], importances[indices[f]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import neighbors\nstr_list = [] # empty list to contain columns with strings (words)\n\nfor colname, colvalue in data.iteritems():\n    if type(colvalue[1]) == str:\n         str_list.append(colname)\n# Get to the numeric columns by inversion            \nnum_list = data.columns.difference(str_list) \nprint(str_list)\nprint(num_list)\ndata_matnum = data[num_list]\ndata_matnum.head(5)\n# y_train = data['G3']\n# x_train = data_matnum\n# x_train .drop(['G3'], axis=1,inplace=True)\n# n_neighbors=5\n# knn=neighbors.KNeighborsRegressor(n_neighbors,weights='uniform')\n# knn.fit(X_train,Y_train)\n# y1_knn=knn.predict(X_train)\n# y1_knn=list(y1_knn)\n\n# train_error_knn = np.mean(abs(y1_knn-Y_train))\n# print(train_error_knn)\n\n# y_test=knn.predict(X_test)\n# y_Predict=list(y_test)\n\n# test_error_knn = np.mean(abs(y_Predict-Y_test))\n# print(test_error_knn)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"from sklearn.model_selection import KFold # import KFold\nX = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) # create an array\ny = np.array([1, 2, 3, 4]) # Create another array\nkf = KFold(n_splits=3) # Define the split - into 2 folds \nkf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator\nprint(kf) \nKFold(n_splits=3, random_state=None, shuffle=False)\n\nfor train_index, test_index in kf.split(X):\n print(“TRAIN:”, train_index, “TEST:”, test_index)\n X_train, X_test = X[train_index], X[test_index]\n y_train, y_test = y[train_index], y[test_index]\n('TRAIN:', array([2, 3]), 'TEST:', array([0, 1]))\n('TRAIN:', array([0, 1]), 'TEST:', array([2, 3]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}