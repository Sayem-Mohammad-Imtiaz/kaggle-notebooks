{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"Initially, the end-goal of this notebook was to preprocess data for topic detection and tag classification.  \nI tried to explain why I choose to diverge or not from a \"classical\" preprocess on this particular case (see optional & specific).","metadata":{}},{"cell_type":"markdown","source":"### Preprocess steps\n \n**1) Noise removal**\n   1. Removing html (specific)  \n   2. Removing contraction  \n   3. Spelling correction \n   4. Lowering the text\n   \n**2) Removing simple character**   \n   1. Removing punctuation, special character and number  \n   3. Removing single character (optional & specific)\n\n**3) Removing StopWords**  \n   1. Removing most frequent word  \n   2. Removing certain type of word (optional & specific)  \n\n**4) Steming/Lemmatization**  \n   1. Stemming   \n   2. Lemmatization  \n\nThe advantage of this preprocess is that it's really straightforward but at the same time, you may loose information that are needed for some analysis. It can be used to make some simple topic detection (LDA, NMF, etc.) or classification.\n\nMost of these steps are **TASK-DEPENDANT**. You may choose to not remove Stopwords or Lemmatize your text in some case.\nThe **ORDER** of these steps may also vary. Make some spelling correction or lemmatization BEFORE removing StopWords may change the result.\nAlso, note that these steps are far from being optimised (I tokenize, untokenize, then tokenize again, etc.).  \nYou may also have heard of the expression \"Text normalization\". This is another step in NLP but it redundant with Noise removal. Those two are not well-defined and are overlapping. So I choose to organise my preprocess only with Noise removal in this case (which is completely arbitrary and it's fine).  \nFinaly, I did not include the removing of punctuation, special character and number in the Noise removal step since, it's more interessting to have this step as an independant step. The data are about programming and in programming, we use a lot of special character. So you may want to not remove those characters. Also, it make the preprocess clearer. We begin with cleaning, then we remove character (the most basic unit when working with text), after that we move to the word unit and finaly, we normalize the word.  ","metadata":{}},{"cell_type":"markdown","source":"### Vocabulary\n\nIf you are new to NLP, here is a small list of concepts that are used in this notebook.\n- **Tokenize:** \"Process of converting a string into a list of substrings, known as tokens.\"\n- **Text normalization:** \"Process of transforming text into a single canonical form that it might not have had before (e.g. lowering the text, removing contractions, spelling correction, stemming/lemmatization, etc.). Text normalization requires being aware of what type of text is to be normalized and how it is to be processed afterwards; there is no all-purpose normalization procedure. \"  \n- **Noise removal:** \"Process of removing anythings that can interfer with your analysis (e.g. removing html, lowering the text, removing punctuation/special character, etc.) \n- **Stemming:** \"Process of reducing inflected words to their word stem, base or root formâ€”generally a written word form (\"fishing\", \"fished\", and \"fisher\" to the stem \"fish\").\"\n- **Lematization:** \"Process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form (ie: \"walking\" to \"walk\", \"better\" to \"good\").\"\n- **StopWord:** \"Words which are filtered out before or after processing of natural language data (text). Stop words usually refers to the most common words in a language (words like \"The\", \"a\", etc. in english).\"","metadata":{}},{"cell_type":"markdown","source":"**Tag list**  \nList of tag use in the tagger (pos_tag function) from NLTK:\nhttps://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html","metadata":{}},{"cell_type":"markdown","source":"# Libraries and Dataset ","metadata":{}},{"cell_type":"code","source":"! pip install bs4\n# ! pip install pycontractions # The package has a depencies that have not been updated, so I couldn't use it.\n! pip install contractions\n! pip install autocorrect ","metadata":{"_kg_hide-output":true,"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generic librairies\nimport time as time\nimport numpy as np\nimport pandas as pd\nimport gc\n\n# Text librairies\nimport re\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom nltk.tokenize import ToktokTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk.tag.util import untag\nimport contractions\n# import pycontractions # Alternative better package for removing contractions\nfrom autocorrect import Speller","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://numpy.org/devdocs/user/basics.types.html\n\ndtypes_questions = {'Id':'int32', 'Score': 'int16', 'Title': 'str', 'Body': 'str'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf_questions = pd.read_csv('../input/pythonquestions/Questions.csv',\n                           usecols=['Id', 'Score', 'Title', 'Body'], \n                           encoding = \"ISO-8859-1\",\n                           dtype=dtypes_questions,\n#                            nrows=100\n                          )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_questions[['Title', 'Body']] = df_questions[['Title', 'Body']].applymap(lambda x: str(x).encode(\"utf-8\", errors='surrogatepass').decode(\"ISO-8859-1\", errors='surrogatepass'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove all questions that have a negative score\ndf_questions = df_questions[df_questions[\"Score\"] >= 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spell = Speller()\ntoken = ToktokTokenizer()\nlemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer()\ncharac = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~0123456789'\nstop_words = set(stopwords.words(\"english\"))\nadjective_tag_list = set(['JJ','JJR', 'JJS', 'RBR', 'RBS']) # List of Adjective's tag from nltk package","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_questions.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1) Noise removal","metadata":{}},{"cell_type":"markdown","source":"Noise removal is about removing anythings that can interfere with your text analysis. It's like the data cleaning step for a classical ML project.","metadata":{}},{"cell_type":"markdown","source":"## 1. Removing html","metadata":{}},{"cell_type":"code","source":"df_questions['Body'][11]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Parse question and title then return only the text\ndf_questions['Body'] = df_questions['Body'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\ndf_questions['Title'] = df_questions['Title'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, BeautifulSoup allow us to remove effectively most of the html code but not all. ","metadata":{}},{"cell_type":"code","source":"df_questions['Body'][11]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, we need to remove the rest here.","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    text = re.sub(r\"\\'\", \"'\", text) # match all literal apostrophe pattern then replace them by a single whitespace\n    text = re.sub(r\"\\n\", \" \", text) # match all literal Line Feed (New line) pattern then replace them by a single whitespace\n    text = re.sub(r\"\\xa0\", \" \", text) # match all literal non-breakable space pattern then replace them by a single whitespace\n    text = re.sub('\\s+', ' ', text) # match all one or more whitespace then replace them by a single whitespace\n    text = text.strip(' ')\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndf_questions['Title'] = df_questions['Title'].apply(lambda x: clean_text(x)) \ndf_questions['Body'] = df_questions['Body'].apply(lambda x: clean_text(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_questions['Body'][11]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Remove contractions","metadata":{}},{"cell_type":"code","source":"def expand_contractions(text):\n    \"\"\"expand shortened words, e.g. 'don't' to 'do not'\"\"\"\n    text = contractions.fix(text)\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndf_questions['Title'] = df_questions['Title'].apply(lambda x: expand_contractions(x)) \ndf_questions['Body'] = df_questions['Body'].apply(lambda x: expand_contractions(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_questions['Body'][11]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Spelling correction","metadata":{}},{"cell_type":"markdown","source":"I put this step here, and the code, but I did not make any corrections (It's far TOO much costly!). But if you want to try, there you are!","metadata":{}},{"cell_type":"code","source":"def autocorrect(text):\n    words = token.tokenize(text)\n    words_correct = [spell(w) for w in words]\n    return ' '.join(map(str, words_correct)) # Return the text untokenize","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n\n# df_questions['Title'] = df_questions['Title'].apply(lambda x: autocorrect(x)) \n# df_questions['Body'] = df_questions['Body'].apply(lambda x: autocorrect(x)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Lowering the text","metadata":{}},{"cell_type":"markdown","source":"I choose to lower the text here since the contractions package may put some capital letters back when removing the contractions.\nLowering the text is a classical and useful step of Noise removal or Text normalization since it reduce the vocabulary, normalize the text and cost almost nothing.","metadata":{}},{"cell_type":"code","source":"%%time\n\ndf_questions['Title'] = df_questions['Title'].str.lower()\ndf_questions['Body'] = df_questions['Body'].str.lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_questions['Body'][11]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2) Removing character","metadata":{}},{"cell_type":"markdown","source":"## 1. Removing all non-alphabetical character","metadata":{}},{"cell_type":"markdown","source":"Note that I choose to remove ALL non-alphabetical character (including punctuation, number and special character). Thus, I do not consider important words that may contain special characters (like \"C#\" in programming). You could choose to remove only punctuation and number or to not remove anything at all depending of you problematic!\nBut I recommend removing at least punctuation in most case, since it can interfere with tokenisation, and number since there are generally not useful.","metadata":{}},{"cell_type":"code","source":"def remove_punctuation_and_number(text):\n    \"\"\"remove all punctuation and number\"\"\"\n    return text.translate(str.maketrans(\" \", \" \", charac)) \n\n\n\ndef remove_non_alphabetical_character(text):\n    \"\"\"remove all non-alphabetical character\"\"\"\n    text = re.sub(\"[^a-z]+\", \" \", text) # remove all non-alphabetical character\n    text = re.sub(\"\\s+\", \" \", text) # remove whitespaces left after the last operation\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndf_questions['Title'] = df_questions['Title'].apply(lambda x: remove_non_alphabetical_character(x)) \ndf_questions['Body'] = df_questions['Body'].apply(lambda x: remove_non_alphabetical_character(x)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_questions['Body'][11]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Removing single character (optional)","metadata":{}},{"cell_type":"markdown","source":"I choose to remove single character since when we do programming we often use single alphabetical character as a variable name (\"x\", \"y\", \"z\", etc.). And I observed that when I tried some topic detection without removing them, I found a lot of topics with them! And even a topic that I could name \"Variable name\"...","metadata":{}},{"cell_type":"code","source":"def remove_single_letter(text):\n    \"\"\"remove single alphabetical character\"\"\"\n    text = re.sub(r\"\\b\\w{1}\\b\", \"\", text) # remove all single letter\n    text = re.sub(\"\\s+\", \" \", text) # remove whitespaces left after the last operation\n    text = text.strip(\" \")\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndf_questions['Title'] = df_questions['Title'].apply(lambda x: remove_single_letter(x)) \ndf_questions['Body'] = df_questions['Body'].apply(lambda x: remove_single_letter(x)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_questions['Body'][11]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3) Removing stopwords","metadata":{}},{"cell_type":"markdown","source":"## 1. Removing most frequent words","metadata":{}},{"cell_type":"markdown","source":"Removing the most frequent words is a classical step in NLP. Most frequent words don't add a lot of information in most case (since they are in almost every sentences). Removing them create more \"space\" to other that may have more useful information.   \nYou can use premade lists from libraries like SciKit-Learn, NLTK and others.\nBut be aware that those list may be more problematic than useful (especially the scikit-learn list, see [Stop Word Lists in Free Open-source Software Packages](https://www.aclweb.org/anthology/W18-2502.pdf) for more information).","metadata":{}},{"cell_type":"code","source":"def remove_stopwords(text):\n    \"\"\"remove common words in english by using nltk.corpus's list\"\"\"\n    words = token.tokenize(text)\n    filtered = [w for w in words if not w in stop_words]\n    \n    return ' '.join(map(str, filtered)) # Return the text untokenize","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndf_questions['Title'] = df_questions['Title'].apply(lambda x: remove_stopwords(x))\ndf_questions['Body'] = df_questions['Body'].apply(lambda x: remove_stopwords(x)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_questions['Body'][11]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Removing adjectives (optional)","metadata":{}},{"cell_type":"markdown","source":"Here, I choose to remove adjectives in addition to the NLTK list. Why? Simply because when I initially tried to make some topic detection in a notebook following this one and it improves my topic detection. I also thought that adjectives wouldn't add any useful information.\nAt the same time, I could also remove verbs with the same reasoning. But I did not because the StackOverflow dataset is about question on programming. And in programming, we have a lot of verbs, or words that may be interpreted as a verb, that may be important (\"return\", \"get\", \"request\", \"replace\", etc.).\nYou can use these types of reasoning to improve your preprocess. It will also reduce the vocabulary and thus, reduce your calculation time later on.","metadata":{}},{"cell_type":"code","source":"\ndef remove_by_tag(text, undesired_tag):\n    \"\"\"remove all words by using ntk tag (adjectives, verbs, etc.)\"\"\"\n    words = token.tokenize(text) # Tokenize each words\n    words_tagged = nltk.pos_tag(tokens=words, tagset=None, lang='eng') # Tag each words and return a list of tuples (e.g. (\"have\", \"VB\"))\n    filtered = [w[0] for w in words_tagged if w[1] not in undesired_tag] # Select all words that don't have the undesired tags\n    \n    return ' '.join(map(str, filtered)) # Return the text untokenize","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf_questions['Title'] = df_questions['Title'].apply(lambda x: remove_by_tag(x, adjective_tag_list))\ndf_questions['Body'] = df_questions['Body'].apply(lambda x: remove_by_tag(x, adjective_tag_list))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_questions['Body'][11]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4) Stemming / Lemmatization","metadata":{}},{"cell_type":"markdown","source":"Stemming and Lemmatization are operation that:\n- can improve your calculation time later on by reducing your vocabulary\n- help to generalize more easily by groupping words together (e.g. \"am\", \"are\", \"be\", etc will be transformed into \"be\" for lemmatization)\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Stemming","metadata":{}},{"cell_type":"markdown","source":"I did not choose to use stemming here but you should always consider this alternative since it's far less costly.\n\nStemming is the process of reducing inflected words to their word stem, base or root formâ€”generally a written word form (\"fishing\", \"fished\", and \"fisher\" to the stem \"fish\"). It generally operate by removing the affix of a word. A affix can be a suffix or a prefix (e.g. \"-ed\", \"-ing\", etc.). It's simple but will not work when the word is \"irregular\" (\"ran\" and \"run\"). Just think of it as a simpler operation than lemmatization, which can be enough in certain case, but can make too much mistake in other case. ","metadata":{}},{"cell_type":"code","source":"words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"]\n  \nfor w in words:\n    print(w, \" : \", stemmer.stem(w))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stem_text(text):\n    \"\"\"Stem the text\"\"\"\n    words = nltk.word_tokenize(text) # tokenize the text then return a list of tuple (token, nltk_tag)\n    stem_text = []\n    for word in words:\n        stem_text.append(stemmer.stem(word)) # Stem each words\n    return \" \".join(stem_text) # Return the text untokenize","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n\n# df_questions['Title'] = df_questions['Title'].apply(lambda x: stem_text(x)) \n# df_questions['Body'] = df_questions['Body'].apply(lambda x: stem_text(x)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Lemmatization\n\nAs said in the beginning, Lemmatization is the process of replacing the inflected form of a word by its lemma (cannonical form or dictionnary form). But in some case, a lemmatizer may not be able to find the right root if you don't precise the type of word as you can see below.","metadata":{}},{"cell_type":"code","source":"print(lemmatizer.lemmatize(\"stripes\", \"v\"))\nprint(lemmatizer.lemmatize(\"stripes\", \"n\"))  \nprint(lemmatizer.lemmatize(\"are\"))\nprint(lemmatizer.lemmatize(\"are\", \"v\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A way to work around this problem is to use a tagger and passe the type of word in the lemmatize function. BUT it's reaaaallly costly. Stemming or a simple lemmatization in this regard is far more efficient.","metadata":{}},{"cell_type":"code","source":"def lemmatize_text(text):\n    \"\"\"Lemmatize the text by using tag \"\"\"\n    \n    tokens_tagged = nltk.pos_tag(nltk.word_tokenize(text))  # tokenize the text then return a list of tuple (token, nltk_tag)\n    lemmatized_text = []\n    for word, tag in tokens_tagged:\n        if tag.startswith('J'):\n            lemmatized_text.append(lemmatizer.lemmatize(word,'a')) # Lemmatisze adjectives. Not doing anything since we remove all adjective\n        elif tag.startswith('V'):\n            lemmatized_text.append(lemmatizer.lemmatize(word,'v')) # Lemmatisze verbs\n        elif tag.startswith('N'):\n            lemmatized_text.append(lemmatizer.lemmatize(word,'n')) # Lemmatisze nouns\n        elif tag.startswith('R'):\n            lemmatized_text.append(lemmatizer.lemmatize(word,'r')) # Lemmatisze adverbs\n        else:\n            lemmatized_text.append(lemmatizer.lemmatize(word)) # If no tags has been found, perform a non specific lemmatization\n    return \" \".join(lemmatized_text) # Return the text untokenize","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndf_questions['Title'] = df_questions['Title'].apply(lambda x: lemmatize_text(x)) \ndf_questions['Body'] = df_questions['Body'].apply(lambda x: lemmatize_text(x)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_questions['Body'][11]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"markdown","source":"Just a little bit of FE. Using the title and the body at the same give far more better result for topic detection. ","metadata":{}},{"cell_type":"code","source":"df_questions['Text'] = df_questions['Title'] + ' ' + df_questions['Body']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data exportation","metadata":{}},{"cell_type":"code","source":"df_questions.to_csv('df_questions_fullclean.csv', encoding='utf-8', errors='surrogatepass')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}