{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DATASET is loaded into our code"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"these are the first five rows of the dataset. In this dataset dependent variable (y) is stroke"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()  #information about the dataset .. we get to know if there are any missing values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we found that the column named bmi has missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape #to know the exact shape of dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['bmi'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"in the bmi column we have 201 missing columns "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data['bmi'].mean())\nprint(data['bmi'].median())\nprint(data['bmi'].mode())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we should fill those na values in bmi \ni will be using median value to fill these na values "},{"metadata":{"trusted":true},"cell_type":"code","source":"data['bmi']=data['bmi'].fillna(data['bmi'].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the missing values are filled "},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns #datavisualization ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(data['age'],data['stroke'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x=data['bmi'],y=data['stroke'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see that there are object columns (gender,ever_married,work_type,Residence_type,smoking_status"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.get_dummies(data,columns=['gender'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.get_dummies(data,columns=['ever_married','work_type','Residence_type','smoking_status'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have used one hot encoding to convert object type to 0 or 1 .. \nwe can also use label encoder from sklearn\n\n\nLets assign columns to dependent and independent variables "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=data[['age','hypertension','heart_disease','avg_glucose_level','bmi','gender_Female','gender_Male','gender_Other','ever_married_No','work_type_Govt_job',\n       'work_type_Never_worked','work_type_Private','work_type_Self-employed','work_type_children','Residence_type_Rural','Residence_type_Urban','smoking_status_Unknown','smoking_status_formerly smoked','smoking_status_never smoked',\n       'smoking_status_smokes']]  #independent variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=data[['stroke']]  #dependent variable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lets split the data into train and test dataset. The dataset will be divided like train dataset contains 70% of the data and rest in test dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.3,random_state=7)\nprint(xtrain.shape)\nprint(xtest.shape)\nprint(ytrain.shape)\nprint(ytest.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets build the model using Logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing necessary libraries for model building\n\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=LogisticRegression()\nlr.fit(xtrain,ytrain)\na=lr.predict(xtest)    #result from logistic regression is stored in a for further metrics evaluation ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b=pd.DataFrame(a)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report as cr\ncr(a,ytest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier \nrfc=RandomForestClassifier()\nrfc.fit(xtrain,ytrain)\nc=rfc.predict(xtest)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cr(c,ytest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['stroke'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Highly imbalanced dataset ,\nsince it is imbalanced dataset we cannot classify correctly,\nthe solution for this issue will be addressed in next notebook \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"4861/249","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0's are 19.52 times that of 1's"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}