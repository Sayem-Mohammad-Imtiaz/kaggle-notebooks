{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"### Roteiro do Trabalho <br>\n1. Import das bibliotecas <br>\n1. Leitura do Dataset <br>\n1.1 Treino e Teste\n1. Captura das Features <br>\n1.1 Amostra das Classes <br>\n1.2 Plot das features\n1. Criação dos datasets de treino e teste <br>\n1. Criação do modelo <br> \n1. Teste de Predição <br>"},{"metadata":{},"cell_type":"markdown","source":"## Import das Bibliotecas"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport IPython.display as ipd\nimport math\nfrom pathlib import Path\nimport urllib\nimport scipy, matplotlib.pyplot as plt, sklearn, urllib, IPython.display as ipd\nimport librosa, librosa.display\nfrom sklearn.metrics import accuracy_score\n\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Leitura do dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"csv = pd.read_csv('/kaggle/input/urbansound8k/UrbanSound8K.csv')\ncsv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#retorna o caminho completo de cada arquivo do CSV. \ndef path_class(filename):\n    #cria um filtro com a linha onde o arquivo está. \n    excerpt = csv[csv['slice_file_name'] == filename]\n    \n    #cria o path completo\n    path_name = os.path.join('/kaggle/input/urbansound8k/', 'fold'+str(excerpt.fold.values[0]), filename)\n    return path_name, excerpt['class'].values[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#retorna o audio e informações do mesmo. \ndef to_dataset (df, fold=[]):\n    # df = dataframe a ser percorrido.\n    # fold = qual/quais folds ler. \n    audio = []\n    audio_signals = []\n    label= []\n    labels=[]\n    paths=[]   \n    sampling_rate=[]\n    librosa_sampling_rate = []\n    \n    # quando fold nulo, significa para ler tudo.         \n    if fold != []:\n        #filtra somentes os folds que foram enviados.  \n        filter_fold = df.fold.isin(fold)\n        df = df[filter_fold]\n\n    ###Descomentar para compilar mais rapido.     \n    #df = df.head(100)   \n\n    #para cada fold, pega todos os arquivos de dentro.            \n    for i in (df.fold.unique()):\n        #filtra o cada fold em cada iteração\n        filter_slice =  df['fold']==i\n        dt_fold = df[filter_slice]\n        \n        #Iteração para ler os arquivos da pasta fold da vez.\n        for p in dt_fold['slice_file_name']:\n            # Librosa já converte os dois canais para um canal e normaliza os dados entre 1 e -1. \n            audio,librosa_sampling_rate  = librosa.load('/kaggle/input/urbansound8k/fold'+str(i)+'/' + p)\n            audio_signals.append(audio)\n            sampling_rate.append(librosa_sampling_rate)\n            \n            #busca as classes e paths.\n            path, label = path_class(p)\n            paths.append(path)\n            labels.append(label)\n\n    \n    print('Reading...')    \n    \n    #audio = contém os arquivos de audio\n    #paths = contém o caminho completo do arquivo.\n    #labels = contem as classificações, \n    #librosa_sampling_rate = contém o sampling rate, que pode ser visto adiante. \n    return audio_signals,paths,labels,sampling_rate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cria os dados de treino.\n#Descomentar para rodar mais rápidp\n#Train, Train_Path, Train_Label,Train_S_Rate = to_dataset(csv,[1,2])\nTrain, Train_Path, Train_Label,Train_S_Rate = to_dataset(csv,[1,2,3,4,5,6,7,8,9])\n\nprint(\"Train set size: \" + str(len(Train)))\n\n#Cria os dados de teste\ntest, test_path, test_label,test_S_Rate  = to_dataset(csv,[10])\nprint(\"Test set size: \" + str(len(test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Análise\n\n> ### Preparacao do dataset de exemplo"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Para anlisar cada classe, primeiro é necessário extrair cada uma e criar um dataframe. \ndt_class = pd.DataFrame()\ndt_fold = (csv[csv['fold']==1])\nfor i in (dt_fold.classID.unique()):\n    dt_class = dt_class.append(dt_fold[dt_fold['classID']==i].head(1))    \ndt_class","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Envia o dataframe com as 10 classes para recuperar os audios e demais informações. \nsample, sample_path, sample_label, sample_S_Rate = to_dataset(dt_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Leitura de um wav de exemplo\nimport struct\nipd.Audio(sample_path[0])\n# encontrei arquivos de audio que nao tem dois canais. Embora a biblioteca deva resolver, \n# seria interessante fazer esse tratamento com wav.\n# nesses mesmo samples, alguns arquivos nao podem ser ouvidos pelo ipd.audio. \nprint('simple rate:')\nprint(sample_S_Rate)\n\n# Librosa converte o sampling rate para 22050 no processo. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotando com Librosa (WAVEPLOT)\nplt.figure(figsize=(15, 6))\nfor i, x in enumerate(sample):\n    plt.subplot(4, 3, i+1)\n    plt.title(label = sample_label[i])\n    librosa.display.waveplot(x[:10000])\n    #plt.ylim(-1, 1)\n\n# É possivel notar algumas diferenças na onda. \n# Se ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotando com Librosa (Fourier Transform)\n#converte tempo em frequencia. \n\nplt.figure(figsize=(10, 5))\nfor i, x in enumerate(sample):\n    plt.subplot(4, 3, i+1)\n    D = librosa.amplitude_to_db(np.abs(librosa.stft(sample[i])), ref=np.max)\n    librosa.display.specshow(D, y_axis='linear')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title(sample_label[i])\n    \n# Fica mais claro as diferenças. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extração das Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#exemplo de plot de cada classe. Usando Mel-Frequency Cepstral Coefficients (MFCC).\n#Define a função vista em aula para captura de feature. Realiza a media. \ndef extract_features(signal):\n    return  librosa.feature.mfcc(y=signal, n_mfcc = 40)\n\n# Cria um array com o MFCC de cada classe. \ndt_features = ([extract_features(x) for x in sample])\nprint(len(dt_features) )\n\n# Cria um plot para cada classe.  \nplt.figure(figsize=(8,8))\nfor i, x in enumerate(sample):\n    plt.subplot(4, 3, i+1)\n    librosa.display.specshow(dt_features[i], sr=sample_S_Rate[i], x_axis='time')\n    plt.colorbar(format='%+2.0f dB');\n    plt.title(sample_label[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#exemplo de plot de cada classe. Usando Mel Spectrogram\ndef extract_features(signal):\n    S = librosa.feature.melspectrogram(signal, n_fft=2048,hop_length=512, n_mels=128)\n    return librosa.power_to_db(S, ref=np.max)\n\n# Cria um array com o MFCC de cada classe. \ndt_features = ([extract_features(x) for x in sample])\n#print(len(dt_features) )\n\n# Cria um plot para cada classe.  \nplt.figure(figsize=(8,8))\nfor i, x in enumerate(sample):\n    plt.subplot(4, 3, i+1)\n    librosa.display.specshow(dt_features[i], hop_length=512, \n                         x_axis='time', y_axis='mel');\n    plt.colorbar(format='%+2.0f dB');\n    plt.title(sample_label[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### !!!!! vou comentar esse trecho, pois na função a seguir usarei o vetor sem fazer a media. \n\n#Extraindo as features para efetivamente Treinar (redefinindo a função para treino)\n#def extract_features(signal):                \n#    return  (\n#        #librosa.feature.zero_crossing_rate(signal).mean(),\n#        librosa.feature.mfcc(signal)\n#    )\n#converte para array para poder criar um dataframe mais a frente. \n#Train_label_np = np.array(Train_Label)\n\n#le a primeira feature\n#data = ([extract_features(x) for x in Train])\n#data\n\n\n#alguns autores nao fazem a media, deixam a coluna feature com o vetor inteiro... qual a diferença?\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Como não é necessário implementar o cross validation pelos paths dos folds, vou usar o train tests split. Por isso, o trecho abaixo coloca o fold separado pra teste pra dentro do de Treino. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train2 = Train+test#\n#Train_Label2 = Train_Label + test_label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remover ao implementar o cross validation\nTrain = Train+test\nTrain_Label = Train_Label+test_label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"def extract_features(signal):\n    mfccs = librosa.feature.mfcc(y=signal,  n_mfcc=40)\n    mfccs_processed = np.mean(mfccs.T,axis=0)\n     \n    return mfccs_processed     \n\nfeatures = []\n# Iterate through each sound file and extract the features \nfor x in Train:\n    features.append(extract_features(x))    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalizar features\n#scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(-1, 1))\n#training_features = scaler.fit_transform(data)\n#print(training_features.min(axis=0))\n#print(training_features.max(axis=0))\n#training_features\n\n#Embora eu tenha feira o scalar, nao vou usar para nada. Vou treinar com a feature no valor integral. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#converte em um pandas df\ndf = pd.DataFrame({'feature':features, 'label':Train_Label})\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Criando o modelo\n\n### Vou usar a biblioteca mais simples que encontrei. Keras"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.optimizers import Adam\nfrom keras.utils import np_utils\nfrom sklearn import metrics \nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Como os datasets estão como list, vou converter pra array para poder fazer o train_test_split. \nX = np.array(df.feature.tolist())\ny = np.array(df.label.tolist())\n\n# Como eu trouxe os labels da colunas de texto, vou fazer um encoder para mudar para numerico. \nle = LabelEncoder()\nyy = to_categorical(le.fit_transform(y)) \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nd = dict(zip(le.classes_, le.transform(le.classes_)))\n\nfilename = 'dict'\npickle.dump(d, open(filename, 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# separação do dataset\nfrom sklearn.model_selection import train_test_split \n\nx_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yy.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# \nnum_labels = yy.shape[1]\n\n# Usar um sSequential do Keras simples. Os parametros são os mais comuns. 3 camadas. \n\n# O input será 40, devido ao n_mfcc=40 que usei antes. \n# Referência https://keras.io/getting-started/sequential-model-guide/\n\ndef build_model(input_shape=(40,)):\n    model = Sequential()\n    # Uma primeira camada\n    model.add(Dense(256,  activation='relu'))\n    \n    #Dropout para reduziro o overfitting\n    model.add(Dropout(0.5))\n    \n    #Camada intermediária para completar o modelo\n    # Relu por ser a mais usada e com boa performance. \n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.5))    \n    \n    # como deve ter uma saida para cada classe, eu faço de acordo com o num_labels\n    model.add(Dense(num_labels))\n    \n    #Softmax por ser mais de duas classes\n    #https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/    \n    model.add(Activation('softmax'))\n    # Compila o modelo\n    # Da lista de metricas, a unica que me parece fazer sentido é a acurácia \n    # Fonte: https://keras.io/metrics/\n    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n    return model\n\n#Cria o modelo\nmodel = build_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Compila o modelo. \n# Descomentar se precisar recompilar\n#model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n\n# Pré Avalia o modelo.  \nscore = model.evaluate(x_test, y_test, verbose=1)\naccuracy = 100*score[1]\nprint(\"Pre-training accuracy: %.4f%%\" % accuracy) \n\n# Mesmo baixa, o valor do treino final fica melhor. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Usando o early stopping para evitar perder tempo. \n# Usando checkpoint para nao perder o melhor resultado. \nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint \nmc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=20)\n\n\nfrom datetime import datetime \nnum_epochs = 300\nnum_batch_size = 32\nmodel.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_split=0.33, verbose=1,callbacks=[es, mc])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Usando a metrica de acurácia que é a mais comum. \nscore = model.evaluate(x_train, y_train, verbose=0)\nprint(\"Training Accuracy: {0:.2%}\".format(score[1]))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Testing Accuracy: {0:.2%}\".format(score[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Export do modelo para usar no proximo algoritmo. (Além do best_model, claro) \nimport pickle\n# save the model to disk\nfilename = 'keras_audio_sequential.sav'\npickle.dump(model, open(filename, 'wb'))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusão\n\nApós ler várias fontes, compilando as ideias de cada uma e entendendo quase todos o funcionamento, foi possível criar um modelo que consegue identificar com quase 87% de acerto, os sons que serão testados na parte 2. \n\nEntendendo como a biblioteca Librosa funciona e como facilita a maioria das operações, pude ver o comportamento do audio e graficamente como são diferentes. \n\nAlguns desafios que surgiram foi plotar de forma que fique mais claro o que elas representam. No momento consegui plotar de forma ainda um pouco atrapalhada. Mas é visível como os arquivos de audio são diferentes nas imagens. \n\nGerar o modelo, após testar combinações de quantidade de unidades, modos de ativação e diferentes métricas, pareceu a melhor estratégia uma rede simples. Conforme visto em aula em diversos artigos. \n\nConsegui gerar um modelo que ainda não está perfeito e pode melhorar, principalmente na configuração das camadas, mas que vai conseguir acertar mais da metade da analise na parte 2 do trabalho. O Early Stopping parou na epoch 106. \n\nForam usados e gerados os seguintes arquivos:\n\nInputs:\n - urbansound8k\nOutputs:\n - best_model.h5 (modelo do checkpoint)\n - keras_audio_sequential.sav (modelo completo)\n - dict (dicionário das classes)\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}