{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Hello\nUsing TFRecords can really boost up training, but sometimes those provided in the original competition dataset are not enough, e.g.:\n* you need to add more data, but external data is only available in .jpeg format\n* you need to prepare augmented data instead of creating it on the fly\n* you need to perform knowledge distillation on soft labels, etc.\n\nA one-stop solution is `from_tensor_slices` method, which makes it just as easy as feeding a dataframe into the network but results in slower (really slower) runtime. E.g. training EfficientNetB4 on dataset of 25K 512x512 images takes nearly 5x more time:\n\n| | from tensor slices | from tfrecord files |\n| --- | --- | --- |\n| Max time per epoch | 996s | **132s** |\n| Average time per epoch | 502s | **81s** | "},{"metadata":{},"cell_type":"markdown","source":"On the other hand, serializing this dataset to TFRecords can be done in just 30 lines of code. Taking only 10 minutes on GPU, this step would save you up to few hours on TPU when training an ensemble or a large model.\n\nIn this short notebook I will take a dataset of soft targets to create TFRecords and then check whether this dataset is readable by feeding EfficientNetB0 with new TFRecords."},{"metadata":{},"cell_type":"markdown","source":"### Imports "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport random\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hardware configuration"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPUv3-8')\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    STRATEGY = tf.distribute.experimental.TPUStrategy(tpu)\n    BATCH_SIZE = 16 * STRATEGY.num_replicas_in_sync\nexcept:\n    print('Running on GPU/CPU')\n    STRATEGY = tf.distribute.get_strategy()\n    BATCH_SIZE = 8\n    \nprint('Number of replicas:', STRATEGY.num_replicas_in_sync)\nprint('Using tensorflow %s' % tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    print('Seeding everything with seed %.i' % seed)\n    \n\nAUTO = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 16 * STRATEGY.num_replicas_in_sync\nIMG_SIZE = 600\n\nseed_everything(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Helper functions (serialization)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"def _serialize_image(path):\n    image = tf.io.read_file(path)\n    image = tf.image.decode_jpeg(image)\n    image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE, IMG_SIZE)\n    return tf.image.encode_jpeg(image).numpy()\n\n\ndef _serialize_sample(image, image_id, p0, p1, p2, p3, p4):\n    feature = {\n        'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),\n        'image_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_id])),\n        'p0': tf.train.Feature(float_list=tf.train.FloatList(value=[p0])),\n        'p1': tf.train.Feature(float_list=tf.train.FloatList(value=[p1])),\n        'p2': tf.train.Feature(float_list=tf.train.FloatList(value=[p2])),\n        'p3': tf.train.Feature(float_list=tf.train.FloatList(value=[p3])),\n        'p4': tf.train.Feature(float_list=tf.train.FloatList(value=[p4]))}\n    sample = tf.train.Example(features=tf.train.Features(feature=feature))\n    return sample.SerializeToString()\n\n\ndef serialize_fold(fold, name):\n    samples = []\n    \n    for image, (image_id, p0, p1, p2, p3, p4) in fold.iterrows():\n        image = _serialize_image(image)\n        samples.append(_serialize_sample(image, image_id.encode(), p0, p1, p2, p3, p4))\n    \n    with tf.io.TFRecordWriter(name + '.tfrec') as writer:\n        [writer.write(x) for x in samples]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Run converter\nHere we combine soft predictions with hard labels (ground truth) and serialize images and new predictions to TFRecords."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_hard = pd.read_csv('../input/cassava-leaf-disease-merged/merged.csv')\n\ndf_hard_2020 = df_hard[df_hard['source'] == 2020].set_index('image_id')\ndf_hard_2019 = df_hard[df_hard['source'] == 2019].set_index('image_id')\n\ndf_soft_2020 = pd.read_csv('../input/cassava-leaf-disease-soft-targets-09-model/soft_targets_2020.csv')\ndf_soft_2019 = pd.read_csv('../input/cassava-leaf-disease-soft-targets-09-model/soft_targets_2019.csv')\n\nhard_labels_2020 = LabelBinarizer().fit_transform(df_hard_2020['label']).astype('float32')\nhard_labels_2019 = LabelBinarizer().fit_transform(df_hard_2019['label']).astype('float32')\n\nmixed_labels_2020 = 3 * df_soft_2020[['p0', 'p1', 'p2', 'p3', 'p4']].values + 7 * hard_labels_2020\nmixed_labels_2020 = np.array([x / np.sum(x) for x in mixed_labels_2020])\n\nmixed_labels_2019 = 3 * df_soft_2019[['p0', 'p1', 'p2', 'p3', 'p4']].values + 7 * hard_labels_2019\nmixed_labels_2019 = np.array([x / np.sum(x) for x in mixed_labels_2019])\n\nroot = '../input/cassava-leaf-disease-merged/train'\n\ndf_soft_2020.index = [os.path.join(root, x) for x in df_soft_2020['image_id']]\ndf_soft_2019.index = [os.path.join(root, x) for x in df_soft_2019['image_id']]\n\ndf_soft_2020.loc[:, 'p0':'p4'] = mixed_labels_2020\ndf_soft_2019.loc[:, 'p0':'p4'] = mixed_labels_2019","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.mkdir('./2020')\nos.mkdir('./2019')\n\nn_folds = 50\nsamples = []\n\nfor i, fold in tqdm(enumerate(np.array_split(df_soft_2020, n_folds)), total=n_folds):\n    serialize_fold(fold, name='./2020/cldc-%.2i-%.i' % (i + 1, len(fold)))\n    \nfor i, fold in tqdm(enumerate(np.array_split(df_soft_2019, n_folds)), total=n_folds):\n    serialize_fold(fold, name='./2019/cldc-%.2i-%.i' % (i + 1, len(fold)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Helper functions (parsing & testing)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"feature_map = {\n    'image': tf.io.FixedLenFeature([], tf.string),\n    'image_id': tf.io.FixedLenFeature([], tf.string),\n    'p0': tf.io.FixedLenFeature([], tf.float32),\n    'p1': tf.io.FixedLenFeature([], tf.float32),\n    'p2': tf.io.FixedLenFeature([], tf.float32),\n    'p3': tf.io.FixedLenFeature([], tf.float32),\n    'p4': tf.io.FixedLenFeature([], tf.float32)}\n\n\ndef count_data_items(filenames):\n    return np.sum([int(x[:-6].split('-')[-1]) for x in filenames])\n\n\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    return image\n\n\ndef scale_image(image, target):\n    image = tf.cast(image, tf.float32) / 255.\n    return image, target\n\n\ndef read_tfrecord(example):\n    example = tf.io.parse_single_example(example, feature_map)\n    image = decode_image(example['image'])\n    target = [\n        example['p0'],\n        example['p1'],\n        example['p2'],\n        example['p3'],\n        example['p4']]\n    return image, target\n\n\ndef get_dataset(filenames):\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n    dataset = dataset.map(scale_image, num_parallel_calls=AUTO)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return STRATEGY.experimental_distribute_dataset(dataset)\n\n\ndef get_model():\n    model = tf.keras.models.Sequential([\n        tf.keras.applications.EfficientNetB0(\n            include_top=False,\n            input_shape=(None, None, 3),\n            weights=None,\n            pooling='avg'),\n        tf.keras.layers.Dense(5, activation='softmax')\n    ])\n    model.compile(\n        optimizer='adam',\n        loss='categorical_crossentropy',\n        metrics=['accuracy'])\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Run test\nHere we check that new TFRecords can be parsed and fed into our convnet"},{"metadata":{"trusted":true},"cell_type":"code","source":"filenames = tf.io.gfile.glob('./2020/cldc-*.tfrec')[:2]\ndataset = get_dataset(filenames)\n\nsteps_per_epoch = count_data_items(filenames) // BATCH_SIZE\n\nwith STRATEGY.scope():\n    model = get_model()\n\nmodel.summary()\n\nhistory = model.fit(\n    dataset, \n    steps_per_epoch=steps_per_epoch,\n    epochs=1,\n    verbose=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Acknowledgements\n* Many thanks to @dimitreoliveira for his **[amazing work](https://www.kaggle.com/dimitreoliveira/cassava-leaf-disease-training-with-tpu-v2-pods)** I learnt and took a lot from"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}