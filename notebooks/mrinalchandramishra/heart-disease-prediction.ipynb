{"cells":[{"metadata":{},"cell_type":"markdown","source":"#                            Heart Disease Prediction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"[](https://www.google.com/imgres?imgurl=https%3A%2F%2Fimages.ctfassets.net%2Fyixw23k2v6vo%2F6BezXYKnMqcG4LSEcWyXlt%2Fb490656e99f34bc18999f3563470eae6%2FiStock-1156928054.jpg%3Fw%3D802%26fm%3Djpg%26fit%3Dthumb%26q%3D65%26fl%3Dprogressive&imgrefurl=https%3A%2F%2Fwww.healthcentral.com%2Farticle%2Fcovid-19-dangerous-heart-disease&tbnid=FdkQRbtTfOzDIM&vet=12ahUKEwi44Oy1kcjqAhVID7cAHV8JBIAQMygHegUIARDbAQ..i&docid=5SLrZ_n3DML6cM&w=802&h=535&q=heart%20disease&ved=2ahUKEwi44Oy1kcjqAhVID7cAHV8JBIAQMygHegUIARDbAQ)\nHeart disease describes a range of conditions that affect your heart. Diseases under the heart disease umbrella include blood vessel diseases, such as coronary artery disease; heart rhythm problems (arrhythmias); and heart defects you're born with (congenital heart defects), among others.\n\nThe term \"heart disease\" is often used interchangeably with the term \"cardiovascular disease.\" Cardiovascular disease generally refers to conditions that involve narrowed or blocked blood vessels that can lead to a heart attack, chest pain (angina) or stroke. Other heart conditions, such as those that affect your heart's muscle, valves or rhythm, also are considered forms of heart disease","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \nimport seaborn as sns\nfrom sklearn.utils import shuffle","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataset=pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Intro to DataSet\nAs we can see here there are 14 columns in this dataset having different feature.\n- Independent Feature:\nAge,Sex,cp,trestbps,chol,fbs,restecg,thalach,exang,oldpeak,slope,ca,thal\n- Dependent Feature:\ntarget\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we can see that it is very hard to understand columns which are in abbrv. form  so lets sort out the columns name to get better understanding of it**\n*what every columns mean*\n- cp : Cerebral palsy (CP)\n- trestbps : Resting blood Pressure\n- chol : cholestrol level\n- fbs : Fasting blood sugar\n- restecg : resting electrocardiographic.\n- thalach : maximum heart rate acheived\n- exang : exercise induced angina(exang)\n- oldpeak : ST depression induced by exercise relative to rest.\n- slope : heart rate slope \n- ca : coronary artery\n- thal : thallesimea\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Exploratory Data Analysis**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# now check the shape of data set\ndataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above code we can see the dataset information which contains mainly 303 **Rows** and 14 **columns**.\n- It can also be analyzed that all the 14 columns are numerical value in the dataset out of which *target* is dependent columns. Now it will be easier for us to perform furthur exploration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets describe the dataset\n\nprint('Unique value available in columns:')\nprint('SEX has',dataset['sex'].unique())\nprint('CP has',dataset['cp'].unique())\nprint('Fbs has',dataset['fbs'].unique())\nprint('EXANG has',dataset['exang'].unique())\nprint('Slope has',dataset['slope'].unique())\nprint('ca has',dataset['ca'].unique())\nprint('thal has',dataset['thal'].unique())\nprint('target has',dataset['target'].unique())\nprint('\\t')\nprint(dataset.describe().T)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Age has mean of 54 years, minimum age is **29** and maximum **77**\n- trestbps has mean of 131, minimum  is **94** and maximum **200**\n- thal has mean of 2.31, minimum age is **0** and maximum **3**\n\n**Lets Check for any Null value in dataset**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isnull().any()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As there are no null values associated with any columns\n**Now we will see the distribution of our data**\n- age\n- sex\n- target","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\nfig,ax=plt.subplots(1,2,figsize=(20,5))\nax[0].set_title('Age Distribution')\nax[1].set_title('Resting Blood Pressure ')\nsns.distplot(dataset['age'],ax=ax[0])\nsns.distplot(dataset['trestbps'],ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this, We can observe the following things:\n- That mostly people are from the above 40 age and less than 70 years old.\n- Most have blood pressure in bw 120 to 140.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Lets analyze about the target distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\nplt.figure(figsize=(6,4))\nsns.countplot(dataset['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that it has:\n- There are approx 140 people not having heart disease\n- There are approx 170 people having heart disease \n\nSo we can say that our dataset is perfectly balanced and equilly distributed","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### We are done with EDA now lets focus on further process.\n\n# Feature Selection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we will look for correlation of different feature.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix=dataset.corr()\nplt.figure(figsize=(15,6))\nsns.heatmap(corr_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like every feature is important for us.\nbefore doing model selection we have to shuffle our dataset as there are data splited to 0 and 1.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset.head(2))\nprint(dataset.tail(2))\n\ndataset=shuffle(dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now our dataset is randomly shuffeled and it will be better to use randomized sample.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now splitting independent variable and dependent variable into different dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_dataset=dataset.drop('target',axis=1)\ny_dataset=dataset['target']\nprint(x_dataset.shape,y_dataset.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n# transform data\nx_dataset = scaler.fit_transform(x_dataset)\nprint(x_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import RandomizedSearchCV\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now everything is good lets divide our dataset into train and test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting trainx,test_x,train_y,tets_y of size 20 percent\nX_train,X_test,y_train,y_test=train_test_split(x_dataset,y_dataset,test_size=0.2,random_state=0)\nprint(X_train.shape,y_train.shape)# validating shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Selection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we are ready for model selection. It is hard to say that any one **GOD** algorithm performs well in every scenario. Every Model performs differently  on different dataset it is upto us to analyze which model perform better in this kind of scenario. There is no **GOD** algorithm or say algorithm which fits perfectly on every dataset.\nFor model selection we have to analyze scores or accuracy of different algorithm and find one which best fits our modela and thus satisfy our problem. Let's say for this kind of dataset where we are predicting the heart disease of a person it is thus important for us to get maximum precision score.\nwe have to focus on decreasing **FALSE POSITIVE RATE (FPR)** = **false positive(FP)**/**false positive + True negative(TN)**\n\nThere are certain classification algorithm which we will analyze for our model:\n- KNNclassifier\n- SVMclassifier\n- RandomForest\n- Xgboost\n- logistics regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model1=KNeighborsClassifier()\nmodel2= SVC()\nmodel3= XGBClassifier()\nmodel4= RandomForestClassifier()\nmodel5= LogisticRegression()\nmodels=[model1,model2,model3,model4,model5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for model in models:\n    model.fit(X_train,y_train)\n    print(model)\n    print('score',model.score(X_test,y_test))\n    y_pred=model.predict(X_test)\n    print('F1-score=',f1_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above cell, we can observe that our accuracy best fits with Random Forest classifiers at an score of 0.88 and f1_score of 0.89.\nwell this score is not that good but we can surely increase it by parameter tuning in the below cell.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating model as RandomizedSearchCV\nmodel=RandomForestClassifier()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hyper parameter tuning**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There are two ways to tune parameters \n- Grid search Cv\n- Randomized Search Cv\n\nwe will use Randomized search cv as it takes less time and check randomly for parameters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining param grid\nn_estimators = [100, 200, 300, 400, 500]\nmax_features = ['auto', 'sqrt']\nmax_depth = [5, 10, 20, 30, 40, 50]\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\n        \nrandom_grid = {'n_estimators': n_estimators,\n               'criterion':['gini','entropy'],\n                       'max_features': max_features,\n                       'max_depth': max_depth,\n                       'min_samples_split': min_samples_split,\n                       'min_samples_leaf': min_samples_leaf,\n                       'bootstrap': bootstrap}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#randomSearch=RandomizedSearchCV(estimator =model, param_distributions = random_grid, cv = 10)\n#randomSearch.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets print the best param and best score of after tuning\n#print('best_params',randomSearch.best_params_)\n#print('best-score',randomSearch.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=RandomForestClassifier(n_estimators=500,min_samples_split=2,min_samples_leaf=4,max_features='sqrt',max_depth=10,bootstrap=False,criterion='gini')\nmodel.fit(X_train,y_train)\nprint(model)\nprint('score',model.score(X_test,y_test))\ny_pred=model.predict(X_test)\nprint('F1-score=',f1_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=model.predict(X_test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n# save the model to disk\nfilename = 'finalized_model.pkl'\npickle.dump(model, open(filename, 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}