{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom collections import Counter\nimport seaborn as sns\nfrom tqdm import tqdm\nimport keras.backend as K\nfrom keras.utils import to_categorical\nfrom keras.layers import Conv2DTranspose\nfrom keras.models import Sequential, Model\nfrom keras.layers import CuDNNLSTM, Dense,Dropout,Conv1D, MaxPool1D, Reshape, UpSampling1D, Flatten,Softmax,Activation,Add,Reshape, Input, MaxPooling1D\nfrom keras.backend.tensorflow_backend import set_session\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.utils import class_weight\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\nconfig.log_device_placement = True  # to log device placement (on which device the operation ran)\nsess = tf.Session(config=config)\nset_session(sess)\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom keras.optimizers import Adam\nimport os\nimport math","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, I am exploring the dataset and doing some basic model testing to see how far I can take the accuracy.\n\nHere are some of the criteria for my tests:\n\n1. Without data augmentation - Trying to see how far the score can be taken without creating fake data.\n2. With data augmentation - To see how much data augmentation helps in this case.\n\nFor each of the criteria above, We shall use 3 models. Two of them were made randomly. The last one is from another kernel(https://www.kaggle.com/coni57/model-from-arxiv-1805-00794)\nFor each of the models, We shall do class weighting, We shall use Adam Optimizer with learning rate scheduler and a batch size of 500 (source https://www.kaggle.com/coni57/model-from-arxiv-1805-00794).\nSo total 6 conditions.\nWe shall also see how precision, recall and F1 score changes with model and augmentation.\n\n****First, Let's load the mitbh dataset\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir=\"../input/heartbeat\"\nmit_train_file = os.path.join(data_dir,\"mitbih_train.csv\")\nrd_mit_ds = pd.read_csv(mit_train_file, header=None,names=([\"data-\"+str(i) for i in range(187)]+[\"target\"]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just printing the total number of samples."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(rd_mit_ds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's a good number of samples if class distribution is nearly even. Now let's divide the dataframe into feature and target. The last(187th,starting from 0) column is our Y or target."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_mit_feat=rd_mit_ds.iloc[:,:187]\ntrain_mit_target=rd_mit_ds[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have the data labels. Let's explore the data distribution first.\nBefore that, Let's replace the class numbers(0.0-4.0) with category labels (from dataset description)."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_category={ 0.0:'N', 1.0:'S', 2.0:'V', 3.0:'F', 4.0:'Q'}\nrd_mit_ds[\"target_cat\"] = rd_mit_ds[\"target\"].map(dataset_category)\nsns.countplot(x=\"target_cat\",data=rd_mit_ds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, hope of getting a well distributed dataset is everyone's dream. But sometimes dream and reality lie in completely different dimensions.\nSo,one class is quite big compared to others. 'N' probably represents normal. So most people have normal heartbeat(good). Even though it's a good thing that most people have normal heartbeat, it's not so good thing for people who play with data.\n\nJust a percentage print to add to the injury "},{"metadata":{"trusted":true},"cell_type":"code","source":"cnt_mit_train=Counter(rd_mit_ds[\"target\"])\ns_lbl_mit=sum([val for key,val in cnt_mit_train.items()])\nfor key,value in cnt_mit_train.items():\n    print(key,cnt_mit_train[key]/s_lbl_mit)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's do it on the test data now"},{"metadata":{"trusted":true},"cell_type":"code","source":"mit_test_file = os.path.join(data_dir,\"mitbih_test.csv\")\nrd_mit_ds_test = pd.read_csv(mit_test_file, header=None,names=([\"data-\"+str(i) for i in range(187)]+[\"target\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(rd_mit_ds_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_mit_feat=rd_mit_ds_test.iloc[:,:187]\ntest_mit_target=rd_mit_ds_test[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"target\",data=rd_mit_ds_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnt_mit_test=Counter(test_mit_target)\ns_lbl_mit_test=sum([val for key,val in cnt_mit_test.items()])\nfor key,value in cnt_mit_test.items():\n    print(int(key),cnt_mit_test[key]/s_lbl_mit_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interestingly, the distribution is exactly the same. So it was split from one big file may be."},{"metadata":{},"cell_type":"markdown","source":"That's one unbalanced dataset(both train and test). So let's prepare some test scenerios. \nWe need to make a class. Which will - \n1. Take model as input\n2. Take data as input(both train and test).\n3. Apply class weights.\n4. Perform training with provided EarlyStopping and ModelCheckpoint and Learningrate Scheduler.\n5. Evaluate with test data.\n6. Get the classification report on a pandas dataframe for later analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"class train_and_evaluate:\n    def __init__(self,\n                 name,\n                 model,\n                 train_feat,\n                 train_label,\n                 test_feat,\n                 test_label,\n                 batch_size=64,\n                 do_class_weighting=False,\n                 custom_class_weights=None,\n                 model_optimizer=\"adam\",\n                 metrics=['accuracy'],\n                 val_split=0.2):\n        self.name=name\n        self.model=model\n        self.train_feat=train_feat\n        self.test_feat=test_feat\n        self.train_label=train_label\n        self.test_label=test_label\n        self.do_class_weighting=do_class_weighting\n        self.custom_class_weights=custom_class_weights\n        self.model_optimizer=model_optimizer\n        self.metrics=metrics\n        self.val_split=val_split\n        self.batch_size=batch_size\n        #data_reshaping\n        self.train_X=np.reshape(self.train_feat.values,(len(self.train_feat),187,1))\n        self.train_Y=to_categorical(self.train_label,num_classes=5)\n        self.test_X=np.reshape(self.test_feat.values,(len(self.test_feat),187,1))\n        self.test_Y=to_categorical(self.test_label,num_classes=5)\n    def exp_decay(self,epoch):\n        initial_lrate = 0.001\n        k = 0.75\n        t = self.train_X.shape[0]//(10000 * self.batch_size)  # every epoch we do n_obs/batch_size iteration\n        lrate = initial_lrate * math.exp(-k*t)\n        return lrate\n    def train(self,\n              eps=100):\n        class_weights=None\n        T_X,V_X,T_Y,V_Y=train_test_split(self.train_X,self.train_Y,test_size=self.val_split,random_state=42)\n        if self.do_class_weighting==True:\n            if self.custom_class_weights is not None:\n                class_weights=self.class_weights\n            else:\n                class_weights=class_weight.compute_class_weight('balanced',\n                                                np.unique(T_Y.argmax(axis=1)),\n                                                T_Y.argmax(axis=1))\n        es=EarlyStopping(patience=5)\n        mcp=ModelCheckpoint(filepath=\"weights_{}.h5\".format(self.name),save_best_only=True,save_weights_only=True)\n        lrate = LearningRateScheduler(self.exp_decay)\n        self.model.compile(loss=\"categorical_crossentropy\",optimizer=self.model_optimizer,metrics=self.metrics)\n        self.model.fit(T_X,T_Y,batch_size=self.batch_size,epochs=eps,verbose=1,validation_data=[V_X,V_Y],class_weight=class_weights,callbacks=[lrate,es,mcp])\n    def evaluate(self):\n        self.model.load_weights(\"weights_{}.h5\".format(self.name))\n        preds=self.model.predict(self.test_X)\n        print(self.model.evaluate(self.test_X,self.test_Y))\n        return confusion_matrix(self.test_label,preds.argmax(axis=1)),classification_report(self.test_label,preds.argmax(axis=1),output_dict=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#introducing model 1, small and cute Conv1D and LSTM based model.\ndef model_initial():\n    mdl=Sequential()\n    mdl.add(Conv1D(64,(3,),activation=\"relu\",input_shape=(187,1),padding=\"same\"))\n    mdl.add(MaxPool1D())\n    mdl.add(CuDNNLSTM(93))\n    mdl.add(Dense(50,activation=\"relu\"))\n    mdl.add(Dense(5,activation=\"softmax\"))\n    return mdl\n#Now comes model 2, no LSTM, Lot bigger but based on conv and pooling layers and some dense layers\ndef model_cnn():\n    mdl=Sequential()\n    mdl.add(Conv1D(32,(3,),activation=\"relu\",input_shape=(187,1),padding=\"same\"))\n    mdl.add(Conv1D(32,(3,),activation=\"relu\"))\n    mdl.add(Conv1D(32,(3,),activation=\"relu\"))\n    mdl.add(MaxPool1D())\n    mdl.add(Conv1D(32,(3,),activation=\"relu\"))\n    mdl.add(Conv1D(32,(3,),activation=\"relu\"))\n    mdl.add(MaxPool1D())\n    mdl.add(Flatten())\n    mdl.add(Dense(50,activation=\"relu\"))\n    mdl.add(Dense(5,activation=\"softmax\"))\n    return mdl\n#The final, super big model from that nice notebook\ndef model_copied():\n    inp = Input(shape=(187, 1))\n    C = Conv1D(filters=32, kernel_size=5, strides=1)(inp)\n\n    C11 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(C)\n    A11 = Activation(\"relu\")(C11)\n    C12 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A11)\n    S11 = Add()([C12, C])\n    A12 = Activation(\"relu\")(S11)\n    M11 = MaxPooling1D(pool_size=5, strides=2)(A12)\n\n\n    C21 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M11)\n    A21 = Activation(\"relu\")(C21)\n    C22 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A21)\n    S21 = Add()([C22, M11])\n    A22 = Activation(\"relu\")(S11)\n    M21 = MaxPooling1D(pool_size=5, strides=2)(A22)\n\n\n    C31 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M21)\n    A31 = Activation(\"relu\")(C31)\n    C32 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A31)\n    S31 = Add()([C32, M21])\n    A32 = Activation(\"relu\")(S31)\n    M31 = MaxPooling1D(pool_size=5, strides=2)(A32)\n\n\n    C41 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M31)\n    A41 = Activation(\"relu\")(C41)\n    C42 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A41)\n    S41 = Add()([C42, M31])\n    A42 = Activation(\"relu\")(S41)\n    M41 = MaxPooling1D(pool_size=5, strides=2)(A42)\n\n\n    C51 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M41)\n    A51 = Activation(\"relu\")(C51)\n    C52 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A51)\n    S51 = Add()([C52, M41])\n    A52 = Activation(\"relu\")(S51)\n    M51 = MaxPooling1D(pool_size=5, strides=2)(A52)\n\n    F1 = Flatten()(M51)\n\n    D1 = Dense(32)(F1)\n    A6 = Activation(\"relu\")(D1)\n    D2 = Dense(32)(A6)\n    D3 = Dense(5)(D2)\n    A7 = Softmax()(D3)\n\n    model = Model(inputs=inp, outputs=A7)\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<H1>Case 1: No augmentation</H1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"global_conf_mat={}\nglobal_class_rpt={}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras.backend as K\ndef get_dict(name):\n    global global_class_rpt\n    global global_conf_mat\n    model_arr=[model_initial,model_cnn,model_copied]\n    dict_conf_mat={}\n    dict_classification_rpt={}\n    for model in model_arr:\n        print(\"current model: \",model.__name__)\n        K.clear_session()\n        model_to_use=model()\n        trainer_obj  =  train_and_evaluate(name=model.__name__+\"_class_wt_no_aug\",\n                                           model=model_to_use,\n                                           train_feat=train_mit_feat,\n                                           train_label=train_mit_target,\n                                           test_feat=test_mit_feat,\n                                           test_label=test_mit_target,\n                                           batch_size=500,\n                                           do_class_weighting=True,\n                                           model_optimizer=Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999))\n        trainer_obj.train()\n        conf_mat,class_rpt=trainer_obj.evaluate()\n        dict_conf_mat[model.__name__]=conf_mat\n        dict_classification_rpt[model.__name__]=class_rpt\n    global_conf_mat[name]=dict_conf_mat\n    global_class_rpt[name]=dict_classification_rpt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_dict(\"no_aug\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"global_class_rpt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nwith open(\"class_rpt_no_aug.pkl\",\"wb\") as f:\n    pickle.dump(global_class_rpt,f)\nwith open(\"conf_mat_no_aug.pkl\",\"wb\") as f:\n    pickle.dump(global_conf_mat,f)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new=pd.DataFrame(global_class_rpt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_lst=[]\nfor key_aug,value_model in global_class_rpt.items():\n    if isinstance(value_model,dict):\n        for key_model,value_class in value_model.items():\n            if isinstance(value_class,dict):\n                for key_class,value_params in value_class.items():\n                    if isinstance(value_params,dict):\n                        try:\n                            lst_item=[key_aug,key_model,int(float(key_class)),value_params[\"precision\"],value_params[\"recall\"],value_params[\"f1-score\"]]\n                            total_lst.append(lst_item)\n                        except:\n                            pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_res=pd.DataFrame.from_records(total_lst,columns=[\"is_aug\",\"model\",\"class\",\"precision\",\"recall\",\"f1_score\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_res.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_res.to_csv(\"no_aug.csv\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}