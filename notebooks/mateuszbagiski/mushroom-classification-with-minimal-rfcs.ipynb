{"cells":[{"metadata":{},"cell_type":"markdown","source":"Classifying mushrooms as edible or poisonous, given 21 categorical features turns out to be very easy task for random forests - they consistently achieve almost 100 % accuracy, even when trained on as little as only 10% of the original data and tested on the remaining 90%.  \n\nHowever, when collecting mushrooms on our own, we probably don't have enough time and/or executive control/willpower to check all 21 features in each mushroom we have collected. So maybe there is a better way. Maybe there is a small subset of features, which we could feed into our model and get a reasonable prediction, whether this mushroom is edible, possibly supplemented by the probability a.k.a. a measure of certainty of our judgment.\n\nHere is where feature importances come into play. We can check what features our models mostly rely on and which features are basically useless in determining, whether we can safely eat it. As it turns out, we can restrict ourselves to only a small sample of features without losing any accuracy.\n\nIn the later part I did some experiments with constraining random forests to see how reducing their maximal depth and number of leaf nodes impacts accuracy and certainty of predictions."},{"metadata":{},"cell_type":"markdown","source":"# 1. Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"First, let's import all the data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\ninput_dir = '/kaggle/input/mushroom-classification/mushrooms.csv'\n\ndata = pd.read_csv(input_dir)\n\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 8124 rows representing individual mushrooms and 23 columns, each of which contains information about one feature.\n\nThere are no nulls in this data and all the data is categorical in type."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print(\"Any nulls?\", data.isnull().any().any()) # no nulls\ndata.info() # all good, all categorical data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can extract the information about what unique labels occur in each feature columna and the number of these unique labels into a DataFrame:"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_data = pd.DataFrame({col: [len(set(data[col])), list(set(data[col]))] for col in data.columns}).T\nlabels_data.columns = ['# unique labels', 'Labels']\nlabels_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since veil-type feature has only one value ('p'), it doesn't give any useful information and thus we can discard it."},{"metadata":{"trusted":true},"cell_type":"code","source":"if 'veil-type' in data.columns:\n    data.drop('veil-type', axis=1, inplace=True)\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can continue our exploration by comparing the distribution of each feature among the edible and poisonous examples:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('darkgrid')\n\nfig, axs = plt.subplots(7,3, figsize=(20,15))\n\nplt.subplots_adjust(top=2, bottom=None)\n\n\nfor i, col in enumerate(data.columns[1:]):\n    sns.countplot(\n        x=col, hue='class',\n        data=data,\n        ax=axs[i//3, i%3]\n    )\n    axs[i//3, i%3].set_title(col)\n    axs[i//3, i%3].set_xlabel(None)\n    axs[i//3, i%3].set_ylabel(None)\n    axs[i//3, i%3].legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**p** (blue color) stands for *poisonous*, whereas **e** (orange) stands for *edible*.\n\nYou can look up the dataset description if you would like to know what individual letters below each blue-orange pair of bars mean.\n\nWhile most individual features can be found both among the edible and poisonous mushrooms, though to varying extents, some can be very clear indicators. For example, look at the spore-print-color chart. Almost all the examples with 'h' label (which stands for 'chocolate') are poisonous."},{"metadata":{"trusted":true},"cell_type":"code","source":"spc_count = data.query('`spore-print-color`==\"h\"')['class'].value_counts()\nprint(spc_count)\nprint(spc_count['p']/spc_count.sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, if you encounter a mushroom with chocolate-color spore print, you can be 97% certain, it's not edible (assuming its dataset is reliable.\n\nGill-color seems to be an even better indicator. On its chart, we don't see any orange above the 'b' label (standing for 'buff')."},{"metadata":{"trusted":true},"cell_type":"code","source":"gc_count = data.query('`gill-color`==\"b\"')['class'].value_counts()\nprint(gc_count)\nprint(gc_count['p']/gc_count.sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You definitely should not eat mushrooms with buff-colored gills.\n\nBut these two rules are not infallible. There are many mushrooms with neither chocolate-colored spore prints nor buff-colored gills, that are still toxic."},{"metadata":{"trusted":true},"cell_type":"code","source":"other_toxic_count = data.query('`spore-print-color`!=\"h\" & `gill-color`!=\"b\"')['class'].value_counts()\nprint(other_toxic_count)\nprint(other_toxic_count['p']/other_toxic_count.sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So you would throw away most of the toxic examples, but I still would not bet my life/health on 88% certainty.\n\nBut maybe if we exclude these two from our data, a third clear indicator will emerge...?"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_excl_1 = data.query('`spore-print-color`!=\"h\" & `gill-color`!=\"b\"')\n\nsns.set_style('darkgrid')\n\nfig, axs = plt.subplots(7,3, figsize=(20,15))\n\nplt.subplots_adjust(top=2, bottom=None)\n\n\nfor i, col in enumerate(data.columns[1:]):\n    sns.countplot(\n        x=col, hue='class',\n        data=data_excl_1,\n        ax=axs[i//3, i%3]\n    )\n    axs[i//3, i%3].set_title(col)\n    axs[i//3, i%3].set_xlabel(None)\n    axs[i//3, i%3].set_ylabel(None)\n    axs[i//3, i%3].legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Odor labels 'p' (pungent) and 'c' (creosote) seem promising."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_excl_2 = data_excl_1.query('odor!=\"c\" & odor!=\"p\"')\ndata_excl_2_counts = data_excl_2['class'].value_counts()\n\nprint(data_excl_2_counts)\nprint(data_excl_2_counts['p']/data_excl_2_counts.sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Down to 3.6%. Let's keep going..."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('darkgrid')\n\nfig, axs = plt.subplots(7,3, figsize=(20,15))\n\nplt.subplots_adjust(top=2, bottom=None)\n\n\nfor i, col in enumerate(data.columns[1:]):\n    sns.countplot(\n        x=col, hue='class',\n        data=data_excl_2,\n        ax=axs[i//3, i%3]\n    )\n    axs[i//3, i%3].set_title(col)\n    axs[i//3, i%3].set_xlabel(None)\n    axs[i//3, i%3].set_ylabel(None)\n    axs[i//3, i%3].legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like we could get close to 100% certainty, if we just threw away all the mushrooms with enlarging stalks (stalk shape 'e' label). This way, however, we would also lose a lot of edible mushrooms."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_excl_3 = data_excl_2.query('`stalk-shape`!=\"e\"')\ndata_excl_3_counts = data_excl_3['class'].value_counts()\nprint(data_excl_3_counts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alternatively, we could discriminate further, based on spore print color. It seems that all the remaining poisonous mushrooms have either white ('w' label) or red ('r' label) spores, although in this way we also lose many edible mushrooms."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_excl_4 = data_excl_2.query('`spore-print-color`!=\"w\" & `spore-print-color`!=\"r\"')\ndata_excl_4_counts = data_excl_4['class'].value_counts()\nprint(data_excl_4_counts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's much better. In this way, we preserved almost 1000 more edible mushrooms."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_excl_4_counts - data_excl_3_counts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nevertheless, we lost quite a lot of edible mushrooms from our original mushroom set."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['class'].value_counts()['e'] - data_excl_4_counts['e']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So maybe there is a better way. Maybe there is a way not to get mushroom-poisoned, while not losing (almost) any edible mushrooms.\n\nMaybe there is a way Machine Learning methods may help us achieve this objective."},{"metadata":{},"cell_type":"markdown","source":"# 2. Encoding the features as binary and one-hot labels"},{"metadata":{},"cell_type":"markdown","source":"Several features contain exactly 2 unique labels. These can be encoded as binary, i.e. 0 for one label and 1 for the other.\n\nI don't think any of the others can be sensibly translated to an ordinal scale, so they will need to be encoded as one-hot sparse matrices.\n\nHowever, note that one feature, 'stalk-root', contains a question mark ('?') label which stands for \"missing\". This should not be encoded as another index in the one-hot representation, since it does not stand for anything real but rather our lack of knowledge. Thus examples with this label will have a vector of zeros as their stalk-root one-hot representation."},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_data = pd.DataFrame({col: [len(set(data[col])), list(set(data[col])), '?' in set(data[col])] for col in data.columns}).T\nlabels_data.columns = ['# unique labels', 'Labels', 'Unknown (\\'?\\') label']\nlabels_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n# For convenience, pre-shuffle the data before splitting it into features and targets\ndata = data.sample(frac=1) \n\n# Target labels (edible - 1 ; poisonous - 0)\ndata_y = data['class'].apply(lambda x: 1 if x=='e' else 0)\n\n# Predictive features. An empty DataFrame for now, but soon we will populate it with properly encoded data\ndata_X = pd.DataFrame()\n\n# Encoder for encoding features with more than 2 labels\nencoder = OneHotEncoder()\n\nfor col_name in data.columns[1:]: # For each column except the first one, which is 'class'\n    # The list of all unique labels in the column\n    col_unique = list(set(data[col_name].values))\n    # If there are only two unique labels\n    if len(col_unique)==2:\n        # Encode as binary\n        col_encoded = data[col_name].apply(lambda x: 0 if x==col_unique[0] else 1)\n        # For better interpretability, contain the meaning of 0s and 1s with respect to the original descriptive labels in the name of the encoded column\n        col_encoded_name = f'{col_name}-bin{col_unique[0]}-{col_unique[1]}'\n        # Add this encoded column to the data_X DataFrame\n        data_X[col_encoded_name] = col_encoded\n    # If there are more than two unique labels\n    else:\n        # Encode as one-hot with previously initialized OneHotEncoder\n        # Immediately convert into a numpy array\n        col_encoded = encoder.fit_transform(data[col_name].values.reshape(-1,1)).toarray()\n        # Labels for each column of the one-hot array, into which this column has just been encoded\n        col_encoded_labels = encoder.categories_[0]\n        # For each of this column's labels\n        for i, label in enumerate(col_encoded_labels):\n            if label=='?': # Skip, if this label is '?'\n                continue\n            # The name of the column encoding this label will contain this label's name (again, for interpretability)\n            col_encoded_name = f'{col_name}-{label}'\n            # Add this column to the data_X DataFrame \n            data_X[col_encoded_name] = col_encoded[:,i]\n            \ndata_X.shape, data_y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sanity check:\n\nWe have 110 columns in the encoded data. We should have one new column for each column in the original data, which had 2 unique values and n for each with n>2 unique values minus one for the '?' stalk-root label.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"expected_n_columns = 0\nfor col in data.columns[1:]:\n    L = len(set(data[col])) \n    if L==2:\n        expected_n_columns += 1\n    else:\n        if '?' in set(data[col]):\n            L-=1\n        expected_n_columns += L\nexpected_n_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All good"},{"metadata":{},"cell_type":"markdown","source":"# 3. Random Forest Classifier"},{"metadata":{},"cell_type":"markdown","source":"## 3.1. Finding the most important featuers"},{"metadata":{},"cell_type":"markdown","source":"Before testing how a Random Forest Classifier performs on this dataset, let's test a single Decision Tree Classifier, of which many instances RFC is an ensemble."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier as DTC\nfrom sklearn.model_selection import cross_val_score\n\ndtc = DTC()\n\ndtc_cvs_acc = cross_val_score(dtc, data_X, data_y, scoring='accuracy', cv=3, n_jobs=-1)\n\nprint(\"\\tDecision Tree Classifier:\")\nprint(dtc_cvs_acc.round(5))\nprint(f\"Mean: {dtc_cvs_acc.mean().round(5)}\\tStd: {dtc_cvs_acc.std().round(5)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like this task is just trivial for this algorithm, it achieves almost 100% accuracy right from the start without any fine-tuning.\n\nNo wonder, why Random Forests do it even better."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier as RFC\n\nrfc = RFC()\n\nrfc_cvs_acc = cross_val_score(rfc, data_X, data_y, scoring='accuracy', cv=3, n_jobs=-1)\nprint(\"\\Random Forest Classifier:\")\nprint(rfc_cvs_acc.round(7))\nprint(f\"Mean: {rfc_cvs_acc.mean().round(5)}\\tStd: {rfc_cvs_acc.std().round(5)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's make it a little harder. Instead of splitting the data into 3 parts and training the classifier on 2 of them and testing on the third, 3 times in all combinations (as we do with the cross_val_score function), let's test the model on a randomly selected 10% of the dataset and test it on the remaining 90%. We will repeat this process 10 times and calculate the average score.\n\nFor that we can use Scikit-Learn's KFold function.\n\n(I don't know if this method has a name. From now on I will call it \"reverse cross-validation\".)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nsplits = KFold(n_splits=10, shuffle=True)\n\nrfc_scores = [] # A list to write scores into \nforests = [] # A list to contain the trained models (to be explained in a while)\n\nfor test_idx, train_idx in tqdm(splits.split(data_X)):\n    train_X, train_y = data_X.iloc[train_idx], data_y.iloc[train_idx]\n    test_X, test_y = data_X.iloc[test_idx], data_y.iloc[test_idx]\n    \n    rfc = RFC()\n    rfc.fit(train_X, train_y)\n    \n    rfc_score = rfc.score(test_X, test_y)\n    rfc_scores.append(rfc_score)\n    forests.append(rfc)\n    \nprint(\"\\tRandom Forest Classifier (reverse cross-validation):\")\nprint(f\"Mean: {np.mean(rfc_scores)}\\tMin: {np.min(rfc_scores)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With scores that high we can be reasonably certain that our model will perform well.\n\nAs I said at the end of section 1., we want to have models that don't need to rely all the features in our dataset, encoded in 110 columns, but rather a small subsection of them.\n\nWe can access information about to what extent the Random Forest Classifier relies on each feature with .feature_importances_ attribute.\n\nHowever, although random forests perform quite well on many different kinds of tasks, they are quite 'chaotic', that is, two models initialized with the exact same set of hyperparameters and then trained on the exact same data may evolve quite differently and thus learn to rely on different features."},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_fi_df = pd.DataFrame()\n\nrfc = RFC()\nrfc.fit(data_X, data_y)\nrfc_fi_df['RFC 1'] = rfc.feature_importances_\nrfc = RFC()\nrfc.fit(data_X, data_y)\nrfc_fi_df['RFC 2'] = rfc.feature_importances_\n\nrfc_fi_df.index = data_X.columns\n\nrfc_fi_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This difference would be even more pronounced, if we decided to select the most important features, e.g. with their importance greater than 0.02. We can see this by plotting a histogram for each of these two Forests:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1,2, figsize=(15,9))\n\nrfc_fi_df['RFC 1'].hist(ax=axs[0])\naxs[0].set_ylim(top=10) # Most of the features are of very low importance, which is represented below as a \"skyscrapper\" at the left, so we will \"zoom in\" a little bit\nrfc_fi_df['RFC 2'].hist(ax=axs[1])\naxs[1].set_ylim(top=10)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This differences in feature importances make it less clear to judge, what features we should select.\n\nThat's why when training 100 models on 100 training mini-splits we saved each model after training. Now we can average over their learned respective feature importances and get a clearer picture:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Average feature importances\nav_fi = np.zeros((rfc.feature_importances_.shape))\n\nfor rfc in forests:\n    av_fi += rfc.feature_importances_\n    \nav_fi /= len(forests)\n\n# Plot the data\n\nfig, ax = plt.subplots(figsize=(15,9))\n\nax.hist(av_fi)\nax.set_ylim(top=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will select three, increasingly smaller and stricter sets of features, with importance thresholds of 0.01, 0.02, and 0.04:"},{"metadata":{"trusted":true},"cell_type":"code","source":"av_fi_df = pd.Series({feature: importance for feature, importance in zip(data_X.columns, av_fi)}).to_frame('importance')\nav_fi_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_01 = av_fi_df.query('importance >= 0.01').index.values\nfeatures_02 = av_fi_df.query('importance >= 0.02').index.values\nfeatures_04 = av_fi_df.query('importance >= 0.04').index.values\nfeatures_01.shape, features_02.shape, features_04.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will repeat the previous training procedure, but now each training mini-split will be trained and evaluated on each of the three subsets of features. Scores will be stored in separate lists for each feature subset."},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_01_scores, rfc_02_scores, rfc_04_scores = [], [], []\n\nsplits = KFold(n_splits=10, shuffle=True)\n\nfor test_idx, train_idx in tqdm(splits.split(data_X)):\n    train_X, train_y = data_X.iloc[train_idx], data_y.iloc[train_idx]\n    test_X, test_y = data_X.iloc[test_idx], data_y.iloc[test_idx]\n    \n    rfc = RFC()\n    rfc.fit(train_X[features_01], train_y)\n    rfc_01_score = rfc.score(test_X[features_01], test_y)\n    rfc_01_scores.append(rfc_01_score)\n    \n    rfc = RFC()\n    rfc.fit(train_X[features_02], train_y)\n    rfc_02_score = rfc.score(test_X[features_02], test_y)\n    rfc_02_scores.append(rfc_02_score)\n    \n    rfc = RFC()\n    rfc.fit(train_X[features_04], train_y)\n    rfc_04_score = rfc.score(test_X[features_04], test_y)\n    rfc_04_scores.append(rfc_04_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can print these scores and compare them to those obtained for forests trained on all available features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"0.01:\\n\\tMean:{np.mean(rfc_01_scores).round(5)}\\tMin:{np.min(rfc_01_scores).round(5)}\\tStd:{np.std(rfc_01_scores).round(5)}\")\nprint(f\"0.02:\\n\\tMean:{np.mean(rfc_02_scores).round(5)}\\tMin:{np.min(rfc_02_scores).round(5)}\\tStd:{np.std(rfc_02_scores).round(5)}\")\nprint(f\"0.04:\\n\\tMean:{np.mean(rfc_04_scores).round(5)}\\tMin:{np.min(rfc_04_scores).round(5)}\\tStd:{np.std(rfc_04_scores).round(5)}\")\nprint(f\"All:\\n\\tMean:{np.mean(rfc_scores).round(5)}\\tMin:{np.min(rfc_scores).round(5)}\\tStd:{np.std(rfc_scores).round(5)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Restricting ourselves to only those features whose importance was greater than 0.01, we didn't lose mean, nor minimum accuracy. There are, however, significant losses for higher thresholds.\n\nWe can see, how applying each threshold influences the model's accuracy, when trained on a bigger chunk of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split as tts\n\nrfc_01_cv = cross_val_score(RFC(), data_X[features_01], data_y, cv=5, scoring='accuracy', n_jobs=-1,)\nrfc_02_cv = cross_val_score(RFC(), data_X[features_02], data_y, cv=5, scoring='accuracy', n_jobs=-1,)\nrfc_04_cv = cross_val_score(RFC(), data_X[features_04], data_y, cv=5, scoring='accuracy', n_jobs=-1,)\nrfc_all_cv = cross_val_score(RFC(), data_X, data_y, cv=5, scoring='accuracy', n_jobs=-1,)\n\nprint(f\"0.01:\\n\\tMean:{np.mean(rfc_01_cv).round(5)}\\tMin:{np.min(rfc_01_cv).round(5)}\\tStd:{np.std(rfc_01_cv).round(5)}\")\nprint(f\"0.02:\\n\\tMean:{np.mean(rfc_02_cv).round(5)}\\tMin:{np.min(rfc_02_cv).round(5)}\\tStd:{np.std(rfc_02_cv).round(5)}\")\nprint(f\"0.04:\\n\\tMean:{np.mean(rfc_04_cv).round(5)}\\tMin:{np.min(rfc_04_cv).round(5)}\\tStd:{np.std(rfc_04_cv).round(5)}\")\nprint(f\"All:\\n\\tMean:{np.mean(rfc_all_cv).round(5)}\\tMin:{np.min(rfc_all_cv).round(5)}\\tStd:{np.std(rfc_all_cv).round(5)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2. Restricting Random Forests' individual estimators"},{"metadata":{},"cell_type":"markdown","source":"So what should we look at, when we're on a mushroom hunting?"},{"metadata":{"trusted":true},"cell_type":"code","source":"important_df = av_fi_df.loc[features_01].sort_values(by='importance', ascending=False)\nimportant_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest Classifiers consistently tend to ascribe quite high importance to some of the features we included in our back-of-the-envelope selection algorithm fom section 1, such as 'buff-colored' gills and chocholate-colored or white spore-prints. They didn't, however, found an \"enlarging\" stalk shape to be a useful indicator, which is quite interesting.\n\nMaybe we can gain some useful insight into the way these algorithms reason, if we visualize graphically one of the Decision Tree Classifiers (of which Random Forest Classifiers are ensembles).\n\nLet's re-run a part of the previous loop. This time we will train only on the features with importance>=0.01 and save the best forest."},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_01_best = None\nrfc_01_best_score = 0\n\nsplits = KFold(n_splits=10, shuffle=True)\nfor test_idx, train_idx in tqdm(splits.split(data_X)):\n    train_X, train_y = data_X.iloc[train_idx][features_01], data_y.iloc[train_idx]\n    test_X, test_y = data_X.iloc[test_idx][features_01], data_y.iloc[test_idx]\n    \n    rfc = RFC()\n    rfc.fit(train_X, train_y)\n    rfc_01_score = rfc.score(test_X[features_01], test_y)\n    if rfc_01_score > rfc_01_best_score:\n        rfc_01_best_score = rfc_01_score\n        rfc_01_best = rfc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have our best RFC, let's make a sorted table with information about the score of its individual DTCs:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc_df = pd.Series({idx: dtc.score(data_X[features_01], data_y) for idx, dtc in enumerate(rfc_01_best.estimators_) }).to_frame('Score').sort_values(by='Score', ascending=False)\n\ndtc_df,","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also plot the distribution of these scores:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc_df['Score'].hist()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take one of the best DTCs and visualize it in form of a graph."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import export_graphviz\n\ndtc_best = rfc_01_best.estimators_[dtc_df.index[0]]\n\nexport_graphviz(\n    dtc_best,\n    out_file='dtc_best.dot',\n    feature_names=features_01,\n    class_names=['p', 'e'],\n    rounded=True,\n    filled=True\n)\n    \n\nos.getcwd(), os.listdir() # See, that the saved .dot file is in our /working directory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! dot -Tpng dtc_best.dot -o dtc_best.png\nos.listdir() # Convert this .dot file to .png","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\n\n# Display this .png\n\nimg = 'dtc_best.png'\nImage(url=img, embed=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Such an elaborate algorithm is basically guaranteed to overfit, even though it was trained on only 10% of the data. Evidently, our data is pretty consistent and unfiform, which allows this DTC to achieve such a high accuracy. However, if we encounter a mushroom, which was quite unlike anything we've ever seen, this decision tree with its elaborate branches may give us a possibly dangerously wrong impression about this mushroom's safety.\n\nWe will therefore try to constrain the freedom of the trees grown by our estimators, so that they do not overfit the data (with regards to possible data from beyond that distribution), while trying to minimize the losses in accuracy.\n\nLet's see how really these trees are \"overgrown\". We can see that by investigating several of its characteristics:\n\n* Depth - How long is **the longest** possible decision process for a given tree\n* Number of leaves (leaf nodes) - How many final decision node a given tree has\n* Mean probability - How certain (averaging over all of its leaf nodes)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need to access the predictor's index in order to access its depth, number of leaves and mean probability\nif 'index' not in dtc_df.columns:\n    dtc_df.reset_index(inplace=True, drop=False)\n\ndtc_df['Depth'] = dtc_df['index'].apply(lambda idx: rfc_01_best.estimators_[idx].get_depth())\ndtc_df['Leaves'] = dtc_df['index'].apply(lambda idx: rfc_01_best.estimators_[idx].get_n_leaves())\ndtc_df['Mean proba'] = dtc_df['index'].apply(lambda idx: rfc_01_best.estimators_[idx].predict_proba(data_X[features_01]).max(axis=1).mean())\n\ndtc_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First things first, All these ters seem absolutely certain in their judgments. That's very dangerous in itself:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc_df['Mean proba'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can plot the depth and number of leaves of all the trees:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize=(15, 9))\n\ndtc_df['Depth'].hist(bins = dtc_df['Depth'].max() - dtc_df['Depth'].min() +1, ax=ax[0] )\nax[0].set_title('Depth')\ndtc_df['Leaves'].hist(bins = dtc_df['Leaves'].max() - dtc_df['Leaves'].min() +1, ax=ax[1] )\nax[1].set_title('Leaves')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These distributions seem to somewhat approximate the normal (Gaussian) distribution.\n\nLet's see what happens to mean accuracy and probability of the forest, if we try to constrain the maximum depth and maximum number of leaves to various maximum values. We will test each pair of max_depth and max_leaf_nodes hyperparameters and write the results into a matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"DEPTH = np.arange(2, 15)\nLEAVES = np.arange(5, 35)\n\naccuracy_matrix = np.zeros((len(DEPTH), len(LEAVES)))\nprobas_matrix = np.zeros((len(DEPTH), len(LEAVES)))\n\nsplits = KFold(n_splits=10, shuffle=True)\nfor i, depth in tqdm(enumerate(DEPTH)):\n    for ii, leaves in enumerate(LEAVES):\n        #print(f\"{np.round(100*(i/len(DEPTH)+(ii/len(LEAVES))/len(DEPTH)), 2)} %\")\n        for test_idx, train_idx in splits.split(data_X):\n            train_X, train_y = data_X.iloc[train_idx][features_01], data_y.iloc[train_idx]\n            test_X, test_y = data_X.iloc[test_idx][features_01], data_y.iloc[test_idx]\n\n            rfc = RFC(max_depth=depth, max_leaf_nodes=leaves)\n            rfc.fit(train_X, train_y)\n            \n            accuracy_matrix[i,ii] += rfc.score(test_X, test_y)\n            probas_matrix[i,ii] += rfc.predict_proba(test_X).max(axis=1).mean()\n\naccuracy_matrix /= 10\nprobas_matrix /= 10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can display these matrices as heatmaps:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,9))\n\naccuracy_heatmap = ax.imshow(accuracy_matrix)\n\nax.set_yticks(np.arange(len(DEPTH)))\nax.set_yticklabels(DEPTH)\n\nax.set_xticks(np.arange(len(LEAVES)))\nax.set_xticklabels(LEAVES)\n\nax.set_title(\"Accuracy\")\n\nfig.tight_layout()\n\nplt.colorbar(accuracy_heatmap)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,9))\n\nprobas_heatmap = ax.imshow(probas_matrix)\n\nax.set_yticks(np.arange(len(DEPTH)))\nax.set_yticklabels(DEPTH)\n\nax.set_xticks(np.arange(len(LEAVES)))\nax.set_xticklabels(LEAVES)\n\nax.set_title(\"Probas\")\n\nfig.tight_layout()\n\nplt.colorbar(probas_heatmap)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Evidently, reducing both maximum depth and number of leaf nodes result in lower accuracy as well as certainty ('probas') of predictions. Nevertheless, such smaller models would probably be more adequate, since they would not foster overconfidence in our predictions.\n\nNevertheless, even the most constrained random forests score pretty high:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\tMaximally constrained forest:\")\nprint(f\"Accuracy: {accuracy_matrix[0,0].round(3)}\")\nprint(f\"Probas: {probas_matrix[0,0].round(3)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even though its individual decision trees score pretty low."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the best constrained forest\nrfc_constrained = None\nrfc_constrained_score = 0\n\nsplits = KFold(n_splits=10, shuffle=True)\nfor test_idx, train_idx in tqdm(splits.split(data_X)):\n    train_X, test_X = data_X.iloc[train_idx][features_01], data_X.iloc[test_idx][features_01]\n    train_y, test_y = data_y.iloc[train_idx], data_y.iloc[test_idx]\n    \n    rfc = RFC(max_depth=2, max_leaf_nodes=5)\n    rfc.fit(train_X, train_y)\n    rfc_score = rfc.score(test_X, test_y)\n    if rfc_score > rfc_constrained_score:\n        rfc_constrained_score = rfc_score\n        rfc_constrained = rfc\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the data about decision trees\n\nconstrained_dtc_df = pd.Series({idx: dtc.score(data_X[features_01], data_y) for idx, dtc in enumerate(rfc_constrained.estimators_)}).to_frame('Score').sort_values(by='Score', ascending=False)\nconstrained_dtc_df.reset_index(inplace=True, drop=False)\nconstrained_dtc_df['Probas'] = constrained_dtc_df['index'].apply(lambda idx: rfc_constrained.estimators_[idx].predict_proba(data_X[features_01]).max(axis=1).mean())\n\n\n# Compare their average individual accuracy and certainty to that of the whole ensemble (tested on the entire dataset)\nprint(f\"Mean DTC accuracy:\\t{np.round(constrained_dtc_df['Score'].mean(), 4)}\")\nprint(f\"Mean DTC probas:\\t{np.round(constrained_dtc_df['Probas'].mean(), 4)}\")\nprint(f\"Mean ensemble accuracy:\\t{np.round(rfc_constrained.score(data_X[features_01], data_y), 4)}\")\nprint(f\"Mean ensemble probas:\\t{np.round(rfc_constrained.predict_proba(data_X[features_01]).max(axis=1).mean())}\")\n\nfig, ax = plt.subplots(1,2,figsize=(15,9))\n\nconstrained_dtc_df['Score'].hist(ax=ax[0])\nax[0].set_title('Score')\nconstrained_dtc_df['Probas'].hist(ax=ax[1])\nax[1].set_title('Probas')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By combining many (in this case, 100) weaker models (mean accuracy 0.8527) into an ensemble we can create a significantly stronger model (mean accuracy 0.9458), as long as its constituent models are sufficiently diverse.\n\nAlso, we should take into account, that although our ensemble has only about 95% accuracy, it reports 100% certainty in its predictions, so the latter probably should not be taken as a good indicator of reliability of its particular predictions. We may rather want to use mean certainty of ensemble's constituent predictors or maybe geometric mean of both.\n\nLet's see how one of its trees \"reasons\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc_constrained = rfc_constrained.estimators_[0]\n\nexport_graphviz(\n    dtc_constrained,\n    out_file='dtc_constrained.dot',\n    feature_names=features_01,\n    class_names=['p', 'e'],\n    rounded=True,\n    filled=True\n)\n! dot -Tpng dtc_constrained.dot -o dtc_constrained.png\nimg = 'dtc_constrained.png'\nImage(url=img, embed=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Come to think of that, we can also plot, how accuracy, probas change as we go from using all available features to only a handful of them.\n\nLet's use a random forest with max_depth set to 10 and max_leaf_nodes set to 30, which does seem to incur huge accuracy or certainty losses."},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_accuracies = np.zeros((av_fi_df.shape[0],))\nfeat_probas = np.zeros((av_fi_df.shape[0],))\n\nsplits = KFold(n_splits=10, shuffle=True)\nfor n_features in tqdm(range(av_fi_df.shape[0])):\n    for test_idx, train_idx in splits.split(data_X):\n        features = av_fi_df.index[:n_features+1]\n        train_X, test_X = data_X.iloc[train_idx][features], data_X.iloc[test_idx][features]\n        train_y, test_y = data_y.iloc[train_idx], data_y.iloc[test_idx]\n        rfc = RFC(max_depth=10, max_leaf_nodes=30)\n        rfc.fit(train_X, train_y)\n        \n        feat_accuracies[n_features] += rfc.score(test_X, test_y)\n        feat_probas[n_features] += rfc.predict_proba(test_X).max(axis=1).mean()\n        \nfeat_accuracies /= 10\nfeat_probas /= 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_df = pd.DataFrame([feat_accuracies, feat_probas]).T\nfeat_df.columns = ['Accuracies', 'Probas']\n\nfig, ax = plt.subplots(figsize=(15,9))\n\nsns.lineplot(data=feat_df, ax=ax)\n\nax.set_xlabel('Number of features')\nax.set_title('Accuracy and certainty')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we can see that both accuracy and cerainty rise steepily, almost hand in hand as we increase the number of features used (starting from the \"most important\" ones), until we reach the point of 23 features or so, which btw corresponds to our choice of 0.01 threshold."},{"metadata":{},"cell_type":"markdown","source":"# To do (maybe):\n\n* Build and optimize a NN for this task\n* Try dimensionality reduction (is it even possible for only categorical variables?)\n* Try clustering\n* "},{"metadata":{"trusted":true},"cell_type":"code","source":"model = models.Sequential(name='shroom_classifier',layers=[\n    layers.Dense(32, activation='relu', kernel_regularizer='l2', input_shape=(train_X.shape[1],)),\n    layers.BatchNormalization(),\n    layers.Dropout(.1),\n    layers.Dense(64, activation='relu', kernel_regularizer='l2'),\n    layers.BatchNormalization(),\n    layers.Dropout(.1),\n    layers.Dense(64, activation='relu', kernel_regularizer='l2'),\n    layers.BatchNormalization(),\n    layers.Dropout(.1),\n    layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(\n    optimizer='rmsprop',\n    loss='binary_crossentropy',\n    metrics=['acc']\n)\n\n\ntrain_X, test_X, train_y, test_y = tts(data_X, data_y_bin, test_size=.9, random_state=42, shuffle=True)\n\nhistory = model.fit(\n    train_X, train_y,\n    validation_split=.1, shuffle=True,\n    batch_size=32, epochs=5\n)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}