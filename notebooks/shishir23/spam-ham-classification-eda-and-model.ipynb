{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport string\nimport nltk\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport time\n\n%matplotlib inline\npd.set_option('display.max_colwidth', 100)\n\nnltk.download('stopwords')\nstopword = nltk.corpus.stopwords.words('english')\nps = nltk.PorterStemmer()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:22.466677Z","iopub.execute_input":"2021-06-30T10:37:22.466974Z","iopub.status.idle":"2021-06-30T10:37:22.47926Z","shell.execute_reply.started":"2021-06-30T10:37:22.46695Z","shell.execute_reply":"2021-06-30T10:37:22.477784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pre-Process raw text data\n\nCleaning up text data is necessary to highlight data attributes that we are going to use for NLP data analysis. Cleaning or pre-processing data basically follows three steps:\n - Remove Punctuation\n - Tokenizing\n - Remove Stopwords\n - Stemming/ Lematizing\n \nLet us apply above steps to perform data cleaning and processing.","metadata":{}},{"cell_type":"code","source":"def apply_styling(df: pd.DataFrame, caption: str = \"\"):\n    '''\n    Return @pd.DataFrame\n    Input  @df:pd.DataFrame\n           @caption: Stirng  \n    It help to apply style to a particular dataframe which is passed into this\n    '''\n    #TODO: Styling dataframe after reading the file\n    \n    st = df.style.format({'percent on rent': '{:.0%}'}).hide_index()    \n    st.set_table_styles([\n           dict(selector=\"th\", props=[('color', 'darkblue'), \n                                      ('vertical-align', 'top')]),\n           dict(selector=\"th:first-child\", props=[('max-width', '70px'), ('text-align', 'left')]),\n           dict(selector=\"th:last-child\", props=[('max-width', '50px')]),\n           dict(selector=\"td:first-child\", props=[('text-align', 'left')])\n            ]) \n    st.caption = caption\n    return st\n\ndef read_data(file_url: str, csv: bool = False, excel: bool = False, tab_delimeted: bool = False):\n    '''\n    Return : @pd.DataFrame\n    Input : @file_url: String\n            @csv: bool\n            @excel: bool\n            @tab_delimeted: bool\n    \n    It is utility function that helps to read a particular file and apply a particular styling\n    to the dataframe, which is returned.\n    '''\n    #TODO: Reading the file after checking the type on the basis of flag.\n    if csv:\n        df = pd.read_csv(file_url)\n    if excel:\n        df = pd.read_excel(file_url)\n    if tab_delimeted:\n        df = pd.read_csv(file_url, delimiter='\\t')\n        \n    if not (csv or excel or tab_delimeted):\n        print(\"please specify file type\")\n        return None\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:22.481599Z","iopub.execute_input":"2021-06-30T10:37:22.48193Z","iopub.status.idle":"2021-06-30T10:37:22.498125Z","shell.execute_reply.started":"2021-06-30T10:37:22.481902Z","shell.execute_reply":"2021-06-30T10:37:22.496599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_url = \"../input/sms-spam-collection-dataset/spam.csv\"\ncaption = 'Spam-Ham Dataframe'\nspam_ham_df = read_data(file_url ,csv=True)\nspam_ham_df = spam_ham_df[['v1','v2']]\nspam_ham_df.columns = ['Type', 'Body']","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:22.49978Z","iopub.execute_input":"2021-06-30T10:37:22.50008Z","iopub.status.idle":"2021-06-30T10:37:22.548362Z","shell.execute_reply.started":"2021-06-30T10:37:22.500052Z","shell.execute_reply":"2021-06-30T10:37:22.546665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"apply_styling(spam_ham_df.head(),caption)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:22.55238Z","iopub.execute_input":"2021-06-30T10:37:22.552741Z","iopub.status.idle":"2021-06-30T10:37:22.560691Z","shell.execute_reply.started":"2021-06-30T10:37:22.552711Z","shell.execute_reply":"2021-06-30T10:37:22.560072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <u>Removing punctuation from the dataframe of Spam/Ham</u> ","metadata":{}},{"cell_type":"code","source":"def removePunct(text: str) -> str:\n    '''\n    @Return str\n    @Input @text: str\n    \n    It removes the punctuations from the text by using 'string' module,\n    and using its punctuations\n    '''\n    \n    return \"\".join([txt for txt in text if txt not in string.punctuation])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:22.56257Z","iopub.execute_input":"2021-06-30T10:37:22.562766Z","iopub.status.idle":"2021-06-30T10:37:22.578632Z","shell.execute_reply.started":"2021-06-30T10:37:22.562745Z","shell.execute_reply":"2021-06-30T10:37:22.578003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO: Applying @removePunct function to each message in spam_ham_df\n\nspam_ham_df['Body_punct_clean'] = spam_ham_df['Body'].apply(lambda x : removePunct(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:22.5799Z","iopub.execute_input":"2021-06-30T10:37:22.580222Z","iopub.status.idle":"2021-06-30T10:37:22.658057Z","shell.execute_reply.started":"2021-06-30T10:37:22.58019Z","shell.execute_reply":"2021-06-30T10:37:22.657173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"apply_styling(spam_ham_df.head(), caption)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:22.659011Z","iopub.execute_input":"2021-06-30T10:37:22.659337Z","iopub.status.idle":"2021-06-30T10:37:22.666733Z","shell.execute_reply.started":"2021-06-30T10:37:22.659295Z","shell.execute_reply":"2021-06-30T10:37:22.665739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <u>Tokenizing each text in spam_ham_df</u>","metadata":{}},{"cell_type":"code","source":"def tokenize_text(text: str) -> list :\n    '''\n    @Return: List<str>\n    @Input: text: String\n    \n    It converts the text into list of words by using regex \n    based apporach to split them on any non-character base.\n    '''\n    return re.split('\\W+',text)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:22.66775Z","iopub.execute_input":"2021-06-30T10:37:22.66807Z","iopub.status.idle":"2021-06-30T10:37:22.686233Z","shell.execute_reply.started":"2021-06-30T10:37:22.668037Z","shell.execute_reply":"2021-06-30T10:37:22.685279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO: Tokenize text using above function @tokenize_text and passing each text iteratively into that function.\n\nspam_ham_df['Body_tokenize'] = spam_ham_df['Body_punct_clean'].apply(lambda x : tokenize_text(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:22.687186Z","iopub.execute_input":"2021-06-30T10:37:22.687513Z","iopub.status.idle":"2021-06-30T10:37:22.739313Z","shell.execute_reply.started":"2021-06-30T10:37:22.687481Z","shell.execute_reply":"2021-06-30T10:37:22.738589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"apply_styling(spam_ham_df.head(),caption)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:22.741944Z","iopub.execute_input":"2021-06-30T10:37:22.742333Z","iopub.status.idle":"2021-06-30T10:37:22.752106Z","shell.execute_reply.started":"2021-06-30T10:37:22.7423Z","shell.execute_reply":"2021-06-30T10:37:22.751074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <u>Stopword removal from the spam_ham_df tokenized sentences.</u>","metadata":{}},{"cell_type":"code","source":"def remove_stopword(text: [str]) -> [str] :\n    '''\n    Return @list[str]\n           @Input: @text : list[str]\n           \n    It remove the stopwords from the tokenized sentences and return the\n    list of strings without stopwords.\n    '''\n    \n    return [txt for txt in text if txt not in stopword]","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:22.753675Z","iopub.execute_input":"2021-06-30T10:37:22.75395Z","iopub.status.idle":"2021-06-30T10:37:22.771179Z","shell.execute_reply.started":"2021-06-30T10:37:22.753926Z","shell.execute_reply":"2021-06-30T10:37:22.770249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO: Removing stopwords from tokenized sentence using @remove_stopword function from spam_ham_df\n\nspam_ham_df['Body_without_stopword'] = spam_ham_df['Body_tokenize'].apply(lambda x: remove_stopword(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:22.772046Z","iopub.execute_input":"2021-06-30T10:37:22.77227Z","iopub.status.idle":"2021-06-30T10:37:22.961612Z","shell.execute_reply.started":"2021-06-30T10:37:22.772247Z","shell.execute_reply":"2021-06-30T10:37:22.960997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"apply_styling(spam_ham_df.head(), caption)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:22.962494Z","iopub.execute_input":"2021-06-30T10:37:22.962809Z","iopub.status.idle":"2021-06-30T10:37:22.971347Z","shell.execute_reply.started":"2021-06-30T10:37:22.962788Z","shell.execute_reply":"2021-06-30T10:37:22.970156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <u>Performing the Stemming on tokenized words of sentences.</u>","metadata":{}},{"cell_type":"code","source":"def stemming(text: [str]) -> [str]:\n    \"\"\"\n    Return: [str]\n    Input: @txt: [str]\n    \n    It perform stemming on each word of text, by using NLTK(natural language toolkit) \n    Porter Stemmer.\n    \"\"\"\n    return [ps.stem(txt) for txt in text]","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:22.972609Z","iopub.execute_input":"2021-06-30T10:37:22.972829Z","iopub.status.idle":"2021-06-30T10:37:22.986986Z","shell.execute_reply.started":"2021-06-30T10:37:22.972806Z","shell.execute_reply":"2021-06-30T10:37:22.985662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO: Perform stemming on each word of tokenized sentences of spam_ham_df by using @stemming function.\n# We are performing stemming to reduce the number of similar tokens that our model have to read.\n\nspam_ham_df['Body_stemmed'] = spam_ham_df['Body_without_stopword'].apply(lambda x : stemming(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:22.988687Z","iopub.execute_input":"2021-06-30T10:37:22.989134Z","iopub.status.idle":"2021-06-30T10:37:24.139018Z","shell.execute_reply.started":"2021-06-30T10:37:22.9891Z","shell.execute_reply":"2021-06-30T10:37:24.138032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"apply_styling(spam_ham_df.head(), caption)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:24.14085Z","iopub.execute_input":"2021-06-30T10:37:24.141383Z","iopub.status.idle":"2021-06-30T10:37:24.151446Z","shell.execute_reply.started":"2021-06-30T10:37:24.141351Z","shell.execute_reply":"2021-06-30T10:37:24.150373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### New Feature Exploration\n\n#### <u>Considering Body Length as a feature</u>","metadata":{}},{"cell_type":"code","source":"#TODO: Performing feature engineering on the spam_ham_df \n#TODO: Exploring body length as a feature\n\nspam_ham_df['Body_len'] = spam_ham_df['Body'].apply(lambda x: len(x) - x.count(\" \"))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:24.155205Z","iopub.execute_input":"2021-06-30T10:37:24.155479Z","iopub.status.idle":"2021-06-30T10:37:24.174432Z","shell.execute_reply.started":"2021-06-30T10:37:24.155454Z","shell.execute_reply":"2021-06-30T10:37:24.173586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"apply_styling(spam_ham_df.head(), caption)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:24.175613Z","iopub.execute_input":"2021-06-30T10:37:24.175886Z","iopub.status.idle":"2021-06-30T10:37:24.205387Z","shell.execute_reply.started":"2021-06-30T10:37:24.175858Z","shell.execute_reply":"2021-06-30T10:37:24.204166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <u>Creating feature for % punctuation in body text.</u>","metadata":{}},{"cell_type":"code","source":"def count_punct(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count/(len(text) - text.count(\" \")), 3)*100\n\nspam_ham_df['punct%'] = spam_ham_df['Body'].apply(lambda x: count_punct(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:24.206479Z","iopub.execute_input":"2021-06-30T10:37:24.206778Z","iopub.status.idle":"2021-06-30T10:37:24.318731Z","shell.execute_reply.started":"2021-06-30T10:37:24.20675Z","shell.execute_reply":"2021-06-30T10:37:24.317485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"apply_styling(spam_ham_df.head(), caption)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:24.320327Z","iopub.execute_input":"2021-06-30T10:37:24.320638Z","iopub.status.idle":"2021-06-30T10:37:24.333674Z","shell.execute_reply.started":"2021-06-30T10:37:24.320607Z","shell.execute_reply":"2021-06-30T10:37:24.331776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <u>Evaluating above feature</u>","metadata":{}},{"cell_type":"code","source":"bins = np.linspace(0, 200, 40)\n\nplt.hist(spam_ham_df[spam_ham_df['Type'] == \"spam\"]['Body_len'], bins, alpha= 0.5,density=True, label=\"spam\")\nplt.hist(spam_ham_df[spam_ham_df['Type'] == \"ham\"]['Body_len'], bins, alpha= 0.5,density=True, label=\"ham\")\nplt.legend(loc=\"upper left\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:24.335023Z","iopub.execute_input":"2021-06-30T10:37:24.335374Z","iopub.status.idle":"2021-06-30T10:37:24.629917Z","shell.execute_reply.started":"2021-06-30T10:37:24.335344Z","shell.execute_reply":"2021-06-30T10:37:24.628682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <u>Observations : </u>\n - As we can see clearly that spam and ham are clearly sperable from each other on the basis of body length.\n - We don't require to perform any Box-Cox transformation on the data as it is a bimodial distribution of data.\n - There are certain messages with small lengths which are persent in both categories but significant number of \n   messages are clearly seperable from this feature.","metadata":{}},{"cell_type":"code","source":"bins = np.linspace(0, 50, 40)\n\nplt.hist(spam_ham_df[spam_ham_df['Type'] == \"spam\"]['punct%'], bins, alpha= 0.5,density=True, label=\"spam\")\nplt.hist(spam_ham_df[spam_ham_df['Type'] == \"ham\"]['punct%'], bins, alpha= 0.5,density=True, label=\"ham\")\nplt.legend(loc=\"upper left\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:24.631108Z","iopub.execute_input":"2021-06-30T10:37:24.631479Z","iopub.status.idle":"2021-06-30T10:37:24.87818Z","shell.execute_reply.started":"2021-06-30T10:37:24.631439Z","shell.execute_reply":"2021-06-30T10:37:24.876726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <u>Observations : </u>\n - This feature is not very significant as both the categories are not sperable from each other.\n - But we can perform transformation on the data as distribution is very skewed.\n\n<strong><i>For more information about transformation [click here](https://machinelearningmastery.com/how-to-transform-data-to-fit-the-normal-distribution/).</i><strong>","metadata":{}},{"cell_type":"markdown","source":"### <u>Performing Transformation punct%</u>","metadata":{}},{"cell_type":"code","source":"#TODO: Performing transformation on the punct% data and considering only +ve values,\n#      as -ve values will not help much in this scanerio\n\nfor i in [1,2,3,4,5]:\n    plt.hist((spam_ham_df['punct%'])**(1/i), bins=40) # Not considering np.linspace for bins as bins size will vary as value will change according to i\n    plt.title(f\"Transformation of 1/{i}\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:24.879178Z","iopub.execute_input":"2021-06-30T10:37:24.879383Z","iopub.status.idle":"2021-06-30T10:37:26.055094Z","shell.execute_reply.started":"2021-06-30T10:37:24.879361Z","shell.execute_reply":"2021-06-30T10:37:26.054134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <u>Observations : </u>\n - As we are increasing value of i, our distribution is getting more and more normalized, as we can see at <strong>i = '4'</strong> and <strong>i = '5'</strong>, our distribution has become normalized.\n - We are seeing one standing rectangle at <strong>'0'</strong>, it is because of any things raised to <strong> power 0 is 0 </strong>.","metadata":{}},{"cell_type":"markdown","source":"### <u>Vectorizing data using TFIDFVectorizer</u>","metadata":{}},{"cell_type":"code","source":"#TODO: Vectorize body tokenized data from spam_ham_df but before that split data into test and train sets.\n\nX_train, X_test, y_train, y_test = train_test_split(spam_ham_df[['Body', 'Body_len', 'punct%']], spam_ham_df['Type'], test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:26.056127Z","iopub.execute_input":"2021-06-30T10:37:26.056356Z","iopub.status.idle":"2021-06-30T10:37:26.064731Z","shell.execute_reply.started":"2021-06-30T10:37:26.056331Z","shell.execute_reply":"2021-06-30T10:37:26.063482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens = re.split('\\W+', text)\n    text = [ps.stem(word) for word in tokens if word not in stopword]\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:26.065857Z","iopub.execute_input":"2021-06-30T10:37:26.06608Z","iopub.status.idle":"2021-06-30T10:37:26.089353Z","shell.execute_reply.started":"2021-06-30T10:37:26.066052Z","shell.execute_reply":"2021-06-30T10:37:26.087191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO: Vectorize X_train, X_test datasets\ntfidf_vect = TfidfVectorizer(analyzer=clean_text)\ntfidf_vect_fit = tfidf_vect.fit(X_train['Body'])\n\ntfidf_train = tfidf_vect_fit.transform(X_train['Body'])\ntfidf_test = tfidf_vect_fit.transform(X_test['Body'])\n\nX_train_vect = pd.concat([X_train[['Body_len', 'punct%']].reset_index(drop=True), \n           pd.DataFrame(tfidf_train.toarray())], axis=1)\nX_test_vect = pd.concat([X_test[['Body_len', 'punct%']].reset_index(drop=True), \n           pd.DataFrame(tfidf_test.toarray())], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:26.090854Z","iopub.execute_input":"2021-06-30T10:37:26.09113Z","iopub.status.idle":"2021-06-30T10:37:29.967Z","shell.execute_reply.started":"2021-06-30T10:37:26.091102Z","shell.execute_reply":"2021-06-30T10:37:29.966142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_vect.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:29.968007Z","iopub.execute_input":"2021-06-30T10:37:29.968448Z","iopub.status.idle":"2021-06-30T10:37:29.997419Z","shell.execute_reply.started":"2021-06-30T10:37:29.968415Z","shell.execute_reply":"2021-06-30T10:37:29.996173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <u>Create Models and evaluate them on the basis of Accuracy, Recall and Precision.</u>","metadata":{}},{"cell_type":"markdown","source":"##### Fitting RandomForestClassifier with different Hyper Parameter settings and using GridSearchCV","metadata":{}},{"cell_type":"code","source":"#TODO: To fit RandomForestClassifier with different hyper parameter settings.\n\ngb = RandomForestClassifier()\n\nparam = { # different parameter settings\n    'n_estimators': [10, 150, 300], \n    'max_depth': [30, 60, 90, None]\n}\n\nclf = GridSearchCV(gb, param, cv=5, n_jobs=-1)\ncv_fit = clf.fit(X_train_vect, y_train)\npd.DataFrame(cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:37:29.998365Z","iopub.execute_input":"2021-06-30T10:37:29.998679Z","iopub.status.idle":"2021-06-30T10:40:21.710273Z","shell.execute_reply.started":"2021-06-30T10:37:29.998648Z","shell.execute_reply":"2021-06-30T10:40:21.709083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations:\n - As we can see that max_depth is not affecting much to test_score but when we are changing the n_estimator, this parameter is affecting the test score of our model.\n \n#### <u>Conclusion:</u>\n - We can say that <strong>max_depth</strong> can be <strong>None</strong> and <strong>n_estimators</strong> will be <strong>150</strong> as we have seen from above dataframe.","metadata":{}},{"cell_type":"code","source":"#TODO : Create a RandomForestClassifier with following configuration:\n# @n_estimators =150\n# @max_depth = None (Mean any amount of depth will do)\n# @n_jobs = -1 (Mean perform parallization in tree creation)\n\n#=======================================================================\nrf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1) \n#=======================================================================\nstart = time.time()\n\nrf_model = rf.fit(X_train_vect, y_train)\nend = time.time()\n\nfit_time = (end - start)\n\n#=======================================================================\nstart = time.time()\n\ny_pred = rf_model.predict(X_test_vect)\nend = time.time()\n\npred_time = (end - start)\n#=======================================================================\n\nprecision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n\nprint('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:40:21.71194Z","iopub.execute_input":"2021-06-30T10:40:21.712204Z","iopub.status.idle":"2021-06-30T10:40:26.014992Z","shell.execute_reply.started":"2021-06-30T10:40:21.712172Z","shell.execute_reply":"2021-06-30T10:40:26.014278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <u>Fitting GradientBoostingClassifier with different Hyper Parameter settings and using GridSearchCV</u>","metadata":{}},{"cell_type":"code","source":"gb = GradientBoostingClassifier()\nparam = {\n    'n_estimators': [100, 150], \n    'max_depth': [7, 11, 15],\n    'learning_rate': [0.1]\n}\n\nclf = GridSearchCV(gb, param, cv=5, n_jobs=-1)\ncv_fit = clf.fit(X_train_vect, y_train)\npd.DataFrame(cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:40:26.016059Z","iopub.execute_input":"2021-06-30T10:40:26.01642Z","iopub.status.idle":"2021-06-30T10:59:46.136227Z","shell.execute_reply.started":"2021-06-30T10:40:26.016391Z","shell.execute_reply":"2021-06-30T10:59:46.134905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations:\n - As we can see that max_depth = 7 and n_estimators = 150\n \n#### <u>Conclusion:</u>\n - We can say that <strong>max_depth</strong> can be <strong>7</strong> and <strong>n_estimators</strong> will be <strong>150</strong> as we have seen from above dataframe.","metadata":{}},{"cell_type":"code","source":"#TODO : Create a GradientBoostingClassifier with following configuration:\n# @n_estimators =150\n# @max_depth = 7\n\n#=======================================================================\ngb = GradientBoostingClassifier(n_estimators=150, max_depth=7)\n#=======================================================================\n\n#=======================================================================\nstart = time.time()\ngb_model = gb.fit(X_train_vect, y_train)\nend = time.time()\nfit_time = (end - start)\n#=======================================================================\n\n#=======================================================================\nstart = time.time()\ny_pred = gb_model.predict(X_test_vect)\nend = time.time()\npred_time = (end - start)\n#=======================================================================\n\nprecision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n\nprint('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:03:25.940349Z","iopub.execute_input":"2021-06-30T11:03:25.94066Z","iopub.status.idle":"2021-06-30T11:04:58.600841Z","shell.execute_reply.started":"2021-06-30T11:03:25.940636Z","shell.execute_reply":"2021-06-30T11:04:58.598834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \"IMPORTANT - You could be entitled up to £3,160 in compensation from mis-sold PPI on a credit card or loan. Please reply PPI for info or STOP to opt out.\"\n#=======================================================================\nlength = lambda x: len(x) - x.count(\" \")\npunct = count_punct(text)\n\npredict_df = pd.DataFrame(data=[(length(text),punct)],columns =['Body_len', 'punct%'])\n#=======================================================================\ngb_model.predict(pd.concat([predict_df, pd.DataFrame(tfidf_vect_fit.transform([text]).toarray())], axis=1))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:05:25.19106Z","iopub.execute_input":"2021-06-30T11:05:25.191451Z","iopub.status.idle":"2021-06-30T11:05:25.274082Z","shell.execute_reply.started":"2021-06-30T11:05:25.191416Z","shell.execute_reply":"2021-06-30T11:05:25.272562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf.predict(pd.concat([predict_df, pd.DataFrame(tfidf_vect_fit.transform([text]).toarray())], axis=1))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:05:28.111951Z","iopub.execute_input":"2021-06-30T11:05:28.112372Z","iopub.status.idle":"2021-06-30T11:05:28.295272Z","shell.execute_reply.started":"2021-06-30T11:05:28.112341Z","shell.execute_reply":"2021-06-30T11:05:28.293739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <u>Conclusion</u>\n- As we can see both the models with hypertuned parameter configuration perform very well and with good accuracy.","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:03:03.93512Z","iopub.execute_input":"2021-06-30T11:03:03.935457Z","iopub.status.idle":"2021-06-30T11:03:03.942419Z","shell.execute_reply.started":"2021-06-30T11:03:03.935428Z","shell.execute_reply":"2021-06-30T11:03:03.940996Z"}}}]}