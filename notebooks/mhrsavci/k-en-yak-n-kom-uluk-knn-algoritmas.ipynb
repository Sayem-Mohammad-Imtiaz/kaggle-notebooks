{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# KNN - EN YAKIN KOMŞULUK ALGORİTMASI","metadata":{}},{"cell_type":"markdown","source":"### İçindekiler\n\n\n[MANTAR VERİ SETİ](#1)\n\n> [Veri Setinde İncelenen Özellikler](#2)\n\n\n[VERİ SETİNİN İNCELENMESİ](#3)\n\n\n[K - EN YAKIN KOMŞULUK ALGORİTMASI (K NEAREST NEIGHBORS)](#4)\n\n\n[PERFORMANS ÖLÇÜMÜ](#5)\n\n> [Karışıklık Matrisi (Confusion Matrix) ](#6)\n\n> [ROC Eğrisi (Receiver Operating Characteristics Curve)](#7)\n\n[KAYNAKÇA](#8)","metadata":{}},{"cell_type":"markdown","source":"-----------------------------------------------------------------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"## GİRİŞ","metadata":{}},{"cell_type":"markdown","source":"Günümüzün popüler konularından biri haline gelen **Yapay Zeka** en basit tanımıyla insan zekasından esinlenerek yapılan çalışmaların tümüne verilen isimdir. Yapay zekanın ne olduğu nasıl çalıştığı ve hangi araçları kullandığı ise oldukça karmaşık bir dünyayı karşımıza çıkarmaktadır. Çünkü birçok açıdan yapay zekayı değerlendirmek mümkündür. Robotların insan yaşantısına egemen olmasından bilinen verilerle bilinemeyen bir varsayımda bulunabilen algoritmalara kadar birçok farklı alanda yapay zeka kullanılmaktadır. Burada yapay zekanın daha bilimsel olan bir yönünden bahsedilecektir.\n\nYapay zeka hesaplamayla insanın anlama yeteneğini buluşturmayı amaçlayan bilgisayar biliminin bir alt dalıdır. İnsanın çevresini anlamlandırırken kullanmış olduğu zekasını yapay zeka, çalışmalarının merkezine almıştır ve bu minvalde geliştirdiği ürünlerde \"insan benzeri\" davranışlar amaçlamaktadır. Günümüzde yapay zeka çalışmaları ile popüler hale gelmiş olan insan gibi okuyabilme, bazı durumlarda insan gibi \"düşünüp\" karar verebilme ve insan gibi görüntüleri algılayabilme sistemleri geliştirilmektedir. Belli bir sorunun çözümü için geliştirilen, veriden öğrenen yapay zeka çözümleri \"Dar Yapay Zeka\" olarak tanımlanırken daha geniş anlamda ses ve görüntü algılayabilen, insansı bir karar verebilme yeteneğine sahip kendi kendine öğrenebilen sistemlere \"Geniş Yapay Zeka\" denilmektedir.\n\nMakine Öğrenmesi ise yapay zeka çalışmalarında bir alt grup teşkil eden karmaşık kavramdan sadece bir tanesidir. Yapay zekanın kapsamış olduğu Makine Öğrenmesi, çeşitli algoritmalar (bir sorun karşısında ortaya konan çözüm adımları) bütünü olarak değerlendirilebilir. Burada kastedilen algoritmalar ise bazen bir görüntüyü işlemek bazen konuşulan bir dili anlamak bazen de belirli bir durum için geçmiş örneklere bakarak gelecekteki durumu tahmin etme işlemidir. Makine öğrenmesi denilince elbette bu gibi işlemlerle sınırlandırılamayacak ölçüde geniş bir kavramdan bahsedilmektedir. Makineler ya yönlendirilmelerle öğrenir (Supervised Machine Learning) ya yönlendirmeler olmadan öğrenir (Unsupervised Machine Learning) ya da deneme yanılma yolu ile öğrenir (Reinforcement Machine Learning).\n\n\nYapmış olduğu gözetimlerle yönlendirilen makine öğrenmesi algoritmaları (Supervised Machine Learning) öncelikle \"eğitilerek\" istenilen amaç için hazırlanır. Eğitme işlemi sisteme sunulan girdilerle yapıldıktan sonra \"öğrenen\" sistem test verileri üzerinde çalıştırılır ve performansına bakılır: Algoritma eğitimde neyi ne doğrulukta öğrendi ? Daha sonrasında ise performansına yönelik çalışmalarla gözetimli olarak çalıştırılan algoritmanın en iyi çalışmasını yapması için uğraş verilir. Yönlendirilen-gözetimli makine öğrenmesinde iki farklı sorun üzerinde çalışılır: Ya var olan sayısal değerler kullanılarak sonucu bilinmeyen bir ifade hesaplanır (Regression - Regresyon) ya da kategorik olarak ayrılmış bir veride kategorisi belli olmayan veriler için kategori ayrımı yapılır (Classification - Sınıflandırma).\n\nKNN - En Yakın Komşuluk Algoritması yönlendirilen-gözetimli makine öğrenmesi algoritmalarından en basitidir. Sınıflandırma ve Regresyon problemlerinin ikisi için de kullanılabilmesine rağmen daha çok sınıflandırma işlemlerinde çalıştırılmaktadır. Sınıflandırmada kullanılacak olan KNN algoritması için kategorize edilmiş ya da edilebilecek bir veri setine ihtiyaç duyulmaktadır. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n## MANTAR VERİ SETİ\n\nMantar veri seti, *Agaricus* ve *Lepiota* mantar ailesine ait 23 mantar türünün varsayımsal örneklerinin bir takım özelliklerini içerir. Veri seti içerisindeki incelenen mantarlar zehirli olup olmamalarına göre *kesinlikle yenilebilir, kesinlikle zehirli* veya *bilinmeyen yenilebilir* olarak tanımlanır. Bilinmeyen yenilebilir ile kesinlikle zehirli olanlar birleştirilip kesinlikle zehirli olarak kabul edilmiştir. Son tahlilde incelenen mantar veri seti içerisinde iki adet sınıf mevcuttur : *Kesinlikle Yenilebilir(Eligible)*, *Kesinlikle Zehirli(Poisonous)*. \n\nMantarın hangi sınıfa dahil olduğunu apaçık belirten sadece birkaç özellik bulunmamaktadır. Bu nedenle mantar sınıfının doğru olarak tespiti için birçok gözlem sonucuna ihtiyaç vardır.\n\n<a id=\"2\"></a> <br>\n### Veri Setinde İncelenen Özellikler\n\n**cap-shape:** bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s \n\n**cap-surface:** fibrous=f,grooves=g,scaly=y,smooth=s \n\n**cap-color:** brown=n,buff=b,cinnamon=c,gray=g,green=r, pink=p,purple=u,red=e,white=w,yellow=y \n\n**bruises?:** bruises=t,no=f \n\n**odor:** almond=a,anise=l,creosote=c,fishy=y,foul=f, musty=m,none=n,pungent=p,spicy=s \n\n**gill-attachment:** attached=a,descending=d,free=f,notched=n \n\n**gill-spacing:** close=c,crowded=w,distant=d \n\n**gill-size:** broad=b,narrow=n \n\n**gill-color:** black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e, white=w,yellow=y \n\n**stalk-shape:** enlarging=e,tapering=t \n\n**stalk-root:** bulbous=b,club=c,cup=u,equal=e, rhizomorphs=z,rooted=r,missing=? \n\n**stalk-surface-above-ring:** fibrous=f,scaly=y,silky=k,smooth=s \n\n**stalk-surface-below-ring:** fibrous=f,scaly=y,silky=k,smooth=s \n\n**stalk-color-above-ring:** brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y \n\n**stalk-color-below-ring:** brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y \n\n**veil-type:** partial=p,universal=u \n\n**veil-color:** brown=n,orange=o,white=w,yellow=y \n\n**ring-number:** none=n,one=o,two=t \n\n**ring-type:** cobwebby=c,evanescent=e,flaring=f,large=l, none=n,pendant=p,sheathing=s,zone=z \n\n**spore-print-color:** black=k,brown=n,buff=b,chocolate=h,green=r, orange=o,purple=u,white=w,yellow=y \n\n**population:** abundant=a,clustered=c,numerous=n, scattered=s,several=v,solitary=y \n\n**habitat:** grasses=g,leaves=l,meadows=m,paths=p, urban=u,waste=w,woods=d\n\n","metadata":{}},{"cell_type":"markdown","source":"> Projeye gerekli resimleri eklemek için Image modülünden yararlanılabilir.","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image # Image modulünü projeye dahil edilir.\nImage(\"../input/knnimage/mantar.png\") # Image fonksiyonuna resmin dosya yolu verilir.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n## VERİ SETİNİN İNCELENMESİ","metadata":{}},{"cell_type":"markdown","source":"Veri seti üzerinde bir makine öğrenmesi algoritması çalıştırmadan önce içeriği hakkında bilgi almak için veriler incelenmelidir. Çünkü tanınan veriler hakkında daha kolay yorumlama yapılabilir.\n\nÖncellikle veri setini projeye dahil etmek ve sonrasında incelemek için gerekli kütüphaneler projeye dahil edilir:","metadata":{}},{"cell_type":"code","source":"import numpy as np # Kullanılacak olan dizi, matris veya vektör gibi lineer cebir araçları için gerekli olacaktır.\nimport pandas as pd # Veriyi düzenlemek, veriyi yüklemek ve bir dataframe yapısında veriyi daha kolay incelemek için gerekli olacaktır.\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Veri seti projeye dahil edilir :**","metadata":{}},{"cell_type":"markdown","source":"> **read_csv(dosya_yolu)** fonksiyonu pandas kütüphanesinin bir fonksiyonu olup .csv formatında dosya okumak için kullanılır.\n\n> *data* değişkeni DataFrame yapısındadır.","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv(\"../input/mushrooms.csv\")","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DataFrame yapısı kurulur :**","metadata":{}},{"cell_type":"markdown","source":"> **DataFrame** pandas kütüphanesi içerisinde bulunan veriyi bir tablo şeklinde tiple ifade eden ve verinin okunabilirliğini dolayısıyla yönetilebilirliğini arttıran bir veri tipidir.\n\n> *data* olarak projeye dahil edilen veri seti artık \"mframe\" değişkenine atanmıştır ve DataFrame yapısındadır.\n","metadata":{}},{"cell_type":"code","source":"mframe=pd.DataFrame(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Veri setinden incelenmek üzere örnek alınır :**","metadata":{}},{"cell_type":"markdown","source":"**head(sayi) :** DataFrame içerisinden ilk baştan *sayi* sayısı kadar satırı getirir. Eğer boş kullanılırsa 5 satır alınır.\n\n**tail(sayi) :**  DataFrame içerisinden en sondan *sayi* sayısı kadar satırı getirir. Eğer boş kullanılırsa 5 satır alınır.","metadata":{}},{"cell_type":"code","source":"mframe.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mframe.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Veri setinde bulunan kolonlar incelenir :**","metadata":{}},{"cell_type":"markdown","source":"> **DataFrame.columns :** DataFrame yapısı içerisinde bulunan kolonları getirir.","metadata":{}},{"cell_type":"code","source":"print(mframe.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Herhangi bir kolon özelinde veri seti incelenir : **","metadata":{}},{"cell_type":"code","source":"# Veri seti içerisinde yer alan \"cap-shape\",\"population\",\"class\" kolonlarının ilk 5 değeri getirilir.\n\nprint(mframe[\"cap-shape\"].head()) \nprint(mframe[\"population\"].head())\nprint(mframe[\"class\"].head()) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n## K - EN YAKIN KOMŞULUK ALGORİTMASI (K NEAREST NEIGHBORS)\n\nVeri seti incelendiğinde değerlerin kategorik veriler olduğu görülmektedir. Kategorik verilerde varsayım ve tahmin için Makine Öğrenmesi Algoritmaları'ndan **Yönlendirilmiş-Gözetimli Öğrenme Algoritmaları** kullanılabilir çünkü algoritmadan veriler hakkında bilgi almadan önce algoritmaya etiketlenmiş, önceden kategorize edilmiş veriler tanıtılacaktır. Gözetimli Algoritmalar içerisinde yer alan **Sınıflandırma Algoritmaları** kategorize edilmiş veriler üzerinde çalışan çözümlerdir. Örnek olarak kullanılan mantar veri seti için **Sınıflandırma Algoritmaları**'ndan herhangi biri kullanılabilir. Veri içerisinde neredeyse her kolonda bir sınıflandırma vardır fakat bazı kolonların sınıflandırılması bazı kolonlara bakılarak daha kolay tahmin edilebilir. Veri seti içerisinde bulunan *class* kolonu mantarın zehirli olup olmadığını kategorize etmiştir. Diğer kolonlara bakılarak bu kolonda yer alan kategori tahmin edilebilir.","metadata":{}},{"cell_type":"code","source":"Image(\"../input/knnimage/knn.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sınıfı belli olmayan verinin k sayısı kadar komşuluğuna bakılır. k=3 için sınıfsız verinin çevresinde 3 adet sınıflı veriye bakılır, bu verilerim sınıfları kontrol edilir. Hangi sınıfın verisi fazla ise sınıfsız veri o sınıfa dahil edilir. Örneğin k=3 komşuluğunda 2 adet A sınıfından eleman var ise 2>1 olduğuundan sınıfsız verinin sınıfı A olarak atanır. Böyle bir yöntemle sınıf belirlenmesinde **oy çokluğu** kullanılır, komşuluk içerisinde elemanı çok olan sınıf oylamayı kazanır ve sınıfsız eleman oylamayı kazanan sınıfa dahil edilir.\n\nK - En Yakın Komşuluk algoritmasının kullanımında **Girdi(X)** ve **Çıktı(y)** değerleri ile **Eğitim** ve **Test** veri setleri belirlenir. Eğitim verilerindeki Girdi ve Çıktı değerlerinden yararlanan algoritmanın modeli verinin kuralını belirler. Kural belirlemeden kasıt *hangi Girdi değerleri için veri seti içerisinde hangi Çıktı değerlerinin olduğunu algoritmanın \" öğrenmesidir \"*. Daha sonra da Test veri setinde bulunan Girdi verileri kullanılarak yine Test veri setinde bulunan Çıktı değerleri tahmin edilir.\n\nYeni verinin sınıfını belirlerken daha önce sınıflandırılmış verilere olan k komşuluğuna bakılırken sınıfsız veri ile sınıflandırılmış veriler arasındaki uzaklık değerine bakılır. K - En Yakın Komşuluk algoritması için genelde üç ayrı uzaklık fonksiyonu kullanılır. \n","metadata":{}},{"cell_type":"code","source":"Image(\"../input/knnimages/uzaklikfonk.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Veri Setinin Uygulanacak Algoritma için Hazırlanması : **","metadata":{}},{"cell_type":"markdown","source":" Model oluşturmak için Girdi(X) ve Çıktı(y) değerleri belirlenir.\n Girdi değerleri bir algoritmik modelde kullanılacak ve çıktı değerleri tahmin edilecektir.","metadata":{}},{"cell_type":"code","source":"X=mframe.iloc[:,1:] # \"class\" kolonu dışındaki tüm kolonlar girdi değeri olarak tanımlandı. Ne kadar çok birbiriyle alakalı girdi kolonu kullanılırsa çıktının tahmini o kadar yüksek doğrulukta çıkacaktır.\n\ny=mframe.iloc[:,0]  # \"class\" kolonu çıktı değeri olarak tanımlandı. Çünkü class kolonu içerisindeki kategori tahmin edilecek.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LabelEncoder kategorize edilmiş verilerde her bir kategoriye sıfırdan başlayarak sayı verir. Bu veri seti için zehirli mantar sınıfına(p: poisonous) 1 değerini atarken zehirsiz mantar sınıfına(e: eligible) 0 değerini atamıştır. Tüm sütunlar içerisinde bulunan kategorik veriler göre sayısal değerler kullanılarak yeniden tanımlanmıştır.","metadata":{}},{"cell_type":"markdown","source":"> **LabelEncoder** kategorize edilmiş verileri etiketlemek için kullanılır.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\n\nd=defaultdict(LabelEncoder)\n\nXFit=X.apply(lambda x: d[x.name].fit_transform(x))\n\nLEncoder=LabelEncoder()\n\nyFit=LEncoder.fit_transform(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Veri seti içerisinde bulunan kategorilerin (her bir kategori için ayrı) sayısal değerlerle ifade edilmesi verileri sınıflandırmak açısından iyidir. Fakat girdi değerlerinin sayısal ifadelerden oluşuyor olması modeli eğitmek açısından yanlış sonuçlar doğurabilir. Çünkü sayısal değerler her ne kadar sınıflandırma amacıyla yapılıyor olsa da sayısal değerler karşılaştırıldığında bir sonuç ortaya çıkmaktadır. Örneğin Elma ile Armut sınıfı karşılaştırıldığında iki ayrı gruplandırma olduğundan herhangi bir aritmetik değer çıkmaz fakat 1 ile 2 karşılaştırıldığında sonuç 1 küçük ya da 2 büyük şekilde olabilir. Sayısal kategorize edilen verileri kullanmak için sonuçların model tarafından çarpıtılmasını **One Hot Encoding** yönetimi engelleyebilir. Örneğin bir kolonda **1, 2, 3** ile ifade edilen kategoriler olsun. Bu kolona One Hot Encoding yöntemi uyguladığında **[1 0 0], [0 1 0], [0 0 1]** şeklinde üç kolon tanımlanır. **[1 0 0]** kolonu 1 değerini, **[0 1 0]** kolonu 2 değerini, **[0 0 1]** kolonu da 3 değerini temsil etmektedir.","metadata":{}},{"cell_type":"code","source":"import warnings # Uyarıların yönetimi için kullanılan kütüphanedir.\n\nwarnings.filterwarnings(\"ignore\") # Çıkacak uyarıların gözardı edilmesi için kullanılır.\n\nfrom sklearn.preprocessing import OneHotEncoder\nohc=defaultdict(OneHotEncoder)\n\nresultFrame=pd.DataFrame()\n\nkolonSayisi=mframe.shape[1]\n\nfor i in range(kolonSayisi-1):\n    \n    Xtemp_i=pd.DataFrame(ohc[XFit.columns[i]].fit_transform(XFit.iloc[:,i:i+1]).toarray())\n    \n    ohc_obj=ohc[XFit.columns[i]]\n    LEncoder_i=d[XFit.columns[i]]\n    Xtemp_i.columns=XFit.columns[i]+ \"_\" + LEncoder_i.inverse_transform(ohc_obj.active_features_)\n    \n    \n    X_ohc_i=Xtemp_i.iloc[:,1:]\n    \n    resultFrame=pd.concat([resultFrame,X_ohc_i],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(mframe.shape,\"->\",resultFrame.shape) # Yeni veri setinin boyutu\n\n# Bir kolonda yer alan kategoriler için ayrı ayrı kolonlar oluşturulmuş dolayısıyla veri setinin boyutu artmıştır.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultFrame.head(10) # Hazırlanan yeni veri setinden örnek alınması.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Eğitim ve Test Verilerinin Ayarlanması :**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split # Eğitim ve Test verilerini ayırmak için kullanılan fonksiyondur.\n\nX_train, X_test, y_train, y_test=train_test_split(resultFrame,yFit,test_size=0.3) # Test Verisi %30 Eğitim Verisi %70 olarak atandı.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**K - En Yakın Komşuluk Algoritmasının Uygulanması : **","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier # KNN Algoritmasının modülü projeye dahil edildi.\n\nKModel=KNeighborsClassifier(n_neighbors=30,metric=\"minkowski\") # KNN Modeli kuruldu ve k komşuluk sayısı 30 olarak alındı. Yani sınıflandırılacak olan verinin 30 eleman komşuluğuna bakara karar verir. \n# Eğer herhangi bir uzaklık metriği verilmez ise algoritma \"minkowski\" uzaklığına göre uzaklık hesaplayacaktır. Burada da minkowski uzaklık metriği kullanılmıştır.\n\nKModel.fit(X_train,y_train) # KNN algoritması ayarlanan Eğitim verileri üzerinde uygulandı ve model \"Eğitildi\"\n\ny_pred=KModel.predict(X_test) # Uygulanan model için Test verileri tahmin edildi.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n## PERFORMANS ÖLÇÜMÜ","metadata":{}},{"cell_type":"markdown","source":"Veri seti kullanılarak sınıflandırma işlemi yapıldı fakat hangi sınıf ne kadar doğru tahmin edildi bilinmemektedir. Makine öğrenmesi algoritmaları veya herhangi bir algoritma çalıştırılırken ayrılan kaynak ve çıkan sonucun takibi oldukça önemlidir. Yapılan tüm geliştirmelerde hiç şüphesiz daha az kaynak kullanımı ile daha iyi sonuç elde etme çabası güdülmektedir. Peki \"daha iyi\" sonuç derken ne kastedilmektedir? Makine öğrenmesi algoritmalarından gözetimli öğrenme olan sınıflandırma algoritmalarında \"daha iyi sonuç\"tan kasıt bir veriyi en doğru şekilde sınıflandırmak demektir. Tabiki en iyi şekilde sınıflandırma işlemi yapılırken de kaynak kullanımı gözardı edilmemelidir. Genelde herhangi bir sınıflandırma algoritmasını özelde ise KNN Algoritmasını incelerken performans ölçümü sırasında algoritmanın ne kadar iyi sınıflandırma yaptığının yanı sıra veri setine yeni katılan dolayısıyla sınıfı belli olmayan verilere karşı davranışı da dikkate alınmaktadır. \n\nAlgoritmanın eğitileceği Eğitim verileri ile ne kadar doğru çalıştığının test edileceği Test verileri arasındaki oran burada önem kazanmaktadır. Eğer algoritma eğitim verileri üzerinde fazla çalışırsa Aşırı Öğrenme(Overfitting) durumu ortaya çıkar ve bu durumda yüksek Varyans ortaya çıkar. Algoritma yetersiz eğitim verisiyle çalıştırılırsa eksik öğrenme durumu oluşur ve yüksek Yanlılık ortaya çıkar.\n","metadata":{}},{"cell_type":"code","source":"Image(\"../input/knnimages/uzaklikfonk.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **Aşırı Öğrenme (Overfitting) :** Bir öğrenme modeli,algoritması, eğitileceği veri üzerinde gereğinden fazla çalışmış ise eğitim verilerini ezberlemeye başlar. Model eğitim verilerini ezberlediğinde yeni gelen verilere karşı duyarsızlaşır ve tek bildiği,cevap verdiği durumlar eğitim verileri olmuş olur.Böyle durumlara modelin **Aşırı Öğrenme**si adı verilir. Aşırı öğrenme durumunda modelin varyansı yüksek yanlılığı düşük çıkar.\n\n> **Eksik Öğrenme (Underfitting) :** Oluşturulan bir öğrenme modeli eğitim verilerinden gerekli modeli çıkartamadığında ortaya **Eksik Öğrenme** durumu çıkar. Modelin eğitileceği verinin azlığı ya da sınıflandırmaya uymayan bir veri seti içerisinde sınıflandırma algoritması kullanılarak model oluşturulmaya çalışıldığında ortaya eksik öğrenme çıkar. Eksik öğrenmede model yüksek varyansa ve düşük yanlılığa sahiptir. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n### 5.1. Karışıklık Matrisi (Confusion Matrix) \n\nGerçek değerleri bilinen, sınıflandırma algoritmalarının performanslarını ölçmek için kullanılan değerleri içeren matrise \"Karışıklık Matrisi (Confusion Matrix)\" denir. Karışıklık matrisi sınıflandırmada oluşturulan modelin neyi ne kadar doğru sınıflandırdığını ölçmek için kullanılmaktadır. İkili bir sınıflandırma örneğinde 2X2 boyutunda bir karışıklık matrisi hesaplanabilir.","metadata":{}},{"cell_type":"code","source":"Image(\"../input/knnimages/karisiklikmatrisi.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">**Doğru Pozitif - True Positive - TP :** Gerçekte olan değerin 1 ile tahmin edilen değerin 1 olduğu örneklerin sayısıdır.\n\n>**Yanlış Pozitif - False Positive - FP :** Gerçekte olan değerin 1 ile tahmin edilen değerin 0 olduğu örneklerin sayısıdır.\n\n>**Yanlış Negatif - False Negative - FN :** Gerçekte olan değerin 0 ile tahmin edilen değerin 1 olduğu örneklerin sayısıdır.\n\n>**Doğru Negatif - True Negative  - TN :** Gerçekte olan değerin 0 ile tahmin edilen değerin 0 olduğu örneklerin sayısıdır.\n\n>**Doğruluk Oranı - Accuracy Rate :** Sınıflayıcı olarak kullanılan algoritmanın ne kadar doğrulukta sınıflandırdığının ölçüsüdür. 0 ile 1 arasında çıkan bir değerdir. Oran 1' e yaklaştıkça sınıflandırmanın doğruluğu artar.","metadata":{}},{"cell_type":"code","source":"Image(\"../input/knnimages/dogrulukOrani.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n**Karışıklık Matrisini Oluşturma : **","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix # Karışıklık matris fonksiyonunun dahil olduğu paket dahil edilir.\n\nKarisiklik_Matrisi=confusion_matrix(y_test,y_pred) # Karışıklık matrisinin girdi değerleri yazılarak matris hesaplanır.\n\nprint(Karisiklik_Matrisi)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Doğruluk Oranını Hesaplama : **","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ndogruluk_Orani=accuracy_score(y_test,y_pred) # Yapılan sınıflandırma işleminin ne oranda doğru olduğunu döndürür. 0 ile 1 arasında değer alır.\n\nprint(dogruluk_Orani) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n### 5.2. ROC Eğrisi (Receiver Operating Characteristics Curve)\n\nROC eğrisi çok sınıf içeren sınıflandırma problemlerinin performanslarını kontrol etmek için kullanılan eğridir.\n**AUC (Area Under The Curve)** kavramı burada önemlidir ve ROC Eğrisinin altında kalan alanın ölçüsünü temsil etmektedir. ROC Eğrisi bir olasılık eğrisi olduğu için mükemmel bir modelde eğrinin altında kalan alan bire eşittir. Yani sıfıra yaklaştıkça modelin performansı, sınıfları ayırabilirliği, düşecektir. Eğer AUC değeri 0.5 gibi bir değer ise ölçülen model neredeyse hiç sınıf ayıramıyor demektir. ROC eğrisinin altındaki alan (Area Under Curve (AUC)), bir parametrenin iki sınıf arasında ne kadar iyi ayırt edilebileceğinin bir ölçüsüdür.","metadata":{}},{"cell_type":"markdown","source":"ROC eğrisi iki boyut içeren bir eğridir. Boyutlarından <u>y eksenine</u> **TPR** değeri, <u>x eksenine</u> **FPR** değeri atanır.","metadata":{}},{"cell_type":"code","source":"Image(\"../input/knnimages/tprfpr.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ROC Eğrisi Değerleri Hesaplanır : ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score # ROC Eğrisi için gerekli değerlerin hesaplanmasında kullanılan modüllerdir.\nimport matplotlib.pyplot as plt # ROC Eğrisini çizmek için gerekli çizim modülüdür.\n\n\nfalse_positive_rate, true_positive_rate, threshold = roc_curve(y_test, y_pred) # roc_curve() fonksiyonu aldığı y_test ve y_pred değerlerini kullanarak FPR, TPR ve Threshold(Eşik Değeri) ifadelerini döndürür\n\nprint('KNN Algoritması için AUC Değeri : ', roc_auc_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ROC Eğrisi Çizilir : ","metadata":{}},{"cell_type":"code","source":"plt.subplots(figsize=(10,10))\nplt.title('ROC Eğrisi - KNN')\n\nplt.plot(false_positive_rate, true_positive_rate)\nplt.plot([0, 1], [0,1])\n\nplt.ylabel('True Positive Rate (TPR)')\nplt.xlabel('False Positive Rate (FPR)')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.3. Değişen K Komşuluk Değerleri için K - En Yakın Komşuluk Algoritmasının Performansı","metadata":{}},{"cell_type":"markdown","source":"K - En Yakın Komşuluk(KNN) algoritmasında hiç şüphesiz en önemli değişken **k-komşuluk sayısı**dır. Komşuluk sayısı algoritmanın çalıştığı veri seti üzerinde ne şekilde uzaklık hesaplayacağını belirttiği için veri setinin özelliğine bağlı olarak da değişir. Eğer sadece 100 elemanlı bir veri setinde bu algoritma çalıştırılırsa mutlaka k-komşuluk sayısı 100'den küçük olmalıdır ki anlamlı sonuçlar çıksın. Çünkü 100 elemanlı bir veri setinde herhangi bir elemanın 100 eleman komşuluğuna bakıldığında veri setinin neredeyse tüm elemanlarıyla çalışılacağı için algoritmanın eğitim ve test verilerine gerek kalmayacaktır, algoritma tüm verileri görecektir. Böylece algoritma hem veriyi **ezberleyecek** hem de veri setine yeni bir veri eklendiğinde o verinin sınıfını bulmakta **duyarsızlaşacak**tır.","metadata":{}},{"cell_type":"code","source":"# Değişen k-komşuluk sayılarına göre algoritmanın doğruluk oranına bakılır:\n\ndef KNNHesapla(komsuluk,uzaklik,X_train,X_test,y_train,y_test):\n    \"\"\"\n    Fonksiyon değişen komşuluk ve uzaklık parametrelerine göre KNN algoritmasının doğruluk oran listesini döndürür.\n    \n    komsuluk: KNN algoritmasında kullanılan k-komşuluk sayısının parametresidir.\n    uzaklık: KNN algoritmasında kullanılan uzaklık tipinin parametresidir. minkowski - euclidean - manhattan olmak üzere üç türdür.\n    X_train: Girdi değerlerinden eğitim verisi olarak kullanılan listedir.\n    y_train: Çıktı değerlerinden eğitim verisi olarak kullanılan listedir.\n    X_test: Girdi değerlerinden test verisi olarak kullanılan listedir.\n    y_test: Çıktı değerlerinden test verisi olarak kullanılan listedir. Tahmin edilecek olan listedir.\n    \n    \"\"\"\n    oranListesi=[]\n    \n    for i in range(1,komsuluk):\n        \n        knnModel=KNeighborsClassifier(n_neighbors=i,metric=uzaklik) \n        \n        knnModel.fit(X_train,y_train)\n        \n        y_pred=knnModel.predict(X_test)\n        \n        dogrulukOrani=accuracy_score(y_test,y_pred)\n        \n        oranListesi.append(dogrulukOrani)\n        \n    return oranListesi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KNNHesapla fonksiyonu kullanılarak değişen uzaklık hesaplama yöntemlerine göre algoritmanın doğruluk oranları hesaplanır:\n\noranListe_Min=KNNHesapla(100,\"minkowski\",X_train,X_test,y_train,y_test) # Minkowski uzaklığı için 1-100 komşuluğunda algoritma doğruluk oranının listesi hesaplandı.\n\noranListe_Euc=KNNHesapla(100,\"euclidean\",X_train,X_test,y_train,y_test) # Euclidean uzaklığı için 1-100 komşuluğunda algoritma doğruluk oranının listesi hesaplandı.\n\noranListe_Man=KNNHesapla(100,\"manhattan\",X_train,X_test,y_train,y_test) # Manhattan uzaklığı için 1-100 komşuluğunda algoritma doğruluk oranının listesi hesaplandı.\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(figsize=(15,15)) # Grafik çizimi için 15x15 boyutunda şablon oluşturulur.\n\nx=range(1,100) # Grafikte kullanılacak olan X-Ekseni k-komşuluk sayısını temsil edeceğinden x-ekseni 1-100 arasında bir sayı dizisi olarak tanımlanır.\n\n# X eksenleri sabit olacak şekilde daha öncesinden Minkowski, Euclidean ve Manhattan uzaklıkları kullanılarak hesaplanan doğruluk oranlarına göre farklı çizgiler aynı grafiğe eklenir:\n\nplt.plot(x,oranListe_Min,color=\"r\",linewidth=3.0,label=\"Minkowski\",marker=\"o\",markersize=5) \nplt.plot(x,oranListe_Euc,color=\"g\",linewidth=3.0,label=\"Euclidean\")\nplt.plot(x,oranListe_Man,color=\"b\",linewidth=3.0,label=\"Manhattan\",linestyle=\"--\")\n\n# Grafiğin X ve Y ekseninde görünecek isimler ile bilgilendirme panosu eklenir: \n\nplt.xlabel(\"K-Komşuluk Değeri\",fontsize=\"xx-large\")\nplt.ylabel(\"Algoritmanın Doğruluk Oranı\",fontsize=\"xx-large\")\nplt.legend(fontsize=\"xx-large\")\n\nplt.show() # Grafikle birlikte bilgilendirme satırının çıkmamasını sağlar. Sadece grafiğin görseli çıktı olarak görünür.\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Minkowski, Euclidean ve Manhattan uzaklıkları bu veri seti için 1-100 komşuluğunda aynı değerleri vermektedir.En uygun k komşuluk sayısı 1-60 arasında seçilmelidir. Komşuluk sayısı yaklaşık olarak 60 dan fazla alındığında algoritmanın doğru sınıflandırma oranında ciddi düşüş görülmektedir.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{"trusted":true}},{"cell_type":"markdown","source":"<a id=\"8\"></a> <br>\n## KAYNAKÇA\n\n> 1. https://medium.com/@ayyucekizrak/yapay-zekaya-ba%C5%9Flama-rehberi-91e79d3de8e1\n\n> 1. https://archive.ics.uci.edu/ml/datasets/mushroom \n\n> 2. https://medium.com/@k.ulgen90/makine-%C3%B6%C4%9Frenimi-b%C3%B6l%C3%BCm-2-6d6d120a18e1\n\n> 3. https://towardsdatascience.com/supervised-machine-learning-classification-5e685fe18a6d\n\n> 4. https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/\n\n> 5. https://setscholars.net/2019/02/03/how-to-plot-roc-curve-in-python/\n\n> 6. https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62\n\n> 7. https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5\n\n> 8. https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n\n> 9. https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.plot.html\n\n> 10. https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229","metadata":{}}]}