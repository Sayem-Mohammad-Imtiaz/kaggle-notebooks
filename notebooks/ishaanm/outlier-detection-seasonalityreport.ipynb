{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\ndf = pd.read_csv(r\"../input/onlineretail/OnlineRetail.csv\", encoding='cp1252', parse_dates=['InvoiceDate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Basic Information-\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxdate = df['InvoiceDate'].dt.date.max()\nmindate = df['InvoiceDate'].dt.date.min()\ncustomers = df['CustomerID'].nunique()\nstock = df['StockCode'].nunique()\nquantity = df['Quantity'].sum()\n\nprint(f'Transactions timeframe: {mindate} to {maxdate}.')\nprint(f'Unique customers: {customers}.')\nprint(f'Unique items sold: {stock}.')\nprint(f'Quantity sold in period {quantity}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#two exceptions here negative prices\ndf=df[df.UnitPrice>=0];\ndf.drop(df[df['Quantity']<0].index, axis=0, inplace=True)\ndf['Revenue'] = df['UnitPrice']*df['Quantity']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Frequency of customer, revenue per customer, revenue per item, revenue per unit price","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users = pd.DataFrame(df['CustomerID'].unique())\nusers.columns = ['CustomerID']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frequency_score = df.groupby('CustomerID')['InvoiceDate'].count().reset_index()\nfrequency_score.columns = ['CustomerID', 'Frequency']\ncustomer_money=df.groupby('CustomerID')['Revenue'].mean().reset_index()\ncustomer_money.columns = ['CustomerID', 'TotalRev']\nusersf = pd.merge(users, frequency_score, on='CustomerID')\nusersf.head()\nuserm=pd.merge(usersf, customer_money, on='CustomerID')\nuserm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items = pd.DataFrame(df['StockCode'].unique())\nitems.columns = ['StockCode']\nitems.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_money = df.groupby('StockCode')['Revenue'].sum().reset_index()\nitem_money.columns = ['StockCode', 'TotalRev']\nitems = pd.merge(items, item_money, on='StockCode')\nitems.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prices = pd.DataFrame(df['UnitPrice'].unique())\nprices.columns = ['UnitPrice']\nprices.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"price_money = df.groupby('UnitPrice')['Revenue'].sum().reset_index()\nprice_money .columns = ['UnitPrice', 'TotalRev']\nprices = pd.merge(prices, price_money, on='UnitPrice')\nprices.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#we can see that there are two unit prices which bring in the most revenue\n#we have a few price which is bringing in negative revenue\nsns.scatterplot(items['TotalRev'],items['StockCode']);\nplt.yticks(items['StockCode'], '')\nplt.show()\n#we can see that some stock codes have generated much much more revenue than others\nsns.scatterplot(prices['TotalRev'],prices['UnitPrice']);\nplt.show()\n#we get 4 customers with extreme values of frequency\n\nsns.scatterplot(userm['Frequency'],userm['CustomerID']);\nplt.show()\n#most money making customers\nsns.scatterplot(userm['TotalRev'],userm['CustomerID']);\nplt.show()\nsns.scatterplot(userm['TotalRev'],userm['Frequency']);\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"userm.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#storing outliers of positive interest in respective dataframes\nmost_freq_customers=userm[userm.Frequency>=1000];\nmost_money_making_customers=userm[userm.TotalRev>=10000];\n#super customers are customers which are both frequent and money making\nsuper_customers=most_money_making_customers[most_money_making_customers.Frequency>=1000]\nmost_successfull_products=items[items.TotalRev>=50000];\nmost_succesfull_unit_prices=prices[prices.TotalRev>=100000];\n#storing negative values as they are of importance\nnegative_money_making_customers=userm[userm.TotalRev<=0];\nleast_successfull_products=items[items.TotalRev<=0];","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Advanced Algorithms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyod\n!pip install --upgrade pyod","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Import models\nfrom pyod.models.abod import ABOD\nfrom pyod.models.cblof import CBLOF\nfrom pyod.models.feature_bagging import FeatureBagging\nfrom pyod.models.hbos import HBOS\nfrom pyod.models.iforest import IForest\nfrom pyod.models.knn import KNN\nfrom pyod.models.lof import LOF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"userm.plot.scatter('Frequency','TotalRev')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0, 1))\nuserm[['Frequency','TotalRev']] = scaler.fit_transform(userm[['Frequency','TotalRev']])\nuserm[['Frequency','TotalRev']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#storing in np array for models\nX1 = userm['Frequency'].values.reshape(-1,1)\nX2 = userm['TotalRev'].values.reshape(-1,1)\n\nX = np.concatenate((X1,X2),axis=1)\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state = np.random.RandomState(45)\noutliers_fraction = 0.005\nclassifiers = {\n        'Angle-based Outlier Detector (ABOD)': ABOD(contamination=outliers_fraction),\n        'Cluster-based Local Outlier Factor (CBLOF)':CBLOF(contamination=outliers_fraction,check_estimator=False, random_state=random_state),\n        'Isolation Forest': IForest(contamination=outliers_fraction,random_state=random_state),\n        'K Nearest Neighbors (KNN)': KNN(contamination=outliers_fraction),\n        'Average KNN': KNN(method='mean',contamination=outliers_fraction)\n}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict={}\nfrom scipy import stats\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef model_printer(X,userm):\n  xx , yy = np.meshgrid(np.linspace(0,1 , 200), np.linspace(0, 1, 200))\n\n  for i, (clf_name, clf) in enumerate(classifiers.items()):\n      clf.fit(X)\n      scores_pred = clf.decision_function(X) * -1\n      y_pred = clf.predict(X)\n      n_inliers = len(y_pred) - np.count_nonzero(y_pred)\n      n_outliers = np.count_nonzero(y_pred == 1)\n      plt.figure(figsize=(20, 20))\n      \n      # copy of dataframe\n      dfx = userm\n      dfx['outlier'] = y_pred.tolist()\n      \n      # IX1 - inlier feature 1,  IX2 - inlier feature 2\n      IX1 =  np.array(dfx['Frequency'][dfx['outlier'] == 0]).reshape(-1,1)\n      IX2 =  np.array(dfx['TotalRev'][dfx['outlier'] == 0]).reshape(-1,1)\n      \n      # OX1 - outlier feature 1, OX2 - outlier feature 2\n      OX1 =  dfx['Frequency'][dfx['outlier'] == 1].values.reshape(-1,1)\n      OX2 =  dfx['TotalRev'][dfx['outlier'] == 1].values.reshape(-1,1)\n          \n      print('OUTLIERS:',n_outliers,'INLIERS:',n_inliers, clf_name)\n          \n      # threshold value to consider a datapoint inlier or outlier\n      threshold = stats.scoreatpercentile(scores_pred,100 * outliers_fraction)\n          \n      # decision function calculates the raw anomaly score for every point\n      Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) * -1\n      Z = Z.reshape(xx.shape)\n            \n      # fill blue map colormap from minimum anomaly score to threshold value\n      plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),cmap=plt.cm.Greens_r)\n          \n      # draw red contour line where anomaly score is equal to thresold\n      a = plt.contour(xx, yy, Z, levels=[threshold],linewidths=2, colors='red')\n          \n      # fill orange contour lines where range of anomaly score is from threshold to maximum anomaly score\n      plt.contourf(xx, yy, Z, levels=[threshold, Z.max()],colors='orange')\n          \n      b = plt.scatter(IX1,IX2, c='white',s=20, edgecolor='k')\n      \n      c = plt.scatter(OX1,OX2, c='yellow',s=20, edgecolor='k')\n        \n      plt.axis('tight')  \n      \n      # loc=2 is used for the top left corner \n      plt.legend(\n          [a.collections[0], b,c],\n          ['learned decision function', 'inliers','outliers'],\n          prop=matplotlib.font_manager.FontProperties(size=20),\n          loc=2)\n        \n      plt.xlim((0, 1))\n      plt.ylim((0, 1))\n      dict[clf_name]=dfx[dfx.outlier==1]\n      plt.title(clf_name)\n      print(\"\\n\")\n      plt.show()\n      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nmodel_printer(X,userm)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0, 1))\nprices[['UnitPrice','TotalRev']] = scaler.fit_transform(prices[['UnitPrice','TotalRev']])\nprices[['UnitPrice','TotalRev']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#storing in np array for models\nX_1 = prices['UnitPrice'].values.reshape(-1,1)\nX_2 = prices['TotalRev'].values.reshape(-1,1)\n\nX_ = np.concatenate((X_1,X_2),axis=1)\nX_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#will have to write the function again as it was set up for frequency\ndict2={}\ndef model_printer2(X,userm):\n  xx , yy = np.meshgrid(np.linspace(0,1 , 200), np.linspace(0, 1, 200))\n\n  for i, (clf_name, clf) in enumerate(classifiers.items()):\n      clf.fit(X)\n      scores_pred = clf.decision_function(X) * -1\n      y_pred = clf.predict(X)\n      n_inliers = len(y_pred) - np.count_nonzero(y_pred)\n      n_outliers = np.count_nonzero(y_pred == 1)\n      plt.figure(figsize=(20, 20))\n      \n      # copy of dataframe\n      dfx = userm\n      dfx['outlier'] = y_pred.tolist()\n      \n      # IX1 - inlier feature 1,  IX2 - inlier feature 2\n      IX1 =  np.array(dfx['UnitPrice'][dfx['outlier'] == 0]).reshape(-1,1)\n      IX2 =  np.array(dfx['TotalRev'][dfx['outlier'] == 0]).reshape(-1,1)\n      \n      # OX1 - outlier feature 1, OX2 - outlier feature 2\n      OX1 =  dfx['UnitPrice'][dfx['outlier'] == 1].values.reshape(-1,1)\n      OX2 =  dfx['TotalRev'][dfx['outlier'] == 1].values.reshape(-1,1)\n          \n      print('OUTLIERS:',n_outliers,'INLIERS:',n_inliers, clf_name)\n          \n      # threshold value to consider a datapoint inlier or outlier\n      threshold = stats.scoreatpercentile(scores_pred,100 * outliers_fraction)\n          \n      # decision function calculates the raw anomaly score for every point\n      Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) * -1\n      Z = Z.reshape(xx.shape)\n            \n      # fill blue map colormap from minimum anomaly score to threshold value\n      plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),cmap=plt.cm.Greens_r)\n          \n      # draw red contour line where anomaly score is equal to thresold\n      a = plt.contour(xx, yy, Z, levels=[threshold],linewidths=2, colors='red')\n          \n      # fill orange contour lines where range of anomaly score is from threshold to maximum anomaly score\n      plt.contourf(xx, yy, Z, levels=[threshold, Z.max()],colors='orange')\n          \n      b = plt.scatter(IX1,IX2, c='white',s=20, edgecolor='k')\n      \n      c = plt.scatter(OX1,OX2, c='yellow',s=20, edgecolor='k')\n        \n      plt.axis('tight')  \n      \n      # loc=2 is used for the top left corner \n      plt.legend(\n          [a.collections[0], b,c],\n          ['learned decision function', 'inliers','outliers'],\n          prop=matplotlib.font_manager.FontProperties(size=20),\n          loc=2)\n        \n      plt.xlim((0, 1))\n      plt.ylim((0, 1))\n      plt.title(clf_name)\n      print(\"\\n\")\n      plt.show()\n      dict2[clf_name]=dfx[dfx.outlier==1]\n\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_printer2(X_,prices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dict has all the outliers of first(userm)\n#dict2 has all the outliers of second(prices)\ndict.items()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict2.items()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Seasonality Report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom datetime import datetime, timedelta\ndateparse = lambda x: pd.datetime.strptime(x, '%m/%d/%Y %H:%M')\ndf3 = pd.read_csv(r\"../input/onlineretail/OnlineRetail.csv\", encoding='cp1252', parse_dates=['InvoiceDate'],date_parser=dateparse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3=df3[df3.Quantity>=0];\ndf3=df3[df3.UnitPrice>=0]\ndf3.info()\ndf3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qt = pd.DataFrame(df3['InvoiceDate'].unique())\nqt.columns = ['InvoiceDate']\nqt.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qt_quan = df3.groupby('InvoiceDate')['Quantity'].sum()\nqt_quan.columns = ['InvoiceDate','Quantity']\nqtm=pd.merge(qt, qt_quan, on='InvoiceDate')\nqtm2=qtm;\nqtm.head()\nqtm3=qtm2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\nqtm3.info()\nqtm3.index.dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qtm4=qtm3.set_index('InvoiceDate')\nqtm4.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qtm4.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = qtm4['Quantity'].resample('d').sum()\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\ndecomposition = sm.tsa.seasonal_decompose(y ,model='additive')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decomposition.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decomposition.seasonal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fig, (ax1,ax2,ax3) = plt.subplots(3,1, figsize=(15,8))\nimport pylab\npylab.rcParams['figure.figsize'] = (14, 9)\n\ndecomposition.seasonal.plot()\nprint(\"Seasonality\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}