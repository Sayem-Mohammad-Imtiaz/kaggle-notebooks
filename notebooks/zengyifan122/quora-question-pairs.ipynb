{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from time import time\nimport pandas as pd\nimport numpy as np\nfrom gensim.models import KeyedVectors\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport itertools\nimport datetime\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model\nfrom keras.layers import Input, Embedding, LSTM, Lambda\nimport keras.backend as K\nfrom keras.optimizers import Adadelta\nfrom keras.callbacks import ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# File paths\nTRAIN_CSV = '../input/question-pairs-dataset/questions.csv'\n# TEST_CSV = '../input/question-pairs-dataset/test.csv'\nEMBEDDING_FILE = '../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin.gz'\n# MODEL_SAVING_DIR = '/home/ecohen/HDD/HDD4/Models/Kaggle/Quora/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load training and test set\ntrain_df = pd.read_csv(TRAIN_CSV)\n# test_df = pd.read_csv(TEST_CSV)\n\nstops = set(stopwords.words('english'))\n\ndef text_to_word_list(text):\n    ''' Pre process and convert texts to a list of words '''\n    text = str(text)\n    text = text.lower()\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n\n    text = text.split()\n\n    return text\n# Prepare embedding\nvocabulary = dict()\ninverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\nword2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n\nquestions_cols = ['question1', 'question2']\n\n# Iterate over the questions only of both training and test datasets\nfor dataset in [train_df]:\n    for index, row in dataset.iterrows():\n\n        # Iterate through the text of both questions of the row\n        for question in questions_cols:\n\n            q2n = []  # q2n -> question numbers representation\n            for word in text_to_word_list(row[question]):\n\n                # Check for unwanted words\n                if word in stops and word not in word2vec.vocab:\n                    continue\n\n                if word not in vocabulary:\n                    vocabulary[word] = len(inverse_vocabulary)\n                    q2n.append(len(inverse_vocabulary))\n                    inverse_vocabulary.append(word)\n                else:\n                    q2n.append(vocabulary[word])\n\n            # Replace questions as word to question as number representation\n            dataset.set_value(index, question, q2n)\nembedding_dim = 300\nembeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\nembeddings[0] = 0  # So that the padding will be ignored\n\n# Build the embedding matrix\nfor word, index in vocabulary.items():\n    if word in word2vec.vocab:\n        embeddings[index] = word2vec.word_vec(word)\n\ndel word2vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model variables\nn_hidden = 50\ngradient_clipping_norm = 1.25\nbatch_size = 64\nn_epoch = 25\n\ndef exponent_neg_manhattan_distance(left, right):\n    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n\n# The visible layer\nleft_input = Input(shape=(max_seq_length,), dtype='int32')\nright_input = Input(shape=(max_seq_length,), dtype='int32')\n\nembedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n\n# Embedded version of the inputs\nencoded_left = embedding_layer(left_input)\nencoded_right = embedding_layer(right_input)\n\n# Since this is a siamese network, both sides share the same LSTM\nshared_lstm = LSTM(n_hidden)\n\nleft_output = shared_lstm(encoded_left)\nright_output = shared_lstm(encoded_right)\n\n# Calculates the distance as defined by the MaLSTM model\nmalstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n\n# Pack it all up into a model\nmalstm = Model([left_input, right_input], [malstm_distance])\n\n# Adadelta optimizer, with gradient clipping by norm\noptimizer = Adadelta(clipnorm=gradient_clipping_norm)\n\nmalstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n\n# Start training\ntraining_start_time = time()\n\nmalstm_trained = malstm.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, nb_epoch=n_epoch,\n                            validation_data=([X_validation['left'], X_validation['right']], Y_validation))\n\nprint(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_seq_length = max(train_df.question1.map(lambda x: len(x)).max(),\n                     train_df.question2.map(lambda x: len(x)).max(),)\n\n# Split to train validation\nvalidation_size = 40000\ntraining_size = len(train_df) - validation_size\n\nX = train_df[questions_cols]\nY = train_df['is_duplicate']\n\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size)\n\n# Split to dicts\nX_train = {'left': X_train.question1, 'right': X_train.question2}\nX_validation = {'left': X_validation.question1, 'right': X_validation.question2}\n\n# Convert labels to their numpy representations\nY_train = Y_train.values\nY_validation = Y_validation.values\n\n# Zero padding\nfor dataset, side in itertools.product([X_train, X_validation], ['left', 'right']):\n    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n\n# Make sure everything is ok\nassert X_train['left'].shape == X_train['right'].shape\nassert len(X_train['left']) == len(Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model variables\nn_hidden = 50\ngradient_clipping_norm = 1.25\nbatch_size = 64\nn_epoch = 25\n\ndef exponent_neg_manhattan_distance(left, right):\n    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n\n# The visible layer\nleft_input = Input(shape=(max_seq_length,), dtype='int32')\nright_input = Input(shape=(max_seq_length,), dtype='int32')\n\nembedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n\n# Embedded version of the inputs\nencoded_left = embedding_layer(left_input)\nencoded_right = embedding_layer(right_input)\n\n# Since this is a siamese network, both sides share the same LSTM\nshared_lstm = LSTM(n_hidden)\n\nleft_output = shared_lstm(encoded_left)\nright_output = shared_lstm(encoded_right)\n\n\n# Calculates the distance as defined by the MaLSTM model\nmalstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n\n# Pack it all up into a model\nmalstm = Model([left_input, right_input], [malstm_distance])\n\n# Adadelta optimizer, with gradient clipping by norm\noptimizer = Adadelta(clipnorm=gradient_clipping_norm)\n\nmalstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n\nfrom keras.callbacks import ModelCheckpoint\n# checkpoint\nfilepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]\n\nmalstm_trained = malstm.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, nb_epoch=5,\n                            validation_data=([X_validation['left'], X_validation['right']], Y_validation),callbacks=callbacks_list)\n\nprint(\"Training time finished.\\n{} epochs \".format(n_epoch))","execution_count":20,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:46: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n","name":"stderr"},{"output_type":"stream","text":"Train on 364351 samples, validate on 40000 samples\nEpoch 1/5\n364351/364351 [==============================] - 3329s 9ms/step - loss: 0.1745 - acc: 0.7461 - val_loss: 0.1608 - val_acc: 0.7705\n\nEpoch 00001: val_acc improved from -inf to 0.77048, saving model to weights-improvement-01-0.77.hdf5\nEpoch 2/5\n364351/364351 [==============================] - 3330s 9ms/step - loss: 0.1534 - acc: 0.7842 - val_loss: 0.1500 - val_acc: 0.7931\n\nEpoch 00002: val_acc improved from 0.77048 to 0.79312, saving model to weights-improvement-02-0.79.hdf5\nEpoch 3/5\n364351/364351 [==============================] - 3313s 9ms/step - loss: 0.1455 - acc: 0.7973 - val_loss: 0.1450 - val_acc: 0.7966\n\nEpoch 00003: val_acc improved from 0.79312 to 0.79660, saving model to weights-improvement-03-0.80.hdf5\nEpoch 4/5\n364351/364351 [==============================] - 3319s 9ms/step - loss: 0.1408 - acc: 0.8045 - val_loss: 0.1419 - val_acc: 0.8042\n\nEpoch 00004: val_acc improved from 0.79660 to 0.80422, saving model to weights-improvement-04-0.80.hdf5\nEpoch 5/5\n364351/364351 [==============================] - 3315s 9ms/step - loss: 0.1375 - acc: 0.8094 - val_loss: 0.1404 - val_acc: 0.8027\n\nEpoch 00005: val_acc did not improve from 0.80422\nTraining time finished.\n25 epochs \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot accuracy\nplt.plot(malstm_trained.history['acc'])\nplt.plot(malstm_trained.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n\n# Plot loss\nplt.plot(malstm_trained.history['loss'])\nplt.plot(malstm_trained.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir('./'))","execution_count":21,"outputs":[{"output_type":"stream","text":"['weights-improvement-03-0.80.hdf5', 'weights-improvement-02-0.79.hdf5', 'weights-improvement-04-0.80.hdf5', '__notebook_source__.ipynb', 'weights-improvement-01-0.77.hdf5', '.ipynb_checkpoints']\n","name":"stdout"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}