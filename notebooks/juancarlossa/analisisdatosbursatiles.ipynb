{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Análisis de mercados bursátiles\n\n## Índice de contenidos\n\n1. Objetivos\n    - Motivación \n    - Datasets seleccionados\n1. Extracción y preprocesamiento de datos\n1. Análisis exploratorio\n1. Clusterización\n1. Regresión y clasificación\n1. Conclusiones","metadata":{}},{"cell_type":"markdown","source":"### 1. Objetivos","metadata":{}},{"cell_type":"markdown","source":"### 1.1 Motivación\n\nEl objetivo de este trabajo es crear un modelo para predecir el rendimiento que tendrán las empresas noveles en bolsa. Para ello, nos pondremos en la piel de unos gestores de una Hedge Fund especializada en inversiones en empresas que han salido a bolsa hace poco tiempo. \n\nPara llevar a cabo este modelo, necesitamos hacer un estudio de mercado sobre la situación promedio y el recorrido que han tenido las empresas que hacen una IPO (*Initial Public Offering*) para adquirir conocimientos aplicables a este subsector. Una vez tengamos una visión global de este, crearemos un modelo de clasificación para predecir si el próximo rendimiento trimestral de la acción de una empresa que salió a bolsa hace 1 año va a ser negativo, positivo o remarcable. \n\nCon esta predicción, lograremos separar las empresas cuyo rendimiento pudiera ser interesante de las que no ofrecerían, *a priori*, gran rentabilidad. Una vez que hayamos clasificado las empresas en base a su próxima evolución trimestral, seleccionaremos a las más prometedoras para realizar un estudio adicional: estimar la rentabilidad exacta esperada de estas.\n\nDefinimos las empresas más prometedoras como aquellas que obtienen una mayor probabilidad de ofrecer un rendimiento excepcional en el trimestre siguiente a su primer año cotizando en bolsa. Para ello, se generará un modelo LSTM individual para cada empresa en base a la variación de su cotización durante el último año, con el fin de predecir su variación final en los siguientes 3 meses.","metadata":{}},{"cell_type":"markdown","source":"### 1.2 Datasets\n\nHemos seleccionado dos datasets sobre IPOs:\n\n#### [Startup Investments](https://www.kaggle.com/justinas/startup-investments?select=ipos.csv)\n\nEste dataset contiene información sobre el recorrido que han realizado distintas empresas hasta llegar a su IPO. Dicha información nos permitirá obtener un conocimiento general sobre el proceso que sufre una empresa hasta su salida a bolsa. Algunas variables disponibles en este dataset son:\n\n1. Número de empleados\n2. Eventos remarcables en la historia de la empresa\n3. Localización\n4. Rondas de financiación\n\n\n#### [Stocks IPO information & results](https://www.kaggle.com/proselotis/financial-ipo-data)\n\nEste conjunto de datos ofrece información relativa al desempeño de la empresa durante su primer año en bolsa después de su salida a bolsa. Mediante esta información, crearemos los modelos de predicción que nos permitan discernir el rendimiento esperado del precio de una acción en el trimestre siguiente a su primer año en bolsa. Aunque este dataset sólo aporta datos del primer año de la empresa en bolsa, hemos obtenido los precios de las acciones 3 meses después empleando la librería `yfinance`. Un ejemplo de datos que podemos encontrar en este dataset son:\n\n1. Número de días en los que la acción superó el rendimiento del S&P500\n2. Año de fundación y fecha de IPO de la empresa\n3. Ingresos y beneficio neto de la empresa","metadata":{}},{"cell_type":"markdown","source":"### 2. Extracción y preprocesamiento de datos","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:15:35.091Z","iopub.execute_input":"2021-06-20T23:15:35.091388Z","iopub.status.idle":"2021-06-20T23:15:35.096174Z","shell.execute_reply.started":"2021-06-20T23:15:35.091353Z","shell.execute_reply":"2021-06-20T23:15:35.09493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install exchange\n# Kaggle no lo soporta\n#!pip install yfinance","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:15:35.098804Z","iopub.execute_input":"2021-06-20T23:15:35.099179Z","iopub.status.idle":"2021-06-20T23:15:41.788629Z","shell.execute_reply.started":"2021-06-20T23:15:35.099146Z","shell.execute_reply":"2021-06-20T23:15:41.7875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importar las librerías de Python necesarias para este notebook\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nimport os\nimport exchange\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.cluster import KMeans\nfrom mpl_toolkits.mplot3d import Axes3D","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-20T23:18:00.931223Z","iopub.execute_input":"2021-06-20T23:18:00.931672Z","iopub.status.idle":"2021-06-20T23:18:02.233395Z","shell.execute_reply.started":"2021-06-20T23:18:00.931626Z","shell.execute_reply":"2021-06-20T23:18:02.232364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cargar ficheros\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:15:43.137234Z","iopub.execute_input":"2021-06-20T23:15:43.13769Z","iopub.status.idle":"2021-06-20T23:15:43.156853Z","shell.execute_reply.started":"2021-06-20T23:15:43.137645Z","shell.execute_reply":"2021-06-20T23:15:43.155646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1 - Extracción y limpieza de datos\n\nEn este apartado se lleva a cabo el proceso de extracción y limpieza. Los datos se encuentran separados en distintos datasets, por tanto, se deben hacer múltiples extracciones.\n","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Stocks IPO information & results\n\nEste dataset contiene información sobre las acciones estadounidenses de empresas (antes de cotizar en bolsa), información como el número de empleados, la ubicación de la sede, cuándo la empresa declara un mes fiscal, metadatos sobre la propiedad y más. Por supuesto, no podría validar los resultados de ninguna predicción sin los resultados del stock. Por lo tanto, el primer año fiscal después de salir a bolsa contiene datos (incluidos la apertura, el cierre, el volumen y el máximo de cada día).","metadata":{}},{"cell_type":"code","source":"# Cargar fichero objects\nobjects = pd.read_csv('/kaggle/input/startup-investments/objects.csv')\n\n# Crear dataset de compañías\ncompanies = objects[objects['entity_type'] == \"Company\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:15:43.158792Z","iopub.execute_input":"2021-06-20T23:15:43.159114Z","iopub.status.idle":"2021-06-20T23:15:52.734465Z","shell.execute_reply.started":"2021-06-20T23:15:43.159082Z","shell.execute_reply":"2021-06-20T23:15:52.733354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seleccionar columnas relevantes\ncompanies = companies[['id','normalized_name','category_code','status', \n                    'country_code', 'state_code', 'city', 'region','milestones', 'founded_at']]\n\ncompanies.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:15:52.73598Z","iopub.execute_input":"2021-06-20T23:15:52.736387Z","iopub.status.idle":"2021-06-20T23:15:52.811473Z","shell.execute_reply.started":"2021-06-20T23:15:52.736343Z","shell.execute_reply":"2021-06-20T23:15:52.810157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cargar fichero de relaciones [Compañía - Empleado]\nrelationships = pd.read_csv('/kaggle/input/startup-investments/relationships.csv')\n\n# Agrupar por compañía haciendo un conteo del número de empleados\nrelationships = relationships.groupby('relationship_object_id')['relationship_object_id'].count().reset_index(name='employees')\n\n# Renombrar columna para hacer merge\nrelationships = relationships.rename(columns = {\"relationship_object_id\":\"id\"})\n\n# Añadir el número de empleados a la compañía\ncompanies = pd.merge(companies, relationships, how=\"left\", on=[\"id\"])\n\ncompanies['employees'] = companies['employees'].fillna(0)\n\n# Visualizar dataset resultante\ncompanies.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:15:52.813118Z","iopub.execute_input":"2021-06-20T23:15:52.813555Z","iopub.status.idle":"2021-06-20T23:15:54.961063Z","shell.execute_reply.started":"2021-06-20T23:15:52.813512Z","shell.execute_reply":"2021-06-20T23:15:54.959735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"investments = pd.read_csv('/kaggle/input/startup-investments/investments.csv')\ninvestments.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:15:54.962306Z","iopub.execute_input":"2021-06-20T23:15:54.962587Z","iopub.status.idle":"2021-06-20T23:15:55.151502Z","shell.execute_reply.started":"2021-06-20T23:15:54.96256Z","shell.execute_reply":"2021-06-20T23:15:55.150639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seleccionar columnas relevantes\ninvestments = investments[['investor_object_id','funded_object_id']]\n\n# Renombrar columnas\ninvestments = investments.rename(columns={\"investor_object_id\": \"object_id\"})\n\n# Agrupar por compañía haciendo un conteo del número de empleados\ninvestments = investments.groupby(['object_id'])['funded_object_id'].count().reset_index(name='count_investments')\n\n# Renombrar columna para hacer merge\ninvestments = investments.rename(columns = {\"object_id\":\"id\"})\n\n# Visualizar dataset resultante\ninvestments.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:15:55.153965Z","iopub.execute_input":"2021-06-20T23:15:55.154468Z","iopub.status.idle":"2021-06-20T23:15:55.217444Z","shell.execute_reply.started":"2021-06-20T23:15:55.154419Z","shell.execute_reply":"2021-06-20T23:15:55.216554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Unir datasets por clave\ncompanies = pd.merge(companies, investments, how=\"left\", on=[\"id\"])\n\n# Sustituir nan por 0\ncompanies['count_investments'] = companies['count_investments'].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:15:55.219283Z","iopub.execute_input":"2021-06-20T23:15:55.219775Z","iopub.status.idle":"2021-06-20T23:15:55.432045Z","shell.execute_reply.started":"2021-06-20T23:15:55.219725Z","shell.execute_reply":"2021-06-20T23:15:55.430626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cargar fichero de adquisiciones\nacquisitions = pd.read_csv('/kaggle/input/startup-investments/acquisitions.csv')\n\n# Seleccionar columnas relevantes\nacquisitions = acquisitions[['acquiring_object_id','acquired_object_id','price_amount',\n                    'price_currency_code', 'acquired_at']]\n\n# Renombrar columnas\nacquisitions = acquisitions.rename(columns={\"acquiring_object_id\": \"object_id\"})\n\n# Agrupar por compañía haciendo un conteo del número de empleados\nacquisitions = acquisitions.groupby(['object_id', 'price_currency_code'])['price_amount'].sum().reset_index(name='price_amount')\n\n# Renombrar columna para hacer merge\nacquisitions = acquisitions.rename(columns = {\"object_id\":\"id\"})\n\n# Agrupar adquisiciones por empresa\nacquisitions = acquisitions.groupby('price_currency_code')\n\n# Visualizar dataset resultante\nacquisitions.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:15:55.433634Z","iopub.execute_input":"2021-06-20T23:15:55.433989Z","iopub.status.idle":"2021-06-20T23:15:55.549118Z","shell.execute_reply.started":"2021-06-20T23:15:55.433952Z","shell.execute_reply":"2021-06-20T23:15:55.548363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exchange_rate_CAD = float(exchange.rate('CAD', 'USD'))\nexchange_rate_GBP = float(exchange.rate('GBP', 'USD'))\nexchange_rate_JPY = float(exchange.rate('JPY', 'USD'))\n\ndef price_amount_USD(data):\n\n    if data['price_currency_code'] == 'CAD':\n        price_amount_USD = data['price_amount'] * exchange_rate_CAD\n    elif data['price_currency_code'] == 'GBP':\n        price_amount_USD = data['price_amount'] * exchange_rate_GBP\n    elif data['price_currency_code'] == 'JPY':\n        price_amount_USD = data['price_amount'] * exchange_rate_JPY\n    else:\n        price_amount_USD = data['price_amount']\n\n    return round(price_amount_USD,0)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:15:55.550274Z","iopub.execute_input":"2021-06-20T23:15:55.550685Z","iopub.status.idle":"2021-06-20T23:15:59.617573Z","shell.execute_reply.started":"2021-06-20T23:15:55.550654Z","shell.execute_reply":"2021-06-20T23:15:59.616459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acquisitions = acquisitions.apply(lambda x: x)\nacquisitions['acquirements'] = acquisitions.apply(price_amount_USD, axis=1)\nacquisitions = acquisitions[['id', 'acquirements', 'price_currency_code']]\nacquisitions","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:15:59.619403Z","iopub.execute_input":"2021-06-20T23:15:59.619856Z","iopub.status.idle":"2021-06-20T23:15:59.792415Z","shell.execute_reply.started":"2021-06-20T23:15:59.619811Z","shell.execute_reply":"2021-06-20T23:15:59.791288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"companies = pd.merge(companies, acquisitions, how=\"left\", on=[\"id\"])\ncompanies['acquirements'] = companies['acquirements'].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:15:59.794039Z","iopub.execute_input":"2021-06-20T23:15:59.794505Z","iopub.status.idle":"2021-06-20T23:15:59.932411Z","shell.execute_reply.started":"2021-06-20T23:15:59.794459Z","shell.execute_reply":"2021-06-20T23:15:59.931452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **id**: Identificador de la compañía\n* **normalized_name**: Nombre de la compañía\n* **category_code**: Código de la categoría a la que pertenece\n* **status**: Estado en el que se encuentra (operating, acquired, closed, ipo)\n* **country_code**: Código del pais al que pertenece\n* **state_code**: Código del estado al que pertenece\n* **city**:  Ciudad a la que pertenece\n* **region**: Región a la que pertenece\n* **milestones**: Número de noticias generadas (Premios, noticias, lanzamientos... )\n* **employees**: Número de empleados\n* **acquirements**: Dinero empleado en adquisiciones\n* **count_investments**: Número de inversiones","metadata":{}},{"cell_type":"code","source":"# Análisis del dataset\n\ncompanies = companies.dropna(0,how='all') # Eliminar filas vacías, si las hay\ncompanies = companies.dropna(subset=['id', 'normalized_name', 'category_code']) # Eliminar filas con valores nulos en columnas relevantes\n\nprint('Dimensiones: ', companies.shape) # Obtener dimensiones del dataset (filas, columnas)\ncompanies.describe(include=['object'])","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:15:59.933972Z","iopub.execute_input":"2021-06-20T23:15:59.934617Z","iopub.status.idle":"2021-06-20T23:16:00.850588Z","shell.execute_reply.started":"2021-06-20T23:15:59.934569Z","shell.execute_reply":"2021-06-20T23:16:00.849414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observamos que el número de filas se corresponde con el número de ids únicos y nombres, por tanto, podemos confirmar que en este dataset no existen compañías duplicadas.\n\nLas compañías estan divididas en 42 categorías, de las cuales, Software es la más frecuente. \nLas compañías pueden estar en 4 estados (operating, acquired, closed, ipo), el más frecuente es operating.  \nEl dataset contiene información de compañías de 147 paises. El país del cual hay más compañías es USA.","metadata":{}},{"cell_type":"markdown","source":"Todas las columnas son strings excepto el número de milestones y empleados, que son de tipo numérico. Esto supone un problema puesto que los modelos no siempre pueden trabajar con datos de tipo categorico. Para solucionarlo empleamos la técnica de One Hot Encoding a partir de la cual se obtiene una codificación de características categóricas como una matriz numérica.","metadata":{}},{"cell_type":"markdown","source":"## 1.x Matriz de correlación\n\nLa matriz de correlación es un dato tabular que representa las correlaciones entre pares de variables en un dato dado. Cada fila y columna representa una variable, y cada valor de esta matriz es el coeficiente de correlación entre las variables representadas por la fila y columna correspondientes.\n\nLa matriz de correlación es una importante métrica de análisis de datos que se calcula para resumir los datos a fin de comprender la relación entre las diversas variables y tomar decisiones en consecuencia.\n\nTambién es un importante paso de preprocesamiento en aprendizaje automático cuando se desea reducir la dimensionalidad de un dato de alta dimensión.\n\nLa interpretación, es sencilla: cada fila representa una variable, y todas las columnas representan las mismas variables que las filas, de ahí el número de filas sea igual que el número de columnas.\n\nEl valor de cada casilla representa la correlación entre pares de variables:\n\n* Un valor positivo grande (cercano a 1,0) indica una fuerte correlación positiva, es decir, si el valor de una de las variables aumenta, el valor de la otra variable aumenta también.\n\n* Un valor negativo grande (cercano a -1,0) indica una fuerte correlación negativa, es decir, que el valor de una de las variables disminuye al aumentar el de la otra y viceversa.\n* Un valor cercano a 0 (tanto positivo como negativo) indica la ausencia de cualquier correlación entre las dos variables, y por lo tanto esas variables son independientes entre sí.","metadata":{}},{"cell_type":"code","source":"def get_binary_status(data):\n\n    status_operating = False\n    status_acquired = False\n    status_closed = False\n    status_ipo = False\n\n    if data['status'] == 'operating':\n        status_operating = True\n    elif data['status'] == 'acquired':\n        status_acquired = True\n    elif data['status'] == 'closed':\n        status_closed = True\n    elif data['status'] == 'ipo':\n        status_ipo = True\n\n    return pd.Series([status_operating, status_acquired, status_closed, status_ipo])","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:16:00.85229Z","iopub.execute_input":"2021-06-20T23:16:00.852724Z","iopub.status.idle":"2021-06-20T23:16:00.860519Z","shell.execute_reply.started":"2021-06-20T23:16:00.852681Z","shell.execute_reply":"2021-06-20T23:16:00.859446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"companies_status = companies.copy()\ncompanies_status[['status_operating','status_acquired','status_closed','status_ipo']] = companies.apply(get_binary_status, axis=1)\ncompanies_status.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:16:00.861866Z","iopub.execute_input":"2021-06-20T23:16:00.862181Z","iopub.status.idle":"2021-06-20T23:16:30.560015Z","shell.execute_reply.started":"2021-06-20T23:16:00.862143Z","shell.execute_reply":"2021-06-20T23:16:30.559023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"services = ['biotech', 'hospitality', 'pets', 'medical', 'health', 'education',\n            'security', 'government', 'travel', 'nonprofit', 'public_relations']\n\ntechnology = ['web', 'games_video', 'network_hosting', 'cleantech', 'software', 'search', 'social', 'news', 'messaging','nanotech', \n             'photo_video','music']\n\nbusiness = ['advertising', 'enterprise', 'consulting', 'analytics', 'ecommerce', 'finance', 'legal']\n\nmanufacturing = ['mobile', 'manufacturing', 'design', 'hardware', 'automotive', 'sports', 'transportation', \n                'fashion', 'real_estate',  'semiconductor', 'local']\n\nother = ['other']","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:16:30.561365Z","iopub.execute_input":"2021-06-20T23:16:30.561665Z","iopub.status.idle":"2021-06-20T23:16:30.567962Z","shell.execute_reply.started":"2021-06-20T23:16:30.561634Z","shell.execute_reply":"2021-06-20T23:16:30.567015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_binary_category(data):\n    category_services = False\n    category_technology = False\n    category_business = False\n    category_manufacturing = False\n    category_other = False\n    category_name = \"\"\n\n    if data['category_code'] in services:\n        category_services = True\n        category_name = \"services\"\n    elif data['category_code'] in technology:\n        category_technology = True \n        category_name = \"technology\"\n\n    elif data['category_code'] in business:\n        category_business = True\n        category_name = \"business\"\n\n    elif data['category_code'] in manufacturing:\n        category_manufacturing = True\n        category_name = \"manufacturing\"\n\n    elif data['category_code'] in other:\n        category_other = True\n        category_name = \"other\"\n\n\n    return pd.Series([category_name,category_services,category_technology,category_business,category_manufacturing,category_other])","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:16:30.569225Z","iopub.execute_input":"2021-06-20T23:16:30.569592Z","iopub.status.idle":"2021-06-20T23:16:30.581425Z","shell.execute_reply.started":"2021-06-20T23:16:30.569561Z","shell.execute_reply":"2021-06-20T23:16:30.580303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"companies_status = companies_status.copy()\ncompanies_status[['category_name', 'category_services','category_technology','category_business','category_manufacturing','category_other']] = companies.apply(get_binary_category, axis=1)\ncompanies_status.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:16:30.582771Z","iopub.execute_input":"2021-06-20T23:16:30.583088Z","iopub.status.idle":"2021-06-20T23:17:04.745992Z","shell.execute_reply.started":"2021-06-20T23:16:30.583048Z","shell.execute_reply":"2021-06-20T23:17:04.745085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mat = companies_status[['category_name','count_investments', 'acquirements', 'status_operating', 'status_closed',\n                        'status_acquired', 'status_ipo', 'category_services','category_technology',\n                        'category_business', 'category_manufacturing', 'category_other']].corr().abs()\n\nmask = np.triu(np.ones_like(mat, dtype=bool))\nmat_masked = mat.mask(mask)  # Pone a NaN todo lo que aparezca como True en la máscara\n\nfig, ax = plt.subplots(figsize=(10,7)) \nsns.heatmap(mat_masked, annot=True, ax=ax)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:17:04.747525Z","iopub.execute_input":"2021-06-20T23:17:04.747807Z","iopub.status.idle":"2021-06-20T23:17:05.520444Z","shell.execute_reply.started":"2021-06-20T23:17:04.74778Z","shell.execute_reply":"2021-06-20T23:17:05.519082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como se puede observar en esta última matriz de correlación, tanto las adquisiciones como el número de inversiones se relacionan de forma inversa con los estados 'acquired' e 'ipo'. Esto nos da información acerca de la lógica incluida en el dataset, cuando una empresa adquiere a otra, la primera pasa a estar en estado 'ipo' y la segunda en 'acquired'. Las compañías en estado 'operating' o 'closed' no tienen información de adquisiciones o inversiones y, por tanto, podemos identificarlas como compañías que realizan su actividad de forma independiente, es decir, no perteneciendo a ningún grupo de inversión (o al menos que conste en este dataset).\n\nTambién podemos asumir que no se pueden inferir relaciones de \"tercer grado\" a partir del estado, es decir, no podemos identificar compañías que adquieran otras y, estas a su vez, que adquieran otras distintas. Nos hacemos a la idea de que todas las empresas 'acquired' van a depender directamente de una empresa en estado 'ipo'.","metadata":{}},{"cell_type":"code","source":"def get_acquirements_per_country(data):\n\n    acquariments_USA = 0\n    acquariments_OTHERS = 0\n\n    if data['country_code'] == 'USA':\n        acquariments_USA = data['acquirements']\n    else:\n        acquariments_OTHERS = data['acquirements']\n\n    return pd.Series([acquariments_USA, acquariments_OTHERS])","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:17:05.521971Z","iopub.execute_input":"2021-06-20T23:17:05.522291Z","iopub.status.idle":"2021-06-20T23:17:05.528721Z","shell.execute_reply.started":"2021-06-20T23:17:05.522233Z","shell.execute_reply":"2021-06-20T23:17:05.527426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"companies_status[['acquirements_USA','acquirements_OTHERS']] = companies_status.apply(get_acquirements_per_country, axis=1)\ncompanies_status.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:17:05.530592Z","iopub.execute_input":"2021-06-20T23:17:05.531054Z","iopub.status.idle":"2021-06-20T23:17:34.769096Z","shell.execute_reply.started":"2021-06-20T23:17:05.531007Z","shell.execute_reply":"2021-06-20T23:17:34.768107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.x Integramos los dos datasets\n\nExtraemos los códigos que representan a las empresas de ambos conjuntos de datos: IPODataFull.csv e ipos.csv ","metadata":{}},{"cell_type":"code","source":"ipos = pd.read_csv('/kaggle/input/startup-investments/ipos.csv')\nipos.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:17:34.772626Z","iopub.execute_input":"2021-06-20T23:17:34.773043Z","iopub.status.idle":"2021-06-20T23:17:34.808111Z","shell.execute_reply.started":"2021-06-20T23:17:34.773005Z","shell.execute_reply":"2021-06-20T23:17:34.80738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ipodata = pd.read_csv('/kaggle/input/financial-ipo-data/IPODataFull.csv', encoding='iso-8859-1')\nipodata = ipodata[['Symbol', 'Year']]","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:17:34.809759Z","iopub.execute_input":"2021-06-20T23:17:34.810185Z","iopub.status.idle":"2021-06-20T23:17:36.218934Z","shell.execute_reply.started":"2021-06-20T23:17:34.810141Z","shell.execute_reply":"2021-06-20T23:17:36.217753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En el fichero IPODataFull.csv tenemos datos del NASDAQ (National Association of Securities Dealers Automated Quotation), mercado caracterizado por comprender empresas de alta tecnología en electrónica, informática, telecomunicaciones, biotecnología, entre otras.\n\nPor otro lado, un segundo conjunto de datos, del que forma parte el archivo ipos.csv, contiene información sobre las IPOs (Initial Public Offer) a nivel mundial. Una IPO se define como la oferta inicial al público de las acciones de una empresa en su salida a bolsa. \n\nPor tanto, para unificar ambos conjuntos, vamos a restringir la información al ámbito del NASDAQ.","metadata":{}},{"cell_type":"code","source":"NASDAQ_ipos = ipos[ipos['stock_symbol'].str.contains('NASDAQ:')]\nNASDAQ_ipos['stock_symbol'] = NASDAQ_ipos['stock_symbol'].str.slice(start=7).str.strip()\nNASDAQ_ipos = NASDAQ_ipos[['object_id', 'stock_symbol']]\nNASDAQ_ipos = NASDAQ_ipos.rename(columns={\"stock_symbol\": \"Symbol\"})\nNASDAQ_ipos.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:17:36.22039Z","iopub.execute_input":"2021-06-20T23:17:36.220691Z","iopub.status.idle":"2021-06-20T23:17:36.240699Z","shell.execute_reply.started":"2021-06-20T23:17:36.220661Z","shell.execute_reply":"2021-06-20T23:17:36.239494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_data = pd.merge(ipodata, NASDAQ_ipos, how=\"inner\", on=[\"Symbol\"])\nfull_data = full_data.rename(columns={\"object_id\": \"id\"})\nfull_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:17:36.242273Z","iopub.execute_input":"2021-06-20T23:17:36.242662Z","iopub.status.idle":"2021-06-20T23:17:36.267497Z","shell.execute_reply.started":"2021-06-20T23:17:36.242627Z","shell.execute_reply":"2021-06-20T23:17:36.266282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_acquirements_per_year(data):\n    return data['acquirements'] / data['Year_Diff'] if data['Year_Diff'] > 0 else 0","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:17:36.268853Z","iopub.execute_input":"2021-06-20T23:17:36.269144Z","iopub.status.idle":"2021-06-20T23:17:36.273717Z","shell.execute_reply.started":"2021-06-20T23:17:36.269116Z","shell.execute_reply":"2021-06-20T23:17:36.272643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"companies_NASDAQ_IPO = pd.merge(companies_status, full_data, how=\"inner\", on=[\"id\"])\ncompanies_NASDAQ_IPO['Year_Diff'] = 2021 - companies_NASDAQ_IPO['Year'] \ncompanies_NASDAQ_IPO['acq_per_year'] = companies_NASDAQ_IPO.apply(get_acquirements_per_year, axis=1)\ncompanies_NASDAQ_IPO.to_csv('companies_NASDAQ_IPO.csv')\ncompanies_NASDAQ_IPO.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:17:36.275054Z","iopub.execute_input":"2021-06-20T23:17:36.275552Z","iopub.status.idle":"2021-06-20T23:17:36.411066Z","shell.execute_reply.started":"2021-06-20T23:17:36.275515Z","shell.execute_reply":"2021-06-20T23:17:36.410362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  1.x Oficinas\nA continuación se va a cargar y preprocesar el dataset offices. Este dataset cuenta con información geográfica de las oficinas que tiene cada empresa, (ciudad, región, pais, código postal, latitud, longitud, etc.). Además, también contiene alguna información relacionada con las oficinas, como el número de empleados o la fecha en la que se inaguraron. ","metadata":{}},{"cell_type":"code","source":"oficinas = pd.read_csv('../input/startup-investments/offices.csv',index_col='id')\noficinas","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:25:22.068286Z","iopub.execute_input":"2021-06-20T23:25:22.068669Z","iopub.status.idle":"2021-06-20T23:25:22.651855Z","shell.execute_reply.started":"2021-06-20T23:25:22.068635Z","shell.execute_reply":"2021-06-20T23:25:22.650709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imprimimos alguna informción de este dataset\nprint(\"Número de empresas con oficinas: \", \n    len(oficinas[\"object_id\"].unique()))\n\nprint(\"Número total de oficinas: \", \n    len(oficinas[\"object_id\"].dropna()))\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:25:50.151917Z","iopub.execute_input":"2021-06-20T23:25:50.152331Z","iopub.status.idle":"2021-06-20T23:25:50.194083Z","shell.execute_reply.started":"2021-06-20T23:25:50.152296Z","shell.execute_reply":"2021-06-20T23:25:50.193028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# created_at y updated_at no contienen datos\n# eliminamos estas columnas\noficinas.drop(columns=['created_at', 'updated_at'], inplace=True)\noficinas.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:26:03.40437Z","iopub.execute_input":"2021-06-20T23:26:03.404764Z","iopub.status.idle":"2021-06-20T23:26:03.439638Z","shell.execute_reply.started":"2021-06-20T23:26:03.404727Z","shell.execute_reply":"2021-06-20T23:26:03.438874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cambiamos nan en description\n# Evitamos errores a la hora de graficar\noficinas['description'].fillna('None', inplace=True)\noficinas['city'].fillna('None', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:26:41.215736Z","iopub.execute_input":"2021-06-20T23:26:41.216463Z","iopub.status.idle":"2021-06-20T23:26:41.246593Z","shell.execute_reply.started":"2021-06-20T23:26:41.216392Z","shell.execute_reply":"2021-06-20T23:26:41.245507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combinamos address1 y addres2\noficinas[\"address\"] = oficinas[\"address1\"].fillna('None') + \\\n     oficinas['address2'].fillna(' ')\n\noficinas.drop(columns=[\"address1\", \"address2\"], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:26:58.260571Z","iopub.execute_input":"2021-06-20T23:26:58.261091Z","iopub.status.idle":"2021-06-20T23:26:58.341175Z","shell.execute_reply.started":"2021-06-20T23:26:58.261047Z","shell.execute_reply":"2021-06-20T23:26:58.340397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cambio nombres de algunas columnas \n# Estos nombres salen en las gráficas\noficinas.rename(columns={\n    \"description\": \"Descripción\",\n    \"address\": \"Dirección\",\n    \"object_id\": \"ID\",\n    'latitude': \"Latitud\",\n    \"longitude\": \"Longitud\"\n    }, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:27:25.772415Z","iopub.execute_input":"2021-06-20T23:27:25.772798Z","iopub.status.idle":"2021-06-20T23:27:25.779679Z","shell.execute_reply.started":"2021-06-20T23:27:25.772766Z","shell.execute_reply":"2021-06-20T23:27:25.778261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Análisis exploratorio\n\nEn este apartado se realizan ciertas gráficas con las datos cargados en el apartado anterior. \nA continuación se muestra un mapa donde cada punto rojo corresponde con una oficina dentro del conjunto de datos. Seleccionando en el mapa cada punto rojo podemos obtener cierta información, como su posición geográfica exacta, el ID de la empresa a la que pertenece la oficina o una pequña decripcion. ","metadata":{}},{"cell_type":"code","source":"fig = px.scatter_mapbox(\n    oficinas,\n    lat=\"Latitud\",\n    lon=\"Longitud\",\n    zoom=3,\n    height=300,\n    hover_name=\"city\",\n    color_discrete_sequence=[\"red\"],\n    hover_data={\"Descripción\", \"Dirección\", \"ID\"}\n)\n\nfig.update_layout(mapbox_style=\"open-street-map\")\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:30:39.760712Z","iopub.execute_input":"2021-06-20T23:30:39.761156Z","iopub.status.idle":"2021-06-20T23:30:44.709418Z","shell.execute_reply.started":"2021-06-20T23:30:39.76112Z","shell.execute_reply":"2021-06-20T23:30:44.707962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A continuación graficamos alguna información sobre las compañías de las que disponemos datos. ","metadata":{}},{"cell_type":"code","source":"df = companies_NASDAQ_IPO.copy()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:35:16.914987Z","iopub.execute_input":"2021-06-20T23:35:16.915666Z","iopub.status.idle":"2021-06-20T23:35:16.921005Z","shell.execute_reply.started":"2021-06-20T23:35:16.91561Z","shell.execute_reply":"2021-06-20T23:35:16.920028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Veamos a qué sector pertenecen las empresas de las que tenemos información.","metadata":{}},{"cell_type":"code","source":"# Pie plot \naux = df['category_code'].value_counts()\naux = pd.DataFrame([aux.index, aux]).transpose()\naux.columns = ['Sector', 'Recuento']\naux['Sector'] = aux['Sector'].str.title().str.replace('_', ' ')\nfig = px.pie(aux, values='Recuento', names='Sector',\n       title='Porcentaje de empresas que pertenecen a cada sector')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:38:25.515674Z","iopub.execute_input":"2021-06-20T23:38:25.516314Z","iopub.status.idle":"2021-06-20T23:38:25.595405Z","shell.execute_reply.started":"2021-06-20T23:38:25.516274Z","shell.execute_reply":"2021-06-20T23:38:25.594635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vemos que la mayoría de empresas pertenecen al sector de la biotecnologia, seguido por los sectores de hardware y software.\n\nA continuación graficamos el número de empleados que tiene cada empresa. ","metadata":{}},{"cell_type":"code","source":"aux = df.loc[:,('normalized_name', 'employees')]\naux.rename(columns={'normalized_name': 'Nombre',\n           'employees': 'Empleados'}, inplace=True)\naux['Nombre'] = aux['Nombre'].str.title()\nfig = px.scatter(aux, x='Nombre', y='Empleados', title='Número de empleados de cada empresa')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:40:07.408228Z","iopub.execute_input":"2021-06-20T23:40:07.408611Z","iopub.status.idle":"2021-06-20T23:40:07.50838Z","shell.execute_reply.started":"2021-06-20T23:40:07.408579Z","shell.execute_reply":"2021-06-20T23:40:07.507368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vemos que la empresa que más empleados tiene, es con diferencia Google. Seguida a Google tenemos a Ebay, Facebook y Amazón. Ampliando el gráfico podemos ver que la mayoría de empresas tienen menos de 50 empleados.","metadata":{}},{"cell_type":"markdown","source":"A continuación mostramos el año de fundación de cada empresa. ","metadata":{}},{"cell_type":"code","source":"aux = df.loc[:,('normalized_name', 'Year')]\naux.rename(columns={'normalized_name': 'Nombre',\n           'Year': 'Año de fundación'}, inplace=True)\naux['Nombre'] = aux['Nombre'].str.title()\nfig = px.scatter(aux, x='Año de fundación', y='Nombre', title='Año de fundación de cada empresa')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:46:01.703784Z","iopub.execute_input":"2021-06-20T23:46:01.70419Z","iopub.status.idle":"2021-06-20T23:46:01.780783Z","shell.execute_reply.started":"2021-06-20T23:46:01.704154Z","shell.execute_reply":"2021-06-20T23:46:01.779982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En la siguiente imágen podemos ver las adquisiciones que ha realizado cada empresa. La diferencia de adquisicoines es tan grande en algunas empresas que es necesario ampliar el gráfico para poder ver los resultados claramente. ","metadata":{}},{"cell_type":"code","source":"aux = df.loc[:,('normalized_name', 'acquirements')]\naux = aux[aux['acquirements']>0]\naux.rename(columns={'normalized_name': 'Nombre',\n           'acquirements': 'Adquisiciones'}, inplace=True)\naux['Nombre'] = aux['Nombre'].str.title()\nfig = px.scatter(aux, x='Nombre', y='Adquisiciones', title='Adquisiciones de cada empresa')\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:46:24.504225Z","iopub.execute_input":"2021-06-20T23:46:24.504788Z","iopub.status.idle":"2021-06-20T23:46:24.574532Z","shell.execute_reply.started":"2021-06-20T23:46:24.50475Z","shell.execute_reply":"2021-06-20T23:46:24.573797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Clustering\nEn este apartado aplicaremos una estrategia de clustering al conjunto de datos con el objetivo de descubrir nuevas relaciones entre las columnas.\n\nPara ello emplearemos el algoritmo Kmeans el cual trabaja iterativamente para asignar a cada muestra uno de los “K” grupos basado en sus características. Son agrupados en base a la similitud de sus columnas.\n\nLos grupos se van definiendo de manera “orgánica”, es decir que se va ajustando su posición en cada iteración del proceso, hasta que converge el algoritmo. Una vez hallados los centroides debemos analizarlos para ver cuales son sus características únicas, frente a la de los otros grupos. Estos grupos son las etiquetas que genera el algoritmo.\n\nPara reducir la complejidad del problema reduciremos el número de columnas del dataset.","metadata":{}},{"cell_type":"code","source":"# Crear instancia del encoder\nlabelencoder = LabelEncoder()\n\nclustering_dataset = companies_NASDAQ_IPO.copy()\nclustering_dataset['status'] = labelencoder.fit_transform(companies_NASDAQ_IPO['status'])\n\nsns.pairplot(clustering_dataset, hue='category_name',size=4,vars=[\"status\", \"milestones\", \"employees\",\"count_investments\",\"acq_per_year\"],kind='scatter')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:17:36.412323Z","iopub.execute_input":"2021-06-20T23:17:36.412777Z","iopub.status.idle":"2021-06-20T23:17:44.747054Z","shell.execute_reply.started":"2021-06-20T23:17:36.412729Z","shell.execute_reply":"2021-06-20T23:17:44.745801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Posibles outliers","metadata":{}},{"cell_type":"code","source":"green_diamond = dict(markerfacecolor='g', marker='D')\n\nfig, axs = plt.subplots(1, 4, figsize=(10,10))\n\naxs[0].set_title('Boxplot por Edades')\naxs[0].boxplot(clustering_dataset[\"milestones\"], flierprops=green_diamond, labels=[\"milestones\"])\naxs[1].set_title('Boxplot por Edades')\naxs[1].boxplot(clustering_dataset[\"employees\"], flierprops=green_diamond, labels=[\"employees\"])\naxs[2].set_title('Boxplot por Edades')\naxs[2].boxplot(clustering_dataset[\"count_investments\"], flierprops=green_diamond, labels=[\"count_investments\"])\naxs[3].set_title('Boxplot por Edades')\naxs[3].boxplot(clustering_dataset[\"acq_per_year\"], flierprops=green_diamond, labels=[\"acq_per_year\"])\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:17:44.748502Z","iopub.execute_input":"2021-06-20T23:17:44.748845Z","iopub.status.idle":"2021-06-20T23:17:45.192017Z","shell.execute_reply.started":"2021-06-20T23:17:44.748813Z","shell.execute_reply":"2021-06-20T23:17:45.190734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para seleccionar el número de clusters empleamos la regla del codo, la cual determina que el número óptimo de clusters se encuentra en el punto donde la curva deja de mejorar notablemente:","metadata":{}},{"cell_type":"code","source":"clustering_dataset['category_name'] = labelencoder.fit_transform(companies_NASDAQ_IPO['category_name'])\n\nX = np.array(clustering_dataset[[\"status\",\"milestones\", \"employees\",\"count_investments\",\"acquirements_USA\"]])\ny = np.array(clustering_dataset['category_name'])\n\nN_clusters = range(1, 10)\nkmeans = [KMeans(n_clusters=i) for i in N_clusters]\nscore = [kmeans[i].fit(X).score(X) for i in range(len(kmeans))]\n\nplt.plot(N_clusters,score)\nplt.xlabel('Número del clusters')\nplt.ylabel('Score')\nplt.title('Curva del codo')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:17:45.19347Z","iopub.execute_input":"2021-06-20T23:17:45.193765Z","iopub.status.idle":"2021-06-20T23:17:45.759711Z","shell.execute_reply.started":"2021-06-20T23:17:45.193737Z","shell.execute_reply":"2021-06-20T23:17:45.757433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observando la curva del codo podemos determinar que el número óptimo de clusters es 3.","metadata":{}},{"cell_type":"code","source":"kmeans = KMeans(n_clusters=3).fit(X)\nlabels = kmeans.predict(X)\n\nC = kmeans.cluster_centers_\ncolors=[\"red\",\"green\",\"blue\"]\nvalues=[]\nfor row in labels:\n    values.append(colors[row])\n\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=values,s=60)\nax.scatter(C[:, 0], C[:, 1], C[:, 2], marker='.', c=colors, s=1000)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:17:45.761495Z","iopub.execute_input":"2021-06-20T23:17:45.761906Z","iopub.status.idle":"2021-06-20T23:17:46.037803Z","shell.execute_reply.started":"2021-06-20T23:17:45.76186Z","shell.execute_reply":"2021-06-20T23:17:46.036453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Incorporamos el resultado de la claserización al conjunto de datos","metadata":{}},{"cell_type":"code","source":"clustering_dataset['Class']  = labels\nclustering_dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:17:46.03953Z","iopub.execute_input":"2021-06-20T23:17:46.039844Z","iopub.status.idle":"2021-06-20T23:17:46.095841Z","shell.execute_reply.started":"2021-06-20T23:17:46.039813Z","shell.execute_reply":"2021-06-20T23:17:46.094401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Aqui podemos ver que el Algoritmo de K-Means con K=3 ha agrupado a las muestras por categoría, teniendo en cuenta las dimensiones que utilizamos: Employees, Count_investments y Acquirements\n\nA continuación haremos 3 gráficas en 2 dimensiones con las proyecciones a partir de nuestra gráfica 3D para que nos ayude a visualizar los grupos y su clasificación:","metadata":{}},{"cell_type":"code","source":"f1 = clustering_dataset[\"employees\"].values\nf2 = clustering_dataset[\"count_investments\"].values\n\nplt.scatter(f1, f2, c=values, s=70)\nplt.scatter(C[:, 0], C[:, 1], marker='.', c=colors, s=1000)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:17:46.097881Z","iopub.execute_input":"2021-06-20T23:17:46.098332Z","iopub.status.idle":"2021-06-20T23:17:46.265689Z","shell.execute_reply.started":"2021-06-20T23:17:46.098287Z","shell.execute_reply":"2021-06-20T23:17:46.264376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1 = clustering_dataset[\"employees\"].values\nf2 = clustering_dataset[\"acquirements\"].values\n\nplt.scatter(f1, f2, c=values, s=70)\nplt.scatter(C[:, 0], C[:, 1], marker='.', c=colors, s=1000)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:17:46.267106Z","iopub.execute_input":"2021-06-20T23:17:46.267433Z","iopub.status.idle":"2021-06-20T23:17:46.401993Z","shell.execute_reply.started":"2021-06-20T23:17:46.267402Z","shell.execute_reply":"2021-06-20T23:17:46.401197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1 = clustering_dataset[\"count_investments\"].values\nf2 = clustering_dataset[\"acquirements\"].values\n\nplt.scatter(f1, f2, c=values, s=70)\nplt.scatter(C[:, 0], C[:, 1], marker='.', c=colors, s=1000)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:17:46.402954Z","iopub.execute_input":"2021-06-20T23:17:46.403225Z","iopub.status.idle":"2021-06-20T23:17:46.553341Z","shell.execute_reply.started":"2021-06-20T23:17:46.403198Z","shell.execute_reply":"2021-06-20T23:17:46.551896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Rondas de financiación","metadata":{}},{"cell_type":"code","source":"funding_rounds = pd.read_csv('/kaggle/input/startup-investments/funding_rounds.csv')\nfunding_rounds = funding_rounds[['object_id', 'funding_round_type', 'raised_amount_usd','participants', 'is_first_round', 'is_last_round']]\nfunding_rounds.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:17:46.555207Z","iopub.execute_input":"2021-06-20T23:17:46.555666Z","iopub.status.idle":"2021-06-20T23:17:47.161763Z","shell.execute_reply.started":"2021-06-20T23:17:46.555616Z","shell.execute_reply":"2021-06-20T23:17:47.160577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_binary_fund_type(data):\n    type_venture = False\n    type_angel = False\n    type_series_a = False\n    type_series_b = False\n    type_series_cplus = False\n    type_other = False\n    type_private_equity = False\n    type_crowdfunding = False\n    type_post_ipo = False\n\n    if data['funding_round_type'] == 'venture':\n        type_venture = True\n    elif data['funding_round_type'] == 'angel': \n        type_angel = True\n    elif data['funding_round_type'] == 'series-a':   \n        type_series_a = True\n    elif data['funding_round_type'] == 'series-b':\n        type_series_b = True\n    elif data['funding_round_type'] == 'series-c+':\n        type_series_cplus = True\n    elif data['funding_round_type'] == 'other':\n        type_other = True\n    elif data['funding_round_type'] == 'private-equity':\n        type_private_equity = True\n    elif data['funding_round_type'] == 'crowdfunding':\n        type_crowdfunding = True\n    elif data['funding_round_type'] == 'post-ipo':\n        type_post_ipo = True\n\n    return pd.Series([type_venture,type_angel,type_series_a,type_series_b,type_series_cplus,\n    type_other,type_private_equity,type_crowdfunding,type_post_ipo])\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:17:47.163563Z","iopub.execute_input":"2021-06-20T23:17:47.164021Z","iopub.status.idle":"2021-06-20T23:17:47.174623Z","shell.execute_reply.started":"2021-06-20T23:17:47.163972Z","shell.execute_reply":"2021-06-20T23:17:47.173299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"funding_rounds[['type_venture','type_angel','type_series_a','type_series_b','type_series_cplus'\n,'type_other','type_private_equity','type_crowdfunding','type_post_ipo']] = funding_rounds.apply(get_binary_fund_type, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:17:47.175956Z","iopub.execute_input":"2021-06-20T23:17:47.176316Z","iopub.status.idle":"2021-06-20T23:18:00.119039Z","shell.execute_reply.started":"2021-06-20T23:17:47.176275Z","shell.execute_reply":"2021-06-20T23:18:00.117756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mat = funding_rounds[['raised_amount_usd','participants','type_venture','type_angel','type_series_a','type_series_b','type_series_cplus'\n,'type_other','type_private_equity','type_crowdfunding','type_post_ipo']].corr().abs()\n\nmask = np.triu(np.ones_like(mat, dtype=bool))\nmat_masked = mat.mask(mask)  # Pone a NaN todo lo que aparezca como True en la máscara\n\nfig, ax = plt.subplots(figsize=(10,7)) \nsns.heatmap(mat_masked, annot=True, ax=ax)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:18:00.120549Z","iopub.execute_input":"2021-06-20T23:18:00.120875Z","iopub.status.idle":"2021-06-20T23:18:00.804525Z","shell.execute_reply.started":"2021-06-20T23:18:00.120843Z","shell.execute_reply":"2021-06-20T23:18:00.803383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"funding_rounds_grouped = funding_rounds.groupby('object_id')['raised_amount_usd'].sum().reset_index(name='raised_amount_usd_total')\nfunding_rounds_grouped.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:18:00.80588Z","iopub.execute_input":"2021-06-20T23:18:00.80618Z","iopub.status.idle":"2021-06-20T23:18:00.86852Z","shell.execute_reply.started":"2021-06-20T23:18:00.806149Z","shell.execute_reply":"2021-06-20T23:18:00.867461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"funding_rounds_grouped = pd.merge(funding_rounds, funding_rounds_grouped, how=\"left\", on=[\"object_id\"])\nfunding_rounds_grouped['percentage'] = funding_rounds_grouped['raised_amount_usd'] / funding_rounds_grouped['raised_amount_usd_total']\nfunding_rounds_grouped.drop(columns=['raised_amount_usd_total'], inplace=True)\nfunding_rounds_grouped.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T23:18:00.870012Z","iopub.execute_input":"2021-06-20T23:18:00.870377Z","iopub.status.idle":"2021-06-20T23:18:00.929608Z","shell.execute_reply.started":"2021-06-20T23:18:00.870347Z","shell.execute_reply":"2021-06-20T23:18:00.928489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 - Predicción del Rendimiento Futuro ","metadata":{}},{"cell_type":"markdown","source":"Importamos las librerías necesarias para esta parte:","metadata":{}},{"cell_type":"code","source":"from datetime import date\nfrom dateutil.relativedelta import relativedelta\nimport re","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cargamos los datos\ndatos = pd.read_csv('/kaggle/input/financial-ipo-data/IPODataFull.csv')\ndatos.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datos.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tenemos un dataset con 3762 empresas y 1664 columnas - muchas de ellas representan datos históricos (como precio de inicio, cierre, volumen...) que emplearemos en la exploración de datos.","metadata":{}},{"cell_type":"markdown","source":"### Preproceso\n\nComenzamos comprobando si tenemos alguna empresa duplicada:","metadata":{}},{"cell_type":"code","source":"datos['Symbol'].duplicated().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vemos que hay 63 duplicidades (la empresa `MITT` está duplicada, por lo que eliminamos sus copias).","metadata":{}},{"cell_type":"code","source":"datos.drop_duplicates(subset=['Symbol'], inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Listamos las columnas, el número de valores únicos para estas y los valores nulos:","metadata":{}},{"cell_type":"code","source":"datos.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datos.nunique(axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datos.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Podemos observar como hay una gran cantidad de valores vacíos en la matriz de datos. En lugar de eliminarlos todos, iremos desechando aquellos de los que podamos prescindir sin influenciar en el rendimiento del predictor.","metadata":{}},{"cell_type":"markdown","source":"Trasponemos la matriz para trabajar sobre las columnas como índice, lo que nos facilitará el preprocesamiento en algunos casos:","metadata":{}},{"cell_type":"code","source":"datos_t = datos.transpose().copy()\ndatos_t.reset_index(inplace = True)\ndatos_t.rename(columns={'index':'variable'}, inplace = True)\ndatos_t.head(3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Combinamos los campos de fecha para obtener una columna que nos permita operar sobre campos temporales:","metadata":{}},{"cell_type":"code","source":"mask = datos_t[datos_t['variable'].isin(['Year', 'Month', 'Day'])]\n# Añadimos un DF auxiliar\nd = {'variable':['date']}\ndf_dates = pd.DataFrame(d)\n# Iteramos para añadir las fechas\nfor i in mask.columns[1:]:\n    year = mask.loc[4,i]\n    month = mask.loc[5, i]\n    day = mask.loc[6, i]\n    df_dates[i] = pd.to_datetime(str(year)+'-'+str(month)+'-'+str(day))\n    \n# Juntamos los DFs\ndatos_t = datos_t.append(df_dates, ignore_index = True)\ndatos_t","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vamos a añadir el precio de la acción para cada compañía 3 meses después de su primer año en bolsa. Con esto podremos obtener la variación porcentual del valor de su acción, que servirá como variable objetivo a la hora de realizar las predicciones:","metadata":{}},{"cell_type":"code","source":"# DF auxiliar para guardar los valores\n##### Kaggle no soporta la librería ####\n\"\"\"\nd = {'variable':['stockprice3m']}\nstock3m = pd.DataFrame(d)\n\nfor i in datos_t.columns[1:]:\n    try:\n        \n        date = datos_t.loc[1663, i]\n        date3t = date + pd.DateOffset(months=15)\n        # Objeto Ticker de yfinance para obtener cotizaciones históricas\n        ticker = yf.Ticker(i)\n        df_temp = ticker.history(start = date, end = date3t)\n        # La última fecha disponible para el precio \n        stockprice = df_temp.loc[df_temp.index.max(),'Close']\n        stock3m[i] = stockprice\n# Si la librería no es capaz de encontrar los valores, devuelve un NaN\n    except: \n        stock3m[i] = np.nan\n        \n# Unimos los DFs\ndatos_t = datos_t.append(stock3m)\n# Guardamos los datos \ndatos_t.to_csv('datostprecio3m.csv', sep=';')\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datos_t = pd.read_csv('../input/yfprice/datostprecio3m.csv', sep = ';', index_col = 0)\ndatos_t.reset_index(drop = True, inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comprobamos cuántos precios finales hemos sido capaces de obtener","metadata":{}},{"cell_type":"code","source":"datos_t[datos_t['variable']=='stockprice3m'].count().sum(axis = 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Eliminamos las que no tengan precio en el periodo final","metadata":{}},{"cell_type":"code","source":"drop = []\nfor i in datos_t.columns[1:]:\n    if datos_t[datos_t['variable']=='stockprice3m'][i].isnull().values.any():\n        drop.append(i)\n\nlen(drop)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datos_t.drop(drop, axis = 1, inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculamos la variación de los 3 meses siguientes tras 1 año después de la IPO. Dado que algunas compañías no tienen precios de los últimos días, consideramos como máximo el día 255 para considerarlas como válidas.","metadata":{}},{"cell_type":"code","source":"df_temp = pd.DataFrame({'variable':['variation']})\ndatos_t.set_index('variable', inplace = True)\nfor i in datos_t.columns:\n    for j in ['closeDay261', 'closeDay260', 'closeDay259', 'closeDay258', 'closeDay257', 'closeDay256', 'closeDay255']:\n        b = False\n        if datos_t.loc[j, i] != np.nan and b == False:\n            df_temp[i] = (float(datos_t.loc['stockprice3m', i]) - float(datos_t.loc[j, i])) / float(datos_t.loc[j, i])\n            b = True\n            \n# Contamos los que no están vacíos            \ndf_temp.count().sum(axis = 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unimos los datos","metadata":{}},{"cell_type":"code","source":"datos_t.reset_index(inplace = True)\ndatos_t = datos_t.append(df_temp)\ndatos_t.reset_index(drop = True, inplace = True)\ndatos_t","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Desechamos los que tengan NaN en la variación","metadata":{}},{"cell_type":"code","source":"drop = []\nfor i in datos_t.columns[1:]:\n    if datos_t[datos_t['variable']=='stockprice3m'][i].isnull().values.any():\n        drop.append(i)\n\nlen(drop)\n\ndatos_t.drop(drop, axis = 1, inplace = True)\ndatos_t.set_index('variable', inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Volvemos a trasponer los datos para tratar las columnas como variables y las filas como empresas:\nvariacion = datos_t.transpose()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Llegados a este punto, vamos a asegurar la calidad de los datos eliminando las empresas que no cumplan estos parámetros:\n\n1. Que tengan datos de empleados\n2. Que tengan datos de capitalización de mercado\n3. Que presenten ingresos y beneficios netos\n4. Que cuenten con año de fundación","metadata":{}},{"cell_type":"code","source":"# En ocasiones, aparece '-' como dato de empleados\nvariacion['employees'] = variacion['employees'].replace('-', np.nan)\nvariacion['employees'] = variacion['employees'].replace('0', np.nan)\nvariacion.dropna(subset=['employees'], inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compañias sin capitalización total de mercado\nvariacion['MarketCap'] = variacion['MarketCap'].replace('0.0', np.nan)\nvariacion.dropna(subset=['MarketCap'], inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Companías que no presentan ingresos, beneficios y año de fundación\nvariacion.dropna(subset=['Revenue'], inplace = True)\nvariacion.dropna(subset=['netIncome'], inplace = True)\nvariacion.dropna(subset=['YearFounded'], inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Los valores monetarios están representados en formato String, con una B o una M en ocasiones para indicar la magnitud de los ingresos o capitalizaciones. Para solventar esto, creamos una función auxiliar que formatee correctamente los datos","metadata":{}},{"cell_type":"code","source":"# Diccionario para formatear los números\ndval = {'M': 1000000, 'B': 1000000000}\n\n# Función auxiliar para formatear\ndef format_currency(x):\n    nodollar = x.replace('$', '')\n    dollar = re.findall(r'[-,0-9,\\.,\\,]+', nodollar)\n    magnitude = re.split(r'[-,0-9,\\.,\\,]+', nodollar)\n    dollar = dollar[0].replace(',','')\n    if magnitude[-1] in ['B', 'M']:\n        return float(dollar) * dval[magnitude[-1]]\n    else:\n        return float(dollar)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"variacion['Revenue'] = variacion['Revenue'].apply(format_currency)\nvariacion['netIncome'] = variacion['netIncome'].apply(format_currency)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Guardamos los datos filtrados para emplearlos posteriormente","metadata":{}},{"cell_type":"code","source":"variacion.to_csv('empfiltradas.csv', sep=';')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Eliminamos en este punto los datos de las operaciones intra diarias, ya que no los emplearemos en la predicción","metadata":{}},{"cell_type":"code","source":"dropcol = []\nfor i in range(262):\n    dropcol += ['openDay'+str(i), 'highDay'+str(i), 'volumeDay'+str(i), 'lowDay'+str(i)]\n    \nvariacion.drop(dropcol, axis = 'columns', inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vamos a crear columnas que representen la variación mensual del precio de la acción. Como las acciones sólo cotizan en días laborables, para un año tenemos 262 valores en lugar de 365. Por ello, definimos los meses como periodos de 22 días.\n\nPara el último mes, emplearemos el último dato de cierre como precio final, ya que hay algunas empresas que cuentan con NaN en los últimos días del dataset. ","metadata":{}},{"cell_type":"code","source":"closecols = ['closeDay'+str(i) for i in range(262)]\nvariacion[closecols] = variacion[closecols].astype(float)\nvariacion[closecols].isna().sum().to_dict()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creamos la variación de los 11 primeros meses:","metadata":{}},{"cell_type":"code","source":"for i in range(11):\n    if i < 11:\n        variacion['after'+str(i+1)+'m'] = (variacion['closeDay'+str((i+1)*22)] - variacion['closeDay'+str((i)*22)]) / variacion['closeDay'+str((i)*22)]\n    else: \n        variacion['after'+str(i+1)+'m'] = (variacion['closeDay'+str((i+1)*19)] - variacion['closeDay'+str((i)*22)]) / variacion['closeDay'+str((i)*22)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creamos la variación del último mes del stock. Como máximo, consideramos el día 256 de tradeo de la acción:","metadata":{}},{"cell_type":"code","source":"for inc in variacion.index:\n    pause = False\n    for lastday in ['closeDay261', 'closeDay260', 'closeDay259', 'closeDay258', 'closeDay257', 'closeDay256', 'closeDay255']:\n        if pause == False and variacion.loc[inc, lastday] != np.nan:\n            variacion.loc[inc, 'after12m'] = (variacion.loc[inc, lastday] - variacion.loc[inc, 'closeDay242']) / variacion.loc[inc, 'closeDay242']\n            \nvariacion[variacion.columns[-12:]].isna().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No obtenemos ningún NaN, por lo que continuamos.\n\nProcedemos a eliminar las columnas de cierre, ya que ya nos las vamos a emplear:","metadata":{}},{"cell_type":"code","source":"df = variacion.drop(closecols, axis = 'columns')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seleccionamos las columnas finales que emplearemos en la predicción. Estas son:\n\n* `DaysBetterThenSP`: Días en los que el stock superó al índice S&P500 en rendimiento.\n* `daysProfit`: Días en los que el stock mostró un rendimiento positivo.\n* `Year`: Año de salida a bolsa.\n* `Month`: Mes de salida a bolsa.\n* `Day`: Día de salida a bolsa.\n* `dayOfWeek`: Día de la semana de salida a bolsa (del 1 al 5).\n* `LastSale`: Número de acciones vendidas en la IPO.\n* `MarketCap`: Capitalización bursátil tras la IPO.\n* `Sector`: Sector de actividad.\n* `Revenue`: Ingresos en el primer año tras la IPO.\n* `netIncome`: Beneficio neto en el primer año tras la IPO.\n* `employees`: Número de empleados de la empresa.\n* `USACompany`: Si la empresa es de EEUU (Yes / No).\n* `YearFounded`: Año en el que se fundó la empresa.\n* `Profitable`: Si la IPO consiguió beneficios sobre el precio de salida.\n* `Homerun`: Si el stock mostró un rendimiento excepcional en su primer año. \n* `Safe`: Si la IPO fue de tipo Safe.\n\nAdemás, se incluyen las variables creadas por nosotros, representando las variaciones totales y mensuales.\n","metadata":{}},{"cell_type":"code","source":"selectedcols = ['DaysBetterThanSP', 'daysProfit', 'Year', 'Month', 'Day', 'dayOfWeek', 'LastSale', \n'MarketCap', 'Sector', 'ipoDate', 'Revenue', 'USACompany', 'netIncome', 'employees', 'YearFounded', 'Profitable',\n'Safe', 'HomeRun', 'variation', 'after1m',  'after2m',\n'after3m', 'after4m', 'after5m', 'after6m', \n'after7m', 'after8m', 'after9m', 'after10m', \n'after11m', 'after12m']\n\ndatos_final = df[selectedcols]\ndatos_final.dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Casteamos las columnas numéricas a float","metadata":{}},{"cell_type":"code","source":"# We convert all those variables to float\ntocast = ['DaysBetterThanSP', 'daysProfit', 'Year', 'Month', 'Day', 'LastSale', \n'MarketCap', 'Revenue', 'netIncome', 'employees', 'YearFounded', 'Profitable',\n'Safe', 'HomeRun', 'variation', 'after1m',  'after2m',\n'after3m', 'after4m', 'after5m', 'after6m', \n'after7m', 'after8m', 'after9m', 'after10m', \n'after11m', 'after12m']\ndatos_final[tocast] = datos_final[tocast].astype(float)\ndatos_final.dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculamos la diferencia en años entre la fundación de la empresa y su salida a bolsa","metadata":{}},{"cell_type":"code","source":"datos_final['diffpublic'] = datos_final['Year'] - datos_final['YearFounded'] \ndatos_final.drop(['YearFounded', 'ipoDate'], inplace = True, axis = 'columns')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Agrupamos los sectores según el mismo criterio empleado en el análisis de mercado:","metadata":{}},{"cell_type":"code","source":"dsector = {np.nan: 'other', 'Transportation':'services', 'Energy':'services', 'Public Utilities':'services', \n            'Consumer Non-Durables':'other', 'Miscellaneous':'other', 'Basic Industries':'manufacturing',\n            'Finance': 'business', 'Consumer Durables':'manufacturing', 'Health Care':'services', 'Consumer Services':'services', 'Consumer Non-Durables':'other',\n            'Technology':'technology', 'Capital Goods':'manufacturing'}\ndatos_final['Sector'].replace(dsector, inplace = True)\ndatos_final['Sector'].unique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para clasificar las empresas, vamos a observar la distribución de su rendimiento en el primer trimestre después de su primer año siendo públicas. ","metadata":{}},{"cell_type":"code","source":"datos_final['variation'].describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vemos que la mediana está en -10% de rentabilidad. ","metadata":{}},{"cell_type":"code","source":"datos_final[datos_final['variation'] >0]['variation'].describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este caso, la mediana de las positivas está en 20% tras 3 meses, por lo que las que tengan una rentabilidad superior a esta serán clasificadas como con rendimiento bueno. ","metadata":{}},{"cell_type":"code","source":"datos_final['performance'] = 'negative'\nmask = datos_final[datos_final['variation'] > 0] \ndatos_final.loc[mask.index, 'performance'] = 'positive'\nmask = datos_final[datos_final['variation'] > 0.2]\ndatos_final.loc[mask.index, 'performance'] = 'good'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datos_final['performance'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Observamos que hay 301 empresas con un buen rendimiento, 283 con rendimiento positivo y 1043 con un rendimiento malo. ","metadata":{}},{"cell_type":"markdown","source":"Guardamos los datos:","metadata":{}},{"cell_type":"code","source":"datos_final.to_csv('datapred.csv', sep=';')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 - Predicción: Clasificación y regresión","metadata":{}},{"cell_type":"markdown","source":"Librerías para esta parte:","metadata":{}},{"cell_type":"code","source":"# Clasificación \nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\n# Selección\nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Visualización\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = datos_final.copy()\ndata.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Funciñon auxiliar para comparar resultados\ndef resultados_class(modelo, y, x):\n    pred = modelo.predict(x)\n    acc = accuracy_score(y, pred)\n    f1 =  f1_score(y, pred, average='macro')\n\n    results = {\n        'Precision': acc,\n        'F1-Score': f1,\n    }\n\n    # Classification Report\n    print(classification_report(y, pred))\n\n    # Matrices de confusión\n    cnftot = plot_confusion_matrix(modelo, x, y)\n    cnftot.ax_.set_title('Matriz de Confusión Absoluta')\n    cnfacc = plot_confusion_matrix(modelo, x, y, normalize='pred')\n    cnfacc.ax_.set_title('Matriz de Confusión (%True)')\n\n    return results\n\n# Función auxiliar para transformar los datos categoricos en One  Hot\n def get_add_onehot(column, data):\n    one = OneHotEncoder(drop = 'first')\n    onehot = one.fit_transform(data[column].values.reshape(-1,1)).toarray()\n    dfOneHot = pd.DataFrame(onehot, columns = [str(column)+str(int(i)) for i in range(onehot.shape[1])])\n    data = pd.concat([data, dfOneHot], axis=1)\n    data.drop([column], axis=1, inplace = True) \n\n    return data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reseteamos el índice y transformamos las variables categóricas:","metadata":{}},{"cell_type":"code","source":"data.reset_index(inplace = True)\nfor i in ['dayOfWeek', 'Sector']:\n    data = get_add_onehot(i, data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cambiamos también la variable `USACompany` a tipo binaria:","metadata":{}},{"cell_type":"code","source":"data['USACompany'] = label_binarize(data['USACompany'], classes = ['No', 'Yes'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tras comprobar los rendimientos de los datos, hemos visto que hay empresas cuyos datos de precios en el dataset original no coinciden con los reales, lo que les lleva a formar outliers en el rendimiento. Por ello, vamos a eliminar las empresas que tienen un rendimiento superior al 1,000%, puesto que las consideraremos outliers.","metadata":{}},{"cell_type":"code","source":"mask = data[data['variation'] > 10]\ndata.drop(mask.index, axis = 'rows', inplace = True)\ndata.reset_index(drop = True, inplace= True)\ndata.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creamos la lista de predictores y dividimos el dataset entre train y test:","metadata":{}},{"cell_type":"code","source":"predictors = data.drop(['index', 'variation', 'performance'], axis = 1).columns.tolist()\nseed = 0\n\ntrain, test = train_test_split(data, test_size = 0.2, random_state = seed)\n\nx_train = train[predictors]\ny_train = train['performance']\nx_test = test[predictors]\ny_test = test['performance']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### NB Gausiano\n\nComenzamos implementando esta sencilla variación del NB que nos servirá como baseline:","metadata":{}},{"cell_type":"code","source":"%%time\nmodelo = GaussianNB()\nmodelo.fit(x_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultados_class(modelo, y_train, x_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El rendimiento no es muy bueno, por lo que probamos a transformar los datos:","metadata":{}},{"cell_type":"code","source":"modelo2 = GaussianNB()\nscaler = MinMaxScaler()\naux = pd.DataFrame(scaler.fit_transform(x_train), columns = x_train.columns)\nmodelo2.fit(aux, y_train)\nresultados_class(modelo2, y_train, aux)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No conseguimos mejorar el rendimiento, por lo que pasamos al siguiente modelo","metadata":{}},{"cell_type":"markdown","source":"### KNN\n\nImplantamos un modelo KNN de clasificación. Para buscar los mejores hiperparámetros, realizaremos un `GridSearch`.","metadata":{}},{"cell_type":"code","source":"# Número de vecinos:\nk = np.arange(1, 30)\n\n# Número de observaciones a considerar:\nleafs = np.array([20, 30, 40, 50])\n\n# Tipo de distancia:\np = np.array([1, 2])\n\n# Creamos el diccionario que empleará GridSearchCV:\nhparams = dict(n_neighbors = k, \n               leaf_size = leafs, \n               p = p)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Declaramos la búsqueda de hiperparámetros con CV = 5:\nsearch = GridSearchCV(knn, hparams, cv = 5, verbose = 1, scoring = 'f1_weighted')\n# Buscamos los parámetros del modelo:\nbest_knn = search.fit(x_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comprobamos los parámetros ideales:\nprint('Leaf_size:', best_knn.best_estimator_.get_params()['leaf_size'])\nprint('P:', best_knn.best_estimator_.get_params()['p'])\nprint('N_neighbors:', best_knn.best_estimator_.get_params()['n_neighbors'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Declaramos el modelo\nknn = KNeighborsClassifier(n_neighbors = 14, leaf_size = 20, p = 2)\nknn.fit(x_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lo evaluamos frente a los datos de entrenamiento:\nresultados_class(knn, y_train, x_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultados_knn = resultados_class(knn, y_test, x_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SVC\n\nDe manera similar, proponemos unos hiperparámetros para encontrar el mejor modelo posible:","metadata":{}},{"cell_type":"code","source":"hparams = {\n    'kernel': ['linear', 'rbf', 'sigmoid'],\n    'class_weight': [None, 'balanced']\n}\nsvc = SVC()\n\nsearch = GridSearchCV(svc, hparams, cv = 10, scoring = 'f1_weighted')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Declaramos el escalador\nescalador = StandardScaler()\n\n# Escalamos las variables predictoras\nx_train_e = escalador.fit_transform(x_train)\nx_test_e = escalador.fit_transform(x_test)\nbest_svc = search.fit(x_train_e, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_svc.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc = SVC(kernel = 'sigmoid', probability=True)\nsvc.fit(x_train_e, y_train)\nresultados_class(svc, y_train, x_train_e)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observamos unos resultados peores que los ofrecidos por KNN en el conjunto train. Probamos para test:","metadata":{}},{"cell_type":"code","source":"resultados_svc = resultados_class(svc, y_test, x_test_e)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RandomForest\n\nProponemos ahora una agrupación de DTs para clasificar el tipo de rendimiento de cada stock","metadata":{}},{"cell_type":"code","source":"# Instanciamos el modelo\n\nrforest = RandomForestClassifier()\n\n# Generamos el diccionario para la búsqueda de los hiperparámetros óptimos:\n\nhparams = {\n    'min_samples_split': np.arange(2, 40, 1), # Número mínimo de observaciones para dividir\n    'max_features': ['sqrt', 'log2', 5] # Máximas columnas a considerar\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"search = GridSearchCV(rforest, hparams, cv = 5, scoring = 'f1_weighted')\nsearch.fit(x_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(search.best_score_)\nsearch.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Instanciamos con los parámetros óptimos y probamos en el conjunto train:","metadata":{}},{"cell_type":"code","source":"rforest = RandomForestClassifier(max_features='sqrt', min_samples_split=3)\nrforest.fit(x_train, y_train)\nresultados_class(rforest, y_train, x_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este caso obtenemos una clasificación perfecta, lo que puede significar que estamos cometiendo overfitting. Probamos en el conjunto test para asegurarnos:","metadata":{}},{"cell_type":"code","source":"resultado_rforest = resultados_class(rforest, y_test, x_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### NLPClassifier\n\nVamos a crear una pequeña red neuronal como clasificador utilizando la clase `NLPClassifier` de sklearn. Utilizaremos los datos transformados, como en el SVC. ","metadata":{}},{"cell_type":"code","source":"clf = MLPClassifier(hidden_layer_sizes = (128, 64, 24),random_state=1, max_iter=1000).fit(x_train_e, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultados_class(clf, y_train, x_train_e)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este caso también obtenemos una clasificación perfecta. Probamos en el conjunto test:","metadata":{}},{"cell_type":"code","source":"nn = resultados_class(clf, y_test, x_test_e)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### AdaBoost\n\nPasamos a probar otro emsemble, el Adaboost:","metadata":{}},{"cell_type":"code","source":"clf = AdaBoostClassifier(n_estimators=100, random_state=0)\nclf.fit(x_train_e, y_train)\nresultados_class(clf, y_train, x_train_e)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En principio, obtenemos resultados prometedores, sin síntomas ya de overfitting.","metadata":{}},{"cell_type":"code","source":"ada = resultados_class(clf, y_test, x_test_e)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Comparación de Resultados\n\nObservamos como el RandomForest es el modelo que mejor predice el conjunto test:","metadata":{}},{"cell_type":"code","source":"ddatos = {'KNN': resultados_knn.values(),\n        'SVC': resultados_svc.values(),\n         'RandomForest': resultado_rforest.values(),\n         'NLP':nn.values(),\n         'AdaBoost':ada.values()}\n\nresultados = pd.DataFrame.from_dict(ddatos,\n                                    orient = 'index',\n                                    columns = resultados_knn.keys())  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Clasificación\n\nComo el RandomForest ha sido el algoritmo que mejor nos ha funcionado en el conjunto test, lo vamos a emplear para predecir la clase de las empresas. Obtendremos, además, las probabilidades de pertenencia a cada clase - esto nos permitirá realizar un modelo individual para las más prometedoras:","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(rforest.predict_proba(escalador.fit_transform(data[predictors])), columns = ['p_good', 'p_negative', 'p_positive'])\ndata['pred'] = rforest.predict(escalador.fit_transform(data[predictors]))\ndata = pd.concat([data, df], axis = 'columns')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.sort_values('p_good', ascending = False).head()","metadata":{},"execution_count":null,"outputs":[]}]}