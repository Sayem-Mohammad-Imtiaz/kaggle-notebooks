{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"test_path = '/kaggle/input/100-bird-species/test'\ntrain_path = '/kaggle/input/100-bird-species/train'\nos.listdir(test_path)\nos.listdir(train_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(os.listdir(train_path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count images for each species\ndef cntSamples(directory):\n    specs = []\n    for root, dirs, files in os.walk(directory, topdown=True):\n        dirs.sort()\n        for name in dirs:\n            if name not in specs:\n                specs.append(name)\n\n    # file counts for each species \n    nums = []\n    for b in specs:\n        path = os.path.join(directory,b)\n        num_files = len(os.listdir(path))\n        nums.append(num_files)\n \n    # Create Dictionary\n    adict = {specs[i]:nums[i] for i in range(len(specs))}\n    return adict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testDict =  cntSamples(test_path)\ntrainDict = cntSamples(train_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.display import display, HTML\ntrain_tbl = pd.DataFrame.from_dict(trainDict, \n                orient='index', dtype=None, columns=['Images'])\n\nnum_classes = len(trainDict)\nlabel_index = list(range(num_classes))\nprint(num_classes)\ntrain_tbl.insert(0,'Label Index',label_index, True)\n\ndisplay(HTML(train_tbl.to_html()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nimage_gen = ImageDataGenerator(rotation_range=20, # rotate the image 20 degrees\n                               width_shift_range=0.10, # Shift the pic width by a max of 5%\n                               height_shift_range=0.10, # Shift the pic height by a max of 5%\n                               rescale=1/255, # Rescale the image by normalzing it.\n                               shear_range=0.1, # Shear means cutting away part of the image (max 10%)\n                               zoom_range=0.1, # Zoom in by 10% max\n                               horizontal_flip=True, # Allo horizontal flipping\n                               fill_mode='nearest' # Fill in missing pixels with the nearest filled value\n                              )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_gen.flow_from_directory(train_path)\nimage_gen.flow_from_directory(test_path)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Building model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D\nimage_shape = (224, 224, 3)\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=image_shape, activation='relu',))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(filters=128, kernel_size=(3,3),input_shape=image_shape, activation='relu',))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(filters=256, kernel_size=(3,3),input_shape=image_shape, activation='relu',))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(256))\nmodel.add(Activation('relu'))\n\n# Dropouts help reduce overfitting by randomly turning neurons off during training.\n# Here we say randomly turn off 50% of neurons.\nmodel.add(Dropout(0.5))\nmodel.add(Dense(200))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss',patience=4)\n\nbatch_size = 30\ntrain_image_gen = image_gen.flow_from_directory(train_path,\n                                               target_size=image_shape[:2],\n                                                color_mode='rgb',\n                                               batch_size=batch_size,\n                                              class_mode='categorical')\ntest_image_gen = image_gen.flow_from_directory(test_path,\n                                               target_size=image_shape[:2],\n                                               color_mode='rgb',\n                                               batch_size=batch_size,\n                                               class_mode='categorical',shuffle=False)\n\n#train_image_gen.class_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_image_gen.class_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_image_gen.class_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nresults = model.fit_generator(train_image_gen,epochs=100,\n                              validation_data=test_image_gen,\n                             callbacks=[early_stop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nmodel.save('100_bird_species.h5')\nlosses = pd.DataFrame(model.history.history)\nlosses[['loss','val_loss']].plot()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"losses[['accuracy','val_accuracy']].plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate_generator(test_image_gen)\nfrom tensorflow.keras.preprocessing import image\npred_probabilities = model.predict_generator(test_image_gen)\npred_probabilities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = pred_probabilities > 0.5\npredictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(test_image_gen.classes,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(test_image_gen.classes,predictions)\n#os.listdir(test_path+'\\\\def_front')[0]\neastern_rosella_image_cell = test_path+'\\\\EASTERN ROSELLA\\1.jpg\ncasting_image_cell = test_path+'\\YELLOW HEADED BLACKBIRD\\1.jpgjpeg'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eastern_rosella_image_test = image.load_img(eastern_rosella_image_cell,target_size=image_shape)\ncasting_image_cell_test = image.load_img(casting_image_cell,target_size=image_shape)\neastern_rosella_image_test = image.img_to_array(eastern_rosella_image_test)\ncasting_image_cell_test = image.img_to_array(casting_image_cell_test)\n\neastern_rosella_image_test = np.expand_dims(eastern_rosella_image_test, axis=0)\ncasting_image_cell_test = np.expand_dims(casting_image_cell_test, axis=0)\ncasting_image_cell_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict(eastern_rosella_image_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict(casting_image_cell_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}