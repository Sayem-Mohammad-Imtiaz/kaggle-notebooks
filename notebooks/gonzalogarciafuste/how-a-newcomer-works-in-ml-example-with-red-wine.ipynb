{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### Red wine quality classification\nthis is my first notebook; of course there will be many mistakes and things to improve, but I hope that with your kind comments I can improve it :)\n\nThese datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are much more normal wines than excellent or poor ones).\n\nfirst we import all the necessary libraries (I'm sure I import more than I need; this is my first point to improve :) )\n"},{"metadata":{"trusted":true},"cell_type":"code","source":" import os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n# for cross validation\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\n# for metrics\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.metrics import classification_report\n# plot\nfrom matplotlib import rcParams\n%config InlineBackend.figure_format = 'retina'\nsns.set_style(\"white\")\nrcParams['figure.figsize'] = 6,4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import data\ndataset_url = '../input/winequalityredcsv/winequality-red.csv'\ndata = pd.read_csv(dataset_url,sep=\";\")\ndata.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All of the features are numeric, which is convenient. However, they have some very different scales, so let's make a mental note to standardize the data later ¡¡\n\nAs we said before there are much more normal wines than excellent or poor ones"},{"metadata":{"trusted":true},"cell_type":"code","source":"#I count the values and order them\nquality_count = data.quality.value_counts().sort_index(ascending=False)\n\n#I make a plot with seaborn, and i create a list of colours\ncolores = [\"#F6D772\",\"#EE8B73\",\"#EE8B73\",\"#EE8B73\",\"#EE8B73\",\"#F6D772\"]\nsns.barplot(x=quality_count.index,y=quality_count.values,palette=colores,saturation=1)\nplt.title(\"Quantity of quality results\")\nplt.xlabel(\"Quality\")\nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will create a function that first divides the wines into good and bad depending on their rating (up to 6.5 will be bad and above that good).\n\nthen I make a labelencoder to transform them into 0 and 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"def label_wine(Y):\n    bins = (2, 6.5, 8)\n    group_names = ['bad', 'good']    \n    df = pd.cut(Y.astype(\"float\"),bins=bins,labels=group_names)\n    le = preprocessing.LabelEncoder()\n    return le.fit_transform(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I want to contrast the characteristics of the wines according to whether they are good or bad.\n\nFirst I make a copy of the dataframe, and then a new column using the old function (bad would be 0 and good would be 1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# copy dataframe and create new column\ndata_copy = data.copy()\ndata_copy[\"quality_bin\"] = label_wine(data_copy[\"quality\"])\n\n# now I filter the good ones and the bad ones, and I join them in a dataframe to make it visually more beautiful\nbad_wine = data_copy.loc[data_copy[\"quality_bin\"] == 0].mean()\ngood_wine = data_copy.loc[data_copy[\"quality_bin\"] == 1].mean()\ncomparative_wine = pd.DataFrame(data={\"bad_wine\":bad_wine,\"good_wine\":good_wine})\ncomparative_wine.style.background_gradient(cmap=\"Reds\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we see for example that the most relevant characteristics of good wines are fixed acidity, citric acid, residual sugar, sulphates and alcohol; and also a low proportion of sulphur, chlorides and volatile acidity\t\n\nwe can do the same for example if we see the correlation of the characteristics with the quality"},{"metadata":{"trusted":true},"cell_type":"code","source":"# first we use the correlation function; and then to visualize it I use style.background_gradient (in this case I use to_frame because as I have selected only one column and style it only works with dataframe)\n#I know I'm not explaining myself very well, I hope you guys forgive me!\n\ncorr = data.corr()\ncorr.iloc[:,-1].sort_values(ascending=False).to_frame().style.background_gradient(cmap='PuRd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"you can also see all the correlation of variables to see if there is multicollinearity (which there is not, there was a VIF of 2,0119)"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr.style.background_gradient(cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, let's go with the model (I could do more graphics but I prefer to go directly to the model and leave the analysis for another time)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split data into training and test sets.\nX = data.drop(\"quality\",axis=1)\nY = data.quality\n\nX_train, X_test, y_train,y_test = train_test_split(X,Y, test_size=0.2,random_state=0,stratify=Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now I use again the previous function to binarize Y at 0 and 1 (bad, good)\ny_train_label = label_wine(y_train)\ny_test_label = label_wine(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My approach will be as follows: first make a baseline with several models to see how they behave, then choose the one that works best, and then I will look for the best hyperparameters.\nLet's hope it works and nobody dies in the attempt :)\n\nI know there is a problem with the data, as it is not balanced. I will try to adjust certain parameters to see if I can improve them and change my Performance Metric (ideally find more data, Try Resampling Your Dataset, Try Generate Synthetic Samples ...)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# first I create a dictionary with several models, then (I hadn't forgotten!) I have to use standarscaler, and I use kfold to fight a bit with the Imbalanced Classes problem \n\ndef baseline(X_train,y_train_label):\n    models = {\"rid\":RidgeClassifier(),\"log\":LogisticRegression(random_state=0,                  \n    class_weight=\"balanced\"),\"rdf\":RandomForestClassifier(),\"kne\":KNeighborsClassifier(),\"svc\":SVC()}\n    scaler = preprocessing.StandardScaler().fit(X_train)\n    X_train_scaled = scaler.transform(X_train)\n    kfold = KFold(n_splits=10, random_state=100)\n    for key,model_kfold in models.items():        \n        results_kfold = cross_val_score(model_kfold, X_train_scaled, y_train_label, cv=kfold)\n        print(f\"Accuracy {key}: {results_kfold.mean()*100.0}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline(X_train,y_train_label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I should use performance measures that can give me more insight into the accuracy of the model than traditional classification accuracy:\n\n* Confusion Matrix: A breakdown of predictions into a table showing correct predictions (the diagonal) and the types of incorrect predictions made (what classes incorrect predictions were assigned).\n* Precision: A measure of a classifiers exactness.\n* Recall: A measure of a classifiers completeness\n* F1 Score (or F-score): A weighted average of precision and recall.\n\nbut for the moment I will only use accuracy to decide which model to use.\n\nI find this article very good (in general everything he writes) and it has served me well\n\nhttps://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we see that random forest had the best results, now we are going to look for the best hyperparameters ! \n\nn_estimators = [100, 300, 500]\nmax_depth = [5, 8, 15,30]\nmin_samples_split = [2, 5, 10, 15, 100]\nmin_samples_leaf = [1, 2, 5, 10,50] \n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n              min_samples_split = min_samples_split, \n             min_samples_leaf = min_samples_leaf)\n\ndef final_model_get_params(X_train,y_train_label):\n    scaler = preprocessing.StandardScaler().fit(X_train)\n    X_train_scaled = scaler.transform(X_train)\n    gridF = GridSearchCV(RandomForestClassifier(), hyperF, cv = 3, verbose = 1, \n                        n_jobs = -1)\n    bestF = gridF.fit(X_train_scaled,y_train_label)\n    print(f\"best params: {bestF.best_params_}\")\n    print(f\"best results: {bestF.best_score_}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model_get_params(X_train,y_train_label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"well we already have the best parameters, let's use them and see what happens (I corrected a couple of things: min_samples_leaf=2 and max_features=4, because I had read that with unbalanced classes it was not good to use 1 in sample leaf, and in max features to use between 30-50% of the features)\n\nplease any correction, help and improvement will be very useful and very welcome !"},{"metadata":{"trusted":true},"cell_type":"code","source":"def final_model_predict_classification(X_train,X_test,y_train_label,y_test_label):\n    scaler = preprocessing.StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    gridF = RandomForestClassifier(max_depth=15, min_samples_leaf=2, min_samples_split=2,max_features=4, n_estimators=300,random_state=0,n_jobs=1,class_weight=\"balanced\")\n    gridF.fit(X_train_scaled,y_train_label)\n    print(f\"score with train data: {gridF.score(X_train_scaled,y_train_label)}\")    \n    print(f\"score with test data: {gridF.score(X_train_scaled,y_train_label)}\")    \n    print(classification_report(y_test_label,gridF.predict(X_test_scaled)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model_predict_classification(X_train,X_test,y_train_label,y_test_label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it seems that there are problems of overfitting, the results are too good :(\nthat's why I want to use other metrics to see what's happening (classification report): we see as expected, as the classes are unbalanced, which predicts very well when the wine is bad but not so much when it's good (recall 0.53)\n\nany idea that could save this project ?? :)\n\nwell, this is my first article on kaggle; of course it is very improvable, and for future challenges I promise to do my best.\n thank you very much for your patience and comments!\npeace for all!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}