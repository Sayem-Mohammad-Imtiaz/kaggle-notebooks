{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThe purpose of this investigation to create models to produce accurate decisions based on a data set.\nThe data set that I have based my investigation on is star classification. By using the data from the star-type-classification data set, I hope to create accurate predictions on classyfing specific star types. I think it will be cool to be able to predict star types only from it's features and who knows, maybe it will be useful in the future for real life situations.\n## Hypothesis:\nI believe the best model will be a classifier. The output that I intend my predictions to be are categorical rather than numerical so I'll be implementing some code to do this, and the classifier can create categorical predictions unlike a regressor. I hope I'll be able to make some pretty accurate predictions.","metadata":{}},{"cell_type":"markdown","source":"# Setup:\nThe below code contains necessary steps for setting up our machine learning environment. Key features are described in the comments","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # dat processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.tree import DecisionTreeClassifier ,plot_tree# Our model\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-29T06:19:26.961649Z","iopub.execute_input":"2021-06-29T06:19:26.962175Z","iopub.status.idle":"2021-06-29T06:19:28.145126Z","shell.execute_reply.started":"2021-06-29T06:19:26.962088Z","shell.execute_reply":"2021-06-29T06:19:28.144111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gathering and exploring data\nThe type of data we're dealing with here is numerical and some categorical. The data includes things that describe the stars key features. \n- Temperature = Kelvin\n- L = Luminosity/Lo (Lo = Avg luminosity of sun)\n- R = Radius/Ro (Ro = Avg radius of sun)\n- A_M = Absolute magnitude\n- Color = General Color of Spectrum\n- Spectral_Class = O,B,A,F,G,K,M / SMASS\n- Type = Red Dwarf, Brown Dwarf, White Dwarf, Main Sequence , Super Giants, Hyper Giants\n\nThe aim and target is to predict the type of star from the given data. By using the temperature, luminosity, radius, etc, we'll be able to find whether it's a red dward, brown dwarf, white dward, main sequence, super giants and hyper giants.\n\nIn our data set, there is a wide variety of coulours which will make it hard to code it. So to solve this we'll use a specific code from *devchauhan1* where all the mix of colours such as yellow white, whitish, yellowish white are all turned into one colour, where in this case just white. This is the link to that code.\nhttps://www.kaggle.com/devchauhan1/star-type-classification-nasa/data","metadata":{}},{"cell_type":"code","source":"train_file_path = '../input/star-type-classification/Stars.csv'\n\n# Create a new Pandas DataFrame with our training data\nstar_train_data = pd.read_csv(train_file_path)\n\nx=[\"Blue-white\",\"Blue White\",\"yellow-white\",\"Blue white\",\"Yellowish White\",\"Blue-White\",\"White-Yellow\",\"Whitish\",\"white\"]\nfor i in x:\n    star_train_data.loc[star_train_data[\"Color\"]==i,\"Color\"]= \"White\"\nfor i in [\"yellowish\",\"Yellowish\"]:\n    star_train_data.loc[star_train_data[\"Color\"]==i,\"Color\"]=\"Yellow\"\nfor i in [\"Orange-Red\",\"Pale yellow orange\"]:\n    star_train_data.loc[star_train_data[\"Color\"]==i,\"Color\"]=\"Orange\"\n\n\n#star_train_data.columns\nstar_train_data.describe(include='all')\n#star_train_data.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:19:28.148555Z","iopub.execute_input":"2021-06-29T06:19:28.148825Z","iopub.status.idle":"2021-06-29T06:19:28.236042Z","shell.execute_reply.started":"2021-06-29T06:19:28.148798Z","shell.execute_reply":"2021-06-29T06:19:28.235105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare the data\n\nIn this data set, we fortunately have all columns filled out so there is no need to drop any values. Every feature in the data set has an important value to find out which type of star we're classyfing so there is no need to drop any of them and they all have an equal value of importance.","metadata":{}},{"cell_type":"code","source":"\nselected_columns = ['Temperature', 'L', 'R', 'A_M', 'Color', 'Spectral_Class', 'Type']\n\n\nprepared_data = star_train_data[selected_columns]\n\n\nprepared_data.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:19:28.238377Z","iopub.execute_input":"2021-06-29T06:19:28.238655Z","iopub.status.idle":"2021-06-29T06:19:28.275284Z","shell.execute_reply.started":"2021-06-29T06:19:28.238628Z","shell.execute_reply":"2021-06-29T06:19:28.274319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The only thing that we're dropping from our features (X) is the acutaly value that we're trying to predict, which in our case is 'Type' (y) which is the star type we're trying to predict.","metadata":{}},{"cell_type":"code","source":"y = prepared_data.Type\n\n\nX = prepared_data.drop('Type', axis=1)\n\ny = y.replace({0: 'Red Dwarf', 1: 'Brown Dwarf', 2: 'White Dwarf', 3: 'Main Sequence', 4: 'Super Giant', 5: 'Hyper Giant'})\n\n\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:19:28.276826Z","iopub.execute_input":"2021-06-29T06:19:28.277113Z","iopub.status.idle":"2021-06-29T06:19:28.292585Z","shell.execute_reply.started":"2021-06-29T06:19:28.277085Z","shell.execute_reply":"2021-06-29T06:19:28.291608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the get dummies feature included in the pandas import, we are able to essentially turn catagorical data into numerical which is important for us as some of the features in the data set include colour and spectral class which is categorical. By using the get dummies feature we are able to turn it into numerical data because we can't use categorical data in our features when fitting a regressor.","metadata":{}},{"cell_type":"code","source":"X = pd.get_dummies(X)\n\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:19:28.293623Z","iopub.execute_input":"2021-06-29T06:19:28.294055Z","iopub.status.idle":"2021-06-29T06:19:28.329105Z","shell.execute_reply.started":"2021-06-29T06:19:28.293994Z","shell.execute_reply":"2021-06-29T06:19:28.328217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Choosing and training models","metadata":{}},{"cell_type":"markdown","source":"Now that we have prepared our data, we have to split the training set from our validation/testing data.","metadata":{}},{"cell_type":"code","source":"train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:19:28.330155Z","iopub.execute_input":"2021-06-29T06:19:28.330429Z","iopub.status.idle":"2021-06-29T06:19:28.337043Z","shell.execute_reply.started":"2021-06-29T06:19:28.330402Z","shell.execute_reply":"2021-06-29T06:19:28.336357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To make it a little easier to visualize, a decsision tree has been created from our training data set.","metadata":{}},{"cell_type":"code","source":"star_model = DecisionTreeClassifier(max_depth = 100)\nstar_model.fit(train_X, train_y)\n\n\nplt.figure(figsize = (50,40))\nplot_tree(star_model,\n          feature_names=train_X.columns,    \n          class_names=['Red Dwarf', 'Brown Dwarf', 'White Dwarf', 'Main Sequence', 'Super Giants', 'Hyper Giants'],\n          filled=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:19:28.338067Z","iopub.execute_input":"2021-06-29T06:19:28.338507Z","iopub.status.idle":"2021-06-29T06:19:29.730128Z","shell.execute_reply.started":"2021-06-29T06:19:28.338474Z","shell.execute_reply":"2021-06-29T06:19:29.729187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is just an example of what our testing data is actually producing from its own data set.","metadata":{}},{"cell_type":"code","source":"pred = star_model.predict(train_X)\n\nprint(\"The predictions are:\")\n\n\nX['Star_Type'] = y\n\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:19:29.732002Z","iopub.execute_input":"2021-06-29T06:19:29.732326Z","iopub.status.idle":"2021-06-29T06:19:29.757165Z","shell.execute_reply.started":"2021-06-29T06:19:29.732294Z","shell.execute_reply":"2021-06-29T06:19:29.756306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models\nHere is where we fit the models. The models that I've chosen is a decision tree classifier and a random forest classifier. By using two models we can compare the accuracy and find out which model does a better job of predicting the star types on the validation set.\nWe use classifiers instead of regressors because regressors can predict only numerical data whereas we have categorical data in our predictions.\nThe decision tree classifier model creates one decision tree where the random forest model creates multiples decision trees.","metadata":{}},{"cell_type":"code","source":"star_predictor = DecisionTreeClassifier(max_depth = 5)\n\nstar_predictor.fit(train_X, train_y)\n\nclassi_val_predictions =  star_predictor.predict(val_X)\naccuracy_score(val_y, classi_val_predictions)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:47:26.603324Z","iopub.execute_input":"2021-06-29T06:47:26.603809Z","iopub.status.idle":"2021-06-29T06:47:26.617037Z","shell.execute_reply.started":"2021-06-29T06:47:26.603776Z","shell.execute_reply":"2021-06-29T06:47:26.615998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"star_forest_predictor = RandomForestClassifier(random_state=1, max_depth = 3)\n\nstar_forest_predictor.fit(train_X, train_y)\n\nforr_val_preds = star_forest_predictor.predict(val_X)\naccuracy_score(val_y, forr_val_preds)    ","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:47:19.871525Z","iopub.execute_input":"2021-06-29T06:47:19.871928Z","iopub.status.idle":"2021-06-29T06:47:20.082059Z","shell.execute_reply.started":"2021-06-29T06:47:19.871894Z","shell.execute_reply":"2021-06-29T06:47:20.081055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluating and comparing predictions\n\nFrom our results we can conclude that random forest classsifier has more accurate predictions than the decision tree classifier at a 100% accuracy rate! The random forest may create more accurate decisions but the decision tree classifier is also very successful with a 98% accuracy.\nThe random forest had more accurate predictions because it creates many decision trees to get a more accurate prediction whereas the decision tree classifier only uses one decision tree.","metadata":{}},{"cell_type":"markdown","source":"# Hyper Parameter tuning\nTo get the most accurate predictions from our models we have to tweak the hyper parameters a bit. To get the most accurate prediction for the Decision tree classifier, the max depth needs to be 5 or more. To get the most accurate prediction for the random forest which is 100% accuracy, we need a depth of 3 or more.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\nThe purpose of this investigation was too predict star types from the given data which included radius, colour, luminosity, etc. After working through the steps of machine learning we were finally able to create predictions based on the given data set. After testing the accuracy of the predictions we found some interesting results for the models. The decision tree classifier had an amazing accuracy score with 98% however the real mystery was produced from the random forest classifier with 100% accuracy.\nTo get a 100% accuracy with the validation data is a real anomaly. I thought it would be a mistake in the code, but as I was changing the hyperperameters I noticed that the accuracy could go below 1.00, so maybe it really was a 100% accuracy? Maybe different star types are very distinct from each other and when you get the data for a star the predictions that are made are almost certain for what type of star it is. This may be the case for the 100% accuracy, but who knows? To solve this mystery further research is likely required. ","metadata":{}}]}