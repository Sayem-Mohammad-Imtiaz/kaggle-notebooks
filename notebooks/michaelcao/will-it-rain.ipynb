{"cells":[{"metadata":{"_uuid":"260bb6dce89132bb306cbf20aae6f225533cf6a1"},"cell_type":"markdown","source":"# **Will It Rain Tomorrow?**\n\nWe will predict if it will rain tomorrow in Australia. This is a classification problem as we are predicting whether it will Rain or Not Rain.\n### **Table of Contents**\n\n1. Data Preprocessing and Exploratory Data Analysis\n2. Random Forest\n3. Logistic Regression\n4. Comparison Between Random Forest and Logistic Regression\n5. Conclusion\n\n### **1. Data Preprocessing and Exploratory Data Analysis**\n#### **Import Libraries and Dataset**"},{"metadata":{"trusted":true,"_uuid":"504cd146888d2f44cd25a16b6b480cd928e46ac5"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b78058b44e1b48b7f9973ab6c19045faf0c575e8"},"cell_type":"code","source":"# Load data\nrain_data = pd.read_csv('../input/weatherAUS.csv')\nrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"849e0e7269fd6e4e5d197ce7b304e248232dd97b"},"cell_type":"markdown","source":"#### **Missing Data:**"},{"metadata":{"trusted":true,"_uuid":"8870463c9bbf5e37486facb57987ad0f9f894e3a"},"cell_type":"code","source":"# Visualising missing data:\nsns.heatmap(rain_data.isnull(),yticklabels=False,cbar=False,cmap='Reds_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7be43b3ac95f1f8e1e5e2a0f0d9c7bcd3a3ee33b"},"cell_type":"code","source":"# High percentage of missing data for Evaporation, Sunshine, Cloud9am and Cloud3pm features.\n# Date, Location and RISK_MM will be removed.\n# Lastly, remove any observations/rows with missing data\nrain_data.drop(['Evaporation','Sunshine','Cloud9am','Cloud3pm','RISK_MM','Date','Location'],axis=1,inplace=True)\nrain_data.dropna(inplace=True)\nrain_data[['RainTomorrow','RainToday']] = rain_data[['RainTomorrow','RainToday']].replace({'No':0,'Yes':1})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eac83e9fa356e6ba81f12a421643e7ef30388469"},"cell_type":"markdown","source":"#### **Rain vs. No Rain**"},{"metadata":{"trusted":true,"_uuid":"d869bcfb73c9c1c873b379fa0850fc567a725fee"},"cell_type":"code","source":"# Frequency of Rainy and No Rain:\nmpl.style.use('ggplot')\nplt.figure(figsize=(6,4))\nplt.hist(rain_data['RainTomorrow'],bins=2,rwidth=0.8)\nplt.xticks([0.25,0.75],['No Rain','Rain'])\nplt.title('Frequency of No Rain and Rainy days\\n')\nprint(rain_data['RainTomorrow'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c12b29b97d9c63567e9cdffda7cbe6a04fee94d"},"cell_type":"markdown","source":"#### **Histogram of the numerical features:**"},{"metadata":{"trusted":true,"_uuid":"8da140ef4a6518d5d37ec54aff7c150120995808"},"cell_type":"code","source":"# Segregating our numerical features from the categorical\nrain_data_num = rain_data[['MinTemp','MaxTemp','Rainfall','WindSpeed9am','WindSpeed3pm',\n                           'Humidity9am','Humidity3pm','Pressure9am','Pressure3pm',\n                           'Temp9am','Temp3pm','RainToday','RainTomorrow']]\n\n# Histogram of each numerical feature\nmpl.rcParams['patch.force_edgecolor'] = True\nax_list = rain_data_num.drop(['RainTomorrow'],axis=1).hist(figsize=(20,15),bins=20)\nax_list[2,1].set_xlim((0,100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ce16961bc12820baf19e2ba0126a4b2b00b60b6"},"cell_type":"markdown","source":"#### **Correlation matrix between the numerical features:**"},{"metadata":{"trusted":true,"_uuid":"910c8d8c61fb77169a19739675bf8ccb9f776611"},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.heatmap(rain_data_num.corr(),annot=True,cmap='bone',linewidths=0.25)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8848d9d1578bb161c0e5230e80f481505a6d0a1"},"cell_type":"markdown","source":"#### **Categorical Features and Dummy Variables**"},{"metadata":{"trusted":true,"_uuid":"9e1adb68a9c6c3c6f3176b769f1ffce6c21839ed"},"cell_type":"code","source":"# Creating dummy variables for the categorical features:\nWindGustDir_data = pd.get_dummies(rain_data['WindGustDir'])\nWindDir9am_data = pd.get_dummies(rain_data['WindDir9am'])\nWindDir3pm_data = pd.get_dummies(rain_data['WindDir3pm'])\n\n# Dataframe of the categorical features\nrain_data_cat = pd.concat([WindGustDir_data,WindDir9am_data,WindDir3pm_data],\n                          axis=1,keys=['WindGustDir','WindDir9am','WindDir3pm'])\n\n# Combining the Numerical and Categorical/Dummy Variables\nrain_data = pd.concat([rain_data_num,rain_data_cat],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22565c6d05d9e63117e4267b1de2b619cb14c50a"},"cell_type":"markdown","source":"#### **Splitting Data by Train and Test Data**"},{"metadata":{"trusted":true,"_uuid":"ec596136190f37d6f4b2fd111a4bc622771edef5"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = rain_data.drop(['RainTomorrow'],axis=1)\ny = rain_data['RainTomorrow']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=88)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e94c1923ec42dc5b3a56915418059cfa41a2e95"},"cell_type":"markdown","source":"## **2. Random  Forest**\n#### **Instantiate the Random Forest classifier, fit then predict**"},{"metadata":{"trusted":true,"_uuid":"e9f36909493d2ac1ea829213a48a9548a9528f68"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n# Out of Bag (oob) set to True. We will compare the oob_score with accuracy to see if they differ by much\n# n_estimators, or number of decision trees set to 100\nrf = RandomForestClassifier(n_estimators=100,oob_score=True,random_state=88)\nrf.fit(X_train,y_train)\ny_rf_pred = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b250d660efa699100d52df6582089cba4c90f85a"},"cell_type":"markdown","source":"#### **Null Accuracy**"},{"metadata":{"trusted":true,"_uuid":"76d79693d90dd96d1e17b15faefaa1490d9037b5"},"cell_type":"code","source":"# No Rain and Rain frequency in test set\nprint(y_test.value_counts())\nnull_accuracy = float(y_test.value_counts().head(1) / len(y_test))\nprint('Null Accuracy Score: {:.2%}'.format(null_accuracy))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db7806242bf4f07790aeb024f09b07472091a697"},"cell_type":"markdown","source":"If we built a model that guesses 'No Rain' every time, then we would obtain the 'Null Accuracy' or 'Baseline Accuracy' of 77.6%. "},{"metadata":{"_uuid":"c1fa410ba483897cc9432843d475fd004e94d197"},"cell_type":"markdown","source":"#### **Random Forest result:**"},{"metadata":{"trusted":true,"_uuid":"a8f91ea720ff35a42618497f775a801d96076d24"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\nprint('Accuracy Score: {:.2%}'.format(accuracy_score(y_test,y_rf_pred),'\\n'))\nprint('Out of Bag Accuracy Score: {:.2%}'.format(rf.oob_score_),'\\n')\nprint('Confusion Matrix:\\n',confusion_matrix(y_test,y_rf_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3746be4ea3d22d99fbc137dff9948598c41a8e1"},"cell_type":"markdown","source":"Accuracy Rate is 84.98%, however the Baseline Accuracy is 77.6%. Still some improvement though. The False Positives (3,933) are greater than the True Positives (3,653). Let's try and make our model simplier through feature selection as there are 60 features. Then we will try and improve the True Positive Rate."},{"metadata":{"_uuid":"51da1fd998cd821ae23ac1a41825ce3527e94a0b"},"cell_type":"markdown","source":"#### **Which Features Add Value?**\nFeature selection (reducing the number of variables) will make our model simplier and reduce the chances of overfitting. We don't want to use variables that don't any or much value when it comes training our model to predict the out-of-sample data. "},{"metadata":{"trusted":true,"_uuid":"97e25e545fabfcea654a8fcf657da6ef4ded6c52"},"cell_type":"code","source":"# Using feature_importance_ for feature selection\nfeature_importance_rf = pd.DataFrame(rf.feature_importances_,index=X_train.columns,columns=['Importance']).sort_values(['Importance'],ascending=False)\nfeature_importance_rf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"611c481e1cc0654d271e08764a91adbfcf31e787"},"cell_type":"code","source":"# Plot feature_importance\nfeature_importance_rf.plot(kind='bar',legend=False,figsize=(15,8))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"400dba89f1398a3a8a014b46ef0766671e5414f3"},"cell_type":"markdown","source":"Remember our Categorical Variables: WindGustDir,WindDir9am,WindDir3pm, were transformed into dummy variables to make the Random Forest algorithm perform better. Because they were transformed into dummy variables, then they would appear to rank low under the important features as plotted above.\n\nTo see whether the Categorical Variables: WindGustDir,WindDir9am,WindDir3pm, add much value to our model, we will compute the accuracy rate for the Random Forest using only the top 5 features, and compare it with the accuracy rate for the Random Forest using the top 5 features as well as the Categorical Variables."},{"metadata":{"_uuid":"6db99849f99f91106fa2915c70cb7465de7e146b"},"cell_type":"markdown","source":"#### **Train Random Forest with the subset of features. Then, predict.**\n#### **Random Forest Top 5 Features Result:**"},{"metadata":{"trusted":true,"_uuid":"b78441d8c62cf04a47bba4c63e3539cb20f1940e"},"cell_type":"code","source":"# Our Top 5 Features\nfeatures_top_5 = list(feature_importance_rf.index[0:6])\n\n# X dataframe - with only the top 5 features\nsubset_1 = [X.columns.get_loc(x) for x in features_top_5]\n\n# Split, Train, Predict\nX_train, X_test, y_train, y_test = train_test_split(X.iloc[:,subset_1], y, test_size=0.30, random_state=88)\nrf.fit(X_train,y_train)\ny_rf_pred = rf.predict(X_test)\n\nprint('Accuracy Score: {:.2%}'.format(accuracy_score(y_test,y_rf_pred)))\nprint('Out of Bag Score {:.2%}:'.format(rf.oob_score_),'\\n')\nprint('Confusion Matrix:\\n',confusion_matrix(y_test,y_rf_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"867ca1013942e4a5bd417f9ec4f6000997c0a215"},"cell_type":"markdown","source":"#### **Random Forest Top 5 Features with Categorical (Dummy) Variables Result:**"},{"metadata":{"trusted":true,"_uuid":"c6058857bdefd0df0861836b62322019dfecce8e"},"cell_type":"code","source":"# X dataframe - with top 5 features and the categorical variables\nsubset_2 = subset_1 + list(range(12,len(X.columns)))\n\n# Split, Train, Predict\nX_train, X_test, y_train, y_test = train_test_split(X.iloc[:,subset_2], y, test_size=0.30, random_state=88)\nrf.fit(X_train,y_train)\ny_rf_pred = rf.predict(X_test)\n\nprint('Accuracy Score: {:.2%}'.format(accuracy_score(y_test,y_rf_pred)))\nprint('Out of Bag Score {:.2%}:'.format(rf.oob_score_),'\\n')\nprint('Confusion Matrix:\\n',confusion_matrix(y_test,y_rf_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e480ca2fd518a05995d7cff9e6bbc89eb595599"},"cell_type":"markdown","source":"From here we can conclude that the categorical features do not add significant amount of value to the accuracy rate. Therefore we can remove the categorical features."},{"metadata":{"_uuid":"cd92edc9a154a128073d0dbdd434defcd87cbc7b"},"cell_type":"markdown","source":"#### **How many features should we use?**\nThere's no one right answer to this. To help us choose what number of features to use, we will visualise the relationship between number of features used vs. accuracy rate."},{"metadata":{"trusted":true,"_uuid":"40e81086bc9c0b99c7c87c26b6d645d136809ca0"},"cell_type":"code","source":"%%time \n\n# Up to what number of features to plot\nindex = np.array(list(range(2,9)) + [15, 30, 60])\n\n# creating list of index location\nfeatures = list(feature_importance_rf.index)\nfeatures = [X.columns.get_loc(x) for x in features]\n\n# instantiate classifier\nrf = RandomForestClassifier(n_estimators=100,random_state=88)\n\naccuracy_rate = []\n\n# append the accuracy rate\nfor i in index:\n    X_train, X_test, y_train, y_test = train_test_split(X.iloc[:,features[0:i]], y, test_size=0.30, random_state=88)\n    rf.fit(X_train,y_train)\n    y_rf_pred = rf.predict(X_test)    \n    accuracy_rate.append(accuracy_score(y_test,y_rf_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a97e1ade72fc2b272760d2064fead7bbd41b07c4"},"cell_type":"code","source":"# Plot accuracy vs. number of features\nplt.figure(figsize=(7,5))\nplt.scatter(x=index-1,y=accuracy_rate)\nplt.ylabel('Accuracy Rate',fontsize=12)\nplt.xlabel('Number of Features',fontsize=12)\nplt.xlim(-0.2,60)\nplt.title('Random Forest \\nAccuracy Rate vs. Number of Features', fontsize = 14)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0386bff330b50b075d44b1e524e708275d12ef93"},"cell_type":"markdown","source":"As you can see, there is not much improvement to the accuracy rate when the number of features are at 5 or more. To keep our model simple, we will use 7 features instead of 59. The accuracy rate from 7 to 59 features differs by only 0.68%. "},{"metadata":{"trusted":true,"_uuid":"306e79abe69749b32d8ca62eb0502e76338850dd"},"cell_type":"code","source":"# Split, Train, Predict on The 7 Features\nX_train, X_test, y_train, y_test = train_test_split(X[feature_importance_rf.head(7).index], y, test_size=0.30, random_state=88)\nrf.fit(X_train,y_train)\ny_rf_pred = rf.predict(X_test)\ncm = pd.DataFrame(confusion_matrix(y_test,y_rf_pred), index=['NO RAIN','RAIN'],columns=['NO RAIN','RAIN'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"774ecfb332aba874c3addf5771c2c9b46cbc3c6a"},"cell_type":"markdown","source":"#### **Confusion Matrix with simplier Random Forest model (Top 7 Features)**"},{"metadata":{"trusted":true,"_uuid":"51088dca01450159268a51278786f9be46312bc0"},"cell_type":"code","source":"print('Accuracy Score (Top 7 Features): {:.2%}'.format(accuracy_score(y_test,y_rf_pred)),'\\n')\n\n# plot confusion matrix\nfig = plt.figure(figsize=(8,6))\nax = sns.heatmap(cm,annot=True,cbar=False, cmap='CMRmap_r',linewidths=0.5,fmt='.0f')\nax.set_title('Random Forest Confusion Matrix',fontsize=16,y=1.25)\nax.set_ylabel('ACTUAL',fontsize=14)\nax.set_xlabel('PREDICTED',fontsize=14)\nax.xaxis.set_ticks_position('top')\nax.xaxis.set_label_position('top')\nax.tick_params(labelsize=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a566601f656de75442853752297e4731b6f16879"},"cell_type":"code","source":"TP = cm.iloc[1,1] # True Positive - Predicted Rain Correctly\nTN = cm.iloc[0,0] # True Negative - Predicted No Rain Incorrectly\nFP = cm.iloc[0,1] # False Positive - Predicted Rain when it didn't rain\nFN = cm.iloc[1,0] # False Negative - Predicted No Rain when it did rain","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c879e701b9179c7126edbc8092c1f496088ab43d"},"cell_type":"markdown","source":"#### **Sensitivty vs. Specificity**\n**Sensitivity:** When it rains, how often are our predictions correct?\n**Specificity:** When it does not rain, how often are our predictions correct? "},{"metadata":{"trusted":true,"_uuid":"1c6aece89cfef512ca4e29d364aa11008ad36d2b"},"cell_type":"code","source":"print('Sensitivity: {:.2%}'.format(TP/(FN+TP)))\nprint('Specificity: {:.2%}'.format(TN/(FP+TN)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58dbcb5f71367a3b35f4cd88ed0232ae9c4eb0f0"},"cell_type":"markdown","source":"Of the time it rained, we have correctly predicted Rain 47.76% of the time. Of the time it did not rain, we have correctly predicted No Rain 94.78% of the time. Considering it doesn't rain 77.6% of the time, you would expect the specificity rate to be higher."},{"metadata":{"_uuid":"7c2185f4e1d7bb9f8151875721adb1eb2923d430"},"cell_type":"markdown","source":"#### **Probability Score of Rain and No Rain from the Random Forest**\nThe Random Forest algorithm produces a probability score (the proportion of votes of the trees in the ensemble) in order to make classification. Let's visualise the probability scores for Rain vs. No Rain."},{"metadata":{"trusted":true,"_uuid":"dc4f324d673b9a1f03addea1346971f7ef649ea9"},"cell_type":"code","source":"# proves np.array of the probability scores\ny_prob_rain = rf.predict_proba(X_test)\n\n# To convert x-axis to a percentage\nfrom matplotlib.ticker import PercentFormatter\n\n# Plot histogram of predicted probabilities\nfig,ax = plt.subplots(figsize=(10,6))\nplt.hist(y_prob_rain[:,1],bins=50,alpha=0.5,color='teal',label='Rain')\nplt.hist(y_prob_rain[:,0],bins=50,alpha=0.5,color='orange',label='No Rain')\nplt.xlim(0,1)\nplt.title('Histogram of Predicted Probabilities')\nplt.xlabel('Predicted Probability (%)')\nplt.ylabel('Frequency')\n\nax.xaxis.set_major_formatter(PercentFormatter(1))\nax.text(0.025,0.83,'n = 33,878',transform=ax.transAxes)\n\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c4dfb068dd0921c37021e4a12a00a82e1b40209"},"cell_type":"markdown","source":"The Random Forest classication model uses 50% as the threshold for classification. Meaning that if probability of Rain is > 50%, it would predict Rain or assign a value of 1. Else if, the probability of Rain is < 50%, it would predict No Rain or assign a value of 0. \n\nBecause of the binary relationship between Rain and No Rain, the histogram has the highest frequency at 0% for Rain and 100% for No Rain.\n\nBut what if, the cost of a False Negative is greater than a False Positive? That is, the cost of it actually raining and predicting that it won't rain, is greater than the cost of when it does not rain when we have predicted that it will rain.\n\nThe 50% threshold can be reduced in order to increase the sensitivity rate. However this will reduce the specificity rate because there is an inverse relationship between sensitivity and specificity."},{"metadata":{"_uuid":"abd2f086ed5451448094783f5749219397d8bbc7"},"cell_type":"markdown","source":"#### **ROC Curve (True Positive vs. False Positive Rate) and Threshold Curve for the Random Forest Model**"},{"metadata":{"trusted":true,"_uuid":"451ca81070b5e62b1889a76a934b253223a6db66"},"cell_type":"code","source":"#ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test,y_prob_rain[:,1])\n\nfig,ax1 = plt.subplots(figsize=(9,6))\nax1.plot(fpr, tpr,color='orange')\nax1.legend(['ROC Curve'],loc=1)\nax1.set_xlim([-0.005, 1.0])\nax1.set_ylim([0,1])\nax1.set_ylabel('True Positive Rate (Sensitivity)')\nax1.set_xlabel('False Positive Rate \\n(1 - Specificity)\\n FP / (TN + FP)')\nax1.set_title('ROC Curve for RainTomorrow Random Forest Classifier\\n')\n\nplt.plot([0,1],[0,1],linestyle='--',color='teal')\nplt.plot([0,1],[0.5,0.5],linestyle='--',color='red',linewidth=0.25)\n\n#Threshold Curve\nax2 = plt.gca().twinx()\nax2.plot(fpr, thresholds, markeredgecolor='r',linestyle='dashed', color='black')\nax2.legend(['Threshold'],loc=4)\nax2.set_ylabel('Threshold',color='black')\nax2.set_ylim([0,1])\nax2.grid(False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d45f7768e9b8b85368b441063df80bf1b5d213c0"},"cell_type":"markdown","source":"The default threshold is 50%, which had resulted in a low sensitivity rate of 47.76% and specificity of 94.78%. Now will change the threshold so that it provides a higher sensitivity rate at the cost of a lower specificity rate."},{"metadata":{"_uuid":"44b7a76a57f14d49bac7290eb28ad10d8685281e"},"cell_type":"markdown","source":"#### **Selecting The Threshold Rate**"},{"metadata":{"trusted":true,"_uuid":"8c01b4b8f934061a870155bbf350b0c7c2bded9c"},"cell_type":"code","source":"# Function to calc sensitivity and specificity rate for a given threshold\ndef evaluate_threshold(threshold):\n    print('Sensitivity: {:.2%}'.format(tpr[thresholds > threshold][-1]))\n    print('Specificity: {:.2%}'.format(1 - fpr[thresholds > threshold][-1]))\n    \nevaluate_threshold(0.25)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c720572e243b0fa9002401096a9ec597c67e3037"},"cell_type":"markdown","source":"#### **We will reduce the threshold from 50% to 25%.**\n\nIf the probability of Rain is > 25%, the model will predict Rain (for tomorrow). If the probability of Rain is < 25%, the model will predict No Rain (for tomorrow)."},{"metadata":{"_uuid":"67393245da58f727228a7858746a719358a529ae"},"cell_type":"markdown","source":"#### **Confusion Matrix with simplier Random Forest model (top 7 Features) and threshold rate set to 25%**"},{"metadata":{"trusted":true,"_uuid":"ffdfce1339112a8beeafa3cb412f494084c1e91c"},"cell_type":"code","source":"from sklearn.preprocessing import binarize\n# change the predicted class with 25% threshold\ny_pred_class = binarize(y_prob_rain,0.25)[:,1]\n\ncm = pd.DataFrame(confusion_matrix(y_test,y_pred_class), index=['NO RAIN','RAIN'],columns=['NO RAIN','RAIN'])\n\nprint('Accuracy Score (Top 7 Features with 25% Threshold): {:.2%}'.format(accuracy_score(y_test,y_pred_class)),'\\n')\n\n# Plot Confusion Matrix\nfig = plt.figure(figsize=(8,6))\nax = sns.heatmap(cm,annot=True,cbar=False, cmap='CMRmap_r',linewidths=0.5,fmt='.0f')\nax.set_title('Random Forest Confusion Matrix',fontsize=16,y=1.25)\nax.set_ylabel('ACTUAL',fontsize=14)\nax.set_xlabel('PREDICTED',fontsize=14)\nax.xaxis.set_ticks_position('top')\nax.xaxis.set_label_position('top')\nax.tick_params(labelsize=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c565dcd94c661055a33355fc76c8e62f43cab4d"},"cell_type":"code","source":"TP = cm.iloc[1,1] # True Positive - Predicted Rain Correctly\nTN = cm.iloc[0,0] # True Negative - Predicted No Rain Incorrectly\nFP = cm.iloc[0,1] # False Positive - Predicted Rain when it didn't rain\nFN = cm.iloc[1,0] # False Negative - Predicted No Rain when it did rain\n\nsens_rf = TP/(FN+TP)\nspec_rf = TN/(FP+TN)\n\nprint('Sensitivity: {:.2%}'.format(sens_rf))\nprint('Specificity: {:.2%}'.format(spec_rf))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56868b5aeaf04282cc004a337530e23623adeaab"},"cell_type":"markdown","source":"Of the time it rained, we have correctly predicted Rain 74.26% of the time. Of the time it did not rain, we have correctly predicted No Rain 81.16% of the time."},{"metadata":{"_uuid":"f517ece32a8194cffc5cb8febca317ec57f278e5"},"cell_type":"markdown","source":"#### **AUC for Random Forest**\nThe AUC is the Area Under the ROC Curve. If the model produces a high sensitivity and specificity rate (which is what you would want to achieve), then the ROC curve will be stretched towards the top left of the x-y axis.\nThe AUC provides in indication on how well the model had performed in comparison to another model."},{"metadata":{"trusted":true,"_uuid":"12d9af12ffcce5e3a08ab93ad3612d4cf733bfee"},"cell_type":"code","source":"rf_auc = roc_auc_score(y_test,y_prob_rain[:,1])\nprint('AUC Score: {:.2%}'.format(rf_auc))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2284a5cd8d816c7f48d105efe54f62857772dd51"},"cell_type":"markdown","source":"### **3. Logistic Regression**\n#### **Train and Predict using the entire number of features**"},{"metadata":{"trusted":true,"_uuid":"ec77746f8150dbe8b3cef9c125cacd8cb5c2d710"},"cell_type":"code","source":"# Libraries for the Logistic Regression \nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nX = rain_data.drop(['RainTomorrow'],axis=1)\ny = rain_data['RainTomorrow']\n# Remove the Categorical (Dummy) Variables, as we have identified earlier that they do not add much value\nX = X.iloc[:,0:12] \n\n\n# Train Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=88)\n\n# Logistic Regression train\nlr = LogisticRegression(random_state=88, solver='liblinear')\nlr.fit(X_train,y_train)\n\n# predict\ny_lr_pred = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c90fccd5086c503dadf0dc20f11216a02e697db9"},"cell_type":"markdown","source":"#### **Logistic Regression result using all the features:**"},{"metadata":{"trusted":true,"_uuid":"4c84caae7d0df18e1fcd5cc5fbcb4f4969a2f14d"},"cell_type":"code","source":"# The 10-Fold Cross Validation method is used to calculate the accuracy score of the Logistic Regression model.\nprint('Accuracy Score with 10-KFolds: {:.2%}'.format(cross_val_score(lr,X,y,cv=10,scoring='accuracy').mean()),'\\n')\nprint('Confusion Matrix:\\n',confusion_matrix(y_test,y_lr_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b334f83bda61274c1e3eccad1878c0c6dd2bf7e"},"cell_type":"markdown","source":"#### **Feature Selection - using Recursive Feature Elimination (RFE)**\n\n*\"Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\"*\n\nThe Final Random Forest model used 7 features. We will have the same number of features for the Logistic Regression Model. "},{"metadata":{"trusted":true,"_uuid":"0be00f16efa49ca19b5f67c3dd25ab523eae6dd7"},"cell_type":"code","source":"%%time\n#Feature Selection Method: Recursive Feature Elimination \nrfe = RFE(estimator=lr, n_features_to_select=7)\nrfe = rfe.fit(X_train,y_train)\n\nprint(\"Number of Features: {}\".format(rfe.n_features_)) \nprint(\"Selected Features: {}\".format(rfe.support_))\nprint(\"Feature Ranking: {}\".format(rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc05db231c042bac0eb08890e7577be5bb79994e"},"cell_type":"markdown","source":"#### **The Selected 7 Features:**"},{"metadata":{"trusted":true,"_uuid":"723d18e9799a89250db4ac64a86907192e63b0e3"},"cell_type":"code","source":"pd.DataFrame(X.iloc[:,rfe.support_].columns,columns=['Importance'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49cf4b8022ddeb16b1af45ab10104e62dbfa75a6"},"cell_type":"markdown","source":"#### **Train and Predict using the 7 chosen features**"},{"metadata":{"trusted":true,"_uuid":"593af1f8ed1a263ae649514a3d1037506c2f4f3d"},"cell_type":"code","source":"X_rfe = X.iloc[:,rfe.support_]\n# Train Test split with subset of X features\nX_train, X_test, y_train, y_test = train_test_split(X_rfe, y, test_size=0.30, random_state=88)\n# Train and Predict\nlr.fit(X_train,y_train)\ny_lr_pred = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34ac210cbaf82b7216317cfe5287f19f3ec5a5a3"},"cell_type":"markdown","source":"#### **Logistic Regression result using the selected 7 features:**"},{"metadata":{"trusted":true,"_uuid":"46b630d6e70c12d6a94d0a579e711abebe6645f5"},"cell_type":"code","source":"#accuracy rate using 10-Fold CV\naccuracy_kfold = cross_val_score(lr,X_rfe,y,cv=10,scoring='accuracy').mean()\nprint('Accuracy Score with 7 Features and 10-KFolds: {:.2%}'.format(accuracy_kfold),'\\n')\nprint('Confusion Matrix:\\n',confusion_matrix(y_test,y_lr_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"739b02032b86fd6fd534c3b0ddae48e85ba279d3"},"cell_type":"markdown","source":"#### **The coefficients and features in the Logistic Regression**"},{"metadata":{"trusted":true,"_uuid":"7a30df0664406f470c9052329aa9ef68d4e7c378"},"cell_type":"code","source":"pd.concat([pd.DataFrame(lr.coef_,index=['coefficient'],columns=X_train.columns).T, \n                         X_train.aggregate([np.mean,np.std,np.min,np.max]).T],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2594186209dc84fbd8681d6abf0bea7ce4513e70"},"cell_type":"markdown","source":"#### **Confusion Matrix with simplier Logistic Regression model (7 Features)**"},{"metadata":{"trusted":true,"_uuid":"1075b4519aa8350b200b6cb1db843b1ffba27ba7"},"cell_type":"code","source":"cm = pd.DataFrame(confusion_matrix(y_test,y_lr_pred), index=['NO RAIN','RAIN'],columns=['NO RAIN','RAIN'])\n\nprint('Accuracy Score with 7 Features and 10-KFolds: {:.2%}'.format(accuracy_kfold),'\\n')\n\n# Plot CM\nfig = plt.figure(figsize=(8,6))\nax = sns.heatmap(cm,annot=True,cbar=False, cmap='CMRmap_r',linewidths=0.5,fmt='.0f')\nax.set_title('Logistic Regression Confusion Matrix',fontsize=16,y=1.25)\nax.set_ylabel('ACTUAL',fontsize=14)\nax.set_xlabel('PREDICTED',fontsize=14)\nax.xaxis.set_ticks_position('top')\nax.xaxis.set_label_position('top')\nax.tick_params(labelsize=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"871bf8ac825afaf2aa3961fbc0e15dcedfe3dbd6"},"cell_type":"code","source":"TP = cm.iloc[1,1] # True Positive - Predicted Rain Correctly\nTN = cm.iloc[0,0] # True Negative - Predicted No Rain Incorrectly\nFP = cm.iloc[0,1] # False Positive - Predicted Rain when it didn't rain\nFN = cm.iloc[1,0] # False Negative - Predicted No Rain when it did rain\n\nprint('Sensitivity: {:.2%}'.format(TP/(FN+TP)))\nprint('Specificity: {:.2%}'.format(TN/(FP+TN)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f52388627825e37c92f5e18aeecfc37417f33e24"},"cell_type":"markdown","source":"Of the time it rained, we have correctly predicted Rain 42.35% of the time. Of the time it did not rain, we have correctly predicted No Rain 95.20% of the time. Like what we did before, assuming the cost of False Positive is greater than False Negative, we will adjust the threshold level to 25%."},{"metadata":{"_uuid":"e5e71caf956c894fc3bda2a495ba812d28bca3a8"},"cell_type":"markdown","source":"#### **ROC Curve and Threshold Curve for the Logistic Regression Model**"},{"metadata":{"trusted":true,"_uuid":"a77ff7655b3c53f5c92a11f2dbe5c75273a63064"},"cell_type":"code","source":"# Probability of Rain for X_test\ny_prob_rain = lr.predict_proba(X_test)\n\nfpr, tpr, thresholds = roc_curve(y_test,y_prob_rain[:,1])\n\n#ROC Curve\nfig,ax1 = plt.subplots(figsize=(9,6))\nax1.plot(fpr, tpr,color='orange')\nax1.legend(['ROC Curve'],loc=1)\nax1.set_xlim([-0.005, 1.0])\nax1.set_ylim([0,1])\nax1.set_ylabel('True Positive Rate (Sensitivity)')\nax1.set_xlabel('False Positive Rate \\n(1 - Specificity)\\n FP / (TN + FP)')\nax1.set_title('ROC Curve for RainTomorrow Logistic Regression Classifier\\n')\n\nplt.plot([0,1],[0,1],linestyle='--',color='teal')\nplt.plot([0,1],[0.5,0.5],linestyle='--',color='red',linewidth=0.25)\n\n#Threshold Curve\nax2 = plt.gca().twinx()\nax2.plot(fpr, thresholds, markeredgecolor='r',linestyle='dashed', color='black')\nax2.legend(['Threshold'],loc=4)\nax2.set_ylabel('Threshold',color='black')\nax2.set_ylim([0,1])\nax2.grid(False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84741d67fb83c541eda8fa4163719cc023846363"},"cell_type":"markdown","source":"#### **Confusion Matrix with simplier Logistic Regression model (7 Features) and 25% threshold**"},{"metadata":{"trusted":true,"_uuid":"38b23b410d55dc1603269b38f914edbb9482f3f7"},"cell_type":"code","source":"# Changing predictions using threshold of 25%\ny_pred_class = binarize(y_prob_rain,0.25)[:,1]\n\ncm = pd.DataFrame(confusion_matrix(y_test,y_pred_class), index=['NO RAIN','RAIN'],columns=['NO RAIN','RAIN'])\n\nprint('Accuracy Score (Top 7 Features with 25% Threshold): {:.2%}'.format(accuracy_score(y_test,y_pred_class)),'\\n')\n\nfig = plt.figure(figsize=(8,6))\nax = sns.heatmap(cm,annot=True,cbar=False, cmap='CMRmap_r',linewidths=0.5,fmt='.0f')\nax.set_title('Logistic Regression Confusion Matrix',fontsize=16,y=1.25)\nax.set_ylabel('ACTUAL',fontsize=14)\nax.set_xlabel('PREDICTED',fontsize=14)\nax.xaxis.set_ticks_position('top')\nax.xaxis.set_label_position('top')\nax.tick_params(labelsize=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d175ad059afa27651e15e70c0ec1bebd358dec13"},"cell_type":"code","source":"TP = cm.iloc[1,1] # True Positive - Predicted Rain Correctly\nTN = cm.iloc[0,0] # True Negative - Predicted No Rain Incorrectly\nFP = cm.iloc[0,1] # False Positive - Predicted Rain when it didn't rain\nFN = cm.iloc[1,0] # False Negative - Predicted No Rain when it did rain\n\nsens_lr = TP/(FN+TP)\nspec_lr = TN/(FP+TN)\n\nprint('Sensitivity: {:.2%}'.format(sens_lr))\nprint('Specificity: {:.2%}'.format(spec_lr))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9fc7ddd8015ac615bef2089fa30c23b794fcfda0"},"cell_type":"markdown","source":"Of the time it rained, we have correctly predicted Rain 70.05% of the time. Of the time it did not rain, we have correctly predicted No Rain 79.53% of the time."},{"metadata":{"_uuid":"7b65112344499dc391ae732a52421d0af0a73c6f"},"cell_type":"markdown","source":"## 4. **Comparison Between Random Forest and Logistic Regression Model**\nWe will compare the results. The Area Under the ROC Curve is an indicator on which classifier model has the stronger performance."},{"metadata":{"trusted":true,"_uuid":"7f89c9b4c0c7d0bdd265164229298958b0613ba1"},"cell_type":"code","source":"lr_auc = cross_val_score(lr,X,y,cv=10,scoring='roc_auc').mean()\n\nprint('Null Accuracy Score: {:.2%}\\n'.format(null_accuracy))\nprint('{:>30} {:>26}'.format('Random Forest','Logistic Regression'))\nprint('{} {:>17.2%} {:>22.2%}'.format('AUC Score',rf_auc,lr_auc))\nprint('{} {:>14.2%} {:>22.2%}'.format('Sensitivity*',sens_rf,sens_lr))\nprint('{} {:>14.2%} {:>22.2%}'.format('Specificity*',spec_rf,spec_lr))\nprint('\\n*25% Threshold')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38e89eea86ebe6ea7d12db795dfa7a6ed17254ec"},"cell_type":"markdown","source":"## **5. Conclusion**\n#### **The Random Forest model is the better performer as the AUC is 85.83% vs. 83.41% for the Logistic Regression model.**\n\nFrom comparing accuracy rates, the Categorical Features: WindGustDir,WindDir9am,WindDir3pm, offered little value. We saw the increase in the accuracy rates from having 1 feature to 59 and chose 7 features as that was the approximate point at which the increase in accuracy rate was very small. Having less features would simplify the model, reduce chances of overfitting, and provide better interpretability.\n\nBoth the Random Forest and Logistic Regression had a low sensitivity rate which often incorrectly predicted that it won't rain the next day when it actually did rain. \n\nThe ROC and Threshold Curve demonstrates the relationshp between sensitivity and specificity at each threshold. Assuming that the cost of a False Positive was greater than a False Negative, we have chosen to reduce the threshold from 50% to 25%. This resulted in the sensitivity rate for the Random Forest model increasing from 47.76% to 74.26% at the trade-off of decreasing the specificity rate from 94.78% to 81.16%. \n\nThe features used in the Random Forest and Logistic Regression differ. Features used in the final model were:\n\n    Random Forest                 Logistic Regression\n    1. Humidity3pm                1. Humidity3pm\n    2. Pressure3pm                2. Pressure3pm\n    3. Humidity9am                3. RainToday\n    4. Pressure9am                4. Pressure9am\n    5. Temp3pm                    5. Temp3pm\n    6. Rainfall                   6. MaxTemp\n    7. MinTemp                    7. MinTemp"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}