{"cells":[{"metadata":{"_cell_guid":"0eac874b-1bea-4cf4-952c-e302f5c44ea2","_uuid":"6e0a34007c208412207de2bbebbdec24ed540a7b"},"cell_type":"markdown","source":"# Introduction\nMost existing algorithms for learning continuous word representation only model the syntactic context of the words but ignored the sentiment of text. This is really problematic in sentiment analysis. In this notebook, I am going to present one of the approach for sentiment specific learning used in the paper [<b>\"Learning Sentiment Specific Word Embedding for Twitter Sentiment Classification\"</b>](https://www.aclweb.org/anthology/P14-1146.pdf) on the TwitterAirline data set.\n\nIn this kernel we will:\n* We used the Embedding layer of Keras for word embeddings for training data\n* We also used pretrained word embeddings (GLOVE)"},{"metadata":{"_cell_guid":"af6fd7d5-043e-43ab-acf6-bb438b779417","_uuid":"06440c19546700dd01d9d649ddc735e03206553c"},"cell_type":"markdown","source":"# Word Embeddings\nIn simple terms, Word Embedding is a way of converting texts into numbers for the machine to understand that text. When applying one-hot encoding to the words in the tweets, a sparse vectors of high dimensionality will be obtainedand results in performance issues in case of large datasets. Additionally, one-hot encoding does not take into account the semantics of the words. For example, *tea* and *coffee* are different words but have a similar meaning. \n\nBasically, word embeddings are dense vectors with a much lower dimensionality and the distance and direction of the vectors in the matrix tells the semantic relationships between words.   "},{"metadata":{"_cell_guid":"2afb7748-1536-41f7-a2ad-3dfbdee61263","_uuid":"655d234070fef5815ed4d9618f4e430508b8809d"},"cell_type":"markdown","source":"# Analysis"},{"metadata":{"_cell_guid":"21cee267-f257-4ba3-88e1-d22e2fc57c7e","_uuid":"2fec7805d8d0e999fc405d136beff46ac331b963","trusted":true},"cell_type":"code","source":"# Basic packages required for analysis\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\n#from pathlib import Path\nimport re\nimport collections\nimport tensorflow as tf\nimport nltk\nimport itertools\nimport collections\n\n# To ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Packages required for data preparation\nfrom sklearn.model_selection import train_test_split\n## Packages for clearning the text\nfrom nltk.corpus import stopwords\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\n\n# for reproducibility\nrand = np.random.seed(78) \n\n# Packages required for modeling the data \nimport keras\nfrom keras import models\nfrom keras import layers\nfrom keras import regularizers\n\n# libraries for visualization\n#pd.options.mode.chained_assignment = None \nfrom gensim.models import word2vec\nfrom sklearn.manifold import TSNE\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Packages required for visualize the sentiment polarity\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1eeb591e-330a-4177-be9a-466b388cc2a1","_uuid":"6f2221dcf02f8bb28bf295ed95f6cf41c715dab4","trusted":true},"cell_type":"code","source":"nb_words = 10000  # number of words in the dictionary as per our choice\nbatch_size = 512  # size of the batches for gradient descent\nmax_len = 24  # maximum number of words in a sequence\nsize_valid = 1000  # size of validation set\nepochs = 20  # epochs to start train with\ndim_glove = 50  # dimensions of the GLOVE word embeddings","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some function for pre- processing the text"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(input_text):\n    '''\n    Function to remove English stopwords from a Pandas Series.\n    \n    Parameters:\n        input_text : text to clean\n    Output:\n        cleaned Pandas Series \n    '''\n    stopwords_list = stopwords.words('english')\n    # Some words which might indicate a certain sentiment are kept via a whitelist\n    whitelist = [\"n't\", \"not\", \"no\",\" \"]\n    words = input_text.split() \n    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n    return \" \".join(clean_words) \n    \ndef remove_mentions(input_text):\n    '''\n    Function to remove mentions, preceded by @, in a Pandas Series\n    \n    Parameters:\n        input_text : text to clean\n    Output:\n        cleaned Pandas Series \n    '''\n    return re.sub('([^\\s\\w]|_@?)+', '', input_text)\n#r'^@s\\w\\+|_?'#r'@\\w+'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"30b2a5cd-6e5c-4850-9fc3-acbaa34d8640","_uuid":"f6eff68f92d117c2556e403bc97c7800521ce4eb"},"cell_type":"markdown","source":"# Data Preparation\n### Reading and cleaning data"},{"metadata":{"_cell_guid":"ad9e0531-8a58-4c5b-974d-891745af8ac5","_uuid":"eb600f7b6b278e73117bfea5e6b297fb10aa2d56","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/twitter-airline-sentiment/Tweets.csv')\ndf = df.reindex(np.random.permutation(df.index))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[['text', 'airline_sentiment']]\ndf.text = df.text.apply(remove_stopwords).apply(remove_mentions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(5,5))\nsns.catplot(x=\"airline_sentiment\", data=df, kind=\"count\", height=6, aspect=1.5, palette=\"husl\")\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_corpus(df):\n    \"Creates a list of lists containing words from each sentence\"\n    corpus = []\n    for col in ['text']:\n        for sentence in df[col].iteritems():\n            word_list = sentence[1].split(\" \")\n            corpus.append(word_list)\n            \n    return corpus\n\ncorpus = build_corpus(df)        \ncorpus[0:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of all words across tweets\nlist_of_corpus = list(itertools.chain(*corpus))\n\n# Create counter\ncounts_of_words = collections.Counter(list_of_corpus)\n\ncounts_of_words.most_common(10)\n\nclean_tweets = pd.DataFrame(counts_of_words.most_common(10),\n                             columns=['words', 'count'])\n\nclean_tweets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1, ax = plt.subplots(figsize=(8, 8))\n\n# Plot horizontal bar graph\nclean_tweets.sort_values(by='count').plot.barh(x='words',\n                      y='count',\n                      ax=ax,\n                      color=\"purple\")\n\nax.set_title(\"Common Words Found in Tweets (Including All Words)\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"62132bff-f75d-4d9e-b4d5-d9c1ced79e3f","_uuid":"fa8df3743c04dfda0ec297245bc102d9b3227b03"},"cell_type":"markdown","source":"### Train-Test split"},{"metadata":{"_cell_guid":"df742ade-52d3-4b47-80b0-1f7cc26545eb","_uuid":"6ab9296159749e4525244fae6d6f4e6a37121c93","trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df.text, df.airline_sentiment, test_size=0.1, random_state=rand)\nprint('# Train data samples:', X_train.shape[0])\nprint('# Test data samples:', X_test.shape[0])\nassert X_train.shape[0] == y_train.shape[0]\nassert X_test.shape[0] == y_test.shape[0]\n#print(X_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b76321cb-6b78-49e5-9f81-72b9ce0c0fff","_uuid":"083757435330306da9b9fa9aae610ba76e20471e"},"cell_type":"markdown","source":"### Converting words to numbers"},{"metadata":{"_cell_guid":"77a831f6-0261-478e-94ec-72734e0373e4","_uuid":"b7dd99fd85fab14bf865f7d984766911f048cdeb","trusted":true},"cell_type":"code","source":"tk = Tokenizer(num_words=nb_words,\n               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n               lower=True,\n               split=\" \")\ntk.fit_on_texts(X_train)\n\nX_train_seq = tk.texts_to_sequences(X_train)\nX_test_seq = tk.texts_to_sequences(X_test)\n#print(X_train_seq)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"53358459-0564-4f74-a8d3-53fbb9070260","_uuid":"554f40d8dd9f7f37147731b97d5581da6ad7112d"},"cell_type":"markdown","source":"### Creating word sequences of equal length\nFirst, look at the length of the (cleaned) tweets as we need sequence of equal length for word embedding. To achieve this,  we either truncate sequences to max_len, or pad them with zeroes. "},{"metadata":{"_cell_guid":"1c65a612-b310-4e94-bb90-e98a5543bc1b","_uuid":"ec28cfcc5c7d88d2ac2da645ebd331c4f983d3d6","trusted":true},"cell_type":"code","source":"# calculating length of each sequence and displaying the five number sumary for length of sequence\nseq_lengths = X_train.apply(lambda x: len(x.split(' ')))\nseq_lengths.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7fbdae06-3e2d-4e58-beec-b526c4ceb49d","_uuid":"0801bf3d7e2adcc13d07855f052578038d24ba34"},"cell_type":"markdown","source":"Since, the maximum length is 24. So we and minimum length is 1. So, we will pad with zeros to avoid loss of information as tweetsare short."},{"metadata":{"_cell_guid":"4caee2ba-a258-4908-b58c-c6280a35e436","_uuid":"dde2572f8b73b2ba795e4a5a9a6333b06924990d","trusted":true},"cell_type":"code","source":"X_train_seq_trunc = pad_sequences(X_train_seq, maxlen=max_len)\nX_test_seq_trunc = pad_sequences(X_test_seq, maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3d2bf008-932b-4b44-bea4-52626876d419","_uuid":"363f55f2218dc983ea5f7aeb605321becbfeb300","trusted":true},"cell_type":"code","source":"X_train_seq_trunc[10]  # Example of padded sequence","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b0188904-9adc-4b01-823b-24a6281b4dc9","_uuid":"afa786f39cdc5a09bb40ffb4a65ea02ff917575c"},"cell_type":"markdown","source":"### Converting the target classes to numbers"},{"metadata":{"_cell_guid":"e8cde24c-641b-474f-a569-fa1a778dec6c","_uuid":"6b2b84b137f44315eade504c8d4675a50bf78bc6","trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ny_train_le = le.fit_transform(y_train)\ny_test_le = le.transform(y_test)\ny_train_oh = to_categorical(y_train_le)\ny_test_oh = to_categorical(y_test_le)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"33c4e935-4d09-45d0-a0db-8e79e24aee09","_uuid":"59cd92aa3bc7417176ce2d8dc1a1907823cb1d83"},"cell_type":"markdown","source":"> ### Splitting train and validation data"},{"metadata":{"_cell_guid":"1ff1d8d2-2d98-4449-9298-e2594bed577d","_uuid":"4cf52b83651a836882276c58b3c5000ba430b443","trusted":true},"cell_type":"code","source":"X_train_emb, X_valid_emb, y_train_emb, y_valid_emb = train_test_split(X_train_seq_trunc, y_train_oh, test_size=0.1, random_state=rand)\n\nassert X_valid_emb.shape[0] == y_valid_emb.shape[0]\nassert X_train_emb.shape[0] == y_train_emb.shape[0]\n\nprint('Shape of validation set:',X_valid_emb.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some custom function to help analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\n\n# Custom loss function for SSWE_h\ndef custom_loss_u(y_true,y_pred):\n    \"\"\"Custom loss function for SSWE_h.\n\n    Parameters\n    ----------\n        y_true : true sentiment classes\n        y_pred : predicted sentiment classes\n\n    Returns\n        Output:\n        loss: loss value\n    -------\n\n    \"\"\"\n    loss=(-1)*(K.sum(y_true * K.log(y_pred)))\n    return loss\n\n# Custom Activation function Hard hyperbolic tangent\n__all__ = ['htanh']\n\ndef hard_tanh(x, name='htanh'):\n    \"\"\"Hard tanh activation function.\n\n    A ramp function with low bound of -1 and upper bound of 1,\n\n    Parameters\n    ----------\n    x : Input Tensor.\n    name : str\n        The function name (optional).\n\n    Returns\n    -------\n\n    \"\"\"\n    return tf.clip_by_value(x, -1, 1, name=name)\n\n# Alias\nhtanh = hard_tanh\n\ndef deep_model(model, X_train, y_train, X_valid, y_valid):\n    '''\n    Function to train a multi-class model.\n    \n    Parameters:\n        model : model with the chosen architecture\n        X_train : training features\n        y_train : training target\n        X_valid : validation features\n        Y_valid : validation target\n    Output:\n        model training\n    '''\n    # setting up the optimizer as per the specification\n    opt = keras.optimizers.Adagrad(learning_rate=0.1)\n    model.compile(optimizer=opt\n                  , loss=custom_loss_u\n                  , metrics=['accuracy'])\n    \n    training = model.fit(X_train\n                       , y_train\n                       , epochs=epochs\n                       , batch_size=batch_size\n                       , validation_data=(X_valid, y_valid)\n                       , verbose=1\n                       ,shuffle=False)\n    return training\n\n\ndef eval_metric(training, metric_name):\n    '''\n    Function to evaluate a trained model. \n    Plots are shown as a line chart corresponding \n    to each epoch for training and validation set\n    \n    Parameters:\n        training : model training\n        metric_name : loss or accuracy\n    Output:\n        line chart with epochs with metric on\n        y-axis and epochs on x-axis\n    '''\n    metric = training.history[metric_name]\n    val_metric = training.history['val_' + metric_name]\n\n    e = range(1, epochs + 1)\n\n    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n    plt.legend()\n    plt.show()\n\ndef test_model(model, X_train, y_train, X_test, y_test, epoch_stop):\n    '''\n    Function to test the model on new data\n    with the optimal number of epochs.\n    \n    Parameters:\n        model : trained model\n        X_train : training features\n        y_train : training target\n        X_test : test features\n        y_test : test target\n        epochs : optimal number of epochs\n    Output:\n        test accuracy and test loss\n    '''\n    model.fit(X_train\n              , y_train\n              , epochs=epoch_stop\n              , batch_size=batch_size\n              , verbose=0\n              ,shuffle=False)\n    results = model.evaluate(X_test, y_test)\n    \n    return results","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8101462d-eba1-4fae-9e77-a2a1166aa2c3","_uuid":"c6f2dcdeebbef27add78e0eb54ac76ce19052d96"},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"_cell_guid":"706bdf6e-7248-48e5-9740-3bd16b3d8dfc","_uuid":"0f9fe95cca79087a8cc87396cc62420fe4e41043"},"cell_type":"markdown","source":"### Training word embeddings\nKeras provides an **Embedding layer** which helps us to train specific word embeddings based on our training data converting words to multi-dimensional vectors. "},{"metadata":{"_cell_guid":"be7731ff-5fd4-4006-93ab-7935608870a8","_uuid":"9b0f7a95f5ce87f2f63dac719eb47cced4ed5d4b","trusted":true},"cell_type":"code","source":"emb_model = models.Sequential()\nemb_model.add(layers.Embedding(nb_words, 50, input_length=max_len))\nemb_model.add(layers.Flatten())\nemb_model.add(layers.Dense(20, activation=htanh))\nemb_model.add(layers.Dense(3, activation='softmax'))\nemb_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"97113b34-2ee1-4edf-a411-5cfd6c2033e0","_uuid":"1227cf605366f4be25b74dcba1a29389d11ab680","trusted":true},"cell_type":"code","source":"emb_history = deep_model(emb_model, X_train_emb, y_train_emb, X_valid_emb, y_valid_emb)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"97389a42-d903-4ade-bf58-8a0feb6d84fa","_uuid":"f34fd856c0b4b19c9630e372bcb739ee4c24c2c1","trusted":true},"cell_type":"code","source":"eval_metric(emb_history, 'accuracy')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"24dc6adb-7368-44c4-836b-5f7a88b89c9c","_uuid":"825b7a981d45c79419f9768d5b30b3a4483380e3","trusted":true},"cell_type":"code","source":"eval_metric(emb_history, 'loss')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b2ee9d54-d504-4cba-9738-fa8acb2af538","_uuid":"41d0e88fa1528fd93253b4a1bdeb3be3b3ba504d","trusted":true},"cell_type":"code","source":"emb_results = test_model(emb_model, X_train_seq_trunc, y_train_oh, X_test_seq_trunc, y_test_oh, 6)\nprint('/n')\nprint('Test accuracy of word embeddings model: {0:.2f}%'.format(emb_results[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5d7aa101-acce-4a75-a88b-b7e1c49a32ca","_uuid":"45ef94d2bb38d78ddf288d15237e51aca8dad600"},"cell_type":"markdown","source":"* This test result is satisfactory. However, the model overfits fast, after 2 epochs"},{"metadata":{},"cell_type":"markdown","source":"### Using pre-trained word embeddings\nSince the training data is not so big, the model might not be able to learn good embeddings for the sentiment analysis. To vercome this, we can load pre-trained word embeddings built on a much larger training data. \n\nThe [GloVe database](https://nlp.stanford.edu/projects/glove/) contains multiple pre-trained word embeddings, and more specific embeddings trained on tweets."},{"metadata":{"_cell_guid":"189abce5-0663-4bc1-9e33-66a86f6052b1","_uuid":"13f3c179266bb21577200f7ff62f4d3debf07449","trusted":true},"cell_type":"code","source":"glove_file = 'glove.twitter.27B.' + str(dim_glove) + 'd.txt'\n\nglove_dir = '../input/glove-global-vectors-for-word-representation'\nemb_dict = {}\n#print(glove_dir+str('/')+glove_file)\nglove = open(glove_dir+str('/')+glove_file)\nfor line in glove:\n    values = line.split()\n    word = values[0]\n    vector = np.asarray(values[1:], dtype='float32')\n    emb_dict[word] = vector\nglove.close()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3d2183ad-84da-455b-9442-e84a38a34461","_uuid":"ec086a128a08cb835e425adc6338f85da32325ce"},"cell_type":"markdown","source":"The first task is to see that we have some airline related words in the dictionary"},{"metadata":{"_cell_guid":"c972f699-652c-493f-ac18-fd091b3f9289","_uuid":"5957cc228233c9b96f0ad93e0632d91473cd97de","trusted":true},"cell_type":"code","source":"airline_words = ['airplane', 'airline', 'flight', 'luggage']\nfor w in airline_words:\n    if w in emb_dict.keys():\n        print('Found the word {} in the dictionary'.format(w))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"17b50696-d448-483b-a4ac-a7039ea8f251","_uuid":"a5638dd38ef160a7632f7dbc7e47ef055654a07f"},"cell_type":"markdown","source":"Now we need to build a matrix of shape (nb_words, dim_glove) containing the words in the tweets and their representative word embedding for it to be processed by embedding layer."},{"metadata":{"_cell_guid":"bdd5d3b7-f19c-448a-a7bc-defcfe8ea034","_uuid":"68cb90dd9939915adc4c1500e46ca76bfeb485bc","trusted":true},"cell_type":"code","source":"emb_matrix = np.zeros((nb_words, dim_glove))\n\nfor w, i in tk.word_index.items():\n    # The word_index contains a token for all words of the training data so we need to limit that\n    if i < nb_words:\n        vect = emb_dict.get(w)\n        # Check if the word from the training data occurs in the GloVe word embeddings\n        # Otherwise the vector is kept with only zeros\n        if vect is not None:\n            emb_matrix[i] = vect\n    else:\n        break","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f2edc95b-5654-4c04-97a9-57fd2e4a56a0","_uuid":"4728c6ccb4308ed6f2efffe0545e564acc13bcba","trusted":true},"cell_type":"code","source":"glove_model = models.Sequential()\nglove_model.add(layers.Embedding(nb_words, dim_glove, input_length=max_len))\nglove_model.add(layers.Flatten())\nglove_model.add(layers.Dense(20, activation=htanh))\nglove_model.add(layers.Dense(3, activation='softmax'))\nglove_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b21eae7c-c9b4-495d-ba20-11ccfeedcb64","_uuid":"06376000aac2e4904fd0591f27f445cf9a103026"},"cell_type":"markdown","source":"We load the pre-trained embeddings in the Embedding layer (here layer 0) using *set_weights* method and by putting *trainable* attribute to False to make sure we are using pre-trained embeddings."},{"metadata":{"_cell_guid":"75967b04-88f6-4785-9f97-a5a8f1a99a85","_uuid":"8487f958f0fc57706c9b649f839fbec41c6032a6","trusted":true},"cell_type":"code","source":"glove_model.layers[0].set_weights([emb_matrix])\nglove_model.layers[0].trainable = False","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c2301196-b995-4e71-8b34-21367b4f8962","_uuid":"8febdbc6b2e9598e06228b0b4b492552d7c5ec9f","trusted":true},"cell_type":"code","source":"glove_history = deep_model(glove_model, X_train_emb, y_train_emb, X_valid_emb, y_valid_emb)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"17c7fd72-2e4d-49e3-bbf2-cb7ad98bdafd","_uuid":"a87cfcffc7ce7a57fbe0fe961608d0880d884913","trusted":true},"cell_type":"code","source":"eval_metric(glove_history, 'loss')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"df249c7f-bd7d-4b54-8e2f-3328518c4db9","_uuid":"668daafacb1a99527158ce11df268144cd2877ae","trusted":true},"cell_type":"code","source":"eval_metric(glove_history, 'accuracy')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8bf5090f-9fd3-477e-92a3-c38445b9f735","_uuid":"b2d7e0010319448b7a47b45599c42f923c79b418","trusted":true},"cell_type":"code","source":"glove_results = test_model(glove_model, X_train_seq_trunc, y_train_oh, X_test_seq_trunc, y_test_oh, 3)\nprint('/n')\nprint('Test accuracy of word glove model: {0:.2f}%'.format(glove_results[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e1c3cf1e-529c-40f3-bf47-16f13338ce8d","_uuid":"1ff4afd68e9ea50de43d22cd64e59dbba4a09409"},"cell_type":"markdown","source":"The model overfits fast, after 3 epochs. Howerver, the validation accuracy is lower as compared to embeddings trained on the training data. \n\nNow, we will analyse the results for training the embeddings with the same number of dimensions as the GloVe data."},{"metadata":{},"cell_type":"markdown","source":"### Training word embeddings with more dimensions"},{"metadata":{"_cell_guid":"d6b74e20-1320-4095-889d-c79489567a7e","_uuid":"2f84b6e5a84ad918f1a7bef9eaf5c36a68748b99","trusted":true},"cell_type":"code","source":"emb_model2 = models.Sequential()\nemb_model2.add(layers.Embedding(nb_words, dim_glove, input_length=max_len))\nemb_model2.add(layers.Flatten())\nemb_model2.add(layers.Dense(20, activation=htanh))\nemb_model2.add(layers.Dense(3, activation='softmax'))\nemb_model2.summary()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4a5ddb8d-84d8-46cd-8679-76ee052b81a5","_uuid":"6856e89e9bb3e295b5f2319200fe1b18e1d5b02b","trusted":true},"cell_type":"code","source":"emb_history2 = deep_model(emb_model2, X_train_emb, y_train_emb, X_valid_emb, y_valid_emb)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f983431c-96bb-4857-ad43-8dd9b2a614cc","_uuid":"d48a27e29d1997e60636a87c40028267b52200cf","trusted":true},"cell_type":"code","source":"eval_metric(emb_history2, 'loss')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1b6c8732-e8b5-44e2-9612-1397b72c8687","_uuid":"b18a0656e095287a27442f8888f0bcf327682aeb","trusted":true},"cell_type":"code","source":"eval_metric(emb_history2, 'accuracy')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"16994fa6-9c11-4d51-afe6-02d6065bc50e","_uuid":"58ffb6fc61e6117fbd7267d43702e5016959fd8c","trusted":true},"cell_type":"code","source":"emb_results2 = test_model(emb_model2, X_train_seq_trunc, y_train_oh, X_test_seq_trunc, y_test_oh, 3)\nprint('/n')\nprint('Test accuracy of word embedding model 2: {0:.2f}%'.format(emb_results2[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1c8cefad-3b74-488e-a324-effaa567daaf","_uuid":"8d3f8d489dd78323246b0814c75e7b081ebbe0ae"},"cell_type":"markdown","source":"This result is very close to the model with 50-dimensional word embeddings. So there is no strong improvement. "},{"metadata":{},"cell_type":"markdown","source":"# Conclusion for model\nThe best result is achieved with 50-dimensional word embeddings that are trained on the available data. This even outperforms the use of word embeddings that were trained on a much larger Twitter corpus."},{"metadata":{},"cell_type":"markdown","source":"# Visualizing Word Vectors with t-SNE"},{"metadata":{},"cell_type":"markdown","source":"# Word 2 Vec\n\nThe Word to Vec model produces a vocabulary, with each word being represented by an n-dimensional numpy array (100 values in this example)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=200, workers=4)\nmodel.wv['flights']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tsne_plot(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(16, 16)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\ntsne_plot(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A more selective model\nmodel = word2vec.Word2Vec(corpus, size=100, window=20, min_count=500, workers=4)\ntsne_plot(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A less selective model\nmodel = word2vec.Word2Vec(corpus, size=100, window=20, min_count=100, workers=4)\ntsne_plot(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.most_similar('flights')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.most_similar('right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion for visualization using t-SNE\n\nIt is hard to visualize these words using t-SNE. The better way to look is most similar words"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}