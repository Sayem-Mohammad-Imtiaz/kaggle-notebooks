{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Introduction**\n\nBitcoins are one of the largest and most well-known cryptocurrencies in the world. It first appeared in 2009 and has grown exponentially since gaining mainstream appeal. Over the years a is a wide range of opinions about the currency formed. Some consider it an investment, a fad and I simply consider it an eco-friendly alternative to burning money.\n\nThis notebook has two goals:\n\n* Finishing an exploratory data analysis of the Bitcoin price\n* Creating a SARIMAX univariate prediction of the Bitcoin price ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Library and data import**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport missingno as msno\n\nimport datetime\nfrom pandas.tseries.offsets import DateOffset\nfrom pandas.tseries.offsets import MonthEnd\n\nimport statsmodels.api as sm\nfrom scipy import stats\nimport itertools\n\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.style.use(\"fivethirtyeight\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"../input/bitcoin-historical-data/bitstampUSD_1-min_data_2012-01-01_to_2020-04-22.csv\"\ndf = pd.read_csv(path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Exploratory data analysis**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each row presents a 1-minute trade interval. The NaN fields represent timestamps without any trades occurring. Most of the NaN fields are present very early in the dataset. Bitcoins were mostly unknown in this period, which means no a lot of trades.\n\nLooking at the Timestamp column, we need to convert it from seconds.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Date conversion\ndf[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], unit=\"s\",origin=\"unix\")","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"ax, fig = plt.subplots(figsize=(10,5))\n\nmsno.bar(df)\n\nax.text(0.07,1, s=\"Missing data check\", fontsize=32, weight=\"bold\", alpha=0.75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.set_index(df[\"Timestamp\"],drop=True,inplace=True)\n#Hour\ndf_hour = df.resample(\"h\").mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am going to resample the data to an hourly format and analyse it further as such.","execution_count":null},{"metadata":{"tags":[],"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"ax, fig = plt.subplots(figsize = (10,5))\n\nplt.plot(df_hour[\"Open\"], label=\"Opening price\")\nplt.plot(df_hour[\"Close\"], label=\"Closing price\")\n\nplt.xticks(alpha=0.75, weight=\"bold\")\nplt.yticks(alpha=0.75, weight=\"bold\")\n\nplt.xlabel(\"Date\",alpha=0.75, weight=\"bold\")\nplt.ylabel(\"Price\",alpha=0.75, weight=\"bold\")\n\nplt.legend()\n\nplt.text(x=datetime.date(2011, 6, 30), y=22000, s=\"Hourly opening and closing price of Bitcoin (2012-2020)\",\nfontsize=15, weight=\"bold\", alpha=0.75)\nplt.text(x=datetime.date(2011, 6, 30), y=21000, s=\"There is no major difference between the mean opening and closing prices.\",fontsize=12, alpha=0.75)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To further highlight the differences between the opening and closing reads, I am going to create a column that represents the differences between these prices on the hourly dataset. The modification will allow the data to be clearly visually presented.","execution_count":null},{"metadata":{"tags":[],"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Data\n\ndf_hour[\"hourly_diff\"] = df_hour[\"Close\"] - df_hour[\"Open\"]\n\n#Plot\nax, fig = plt.subplots(figsize = (10,5))\n\nplt.plot(df_hour[\"hourly_diff\"])\n\nplt.xticks(alpha=0.75, weight=\"bold\")\nplt.yticks(alpha=0.75, weight=\"bold\")\n\nplt.xlabel(\"Date\",alpha=0.75, weight=\"bold\")\nplt.ylabel(\"Price\",alpha=0.75, weight=\"bold\")\n\nplt.legend()\n\nplt.text(x=datetime.date(2011, 6, 30), y=25, s=\"Hourly difference between the opening and closing Bitcoin prices (2012-2020)\",\nfontsize=15, weight=\"bold\", alpha=0.75)\nplt.text(x=datetime.date(2011, 6, 30), y=22, s=\"Larger price fluctuations started happening in 2018 when Bitcoin started gaining mainstream appeal.\",fontsize=12, alpha=0.75)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"ax, fig = plt.subplots(figsize = (10,5))\n\nplt.plot(df_hour[\"Weighted_Price\"])\n\nplt.xticks(alpha=0.75, weight=\"bold\")\nplt.yticks(alpha=0.75, weight=\"bold\")\n\nplt.xlabel(\"Date\",alpha=0.75, weight=\"bold\")\nplt.ylabel(\"Price\",alpha=0.75, weight=\"bold\")\n\nplt.legend()\n\nplt.text(x=datetime.date(2011, 6, 30), y=22000, s=\"Weighted Price for Bitcoins (2012-2020)\",\nfontsize=15, weight=\"bold\", alpha=0.75)\nplt.text(x=datetime.date(2011, 6, 30), y=21000, s=\"This is the main metric that we would like to predict.\",fontsize=12, alpha=0.75)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The metric that makes the most sense to predict would be the weighted prices of bitcoins. Again, I am going to use the hourly resampled dataset from now on and the prediction will be done using that dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Most time-series data is composed of three elements:\n\n* Season - a repeating cycle in the series\n* Trend - an upwards or downwards movement in the series\n* Residual or noise - random variation in the data\n\nSome literature also adds \"level\" to the decomposition. A \"level\" can be described as the average value in the series. \n\n**Seasonal decomposition** can be a great way structured approach to a time series problem. The acquired information is useful when thinking about the specific problem and planing the future approach to the model. I am going to use the automatic seasonal decomposition tool and plot the results.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Seasonal Decompose\nax, fig = plt.subplots(figsize=(15,8), sharex=True)\n\ndf_month = df.resample(\"M\").mean()\ndec = sm.tsa.seasonal_decompose(df_month[\"Weighted_Price\"])\n\n\nplt.subplot(411)\nplt.plot(df_hour[\"Weighted_Price\"], label=\"Weighted Price\")\nplt.title(\"Observed\",loc=\"left\", alpha=0.75, fontsize=18)\n\nplt.subplot(412)\nplt.plot(dec.trend, label=\"Trend\")\nplt.title(\"Trend\",loc=\"left\", alpha=0.75, fontsize=18)\n\nplt.subplot(413)\nplt.plot(dec.seasonal, label=\"Seasonal\")\nplt.title(\"Seasonal\",loc=\"left\", alpha=0.75, fontsize=18)\n\nplt.subplot(414)\nplt.plot(dec.resid, label=\"Residual\")\nplt.title(\"Residual\",loc=\"left\", alpha=0.75, fontsize=18)\nplt.tight_layout()\n\nplt.text(x=datetime.date(2011, 6, 30), y=63000, s=\"Seasonal time series decomposition\",fontsize=24, weight=\"bold\", alpha=0.75)\nplt.text(x=datetime.date(2011, 6, 30), y=60700, s=\"Decomposition of the weighted price data ranging from 2012 to 2020.\",fontsize=18, alpha=0.75)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plot above we can see a few things:\n\n* Bitcoin prices are facing a strong upward trend.\n* There are some seasonal elements to the price (+- 500)\n* Most of the noise in the data was generated during the \"hype\" phase of 2018-2019.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Stationarity**\n\n**What is stationarity?** Stationarity in (plain English) means that the statistical properties of a ceratin variable do not change over time.\n\n**Why you do it?** It simplifies the whole analytics process and allows for a structured approach to the problem.\n\n**Do I need it for SERIMAX?** Not really. Statsmodels SARIMAX has a (by default enabled option) that enforces stationarity. However, it is a great tool to analyse the data and should be used.\n\nIts common practice to use it and most models nowadays assume that the data is stationary. To determine the stationarity of data I am going to use the Dickey-Fuller test. The DF tests a null hypothesis that a unit root is present in an autoregressive model. If the value is less then <0.05 then the data is stationary.","execution_count":null},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"print(\"Dicky-Fuller stationarity test - p: %f\" % sm.tsa.adfuller(df_month[\"Weighted_Price\"])[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Box-Cox**\n\nBox-Cox comes from the family of power transformations and is often used as a mean to stabilize variance in a dataset. It is indexed by lambda and in certain times can be used as a differencing technique.","execution_count":null},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"#Box-Cox\n\ndf_month[\"Box-Cox\"], _ = stats.boxcox(df_month[\"Weighted_Price\"])\nprint(\"Dicky-Fuller stationarity test - p: %f\" % sm.tsa.adfuller(df_month[\"Box-Cox\"])[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Panads diff()**\n\nPandas library provides an option for automatic differencing with diff().","execution_count":null},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"#Automatic Differencing\n\nfirst_diff = df_month[\"Weighted_Price\"].diff()\nprint(\"Dicky-Fuller stationarity test - p: %f\" % sm.tsa.adfuller(first_diff[1:])[1])\nprint(\"This series is stationary\")\n\n\ndf_month[\"Auto_Diff\"] = first_diff","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets plot the seasonal decomposition and AC & PAC on the stationary data so we can see the results.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Data\nseasonal_dec = sm.tsa.seasonal_decompose(df_month[\"Auto_Diff\"][1:])\n\n#Seasonal Decompose on stationary series\nax, fig = plt.subplots(figsize=(15,8), sharex=True)\n\ndf_month = df.resample(\"M\").mean()\ndec = sm.tsa.seasonal_decompose(df_month[\"Weighted_Price\"])\n\n\nplt.subplot(411)\nplt.plot(df_hour[\"Weighted_Price\"], label=\"Weighted Price\")\nplt.title(\"Observed\",loc=\"left\", alpha=0.75, fontsize=18)\n\nplt.subplot(412)\nplt.plot(seasonal_dec.trend, label=\"Trend\")\nplt.title(\"Trend\",loc=\"left\", alpha=0.75, fontsize=18)\n\nplt.subplot(413)\nplt.plot(seasonal_dec.seasonal, label=\"Seasonal\")\nplt.title(\"Seasonal\",loc=\"left\", alpha=0.75, fontsize=18)\n\nplt.subplot(414)\nplt.plot(seasonal_dec.resid, label=\"Residual\")\nplt.title(\"Residual\",loc=\"left\", alpha=0.75, fontsize=18)\nplt.tight_layout()\n\nplt.text(x=datetime.date(2011, 6, 30), y=63000, s=\"Seasonal decomposition on stationary time series\",fontsize=24, weight=\"bold\", alpha=0.75)\nplt.text(x=datetime.date(2011, 6, 30), y=60700, s=\"Decomposition of the stationary weighted price data ranging from 2012 to 2020.\",fontsize=18, alpha=0.75)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"ax, fig = plt.subplots(figsize=(15,10))\n\nplt.subplot(411)\nx = sm.graphics.tsa.plot_acf(first_diff[1:], ax=plt.gca())\nplt.subplot(412)\ny = sm.graphics.tsa.plot_pacf(first_diff[1:],ax=plt.gca())\nplt.tight_layout()\n\ngc.collect()\ndel x,y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **SARIMAX**\n\nSARIMAX is short for the Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors model. It is a widely used forecasting method for univariate time-series forecasting SARIMAX can handle both trends and seasonality in data. This makes it an excellent choice in forecasting data that has both of these elements.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"###SARIMAX###\n\n#Constructs all possible parameter combinations.\np = d = q = range(0,2)\npdq = list(itertools.product(p,d,q))\n\nseasonal_pdq = [(x[0],x[1],x[2],12) for x in list(itertools.product(p,d,q))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sarimax_function(data,pdq,s_pdq):\n\n    \"\"\"\n    The function uses a brute force approach to apply all possible pdq combinations and evaluate the model\n    \"\"\"\n\n    result_list = []\n    for param in pdq:\n        for s_param in s_pdq:\n\n            model = sm.tsa.statespace.SARIMAX(data, order=param, seasonal_order=s_param,\n            enforce_invertibility=False,enforce_stationarity=False)\n\n            results = model.fit()\n            result_list.append([param,s_param,results.aic])\n            print(\"ARIMA Parameters: {} x: {}. AIC: {}\".format(param,s_param,results.aic))\n\n    return result_list,results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The evaluation metric for the model selection was AIC (Akaike Information Criterion -> AIC=ln (sm2) + 2m/T). As a model selection tool, AIC has some limitations as it only provides a relative evaluation of the model. However, it is an excellent metric for checking the general quality of a model such as SARIMAX.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets start training.","execution_count":null},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"result_list,results = sarimax_function(df_month[\"Weighted_Price\"],pdq,seasonal_pdq)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dataframe of all results and parameters.\n\nresults_dataframe = pd.DataFrame(result_list, columns=[\"dpq\",\"s_dpq\",\"aic\"]).sort_values(by=\"aic\")\nresults_dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"model = sm.tsa.statespace.SARIMAX(df_month[\"Weighted_Price\"], order=(0, 1, 1), seasonal_order=(1, 1, 1, 12),\n            enforce_invertibility=False,enforce_stationarity=False).fit()\nprint(model.summary().tables[1])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Residual analysis\nax, fig = plt.subplots(figsize = (10,5))\n\nmodel.resid.plot(label=\"Residual\")\n\nplt.xticks(alpha=0.75, weight=\"bold\")\nplt.yticks(alpha=0.75, weight=\"bold\")\n\nplt.xlabel(\"Date\",alpha=0.75, weight=\"bold\")\nplt.ylabel(\"Price\",alpha=0.75, weight=\"bold\")\n\nplt.legend()\n\nplt.text(x=datetime.date(2011, 6, 30), y=7200, s=\"Residual Analysis\",\nfontsize=15, weight=\"bold\", alpha=0.75)\nplt.text(x=datetime.date(2011, 6, 30), y=6700, s=\"Analaysis of the residual values for the best model acording to AIC.\",fontsize=12, alpha=0.75)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"x = model.plot_diagnostics(figsize=(18, 8))\n\ngc.collect()\ndel x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_month_prediction = df_month[[\"Weighted_Price\"]]\n\ndf_month_prediction[\"Forcasting\"] = model.predict(start=pd.to_datetime(\"2011-12-31\"), end=pd.to_datetime(\"2020-04-30\"))","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"ax, fig = plt.subplots(figsize = (10,5))\n\nplt.plot(df_month_prediction[\"Forcasting\"], ls=\"--\", label=\"Prediction\")\nplt.plot(df_month_prediction[\"Weighted_Price\"], label=\"Actual Data\")\n\nplt.xticks(alpha=0.75, weight=\"bold\")\nplt.yticks(alpha=0.75, weight=\"bold\")\n\nplt.xlabel(\"Date\",alpha=0.75, weight=\"bold\")\nplt.ylabel(\"Price\",alpha=0.75, weight=\"bold\")\n\nplt.legend()\n\nplt.text(x=datetime.date(2011, 6, 30), y=18000, s=\"Forcasting test of SARIMAX\",\nfontsize=18, weight=\"bold\", alpha=0.75)\nplt.text(x=datetime.date(2011, 6, 30), y=17000, s=\"Prediction testing of the best SARIMAX model.\",fontsize=15, alpha=0.75)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"#Datetimeindex dates I want to predict\n\nfuture_dates = [df_month_prediction.index[-1] + DateOffset(months = x)for x in range(1,12)]\nfuture_dates = pd.to_datetime(future_dates)  +  MonthEnd(0)\nfuture = pd.DataFrame(index=future_dates)\ndf_month_prediction = pd.concat([df_month_prediction,future])\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"#Prediction\n\ndf_month_prediction[\"Future_forcast\"] = model.predict(start=pd.to_datetime(\"2020-03-31\"),end=pd.to_datetime(\"2021-03-31\"))\n\npred = model.get_prediction(start=pd.to_datetime(\"2020-03-31\"),end=pd.to_datetime(\"2021-03-31\"))\npred_ci = pred.conf_int()\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"ax, fig = plt.subplots(figsize=(10,5))\n\nplt.plot(df_month_prediction[\"Weighted_Price\"], label=\"Actual\")\nplt.plot(df_month_prediction[\"Future_forcast\"],ls=\"--\", label=\"Prediction\")\n\nplt.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\nplt.legend()\n\n\nplt.xticks(alpha=0.75, weight=\"bold\")\nplt.yticks(alpha=0.75, weight=\"bold\")\n\nplt.xlabel(\"Date\",alpha=0.75, weight=\"bold\")\nplt.ylabel(\"Price\",alpha=0.75, weight=\"bold\")\n\nplt.legend()\n\nplt.text(x=datetime.date(2011, 6, 30), y=19500, s=\"SARIMAX Forcasting\",\nfontsize=18, weight=\"bold\", alpha=0.75)\nplt.text(x=datetime.date(2011, 6, 30), y=18500, s=\"Prediction of the weighted price for the next 12 months.\",fontsize=15, alpha=0.75)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Conclusion**\n\nSARIMAX is a great and interesting method to predict univariate time-series data. It is a great way to refresh your knowledge on time-series data and strengthen the number of prediction models you know (because there is no free lunch in ML). I had fun making it and I hope you had fun reading it.\n\nThank you for your time.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}