{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pipelines do SkLearn\n\nHoje vamos aprender uma ferramenta poderosíssima do `sklearn`: _pipelines_ (que em tradução literal é _oleoduto_, mas eu prefiro algo como _linha de montagem_). \n\nAs pipelines nos permitem colocar em sequência todos o passos do nosso projeto de Machine Learning e também automatizar o pré-processamento, treinamento e afinação (_tuning_) de hiperparâmetros. Além disso, elas nos permitem fazer um `GridSearch` não só nos hyperparâmetros de um determinado modelo, mas também nos parâmetros que usamos no pré-processamento.\n\n- _Será que eu preencho os valores nulos com a média ou mediana?_\n- _Será que eu uso ou não essa determinada feature?_\n\nEssas entre outras perguntas são muito comuns quando estamos lidando com um projeto. As pipelines permitem que a gente ache exatamente qual é o melhor score que nossos modelos podem ter lidando com essas perguntas através de parâmetros que passamos para um `GridSearch`.","metadata":{}},{"cell_type":"markdown","source":"# Importantando bibliotecas e dados\n\nPrimeiro, como sempre, vamos importar as bibliotecas e dados que vamos utilizar.\n\nEsse tutorial foi inspirado no segundo capítulo do livro _Hands-On Machine Learning_, o notebook do capítulo pode ser encontrado no [GitHub](https://github.com/ageron/handson-ml2). Então vamos importar o data set utilizado por ele.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Contextualizando, temos dados de casas no estado da California e seus preços, que é o que queremos predizer.","metadata":{}},{"cell_type":"code","source":"input_path = '../input/california-housing-prices/housing.csv'\ndf = pd.read_csv(input_path)\ndf.sample(7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vemos que existem alguns valores nulos na coluna `total_bedrooms`. ","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pré-processamento sem pipelines\n\nAntes de ver como usar pipelines, vamos demonstrar como faríamos o pré-processamento dos dados sem elas.\nOs passos do pré-processamento serão:\n- Preencher os valores nulos\n- Normalizar e padronizar os valores numéricos\n- Codificar os valores categóricos utilizando One-Hot Encoding","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\ndef fill_na(df, strategy='median'):\n\n    num_values = list(df.columns[:-1])\n\n    imputer = SimpleImputer(strategy=strategy)\n    imputer.fit(df[num_values])\n    df[num_values] = imputer.transform(df[num_values])\n    return df\n\n\ndf = fill_na(df)\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\ndef scale(df):\n    num_values = list(df.columns[:-1])\n\n    scaler = StandardScaler()\n    scaler.fit(df[num_values])\n    df[num_values] = scaler.transform(df[num_values])\n    return df\n\n\ndf = scale(df)\ndf.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\ndef encode(df):\n    cat_values = ['ocean_proximity']\n    \n    encoder = OneHotEncoder()\n    encoder.fit(df[cat_values])\n    columns = [cat_values[0] + '_' + cat_name for cat_name in encoder.categories_][0]\n    encoded = pd.DataFrame(encoder.transform(df[cat_values]).toarray(), columns=columns)\n    return pd.concat([df, encoded.astype(int)], axis=1).drop('ocean_proximity', axis=1)\n\n\ndf = encode(df)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Então, basicamente criei três funções, uma para cada tarefa e vamos modificando o `DataFrame` original até ele se tornar a matriz `X` que vamos passar para nosso modelo de Machine Learning (usando `.fit`).","metadata":{}},{"cell_type":"markdown","source":"# Pipelines\n\nAgora vejamos como fazer o mesmo processo, mas utilizando Pipelines. \n\nPara criar um `Pipeline`, importamos esse objeto e o instanciamos passando uma lista de tuplas. \n- O primeiro valor da tupla é o nome daquele passo na nossa linha de montagem;\n- O segundo valor é um `Transformer` do `sklearn`. Ou seja, deve ser um objeto que possua os métodos `transform` e `fit` (pelo menos).\n\nVamos falar mais em detalhes de `Transformers` mais para frente.\n\n**_Nota_**: o mais correto é dizer que todos os passos devem ser `Transformers`, exceto o último, que pode ser um `Estimator` (ou seja, possuir o método `predict`).\n\nApós criar esse `Pipeline`, podemos chamar os seus métodos `fit` e `transform`. Basicamente, o que ele faz é chamar em sequência cada `Transformer` passando para o próximo a saída retornada pelo anterior. ","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\n\nnum_pipeline = Pipeline([\n     ('fillna', SimpleImputer(strategy='median')),\n     ('scaler', StandardScaler()),\n])\n\nnum_values = list(df.columns[:9])\nnum_pipeline.fit(df[num_values])\nnum_df_transformed = num_pipeline.transform(df[num_values])\nnum_df_transformed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compare com o `DataFrame` que geramos pelo pré-processamento anteriormente e você verá que o resultado é o mesmo. A diferença é que aqui temos diretamente um `ndarray` do `numpy` ao invés de um `DataFrame` do `pandas` (na prática, não muda muito, pois o método `fit` de um estimador transforma internamente `DataFrames` em `ndarrays`).","metadata":{}},{"cell_type":"markdown","source":"# Transformações diferentes para colunas diferentes\n\nBem, agora precisamos fazer o One-Hot Encoding na coluna `ocean_proximity` e juntar com essa matriz que o `Pipeline` gerou, certo?\n\nInfelizmente, apenas com `Pipelines` isso não é possível, pois o `Pipeline` não considera as diferentes colunas de uma matriz. Ele apenas aplica as transformações (é por isso que na hora de dar fit e transform, eu chamo `df[num_values]` ao invés de `df` inteiro).\n\nEntretanto, o objeto `ColumnTransformer` nos permite fazer exatamente o que precisamos: transformar um conjunto de colunas de um jeito e um outro conjunto de colunas de outro e juntar essas colunas em uma única matriz.\n\nEles funcionam muito similar a `Pipelines`, a diferença é que a tupla recebe um valor a mais: as colunas em que aquela transformação será aplicada.","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\ncat_values = ['ocean_proximity']\n\nfull_pipeline = ColumnTransformer([\n    ('numeric', num_pipeline, num_values), \n    ('categorical', OneHotEncoder(), cat_values)\n])\n\ndf = pd.read_csv(input_path)\nX = full_pipeline.fit_transform(df)\nX","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note que o objeto `Pipeline` também é um `Transformer` (pois tem os métodos `fit` e `transform`), então podemos considerá-lo como se fosse uma caixa preta e passar para nosso `ColumnTransformer`.\n\nE é isso, temos exatamente a mesma coisa que tínhamos feito antes, mas de uma forma muuito mais prática (claro, entender como `Pipelines` funcionam talvez não seja exatamente fácil, mas com certeza recompensa muito).\n\nEsse tutorial poderia parar por aqui. Mas vamos ver algumas outras coisinhas que podemos fazer utilizando `Pipelines`.","metadata":{}},{"cell_type":"markdown","source":"# Criando seus próprios Transformers\n\nO sklearn nos fornece vários transformadores nativos, mas podemos querer criar outros. Um exemplo muito prático é fazer um `Transformer` que gere novas features do seu processo de Feature Engineering. Outro exemplo seria um `Transformer` que matém apenas determinadas features do Data Set (Feature Selection).\n\nVamos mostrar uma mistura desses dois exemplos. Vou criar um `Transformer` que cria duas novas features e tem um parâmetro booleano se devemos criar ou não uma terceira nova feature (assim poderemos testar mais para frente se incluir ou não incluir essa feature é melhor ou não). ","metadata":{}},{"cell_type":"markdown","source":"Para criar um `Transformer`, basicamente basta criar uma classe que implemente os métodos `fit` e `transform`, como já falei (sim, não precisa herdar de nenhuma classe do `sklearn` com o nome `Transformer` ou coisa do tipo. Mostrei isso no Apêndice A do notebook).\n\nEntretanto, existem duas classes que costumamos herdar, pois elas nos ajudam:\n- `TransformerMixin` cria para nós um método `fit_transform` automaticamente usando nossos método `fit` e `transform`; e\n- `BaseEstimator` cria para nós os métodos `set_params` e `get_params` que são utilizados pelo `GridSearchCV` internamente.","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass CreateNewFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room=True):\n        self.rooms_ix = 3\n        self.bedrooms_ix = 4\n        self.population_ix = 5\n        self.households_ix = 6\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    \n    def fit(self, X, y=None):\n        return self # não fazemos nada\n    \n    def transform(self, X):\n        rooms_per_household = X[:, self.rooms_ix] / X[:, self.households_ix]\n        population_per_household = X[:, self.population_ix] / X[:, self.households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, self.bedrooms_ix] / X[:, self.rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n        return np.c_[X, rooms_per_household, population_per_household]\n\nnew_features = CreateNewFeatures(add_bedrooms_per_room=True)\nhousing_extra_features = new_features.transform(df.values)\nhousing_extra_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"E pronto, temos um `Transformer` feito por nós mesmos. O limite do que se pode fazer é basicamente definido pela nossa imaginação =)\n\nAgora podemos utilizar esses `Transformers` na nossa `Pipeline` de antes para deixá-la mais sofisticada ainda.","metadata":{}},{"cell_type":"code","source":"num_pipeline = Pipeline([\n     ('fillna', SimpleImputer(strategy='median')),\n     ('feature_creator', CreateNewFeatures(add_bedrooms_per_room=True)),\n     ('scaler', StandardScaler()),\n])\n\n\ndf_transformed = num_pipeline.fit_transform(df[num_values])\ndf_transformed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessing_pipeline = ColumnTransformer([\n    ('numeric', num_pipeline, num_values),\n    ('categorical', OneHotEncoder(), cat_values)\n])\n\ndf = pd.read_csv(input_path)\nX = preprocessing_pipeline.fit_transform(df)\nX","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"E _voilà_, temos novamente uma matriz prontinha para qualquer modelo de Machine Learning utilizar.","metadata":{}},{"cell_type":"markdown","source":"# Pipelines e Hyperparameter Tuning\n\nPor fim, uma das funcionalidade que eu mais acho incrível dos `Pipelines` é a capacidade usá-los junto do `GridSearchCV` (ou `RandomizedSearchCV`) e podermos passar parâmetros dos transformadores em si para serem testados.\n\nVamos dar uma olhada:","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\n\n# Dividindo o DataFrame em matrizes X (de features) e y (o target)\n# Além disso, definimos quais colunas são numéricas e quais são categóricas (para usar no ColumnTransformer)\ntarget = 'median_house_value'\nX = df.loc[:, df.columns != target]\ny = df[target]\nnum_values = np.delete(X.columns, np.where(X.columns == 'ocean_proximity'))\ncat_values = ['ocean_proximity']\n\n# Agora definimos nossas Pipelines\nnum_pipeline = Pipeline([\n     ('fillna', SimpleImputer(strategy='median')),\n     ('feature_creator', CreateNewFeatures(add_bedrooms_per_room=True)),\n     ('scaler', StandardScaler()),\n])\npreprocessing_pipeline = ColumnTransformer([\n    ('numeric', num_pipeline, num_values),\n    ('categorical', OneHotEncoder(categories=[df['ocean_proximity'].unique()]), cat_values)\n])\nfinal_pipe = Pipeline([\n    ('preprocessing', preprocessing_pipeline),\n    ('ridge', Ridge())\n])\n\n# Veja que a última Pipeline tem dois passos: o pré-processamento e o estimador (o Ridge, que é um modelo linear)\n\n# Agora tem uma parte que pode ser um pouco complexa, mas basta enteder o que está dentro de o que\n# Na hora de passar os nomes dos parâmetros para o GridSearch, ele utiliza os nomes que passamos nas tuplas \n# o utiliza dois underlines para se referir a um parâmetro daquele objeto (seria análogo ao ponto que usamos normalmente)\nparams = {\n    'preprocessing__numeric__feature_creator__add_bedrooms_per_room' : [False, True],\n    'preprocessing__numeric__fillna__strategy': ['median', 'mean'],\n    'ridge__alpha' : [0.1, 1, 10],\n}\n\n# De resto, é tudo igual, passamos o estimador (esse Pipeline é um estimador pois o último passo é o Ridge, que é um estimador)\n# e passamos os parâmetros, além de outros parâmetros que normalmente usamos em um GridSearch\ngs = GridSearchCV(final_pipe, params, cv=5, n_jobs=-1)\ngs.fit(X, y)\ngs.best_params_ # E temos os melhores parâmetros automaticamente =)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Na minha opinião, a parte mais difícil de entender é esses nomes enormes de parâmetros e entender as abstrações de um `Pipeline` (pensar que o `Pipeline` herda o tipo do objeto no último passo - um `Estimator` ou `Transformer`). Por isso, vou deixar uma imagem para ficar mais claro.\n\n<img src=\"https://github.com/Giatroo/BeeData_Pipelines-in-Sklearn/blob/main/Arvore-de-Pipelines.jpg?raw=true\" alt=\"drawing\" width=\"600\"/>\n\nAlém disso, para saber quais são os parâmetros de um `Pipeline`, podemos utilizar o método `get_params`, que nos retorna um dicionário com cada atributo e seu valor padrão. Olhe o Apêndice B para ver o retorno desse método para o nosso `Pipeline` final.","metadata":{}},{"cell_type":"markdown","source":"# Apêndice A","metadata":{}},{"cell_type":"code","source":"class MyTransformer():\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return np.ones_like(X)\n        \ndf = pd.read_csv(input_path)\n\nmy_pipeline = Pipeline([\n    ('imputer', SimpleImputer()),\n    ('test', MyTransformer()), \n])\n\nmy_pipeline.fit_transform(df[num_values])\n# Mesmo não herdando de nada, o sklearn reconhece nossa classe como um Transformer =)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Apêndice B","metadata":{}},{"cell_type":"code","source":"final_pipe.get_params()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusão e Aprofundamento\n\nPor hoje é tudo =)\n\nEspero que você tenha gostado e aproveitado bastante. \n\nConcluímos que os `Pipelines` são uma ferramenta incrível que podemos utilizar para agilizar muito nossa vida como cientistas de dados. Além disso, aprendemos um pouco sobre como o `sklearn` opera (os diferentes tipos de objetos dele) e como criar nossos próprios `Transformers`, além de aplicar diferentes transformações a diferentes colunas utilizando `ColumnTransformers`.\n\nPara se aprofundar e revisar, recomendo dar uma buscada no livro de onde eu tirei esses exemplos, que já citei, além de dar uma olhada nos seguintes recursos:\n\n[Livro - Hands On Machine Learning](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n\n[Guia do usuário - Pipeline and composite estimators](https://scikit-learn.org/stable/modules/compose.html#combining-estimators)\n\n[Documentação Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n\n[Documentação make_pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline)\n\n[Documentação ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer)\n\n[Documentação TransformedTargetRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.compose.TransformedTargetRegressor.html#sklearn.compose.TransformedTargetRegressor)\n\n[Tutorial do towards data science](https://towardsdatascience.com/pipelines-custom-transformers-in-scikit-learn-the-step-by-step-guide-with-python-code-4a7d9b068156)\n\n\n\n**~Lucas Paiolla, 23/04/2021**","metadata":{}}]}