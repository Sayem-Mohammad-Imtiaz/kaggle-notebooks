{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**I found out from this dataset that \"time\" feature should not be used as a predictor because it causes problem when applying different feature selection techniques. When using lasso the variables are affected by this feature, but when using correlation technique it shows that this variable is negativly correlated with target variable. Although my model gets a better prediction accurecy when using time as a predictor, i still decided to remove it**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:14:01.526942Z","iopub.execute_input":"2021-07-06T09:14:01.527284Z","iopub.status.idle":"2021-07-06T09:14:01.531775Z","shell.execute_reply.started":"2021-07-06T09:14:01.527244Z","shell.execute_reply":"2021-07-06T09:14:01.530667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\ndf.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-06T09:14:01.54369Z","iopub.execute_input":"2021-07-06T09:14:01.544036Z","iopub.status.idle":"2021-07-06T09:14:01.565884Z","shell.execute_reply.started":"2021-07-06T09:14:01.544007Z","shell.execute_reply":"2021-07-06T09:14:01.56466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:14:01.567677Z","iopub.execute_input":"2021-07-06T09:14:01.568069Z","iopub.status.idle":"2021-07-06T09:14:01.5874Z","shell.execute_reply.started":"2021-07-06T09:14:01.568031Z","shell.execute_reply":"2021-07-06T09:14:01.586507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:14:01.588969Z","iopub.execute_input":"2021-07-06T09:14:01.589338Z","iopub.status.idle":"2021-07-06T09:14:01.600529Z","shell.execute_reply.started":"2021-07-06T09:14:01.5893Z","shell.execute_reply":"2021-07-06T09:14:01.599331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:14:01.603039Z","iopub.execute_input":"2021-07-06T09:14:01.603805Z","iopub.status.idle":"2021-07-06T09:14:01.663046Z","shell.execute_reply.started":"2021-07-06T09:14:01.603745Z","shell.execute_reply":"2021-07-06T09:14:01.66208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Use pca to check for multicollinearity in dataset and reduce the dimensions to only two variables\n\n#### Scale data, create a copy of df and perform pca","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc= StandardScaler()\ndf_copy=df.copy()\n\n\ndf_copy=sc.fit_transform(df_copy)\n\nfrom sklearn.decomposition import PCA\npca=PCA(n_components=2)\ndf_pca=pca.fit_transform(df_copy)\n\nprincipaldF=pd.DataFrame(data=df_pca,columns=['PC1','PC2'])\nfinaldf = pd.concat([principaldF, df[['DEATH_EVENT']]], axis = 1)\n\nfinaldf.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-06T09:14:01.664956Z","iopub.execute_input":"2021-07-06T09:14:01.665332Z","iopub.status.idle":"2021-07-06T09:14:01.6861Z","shell.execute_reply.started":"2021-07-06T09:14:01.665292Z","shell.execute_reply":"2021-07-06T09:14:01.685369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### the variance captured by pca is low, this shows that variables are not correlated with each other and the reduced dimensions dont capture the variance in dataset","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nexp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n\npx.area(\n    x=range(1, exp_var_cumul.shape[0] + 1),\n    y=exp_var_cumul,\n    labels={\"x\": \"Components\", \"y\": \"Explained Variance\"}\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:14:01.687075Z","iopub.execute_input":"2021-07-06T09:14:01.687461Z","iopub.status.idle":"2021-07-06T09:14:01.859562Z","shell.execute_reply.started":"2021-07-06T09:14:01.687427Z","shell.execute_reply":"2021-07-06T09:14:01.858856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Before preceeding with lasso and model prediction a exploatry analysis is done\n\n#### First check for amount of deaths between genders","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"darkgrid\", {\"axes.facecolor\": \"0.95\"})\nfig,ax = plt.subplots(1, 1,figsize = (15,6))\nsns.countplot(x='sex',\n              hue = 'DEATH_EVENT', \n              data=df,\n              palette=[\"cornflowerblue\", \"khaki\"])\n\n\n\nax.legend([\"No\",\"Yes\"], \n              bbox_to_anchor=(1,1), \n              title='Survival')\n\nax.set_xticklabels(['Male','Female'],fontdict= { 'fontsize': 10, 'fontweight':'bold'})\n# Customize the axes and title\nax.set_title(\"Death by heart failure amoung genders\",fontdict= { 'fontsize': 20, 'fontweight':'bold'})\nax.set_ylabel(\"Amount\",fontdict= { 'fontsize': 15, 'fontweight':'bold'})\nax.set_xlabel(\"Gender\",fontdict= { 'fontsize': 15, 'fontweight':'bold'})\n\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2., height + .3,height ,ha=\"center\")\n","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:14:01.860786Z","iopub.execute_input":"2021-07-06T09:14:01.861052Z","iopub.status.idle":"2021-07-06T09:14:02.028051Z","shell.execute_reply.started":"2021-07-06T09:14:01.861026Z","shell.execute_reply":"2021-07-06T09:14:02.026868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Amount of deaths due to smoking and its distribution among genders","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(1, 2,figsize = (15,6))\nsns.countplot(x=\"smoking\",data=df,ax = ax[0], palette=[\"cornflowerblue\", \"khaki\"])\nsns.countplot(x=\"DEATH_EVENT\",hue = 'smoking', data=df,ax = ax[1], palette=[\"cornflowerblue\", \"khaki\"])\n\n\n#annotatinos\nfor i in range(2):\n    for p in ax[i].patches:\n        height = p.get_height()\n        ax[i].text(p.get_x()+p.get_width()/2., height + .3,height ,ha=\"center\")\n        \n\nax[0].set_xticklabels(['Non smoker','Smoker'],fontdict= { 'fontsize': 10, 'fontweight':'bold'})\nax[0].set_title(\"Distrubution of smokers\",fontdict= { 'fontsize': 20, 'fontweight':'bold'})\n\n\nax[1].legend([\"Non smoker\",\"Smoker\"], \n              bbox_to_anchor=(1,1))\n\nax[1].set_xticklabels(['Didnt survive','Survived'],fontdict= { 'fontsize': 10, 'fontweight':'bold'})\nax[1].set_title(\"Heart failure due to smoking\",fontdict= { 'fontsize': 20, 'fontweight':'bold'})\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:14:02.029592Z","iopub.execute_input":"2021-07-06T09:14:02.030005Z","iopub.status.idle":"2021-07-06T09:14:02.305646Z","shell.execute_reply.started":"2021-07-06T09:14:02.029964Z","shell.execute_reply":"2021-07-06T09:14:02.304782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots(1, 2,figsize = (15,6))\n\nsns.countplot(x=\"smoking\",hue = 'DEATH_EVENT',data=df[df['sex'] == 1],ax = ax[0],palette=[\"cornflowerblue\", \"khaki\"])\nsns.countplot(x=\"smoking\",hue = 'DEATH_EVENT', data=df[df['sex'] == 0],ax = ax[1],palette=[\"cornflowerblue\", \"khaki\"])\n\n\nax[0].set_title('Male')\n\nax[0].legend([\"Didnt Survive\",\"Survived\"], \n              loc=\"upper right\")\n\nax[1].legend([\"Didnt Survive\",\"Survived\"], \n              loc=\"upper right\")\nax[1].set_title('Female')\n\n#annotatinos\nfor i in np.arange(2):\n    for p in ax[i].patches:\n        height = p.get_height()\n        ax[i].text(p.get_x()+p.get_width()/2., height + .3,height ,ha=\"center\")\nfig.suptitle('Amount of deaths and smoking among genders', fontsize =15)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:14:02.309236Z","iopub.execute_input":"2021-07-06T09:14:02.309503Z","iopub.status.idle":"2021-07-06T09:14:02.59864Z","shell.execute_reply.started":"2021-07-06T09:14:02.309479Z","shell.execute_reply":"2021-07-06T09:14:02.597606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 67 males that didnt smoke died\n\n#### 3 females that did smoke survived\n\n#### Age might be an important factor","metadata":{}},{"cell_type":"code","source":"age_counts = df[\"age\"].value_counts()\nfig = px.bar(age_counts, title=\"Age distribution\")\nfig.update_layout(\n    xaxis_title = \"Age\",\n    yaxis_title = \"Frequency\",\n    title_x = 0.5, \n    showlegend = False\n)\nfig.show()\n\nage = pd.cut(df['age'], 8)\nfig, axs = plt.subplots(figsize=(15, 8))\nsns.countplot(x=age,hue='DEATH_EVENT', \n              data=df,palette=[\"cornflowerblue\", \"khaki\"]).set_title(\"Age distrubation with deaths\",\n                                                                { 'fontsize': 20, 'fontweight':'bold'});\naxs.legend([\"Didnt Survive\",\"Survived\"], \n              loc=\"upper right\")","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:14:02.60058Z","iopub.execute_input":"2021-07-06T09:14:02.600993Z","iopub.status.idle":"2021-07-06T09:14:02.895037Z","shell.execute_reply.started":"2021-07-06T09:14:02.600952Z","shell.execute_reply":"2021-07-06T09:14:02.894176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### More people died in lower spectrums, this is a bit strange. Dataset could be wrong\n\n#### Even if we look at the probability distribution then it would show that you have a higher probability to die the younger you are....","metadata":{}},{"cell_type":"code","source":"df['ejection_fraction'] = df['ejection_fraction'].div(100).round(2)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:14:02.896362Z","iopub.execute_input":"2021-07-06T09:14:02.896656Z","iopub.status.idle":"2021-07-06T09:14:02.905622Z","shell.execute_reply.started":"2021-07-06T09:14:02.896626Z","shell.execute_reply":"2021-07-06T09:14:02.904664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"age_counts = df[\"ejection_fraction\"].value_counts()\nfig = px.bar(age_counts, title=\"Distribution of how much blood is pumped out (ejection) as a percentage (normal 50-70%)\")\nfig.update_layout(\n    xaxis_title = \"Ejection percentage\",\n    yaxis_title = \"Frequency\",\n    title_x = 0.5, \n    showlegend = False\n)\nfig.show()\n\nfig, axs = plt.subplots(figsize=(15, 8))\nsns.countplot(x='ejection_fraction',hue='DEATH_EVENT', \n              data=df,palette=[\"cornflowerblue\", \"khaki\"]).set_title(\"Ejection distribution and heart failure\",\n                                                                { 'fontsize': 20, 'fontweight':'bold'});\naxs.legend([\"Didnt Survive\",\"Survived\"], \n              loc=\"upper right\")","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:14:02.906784Z","iopub.execute_input":"2021-07-06T09:14:02.907041Z","iopub.status.idle":"2021-07-06T09:14:03.290743Z","shell.execute_reply.started":"2021-07-06T09:14:02.907016Z","shell.execute_reply":"2021-07-06T09:14:03.28987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### More people died having an ejection rate of <50%. Still alot of people survived which shows inconsistency in dataset\n\n\n#### Perform lasso regularization to shrink variables to zero and choose the most important features","metadata":{}},{"cell_type":"code","source":"X = df.iloc[:, :11].values\ny = df.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:14:03.291739Z","iopub.execute_input":"2021-07-06T09:14:03.291999Z","iopub.status.idle":"2021-07-06T09:14:03.301902Z","shell.execute_reply.started":"2021-07-06T09:14:03.291975Z","shell.execute_reply":"2021-07-06T09:14:03.30077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import Lasso\n\npipeline = Pipeline([\n                     ('scaler',StandardScaler()),\n                     ('model',Lasso())\n])\n\nsearch = GridSearchCV(pipeline,\n                      {'model__alpha':np.arange(0.1,10,0.1)},\n                      cv = 5, scoring=\"neg_mean_squared_error\",verbose=3\n                      )\nsearch.fit(X_train,y_train)\nsearch.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:14:03.303162Z","iopub.execute_input":"2021-07-06T09:14:03.303433Z","iopub.status.idle":"2021-07-06T09:14:04.782048Z","shell.execute_reply.started":"2021-07-06T09:14:03.303409Z","shell.execute_reply":"2021-07-06T09:14:04.781004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Optimal shrinkage level is 0.1","metadata":{}},{"cell_type":"code","source":"coefficients = search.best_estimator_.named_steps['model'].coef_\nimportance = np.abs(coefficients)\nimportance","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:14:04.783415Z","iopub.execute_input":"2021-07-06T09:14:04.783792Z","iopub.status.idle":"2021-07-06T09:14:04.789919Z","shell.execute_reply.started":"2021-07-06T09:14:04.783733Z","shell.execute_reply":"2021-07-06T09:14:04.788951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Only the age, ejection_fraction and serum_creatinine are important here\n\n#### Split data to training and testing set and scale","metadata":{}},{"cell_type":"code","source":"X=df.iloc[:,[0,4,7]].values\ny=df.iloc[:, -1].values\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:14:04.79143Z","iopub.execute_input":"2021-07-06T09:14:04.791807Z","iopub.status.idle":"2021-07-06T09:14:04.804052Z","shell.execute_reply.started":"2021-07-06T09:14:04.791768Z","shell.execute_reply":"2021-07-06T09:14:04.802916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Randomforest and 10-kfold crossvalidation","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, np.ravel(y_train))\ny_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accurecy score of CM: \",round(accuracy_score(y_test, y_pred),2))\n\n\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"\\nAccuracy with 10-kfold: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:14:04.805385Z","iopub.execute_input":"2021-07-06T09:14:04.805738Z","iopub.status.idle":"2021-07-06T09:14:06.365568Z","shell.execute_reply.started":"2021-07-06T09:14:04.805701Z","shell.execute_reply":"2021-07-06T09:14:06.364655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Naive bayes","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accurecy score of CM: \",round(accuracy_score(y_test, y_pred),2))\n\n\n\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"\\nAccuracy with 10-kfold: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:14:06.367521Z","iopub.execute_input":"2021-07-06T09:14:06.367841Z","iopub.status.idle":"2021-07-06T09:14:06.392337Z","shell.execute_reply.started":"2021-07-06T09:14:06.367813Z","shell.execute_reply":"2021-07-06T09:14:06.391279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Decision trees","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\n\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accurecy score of CM: \",round(accuracy_score(y_test, y_pred),2))\n\n\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"\\nAccuracy with 10-kfold: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:14:06.393571Z","iopub.execute_input":"2021-07-06T09:14:06.393856Z","iopub.status.idle":"2021-07-06T09:14:06.425055Z","shell.execute_reply.started":"2021-07-06T09:14:06.393828Z","shell.execute_reply":"2021-07-06T09:14:06.423855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Light gradient boosting machine","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nclf = lgb.LGBMClassifier()\nclf.fit(X_train, y_train)\ny_pred=clf.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accurecy score of CM: \",round(accuracy_score(y_test, y_pred),2))\n\n\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"\\nAccuracy with 10-kfold: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:14:06.426287Z","iopub.execute_input":"2021-07-06T09:14:06.426554Z","iopub.status.idle":"2021-07-06T09:14:06.490093Z","shell.execute_reply.started":"2021-07-06T09:14:06.426526Z","shell.execute_reply":"2021-07-06T09:14:06.489039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Naive bayes gave a good accurecy according to the score of crossvalidation. With time added as feature the accurecy is improved more, but i excluded it from dataset \n\n#### Future work: A more throughful exploatry analysis could further simply the relationships in dataset or clear up my assumptions about time and this dataset having wrong data","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}