{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn import datasets\nfrom IPython.display import display, HTML\nfrom sklearn.model_selection import learning_curve, StratifiedKFold, train_test_split\nimport seaborn as sns\nimport itertools\nimport graphviz \n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Calculate Entropy\n\ndef compute_entropy(y):\n    \"\"\"\n    :param y: The data samples of a discrete distribution\n    \"\"\"\n    if len(y) < 2: #  a trivial case\n        return 0\n    freq = np.array( y.value_counts(normalize=True) )\n    return -(freq * np.log2(freq + 1e-6)).sum() # the small eps for \n    # safe numerical computation \n\n# Calculate Info Gain\n\ndef compute_info_gain(samples, attr, target):\n    values = samples[attr].value_counts(normalize=True)\n    split_ent = 0\n    for v, fr in values.iteritems():\n        index = samples[attr]==v\n        sub_ent = compute_entropy(target[index])\n        split_ent += fr * sub_ent\n    ent = compute_entropy(target)\n    return ent - split_ent\n\nclass TreeNode:\n    \"\"\"\n    A recursively defined data structure to store a tree.\n    Each node can contain other nodes as its children\n    \"\"\"\n    def __init__(self, node_name=\"\", min_sample_num=10, default_decision=None):\n        self.children = {} # Sub nodes --\n        # recursive, those elements of the same type (TreeNode)\n        self.decision = None # Undecided\n        self.split_feat_name = None # Splitting feature\n        self.name = node_name\n        self.default_decision = default_decision\n        self.min_sample_num = min_sample_num\n\n    def pretty_print(self, prefix=''):\n        if self.split_feat_name is not None:\n            for k, v in self.children.items():\n                v.pretty_print(f\"{prefix}:When {self.split_feat_name} is {k}\")\n                #v.pretty_print(f\"{prefix}:{k}:\")\n        else:\n            print(f\"{prefix}:{self.decision}\")\n\n    def predict(self, sample):\n        if self.decision is not None:\n            # uncomment to get log information of code execution\n            print(\"Decision:\", self.decision)\n            return self.decision\n        elif self.split_feat_name is None:\n            # uncomment to get log information of code execution\n            print(\"Decision:\", self.decision)\n            return self.decision\n        else: \n            # this node is an internal one, further queries about an attribute \n            # of the data is needed.\n            # print(sample)\n            print(\"MY FEATURE\" + self.split_feat_name)\n            attr_val = sample[self.split_feat_name]\n            # print(self.children)\n            child = self.children[attr_val]\n            # uncomment to get log information of code execution\n            print(\"Testing \", self.split_feat_name, \"->\", attr_val)\n            return child.predict(sample)\n\n    def fit(self, X, y):\n        \"\"\"\n        The function accepts a training dataset, from which it builds the tree \n        structure to make decisions or to make children nodes (tree branches) \n        to do further inquiries\n        :param X: [n * p] n observed data samples of p attributes\n        :param y: [n] target values\n        \"\"\"\n        if self.default_decision is None:\n            self.default_decision = y.mode()[0]\n            \n            \n        print(self.name, \"received\", len(X), \"samples\")\n        if len(X) < self.min_sample_num:\n            # If the data is empty when this node is arrived, \n            # we just make an arbitrary decision\n            if len(X) == 0:\n                self.decision = self.default_decision\n                print(\"DECISION\", self.decision)\n            else:\n                self.decision = y.mode()[0]\n                print(\"DECISION\", self.decision)\n            return\n        else: \n            unique_values = y.unique()\n            if len(unique_values) == 1:\n                self.decision = unique_values[0]\n                print(\"DECISION\", self.decision)\n                return\n            else:\n                info_gain_max = 0\n                for a in X.keys(): # Examine each attribute\n                    aig = compute_info_gain(X, a, y)\n                    if aig > info_gain_max:\n                        info_gain_max = aig\n                        self.split_feat_name = a\n                if self.split_feat_name == None:\n                  return\n                print(f\"Split by {self.split_feat_name}, IG: {info_gain_max:.2f}\")\n                self.children = {}\n                for v in X[self.split_feat_name].unique():\n                    index = X[self.split_feat_name] == v\n                    self.children[v] = TreeNode(\n                        node_name=self.name + \":\" + self.split_feat_name + \"==\" + str(v),\n                        min_sample_num=self.min_sample_num,\n                        default_decision=self.default_decision)\n                    self.children[v].fit(X[index], y[index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing out using Data From Kaggle - using the breast cancer wisconsin dataset \n# Reference: https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\n\nimport pandas as pd\ndf = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\n\n#finding out the rows and columns\nprint(df)\n\n#Setting up train test split for the first time\nX = df.drop(columns=[\"id\",\"diagnosis\"])\ny = df[\"diagnosis\"]\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)\n \n#Healthy cells typically have nuclei with a standard size and shape while cancer cells often have nuclei that are large and mishapen. \n#As such, the size and shape of the nucleus should be a good predictor for whether or not a sample is cancerous.\n\n#Doing feature selection - viewing which features are correlated\nprint(\"\\nFeature Correlation:\\n\")\ng = sns.heatmap(X_train.corr(),cmap=\"BrBG\",annot=False)\n\n#Removing all features which are correlated\nX2 = X.drop(columns=['perimeter_mean', 'radius_mean',\n       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean',  'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'])\ny2 = y\n\n# Binning the numerical data so that we can use it for the ID3 decision tree algorithm\nX2['texture_mean'] = pd.qcut(X2['texture_mean'], q=3, labels=[f'texture_mean_{i}' for i in range(3)])\nX2['area_mean'] = pd.qcut(X2['area_mean'], q=3, labels=[f'area_mean_{i}' for i in range(3)])\nX2['symmetry_mean'] = pd.qcut(X2['symmetry_mean'], q=3, labels=[f'symmetry_mean_{i}' for i in range(3)])\n\n# Setting up the new train test split\nX_train2, X_test2, Y_train2, Y_test2 = train_test_split(X2, y2, test_size=0.2)\n\nprint(X_train2)\nprint(X_test2)\n\nt = TreeNode() \nt.fit(X_train2, Y_train2)\nt.pretty_print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Setting up the variables for calculation and correct modelling\ncorr = 0\nerr_fp = 0\nerr_fn = 0\n\n# Test tree working\n# print(X_test2)\n# print(type(X_test2))\n# print(Y_test2)\n# print(X_test2.iloc[0])\n\nfor (i, data), tgt in zip(X_test2.iterrows(), Y_test2):\n    # print(i)\n    print(tgt)\n    print(data)\n    a = t.predict(data)\n    if a and not tgt:\n        err_fp += 1\n    elif not a and tgt:\n        err_fn += 1\n    else:\n        corr += 1\n\nprecision = corr/(corr+err_fp)\nrecall = corr/(corr+err_fn)\nf1_score = 2*((precision*recall)/(precision+recall))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr, err_fp, err_fn, precision, recall, f1_score","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}