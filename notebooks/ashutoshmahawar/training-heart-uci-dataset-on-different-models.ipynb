{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Statistical Visualization and Model Training on Machine learning Algoriths, Perceptron and Sigmoid neuron.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data contains following information:\n\n    age: The person's age in years\n    sex: The person's sex (1 = male, 0 = female)\n    cp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n    trestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\n    chol: The person's cholesterol measurement in mg/dl\n    fbs: The person's fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)\n    restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n    thalach: The person's maximum heart rate achieved\n    exang: Exercise induced angina (1 = yes; 0 = no)\n    oldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n    slope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n    ca: The number of major vessels (0-3)\n    thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n    target: Heart disease (0 = no, 1 = yes)\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndataframe = pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv', dtype={'sex':'object', 'cp':'object','fbs':'object', 'restecg':'object',\n                                                                            'exang':'object','slope':'object', 'thal':'object'})\ndataframe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('No. of rows: {0} and columns: {1}'.format(dataframe.shape[0], dataframe.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gives information about the data type of each feature and Non-Null count.\ndataframe.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gives statistical description of each numerical feature.\ndataframe.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(dataframe.loc[0, 'sex'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column = dataframe.select_dtypes('object').columns\n\nfor col in column:\n    print(col,\":\\n\", dataframe[col].value_counts())\n    print('-----------------------------------')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe.isnull().sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:**\n\nAs we can see that dataset contain zero null values, but if we read the description of dataset we will find out that thal contains only 3 categories but in our data set we are getting four categories. **So in 'thal' feature we can either change the values with median category or we can just drop these rows which have 'thal feature value as zero**.\n\nHere i am going to remove these particular rows as these rows are totally different from other data points. So i will remove the datapoints which contain **thal==0**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# to find out rows which are having thal==0\n(dataframe.thal== str(0)).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe = dataframe.loc[dataframe.thal != str(0)]\ndataframe.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe.thal.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Visulaization**\n## **Univariate Analysis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_num = dataframe.select_dtypes('int64', 'float64')\ndf_num.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_num.hist(figsize=(10,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cat = dataframe.select_dtypes('object')\ndf_cat.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Preprocessing**"},{"metadata":{},"cell_type":"markdown","source":"## **Train-Test split**"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ny = dataframe['target']\ndataframe.drop(['target'], axis=1, inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(dataframe, y, test_size=0.2, stratify=y, random_state=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing libraries required for scaling and encoding features. \n# we will fit on train data first and then transform test data.\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nscaler = StandardScaler()\nencoder = OneHotEncoder(drop='first', sparse=False)\ndf_num = X_train.select_dtypes(['int64','float64'])\ndf_cat = X_train.select_dtypes('object')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#standardizing numerical train data\ndf_num = pd.DataFrame(scaler.fit_transform(df_num), columns=df_num.columns)\ndf_num.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cat.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#encoding categorical data\ndata = encoder.fit_transform(df_cat)\ncol = encoder.get_feature_names(df_cat.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cat = pd.DataFrame(data, columns=col)\ndf_cat.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.concat([df_num, df_cat], axis=1)\nX_train.isna().sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transforming the features on test data\ndf_num = X_test.select_dtypes(['int64', 'float64'])\ndf_cat = X_test.select_dtypes('object')\ndf_num = pd.DataFrame(scaler.transform(df_num), columns=df_num.columns)\ndf_num.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = encoder.transform(df_cat)\ncol = encoder.get_feature_names(df_cat.columns)\ndf_cat = pd.DataFrame(data, columns=col)\ndf_cat.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = pd.concat([df_num, df_cat], axis=1)\nX_test.isna().sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model Training on Logistic Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nregress = LogisticRegression()\nregress.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Prediction on Training data and Test data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\npred_train = regress.predict(X_train)\npred_test = regress.predict(X_test)\nprint('Accuracy on Training data using Logistic Regression: ', accuracy_score(y_train, pred_train))\nprint('Accuracy on Test data using Logistic Regression', accuracy_score(y_test, pred_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model Training using Mcculloh pits neuron**\n\nFor this firstly we have to binarised our data then only we can apply this and we have to find a perfect bias which could give us maximum accuracy.\n"},{"metadata":{},"cell_type":"markdown","source":"**Binarisiation of Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(X_train.T, '*')\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are features which have contiuour values, so we have to convert those features to implement this model. \nWe can do that using pd.cut and apply this to every feature using below code."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_binarised_train = X_train.apply(pd.cut, bins=2, labels=[0,1])\nplt.plot(X_binarised_train.T, '*')\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now each feature is binarised. Now we can implement our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(X_test.T, '*')\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_binarised_test = X_test.apply(pd.cut, bins=2, labels=[0,1])\nplt.plot(X_binarised_test.T, '*')\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_binarised_train = X_binarised_train.values\nX_binarised_test = X_binarised_test.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will be making a class which will help us to perform mcculloh pits function."},{"metadata":{"trusted":true},"cell_type":"code","source":"class mp_neuron:\n    \n    def __init__(self):\n        self.b = None\n        \n    def model(self, x):\n        return (sum(x) >= self.b)\n    \n    def predict(self, X):\n        Y =[]\n        for x in X:\n            result = self.model(x)\n            Y.append(result)\n        return np.array(Y)\n        \n    def fit(self, X, Y):\n        accuracy = {}\n        \n        for b in range(X.shape[1] + 1):\n            self.b = b\n            Y_pred = self.predict(X)\n            accuracy[b] = accuracy_score(Y_pred, Y)\n            print(b,\":\",accuracy[b])\n        best_b = max(accuracy, key = accuracy.get)\n        self.b = best_b\n        \n        print('Optimal value of b is', best_b)\n        print('Highest accuracy is', accuracy[best_b])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mp_neuron = mp_neuron()\nmp_neuron.fit(X_binarised_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Perceptron class**"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Perceptron:\n    \n    def __init__(self):\n        self.w = None\n        self.b = None\n    \n    def model(self, x):\n        return 1 if (np.dot(self.w, x) >= self.b) else 0\n    \n    def predict(self, X):\n        Y = []\n        for x in X:\n            result = self.model(x)\n            Y.append(result)\n        return np.array(Y)\n    \n    def fit(self, X, Y, epochs = 1, lr = 1):\n        self.w = np.random.rand(X.shape[1])\n        self.b = 0\n        \n        accuracy = {}\n        max_accuracy = 0\n        \n        for i in range(epochs):\n            for x, y in zip(X, Y):\n                y_pred = self.model(x)\n                if y == 1 and y_pred == 0:\n                    self.w = self.w + lr * x\n                    self.b = self.b + lr * 1\n                elif y == 0  and y_pred == 1:\n                    self.w = self.w - lr * x\n                    self.b = self.b - lr * 1\n            accuracy[i] = accuracy_score(self.predict(X), Y)\n            if accuracy[i] > max_accuracy:\n                max_accuracy = accuracy[i]\n                chkptw = self.w\n                chkptb = self.b\n        self.w = chkptw\n        self.b = chkptb\n        print(max_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_perceptron = X_train.values\nX_test_perceptron = X_test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perceptron = Perceptron()\nperceptron.fit(X_train_perceptron, y_train, epochs=100000, lr=0.45)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Prediction**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = perceptron.predict(X_test_perceptron)\nprint(\"Accuracy on test data:\", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion:\n\nWe can say that Logistic regression performs really well on this dataset but basic neurons like mcculloh pits and percepton are also performing comparatively well. if we with some deep learning modules they might perform much better than these too. \n\nNow my other notebooks for this data set will be regarding how to implement feed forward networks using backpropogation."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}