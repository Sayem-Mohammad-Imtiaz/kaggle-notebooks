{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader,TensorDataset\nfrom sklearn import model_selection\nfrom sklearn import metrics\nimport io\nfrom string import punctuation\nimport tensorflow as tf\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-30T10:24:54.154552Z","iopub.execute_input":"2021-07-30T10:24:54.154989Z","iopub.status.idle":"2021-07-30T10:24:54.169117Z","shell.execute_reply.started":"2021-07-30T10:24:54.154939Z","shell.execute_reply":"2021-07-30T10:24:54.168126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    df = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n    df.sentiment = df.sentiment.apply(lambda x: 1 if x=='positive' else 0)\n    df['review'] = df['review'].apply(lambda x : x.lower())\n    df['review'] = df['review'].apply(lambda x : ''.join([c for c in x if c not in punctuation]))\n    df = df.sample(frac=1).reset_index(drop=True)\n    y = df.sentiment.values\n    df['fold'] = -1\n    kf =  model_selection.StratifiedKFold(n_splits=5)\n    for f,(tr,ts) in enumerate(kf.split(X=df,y=y)):\n        df.loc[ts,'fold'] = f","metadata":{"execution":{"iopub.status.busy":"2021-07-30T10:24:54.170792Z","iopub.execute_input":"2021-07-30T10:24:54.171148Z","iopub.status.idle":"2021-07-30T10:25:01.722125Z","shell.execute_reply.started":"2021-07-30T10:24:54.171112Z","shell.execute_reply":"2021-07-30T10:25:01.721188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class IMDBdataset:\n\n    def __init__(self, reviews, targets):\n        self.reviews = reviews\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.reviews)\n\n    def __getitem__(self, item):\n        review = self.reviews[item, :]\n        target = self.targets[item]\n\n        return {\n            \"review\": torch.tensor(review, dtype=torch.long),\n            \"target\": torch.tensor(target, dtype=torch.float)\n        }","metadata":{"execution":{"iopub.status.busy":"2021-07-30T10:25:01.723928Z","iopub.execute_input":"2021-07-30T10:25:01.72429Z","iopub.status.idle":"2021-07-30T10:25:01.732222Z","shell.execute_reply.started":"2021-07-30T10:25:01.72426Z","shell.execute_reply":"2021-07-30T10:25:01.730554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model architecture\nclass LSTM(nn.Module):\n    def __init__(self,embedding_matrix,hidden_dim):\n        super().__init__()\n        number_of_words = embedding_matrix.shape[0]\n        embed_dims = embedding_matrix.shape[1]\n        self.hidden_dim = hidden_dim\n        self.n_layers = 2\n        # embedding layer\n        self.embedding = nn.Embedding(num_embeddings = number_of_words,embedding_dim = embed_dims)\n        self.embedding.weights = nn.Parameter(torch.tensor(embedding_matrix,dtype = torch.float32))\n        self.embedding.weights.requires_grad = False\n        self.lstm = nn.LSTM(input_size=embed_dims,\n                           hidden_size=self.hidden_dim,\n                           num_layers=self.n_layers,\n                           batch_first=True)\n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(128,1)\n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self,x,hidden):\n        batch_size = x.size(0)\n        x = self.embedding(x)\n        x,hidden = self.lstm(x,hidden)\n        x = x.contiguous().view(-1,self.hidden_dim)\n        out = self.dropout(x)\n        out = self.fc(out)\n        sig_out = self.sigmoid(out)\n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1]\n        sig_out = torch.unsqueeze(sig_out,1)\n        return sig_out,hidden\n        \n    def init_hidden(self,batch_size):\n        # initialise hidden and cell states\n        weight = next(self.parameters()).data\n        if (torch.cuda.is_available()):\n            hidden = (weight.new(self.n_layers, batch_size,self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden","metadata":{"execution":{"iopub.status.busy":"2021-07-30T10:25:01.734626Z","iopub.execute_input":"2021-07-30T10:25:01.734978Z","iopub.status.idle":"2021-07-30T10:25:01.748423Z","shell.execute_reply.started":"2021-07-30T10:25:01.734943Z","shell.execute_reply":"2021-07-30T10:25:01.747538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(data_loader, model, optimizer, device,batch_size):\n    model.train()\n    h = model.init_hidden(batch_size)\n    for data in data_loader:\n        reviews = data[\"review\"]\n        targets = data[\"target\"]\n        h = tuple([each.data for each in h])\n        # move the data to device that we want to use\n        reviews = reviews.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n        optimizer.zero_grad()\n        predictions,h = model(reviews,h)\n        loss = nn.BCEWithLogitsLoss()(predictions, targets.view(-1, 1))\n        loss.backward()\n        optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T10:25:01.749728Z","iopub.execute_input":"2021-07-30T10:25:01.750368Z","iopub.status.idle":"2021-07-30T10:25:01.759296Z","shell.execute_reply.started":"2021-07-30T10:25:01.750327Z","shell.execute_reply":"2021-07-30T10:25:01.758515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(data_loader, model, device,batch_size):\n    final_predictions = []\n    final_targets = []\n    test_h = model.init_hidden(batch_size)\n    model.eval()\n\n    # disable the gradient calculation\n    with torch.no_grad():\n        for data in data_loader:\n            reviews = data[\"review\"]\n            test_h = tuple([each.data for each in test_h])\n            reviews = reviews.to(device, dtype=torch.long)\n            predictions,test_h = model(reviews,test_h)\n            predictions = predictions.cpu().numpy().tolist()\n            targets = data[\"target\"].cpu().numpy().tolist()\n            final_predictions.extend(predictions)\n            final_targets.extend(targets)\n\n    return final_predictions, final_targets","metadata":{"execution":{"iopub.status.busy":"2021-07-30T10:25:01.760624Z","iopub.execute_input":"2021-07-30T10:25:01.761053Z","iopub.status.idle":"2021-07-30T10:25:01.771095Z","shell.execute_reply.started":"2021-07-30T10:25:01.761018Z","shell.execute_reply":"2021-07-30T10:25:01.77032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['len'] = df['review'].apply(lambda x : len(x))\nprint(df['len'].describe())\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T10:25:01.772492Z","iopub.execute_input":"2021-07-30T10:25:01.773062Z","iopub.status.idle":"2021-07-30T10:25:01.82386Z","shell.execute_reply.started":"2021-07-30T10:25:01.773026Z","shell.execute_reply":"2021-07-30T10:25:01.822917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN =2000\nbatch_size = 50\nEPOCHS = 5","metadata":{"execution":{"iopub.status.busy":"2021-07-30T10:25:01.826099Z","iopub.execute_input":"2021-07-30T10:25:01.826445Z","iopub.status.idle":"2021-07-30T10:25:01.831447Z","shell.execute_reply.started":"2021-07-30T10:25:01.82641Z","shell.execute_reply":"2021-07-30T10:25:01.830709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def GLOVE(fname):\n\n    f = open(fname,'r')\n    gloveModel = {}\n    for line in f:\n        splitLines = line.split()\n        word = splitLines[0]\n        wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n        gloveModel[word] = wordEmbedding\n    print(len(gloveModel),\" words loaded!\")\n    f.close()\n    return gloveModel\n\ndef create_embedding_matrix(word_index, embedding_dict):\n    # 100 dimensional glove model\n    embedding_matrix = np.zeros((len(word_index) + 1, 100))\n    for word, i in word_index.items():\n        if word in embedding_dict:\n            embedding_matrix[i] = embedding_dict[word]\n    return embedding_matrix","metadata":{"execution":{"iopub.status.busy":"2021-07-30T10:25:01.833116Z","iopub.execute_input":"2021-07-30T10:25:01.833515Z","iopub.status.idle":"2021-07-30T10:25:01.842064Z","shell.execute_reply.started":"2021-07-30T10:25:01.83348Z","shell.execute_reply":"2021-07-30T10:25:01.840298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run(df, fold, params=None):\n    PATH = './model_vals'\n    train_df = df[df.fold != fold].reset_index(drop=True)\n    test_df = df[df.fold == fold].reset_index(drop=True)\n    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n    tokenizer.fit_on_texts(df.review.values.tolist())\n    xtrain = tokenizer.texts_to_sequences(train_df.review.values)\n    xtest = tokenizer.texts_to_sequences(test_df.review.values)\n\n    xtrain = tf.keras.preprocessing.sequence.pad_sequences(xtrain, maxlen=MAX_LEN)\n    xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest, maxlen=MAX_LEN)\n\n    train_data = IMDBdataset(reviews = xtrain,targets = train_df.sentiment.values)\n    test_data = IMDBdataset(reviews = xtest,targets = test_df.sentiment.values)\n\n    train_data_loader = torch.utils.data.DataLoader(train_data,batch_size = batch_size,num_workers = 2)\n    test_data_loader = torch.utils.data.DataLoader(test_data,batch_size = batch_size,num_workers = 1)\n\n    # Loading the embeddings \n    embedding_dict = GLOVE(\"../input/glove6b100dtxt/glove.6B.100d.txt\")\n    embedding_matrix = create_embedding_matrix(tokenizer.word_index, embedding_dict)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(device)\n    model = LSTM(embedding_matrix,128)\n    try:\n        checkpoint = torch.load(PATH)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    except:\n        pass\n    model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    best_accuracy = 0.0\n    early_stopping_counter = 0\n\n    for epoch in range(EPOCHS):\n        \n        train(train_data_loader, model, optimizer, device,batch_size)\n        output, targets = evaluate(test_data_loader, model, device,batch_size)\n        outputs = np.array(output) >= 0.5\n        \n        accuracy = metrics.accuracy_score(targets, outputs)\n\n        print(f\"Fold: {fold}, Epoch: {epoch}, Accuracy Score = {accuracy}\")\n        \n        # simple early stopping\n        if accuracy > best_accuracy:\n            best_accuracy = accuracy\n\n        else:\n            early_stopping_counter += 1\n\n        if early_stopping_counter > 2:\n            break\n    torch.save({\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict()}, PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T10:25:01.843602Z","iopub.execute_input":"2021-07-30T10:25:01.844022Z","iopub.status.idle":"2021-07-30T10:25:01.858789Z","shell.execute_reply.started":"2021-07-30T10:25:01.843987Z","shell.execute_reply":"2021-07-30T10:25:01.858069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading and preprocessing\nif __name__ == '__main__':\n    for f in range(5):\n        run(df, f)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T10:25:01.861227Z","iopub.execute_input":"2021-07-30T10:25:01.862384Z","iopub.status.idle":"2021-07-30T11:20:03.174981Z","shell.execute_reply.started":"2021-07-30T10:25:01.862356Z","shell.execute_reply":"2021-07-30T11:20:03.173997Z"},"trusted":true},"execution_count":null,"outputs":[]}]}