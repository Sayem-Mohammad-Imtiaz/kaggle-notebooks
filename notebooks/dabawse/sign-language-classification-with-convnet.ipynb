{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Sign Language Classification\nWelcome to the sign language classification dataset, where we will be given a set images depicting hand gestures of the sign language and use those to create a model which predicts them. This prediction is useful because it can help deaf people communicate with others through the use of these gestures."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom cv2 import imread\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, BatchNormalization, Dense, Flatten\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/sign-language-mnist/sign_mnist_train/sign_mnist_train.csv')\ntest = pd.read_csv('../input/sign-language-mnist/sign_mnist_test/sign_mnist_test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualisation\nBefore we begin data cleaning, we will look at the different sign gestures and how they appear. Below are three plots which show the different gesticulations and their corresponding letters in the alphabet."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 20))\n\nax1.imshow(imread('../input/sign-language-mnist/amer_sign2.png'))\nax1.axis('off')\n\nax2.imshow(imread('../input/sign-language-mnist/amer_sign3.png'))\nax2.axis('off')\n\nax3.imshow(imread('../input/sign-language-mnist/american_sign_language.PNG'))\nax3.axis('off')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering\nThe first step we will be taking towards creating a model which predicts this dataset is feature engineering by reshaping the X data into a form which can be inputted into the neural network. We will also categorise the y data using to_categorical."},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop('label', axis=1)\ny_train = train['label']\n\nX_test = test.drop('label', axis=1)\ny_test = test['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.array(X_train).reshape(27455, 28, 28, 1)\nX_test = np.array(X_test).reshape(7172, 28, 28, 1)\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dividing the train and test sets by 255 is important engineering because it helps improve the accuracy of the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train / 255.0\nX_test = X_test / 255.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we use an ImageDataGenerator which creates random augmentations of the images. This is useful as it can lessen the impact which non-relevant features of the images have upon our predictor and it provides more data for it to use."},{"metadata":{"trusted":true},"cell_type":"code","source":"idg = ImageDataGenerator()\ntrain_gen = idg.flow(X_train, y_train, batch_size=64)\ntest_gen = idg.flow(X_test, y_test, batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification\nFinally, we create the ConvNet model which is used to classify the sign gesture images.\n#### It has:\n* 2 hidden layers with 32 units\n* 2 hidden layers with 64 units\n* 2 dropout layers with a value of 0.2\n* 2 Max Pooling layers\n* a hidden activation function of relu\n* a dense output layer with 25 units and softmax activation"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1), activation='relu', input_shape=(28, 28, 1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(64, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(25, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model is compiled using a categorical crossentropy loss, a categorical accuracy metric and an Adam optimizer."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', metrics='categorical_accuracy', optimizer='adam')\nhistory = model.fit_generator(train_gen, validation_data=test_gen, epochs=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here are the accuracies and losses from the ConvNet:"},{"metadata":{"trusted":true},"cell_type":"code","source":"results = history.history\n\nfor i in results:\n    plt.plot(results[i])\n    plt.title(i+' over epochs')\n    plt.ylabel(i)\n    plt.xlabel('epochs')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Thank you for reading my notebook.\n## If you enjoyed this notebook and found it helpful, please upvote it as it will help me make more of these."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}