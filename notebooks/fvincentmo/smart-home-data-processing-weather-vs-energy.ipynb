{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Sensor Time-Series Analysis"},{"metadata":{},"cell_type":"markdown","source":"* This notebook serves as an introduction to work on data collected from sensors embedded in the IoT devices.\n> \"Time series is a series of data points indexed (or listed or graphed) in time order.\""},{"metadata":{},"cell_type":"markdown","source":"![](http://)### A. Importing the required Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"## NumPy is a package in Python used for Scientific Computing. NumPy package is used to perform different operations. The ndarray (NumPy Array) is a multidimensional array used to store values of same datatype.\nimport numpy as np\n## Pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real world data analysis in Python.\nimport pandas as pd\n## Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. Matplotlib can be used in Python scripts, the Python and IPython shells, the Jupyter notebook, web application servers, and four graphical user interface toolkits.\nimport matplotlib\nimport matplotlib.pyplot as plt\n## Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## `%matplotlib` is a magic function in IPython. With this, the output of plotting commands is displayed inline within frontends like the Jupyter notebook, directly below the code cell that produced it. The resulting plots will then also be stored in the notebook document.\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **NOTE:** If you need to know more about Python programming language, this free book is highly recommended:\n* PDF: [A Whirlwind Tour of Python](https://www.oreilly.com/programming/free/files/a-whirlwind-tour-of-python.pdf)  \n* Code: https://github.com/jakevdp/WhirlwindTourOfPython"},{"metadata":{},"cell_type":"markdown","source":"### B. Importing the weather and energy dataset\nThe dataset contains the readings with a time span of 1 minute of house appliances in kW from a smart meter and weather conditions of that particular region.\n\n#### Data Columns Descriptions:\n(source: Data Source: https://www.kaggle.com/taranvee/smart-home-dataset-with-weather-information)\n##### Index \n- **time**\n    * Time of the readings, with a time span of 1 minute.\n\n##### Energy Usage \n- **use [kW]**\n    * Total energy consumption\n- **gen [kW]**\n    * Total energy generated by means of solar or other power generation resources\n- **House overall [kW]**\n    * overall house energy consumption\n- **Dishwasher [kW]** \n    * energy consumed by specific appliance\n- **Furnace 1 [kW]**\n    * energy consumed by specific appliance\n- **Furnace 2 [kW]**\n    * energy consumed by specific appliance\n- **Home office [kW]**\n    * energy consumed by specific appliance\n- **Fridge [kW]**\n    * energy consumed by specific appliance\n- **Wine cellar [kW]**\n    * energy consumed by specific appliance\n- **Garage door [kW]**\n    * energy consumed by specific appliance\n- **Kitchen 12 [kW]**\n    * energy consumption in kitchen 1\n- **Kitchen 14 [kW]**\n    * energy consumption in kitchen 2\n- **Kitchen 38 [kW]**\n    * energy consumption in kitchen 3\n- **Barn [kW]**\n    * energy consumed by specific appliance\n- **Well [kW]**\n    * energy consumed by specific appliance\n- **Microwave [kW]**\n    * energy consumed by specific appliance\n- **Living room [kW]**\n    * energy consumption in Living room\n- **Solar [kW]**\n    * Solar power generation\n\n##### Weather\n- **temperature**:\n    * Temperature is a physical quantity expressing hot and cold.\n- **humidity**:\n    * Humidity is the concentration of water vapour present in air.\n- **visibility**:\n    * Visibility sensors measure the meteorological optical range which is defined as the length of atmosphere over which a beam of light travels before its luminous flux is reduced to 5% of its original value.\n\n- **apparentTemperature**:\n    * Apparent temperature is the temperature equivalent perceived by humans, caused by the combined effects of air temperature, relative humidity and wind speed. The measure is most commonly applied to the perceived outdoor temperature.\n- **pressure**: \n    * Falling air pressure indicates that bad weather is coming, while rising air pressure indicates good weather\n- **windSpeed**:\n    * Wind speed, or wind flow speed, is a fundamental atmospheric quantity caused by air moving from high to low pressure, usually due to changes in temperature.\n- **cloudCover**:\n    * Cloud cover (also known as cloudiness, cloudage, or cloud amount) refers to the fraction of the sky obscured by clouds when observed from a particular location. Okta is the usual unit of measurement of the cloud cover.\n- **windBearing**:\n    * In meteorology, an azimuth of 000° is used only when no wind is blowing, while 360° means the wind is from the North. True Wind Direction True North is represented on a globe as the North Pole. All directions relative to True North may be called \"true bearings.\"\n- **dewPoint**:\n    * the atmospheric temperature (varying according to pressure and humidity) below which water droplets begin to condense and dew can form.\n- **precipProbability**:\n    * A probability of precipitation (POP), also referred to as chance of precipitation or chance of rain, is a measure of the probability that at least some minimum quantity of precipitation will occur within a specified forecast period and location.\n- **precipIntensity**:\n    * The intensity of rainfall is a measure of the amount of rain that falls over time. The intensity of rain is measured in the height of the water layer covering the ground in a period of time. It means that if the rain stays where it falls, it would form a layer of a certain height.\n \n##### Others\n- **summary**:\n    * Report generated by the by the data collection systm (apparently!).\n    * Including:\n    ```\n    Clear, Mostly Cloudy, Overcast, Partly Cloudy, Drizzle,\n       Light Rain, Rain, Light Snow, Flurries, Breezy, Snow,\n       Rain and Breezy, Foggy, Breezy and Mostly Cloudy,\n       Breezy and Partly Cloudy, Flurries and Breezy, Dry,\n       Heavy, Snow.\n    ```\n- **icon**:\n    * The icon that is used by the data collection systm (apparently!).\n    * Including:\n    ```\n    cloudy, clear-night, partly-cloudy-night, clear-day, partly-cloudy-day, rain, snow, wind, fog.\n    ```\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"## pandas.read_csv: Read a comma-separated values (csv) file into DataFrame.\ndataset = pd.read_csv(\"/kaggle/input/smart-home-dataset-with-weather-information/HomeC.csv\", low_memory=False)\ndataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp_str = \"Feature(attribute)     DataType\"; print(tmp_str+\"\\n\"+\"-\"*len(tmp_str))\nprint(dataset.dtypes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> To know more about Pandas DataFrame see this:\n* https://towardsdatascience.com/pandas-dataframe-a-lightweight-intro-680e3a212b96"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Return a tuple representing the dimensionality of the DataFrame.\nprint(\"Shape of the data: {} --> n_rows = {}, n_cols = {}\".format(dataset.shape, dataset.shape[0],dataset.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## pandas.DataFrame.head: This function returns the first n rows for the object based on position. It is useful for quickly testing if your object has the right type of data in it.\ndataset.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## This function returns last n rows from the object based on position. It is useful for quickly verifying data, for example, after sorting or appending rows.\ndataset.tail(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Wee see that the last row is invalid, so let's remove it."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset[0:-1] ## == dataset[0:dataset.shape[0]-1] == dataset[0:len(dataset)-1] == dataset[:-1]\ndataset.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> A numpy trick: `numpy.r` is the simple way to build up arrays quickly"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.r_[0:5, -5:0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.iloc[np.r_[0:5, -5:0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## pandas.DataFrame.columns: The column labels of the DataFrame.\ndataset.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Let's clean the columns names by removing the `[kW]` uint."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Python string method replace() returns a copy of the string in which the occurrences of old have been replaced with new, optionally restricting the number of replacements to max.\ndataset.columns = [col.replace(' [kW]', '') for col in dataset.columns]\ndataset.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['Kitchen'] = dataset[['Kitchen 12','Kitchen 14','Kitchen 38']].mean(axis=1)\ndataset = dataset.drop(['Kitchen 12','Kitchen 14','Kitchen 38'], axis=1)\n\ndataset['Furnace'] = dataset[['Furnace 1','Furnace 2']].mean(axis=1)\ndataset = dataset.drop(['Furnace 1','Furnace 2'], axis=1)\n\ndataset.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### B. Indexing rows by `Time`"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Unix Time  (https://en.wikipedia.org/wiki/Unix_time)\n## It represents the number of seconds that have passed since 00:00:00 UTC Thursday, 1 January 1970.\ndataset['time'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> this large number represents a unix timestamp (i.e. \"1284101485\") in Python, and we'd like to convert them to a readable date."},{"metadata":{"trusted":true},"cell_type":"code","source":"import time \nprint(' start ' , time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(1451624400)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">  The dataset contains the readings with a time span of 1 minute of house appliances\nin kW from a smart meter and weather conditions of that particular region.\nSo, we set `freq='min'` and convert Uinx time to readable date."},{"metadata":{"trusted":true},"cell_type":"code","source":"time_index = pd.date_range('2016-01-01 05:00', periods=len(dataset),  freq='min')  \ntime_index = pd.DatetimeIndex(time_index)\ndataset = dataset.set_index(time_index)\ndataset = dataset.drop(['time'], axis=1)\ndataset.iloc[np.r_[0:5,-5:0]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### C. ReSampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We have 500K rows and each row shows the home status at a specific `minute`.\nLet's plot the `temperature` data and see what is the result."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['temperature'].plot(figsize=(25,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> It may seem too noisy to you. We can `resample` data by taking the `average temperature` every `day` and then plot it."},{"metadata":{"trusted":true},"cell_type":"code","source":"## pandas.DataFrame.resample: Convenience method for frequency conversion and resampling of time series. \ndataset['temperature'].resample(rule='D').mean().plot(figsize=(25,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Here are the `rule`s you can use:\n- B         business day frequency\n- C         custom business day frequency (experimental)\n- D         calendar day frequency\n- W         weekly frequency\n- M         month end frequency\n- SM        semi-month end frequency (15th and end of month)\n- BM        business month end frequency\n- CBM       custom business month end frequency\n- MS        month start frequency\n- SMS       semi-month start frequency (1st and 15th)\n- BMS       business month start frequency\n- CBMS      custom business month start frequency\n- Q         quarter end frequency\n- BQ        business quarter endfrequency\n- QS        quarter start frequency\n- BQS       business quarter start frequency\n- A         year end frequency\n- BA, BY    business year end frequency\n- AS, YS    year start frequency\n- BAS, BYS  business year start frequency\n- BH        business hour frequency\n- H         hourly frequency\n- T, min    minutely frequency\n- S         secondly frequency\n- L, ms     milliseconds\n- U, us     microseconds\n- N         nanoseconds"},{"metadata":{},"cell_type":"markdown","source":"### Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"> First, we fix our desired figure size."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (25,5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Second, we look at the dataset columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> IT seems `use` and `House overall` show the same data."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=1)\ndataset['use'].resample('D').mean().plot(ax=axes[0])\ndataset['House overall'].resample('D').mean().plot(ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> It's better to remove one of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.drop(columns=['House overall'])\ndataset.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Columns `summary` and `icon` are not numerical. In this tutorial we do not need them. "},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.drop(columns=['summary', 'icon'])\ndataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## pandas.Series.unique: Uniques are returned in order of appearance. Hash table-based unique, therefore does NOT sort.\ndataset['cloudCover'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> It seems for some rows we have an invalid value for `cloudCover`. "},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[dataset['cloudCover']=='cloudCover'].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> There are plenty of ways deal with this kind of invalid values. The simplest one is to remove rows that include this invalid value. but more sophisticated way is to replace them. see this: https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['cloudCover'][56:60]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We replace this missing valuess with the next valid observagion  we have."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['cloudCover'].replace(['cloudCover'], method='bfill', inplace=True)\ndataset['cloudCover'] = dataset['cloudCover'].astype('float')\ndataset['cloudCover'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['cloudCover'][56:60]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Now everything is neumerical. From now on, for the sake of simplicity, let's only work on `hourly` dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.resample('H').mean()\nprint(\"Shape of the data: {} --> n_rows = {}, n_cols = {}\".format(dataset.shape, dataset.shape[0], dataset.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualization"},{"metadata":{},"cell_type":"markdown","source":"> We want to see what is the Microwave usage pattern during a day (24 hours)"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['Microwave'].resample(\"h\").mean().iloc[:24].plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The above plot just shows the usage for 1 specific day (02-Jan). What if we want average croos all the days."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.groupby(dataset.index.hour).mean()['Microwave'].plot(xticks=np.arange(24)).set(xlabel='Daily Hours', ylabel='Microwave Usage (kW)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Now we see that a usual pattern around 11am-1pm and 16pm-18pm. However, at late night there is a weird usage!"},{"metadata":{},"cell_type":"markdown","source":"## Moving Average"},{"metadata":{},"cell_type":"markdown","source":"Let's start with a naive hypothesis: \"tomorrow will be the same as today\". However, instead of a model like $\\hat{y}_{t} = y_{t-1}$ (which is actually a great baseline for any time series prediction problems and sometimes is impossible to beat), we will assume that the future value of our variable depends on the average of its $k$ previous values. Therefore, we will use the **moving average (MA)**.\n\n$\\hat{y}_{t} = \\frac{1}{k} \\displaystyle\\sum^{k}_{n=1} y_{t-n}$\n > More info on MA: https://www.investopedia.com/terms/m/movingaverage.asp\n\n> Pandas has an implementation available with [`DataFrame.rolling(window).mean()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html). The wider the window, the smoother the trend. In the case of very noisy data, which is often encountered in finance, this procedure can help detect common patterns.[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\nfrom sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n\ndef mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\ndef plotMovingAverage(series, window, plot_intervals=False, scale=1.96, plot_anomalies=False):\n\n    \"\"\"\n        series - dataframe with timeseries\n        window - rolling window size \n        plot_intervals - show confidence intervals\n        plot_anomalies - show anomalies \n    \"\"\"\n    \n    rolling_mean = series.rolling(window=window).mean()\n\n    plt.figure(figsize=(25,5))\n    plt.title(\"Moving average with window size = {}\".format(window))\n    plt.plot(rolling_mean, \"g\", label=\"Rolling mean trend\")\n\n    # Plot confidence intervals for smoothed values\n    if plot_intervals:\n        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n        deviation = np.std(series[window:] - rolling_mean[window:])\n        lower_bond = rolling_mean - (mae + scale * deviation)\n        upper_bond = rolling_mean + (mae + scale * deviation)\n        plt.plot(upper_bond, \"r--\", label=\"Upper Bond / Lower Bond\")\n        plt.plot(lower_bond, \"r--\")\n        \n        # Having the intervals, find abnormal values\n        if plot_anomalies:\n            anomalies = pd.DataFrame(index=series.index, columns=series.columns)\n            anomalies[series<lower_bond] = series[series<lower_bond]\n            anomalies[series>upper_bond] = series[series>upper_bond]\n            plt.plot(anomalies, \"ro\", markersize=10)\n        \n    plt.plot(series[window:], label=\"Actual values\")\n    plt.legend(loc=\"upper left\")\n    plt.grid(True)\n\nn_samples = 24*30 # 1 month\ncols = ['use']\nplotMovingAverage(dataset[cols][:n_samples], window=6) # A window of 6 hours","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMovingAverage(dataset[cols][:n_samples], window=12) # A window of 12 hours","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Anomaly Detection \n\nThe simplest way to detect anomaly in time-series is using the moving average as the trend of the data and points that are feviate from the moving average be considered as anomaly."},{"metadata":{"trusted":true},"cell_type":"code","source":"plotMovingAverage(dataset[cols][:n_samples], window=24, plot_intervals=True, plot_anomalies=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"> More info on Detecting Anomalies with Moving Average and Median Decomposition: https://anomaly.io/anomaly-detection-moving-median-decomposition/index.html"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Exponential smoothing\n\nNow, let's see what happens if we start weighting all available observations while exponentially decreasing the weights as we move further back in time. There exists a formula for **[exponential smoothing](https://en.wikipedia.org/wiki/Exponential_smoothing)** that will help us with this:\n\n$$\\hat{y}_{t} = \\alpha \\cdot y_t + (1-\\alpha) \\cdot \\hat y_{t-1} $$\n\nHere the model value is a weighted average between the current true value and the previous model values. The $\\alpha$ weight is called a smoothing factor. It defines how quickly we will \"forget\" the last available true observation. The smaller $\\alpha$ is, the more influence the previous observations have and the smoother the series is.\n\nExponentiality is hidden in the recursiveness of the function -- we multiply by $(1-\\alpha)$ each time, which already contains a multiplication by $(1-\\alpha)$ of previous model values."},{"metadata":{"trusted":true},"cell_type":"code","source":"def exponential_smoothing(series, alpha):\n    \"\"\"\n        series - dataset with timestamps\n        alpha - float [0.0, 1.0], smoothing parameter\n    \"\"\"\n    result = [series[0]] # first value is same as series\n    for n in range(1, len(series)):\n        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n    return result\n\ndef plotExponentialSmoothing(series, alphas):\n    \"\"\"\n        Plots exponential smoothing with different alphas\n        \n        series - dataset with timestamps\n        alphas - list of floats, smoothing parameters\n        \n    \"\"\"\n    with plt.style.context('seaborn-white'):    \n        plt.figure(figsize=(25, 5))\n        for alpha in alphas:\n            plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n        plt.plot(series.values, \"c\", label = \"Actual\")\n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Exponential Smoothing\")\n        plt.grid(True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_samples = 24*30 # 1 month\ncol = 'use'\nplotExponentialSmoothing(dataset[col][:n_samples], [0.3, 0.05])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Autoregressive Integrated Moving Average Model (ARIMA)\nThis acronym is descriptive, capturing the key aspects of the model itself. Briefly, they are:\n\n- AR: Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations.\n- I: Integrated. The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.\n- MA: Moving Average. A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\ndef forcast_ts(data, tt_ratio):\n    X = data.values\n    size = int(len(X) * tt_ratio)\n    train, test = X[0:size], X[size:len(X)]\n    history = [x for x in train]\n    predictions = list()\n    for t in range(len(test)):\n        model = ARIMA(history, order=(5,1,0))\n        model_fit = model.fit(disp=0)\n        output = model_fit.forecast()\n        yhat = output[0]\n        predictions.append(yhat)\n        obs = test[t]\n        history.append(obs)\n        print('progress:%',round(100*(t/len(test))),'\\t predicted=%f, expected=%f' % (yhat, obs), end=\"\\r\")\n    error = mean_squared_error(test, predictions)\n    print('\\n Test MSE: %.3f' % error)\n\n    plt.rcParams[\"figure.figsize\"] = (25,10)\n    preds = np.append(train, predictions)\n    plt.plot(list(preds), color='green', linewidth=3, label=\"Predicted Data\")\n    plt.plot(list(data), color='blue', linewidth=2, label=\"Original Data\")\n    plt.axvline(x=int(len(data)*tt_ratio)-1, linewidth=5, color='red')\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'use'\ndata = dataset[col].resample('w').mean()\ndata.shape\ntt_ratio = 0.70 # Train to Test ratio\nforcast_ts(data, tt_ratio)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'use'\ndata = dataset[col].resample('d').mean()\ndata.shape\ntt_ratio = 0.70 # Train to Test ratio\nforcast_ts(data, tt_ratio)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## A note on time series cross validation\n\nBefore we start building a model, let's first discuss how to estimate model parameters automatically.\n\nThere is nothing unusual here; as always, we have to choose a loss function suitable for the task that will tell us how closely the model approximates the data. Then, using cross-validation, we will evaluate our chosen loss function for the given model parameters, calculate the gradient, adjust the model parameters, and so on, eventually descending to the global minimum.\n\nYou may be asking how to do cross-validation for time series because time series have this temporal structure and one cannot randomly mix values in a fold while preserving this structure. With randomization, all time dependencies between observations will be lost. This is why we will have to use a more tricky approach in optimizing the model parameters. I don't know if there's an official name to this, but on [CrossValidated](https://stats.stackexchange.com/questions/14099/using-k-fold-cross-validation-for-time-series-model-selection), where one can find all answers but the Answer to the Ultimate Question of Life, the Universe, and Everything, the proposed name for this method is \"cross-validation on a rolling basis\".\n\nThe idea is rather simple -- we train our model on a small segment of the time series from the beginning until some $t$, make predictions for the next $t+n$ steps, and calculate an error. Then, we expand our training sample to $t+n$ value, make predictions from $t+n$ until $t+2*n$, and continue moving our test segment of the time series until we hit the last available observation. As a result, we have as many folds as $n$ will fit between the initial training sample and the last observation.\n\n<img src=\"https://habrastorage.org/files/f5c/7cd/b39/f5c7cdb39ccd4ba68378ca232d20d864.png\"/>\n"},{"metadata":{},"cell_type":"markdown","source":"## Credits and Further Reading\n 1. https://mlcourse.ai/articles/topic1-exploratory-data-analysis-with-pandas/\n 2. https://mlcourse.ai/articles/topic2-visual-data-analysis-in-python/\n 3. https://mlcourse.ai/articles/topic9-part1-time-series/\n 4. https://mlcourse.ai/articles/topic9-part2-prophet/\n 5. https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}