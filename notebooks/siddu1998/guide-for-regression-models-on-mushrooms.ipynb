{"cells":[{"metadata":{"_cell_guid":"8213fd2b-83d6-4f2f-8d2c-ef6e454355f7","_uuid":"c7d588cd8d8492abf82e3387092e732b207ccb82"},"cell_type":"markdown","source":"What we aim to do is demonstrate a few regression models which can be used to predict the nature of Mushrooms.  Step 1: Is to import the data set into your current working directory. Starting from scratch pandas is a python library used to handle data files and numpy will be used to solve and manipulate matrices."},{"metadata":{"_cell_guid":"84928967-2197-4814-8627-fe3a12e4a232","_uuid":"025fd29d6b20018dd01fd85e91586c8afefc76b2","collapsed":true},"cell_type":"code","execution_count":null,"source":"import numpy as np\nimport pandas as pd\n\n#Importing the dataset\ndataset=pd.read_csv(\"../input/mushrooms.csv\") \n#the variable dataset will now contain records os mushroom.cvs","outputs":[]},{"metadata":{"_cell_guid":"ed1f6e3e-ae77-4ee0-9a69-1cc6c0515361","_uuid":"9c5bf7b5d0d02de25f05964688b0025b1974b811"},"cell_type":"markdown","source":"Step 2 : Mathematics is the backbone of Machine Learning we need to have numerical information to let sklearn solve the equations. So the data in .csv file is clearly in the form of characters to be more precise alphabets. So we will be using a technique called enoding to encode the categorical features.  LabelEncode and OneHotEncoder are two famous ways classes which are used to encode categorical data. LabelEncoder assigns a value or weight to all the individual categories. While on the other hand we can use OneHotEncoder assigns dummy variables to each of the category and apart from that we have to makesure we avoid the dummy variable trap. To avoid any further confusion i will be using only the LabelEncoder Here."},{"metadata":{"_cell_guid":"c11d5032-dbff-4d8e-8193-54c4d85b9627","_uuid":"cffb01682f84d18279ea0165502ebaa93dc8ab54","collapsed":true},"cell_type":"code","execution_count":null,"source":"#Encoding the Categorical Data\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\nfor col in dataset.columns:\n    dataset[col] = labelencoder.fit_transform(dataset[col])\n","outputs":[]},{"metadata":{"_cell_guid":"57694997-ebc6-4d3c-8267-bbc254e8df8c","_uuid":"f35be980c1adbd8af706775257acd72ccfd83fb9"},"cell_type":"markdown","source":"Now we need to split the data into the feature matrix and dependent variables. The feature matrix contains the all the independent variables with whose help we will be predicting the dependent variable. The frame or dataset's 1st coloumn contains the dependent variable so we will remove the first coloumn and generate the fetaure data and the removed coloumn will be the dependent variable."},{"metadata":{"_cell_guid":"4f906623-e120-455b-859f-1d08c8461b2e","_uuid":"0187a98972b47673a0cb953f3388064a59ea36e9","collapsed":true},"cell_type":"code","execution_count":null,"source":"#Splitting the data into dependet and independent variables\nX=dataset.iloc[:,1:].values\ny=dataset.iloc[:,0].values","outputs":[]},{"metadata":{"_cell_guid":"eac9cc2b-1fcb-4607-9484-7a40e2a7b89c","_uuid":"aa10db7bcdeaea366f28e7a33b48ff05d0796956"},"cell_type":"markdown","source":"Now since we have pre-processed the data all that is left for us to do is generate the training data and test data.  The sklearn has the model_selection library and the trian_test_split method which does the job for. But just incase you have a lot of time in life you can split the data on your own as per convenience. "},{"metadata":{"_cell_guid":"8f66a35d-96a4-417d-acfd-6e75112bec1e","_uuid":"a5db183f5bca795c7b6230038bc312fc277de039","collapsed":true},"cell_type":"code","execution_count":null,"source":"#Splitting the data into training\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)","outputs":[]},{"metadata":{"_cell_guid":"25c03351-0ba2-4af9-b131-3c1e937f0e59","_uuid":"39cdcdf75a2688a894ece9e7bd0db4522b008340"},"cell_type":"markdown","source":"Now to start of we will be using the simplest regressor that is the logistic regression and we will build upon this. LinearRegression is a class in the linear_model and line any programming language we need to create an object of that class. So here we name the object regressor. Now to the object we have to fit the training sets. The regressor's job is to train itself based on the training data we have provided. Once done, all we have to do is test it on the test set. We can use the confusion matrix or the mean method to give a rough estimation of how well we are doing. "},{"metadata":{"_cell_guid":"081a6a29-0e16-4372-9345-859e3e9c48f9","_uuid":"8d3c7f189826ae673e25a66226992aa19a32d81b"},"cell_type":"code","execution_count":null,"source":"#Using Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nregressor=LogisticRegression()\nregressor.fit(X_train,y_train)\ny_pred=regressor.predict(X_test)\nprint('Using Simple Logistic Regression we achieve an accuracy of',np.mean(y_test==y_pred)*100)","outputs":[]},{"metadata":{"_cell_guid":"4f85e5dd-556e-4970-ba8a-a80e15b55786","_uuid":"91b5cb4c9430163e67a47e5371745856e8c3e6d9"},"cell_type":"markdown","source":"In the same way we did Linear Regression we will be doing Decision Tree regression and Random Forest Regression"},{"metadata":{"_cell_guid":"a1c1adb1-f49a-41cf-a623-886bb38baf03","_uuid":"a823c49a6210e45dcea32b2fb42636380b776b2b"},"cell_type":"code","execution_count":null,"source":"#Using Decision Tree Regression\nfrom sklearn.tree import DecisionTreeRegressor\nregressor=DecisionTreeRegressor(random_state=0)\nregressor.fit(X,y)\ny_pred=regressor.predict(X_test)\nprint(\"Using Decision Tree Regression we achieve an accuracy of\",np.mean(y_pred==y_test)*100)\n\n\n#Using Random Forest Regression\nfrom sklearn.ensemble import RandomForestRegressor\nregressor=RandomForestRegressor(n_estimators=300,random_state=0)\nregressor.fit(X,y)\ny_pred=regressor.predict(X_test)\nprint('Using Random Forest Regression we achieve an accuray of',np.mean(y_test==y_pred)*100)","outputs":[]},{"metadata":{"_cell_guid":"7da96065-94be-411d-8abf-946d0e887c9d","_uuid":"e49f65a329295a1b7e5e22a4f2038b12eef48ba1"},"cell_type":"markdown","source":"There is some problem with linear regression, the thing is we use all the features without even considering theier influence on the outcome. So backward elimination is an intresting method where we can filter and remove features whose influence on the outcome is minimum or negligable. The simple maths behing linear regression is it draws a line of the form y=a+bx here we can write a as a*x1 where x1=1, it still stays the same so that is the first thing we do. \n\nNext we first consider all the features and run the model and check the summary, we eliminate the first feature whose weight is unsatisfactory and this process will continue till we reach a state where all the parameters are only those whose weight is significant. Instead of hard coding an checking for each parameter feel free to use the sinppet below which will come up directly with the best parameters. (Click below to expand the table i.e the summary of the best parameters)\n\n**==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.5967      0.035     16.970      0.000       0.528       0.666\nx1             0.0232      0.003      9.094      0.000       0.018       0.028\nx2            -0.3468      0.013    -26.312      0.000      -0.373      -0.321\nx3            -0.0211      0.002    -12.366      0.000      -0.024      -0.018\nx4            -0.4658      0.013    -35.047      0.000      -0.492      -0.440\nx5             0.5251      0.010     52.261      0.000       0.505       0.545\nx6            -0.0099      0.001     -8.342      0.000      -0.012      -0.008\nx7            -0.0617      0.009     -6.922      0.000      -0.079      -0.044\nx8            -0.0769      0.004    -17.140      0.000      -0.086      -0.068\nx9            -0.1601      0.006    -27.978      0.000      -0.171      -0.149\nx10           -0.0570      0.006    -10.232      0.000      -0.068      -0.046\nx11           -0.0107      0.002     -5.651      0.000      -0.014      -0.007\nx12           -0.0059      0.002     -3.155      0.002      -0.010      -0.002\nx13        -1.824e-16   1.11e-17    -16.445      0.000   -2.04e-16   -1.61e-16\nx14            0.3709      0.014     26.451      0.000       0.343       0.398\nx15            0.0185      0.004      5.171      0.000       0.011       0.026\nx16           -0.0459      0.002    -18.779      0.000      -0.051      -0.041\nx17           -0.0104      0.003     -3.132      0.002      -0.017      -0.004\n* x18            0.0180      0.002      9.069      0.000       0.014       0.022**\n\n\n\nYou can see here the value of P is very close to zero, This indicates the deviation is less and these parameters are the most apt to use.\n\n"},{"metadata":{"_cell_guid":"3dc1b05f-8c4b-4e5e-9df3-cc14bb89979c","_uuid":"70c27ef92275cdfc463a14607fa4dd77c309275b"},"cell_type":"markdown","source":""},{"metadata":{"_cell_guid":"b154116f-f3fb-49eb-84d8-a0db842f5247","_uuid":"557c4c64bf59cb771e7c08d41d236eeeee7399ed"},"cell_type":"code","execution_count":null,"source":"#Using Multipe Linear Regression with a combination Back-tracking\nimport statsmodels.formula.api as sm\nX=np.append(arr=np.ones((8124, 1)).astype(int),values=X,axis=1)\nX_opt = X[:, :]\nregressor_OLS = sm.OLS(endog=y, exog=X_opt).fit();\nregressor_OLS.summary()\npvalues = regressor_OLS.pvalues\nsl = 0.05\nwhile 1:\n    high = pvalues[0]\n    highPos = 0\n    for i in range(1, len(pvalues)):\n        if pvalues[i] > high:\n            high = pvalues[i]\n            highPos = i\n    # Check if > SL and if so remove the column\n    if (high > sl):\n        X_opt = np.delete(X_opt, highPos, 1)\n        regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit();\n        pvalues = regressor_OLS.pvalues\n    else:\n        regressor_OLS.summary()\n        break\nprint(regressor_OLS.summary())\nX_opt = X[:,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]]\nX_train ,X_test ,y_train, y_test=train_test_split(X_opt,y,test_size=0.2,random_state=0)\nregressor.fit(X_train,y_train)\ny_pred_with_multi=regressor.predict(X_test)\nprint('Using Multiple Linear Regression we achieve an accuracy of',np.mean(y_test==y_pred)*100)\n\n","outputs":[]}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","version":"3.6.4","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":1}