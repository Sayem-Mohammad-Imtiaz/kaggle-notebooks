{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Distinguish fake & real news 真假新闻判别\n- 对数据执行了一些分析\n- 进行了一部分特征提取工程，并利用一些传统SML方法尝试进行分类\n- 构建一个RNN网络，利用文本内容进行分类\n- 载入transformers的预训练Bert，尝试完成分类任务","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-04T03:26:08.431859Z","iopub.execute_input":"2021-06-04T03:26:08.432262Z","iopub.status.idle":"2021-06-04T03:26:08.443555Z","shell.execute_reply.started":"2021-06-04T03:26:08.43223Z","shell.execute_reply":"2021-06-04T03:26:08.442144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_news = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')\nfake_news = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:26:08.446211Z","iopub.execute_input":"2021-06-04T03:26:08.447062Z","iopub.status.idle":"2021-06-04T03:26:09.386617Z","shell.execute_reply.started":"2021-06-04T03:26:08.447015Z","shell.execute_reply":"2021-06-04T03:26:09.385532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 数据整理\n- 去除real news的文本里特有的前缀（报道来源），只针对新闻本身报道的内容文本进行分析","metadata":{}},{"cell_type":"code","source":"import re \ndef removePrefix(text):\n    pattern = r\"^([A-Z]).*?-\\s\"\n    text = re.sub(pattern, '', text)\n    return text\n\nreal_news.text = real_news.text.apply(lambda x : removePrefix(x))\nreal_news.text[1]","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:26:09.389343Z","iopub.execute_input":"2021-06-04T03:26:09.389767Z","iopub.status.idle":"2021-06-04T03:26:09.975803Z","shell.execute_reply.started":"2021-06-04T03:26:09.389723Z","shell.execute_reply":"2021-06-04T03:26:09.974517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 为数据添加标记维度\n- 去除部分空新闻（内容为空白但不为Null）\n- 去除部分重复出现的新闻","metadata":{}},{"cell_type":"code","source":"real_news['valid'] = 1\nfake_news['valid'] = 0\n\nnews_source = pd.concat([real_news, fake_news], axis=0)\nnews_source = news_source[news_source[['text', 'title', 'date']].duplicated() == False]\nnews_source.text = news_source.text.apply(lambda x: np.nan if len(x.strip()) < 1 else x)\nnews_source = news_source.dropna()\nnews_source.drop(columns=['subject'], inplace=True)\nnews_source.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:26:09.979009Z","iopub.execute_input":"2021-06-04T03:26:09.979526Z","iopub.status.idle":"2021-06-04T03:26:10.635192Z","shell.execute_reply.started":"2021-06-04T03:26:09.979478Z","shell.execute_reply":"2021-06-04T03:26:10.633747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_source.to_csv('./Source.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:26:10.637036Z","iopub.execute_input":"2021-06-04T03:26:10.637728Z","iopub.status.idle":"2021-06-04T03:26:14.437069Z","shell.execute_reply.started":"2021-06-04T03:26:10.637652Z","shell.execute_reply":"2021-06-04T03:26:14.435832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 特征提取\n- 利用Rattle与pandas等工具对数据的一些分布进行观察后，尝试提取出一些有用的信息（较主观/直觉）","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nimport string\nimport pandas as pd\n\nnews_source = pd.read_csv('./Source.csv')\nstop_words = set(stopwords.words('english'))\npunctuations = set(string.punctuation)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:26:14.439544Z","iopub.execute_input":"2021-06-04T03:26:14.43998Z","iopub.status.idle":"2021-06-04T03:26:15.567259Z","shell.execute_reply.started":"2021-06-04T03:26:14.439938Z","shell.execute_reply":"2021-06-04T03:26:15.566277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 获取真假新闻的标题/文本中前十的高频词\n- 将任意一条新闻的标题/文本内容中出现此两类高频词的次数当做特征，分别进行统计","metadata":{}},{"cell_type":"code","source":"def token_freq(df, feature, valid):\n    tar_texts = df[df.valid == valid][feature].values\n    texts = ' '.join(tar_texts).lower()\n    tokens = ''.join(char for char in texts if char not in punctuations).split()\n    tokens_cleaned = [word for word in tokens if word not in stop_words]\n    return pd.DataFrame(nltk.FreqDist(tokens_cleaned).most_common(10))[0]\n\nreal_title_freq = token_freq(news_source, 'title', 1)\nreal_text_freq = token_freq(news_source, 'text', 1)\nfake_title_freq = token_freq(news_source, 'title', 0)\nfake_text_freq = token_freq(news_source, 'text', 0)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:26:15.569495Z","iopub.execute_input":"2021-06-04T03:26:15.570258Z","iopub.status.idle":"2021-06-04T03:26:38.010782Z","shell.execute_reply.started":"2021-06-04T03:26:15.57021Z","shell.execute_reply":"2021-06-04T03:26:38.009568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_freq_token(text, freq_df):\n    text = text.lower()\n    tokens = ''.join(char for char in text if char not in punctuations).split()\n    count = 0\n    for token in tokens:\n        if token in freq_df.values:\n            count += 1\n    return count","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:26:38.012638Z","iopub.execute_input":"2021-06-04T03:26:38.013111Z","iopub.status.idle":"2021-06-04T03:26:38.02283Z","shell.execute_reply.started":"2021-06-04T03:26:38.013052Z","shell.execute_reply":"2021-06-04T03:26:38.019462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 即：标题中出现的真/假新闻标题高频词数目，文本中出现的真/假新闻文本高频词数目","metadata":{}},{"cell_type":"code","source":"news_source['fake_title_token_freq'] = news_source.title.apply(lambda x : count_freq_token(x, fake_title_freq))\nnews_source['read_title_token_freq'] = news_source.title.apply(lambda x : count_freq_token(x, real_title_freq))\nnews_source['fake_text_token_freq'] = news_source.title.apply(lambda x : count_freq_token(x, fake_text_freq))\nnews_source['read_text_token_freq'] = news_source.title.apply(lambda x : count_freq_token(x, real_text_freq))","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:26:38.024557Z","iopub.execute_input":"2021-06-04T03:26:38.02523Z","iopub.status.idle":"2021-06-04T03:26:58.755795Z","shell.execute_reply.started":"2021-06-04T03:26:38.025182Z","shell.execute_reply":"2021-06-04T03:26:58.754746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 查看部分标点的运用，此处选择了'？'与'！'两个通常带有浓烈感情色彩和引导性的符号","metadata":{}},{"cell_type":"code","source":"def countPunctuation(text):\n    ques = re.subn(r\"\\?\", \"\", text)[1]\n    exclam = re.subn(r\"\\!\", \"\", text)[1]\n    return ques, exclam","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:26:58.757325Z","iopub.execute_input":"2021-06-04T03:26:58.757764Z","iopub.status.idle":"2021-06-04T03:26:58.767399Z","shell.execute_reply.started":"2021-06-04T03:26:58.75772Z","shell.execute_reply":"2021-06-04T03:26:58.766115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_source[['title_ques_num', 'title_exclam_num']] = news_source.title.apply(lambda x : pd.Series(countPunctuation(x)))\nnews_source[['text_ques_num', 'text_exclam_num']] = news_source.text.apply(lambda x : pd.Series(countPunctuation(x)))","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:26:58.769253Z","iopub.execute_input":"2021-06-04T03:26:58.769969Z","iopub.status.idle":"2021-06-04T03:27:24.092665Z","shell.execute_reply.started":"2021-06-04T03:26:58.769924Z","shell.execute_reply":"2021-06-04T03:27:24.091438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 标题长度与标题长度占正文长度比，不过要注意如果新闻文本内容本身不完整，此项会受到较大影响","metadata":{}},{"cell_type":"code","source":"news_source['title_len'] = news_source.title.apply(lambda x : len(x))\nnews_source['title_ratio'] = news_source.text.apply(lambda x : len(x))\nnews_source['title_ratio'] = news_source['title_len'] / news_source['title_ratio']","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:27:24.100005Z","iopub.execute_input":"2021-06-04T03:27:24.100327Z","iopub.status.idle":"2021-06-04T03:27:24.162099Z","shell.execute_reply.started":"2021-06-04T03:27:24.100286Z","shell.execute_reply":"2021-06-04T03:27:24.161274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_source.to_csv('./NewsAna.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:27:24.164423Z","iopub.execute_input":"2021-06-04T03:27:24.16486Z","iopub.status.idle":"2021-06-04T03:27:27.904436Z","shell.execute_reply.started":"2021-06-04T03:27:24.164816Z","shell.execute_reply":"2021-06-04T03:27:27.902886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 查看提取出的各个特征与其类别的关系，是否存在一定程度的正/负相关","metadata":{}},{"cell_type":"code","source":"news_source.corr().valid","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:27:27.906355Z","iopub.execute_input":"2021-06-04T03:27:27.90711Z","iopub.status.idle":"2021-06-04T03:27:27.935696Z","shell.execute_reply.started":"2021-06-04T03:27:27.907062Z","shell.execute_reply":"2021-06-04T03:27:27.934151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_source.drop(columns=['Unnamed: 0', 'title', 'text', 'date'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:27:27.937556Z","iopub.execute_input":"2021-06-04T03:27:27.938032Z","iopub.status.idle":"2021-06-04T03:27:27.957266Z","shell.execute_reply.started":"2021-06-04T03:27:27.937972Z","shell.execute_reply":"2021-06-04T03:27:27.956293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_source.reset_index()\nnews_source = news_source.sample(frac=1.)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:27:27.960581Z","iopub.execute_input":"2021-06-04T03:27:27.960938Z","iopub.status.idle":"2021-06-04T03:27:27.971915Z","shell.execute_reply.started":"2021-06-04T03:27:27.960905Z","shell.execute_reply":"2021-06-04T03:27:27.970877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 一些常见SML模型预测\n- 随机分为train与test集，利用提取出的特征进行分类\n- 采用RandomForest，DecisionTree和LinearSVC三个模型","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\n\ntrain_set, test_set = train_test_split(news_source, test_size=0.2, random_state=7)\ntrain_x = train_set[['fake_title_token_freq', 'read_title_token_freq', 'fake_text_token_freq', \n                 'read_text_token_freq', 'title_ques_num', 'title_exclam_num', 'text_ques_num', \n                 'text_exclam_num', 'title_len', 'title_ratio']]\ntrain_y = train_set['valid']\ntest_x = test_set[['fake_title_token_freq', 'read_title_token_freq', 'fake_text_token_freq', \n                 'read_text_token_freq', 'title_ques_num', 'title_exclam_num', 'text_ques_num', \n                 'text_exclam_num', 'title_len', 'title_ratio']]\ntest_y = test_set['valid']","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:27:27.973976Z","iopub.execute_input":"2021-06-04T03:27:27.974652Z","iopub.status.idle":"2021-06-04T03:27:28.138043Z","shell.execute_reply.started":"2021-06-04T03:27:27.974606Z","shell.execute_reply":"2021-06-04T03:27:28.137005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_report(model, tar_x, tar_y):\n    pred = model.predict(tar_x)\n    f1 = f1_score(tar_y, pred)\n    print(\"f1-score: \", f1)\n    acc = accuracy_score(tar_y, pred)\n    print(\"accuracy: \", acc)\n    cm = confusion_matrix(tar_y, pred)\n    print(\"confusion matrix:\\n\",cm)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:27:28.139961Z","iopub.execute_input":"2021-06-04T03:27:28.140632Z","iopub.status.idle":"2021-06-04T03:27:28.148135Z","shell.execute_reply.started":"2021-06-04T03:27:28.14057Z","shell.execute_reply":"2021-06-04T03:27:28.146769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"randomForest = RandomForestClassifier(random_state=7)\nrandomForest.fit(train_x, train_y)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:27:28.150077Z","iopub.execute_input":"2021-06-04T03:27:28.150689Z","iopub.status.idle":"2021-06-04T03:27:31.254517Z","shell.execute_reply.started":"2021-06-04T03:27:28.150638Z","shell.execute_reply":"2021-06-04T03:27:31.253336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"RF on training set:\")\nmodel_report(randomForest, train_x, train_y)\nprint(\"\\nRF on testing set:\")\nmodel_report(randomForest, test_x, test_y)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:27:31.256282Z","iopub.execute_input":"2021-06-04T03:27:31.25676Z","iopub.status.idle":"2021-06-04T03:27:32.02972Z","shell.execute_reply.started":"2021-06-04T03:27:31.256711Z","shell.execute_reply":"2021-06-04T03:27:32.02797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndecisionTree = DecisionTreeClassifier()\ndecisionTree.fit(train_x, train_y)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:27:32.031439Z","iopub.execute_input":"2021-06-04T03:27:32.032032Z","iopub.status.idle":"2021-06-04T03:27:32.152108Z","shell.execute_reply.started":"2021-06-04T03:27:32.031981Z","shell.execute_reply":"2021-06-04T03:27:32.15069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"tree on training set:\")\nmodel_report(decisionTree, train_x, train_y)\nprint(\"\\ntree on testing set:\")\nmodel_report(decisionTree, test_x, test_y)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:27:32.153988Z","iopub.execute_input":"2021-06-04T03:27:32.154608Z","iopub.status.idle":"2021-06-04T03:27:32.256627Z","shell.execute_reply.started":"2021-06-04T03:27:32.154559Z","shell.execute_reply":"2021-06-04T03:27:32.254423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import LinearSVC\n\nlinearSVC = LinearSVC(max_iter=5000, penalty='l2')\nlinearSVC.fit(train_x, train_y)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:27:32.258679Z","iopub.execute_input":"2021-06-04T03:27:32.25924Z","iopub.status.idle":"2021-06-04T03:27:43.52904Z","shell.execute_reply.started":"2021-06-04T03:27:32.259169Z","shell.execute_reply":"2021-06-04T03:27:43.5279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"LinearSVC on training set:\")\nmodel_report(linearSVC, train_x, train_y)\nprint(\"\\nLinearSVC on testing set:\")\nmodel_report(linearSVC, test_x, test_y)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:27:43.530747Z","iopub.execute_input":"2021-06-04T03:27:43.531159Z","iopub.status.idle":"2021-06-04T03:27:43.62454Z","shell.execute_reply.started":"2021-06-04T03:27:43.531117Z","shell.execute_reply":"2021-06-04T03:27:43.622782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 预训练Bert模型\n- 采用huggingface的预训练模型bert-based-uncased\n- Tokenizer也是他们的","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModel, AutoConfig, AutoTokenizer, AdamW\nfrom torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler\nimport torch\n\nbatch_size = 64\nepoch_num = 4\nmax_seq_length = 128\n\nnews_source = pd.read_csv('./NewsAna.csv')\nnews_source.reset_index()\nnews_source = news_source.sample(frac=1.)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbert_model_path = \"bert-base-uncased\"","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:27:43.62634Z","iopub.execute_input":"2021-06-04T03:27:43.626807Z","iopub.status.idle":"2021-06-04T03:27:45.15299Z","shell.execute_reply.started":"2021-06-04T03:27:43.626763Z","shell.execute_reply":"2021-06-04T03:27:45.151784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NewsModel(torch.nn.Module):\n    def __init__(self, bert_model=bert_model_path, num_class=1):\n        super(NewsModel, self).__init__()\n        # 加载预训练模型(from huggingface)\n        self.bert_layer = AutoModel.from_pretrained(pretrained_model_name_or_path=bert_model)\n        # 或许可以补入一点特征数据，来求取最终结果\n        self.bert_config = AutoConfig.from_pretrained(pretrained_model_name_or_path=bert_model)\n        self.mid_dim = self.bert_config.hidden_size\n        # 进行最终分类\n        self.output = torch.nn.Sequential(\n            torch.nn.Linear(self.mid_dim, self.mid_dim//2),\n            torch.nn.LeakyReLU(),\n            torch.nn.Linear(self.mid_dim//2, self.mid_dim),\n            torch.nn.LeakyReLU(),\n            torch.nn.Linear(self.mid_dim, num_class),\n            torch.nn.Sigmoid()\n        )\n    def forward(self, input_ids, attn_mask=None):\n        bert_out = self.bert_layer(input_ids=input_ids, attention_mask=attn_mask)[1]\n        output = self.output(bert_out)\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:27:45.154815Z","iopub.execute_input":"2021-06-04T03:27:45.155241Z","iopub.status.idle":"2021-06-04T03:27:45.165247Z","shell.execute_reply.started":"2021-06-04T03:27:45.155182Z","shell.execute_reply":"2021-06-04T03:27:45.163734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 利用新闻的text文本内容进行分析\n- 最大长度限制在128","metadata":{}},{"cell_type":"code","source":"def covertTokenFormat(df, bert_model_path, max_seq_len):\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=bert_model_path)\n    texts = df.text.tolist()\n    text_tokens = tokenizer(texts, padding='max_length', max_length=max_seq_len, truncation=True, return_tensors=\"pt\")\n    labels = torch.tensor(df.valid.values, dtype=torch.float)\n    return text_tokens, labels","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:27:45.167734Z","iopub.execute_input":"2021-06-04T03:27:45.168208Z","iopub.status.idle":"2021-06-04T03:27:45.18024Z","shell.execute_reply.started":"2021-06-04T03:27:45.168157Z","shell.execute_reply":"2021-06-04T03:27:45.178638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def news_bert_report(pred, label):\n    tar_y = label.squeeze()\n    pred_y = []\n    for item in pred.squeeze():\n        if item >= 0.5:\n            pred_y.append(1)\n        else:\n            pred_y.append(0)\n    f1 = f1_score(tar_y, pred_y)\n    print(\"f1-score: \", f1)\n    acc = accuracy_score(tar_y, pred_y)\n    print(\"accuracy: \", acc)\n    return f1, acc","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:27:45.18217Z","iopub.execute_input":"2021-06-04T03:27:45.182804Z","iopub.status.idle":"2021-06-04T03:27:45.193897Z","shell.execute_reply.started":"2021-06-04T03:27:45.18275Z","shell.execute_reply":"2021-06-04T03:27:45.192567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create model\nnews_model = NewsModel(bert_model=bert_model_path).to(device)\noptimiser = AdamW(news_model.parameters(), lr=1e-5)\n# parepare the train, test set\ntrain_set, test_set = train_test_split(news_source, test_size=0.2, random_state=7)\n# dataloader - training set\ntext_tokens, labels = covertTokenFormat(train_set, bert_model_path, max_seq_length)\ntrain_data = TensorDataset(text_tokens.input_ids, text_tokens.attention_mask, labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:27:45.195294Z","iopub.execute_input":"2021-06-04T03:27:45.197648Z","iopub.status.idle":"2021-06-04T03:29:04.119748Z","shell.execute_reply.started":"2021-06-04T03:27:45.197599Z","shell.execute_reply":"2021-06-04T03:29:04.118745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 在训练中，将每个epoch后的模型进行保存","metadata":{}},{"cell_type":"code","source":"from torch.nn import functional as F\n\ndef train_bert(bert, optimiser, train_dataloader, save_check_point=True):\n    bert = bert.to(device)\n    bert.train()\n    for epoch in range(epoch_num): \n        epoch_loss = 0\n        pred_lis = torch.Tensor()\n        label_lis = torch.Tensor()\n        for batch, (token_ids, attn_mask, label) in enumerate(train_dataloader):\n            # keep all the parameters in the same device\n            token_ids = token_ids.to(device)\n            attn_mask = attn_mask.to(device)\n            label = label.to(device)\n            # the output will be in the same device with the model\n            outputs = bert(token_ids, attn_mask)\n            loss = F.binary_cross_entropy(outputs.squeeze(), label)\n            # do the backprop and update the parameters\n            optimiser.zero_grad()\n            loss.backward()\n            optimiser.step()\n            epoch_loss += loss.cpu().data.numpy()\n            pred_lis = torch.cat([pred_lis, outputs.cpu().squeeze()])\n            label_lis = torch.cat([label_lis, label.cpu().squeeze()])\n            if batch % 50 == 0:\n                print(\"Current batch loss :\", loss.cpu().data.numpy())\n        print(\"Now epoch :\", epoch+1, \" Total epoch loss is: \", epoch_loss)\n        news_bert_report(pred_lis.detach().numpy(), label_lis.detach().numpy())\n        if save_check_point:\n            torch.save({'model_state_dict': bert.state_dict()}, './bert_model_' + str(epoch) + '.weights')\n    return bert","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:35:30.59041Z","iopub.execute_input":"2021-06-04T03:35:30.590825Z","iopub.status.idle":"2021-06-04T03:35:30.603254Z","shell.execute_reply.started":"2021-06-04T03:35:30.590792Z","shell.execute_reply":"2021-06-04T03:35:30.601198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_model = train_bert(\n    news_model,\n    optimiser,\n    train_dataloader\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:35:35.464767Z","iopub.execute_input":"2021-06-04T03:35:35.46511Z","iopub.status.idle":"2021-06-04T03:59:10.427079Z","shell.execute_reply.started":"2021-06-04T03:35:35.465078Z","shell.execute_reply":"2021-06-04T03:59:10.425777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 模型效能评估，注意内存溢出","metadata":{}},{"cell_type":"code","source":"test_tokens, test_labels = covertTokenFormat(test_set, bert_model_path, max_seq_length)\ntest_data = TensorDataset(test_tokens.input_ids, test_tokens.attention_mask, test_labels)\ntest_sampler = RandomSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=16)\n\n# close the gradient\nwith torch.no_grad():\n    news_model.eval()\n    pred_lis = torch.Tensor()\n    label_lis = torch.Tensor()\n    for batch, (token_ids, attn_mask, label) in enumerate(test_dataloader):\n        token_ids = token_ids.to(device)\n        attn_mask = attn_mask.to(device)\n        label = label.to(device)\n        outputs = news_model(token_ids, attn_mask)\n        pred_lis = torch.cat([pred_lis, outputs.cpu().squeeze()])\n        label_lis = torch.cat([label_lis, label.cpu().squeeze()])\n\n    news_bert_report(pred_lis.detach().numpy(), label_lis.detach().numpy())","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:59:10.431309Z","iopub.execute_input":"2021-06-04T03:59:10.432Z","iopub.status.idle":"2021-06-04T03:59:56.253431Z","shell.execute_reply.started":"2021-06-04T03:59:10.431952Z","shell.execute_reply":"2021-06-04T03:59:56.252166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RNN模型构建（GRU)\n- 使用了Glove的词向量进行Embedding\n- 进行变长输入的padding，使其能够以Batch为单位放入GPU处理\n- 这里使用最终实际位置的hidden output进行句子描述，注意拼接(bidirectional=True时)\n- 标明了batch_first=True，不转置输入矩阵的前两维\n- 有利用multi-head attention，注意输入的词向量维度要能整除head num","metadata":{}},{"cell_type":"code","source":"import torch\nimport string\nimport torch.nn.utils.rnn as rnn_utils\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom torchtext.vocab import GloVe\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n\nimport pandas as pd\nnews_source = pd.read_csv('./NewsAna.csv')\nnews_source.reset_index()\nnews_source = news_source.sample(frac=1.)\n\nstop_words = set(stopwords.words('english'))\npunctuations = set(string.punctuation)\ncache_dir = './glove'\nglove = GloVe(name='6B', dim=50, cache=cache_dir)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nepoch_num = 40\nbatch_size = 128\nlr = .001\ninput_dim = 50\nnum_heads = 5\noutput_dim = 1\ngru_num_layers = 2","metadata":{"execution":{"iopub.status.busy":"2021-06-04T03:59:56.255179Z","iopub.execute_input":"2021-06-04T03:59:56.255685Z","iopub.status.idle":"2021-06-04T03:59:57.930348Z","shell.execute_reply.started":"2021-06-04T03:59:56.255639Z","shell.execute_reply":"2021-06-04T03:59:57.929348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 添加一个简单的AttentionBlock层\n- 非常简单，没有用Layer Norm什么的","metadata":{}},{"cell_type":"code","source":"class AttentionBlock(torch.nn.Module):\n    def __init__(self, input_dim, attn_heads):\n        super(AttentionBlock, self).__init__()\n        self.query = torch.nn.Linear(input_dim, input_dim, bias=False)\n        self.key = torch.nn.Linear(input_dim, input_dim, bias=False)\n        self.value = torch.nn.Linear(input_dim, input_dim, bias=False)\n        self.mul_attn = torch.nn.MultiheadAttention(input_dim, attn_heads)\n        self.output = torch.nn.Linear(input_dim, input_dim, bias=False)\n        \n    def forward(self, x):\n        query = self.query(x)\n        key = self.key(x)\n        value = self.value(x)\n        attentions = self.mul_attn(query, key, value)[0] + x\n        output = self.output(attentions) + x\n        return output\n        \n\nclass NewsGRUModel(torch.nn.Module):\n    def __init__(self, input_dim, output_dim, vocab_size=0, num_heads=8, gru_num_layers=1, bidirectional=True, dropout=.1, hidden_layers = [128, 64, 128]):\n        super(NewsGRUModel, self).__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = input_dim // 2 if bidirectional else input_dim\n        self.output_dim = output_dim\n        self.head_num = num_heads\n        self.gru_num_layers = gru_num_layers\n        self.vocab_size = vocab_size\n        self.bidirectional = bidirectional\n        # Embedding\n        if not self.vocab_size == 0:\n            self.embed = torch.nn.Embedding(vocab_size, input_dim)\n        # GRUs\n        self.gru_layer = torch.nn.GRU(\n            input_size=self.input_dim, \n            hidden_size=self.hidden_dim, \n            num_layers=self.gru_num_layers, \n            bidirectional=self.bidirectional, \n            batch_first=True,\n            dropout=dropout\n        )\n        # Add attentions\n        self.attention = AttentionBlock(self.input_dim, self.head_num)\n        # The FFN to adjust the outputs\n        if hidden_layers and not len(hidden_layers) == 0:\n            # the dim is not changed through the two GRU layer\n            hidden_list = [torch.nn.Linear(self.input_dim, hidden_layers[0])]\n            for idx in range(len(hidden_layers) - 1):\n                hidden_list.append(torch.nn.Linear(hidden_layers[idx], hidden_layers[idx + 1]))\n            self.hidden_layer_list = torch.nn.ModuleList(hidden_list)\n            # init the weights\n            for layer in self.hidden_layer_list: \n                torch.nn.init.kaiming_normal_(layer.weight.data)\n            self.hidden_out_dim = hidden_layers[-1]\n        else:\n            self.hidden_layer_list = []\n            self.hidden_out_dim = self.input_dim\n        # Output layer\n        self.output = torch.nn.Linear(self.hidden_out_dim, self.output_dim)\n        # Other functions\n        self.activate = torch.nn.ReLU()\n        self.dropout = torch.nn.Dropout(dropout)\n    \n    def forward(self, x, x_len, pretrained_embed=False):\n        if not (pretrained_embed or self.vocab_size == 0):\n            x = self.embed(x)\n        # pack padded seq\n        x = rnn_utils.pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n        # GRU layer\n        output, hidden_info = self.gru_layer(x)\n        # get the final hidden outputs\n        if self.bidirectional:\n            fin_out = torch.cat([hidden_info[0,:,:], hidden_info[1,:,:]], dim=1).unsqueeze(dim=1)\n        else:\n            fin_out = hidden_info\n        # attentions\n        output = self.attention(fin_out)\n        # ffn process\n        for layer in self.hidden_layer_list:\n            output = layer(output)\n            output = self.activate(output)\n            output = self.dropout(output)\n        # output layer, get logits\n        output = self.output(output)\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-06-04T04:00:31.424881Z","iopub.execute_input":"2021-06-04T04:00:31.425243Z","iopub.status.idle":"2021-06-04T04:00:31.453579Z","shell.execute_reply.started":"2021-06-04T04:00:31.42521Z","shell.execute_reply":"2021-06-04T04:00:31.452058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 忽略停用词和符号，这次使用新闻标题进行分类分析","metadata":{}},{"cell_type":"code","source":"# 将句子转化为tokens，用了Glove的预训练词向量\ndef covertTextToGolveVec(df):\n    golve_vecs = []\n    titles = df.title.values\n    for title in titles:\n        tokens = word_tokenize(title.lower())\n        for token in tokens:\n            if token in stop_words or token in punctuations:\n                tokens.remove(token)\n        golve_vecs.append(glove.get_vecs_by_tokens(tokens))\n    return golve_vecs","metadata":{"execution":{"iopub.status.busy":"2021-06-04T04:00:34.126213Z","iopub.execute_input":"2021-06-04T04:00:34.126632Z","iopub.status.idle":"2021-06-04T04:00:34.132963Z","shell.execute_reply.started":"2021-06-04T04:00:34.126583Z","shell.execute_reply":"2021-06-04T04:00:34.131471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 定义collate_fn函数，获取各句子长度并对其进行padding，以放入Tensor之中在GPU进行批处理","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nclass NewsDataset(Dataset):\n    def __init__(self, train_x, train_y):\n        self.train_x = train_x\n        self.train_y = train_y\n    def __len__(self):\n        return len(self.train_y)\n    def __getitem__(self, idx):\n        idx -= 1\n        return self.train_x[idx], self.train_y[idx]\n    \ndef collate_fn(train_data):\n    (train_data, train_label) = zip(*train_data)\n    data_length = [len(data) for data in train_data]\n    train_data = rnn_utils.pad_sequence(train_data, batch_first=True, padding_value=0)\n    train_label = torch.Tensor(train_label)\n    return train_data, train_label, data_length","metadata":{"execution":{"iopub.status.busy":"2021-06-04T04:00:35.527871Z","iopub.execute_input":"2021-06-04T04:00:35.528235Z","iopub.status.idle":"2021-06-04T04:00:35.537677Z","shell.execute_reply.started":"2021-06-04T04:00:35.528202Z","shell.execute_reply":"2021-06-04T04:00:35.536276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn_set, test_set = train_test_split(news_source, test_size=0.2, random_state=7)\ntrain_set, val_set = train_test_split(learn_set, test_size=0.2, random_state=77)\n\nnews_vecs = covertTextToGolveVec(train_set)\nlabel_vecs = train_set.valid.values\ntrain_dataset = NewsDataset(news_vecs, label_vecs)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn)\n\nval_news = covertTextToGolveVec(val_set)\nval_labels = val_set.valid.values\nval_dataset = NewsDataset(val_news, val_labels)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n\nnews_gru = NewsGRUModel(input_dim=input_dim, output_dim=output_dim, num_heads=num_heads, gru_num_layers=gru_num_layers, bidirectional=True).to(device)\noptimiser = torch.optim.Adam(news_gru.parameters(), lr=lr)\nloss_func = torch.nn.MSELoss()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T04:00:37.610418Z","iopub.execute_input":"2021-06-04T04:00:37.610805Z","iopub.status.idle":"2021-06-04T04:00:48.59437Z","shell.execute_reply.started":"2021-06-04T04:00:37.610771Z","shell.execute_reply":"2021-06-04T04:00:48.593192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def news_gru_report(pred, label):\n    tar_y = label.squeeze()\n    pred_y = []\n    for item in pred.squeeze():\n        if item >= 0.5:\n            pred_y.append(1)\n        else:\n            pred_y.append(0)\n    f1 = f1_score(tar_y, pred_y)\n    acc = accuracy_score(tar_y, pred_y)\n    cm = confusion_matrix(tar_y, pred_y)\n    return f1, acc, cm","metadata":{"execution":{"iopub.status.busy":"2021-06-04T04:00:48.59711Z","iopub.execute_input":"2021-06-04T04:00:48.597609Z","iopub.status.idle":"2021-06-04T04:00:48.605074Z","shell.execute_reply.started":"2021-06-04T04:00:48.597562Z","shell.execute_reply":"2021-06-04T04:00:48.603665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 会将在Validation set上表现最佳的模型保存","metadata":{}},{"cell_type":"code","source":"def check_save_model(model, acc, his_acc):\n    model_path = './news_gru_best_val.weights'\n    flag = True\n    for his in his_acc:\n        if acc < his:\n            flag = False\n    if flag:\n        torch.save({\n            'model_state_dict': model.state_dict()\n        }, model_path)\n\ndef train_gru(news_gru, optimiser, loss_func, train_dataloader, val_dataloader, save_best_cp=True):\n    news_gru = news_gru.to(device)\n    news_gru.train()\n    train_loss = []\n    val_loss = []\n    train_acc = []\n    val_acc = []\n    train_f1 = []\n    val_f1 = []\n    for epoch in range(epoch_num):\n        # now start training\n        epoch_loss = 0\n        pred_lis = torch.Tensor()\n        label_lis = torch.Tensor()\n        for batch_idx, (data, label, length) in enumerate(train_dataloader):\n            input_vec = data.to(device)\n            label = label.to(device)\n            pred = news_gru(input_vec, length, True)\n            loss = loss_func(pred.squeeze(), label.squeeze())\n            optimiser.zero_grad()\n            loss.backward()\n            optimiser.step()\n            # record the batch ouputs\n            epoch_loss += loss.cpu().data.numpy()\n            pred_lis = torch.cat([pred_lis, pred.cpu().squeeze()])\n            label_lis = torch.cat([label_lis, label.cpu().squeeze()])\n        # record the epoch output\n        train_loss.append(epoch_loss)\n        f1, acc, cm = news_gru_report(pred_lis.detach().numpy(), label_lis.detach().numpy())\n        train_acc.append(acc)\n        train_f1.append(f1)\n        # on the validation set\n        epoch_val = 0\n        val_pred = torch.Tensor()\n        val_label = torch.Tensor()\n        # stop the gradient, we not gonna do backprop on validation set\n        with torch.no_grad():\n            for batch_idx, (data, label, length) in enumerate(val_dataloader):\n                input_vec = data.to(device)\n                label = label.to(device)\n                pred = news_gru(input_vec, length, True)\n                loss = loss_func(pred.squeeze(), label.squeeze())\n                # record the batch output\n                epoch_val += loss.cpu().data.numpy()\n                val_pred = torch.cat([val_pred, pred.cpu().squeeze()])\n                val_label = torch.cat([val_label, label.cpu().squeeze()])\n        # record the epoch output\n        val_loss.append(epoch_val)\n        vf1, vacc, vcm = news_gru_report(val_pred.detach().numpy(), val_label.detach().numpy())\n        val_acc.append(vacc)\n        val_f1.append(vf1)\n        if save_best_cp and epoch > (epoch_num / 10):\n            check_save_model(news_gru, vacc, val_acc)\n        if (epoch + 1) % 10 == 0 or epoch == 0:\n            print(\"f1-score on Training set:\", f1, \", Validation set:\", vf1)\n            print(\"accuracy on Training set:\", acc, \", Validation set:\", vacc)\n            print(\"confusion matrix (Training set):\\n\",cm, \"\\nconfusion matrix (Validation set):\\n\", vcm)\n            print(\"Current epoch:\", epoch + 1, \" Total loss:\", epoch_loss, \"\\n\")\n    torch.save({'model_state_dict': news_gru.state_dict()}, './news_gru_final.weights')\n    return news_gru, train_loss, val_loss, train_acc, val_acc, train_f1, val_f1","metadata":{"execution":{"iopub.status.busy":"2021-06-04T04:00:48.606781Z","iopub.execute_input":"2021-06-04T04:00:48.607492Z","iopub.status.idle":"2021-06-04T04:00:48.630485Z","shell.execute_reply.started":"2021-06-04T04:00:48.607443Z","shell.execute_reply":"2021-06-04T04:00:48.629567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_gru, train_loss, val_loss, train_acc, val_acc, train_f1, val_f1 = train_gru(\n    news_gru=news_gru, \n    optimiser=optimiser, \n    loss_func=loss_func, \n    train_dataloader=train_dataloader, \n    val_dataloader=val_dataloader, \n    save_best_cp=True\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T04:00:48.632895Z","iopub.execute_input":"2021-06-04T04:00:48.633501Z","iopub.status.idle":"2021-06-04T04:04:48.210601Z","shell.execute_reply.started":"2021-06-04T04:00:48.633453Z","shell.execute_reply":"2021-06-04T04:04:48.209682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 将训练过程的变化结果绘出","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(16,5))\nsns.lineplot(data=train_loss)\nsns.lineplot(data=val_loss)\nplt.title(\"The loss curve on training & validation set\")\nplt.show()\n\nplt.figure(figsize=(16,5))\nsns.lineplot(data=train_acc)\nsns.lineplot(data=val_acc)\nplt.title(\"The accuracy curve on training & validation set\")\nplt.show()\n\nplt.figure(figsize=(16,5))\nsns.lineplot(data=train_f1)\nsns.lineplot(data=val_f1)\nplt.title(\"The f1 curve on training & validation set\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-04T04:04:48.2168Z","iopub.execute_input":"2021-06-04T04:04:48.217088Z","iopub.status.idle":"2021-06-04T04:04:48.967771Z","shell.execute_reply.started":"2021-06-04T04:04:48.217058Z","shell.execute_reply":"2021-06-04T04:04:48.966789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 训练结果测试（跑完所有epochs的最终模型），同样注意内存溢出问题","metadata":{}},{"cell_type":"code","source":"test_news = covertTextToGolveVec(test_set)\ntest_labels = test_set.valid.values\ntest_dataset = NewsDataset(test_news, test_labels)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)\n\nwith torch.no_grad():\n    news_gru = news_gru.to(device)\n    news_gru.eval()\n    pred_lis = torch.Tensor()\n    label_lis = torch.Tensor()\n    for batch_idx, (data, label, length) in enumerate(test_dataloader):\n        input_vec = data.to(device)\n        label = label.to(device)\n        pred = news_gru(input_vec, length, True)\n        pred_lis = torch.cat([pred_lis, pred.cpu().squeeze()])\n        label_lis = torch.cat([label_lis, label.cpu().squeeze()])\n    f1, acc, cm = news_gru_report(pred_lis.detach().numpy(), label_lis.detach().numpy())\n    print(\"f1-score on testing set:\", f1)\n    print(\"accuracy on testing set:\", acc)\n    print(\"confusion matrix (testing set):\\n\",cm)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T04:04:48.970565Z","iopub.execute_input":"2021-06-04T04:04:48.970994Z","iopub.status.idle":"2021-06-04T04:04:52.94604Z","shell.execute_reply.started":"2021-06-04T04:04:48.970948Z","shell.execute_reply":"2021-06-04T04:04:52.944883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 训练结果测试（Validation set上的最强模型）","metadata":{}},{"cell_type":"code","source":"model_path = './news_gru_best_val.weights'\ncheckpoint = torch.load(model_path)\nnews_gru.load_state_dict(checkpoint['model_state_dict'])\n\nwith torch.no_grad():\n    news_gru = news_gru.to(device)\n    news_gru.eval()\n    pred_lis = torch.Tensor()\n    label_lis = torch.Tensor()\n    for batch_idx, (data, label, length) in enumerate(test_dataloader):\n        input_vec = data.to(device)\n        label = label.to(device)\n        pred = news_gru(input_vec, length, True)\n        pred_lis = torch.cat([pred_lis, pred.cpu().squeeze()])\n        label_lis = torch.cat([label_lis, label.cpu().squeeze()])\n    f1, acc, cm = news_gru_report(pred_lis.detach().numpy(), label_lis.detach().numpy())\n    print(\"f1-score on testing set:\", f1)\n    print(\"accuracy on testing set:\", acc)\n    print(\"confusion matrix (testing set):\\n\",cm)","metadata":{"execution":{"iopub.status.busy":"2021-06-04T04:04:52.948535Z","iopub.execute_input":"2021-06-04T04:04:52.949193Z","iopub.status.idle":"2021-06-04T04:04:54.683982Z","shell.execute_reply.started":"2021-06-04T04:04:52.949146Z","shell.execute_reply":"2021-06-04T04:04:54.683027Z"},"trusted":true},"execution_count":null,"outputs":[]}]}