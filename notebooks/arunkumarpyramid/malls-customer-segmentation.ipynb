{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"# Importing the dataset\nimport pandas as pd\ndataset = pd.read_csv(\"../input/customer-segmentation-tutorial-in-python/Mall_Customers.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying the head of out dataset\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying the datatype and shape of the dataset\n\nprint(dataset.shape)\ndataset.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying the describe statistical info of each attribute\ndataset.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# a. Histogram Distribution Visualisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram visualisation for age column and know what kind of distribution  it is ?\n\nimport seaborn as sb\nsb.distplot(dataset['Age'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age Attribute having similar kind of normal distribution with wider Standard Deviation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram visualisation for Annual income column and know what kind of distribution  it is ?\n\nsb.distplot(dataset['Annual Income (k$)'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above Histogram plot Annual income column data looks like normal distribution with wider Standard deviation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram visualiation for Spending Score column and to know what kind of distribution it is?\n\nsb.distplot(dataset['Spending Score (1-100)'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# b. HeatMap Correlation Visualisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualisation correlation coefficient of each attribute.\n\ncorr_value=dataset.corr()\nsb.heatmap(corr_value,square=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can't display the correlation coefficient values in heatmap, Because each attribute finds the coefficient value with output attribute.\n\nHere we dont have output attribute because it is an un-supervised learning. Here we are segmenting the culster of categories based on annual income and spending score."},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n\n# a. Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displying any empty or null values in our dataset\ndataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displying the empty or null value in our dataset to understand better how many missing cells there in each attribute\n\ndataset.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Perfect we don't have any missing values in our dataset so no need to remove any columns and rows..\n\nCustomerID is not required to make segementation cluster"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping CustomerID column \n\ndataset=dataset.drop(['CustomerID'],axis=1)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# b. Label Encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding the Gender column from categorical value into numerical value\n\ndataset['Gender'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['Gender']=dataset['Gender'].map({'Male':0,'Female':1})\ndataset['Gender'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# c. Outliers\n\nIn unsupervised algorithm we wont have ouput attribute so we cant predict the outliters here."},{"metadata":{},"cell_type":"markdown","source":"# d. OneHotEncoder\n\nAs we done encoding the label of gender column, We dont need to apply onehotencoder because labeled values in between 0 and 1 only. So no need to apply one Hot Encoder."},{"metadata":{},"cell_type":"markdown","source":"# e. Feature Split\n\nSplit the dataframe feature into input attribute of array of matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Split\nx=dataset.values\n\nprint(x[:5,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# f. Feature Scale\n\nApplying the rescale technique to keep all input attribute value in the range of 0 to 1 by using MinMaxScaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Scale\n\nfrom sklearn.preprocessing import MinMaxScaler\nminmaxscaler=MinMaxScaler()\nx=minmaxscaler.fit_transform(x)\nprint(x[:5,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Modeling"},{"metadata":{},"cell_type":"markdown","source":"To find optimal number of segmentation (Clusters) we are going to use Elbow Method.\n\nElbow Method is used get optimal no.of cluster value with elbow visualisation graph."},{"metadata":{},"cell_type":"markdown","source":"# a. K-Means\n\n# K-Means Elbow Method"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Elbow Method\n\nseed=5\n\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\nwcss=[]\n# n_init ----- Number of kmeans will run with different init centroids\n# max_iter------ Max Number of iterations to define that the final clusters\n# init='k-means++' ---- random initlization to handle random intialization trap\nfor i in range(1,11):\n    kmeans=KMeans(n_clusters=i,init='k-means++',max_iter=500,n_init=20,random_state=seed)\n    kmeans.fit(x)\n    wcss.append(kmeans.inertia_)\n    \nplt.plot(range(1,11),wcss)\nplt.title(\"Elbow method\")\nplt.xlabel(\"No.of Clusters\")\nplt.ylabel('WCSS')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As per the above optimal Elbow method graph 4 cluster segemnetation will be great..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# K-Means Cluster Algorithm\nkmeans=KMeans(n_clusters=4,init='k-means++',random_state=seed,max_iter=500,n_init=20)\ny_kmeans=kmeans.fit_predict(x)\n\n# Predicting the Customers with different segments\nprint(y_kmeans)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualising Result And Its Clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x[y_kmeans==0,0],x[y_kmeans==0,1],s=100,color='red',label='Cluster 1')\nplt.scatter(x[y_kmeans==1,0],x[y_kmeans==1,1],s=100,color='blue',label='Cluster 2')\nplt.scatter(x[y_kmeans==2,0],x[y_kmeans==2,1],s=100,color='green',label='Cluster 3')\nplt.scatter(x[y_kmeans==3,0],x[y_kmeans==3,1],s=100,color='cyan',label='cluster 4')\nplt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=200,color='yellow',label='Centroid')\nplt.title(\"Cluster Clients\")\nplt.xlabel('Annual income')\nplt.ylabel('spending score')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# b. Hierarchical Cluster\n\n# Hierarchical Dendo Gram"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dendo Gram plot is used to find optimal number of cluster..\n\nimport scipy.cluster.hierarchy as sch\ndendogram=sch.dendrogram(sch.linkage(x,method='ward'))\nplt.title('Dendogram')\nplt.xlabel('customers')\nplt.ylabel('Eulidean distance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As per Eulidean distance 3 giving the 4 optimal no.of clusters and because those 4 lines not interceting any lines."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hierarchical Clustering Algorithm to the mall dataset\nfrom sklearn.cluster import AgglomerativeClustering\nhc=AgglomerativeClustering(n_clusters=4)\nhc.fit(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Predict the cluster categories based on mall dataset\ny_hc=hc.fit_predict(x)\nprint(y_hc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x[y_kmeans==0,0],x[y_kmeans==0,1],s=100,color='red',label='Cluster 1')\nplt.scatter(x[y_kmeans==1,0],x[y_kmeans==1,1],s=100,color='blue',label='Cluster 2')\nplt.scatter(x[y_kmeans==2,0],x[y_kmeans==2,1],s=100,color='green',label='Cluster 3')\nplt.scatter(x[y_kmeans==3,0],x[y_kmeans==3,1],s=100,color='cyan',label='cluster 4')\nplt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=200,color='yellow',label='Centroid')\nplt.title(\"Cluster Clients\")\nplt.xlabel('Annual income')\nplt.ylabel('spending score')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both K-Means and Hierarchical Cluster will be great algorithms for unsupervised cluster kind of problems but K-Means will give great performance......\n\nIf any questions please let me know..."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}