{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting heart disease using machine learning\n\nThis notebook looks into using various Python-based machine learning and data science libraries in an attempt to build a machine learning model capable of predicting whether or not someone has heart disease based on their medical attributes.\n\n![heartimage](https://2rdnmg1qbg403gumla1v9i2h-wpengine.netdna-ssl.com/wp-content/uploads/sites/3/2019/10/cardiacDocs-1125401691-770x553-650x428.jpg)"},{"metadata":{},"cell_type":"markdown","source":"1. [Problem Definition](#problem_definition)\n\n2. [Importing Libraries](#import)\n\n3. [Data Dictionary](#data_dictionary)\n\n4. [Data Exploration](#data_exploration)\n\n    4.1 [Heart Disease frequency according to Sex](#exploration_by_sex)\n\n    4.2 [Heart Disease frequency for chest pain](#exploration_by_pain)\n\n    4.3 [Correlation matrix](#correlation)\n    \n5. [Model Building](#model_building)\n\n6. [HyperParameter Tuning](#hyperparameter_tuning)\n    \n    6.1 [Manual Hyperparameter Tuning](#manual_tuning)\n    \n    6.2 [RandomizedSearchCV](#random_tuning)\n    \n    6.3 [GridSearchCV](#grid_tuning)\n\n7. [Evaluation beyond accuracy](#evaluation)\n\n    7.1. [Different Evaluation metrics using Cross-Validation](#cross_validation)\n\n9. [Feature Importance](#feature_importance)"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"problem_definition\"></a>\n# Problem Definition\n\nGiven clinical parameters about a patient, can we predict whether or not they have heart disease?"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"import\"></a>\n# Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing all the necessary frame works\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import confusion_matrix , classification_report , precision_score,recall_score,f1_score,plot_roc_curve\n\nfrom sklearn.model_selection import train_test_split , cross_val_score , RandomizedSearchCV , GridSearchCV\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/heart-disease-uci/heart.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"data_dictionary\"></a>\n# Data Dictionary\n\n1. `age` - age in years\n2. `sex` - (1 = male; 0 = female)\n3. `cp` - chest pain \n        type 0: Typical angina: chest pain related decrease blood supply to the heart \n        type 1: Atypical angina: chest pain not related to heart \n        type 2: Non-anginal pain: typically esophageal spasms (non heart related) \n        type 3: Asymptomatic: chest pain not showing signs of disease\n4. `trestbps` - resting blood pressure (in mm Hg on admission to the hospital) anything above 130-140 is typically cause for concern\n5. `chol` - serum cholestoral in mg/dl\n            * serum = LDL + HDL + .2 * triglycerides\n            * above 200 is cause for concern\n6. `fbs` - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) \n            * '>126' mg/dL signals diabetes\n7. `restecg` - resting electrocardiographic results \n        * 0: Nothing to note \n        * 1: ST-T Wave abnormality\n            - can range from mild symptoms to severe problems\n            - signals non-normal heart beat\n        * 2: Possible or definite left ventricular hypertrophy\n            - Enlarged heart's main pumping chamber\n8. `thalach` - maximum heart rate achieved\n9. `exang` - exercise induced angina (1 = yes; 0 = no)\n10. `oldpeak` - ST depression induced by exercise relative to rest looks at stress of heart during excercise unhealthy heart will stress more\n11. `slope` - the slope of the peak exercise ST segment \n        * 0: Upsloping: better heart rate with excercise (uncommon) \n        * 1: Flatsloping: minimal change (typical healthy heart) \n        * 2: Downslopins: signs of unhealthy heart\n12. `ca` - number of major vessels (0-3) colored by flourosopy \n        * colored vessel means the doctor can see the blood passing through\n        * the more blood movement the better (no clots)\n13. `thal` - thalium stress result\n        * 1,3: normal\n        * 6: fixed defect: used to be defect but ok now\n        * 7: reversable defect: no proper blood movement when excercising\n14. `target` - have disease or not (1=yes, 0=no) (the predicted attribute)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"data_exploration\"></a>\n## Data Exploration\n\n1. What questions are you trying to solve \n2. What kind of data do we have and how we handle it\n3. What is missing and how you are going to handle it\n4. Where are the outliers and why should we care about them\n5. How can you add, change or remove features to get more out of your data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['target'].value_counts().plot(kind='bar');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"exploration_by_sex\"></a>\n### Heart disease frequency according to sex"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sex.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" pd.crosstab(df.target,df.sex)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" In this,\n\n    total female = 96\n    affected female = 72 , one third of the female are affected\n    \n    total male = 207\n    affected male = 93 half of the male are affected "},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df.target,df.sex).plot(kind = \"bar\",color = ['salmon','lightblue'],figsize=(10,6));\n    \nplt.title(\"Heart diesase with gender\")\nplt.legend(['female','male'])\nplt.xlabel(\"0 = No heart disease  1 = heart disease\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\n\nplt.scatter(df.age[df.target==1],\n            df.thalach[df.target==1],\n            c = 'darkred')\n\nplt.scatter(df.age[df.target==0],\n            df.thalach[df.target==0],\n            c = 'salmon')\n\nplt.title(\"Heart disease in function of age and max heart rate\")\nplt.xlabel('Age')\nplt.ylabel('Max heart rate (thalach)')\nplt.legend(['Disease','No disease'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## check the distribution of the age column with hist\nplt.hist(df.age)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"exploration_by_pain\"></a>\n### Heart disease frequency for chest pain"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df.cp,df.target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df.cp,df.target).plot(kind='bar',color=['salmon','darkred'])\n\n\nplt.title(\"Heart disease in function chest pain with target\")\nplt.xlabel('Chest pain')\nplt.ylabel('Amount')\nplt.legend(['NO disease','Disease'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"correlation\"></a>\n### Correlation matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_mat = df.corr()\n\nplt.figure(figsize=(15,10))\nsns.heatmap(corr_mat,annot=True,linewidths=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"model_building\"></a>\n## Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into X and y\n\nX = df.drop('target',axis=1)\ny = df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train test spliting\n\nnp.random.seed(42) #To reproduced the randomized data again\n\nX_train , X_test , y_train , y_test = train_test_split(X,y,test_size = 0.2)\n\nX_train.shape , X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Models to build**\n\n1. RandomForestClassifier\n2. Logistic Regression\n3. KNearest neighbor classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Time to build a machine learning model\n\nmodels = {'logistic regression':LogisticRegression(),\n          'Random forest classifier':RandomForestClassifier(),\n          'KNearest neighbor':KNeighborsClassifier()\n         }\n\nnp.random.seed(42)\n\ndef fit_and_score(models,X_train,X_test,y_train,y_test):\n    \"\"\"\n    Fits and evaluatest different machine learning models.\n    models : a dict of different sciit learn machine learning models.\n    X_train : training data (no labels)\n    X_test : Testing data (no labels)\n    y_train : training labels\n    y_test : testing labels    \n    \"\"\"\n    scores = {}\n    \n    for name , model in models.items():\n        model.fit(X_train,y_train)\n        scores[name] = model.score(X_test,y_test)\n        \n    return scores\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_scores = fit_and_score(models=models,X_train=X_train,X_test=X_test,\n                             y_train=y_train,y_test=y_test)\n\nmodel_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_scores_df = pd.DataFrame(model_scores,index=['accuracy'])\nmodel_scores_df.T.plot.bar();\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we've got a baseline model... and we know a model's first predictions aren't always what we should based our next steps off. What should we do?\n\nLet's look at the following:\n\n    - Hypyterparameter tuning\n    - Feature importance\n    - Confusion matrix\n    - Cross-validation\n    - Precision\n    - Recall\n    - F1 score\n    - Classification report\n    - ROC curve\n    - Area under the curve (AUC)"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"hyperparameter_tuning\"></a>\n## Hyper Parameter Tuning\n"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"manual_tuning\"></a>\n### Manual HyperParameter Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tuning knn\n\ntrain_scores = []\ntest_scores = []\n\nneighbors = range(1,21)\n\nknn = KNeighborsClassifier()\n\nfor i in neighbors:\n    knn.set_params(n_neighbors = i)\n    \n    knn.fit(X_train,y_train)\n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(neighbors,train_scores,label = 'train scores')\nplt.plot(neighbors,test_scores,label = 'train scores')\nplt.legend()\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.xticks(np.arange(1,21,1));\nprint(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"random_tuning\"></a>\n### Hyperparameter tuning with Randomized search CV\n\nWe're going to tune , \n\n - LogisticRegression()\n - RandomForestClassifier()"},{"metadata":{"trusted":true},"cell_type":"code","source":"#logistic regression grid\nlog_reg_grid = {'C': np.logspace(-4,4,20),\n                'solver': ['liblinear']}\n\n#random forest grid\nrf_grid = {'n_estimators':np.arange(10,1000,50),\n           'max_depth': [None,3,5,10],\n           'min_samples_split':np.arange(2,20,2),\n           'min_samples_leaf':np.arange(1,20,2)}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(42)\n\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True)\n\nrs_log_reg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rs_log_reg.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rs_log_reg.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(), \n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\n# Fit random hyperparameter search model for RandomForestClassifier()\nrs_rf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rs_rf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rs_rf.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"grid_tuning\"></a>\n### Hyperparameter tuning with GridsearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Different hyperprameters for our logistic regression model\n\nlog_reg_grid = {'C': np.logspace(-4, 4, 30),\n                \"solver\": [\"liblinear\"]}\n\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                         param_grid=log_reg_grid,\n                         cv=5,\n                         verbose=True)\n\ngs_log_reg.fit(X_train,y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_log_reg.best_params_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_log_reg.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"evaluation\"></a>\n### Evaluting our tuned machine learning classifier, beyond accuracy\n\n- ROC curve and AUC score\n- Confusion matrix\n- Classification report\n- Precision\n- Recall\n- F1-score"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds = gs_log_reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(gs_log_reg, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1.5)\n\nsns.heatmap(confusion_matrix(y_test,y_preds),\n            annot=True,\n            cbar=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we've got a ROC curve, an AUC metric and a confusion matrix, let's get a classification report as well as cross-validated precision, recall and f1-score."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"cross_validation\"></a>\n### Different evaluation metrics using cross-validation\n\nWe're going to calculate accuracy, precision, recall and f1-score of our model using cross-validation and to do so we'll be using cross_val_score()."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check best hyperparameters\ngs_log_reg.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new classifier with best parameters\nclf = LogisticRegression(C=0.20433597178569418,\n                         solver=\"liblinear\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross-validated accuracy\ncv_acc = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5,\n                         scoring=\"accuracy\")\ncv_acc = cv_acc.mean()\nprint(f\"Cross Validated accuracy {cv_acc}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross-validated precision\ncv_precision = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5,\n                         scoring=\"precision\")\ncv_precision=np.mean(cv_precision)\nprint(f\"Cross Validated Precision {cv_precision}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross-validated recall\ncv_recall = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5,\n                         scoring=\"recall\")\ncv_recall=np.mean(cv_recall)\nprint(f\"Cross Validated Recall {cv_recall}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross-validated f1_score\ncv_f1 = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5,\n                         scoring=\"f1\")\ncv_f1=np.mean(cv_f1)\nprint(f\"Cross Validated F1_score {cv_f1}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing cross-validated metrics\ncv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n                           \"Precision\": cv_precision,\n                           \"Recall\": cv_recall,\n                           \"F1\": cv_f1},\n                          index=[0])\n\ncv_metrics.T.plot.bar(title=\"Cross-validated classification metrics\",\n                      legend=False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"feature_importance\"></a>\n## Feature Importance\n\nFeature importance is another as asking, \"which features contributed most to the outcomes of the model and how did they contribute?\"\n\nFinding feature importance is different for each machine learning model. One way to find feature importance is to search for \"(MODEL NAME) feature importance\".\n\nLet's find the feature importance for our LogisticRegression model..."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Fit an instance of LogisticRegression\nclf = LogisticRegression(C=0.20433597178569418,\n                         solver=\"liblinear\")\n\nclf.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For logistic regression coef_ is used to find the feature importances of the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check coef_\nclf.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Match coef's of features to columns\nfeature_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeature_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize feature importance\nfeature_df = pd.DataFrame(feature_dict, index=[0])\nfeature_df.T.plot.bar(title=\"Feature Importance\", legend=False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sex is highly negative correlated with the target variable lets look at it.."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df[\"sex\"], df[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sex class is highly imbalanced with lower female and high male values and also we can see that ratio of the female is 3 : 1 and for the male it is more or less 1:2 hence as the sex is increasing the target value is decreasing"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df[\"slope\"], df[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"slope - the slope of the peak exercise ST segment\n\n- 0: Upsloping: better heart rate with excercise (uncommon)\n- 1: Flatsloping: minimal change (typical healthy heart)\n- 2: Downslopins: signs of unhealthy heart"},{"metadata":{},"cell_type":"markdown","source":"# Please do Upvote if you find it useful"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}