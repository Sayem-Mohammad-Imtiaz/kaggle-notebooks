{"cells":[{"metadata":{},"cell_type":"markdown","source":"**The study of classification of types of glass was motivated by criminological investigation. At the scene of the crime, the glass left can be used as evidence...if it is correctly identified.**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%matplotlib inline \nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # read and wrangle dataframes\nimport matplotlib.pyplot as plt # visualization\nimport seaborn as sns # statistical visualizations and aesthetics\nfrom sklearn.preprocessing import StandardScaler # preprocessing\nfrom sklearn.model_selection import (train_test_split, KFold , StratifiedKFold,\ncross_val_score, GridSearchCV,\nlearning_curve, validation_curve) # model selection modules\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=pd.read_csv('/kaggle/input/glass/glass.csv')\ndf1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Attribute Information:***\n1. Id number: 1 to 214\n2. RI: refractive index\n3. Na: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)\n4. Mg: Magnesium\n5. Al: Aluminum\n6. Si: Silicon\n7. K: Potassium\n8. Ca: Calcium\n9. Ba: Barium\n10. Fe: Iron"},{"metadata":{},"cell_type":"markdown","source":"Type of glass: (class attribute)  \n1. building_windows_float_processed \n2. building_windows_non_float_processed \n3. vehicle_windows_float_processed \n4. vehicle_windows_non_float_processed (none in this database) \n5. containers \n6. tableware  \n7. headlamps"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#no null values\ndf1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attribute=df1.columns.tolist()\nattribute","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for atr in attribute:\n    #a=df[atr].skew()\n    plt.hist(df1[atr])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(attribute)-1):\n    figure = plt.figure()\n    ax = sns.boxplot(x='Type', y=attribute[i], data=df1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,9))\nsns.pairplot(df1[attribute],palette='coolwarm')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l=['Type']\n\nx=df1.drop(l,axis=1)\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TARGET\ny=df1['Type']\ny.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(x, y, random_state =42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb=GaussianNB().fit(X_train, y_train)\nprint(gb.score(X_train, y_train))\nprint(gb.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"using GaussianNB model trainnig sore is 0.58125(58%)  and test score is 0.46(46%)"},{"metadata":{"trusted":true},"cell_type":"code","source":"lg=LogisticRegression().fit(X_train, y_train)\nprint(lg.score(X_train, y_train))\nprint(lg.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"using LogisticRegression model trainnig sore is 0.6625(66.25%)  and test score is 0.5740740740740741(57.4%)"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = SVC().fit(X_train, y_train)\nprint(clf.score(X_train, y_train))\nprint(clf.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"using svc model trainnig sore is .70625(70%) and test score is 0.6481481481481481(64.8%)"},{"metadata":{"trusted":true},"cell_type":"code","source":"dt= DecisionTreeClassifier().fit(X_train, y_train)#overfitting\nprint(dt.score(X_train, y_train))\nprint(dt.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"using DecisionTreeClassifier model trainnig sore is 1(100%)  and test score is 0.6296(62.96%) so there is overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"knn=KNeighborsClassifier().fit(X_train, y_train)\nprint(knn.score(X_train, y_train))\nprint(knn.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"using KNeighborsClassifier model trainnig sore is .76875(76.8%) and test score is 0.6481(64.8%)"},{"metadata":{"trusted":true},"cell_type":"code","source":"r= RandomForestClassifier().fit(X_train, y_train)#overfitting\nprint(r.score(X_train, y_train))\nprint(r.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"using RandomForestClassifier model trainnig sore is 0.98125(98.1%)   and test score is 0.79(79.6%)"},{"metadata":{},"cell_type":"markdown","source":"**After Scalling**"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX_train_s = scaler.fit_transform(X_train)\nX_test_s= scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lg1=LogisticRegression().fit(X_train_s, y_train)\nprint(lg1.score(X_train_s, y_train))\nprint(lg1.score(X_test_s, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"using LogisticRegression model(without scaling) trainnig sore is 0.6625(66.25%)  and test score is 0.574(57.4%) and scaling traing score(68.75%) is  improved but test score is not improved"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf1 = SVC().fit(X_train_s, y_train)\nprint(clf1.score(X_train_s, y_train))\nprint(clf1.score(X_test_s, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"using svc model(without scaling) trainnig sore is .70625(70%) and test score is 0.6481481481481481(64.8%) but after scaling, training score is 0.80(80%) and test score is 0.70(70%).so score is improved by using scaling."},{"metadata":{"trusted":true},"cell_type":"code","source":"knn1=KNeighborsClassifier().fit(X_train_s, y_train)\nprint(knn1.score(X_train_s, y_train))\nprint(knn1.score(X_test_s, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"using KNeighborsClassifier model(with scaling) trainnig sore is .73125(73.1%) and test score is 0.629(62.9%)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nall_accuracies = cross_val_score(estimator=clf1, X=x, y=y, cv=5)\nnp.mean(all_accuracies)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"svc with cross_val_score\nuseing train_test_split(divide dataset into two parts and  according to svc(using train_test_split) model  80% is traing data and 20% test data) test score is 70% but using cross_val_score() is 59.2% as here accuracy  is decreased.Because after using cross_val_score there is less overfitting.so accuracy after using cross_val_score  is appropriate."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nall_accuracies = cross_val_score(estimator=r, X=X_train, y=y_train, cv=5)\nnp.mean(all_accuracies)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random forest with cross_val_score\nusing train_test_split test score is 79% but here accuracy is 69.9% But this is correst as it reduces the overfitting problem."},{"metadata":{},"cell_type":"markdown","source":"**TUNNING random forest**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n # Random search of parameters, using 3 fold cross validation, \n rf_random = RandomizedSearchCV(estimator = r, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n # Fit the random search model\n rf_random.fit(X_train, y_train)\n rf_random .best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"using cross validation accuracy for random forest is 69% but accuracy is improved after tunning that is 74.3%"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}