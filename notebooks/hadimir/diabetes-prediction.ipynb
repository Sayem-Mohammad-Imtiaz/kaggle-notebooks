{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names = ['Pregnancies', 'Glucose', 'BloodPressure','SkinThickness','Insulin','BMI', 'DiabetesPedigreeFunction','Age', 'Outcome']\ndata = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = ['Pregnancies', 'Insulin', 'BMI', 'Age']\nX = data[feature_cols]\ny = data['Outcome']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting data into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using logestic regression\nfrom sklearn.linear_model import LogisticRegression\nlogReg = LogisticRegression()\nlogReg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_class  = logReg.predict(X_test)\ny_pred_class","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification accuracy: percentage of correct predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate accuracy\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred_class))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Null Accuracy: accuracy that could be achieved by always predicting the most frequent class"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate the % of ones (using mean coz there is only 0 and 1 present)\ny_test.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate the % of 0\n1 -  y_test.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate null accuracy (for binary classification problem coded as 1 and 0)\nmax(y_test.mean(), 1 -  y_test.mean() )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate null accuracy (for multi class classification)\ny_test.value_counts().head(1)/ len(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CONFUSION MATRIX\nloosly defined as the table that descricibes the performance of a classification model"},{"metadata":{"trusted":true},"cell_type":"code","source":"print( metrics.confusion_matrix(y_test, y_pred_class))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion = metrics.confusion_matrix(y_test, y_pred_class)\nTP = confusion[1,1]\nTN = confusion[0,0]\nFP = confusion[0,1]\nFN = confusion[1,0]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Metrics computed from confusion matrix"},{"metadata":{},"cell_type":"markdown","source":"### classification accuracy: overall how often is the classifier correct?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#both are equal\nprint( (TP + TN) / float(TP + TN + FP + FN))\nprint(metrics.accuracy_score(y_test, y_pred_class))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Classification error: overall how often is the classifier incorrect"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Both are equal \nprint(( FP+ FN)/ float(TP + TN + FP + FN))\nprint(1 - metrics.accuracy_score(y_test, y_pred_class))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### sentivity : when the actual value is positive, how often is the prediction correct?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Both are same\nprint(TP/ float( TP +FN))\nprint( metrics.recall_score(y_test, y_pred_class))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Specificity : when the actual value is Negative, how often is the prediction correct?"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(TN/ float(TN+FP))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### False positive rate: when the actual value is Negative, how often is the prediction INcorrect?"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(FP / float(TN+FP))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Precision: when the positive value is predicted, how often is the prediction correct "},{"metadata":{"trusted":true},"cell_type":"code","source":"print( TP / float(TP+FP))\nprint(metrics.precision_score(y_test, y_pred_class))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adjusting the classification threshold\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the first 10 predicted responses\nlogReg.predict(X_test)[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the first 10 predicted probabilities of class membership\nlogReg.predict_proba(X_test)[0:10, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the first 10 predicted probabilities for class 1\nlogReg.predict_proba(X_test)[0:10, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# store the predicted probabilities for class 1\ny_pred_prob = logReg.predict_proba(X_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# allow plots to appear in the notebook\n%matplotlib inline\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histogram of predicted probabilities\nplt.hist(y_pred_prob, bins=8)\nplt.xlim(0, 1)\nplt.title('Histogram of predicted probabilities')\nplt.xlabel('Predicted probability of diabetes')\nplt.ylabel('Frequency')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict diabetes if the predicted probability is greater than 0.3\nfrom sklearn.preprocessing import binarize\ny_pred_class = binarize([y_pred_prob], 0.3)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the first 10 predicted probabilities\ny_pred_prob[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the first 10 predicted classes with the lower threshold\ny_pred_class[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# previous confusion matrix (default threshold of 0.5)\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# new confusion matrix (threshold of 0.3)\nprint(metrics.confusion_matrix(y_test, y_pred_class))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sensitivity has increased (used to be 0.24)\nprint(46 / float(46 + 16))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# specificity has decreased (used to be 0.91)\nprint(80 / float(80 + 50))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:**\n\n- **Threshold of 0.5** is used by default (for binary problems) to convert predicted probabilities into class predictions\n- Threshold can be **adjusted** to increase sensitivity or specificity\n- Sensitivity and specificity have an **inverse relationship**"},{"metadata":{},"cell_type":"markdown","source":"## ROC Curves and Area Under the Curve (AUC)\n\n**Question:** Wouldn't it be nice if we could see how sensitivity and specificity are affected by various thresholds, without actually changing the threshold?\n\n**Answer:** Plot the ROC curve!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# IMPORTANT: first argument is true values, second argument is predicted probabilities\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for diabetes classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- ROC curve can help you to **choose a threshold** that balances sensitivity and specificity in a way that makes sense for your particular context\n- You can't actually **see the thresholds** used to generate the curve on the ROC curve itself"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a function that accepts a threshold and prints sensitivity and specificity\ndef evaluate_threshold(threshold):\n    print('Sensitivity:', tpr[thresholds > threshold][-1])\n    print('Specificity:', 1 - fpr[thresholds > threshold][-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_threshold(0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_threshold(0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AUC is the **percentage** of the ROC plot that is **underneath the curve**:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# IMPORTANT: first argument is true values, second argument is predicted probabilities\nprint(metrics.roc_auc_score(y_test, y_pred_prob))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- AUC is useful as a **single number summary** of classifier performance.\n- If you randomly chose one positive and one negative observation, AUC represents the likelihood that your classifier will assign a **higher predicted probability** to the positive observation.\n- AUC is useful even when there is **high class imbalance** (unlike classification accuracy)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate cross-validated AUC\nfrom sklearn.model_selection import cross_val_score\ncross_val_score(logReg, X, y, cv=10, scoring='roc_auc').mean()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}