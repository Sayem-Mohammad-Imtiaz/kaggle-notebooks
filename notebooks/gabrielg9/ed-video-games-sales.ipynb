{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-18T18:21:46.15266Z","iopub.execute_input":"2021-06-18T18:21:46.152967Z","iopub.status.idle":"2021-06-18T18:21:46.159381Z","shell.execute_reply.started":"2021-06-18T18:21:46.152939Z","shell.execute_reply":"2021-06-18T18:21:46.158576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:21:46.165231Z","iopub.execute_input":"2021-06-18T18:21:46.165505Z","iopub.status.idle":"2021-06-18T18:21:46.174331Z","shell.execute_reply.started":"2021-06-18T18:21:46.165479Z","shell.execute_reply":"2021-06-18T18:21:46.173651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    #filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:21:46.178506Z","iopub.execute_input":"2021-06-18T18:21:46.178877Z","iopub.status.idle":"2021-06-18T18:21:46.186254Z","shell.execute_reply.started":"2021-06-18T18:21:46.178851Z","shell.execute_reply":"2021-06-18T18:21:46.185567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:21:46.193529Z","iopub.execute_input":"2021-06-18T18:21:46.193842Z","iopub.status.idle":"2021-06-18T18:21:46.20067Z","shell.execute_reply.started":"2021-06-18T18:21:46.193762Z","shell.execute_reply":"2021-06-18T18:21:46.200042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import chi2_contingency, normaltest","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:21:46.212022Z","iopub.execute_input":"2021-06-18T18:21:46.212393Z","iopub.status.idle":"2021-06-18T18:21:46.215641Z","shell.execute_reply.started":"2021-06-18T18:21:46.212367Z","shell.execute_reply":"2021-06-18T18:21:46.215058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Wczytanie bazy danych do lokalnej zmiennej\ndata = pd.read_csv(\"../input/videogamesales/vgsales.csv\")\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:21:46.219466Z","iopub.execute_input":"2021-06-18T18:21:46.21984Z","iopub.status.idle":"2021-06-18T18:21:46.267438Z","shell.execute_reply.started":"2021-06-18T18:21:46.219816Z","shell.execute_reply":"2021-06-18T18:21:46.266656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:21:46.26857Z","iopub.execute_input":"2021-06-18T18:21:46.268792Z","iopub.status.idle":"2021-06-18T18:21:46.283113Z","shell.execute_reply.started":"2021-06-18T18:21:46.26877Z","shell.execute_reply":"2021-06-18T18:21:46.282411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:21:46.284455Z","iopub.execute_input":"2021-06-18T18:21:46.28469Z","iopub.status.idle":"2021-06-18T18:21:46.297857Z","shell.execute_reply.started":"2021-06-18T18:21:46.284668Z","shell.execute_reply":"2021-06-18T18:21:46.297092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Statystyki dotyczące każdej z dostępnych zmiennych ilościowych","metadata":{}},{"cell_type":"code","source":"quant_col = ['NA_Sales', 'EU_Sales', 'JP_Sales','Other_Sales', 'Global_Sales']\nquant_stats = data[quant_col].agg([\"count\",\"mean\",\"median\",\"min\", \"max\", \"std\", \"var\",])\nquant_stats = quant_stats.append(data[quant_col].mode().rename(index={0:\"mode\"}))\nquant_stats","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:21:46.299206Z","iopub.execute_input":"2021-06-18T18:21:46.299456Z","iopub.status.idle":"2021-06-18T18:21:46.326926Z","shell.execute_reply.started":"2021-06-18T18:21:46.299433Z","shell.execute_reply":"2021-06-18T18:21:46.326229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tabele liczności dla zmiennych jakościowych ","metadata":{}},{"cell_type":"code","source":"quali_cols = ['Name', 'Platform','Year', 'Genre']\nfig, axes = plt.subplots(len(quali_cols), 1, figsize=(10,30))\nfor i, col in enumerate(quali_cols):\n    axes[i].set_title(f\"Wykres rozkładu liczności dla {col}\")\n    sns.histplot(data[col], ax=axes[i])\n    print(f\"\"\"Dla kolumny {col} tabela liczności wygląda następująco:\\n\\n\\n{pd.DataFrame(data[col].value_counts())}\\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:21:46.327894Z","iopub.execute_input":"2021-06-18T18:21:46.328116Z","iopub.status.idle":"2021-06-18T18:24:56.540997Z","shell.execute_reply.started":"2021-06-18T18:21:46.328094Z","shell.execute_reply":"2021-06-18T18:24:56.54001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Z powyższych histogramów można wywnioskować, że najwięcej gier sprzedano w latach 2008 oraz 2009 gdzie najpopularniejszą platformą docelową gry było DS oraz PS2. Dominują gry akcji.","metadata":{}},{"cell_type":"markdown","source":"Histogram analizy kolumn","metadata":{}},{"cell_type":"code","source":"plotPerColumnDistribution(data, 10, 5)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:24:56.542304Z","iopub.execute_input":"2021-06-18T18:24:56.542657Z","iopub.status.idle":"2021-06-18T18:24:57.308393Z","shell.execute_reply.started":"2021-06-18T18:24:56.542619Z","shell.execute_reply":"2021-06-18T18:24:57.307729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Macierz korelacji","metadata":{}},{"cell_type":"code","source":"plotCorrelationMatrix(data, 8)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:24:57.309412Z","iopub.execute_input":"2021-06-18T18:24:57.309896Z","iopub.status.idle":"2021-06-18T18:24:57.524377Z","shell.execute_reply.started":"2021-06-18T18:24:57.309865Z","shell.execute_reply":"2021-06-18T18:24:57.523749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wykres rozproszenia i gęstości","metadata":{}},{"cell_type":"code","source":"plotScatterMatrix(data, 18, 10)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:24:57.526132Z","iopub.execute_input":"2021-06-18T18:24:57.526568Z","iopub.status.idle":"2021-06-18T18:25:03.410857Z","shell.execute_reply.started":"2021-06-18T18:24:57.526541Z","shell.execute_reply":"2021-06-18T18:25:03.41003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Uzupełnienie kolumny \"Year\" wartościami tak, aby pozbyć się nulli. Nie zdecydowałem się ich usunąć, lecz uzupełnić średnią wartością. Dzięki temu wiemy, że najwięcej gier spprzedano w 2008 oraz 2009 roku. ","metadata":{}},{"cell_type":"code","source":"mean = data['Year'].mean()\ndata['Year'].fillna(mean, inplace=True)\nplt.xticks(rotation = 75)\nx_axis = data['Year'].astype(int)\nsns.countplot(x= x_axis, data = data)\nplt.title('Total Game Sales Each Year')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:25:03.412596Z","iopub.execute_input":"2021-06-18T18:25:03.412854Z","iopub.status.idle":"2021-06-18T18:25:03.740311Z","shell.execute_reply.started":"2021-06-18T18:25:03.412829Z","shell.execute_reply":"2021-06-18T18:25:03.739661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.xticks(rotation = 75)\nx_axis = data['Genre']\nsns.countplot(x= x_axis, data = data)\nplt.title('Total Game Sales of Each Genre')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:25:03.741333Z","iopub.execute_input":"2021-06-18T18:25:03.741749Z","iopub.status.idle":"2021-06-18T18:25:03.925351Z","shell.execute_reply.started":"2021-06-18T18:25:03.741723Z","shell.execute_reply":"2021-06-18T18:25:03.9245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A najczęściej sprzedającym się typem gier były gry akcji. ","metadata":{}},{"cell_type":"markdown","source":"W każdym typie gier, najlepiej sprzedaje się ","metadata":{}},{"cell_type":"code","source":"game_sales = data[['Genre','Name']].groupby(['Genre']).agg(lambda x:x.value_counts().index[0])\ngenre_games = game_sales.rename(columns = {'Name' : 'Game'}, inplace = False)\ngenre_games","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:25:03.926423Z","iopub.execute_input":"2021-06-18T18:25:03.926664Z","iopub.status.idle":"2021-06-18T18:25:03.954741Z","shell.execute_reply.started":"2021-06-18T18:25:03.926641Z","shell.execute_reply":"2021-06-18T18:25:03.953877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Gry najlepiej sprzedawały się na platformę PS2 oraz DS","metadata":{}},{"cell_type":"code","source":"plt.xticks(rotation = 75)\nx_axis = data['Platform']\nsns.countplot(x= x_axis, data = data)\nplt.title('Total Game Sales on Each Platform')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:25:03.955765Z","iopub.execute_input":"2021-06-18T18:25:03.955998Z","iopub.status.idle":"2021-06-18T18:25:04.225605Z","shell.execute_reply.started":"2021-06-18T18:25:03.955975Z","shell.execute_reply":"2021-06-18T18:25:04.224619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Macierz korelacji, uwzględniająca także rok powstania gry","metadata":{}},{"cell_type":"code","source":"correlation_vg_sales = data.corr()\n\naxis_corr = sns.heatmap(\ncorrelation_vg_sales,\nvmin=-1, vmax=1, center=0,\ncmap=sns.diverging_palette(50, 500, n=500),\nsquare=True\n)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:25:04.226742Z","iopub.execute_input":"2021-06-18T18:25:04.22707Z","iopub.status.idle":"2021-06-18T18:25:04.493547Z","shell.execute_reply.started":"2021-06-18T18:25:04.227032Z","shell.execute_reply":"2021-06-18T18:25:04.492693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dla pustych pól wydawcy gier \"Publisher\" uzupełnione zostały one wartościami \"Unknown\"","metadata":{}},{"cell_type":"code","source":"data['Publisher'] = data['Publisher'].fillna('Unknown')\nnumber_df = data.groupby('Publisher')[['Name']].count().sort_values('Name', ascending = False).head(50)\nnumber_clean = number_df.rename(columns = {'Name' : 'Number'}, inplace = False)\nnumber_clean","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:25:04.494917Z","iopub.execute_input":"2021-06-18T18:25:04.495276Z","iopub.status.idle":"2021-06-18T18:25:04.517654Z","shell.execute_reply.started":"2021-06-18T18:25:04.495225Z","shell.execute_reply":"2021-06-18T18:25:04.516787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Gdzie najpopularniejszym producentem gier był zdecydowanie \"Eloctronic Arts\"","metadata":{}},{"cell_type":"code","source":"number_df.plot(kind = 'bar', figsize = (25, 10));\nplt.xlabel('Publisher', fontsize = 20);\nplt.ylabel('Number of video games released', fontsize = 20);\nplt.title('Top Publishers of Games', fontsize = 40)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:25:04.518646Z","iopub.execute_input":"2021-06-18T18:25:04.518891Z","iopub.status.idle":"2021-06-18T18:25:05.264331Z","shell.execute_reply.started":"2021-06-18T18:25:04.518867Z","shell.execute_reply":"2021-06-18T18:25:05.263706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"W każdym regionie świata podział sprzedaży gier ma się następująco: ","metadata":{}},{"cell_type":"markdown","source":"Północna Ameryka:","metadata":{}},{"cell_type":"code","source":"top_games_NA = data.sort_values('NA_Sales',ascending = False).head(5)\nexplode = [0.1, 0, 0, 0, 0]\nplt.pie(top_games_NA['NA_Sales'], labels = top_games_NA['Name'], explode = explode)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:25:05.265292Z","iopub.execute_input":"2021-06-18T18:25:05.265681Z","iopub.status.idle":"2021-06-18T18:25:05.354023Z","shell.execute_reply.started":"2021-06-18T18:25:05.265654Z","shell.execute_reply":"2021-06-18T18:25:05.353275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Europa","metadata":{}},{"cell_type":"code","source":"top_games_EU = data.sort_values('EU_Sales',ascending = False).head(5)\nexplode = [0.1, 0, 0, 0, 0]\nplt.pie(top_games_EU['EU_Sales'], labels = top_games_EU['Name'], explode = explode)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:25:05.35501Z","iopub.execute_input":"2021-06-18T18:25:05.355271Z","iopub.status.idle":"2021-06-18T18:25:05.435546Z","shell.execute_reply.started":"2021-06-18T18:25:05.355228Z","shell.execute_reply":"2021-06-18T18:25:05.434775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Japonia:","metadata":{}},{"cell_type":"code","source":"top_games_JP = data.sort_values('JP_Sales',ascending = False).head(5)\nexplode = [0.1, 0, 0, 0, 0]\nplt.pie(top_games_JP['JP_Sales'], labels = top_games_JP['Name'], explode = explode)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:25:05.436528Z","iopub.execute_input":"2021-06-18T18:25:05.436768Z","iopub.status.idle":"2021-06-18T18:25:05.524425Z","shell.execute_reply.started":"2021-06-18T18:25:05.436743Z","shell.execute_reply":"2021-06-18T18:25:05.523547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pozostałe:","metadata":{}},{"cell_type":"code","source":"top_games_Other = data.sort_values('Other_Sales',ascending = False).head(5)\nexplode = [0.1, 0, 0, 0, 0]\nplt.pie(top_games_Other['Other_Sales'], labels = top_games_Other['Name'], explode = explode)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:25:05.525676Z","iopub.execute_input":"2021-06-18T18:25:05.526015Z","iopub.status.idle":"2021-06-18T18:25:05.611859Z","shell.execute_reply.started":"2021-06-18T18:25:05.52598Z","shell.execute_reply":"2021-06-18T18:25:05.611003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Globalna sprzedaż","metadata":{}},{"cell_type":"code","source":"top_games_Global = data.sort_values('Global_Sales',ascending = False).head(5)\nexplode = [0.1, 0, 0, 0, 0]\nplt.pie(top_games_Global['Global_Sales'], labels = top_games_Global['Name'], explode = explode)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:25:05.614349Z","iopub.execute_input":"2021-06-18T18:25:05.614589Z","iopub.status.idle":"2021-06-18T18:25:05.696946Z","shell.execute_reply.started":"2021-06-18T18:25:05.614565Z","shell.execute_reply":"2021-06-18T18:25:05.696132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.scatter(data.Genre,data.NA_Sales,color='b',alpha=.5)\nplt.xlabel('Typ gier')           \nplt.ylabel('Ilość')\nplt.title('Rozkład ')           \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:25:05.697967Z","iopub.execute_input":"2021-06-18T18:25:05.698191Z","iopub.status.idle":"2021-06-18T18:25:05.874085Z","shell.execute_reply.started":"2021-06-18T18:25:05.698169Z","shell.execute_reply":"2021-06-18T18:25:05.873416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Procentowy wykres sprzedaży na regiony świata","metadata":{}},{"cell_type":"code","source":"total_sales = pd.DataFrame({'Country':['NA', 'EU', 'JP', 'Other'], 'Sales':[sum(data['NA_Sales']), sum(data['EU_Sales']), sum(data['JP_Sales']), sum(data['Other_Sales'])]})\ntotal_sales['Percentages'] = total_sales['Sales']/sum(total_sales['Sales'])*100\ntotal_sales.sort_values(by='Percentages', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:25:05.87497Z","iopub.execute_input":"2021-06-18T18:25:05.875187Z","iopub.status.idle":"2021-06-18T18:25:05.887114Z","shell.execute_reply.started":"2021-06-18T18:25:05.875165Z","shell.execute_reply":"2021-06-18T18:25:05.886426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(facecolor='whitesmoke')\n\naxes1 = fig.add_axes([0, 0, 1, 1])\naxes2 = fig.add_axes([0.7, 0, 1, 1])\naxes3 = fig.add_axes([1.4, 0, 1, 1])\naxes4 = fig.add_axes([2.1, 0, 1, 1])\n\naxes1.pie([total_sales['Percentages'][0], 100-total_sales['Percentages'][0]], startangle=180,\n         colors=['crimson', 'white'])\naxes1.text(-0.27, -0.85,f\"{round(total_sales['Percentages'][0],2)}%\", fontweight='bold', fontsize=16)\naxes1.text(-0.5, 1.3, 'North America', fontweight='bold', fontsize=16)\n\naxes2.pie([total_sales['Percentages'][1], 100-total_sales['Percentages'][1]], startangle=-4,\n         colors=['crimson', 'white'])\naxes2.text(0.26, 0.355, f\"{round(total_sales['Percentages'][1],2)}%\", fontweight='bold', fontsize=16)\naxes2.text(-0.3, 1.3, 'Europe', fontweight='bold', fontsize=16)\n\naxes3.pie([total_sales['Percentages'][2], 100-total_sales['Percentages'][2]], startangle=95,\n         colors=['crimson', 'white'])\naxes3.text(-0.62, 0.5, f\"{round(total_sales['Percentages'][2],2)}%\", fontweight='bold', fontsize=16)\naxes3.text(-0.25, 1.3, 'Japan', fontweight='bold', fontsize=16)\n\naxes4.pie([total_sales['Percentages'][3], 100-total_sales['Percentages'][3]], startangle=130,\n         colors=['crimson', 'white'])\naxes4.text(-0.77, 0.3, f\"{round(total_sales['Percentages'][3],2)}%\", fontweight='bold', fontsize=16)\naxes4.text(-0.5, 1.3, 'Other countries', fontweight='bold', fontsize=16)\n\naxes2.text(-0.3,1.8, 'Percentage of sales by country', fontweight='bold', color='crimson', fontsize=24)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:25:05.89155Z","iopub.execute_input":"2021-06-18T18:25:05.891775Z","iopub.status.idle":"2021-06-18T18:25:06.150005Z","shell.execute_reply.started":"2021-06-18T18:25:05.891753Z","shell.execute_reply":"2021-06-18T18:25:06.149051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales_by_year = data.groupby('Year')[['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales', 'Global_Sales']].sum().reset_index()\n\nfig = plt.figure(facecolor='whitesmoke', figsize=(6,5))\nsns.set_style('white')\naxes1 = fig.add_axes([0, 0, 1.5, 1.5]) \naxes2 = fig.add_axes([1.6, 0, 1, 1.5]) \naxes3 = fig.add_axes([0, -1.8, 1.5, 1.5])\naxes4 = fig.add_axes([1.6, -1.8, 1, 1.5]) \n\n# figure\nsns.lineplot(x=sales_by_year['Year'], y=sales_by_year['NA_Sales'],color='crimson',lw=4, ax=axes1)\naxes1.scatter(x=[2000, 2003, 2012, 2013, 2008], y=[94.49, 193.59, 154.93, 154.77, 351.44], color='black', lw=4)\naxes1.set_facecolor('whitesmoke')\naxes1.set_xlabel('Year', fontsize=18, color='black')\naxes1.set_ylabel('Sales', fontsize=18, color='black')\naxes1.text(1979, 340, 'Sales in', color='black', fontsize=20, fontweight='bold')\naxes1.text(1985, 340, 'North America', color='crimson', fontsize=24, fontweight='bold')\n\n# growth\naxes1.annotate('', xy=(2006.9, 350), xytext=(1993.6, 60),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('Strong growth with 2 downturns in', xy=(1993.6, 100), xytext=(1993.6, 100), rotation=61.5, color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2000', xy=(2000.6, 250), xytext=(2000.6, 250), rotation=61.5, color='crimson', fontweight='bold', fontsize=12)\naxes1.annotate('and', xy=(2001.8, 276), xytext=(2001.8, 276), rotation=61.5, color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2002', xy=(2002.8, 298), xytext=(2002.8, 298), rotation=61.5, color='crimson', fontweight='bold', fontsize=12)\n\n\n# stagnation\naxes1.annotate('', xy=(2012, 149), xytext=(2008, 100),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('', xy=(2013, 149), xytext=(2012, 100),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('Stagnation in', xy=(2000.5, 90), xytext=(2000.5, 90), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2012', xy=(2006.3, 90), xytext=(2006.3, 90),color='crimson', fontweight='bold', fontsize=12)\naxes1.annotate('and', xy=(2008.5, 90), xytext=(2008.5, 90),color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2013', xy=(2010.3, 90), xytext=(2010.3, 90),color='crimson', fontweight='bold', fontsize=12)\n\n# max\naxes1.annotate('', xy=(2009, 353.8), xytext=(2015, 325),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.3'))\naxes1.annotate('Maximum sales', xy=(2009, 353.8), xytext=(2011, 317.5), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('in', xy=(2011, 303.5), xytext=(2011, 303.5), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2008', xy=(2012, 303.5), xytext=(2012, 303.5), color='crimson', fontweight='bold', fontsize=12)\n\n# conclusion № 1\naxes2.set_facecolor('whitesmoke')\naxes2.axis('off')\naxes2.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes2.text(0, 0.8, 'On the chart we can see that', color='black', fontsize=14)\naxes2.text(0.48, 0.8, 'sales before 1996', color='crimson', fontsize=14)\naxes2.text(0, 0.75, 'were highly volatile -', color='black', fontsize=14)\naxes2.text(0.35, 0.75, 'growth was followed by donwturn.', color='crimson', fontsize=14)\naxes2.text(0, 0.7, 'After 1996', color='crimson', fontsize=14)\naxes2.text(0.18, 0.7, 'we observe', color='black', fontsize=14)\naxes2.text(0.375, 0.7, 'strong growth', color='crimson', fontsize=14)\naxes2.text(0.6, 0.7, ', but were two ', color='black', fontsize=14)\naxes2.text(0, 0.65, 'downturns in 2000 and 2002.', color='crimson', fontsize=14)\naxes2.text(0, 0.6, 'It should be noted that', color='black', fontsize=14)\naxes2.text(0.383, 0.6, 'in 2008', color='crimson', fontsize=14)\naxes2.text(0.518, 0.6, 'were the most sales -', color='black', fontsize=14)\naxes2.text(0, 0.55, '351.44 millions.', color='crimson', fontsize=14)\naxes2.text(0, 0.5, 'After 2008', color='crimson', fontsize=14)\naxes2.text(0.185, 0.5, 'we can see a', color='black', fontsize=14)\naxes2.text(0.41, 0.5, 'strong decrease in sales.', color='crimson', fontsize=14)\naxes2.text(0, 0.45, 'In the period', color='black', fontsize=14)\naxes2.text(0.22, 0.45, '2012 - 2013', color='crimson', fontsize=14)\naxes2.text(0.43, 0.45, 'was', color='black', fontsize=14)\naxes2.text(0.51, 0.45, 'stagnation.', color='crimson', fontsize=14)\n\n# calculation of sales growth rates\nlist_NA = []\nfor n in range(1, 37):\n    d = ((sales_by_year['NA_Sales'][n] - sales_by_year['NA_Sales'][n-1])/sales_by_year['NA_Sales'][n-1])*100\n    list_NA.append(d)\n\n# visualisation growth rates sales in NA by years\ncolors = ['crimson' if _ > 0 else 'darksalmon' for _ in list_NA]\naxes3.bar(height=list_NA, x=sales_by_year['Year'][range(1, 37)], color=colors)\nfor p in axes3.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy()\n    if height >= 0:\n        axes3.annotate('{:.0f}'.format(height), (x + width/2, y + height*1.02), ha='center')\n    elif height <= -55:\n        axes3.annotate('{:.0f}'.format(height), (x + width/2, y + height*1.2), ha='center')\n    else:\n        axes3.annotate('{:.0f}'.format(height), (x + width/2, y + height*1.4), ha='center')\naxes3.set_facecolor('whitesmoke')\naxes3.set_xlabel('Year', fontsize=18, color='black')\naxes3.set_ylabel('Growth rate, %', fontsize=18, color='black')\naxes3.text(1997, 315, 'Growth rate sales(%) in', color='black', fontsize=20, fontweight='bold')\naxes3.text(2001.39, 280, 'North America', color='crimson', fontsize=24, fontweight='bold')\n\n# max +\naxes3.annotate('', xy=(1985, 330), xytext=(1992, 285),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.3'))\naxes3.annotate('Max', xy=(1990, 268), xytext=(1990, 268), color='black', fontweight='bold', fontsize=12)\naxes3.annotate('+', xy=(1991.8, 268), xytext=(1991.8, 268), color='crimson', fontweight='bold', fontsize=24)\n\n# max -\naxes3.annotate('', xy=(2015, -79), xytext=(2008, -63),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.3'))\naxes3.annotate('Max', xy=(2006, -46), xytext=(2006, -46), color='black', fontweight='bold', fontsize=12)\naxes3.annotate('-', xy=(2007.8, -46), xytext=(2007.8, -46), color='darksalmon', fontweight='bold', fontsize=24)\n\n# conclusion № 2\naxes4.set_facecolor('whitesmoke')\naxes4.axis('off')\naxes4.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes4.text(0, 0.8, 'We can see that', color='black', fontsize=14)\naxes4.text(0.27, 0.8, 'maximum positive growth rate', color='crimson', fontsize=14)\naxes4.text(0, 0.75, 'sales in NA was', color='black', fontsize=14)\naxes4.text(0.27, 0.75, 'in 1983 - 1984', color='crimson', fontsize=14)\naxes4.text(0.52, 0.75, 'and', color='black', fontsize=14)\naxes4.text(0.6, 0.75, 'maximum negative', color='crimson', fontsize=14)\naxes4.text(0, 0.70, 'growth rate', color='crimson', fontsize=14)\naxes4.text(0.21, 0.70, 'sales in NA was', color='black', fontsize=14)\naxes4.text(0.48, 0.70, 'in 2015 - 2016.', color='crimson', fontsize=14)\naxes4.text(0, 0.65, 'Average growth rates by period:', color='crimson', fontsize=14)\naxes4.text(0, 0.60, f'* 1980-1984 - {round(statistics.mean(list_NA[:5]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.55, f'* 1985-1989 - {round(statistics.mean(list_NA[5:10]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.50, f'* 1990-1994 - {round(statistics.mean(list_NA[10:15]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.45, f'* 1995-1999 - {round(statistics.mean(list_NA[15:20]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.40, f'* 2000-2004 - {round(statistics.mean(list_NA[20:25]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.35, f'* 2005-2009 - {round(statistics.mean(list_NA[25:30]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.30, f'* 2010-2016 - {round(statistics.mean(list_NA[30:]),2)}%.', color='black', fontsize=14)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:25:06.152367Z","iopub.execute_input":"2021-06-18T18:25:06.15263Z","iopub.status.idle":"2021-06-18T18:25:07.25299Z","shell.execute_reply.started":"2021-06-18T18:25:06.152603Z","shell.execute_reply":"2021-06-18T18:25:07.251503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(facecolor='whitesmoke')\nsns.set_style('white')\naxes1 = fig.add_axes([0, 0, 1.5, 1.5]) \naxes2 = fig.add_axes([1.6, 0, 1, 1.5]) \naxes3 = fig.add_axes([0, -1.8, 1.5, 1.5]) \naxes4 = fig.add_axes([1.6, -1.8, 1, 1.5]) \n\nsns.lineplot(x=sales_by_year['Year'], y=sales_by_year['EU_Sales'],color='crimson',lw=4, ax=axes1)\naxes1.scatter(x=[2000, 2003, 2009, 2013, 2014], y=[52.75, 103.81, 191.59, 125.80, 125.63], color='black', lw=4)\naxes1.set_facecolor('whitesmoke')\naxes1.set_xlabel('Year', color='black', fontsize=18)\naxes1.set_ylabel('Sales', color='black', fontsize=18)\naxes1.text(1979, 180, 'Sales in', color='black', fontsize=20, fontweight='bold')\naxes1.text(1985, 180, 'Europe', color='crimson', fontsize=24, fontweight='bold')\n\n# growth\naxes1.annotate('', xy=(2007, 193), xytext=(1993, 23),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('Strong growth with 2 downturns in', xy=(1992.4, 40), xytext=(1992.4, 40), rotation=56, color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2000', xy=(2000.7, 139.1), xytext=(2000.7, 139.1), rotation=56, color='crimson', fontweight='bold', fontsize=12)\naxes1.annotate('and', xy=(2001.9, 154.2), xytext=(2001.9, 154.2), rotation=56, color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2003', xy=(2003, 166.7), xytext=(2003, 166.7), rotation=56, color='crimson', fontweight='bold', fontsize=12)\n\n# stagnation\naxes1.annotate('', xy=(2012.4, 116), xytext=(2008, 71),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('', xy=(2013.4, 116), xytext=(2012, 71),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('Stagnation in', xy=(2000.5, 61), xytext=(2000.5, 61), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2013', xy=(2006.3, 61), xytext=(2006.3, 61),color='crimson', fontweight='bold', fontsize=12)\naxes1.annotate('and', xy=(2008.5, 61), xytext=(2008.5, 61),color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2014', xy=(2010.3, 61), xytext=(2010.3, 61),color='crimson', fontweight='bold', fontsize=12)\n\n# max\naxes1.annotate('', xy=(2010, 194), xytext=(2016, 169.2),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.3'))\naxes1.annotate('Maximum sales', xy=(2011.5, 161.7), xytext=(2011.5, 161.7), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('in', xy=(2011.5, 152), xytext=(2011.5, 152), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2009', xy=(2012.5, 152), xytext=(2012.5, 152), color='crimson', fontweight='bold', fontsize=12)\n\n# conclusion № 1\naxes2.set_facecolor('whitesmoke')\naxes2.axis('off')\naxes2.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes2.text(0, 0.8, 'On the chart we can see', color='black', fontsize=14)\naxes2.text(0.41, 0.8, 'strong growth after 1994', color='crimson', fontsize=14)\naxes2.text(0.825, 0.8, 'and two', color='black', fontsize=14)\naxes2.text(0, 0.75, 'downturns in 2000 and 2003.', color='crimson', fontsize=14)\naxes2.text(0, 0.7, 'And', color='black', fontsize=14)\naxes2.text(0.07, 0.7, 'max sales', color='crimson', fontsize=14)\naxes2.text(0.245, 0.7, 'were', color='black', fontsize=14)\naxes2.text(0.333, 0.7, 'in 2009.', color='crimson', fontsize=14)\naxes2.text(0.472, 0.7, 'We observe', color='black', fontsize=14)\naxes2.text(0.675, 0.7, 'decrease sales', color='crimson', fontsize=14)\naxes2.text(0, 0.65, 'after 2009', color='crimson', fontsize=14)\naxes2.text(0.18, 0.65, 'with a', color='black', fontsize=14)\naxes2.text(0.29, 0.65, 'small increase in 2011-2012.', color='crimson', fontsize=14)\naxes2.text(0, 0.6, 'In period', color='black', fontsize=14)\naxes2.text(0.155, 0.6, '2013 - 2014 sales almost unchanged.', color='crimson', fontsize=14)\naxes2.text(0, 0.55, '----------------------------------------------------------------------------------------', color='black', fontsize=14)\naxes2.text(0, 0.50, 'In comprasion with NA', color='crimson', fontsize=14)\naxes2.text(0.375, 0.50, 'we can say that the', color='black', fontsize=14)\naxes2.text(0.71, 0.50, 'situation', color='crimson', fontsize=14)\naxes2.text(0, 0.45, 'is almost', color='black', fontsize=14)\naxes2.text(0.16, 0.45, 'identical.', color='crimson', fontsize=14)\naxes2.text(0, 0.4, 'Interestingly that', color='black', fontsize=14)\naxes2.text(0.3, 0.4, 'maximum sales, decrease and stagnation', color='crimson', fontsize=14)\naxes2.text(0, 0.35, 'in EU', color='crimson', fontsize=14)\naxes2.text(0, 0.35, 'in EU', color='crimson', fontsize=14)\naxes2.text(0.1, 0.35, 'were a', color='black', fontsize=14)\naxes2.text(0.22, 0.35, 'year later', color='crimson', fontsize=14)\naxes2.text(0.39, 0.35, 'tnan', color='black', fontsize=14)\naxes2.text(0.47, 0.35, 'in NA.', color='crimson', fontsize=14)\nfig.show()\n\n# calculation of sales growth rates\nlist_EU = []\nfor n in range(1, 37):\n    d = ((sales_by_year['EU_Sales'][n] - sales_by_year['EU_Sales'][n-1])/sales_by_year['EU_Sales'][n-1])*100\n    list_EU.append(d)\n\n# visualisation growth rates sales in EU by years\ncolors = ['crimson' if _ > 0 else 'darksalmon' for _ in list_EU]\naxes3.bar(height=list_EU, x=sales_by_year['Year'][range(1, 37)], color=colors)\nfor p in axes3.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy()\n    if height >= 0:\n        axes3.annotate('{:.0f}'.format(height), (x + width/2, y + height*1.02), ha='center')\n    elif height <= -55:\n        axes3.annotate('{:.0f}'.format(height), (x + width/2, y + height*1.2), ha='center')\n    else:\n        axes3.annotate('{:.0f}'.format(height), (x + width/2, y + height*1.4), ha='center')\naxes3.set_facecolor('whitesmoke')\naxes3.set_xlabel('Year', fontsize=18, color='black')\naxes3.set_ylabel('Growth rate, %', fontsize=18, color='black')\naxes3.text(1997, 315, 'Growth rate sales(%) in', color='black', fontsize=20, fontweight='bold')\naxes3.text(2007.48, 280, 'Europe', color='crimson', fontsize=24, fontweight='bold')\n\n# max +\naxes3.annotate('', xy=(1989, 368), xytext=(1996, 353),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.3'))\naxes3.annotate('Max', xy=(1994, 336), xytext=(1994, 336), color='black', fontweight='bold', fontsize=12)\naxes3.annotate('+', xy=(1995.8, 336), xytext=(1995.8, 336), color='crimson', fontweight='bold', fontsize=24)\n\n# max -\naxes3.annotate('', xy=(2015, -73), xytext=(2008, -57),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.3'))\naxes3.annotate('Max', xy=(2006, -40), xytext=(2006, -40), color='black', fontweight='bold', fontsize=12)\naxes3.annotate('-', xy=(2007.8, -40), xytext=(2007.8, -40), color='darksalmon', fontweight='bold', fontsize=24)\n\n# conclusion № 2\naxes4.set_facecolor('whitesmoke')\naxes4.axis('off')\naxes4.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes4.text(0, 0.8, 'From chart we can see that', color='black', fontsize=14)\naxes4.text(0.455, 0.8, 'maximum positive growth', color='crimson', fontsize=14)\naxes4.text(0, 0.75, 'was', color='black', fontsize=14)\naxes4.text(0.07, 0.75, 'in 1987 - 1988,', color='crimson', fontsize=14)\naxes4.text(0.32, 0.75, 'and', color='black', fontsize=14)\naxes4.text(0.395, 0.75, 'negative in 2015 - 2016.', color='crimson', fontsize=14)\naxes4.text(0, 0.7, 'Average growth rates by period:', color='crimson', fontsize=14)\naxes4.text(0, 0.65, f'* 1980-1984 - {round(statistics.mean(list_EU[:5]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.6, f'* 1985-1989 - {round(statistics.mean(list_EU[5:10]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.55, f'* 1990-1994 - {round(statistics.mean(list_EU[10:15]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.5, f'* 1995-1999 - {round(statistics.mean(list_EU[15:20]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.45, f'* 2000-2004 - {round(statistics.mean(list_EU[20:25]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.4, f'* 2005-2009 -{round(statistics.mean(list_EU[25:30]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.35, f'* 2010-2016 - {round(statistics.mean(list_EU[30:]),2)}%.', color='black', fontsize=14)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:26:46.598872Z","iopub.execute_input":"2021-06-18T18:26:46.599212Z","iopub.status.idle":"2021-06-18T18:26:47.701496Z","shell.execute_reply.started":"2021-06-18T18:26:46.599178Z","shell.execute_reply":"2021-06-18T18:26:47.700413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(facecolor='whitesmoke')\nsns.set_style('white')\naxes1 = fig.add_axes([0, 0, 1.5, 1.5]) \naxes2 = fig.add_axes([1.6, 0, 1, 1.5]) \naxes3 = fig.add_axes([0, -1.8, 1.5, 1.5]) \naxes4 = fig.add_axes([1.6, -1.8, 1, 1.5]) \n\nsns.lineplot(x=sales_by_year['Year'], y=sales_by_year['JP_Sales'],color='crimson',lw=4, ax=axes1)\naxes1.scatter(x=[1996, 2003, 2006], y=[57.44,34.20, 73.73], color='black', lw=4)\naxes1.set_facecolor('whitesmoke')\naxes1.set_xlabel('Year', color='black', fontsize=18)\naxes1.set_ylabel('Sales', color='black', fontsize=18)\naxes1.text(1979, 70, 'Sales in', color='black', fontsize=20, fontweight='bold')\naxes1.text(1985, 70, 'Japan', color='crimson', fontsize=24, fontweight='bold')\n\n# 1st max\naxes1.annotate('', xy=(1995, 57.44), xytext=(1988, 35.6),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=-.3'))\naxes1.annotate('1st max sales in', xy=(1982, 32.6), xytext=(1982, 32.6), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('1996', xy=(1989, 32.6), xytext=(1989, 32.6), color='crimson', fontweight='bold', fontsize=12)\n\n# 2nd max\naxes1.annotate('', xy=(2007, 74.23), xytext=(2016, 64.73),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.3'))\naxes1.annotate('2nd max sales in', xy=(2009.5, 61.6), xytext=(2009.5, 61.6), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2006', xy=(2014.4, 59), xytext=(2014.4, 59), color='crimson', fontweight='bold', fontsize=12)\n\n# downturns\naxes1.annotate('', xy=(2003, 33.6), xytext=(2003, 20.2),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=0'))\naxes1.annotate('Downturn in', xy=(1997, 18.8), xytext=(1999, 18.8), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2003', xy=(2004.3, 18.8), xytext=(2004.3, 18.8), color='crimson', fontweight='bold', fontsize=12)\n\n# conclusion № 1\naxes2.set_facecolor('whitesmoke')\naxes2.axis('off')\naxes2.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes2.text(0, 0.8, 'On the chart we see', color='black', fontsize=14)\naxes2.text(0.34, 0.8, '2 periods of growth', color='crimson', fontsize=14)\naxes2.text(0.67, 0.8, 'the maximum', color='black', fontsize=14)\naxes2.text(0, 0.75, 'points of which were', color='black', fontsize=14)\naxes2.text(0.35, 0.75, 'in 1996 and 2006.', color='crimson', fontsize=14)\naxes2.text(0, 0.7, 'After 1993 was', color='black', fontsize=14)\naxes2.text(0.255, 0.7, 'strong donwturn,', color='crimson', fontsize=14)\naxes2.text(0.545, 0.7, 'which stopped', color='black', fontsize=14)\naxes2.text(0, 0.65, 'in 2003.', color='crimson', fontsize=14)\naxes2.text(0, 0.6, '----------------------------------------------------------------------------------------', color='black', fontsize=14)\naxes2.text(0, 0.55, 'In comparison with the previous charts', color='black', fontsize=14)\naxes2.text(0.65, 0.55, 'this chart is', color='crimson', fontsize=14)\naxes2.text(0, 0.5, 'more variable.', color='crimson', fontsize=14)\naxes2.text(0.25, 0.5, 'We observe', color='black', fontsize=14)\naxes2.text(0.45, 0.5, '2 \"ridges\" with maximum sales.', color='crimson', fontsize=14)\n\n# calculation of sales growth rates\nlist_JP = []\nfor n in range(4, 37):\n    d = ((sales_by_year['JP_Sales'][n] - sales_by_year['JP_Sales'][n-1])/sales_by_year['JP_Sales'][n-1])*100\n    list_JP.append(d)\n\n# visualisation growth rates sales in EU by years\ncolors = ['crimson' if _ > 0 else 'darksalmon' for _ in list_JP]\naxes3.bar(height=list_JP, x=sales_by_year['Year'][range(4, 37)], color=colors)\nfor p in axes3.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy()\n    if height >= 0:\n        axes3.annotate('{:.0f}'.format(height), (x + width/2, y + height*1.02), ha='center')\n    elif height <= -40:\n        axes3.annotate('{:.0f}'.format(height), (x + width/2, y + height*1.1), ha='center')\n    else:\n        axes3.annotate('{:.0f}'.format(height), (x + width/2, y + height*1.4), ha='center')\naxes3.set_facecolor('whitesmoke')\naxes3.set_xlabel('Year', fontsize=18, color='black')\naxes3.set_ylabel('Growth rate, %', fontsize=18, color='black')\naxes3.text(1997, 95, 'Growth rate sales(%) in', color='black', fontsize=20, fontweight='bold')\naxes3.text(2007.68, 83, 'Japan', color='crimson', fontsize=24, fontweight='bold')\n\n# max +\naxes3.annotate('', xy=(1990, 97), xytext=(1986, 73),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=-.3'))\naxes3.annotate('Max', xy=(1985, 69), xytext=(1985, 69), color='black', fontweight='bold', fontsize=12)\naxes3.annotate('+', xy=(1986.5, 69), xytext=(1986.5, 69), color='crimson', fontweight='bold', fontsize=24)\n\n# max -\naxes3.annotate('', xy=(2015, -60), xytext=(2010, -52),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.3'))\naxes3.annotate('Max', xy=(2009, -51), xytext=(2009, -51), color='black', fontweight='bold', fontsize=12)\naxes3.annotate('-', xy=(2010.8, -51), xytext=(2010.8, -51), color='darksalmon', fontweight='bold', fontsize=24)\n\n# conclusion № 2\naxes4.set_facecolor('whitesmoke')\naxes4.axis('off')\naxes4.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes4.text(0, 0.8, 'From chart we can see that', color='black', fontsize=14)\naxes4.text(0.455, 0.8, 'maximum positive growth', color='crimson', fontsize=14)\naxes4.text(0, 0.75, 'was', color='black', fontsize=14)\naxes4.text(0.07, 0.75, 'in 1991 - 1992,', color='crimson', fontsize=14)\naxes4.text(0.32, 0.75, 'and', color='black', fontsize=14)\naxes4.text(0.395, 0.75, 'negative in 2015 - 2016.', color='crimson', fontsize=14)\naxes4.text(0, 0.7, 'Average growth rates by period:', color='crimson', fontsize=14)\naxes4.text(0, 0.65, f'* 1983-1984 - {round(statistics.mean(list_JP[:1]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.6, f'* 1985-1989 - {round(statistics.mean(list_JP[1:6]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.55, f'* 1990-1994 - {round(statistics.mean(list_JP[6:11]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.5, f'* 1995-1999 - {round(statistics.mean(list_JP[11:16]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.45, f'* 2000-2004 - {round(statistics.mean(list_JP[16:21]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.4, f'* 2005-2009 - {round(statistics.mean(list_JP[21:26]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.35, f'* 2010-2016 - {round(statistics.mean(list_JP[26:]),2)}%.', color='black', fontsize=14)\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:26:53.628929Z","iopub.execute_input":"2021-06-18T18:26:53.629227Z","iopub.status.idle":"2021-06-18T18:26:54.58167Z","shell.execute_reply.started":"2021-06-18T18:26:53.6292Z","shell.execute_reply":"2021-06-18T18:26:54.58009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(facecolor='whitesmoke')\nsns.set_style('white')\naxes1 = fig.add_axes([0, 0, 1.5, 1.5]) \naxes2 = fig.add_axes([1.6, 0, 1, 1.5]) \naxes3 = fig.add_axes([0, -1.8, 1.5, 1.5]) \naxes4 = fig.add_axes([1.6, -1.8, 1, 1.5]) \n\nsns.lineplot(x=sales_by_year['Year'], y=sales_by_year['Other_Sales'],color='crimson',lw=4, ax=axes1)\naxes1.scatter(x=[1999, 2003, 2005, 2008, 2013, 2014], y=[10.05, 26.01, 40.55, 82.39, 39.82, 40.02], color='black', lw=4)\naxes1.set_facecolor('whitesmoke')\naxes1.set_xlabel('Year', color='black', fontsize=18)\naxes1.set_ylabel('Sales', color='black', fontsize=18)\naxes1.text(1979, 80, 'Sales in', color='black', fontsize=20, fontweight='bold')\naxes1.text(1985, 80, 'Other countries', color='crimson', fontsize=24, fontweight='bold')\n\n# growth\naxes1.annotate('', xy=(2006, 81), xytext=(1995, 7),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('Strong growth with 3 downturns in', xy=(1994.1, 10), xytext=(1994.1, 10), rotation=62.5, color='black', fontweight='bold', fontsize=12)\naxes1.annotate('1999, 2003', xy=(2001, 55.5), xytext=(2001, 55.5), rotation=62.5, color='crimson', fontweight='bold', fontsize=12)\naxes1.annotate('and', xy=(2003.2, 70.6), xytext=(2003.2, 70.6), rotation=62.5, color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2005', xy=(2004, 76.4), xytext=(2004, 76.4), rotation=62.5, color='crimson', fontweight='bold', fontsize=12)\n\n# stagnation\naxes1.annotate('', xy=(2012.9, 38.5), xytext=(2008, 21),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('', xy=(2013.9, 38.5), xytext=(2012, 21),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('Stagnation in', xy=(2001.5, 18.5), xytext=(2001.5, 18.5), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2013', xy=(2007.35, 18.5), xytext=(2007.35, 18.5),color='crimson', fontweight='bold', fontsize=12)\naxes1.annotate('and', xy=(2009.5, 18.5), xytext=(2009.5, 18.5),color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2014', xy=(2011.2, 18.5), xytext=(2011.2, 18.5),color='crimson', fontweight='bold', fontsize=12)\n\n# max\naxes1.annotate('', xy=(2009, 84), xytext=(2015, 67),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.3'))\naxes1.annotate('Maximum sales in', xy=(2009.8, 65), xytext=(2009.8, 65), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2009', xy=(2015.2, 62), xytext=(2015.2, 62), color='crimson', fontweight='bold', fontsize=12)\n\n# conclusion № 1\naxes2.set_facecolor('whitesmoke')\naxes2.axis('off')\naxes2.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes2.text(0, 0.8, 'Strong growth', color='crimson', fontsize=14)\naxes2.text(0.24, 0.8, 'began', color='black', fontsize=14)\naxes2.text(0.353, 0.8, 'after 1995', color='crimson', fontsize=14)\naxes2.text(0.535, 0.8, 'and continued', color='black', fontsize=14)\naxes2.text(0.787, 0.8, 'until 2008.', color='crimson', fontsize=14)\naxes2.text(0, 0.75, 'Else we can see', color='black', fontsize=14)\naxes2.text(0.27, 0.75, '3 downturns in 1999, 2003 and 2005.', color='crimson', fontsize=14)\naxes2.text(0, 0.7, 'We observe that', color='black', fontsize=14)\naxes2.text(0.28, 0.7, 'sales decreased after 2008.', color='crimson', fontsize=14)\naxes2.text(0, 0.65, 'Sales downturns stopped in 2013-2014', color='crimson', fontsize=14)\naxes2.text(0.64, 0.65, 'and then began again', color='black', fontsize=14)\naxes2.text(0, 0.6, '----------------------------------------------------------------------------------------', color='black', fontsize=14)\naxes2.text(0, 0.55, 'On this chart we see familiar picture, which was', color='black', fontsize=14)\naxes2.text(0, 0.50, 'in North America and Europe.', color='black', fontsize=14)\n\n\n# calculation of sales growth rates\nlist_Other = []\nfor n in range(1, 37):\n    d = ((sales_by_year['Other_Sales'][n] - sales_by_year['Other_Sales'][n-1])/sales_by_year['Other_Sales'][n-1])*100\n    list_Other.append(d)\n\n# visualisation growth rates sales in Other by years\ncolors = ['crimson' if _ > 0 else 'darksalmon' for _ in list_Other]\naxes3.bar(height=list_Other, x=sales_by_year['Year'][range(1, 37)], color=colors)\nfor p in axes3.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy()\n    if height >= 0:\n        axes3.annotate('{:.0f}'.format(height), (x + width/2, y + height*1.02), ha='center')\n    elif height <= -55:\n        axes3.annotate('{:.0f}'.format(height), (x + width/2, y + height*1.2), ha='center')\n    else:\n        axes3.annotate('{:.0f}'.format(height), (x + width/2, y + height*1.4), ha='center')\naxes3.set_facecolor('whitesmoke')\naxes3.set_xlabel('Year', fontsize=18, color='black')\naxes3.set_ylabel('Growth rate, %', fontsize=18, color='black')\naxes3.text(1997, 365, 'Growth rate sales(%) in', color='black', fontsize=20, fontweight='bold')\naxes3.text(2000.5, 330, 'Other countries', color='crimson', fontsize=24, fontweight='bold')\n\n# max +\naxes3.annotate('', xy=(1983, 401), xytext=(1980, 322),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=-.3'))\naxes3.annotate('Max', xy=(1979, 312), xytext=(1979, 312), color='black', fontweight='bold', fontsize=12)\naxes3.annotate('+', xy=(1981, 312), xytext=(1981, 312), color='crimson', fontweight='bold', fontsize=24)\n\n# max -\naxes3.annotate('', xy=(1987, -91), xytext=(1995, -72),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=-.3'))\naxes3.annotate('Max', xy=(1994, -62), xytext=(1994, -62), color='black', fontweight='bold', fontsize=12)\naxes3.annotate('-', xy=(1996, -62), xytext=(1996, -62), color='darksalmon', fontweight='bold', fontsize=24)\n\n# conclusion № 2\naxes4.set_facecolor('whitesmoke')\naxes4.axis('off')\naxes4.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes4.text(0, 0.8, 'Maximum positive growth', color='crimson', fontsize=14)\naxes4.text(0.445, 0.8, '-', color='black', fontsize=14)\naxes4.text(0.475, 0.8, '1983-1984.', color='crimson', fontsize=14)\naxes4.text(0.0, 0.75, 'Maximum negative growth', color='crimson', fontsize=14)\naxes4.text(0.445, 0.75, '-', color='black', fontsize=14)\naxes4.text(0.475, 0.75, '1983-1984.', color='crimson', fontsize=14)\naxes4.text(0, 0.7, 'Average growth rates by period:', color='crimson', fontsize=14)\naxes4.text(0, 0.65, f'* 1980-1984 - {round(statistics.mean(list_Other[:5]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.6, f'* 1985-1989 - {round(statistics.mean(list_Other[5:10]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.55, f'* 1990-1994 - {round(statistics.mean(list_Other[10:15]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.5, f'* 1995-1999 - {round(statistics.mean(list_Other[15:20]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.45, f'* 2000-2004 - {round(statistics.mean(list_Other[20:25]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.4, f'* 2005-2009 - {round(statistics.mean(list_Other[25:30]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.35, f'* 2010-2016 - {round(statistics.mean(list_Other[30:]),2)}%.', color='black', fontsize=14)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:27:00.333989Z","iopub.execute_input":"2021-06-18T18:27:00.334306Z","iopub.status.idle":"2021-06-18T18:27:01.339566Z","shell.execute_reply.started":"2021-06-18T18:27:00.334276Z","shell.execute_reply":"2021-06-18T18:27:01.338264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(facecolor='whitesmoke')\nsns.set_style('white')\naxes1 = fig.add_axes([0, 0, 1.5, 1.5]) \naxes2 = fig.add_axes([1.6, 0, 1, 1.5]) \naxes3 = fig.add_axes([0, -1.8, 1.5, 1.5]) \naxes4 = fig.add_axes([1.6, -1.8, 1, 1.5]) \n\nsns.lineplot(x=sales_by_year['Year'], y=sales_by_year['Global_Sales'],color='crimson',lw=4, ax=axes1)\naxes1.scatter(x=[2000, 2003, 2008, 2012, 2013], y=[ 201.56, 357.85, 678.90, 363.53, 368.11], color='black', lw=4)\naxes1.set_facecolor('whitesmoke')\naxes1.set_xlabel('Year', fontsize=18, color='black')\naxes1.set_ylabel('Sales', fontsize=18, color='black')\naxes1.text(1979, 640, 'Global sales', color='crimson', fontsize=24, fontweight='bold')\n\n# growth\naxes1.annotate('', xy=(2007, 681), xytext=(1992, 100),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('Strong growth with 2 downturns in', xy=(1991.2, 135), xytext=(1991.2, 135), rotation=54, color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2000', xy=(2000, 476), xytext=(2000, 476), rotation=54, color='crimson', fontweight='bold', fontsize=12)\naxes1.annotate('and', xy=(2001.3, 529), xytext=(2001.3, 529), rotation=54, color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2003', xy=(2002.3, 570), xytext=(2002.3, 570), rotation=54, color='crimson', fontweight='bold', fontsize=12)\n\n# stagnation\naxes1.annotate('', xy=(2011.9, 357), xytext=(2010, 183),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('', xy=(2012.9, 357), xytext=(2012, 183),\n              arrowprops=dict(color='black', arrowstyle='->'))\naxes1.annotate('Stagnation in', xy=(2001, 165), xytext=(2001, 165), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2013', xy=(2007, 165), xytext=(2007, 165),color='crimson', fontweight='bold', fontsize=12)\naxes1.annotate('and', xy=(2009.3, 165), xytext=(2009.3, 165),color='black', fontweight='bold', fontsize=12)\naxes1.annotate('2014', xy=(2011.2, 165), xytext=(2011.2, 165),color='crimson', fontweight='bold', fontsize=12)\n\n# # max\naxes1.annotate('', xy=(2008, 690), xytext=(2015, 580),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.6'))\naxes1.annotate('Maximum sales', xy=(2011, 565), xytext=(2011, 565), color='black', fontweight='bold', fontsize=12)\naxes1.annotate('in 2008', xy=(2014.2, 540), xytext=(2014.2, 540), color='crimson', fontweight='bold', fontsize=12)\n\n# conclusion № 1\naxes2.set_facecolor('whitesmoke')\naxes2.axis('off')\naxes2.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes2.text(0, 0.8, 'Between 1993 and 2008', color='crimson', fontsize=14)\naxes2.text(0.422, 0.8, 'was', color='black', fontsize=14)\naxes2.text(0.505, 0.8, 'strong growth', color='crimson', fontsize=14)\naxes2.text(0, 0.75, 'with', color='black', fontsize=14)\naxes2.text(0.08, 0.75, '2 downturns in 2000 and 2003.', color='crimson', fontsize=14)\naxes2.text(0, 0.7, 'After 2008', color='crimson', fontsize=14)\naxes2.text(0.185, 0.7, 'we see', color='black', fontsize=14)\naxes2.text(0.32, 0.7, 'decreased sales.', color='crimson', fontsize=14)\naxes2.text(0, 0.65, 'Also we observe', color='black', fontsize=14)\naxes2.text(0.278, 0.65, 'stagnation in 2013-2014.', color='crimson', fontsize=14)\n\n# calculation of sales growth rates\nlist_Global = []\nfor n in range(1, 37):\n    d = ((sales_by_year['Global_Sales'][n] - sales_by_year['Global_Sales'][n-1])/sales_by_year['Global_Sales'][n-1])*100\n    list_Global.append(d)\n\n# visualisation growth rates sales in Other by years\ncolors = ['crimson' if _ > 0 else 'darksalmon' for _ in list_Global]\naxes3.bar(height=list_Global, x=sales_by_year['Year'][range(1, 37)], color=colors)\nfor p in axes3.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy()\n    if height >= 0:\n        axes3.annotate('{:.0f}'.format(height), (x + width/2, y + height*1.02), ha='center')\n    elif height <= -55:\n        axes3.annotate('{:.0f}'.format(height), (x + width/2, y + height*1.1), ha='center')\n    else:\n        axes3.annotate('{:.0f}'.format(height), (x + width/2, y + height*1.4), ha='center')\naxes3.set_facecolor('whitesmoke')\naxes3.set_xlabel('Year', fontsize=18, color='black')\naxes3.set_ylabel('Growth rate, %', fontsize=18, color='black')\naxes3.text(1994, 180, 'Global growth rate sales(%)', color='crimson', fontsize=24, fontweight='bold')\n\n# max +\naxes3.annotate('', xy=(1981.5, 215), xytext=(1989, 180),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.3'))\naxes3.annotate('Max', xy=(1987, 172), xytext=(1987, 172), color='black', fontweight='bold', fontsize=12)\naxes3.annotate('+', xy=(1988.8, 171), xytext=(1988.8, 171), color='crimson', fontweight='bold', fontsize=24)\n\n# max -\naxes3.annotate('', xy=(2015, -73), xytext=(2008, -57),\n               arrowprops=dict(color='black', arrowstyle='->', connectionstyle='arc3,rad=.3'))\naxes3.annotate('Max', xy=(2006, -46), xytext=(2006, -46), color='black', fontweight='bold', fontsize=12)\naxes3.annotate('-', xy=(2007.8, -46), xytext=(2007.8, -46), color='darksalmon', fontweight='bold', fontsize=24)\n\n# conclusion № 2\naxes4.set_facecolor('whitesmoke')\naxes4.axis('off')\naxes4.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes4.text(0, 0.8, 'Maximum positive growth', color='crimson', fontsize=14)\naxes4.text(0.438, 0.8, 'was', color='black', fontsize=14)\naxes4.text(0.52, 0.8, 'in 1980-1981,', color='crimson', fontsize=14)\naxes4.text(0.76, 0.8, 'and', color='black', fontsize=14)\naxes4.text(0, 0.75, 'maximum negative growth', color='crimson', fontsize=14)\naxes4.text(0.45, 0.75, '-', color='black', fontsize=14)\naxes4.text(0.48, 0.75, 'in 2015-2016.', color='crimson', fontsize=14)\naxes4.text(0, 0.7, 'Average growth rates by period:', color='crimson', fontsize=14)\naxes4.text(0, 0.65, f'* 1980-1984 - {round(statistics.mean(list_Global[:5]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.6, f'* 1985-1989 - {round(statistics.mean(list_Global[5:10]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.55, f'* 1990-1994 - {round(statistics.mean(list_Global[10:15]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.5, f'* 1995-1999 - {round(statistics.mean(list_Global[15:20]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.45, f'* 2000-2004 - {round(statistics.mean(list_Global[20:25]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.4, f'* 2005-2009 - {round(statistics.mean(list_Global[25:30]),2)}%;', color='black', fontsize=14)\naxes4.text(0, 0.35, f'* 2010-2016 - {round(statistics.mean(list_Global[30:]),2)}%.', color='black', fontsize=14)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:27:05.408884Z","iopub.execute_input":"2021-06-18T18:27:05.409222Z","iopub.status.idle":"2021-06-18T18:27:06.385363Z","shell.execute_reply.started":"2021-06-18T18:27:05.409188Z","shell.execute_reply":"2021-06-18T18:27:06.384016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sprzedaż w regionach z podziałem na typy gier","metadata":{}},{"cell_type":"code","source":"# NA\ngroup_genre_na = data.groupby('Genre')['NA_Sales'].sum().reset_index()\ngroup_genre_na['NA_Sales'] = round(group_genre_na['NA_Sales'],2)\ngroup_genre_na['Percentages'] = group_genre_na['NA_Sales']/sum(group_genre_na['NA_Sales'])*100\ngroup_genre_na['Percentages'] = round(group_genre_na['Percentages'],2)\ngroup_genre_na.sort_values(by='Percentages', ascending=False, inplace=True)\ngroup_genre_na.reset_index(inplace=True)\ngroup_genre_na.drop('index', axis=1, inplace=True)\n\n# EU\ngroup_genre_eu = data.groupby('Genre')['EU_Sales'].sum().reset_index()\ngroup_genre_eu['EU_Sales'] = round(group_genre_eu['EU_Sales'],2)\ngroup_genre_eu['Percentages'] = group_genre_eu['EU_Sales']/sum(group_genre_eu['EU_Sales'])*100\ngroup_genre_eu['Percentages'] = round(group_genre_eu['Percentages'],2)\ngroup_genre_eu.sort_values(by='Percentages', ascending=False, inplace=True)\ngroup_genre_eu.reset_index(inplace=True)\ngroup_genre_eu.drop('index', axis=1, inplace=True)\n\n# JP\ngroup_genre_jp = data.groupby('Genre')['JP_Sales'].sum().reset_index()\ngroup_genre_jp['JP_Sales'] = round(group_genre_jp['JP_Sales'],2)\ngroup_genre_jp['Percentages'] = group_genre_jp['JP_Sales']/sum(group_genre_jp['JP_Sales'])*100\ngroup_genre_jp['Percentages'] = round(group_genre_jp['Percentages'],2)\ngroup_genre_jp.sort_values(by='Percentages', ascending=False, inplace=True)\ngroup_genre_jp.reset_index(inplace=True)\ngroup_genre_jp.drop('index', axis=1, inplace=True)\n\n# Other\ngroup_genre_other = data.groupby('Genre')['Other_Sales'].sum().reset_index()\ngroup_genre_other['Other_Sales'] = round(group_genre_other['Other_Sales'],2)\ngroup_genre_other['Percentages'] = group_genre_other['Other_Sales']/sum(group_genre_other['Other_Sales'])*100\ngroup_genre_other['Percentages'] = round(group_genre_other['Percentages'],2)\ngroup_genre_other.sort_values(by='Percentages', ascending=False, inplace=True)\ngroup_genre_other.reset_index(inplace=True)\ngroup_genre_other.drop('index', axis=1, inplace=True)\n\n# Global\ngroup_genre_global = data.groupby('Genre')['Global_Sales'].sum().reset_index()\ngroup_genre_global['Global_Sales'] = round(group_genre_global['Global_Sales'],2)\ngroup_genre_global['Percentages'] = group_genre_global['Global_Sales']/sum(group_genre_global['Global_Sales'])*100\ngroup_genre_global['Percentages'] = round(group_genre_global['Percentages'],2)\ngroup_genre_global.sort_values(by='Percentages', ascending=False, inplace=True)\ngroup_genre_global.reset_index(inplace=True)\ngroup_genre_global.drop('index', axis=1, inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:27:12.416132Z","iopub.execute_input":"2021-06-18T18:27:12.416448Z","iopub.status.idle":"2021-06-18T18:27:12.456724Z","shell.execute_reply.started":"2021-06-18T18:27:12.416421Z","shell.execute_reply":"2021-06-18T18:27:12.455948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_na=[]\nfor n in range(len(group_genre_na['Genre'])):\n    x = group_genre_na.loc[n,:]\n    list_na.append(x)\ncolor_list=[['whitesmoke', 'white', 'white']]\nfig = plt.figure(facecolor='whitesmoke')\naxes1 = fig.add_axes([0, 0, 1, 1]) \naxes2 = fig.add_axes([1.6, 0, 1, 1]) \n\naxes1.set_axis_off() \ntable=axes1.table(cellColours=color_list*12,cellText = list_na, cellLoc ='left', loc ='upper left', colWidths=[0.3,0.3,0.3],\n                  colLabels=group_genre_na.columns,colColours=['crimson']*3)           \ntable.auto_set_font_size(False) \ntable.set_fontsize(16)  \ntable.scale(1.5, 2.7) \naxes1.text(0.1, 1.15, 'Sales by genre in', color='black', fontsize=20, fontweight='bold')\naxes1.text(0.63, 1.15, 'North America', color='crimson', fontsize=24, fontweight='bold')\n\naxes2.set_facecolor('whitesmoke')\naxes2.axis('off')\naxes2.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes2.text(0, 0.8, 'Top 3 genres in North America:', color='crimson', fontsize=14)\naxes2.text(0, 0.72, f\"* {group_genre_na['Genre'][0]};\", color='black', fontsize=14)\naxes2.text(0, 0.64, f\"* {group_genre_na['Genre'][1]};\", color='black', fontsize=14)\naxes2.text(0, 0.56, f\"* {group_genre_na['Genre'][2]};\", color='black', fontsize=14)\naxes2.text(0, 0.48, f\"Top 3 genres = {sum(group_genre_na['Percentages'][:3])}% of the total sales amount\", color='crimson', fontsize=14)\naxes2.text(0, 0.4, '3 least popular genres in North America', color='crimson', fontsize=14)\naxes2.text(0, 0.32, f\"* {group_genre_na['Genre'][9]};\", color='black', fontsize=14)\naxes2.text(0, 0.24, f\"* {group_genre_na['Genre'][10]};\", color='black', fontsize=14)\naxes2.text(0, 0.16, f\"* {group_genre_na['Genre'][11]};\", color='black', fontsize=14)\naxes2.text(0, 0.08, f\"3 least popular genres = {sum(group_genre_na['Percentages'][9:])}% of the total sales amount\", color='crimson', fontsize=14)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:27:15.563451Z","iopub.execute_input":"2021-06-18T18:27:15.563754Z","iopub.status.idle":"2021-06-18T18:27:16.028349Z","shell.execute_reply.started":"2021-06-18T18:27:15.563727Z","shell.execute_reply":"2021-06-18T18:27:16.027458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_eu=[]\nfor n in range(len(group_genre_eu['Genre'])):\n    x = group_genre_eu.loc[n,:]\n    list_eu.append(x)\ncolor_list=[['whitesmoke', 'white', 'white']]\nfig = plt.figure(facecolor='whitesmoke')\naxes1 = fig.add_axes([0, 0, 1, 1]) \naxes2 = fig.add_axes([1.6, 0, 1, 1]) \n\naxes1.set_axis_off() \ntable=axes1.table(cellColours=color_list*12,cellText = list_eu, cellLoc ='left', loc ='upper left', colWidths=[0.3,0.3,0.3],\n                  colLabels=group_genre_eu.columns,colColours=['crimson']*3)      \ntable.auto_set_font_size(False) \ntable.set_fontsize(16)  \ntable.scale(1.5, 2.7) \naxes1.text(0.1, 1.15, 'Sales by genre in', color='black', fontsize=20, fontweight='bold')\naxes1.text(0.63, 1.15, 'Europe', color='crimson', fontsize=24, fontweight='bold')\n\naxes2.set_facecolor('whitesmoke')\naxes2.axis('off')\naxes2.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes2.text(0, 0.8, 'Top 3 genres in Europe:', color='crimson', fontsize=14)\naxes2.text(0, 0.72, f\"* {group_genre_eu['Genre'][0]};\", color='black', fontsize=14)\naxes2.text(0, 0.64, f\"* {group_genre_eu['Genre'][1]};\", color='black', fontsize=14)\naxes2.text(0, 0.56, f\"* {group_genre_eu['Genre'][2]};\", color='black', fontsize=14)\naxes2.text(0, 0.48, f\"Top 3 genres = {round(sum(group_genre_eu['Percentages'][:3]),2)}% of the total sales amount\", color='crimson', fontsize=14)\naxes2.text(0, 0.4, '3 least popular genres in Europe', color='crimson', fontsize=14)\naxes2.text(0, 0.32, f\"* {group_genre_eu['Genre'][9]};\", color='black', fontsize=14)\naxes2.text(0, 0.24, f\"* {group_genre_eu['Genre'][10]};\", color='black', fontsize=14)\naxes2.text(0, 0.16, f\"* {group_genre_eu['Genre'][11]};\", color='black', fontsize=14)\naxes2.text(0, 0.08, f\"3 least popular genres = {sum(group_genre_eu['Percentages'][9:])}% of the total sales amount\", color='crimson', fontsize=14)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:27:19.671053Z","iopub.execute_input":"2021-06-18T18:27:19.671387Z","iopub.status.idle":"2021-06-18T18:27:20.128923Z","shell.execute_reply.started":"2021-06-18T18:27:19.671355Z","shell.execute_reply":"2021-06-18T18:27:20.127956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_jp=[]\nfor n in range(len(group_genre_jp['Genre'])):\n    x = group_genre_jp.loc[n,:]\n    list_jp.append(x)\ncolor_list=[['whitesmoke', 'white', 'white']]\nfig = plt.figure(facecolor='whitesmoke')\naxes1 = fig.add_axes([0, 0, 1, 1]) \naxes2 = fig.add_axes([1.6, 0, 1, 1]) \n\naxes1.set_axis_off() \ntable=axes1.table(cellColours=color_list*12,cellText = list_jp, cellLoc ='left', loc ='upper left', colWidths=[0.3,0.3,0.3],\n                  colLabels=group_genre_jp.columns,colColours=['crimson']*3)      \ntable.auto_set_font_size(False) \ntable.set_fontsize(16)  \ntable.scale(1.5, 2.7) \naxes1.text(0.1, 1.15, 'Sales by genre in', color='black', fontsize=20, fontweight='bold')\naxes1.text(0.63, 1.15, 'Japan', color='crimson', fontsize=24, fontweight='bold')\n\naxes2.set_facecolor('whitesmoke')\naxes2.axis('off')\naxes2.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes2.text(0, 0.8, 'Top 3 genres in Japan:', color='crimson', fontsize=14)\naxes2.text(0, 0.72, f\"* {group_genre_jp['Genre'][0]};\", color='black', fontsize=14)\naxes2.text(0, 0.64, f\"* {group_genre_jp['Genre'][1]};\", color='black', fontsize=14)\naxes2.text(0, 0.56, f\"* {group_genre_jp['Genre'][2]};\", color='black', fontsize=14)\naxes2.text(0, 0.48, f\"Top 3 genres = {round(sum(group_genre_jp['Percentages'][:3]),2)}% of the total sales amount\", color='crimson', fontsize=14)\naxes2.text(0, 0.4, '3 least popular genres in Japan', color='crimson', fontsize=14)\naxes2.text(0, 0.32, f\"* {group_genre_jp['Genre'][9]};\", color='black', fontsize=14)\naxes2.text(0, 0.24, f\"* {group_genre_jp['Genre'][10]};\", color='black', fontsize=14)\naxes2.text(0, 0.16, f\"* {group_genre_jp['Genre'][11]};\", color='black', fontsize=14)\naxes2.text(0, 0.08, f\"3 least popular genres = {sum(group_genre_jp['Percentages'][9:])}% of the total sales amount\", color='crimson', fontsize=14)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:27:22.558035Z","iopub.execute_input":"2021-06-18T18:27:22.558369Z","iopub.status.idle":"2021-06-18T18:27:23.017671Z","shell.execute_reply.started":"2021-06-18T18:27:22.558336Z","shell.execute_reply":"2021-06-18T18:27:23.01685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_other=[]\nfor n in range(len(group_genre_other['Genre'])):\n    x = group_genre_other.loc[n,:]\n    list_other.append(x)\ncolor_list=[['whitesmoke', 'white', 'white']]\nfig = plt.figure(facecolor='whitesmoke')\naxes1 = fig.add_axes([0, 0, 1, 1]) \naxes2 = fig.add_axes([1.6, 0, 1, 1]) \n\naxes1.set_axis_off() \ntable=axes1.table(cellColours=color_list*12,cellText = list_other, cellLoc ='left', loc ='upper left', colWidths=[0.3,0.3,0.3],\n                  colLabels=group_genre_other.columns,colColours=['crimson']*3)      \ntable.auto_set_font_size(False) \ntable.set_fontsize(16)  \ntable.scale(1.5, 2.7) \naxes1.text(0.1, 1.15, 'Sales by genre in', color='black', fontsize=20, fontweight='bold')\naxes1.text(0.63, 1.15, 'Other countries', color='crimson', fontsize=24, fontweight='bold')\n\naxes2.set_facecolor('whitesmoke')\naxes2.axis('off')\naxes2.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes2.text(0, 0.8, 'Top 3 genres in Other countries:', color='crimson', fontsize=14)\naxes2.text(0, 0.72, f\"* {group_genre_other['Genre'][0]};\", color='black', fontsize=14)\naxes2.text(0, 0.64, f\"* {group_genre_other['Genre'][1]};\", color='black', fontsize=14)\naxes2.text(0, 0.56, f\"* {group_genre_other['Genre'][2]};\", color='black', fontsize=14)\naxes2.text(0, 0.48, f\"Top 3 genres = {round(sum(group_genre_other['Percentages'][:3]),2)}% of the total sales amount\", color='crimson', fontsize=14)\naxes2.text(0, 0.4, '3 least popular genres in Other countries', color='crimson', fontsize=14)\naxes2.text(0, 0.32, f\"* {group_genre_other['Genre'][9]};\", color='black', fontsize=14)\naxes2.text(0, 0.24, f\"* {group_genre_other['Genre'][10]};\", color='black', fontsize=14)\naxes2.text(0, 0.16, f\"* {group_genre_other['Genre'][11]};\", color='black', fontsize=14)\naxes2.text(0, 0.08, f\"3 least popular genres = {sum(group_genre_other['Percentages'][9:])}% of the total sales amount\", color='crimson', fontsize=14)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:27:25.706744Z","iopub.execute_input":"2021-06-18T18:27:25.707072Z","iopub.status.idle":"2021-06-18T18:27:26.167814Z","shell.execute_reply.started":"2021-06-18T18:27:25.707039Z","shell.execute_reply":"2021-06-18T18:27:26.166991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_global=[]\nfor n in range(len(group_genre_global['Genre'])):\n    x = group_genre_global.loc[n,:]\n    list_global.append(x)\ncolor_list=[['whitesmoke', 'white', 'white']]\nfig = plt.figure(facecolor='whitesmoke')\naxes1 = fig.add_axes([0, 0, 1, 1]) \naxes2 = fig.add_axes([1.6, 0, 1, 1]) \n\naxes1.set_axis_off() \ntable=axes1.table(cellColours=color_list*12,cellText = list_global, cellLoc ='left', loc ='upper left', colWidths=[0.3,0.3,0.3],\n                  colLabels=group_genre_global.columns,colColours=['crimson']*3)      \ntable.auto_set_font_size(False) \ntable.set_fontsize(16)  \ntable.scale(1.5, 2.7) \naxes1.text(0.1, 1.15, 'Global sales by genre', color='crimson', fontsize=24, fontweight='bold')\n\naxes2.set_facecolor('whitesmoke')\naxes2.axis('off')\naxes2.text(0.2, 0.9, 'Conclusion', color='crimson', fontsize=24, fontweight='bold')\naxes2.text(0, 0.8, 'Top 3 genres:', color='crimson', fontsize=14)\naxes2.text(0, 0.72, f\"* {group_genre_global['Genre'][0]};\", color='black', fontsize=14)\naxes2.text(0, 0.64, f\"* {group_genre_global['Genre'][1]};\", color='black', fontsize=14)\naxes2.text(0, 0.56, f\"* {group_genre_global['Genre'][2]};\", color='black', fontsize=14)\naxes2.text(0, 0.48, f\"Top 3 genres = {round(sum(group_genre_global['Percentages'][:3]),2)}% of the total sales amount\", color='crimson', fontsize=14)\naxes2.text(0, 0.4, '3 least popular genres', color='crimson', fontsize=14)\naxes2.text(0, 0.32, f\"* {group_genre_global['Genre'][9]};\", color='black', fontsize=14)\naxes2.text(0, 0.24, f\"* {group_genre_global['Genre'][10]};\", color='black', fontsize=14)\naxes2.text(0, 0.16, f\"* {group_genre_global['Genre'][11]};\", color='black', fontsize=14)\naxes2.text(0, 0.08, f\"3 least popular genres = {sum(group_genre_global['Percentages'][9:])}% of the total sales amount\", color='crimson', fontsize=14)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:27:28.481063Z","iopub.execute_input":"2021-06-18T18:27:28.481375Z","iopub.status.idle":"2021-06-18T18:27:28.933013Z","shell.execute_reply.started":"2021-06-18T18:27:28.481347Z","shell.execute_reply":"2021-06-18T18:27:28.93228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Najdrożej sprzedana gra w północnej Ameryce:","metadata":{}},{"cell_type":"code","source":"north_usa_highest_sold_game = data.NA_Sales.max()\ndata[data[\"NA_Sales\"] == north_usa_highest_sold_game][[\"Name\",\"NA_Sales\"]]","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:27:47.1051Z","iopub.execute_input":"2021-06-18T18:27:47.105463Z","iopub.status.idle":"2021-06-18T18:27:47.115946Z","shell.execute_reply.started":"2021-06-18T18:27:47.10543Z","shell.execute_reply":"2021-06-18T18:27:47.1153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Na podstawie tak obszernej i dokładnej analizy zmiennych ilościowych oraz jakościowych, określono trzy hipotezy którym można się dokładniej przyjrzeć w tej pracy. \n\n* **Hipoteza 1:** Znaczną część sprzedaży globalnej stanowi sprzedaż gier na rynek Amerykański\n* **Hipoteza 2:** Sprzedaż gier na PS2 rośnie z każdym rokiem na rynku japońskim\n* **Hipoteza 3:** Dominującym gatunkiem gier w Europie jest gatunek gry akcji","metadata":{}},{"cell_type":"code","source":"sns.histplot(x=data[\"Platform\"],hue=data[\"Year\"])\ndata[[\"Platform\", \"Year\", \"Rank\"]].groupby([\"Platform\", \"Year\"]).count()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:27:50.574235Z","iopub.execute_input":"2021-06-18T18:27:50.574712Z","iopub.status.idle":"2021-06-18T18:27:53.933512Z","shell.execute_reply.started":"2021-06-18T18:27:50.574681Z","shell.execute_reply":"2021-06-18T18:27:53.932746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tabele wielodzielcze dla zmiennych jakościowych dla hipotezy 1. Przez to, że kolumna NA_Sales ma wiele różnych wartości, postanowiłem podzielić ją na przedziały","metadata":{}},{"cell_type":"code","source":"hipo_1 = data.copy()\nhipo_1.NA_Sales = hipo_1.NA_Sales.astype(str)\nhipo_1.NA_Sales.values[data[\"NA_Sales\"].values < 5] = \"<5\"\nhipo_1.NA_Sales.values[(data[\"NA_Sales\"].values < 15) & (data[\"NA_Sales\"].values >= 5)] = \"5<=x<15\"\nhipo_1.NA_Sales.values[(data[\"NA_Sales\"].values < 25) & (data[\"NA_Sales\"].values >= 15)] = \"15<=x<25\"\nhipo_1.NA_Sales.values[(data[\"NA_Sales\"].values <= 45) & (data[\"NA_Sales\"].values >= 25)] = \"25<=x<=45\"\n\nfor col in quali_cols:\n    print(pd.crosstab(hipo_1[col], hipo_1['NA_Sales'], normalize=\"index\"))\n    print(\"\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:27:57.370059Z","iopub.execute_input":"2021-06-18T18:27:57.370376Z","iopub.status.idle":"2021-06-18T18:27:57.563786Z","shell.execute_reply.started":"2021-06-18T18:27:57.370347Z","shell.execute_reply":"2021-06-18T18:27:57.56294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"W powyższych tabelach wielodzielczych widzimy udział w statystykach gier o sprzedaży 25-45 mln egzemplarzy. ","metadata":{}},{"cell_type":"markdown","source":"Wizualizacja wyników: ","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(len(quali_cols), 1, figsize=(10,30))\nfor i, col in enumerate(quali_cols):\n    axes[i].set_title(f\"Wykres histogramu skategoryzowanego {col}\")\n    sns.histplot(data = hipo_1, x=hipo_1[col], hue='NA_Sales', ax=axes[i], multiple=\"dodge\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:28:03.494302Z","iopub.execute_input":"2021-06-18T18:28:03.494612Z","iopub.status.idle":"2021-06-18T18:37:08.221828Z","shell.execute_reply.started":"2021-06-18T18:28:03.494584Z","shell.execute_reply":"2021-06-18T18:37:08.22091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dla Hipotezy 2 stworzono podobne tabele wielodzielcze","metadata":{}},{"cell_type":"code","source":"hipo_2 = data.copy()\nhipo_2.JP_Sales = hipo_1.NA_Sales.astype(str)\nhipo_2.JP_Sales.values[data[\"JP_Sales\"].values < 5] = \"<5\"\nhipo_2.JP_Sales.values[(data[\"JP_Sales\"].values < 15) & (data[\"JP_Sales\"].values >= 5)] = \"5<=x<15\"\nhipo_2.JP_Sales.values[(data[\"JP_Sales\"].values < 25) & (data[\"JP_Sales\"].values >= 15)] = \"15<=x<25\"\nhipo_2.JP_Sales.values[(data[\"JP_Sales\"].values <= 45) & (data[\"JP_Sales\"].values >= 25)] = \"25<=x<=45\"\n\nfor col in quali_cols:\n    print(pd.crosstab(hipo_2[col], hipo_2['JP_Sales'], normalize=\"index\"))\n    print(\"\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:37:08.222957Z","iopub.execute_input":"2021-06-18T18:37:08.223185Z","iopub.status.idle":"2021-06-18T18:37:08.397797Z","shell.execute_reply.started":"2021-06-18T18:37:08.223162Z","shell.execute_reply":"2021-06-18T18:37:08.39722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"W powyższych tabelach widzimy zależność, że z każdym rokiem rośnie liczba sprzedanych gier. Jednak ten trend przestaje obowiązywać po 2010 roku.","metadata":{}},{"cell_type":"markdown","source":"Wizualizacja wyników:","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(len(quali_cols), 1, figsize=(10,30))\nfor i, col in enumerate(quali_cols):\n    axes[i].set_title(f\"Wykres histogramu skategoryzowanego {col}\")\n    sns.histplot(data = hipo_2, x=hipo_2[col], hue='JP_Sales', ax=axes[i], multiple=\"dodge\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:37:08.398905Z","iopub.execute_input":"2021-06-18T18:37:08.399263Z","iopub.status.idle":"2021-06-18T18:45:29.16313Z","shell.execute_reply.started":"2021-06-18T18:37:08.399218Z","shell.execute_reply":"2021-06-18T18:45:29.1623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Oraz podobnie dla hipotezy 3","metadata":{}},{"cell_type":"code","source":"hipo_3 = data.copy()\nhipo_3.EU_Sales = hipo_1.NA_Sales.astype(str)\nhipo_3.EU_Sales.values[data[\"EU_Sales\"].values < 5] = \"<5\"\nhipo_3.EU_Sales.values[(data[\"EU_Sales\"].values < 15) & (data[\"EU_Sales\"].values >= 5)] = \"5<=x<15\"\nhipo_3.EU_Sales.values[(data[\"EU_Sales\"].values < 25) & (data[\"EU_Sales\"].values >= 15)] = \"15<=x<25\"\nhipo_3.EU_Sales.values[(data[\"EU_Sales\"].values <= 45) & (data[\"EU_Sales\"].values >= 25)] = \"25<=x<=45\"\n\nfor col in quali_cols:\n    print(pd.crosstab(hipo_3[col], hipo_3['EU_Sales'], normalize=\"index\"))\n    print(\"\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:45:29.164725Z","iopub.execute_input":"2021-06-18T18:45:29.165212Z","iopub.status.idle":"2021-06-18T18:45:29.347704Z","shell.execute_reply.started":"2021-06-18T18:45:29.165173Z","shell.execute_reply":"2021-06-18T18:45:29.346756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tabele rozdzielcze nie do końca pokazują, że gry akcji stanowią największą część w Europie. Wyprzedzają je m.in gry strzelankowe ze sprzedażą 0.23% egzemplarzy pomiędzy 5 a 15 mln sztuk","metadata":{}},{"cell_type":"markdown","source":"Wizualizacja wyników","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(len(quali_cols), 1, figsize=(10,30))\nfor i, col in enumerate(quali_cols):\n    axes[i].set_title(f\"Wykres histogramu skategoryzowanego {col}\")\n    sns.histplot(data = hipo_3, x=hipo_3[col], hue='EU_Sales', ax=axes[i], multiple=\"dodge\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:45:29.348949Z","iopub.execute_input":"2021-06-18T18:45:29.349311Z","iopub.status.idle":"2021-06-18T18:54:13.657894Z","shell.execute_reply.started":"2021-06-18T18:45:29.349274Z","shell.execute_reply":"2021-06-18T18:54:13.657068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wykonanie macierzy korelacji","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,10))\nsns.heatmap(data.corr(), annot=True, ax=ax)\ndata.corr()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:54:13.659008Z","iopub.execute_input":"2021-06-18T18:54:13.659285Z","iopub.status.idle":"2021-06-18T18:54:14.140792Z","shell.execute_reply.started":"2021-06-18T18:54:13.659257Z","shell.execute_reply":"2021-06-18T18:54:14.140005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"W analizie korelacji możemy wykluczyć pole \"Rank\" gdyż jest to traktowane jako ID. Zatem korelacja występuje pomiędzy:\n* \"Year\" a \"Sales\". Są to słabe korelacje, częściowo ujemne, częściowo pozorne. Jednak pole \"Year\" nie może zostać poddane korelacji gdyż wraz ze wzrostem sprzedaży, rosłaby wartość roku. To jest możliwe ale nie w każdym przypadku a na pewno nie jest to czysto logiczne\n* \"Sales\" a \"Sales\". Pomiędzy wszystkimi wartościami sprzedaży istnieje korelacja dodatnia co jest jak najbardziej sensowne. W momemncie gdy rośnie sprzedaż w którymś regionie świata, automatycznie rośnie sprzedaż w aspekcie całego globu\n\nNajmocniejszą korelacją wykazuje się para \"NA_Sales\" oraz \"Global_Sales\" (0.94) co jest bardzo silną korelacją. Oznacza to największy wpływ na wzrost globalnej sprzedaży artykułów na rynek Ameryki Północnej","metadata":{}},{"cell_type":"markdown","source":"Test niezależności przy użyciu chi^2 przy założeniu poziomu istotności alfa=0.05, zakładając za hipotezę zerową, że zmienne są niezależne","metadata":{}},{"cell_type":"markdown","source":"Test niezależności dla hipotezy 1 (NA_Sales)","metadata":{}},{"cell_type":"code","source":"cols_to_drop = [\"Rank\", \"NA_Sales\"]\nfor col in data.columns:\n    if col not in cols_to_drop:\n        contigency = pd.crosstab(data[col], data[\"NA_Sales\"])\n        chi,p_value,degrees_of_freedom , expected_freq = chi2_contingency(contigency)\n        print(f\"Dla kolumny {col} wartość p testu niezależności wynosi {p_value}\")\n        if p_value <= 0.05:\n            print(f\"Wartość p jest mniejsza dla założonego poziomu istotności co pozwala na odrzucenie hipotezy zerowej - zmienne są zależne\\n\")\n        else:\n            print(f\"Wartość p jest większa dla założonego poziomu istotności co potwierdza hipotezę zerową - zmienne są niezależne\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:54:14.14205Z","iopub.execute_input":"2021-06-18T18:54:14.142415Z","iopub.status.idle":"2021-06-18T18:54:15.994749Z","shell.execute_reply.started":"2021-06-18T18:54:14.142377Z","shell.execute_reply":"2021-06-18T18:54:15.993719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test niezależności dla hipotezy 2 (JP_Sales)","metadata":{}},{"cell_type":"code","source":"cols_to_drop = [\"Rank\", \"JP_Sales\"]\nfor col in data.columns:\n    if col not in cols_to_drop:\n        contigency = pd.crosstab(data[col], data[\"JP_Sales\"])\n        chi,p_value,degrees_of_freedom , expected_freq = chi2_contingency(contigency)\n        print(f\"Dla kolumny {col} wartość p testu niezależności wynosi {p_value}\")\n        if p_value <= 0.05:\n            print(f\"Wartość p jest mniejsza dla założonego poziomu istotności co pozwala na odrzucenie hipotezy zerowej - zmienne są zależne\\n\")\n        else:\n            print(f\"Wartość p jest większa dla założonego poziomu istotności co potwierdza hipotezę zerową - zmienne są niezależne\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:54:15.998124Z","iopub.execute_input":"2021-06-18T18:54:15.998395Z","iopub.status.idle":"2021-06-18T18:54:17.153631Z","shell.execute_reply.started":"2021-06-18T18:54:15.998369Z","shell.execute_reply":"2021-06-18T18:54:17.152318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test niezależności dla hipotezy 3 (EU_Sales)","metadata":{}},{"cell_type":"code","source":"cols_to_drop = [\"Rank\", \"EU_Sales\"]\nfor col in data.columns:\n    if col not in cols_to_drop:\n        contigency = pd.crosstab(data[col], data[\"EU_Sales\"])\n        chi,p_value,degrees_of_freedom , expected_freq = chi2_contingency(contigency)\n        print(f\"Dla kolumny {col} wartość p testu niezależności wynosi {p_value}\")\n        if p_value <= 0.05:\n            print(f\"Wartość p jest mniejsza dla założonego poziomu istotności co pozwala na odrzucenie hipotezy zerowej - zmienne są zależne\\n\")\n        else:\n            print(f\"Wartość p jest większa dla założonego poziomu istotności co potwierdza hipotezę zerową - zmienne są niezależne\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:54:17.155229Z","iopub.execute_input":"2021-06-18T18:54:17.155656Z","iopub.status.idle":"2021-06-18T18:54:18.630826Z","shell.execute_reply.started":"2021-06-18T18:54:17.155619Z","shell.execute_reply":"2021-06-18T18:54:18.629881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wykresy ramka-wąsy dla wszystkich zmiennych ilościowych z hipotez. ","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(len(quant_col),1,  figsize=(10,30))\nfor i, col in enumerate(quant_col):\n    axes[i].set_title(f\"Wykres pudełkowy kolumny {col}\")\n    sns.boxplot(data = data, x=data[col], ax=axes[i], orient = \"h\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:54:18.631907Z","iopub.execute_input":"2021-06-18T18:54:18.632149Z","iopub.status.idle":"2021-06-18T18:54:19.265678Z","shell.execute_reply.started":"2021-06-18T18:54:18.632125Z","shell.execute_reply":"2021-06-18T18:54:19.264779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wykresy pudełkowe pozwalają lepiej zapoznać sie z rozkładem zmiennych w zespole danych oraz dodatkowo pozwalają w łatwy sposób zauważyć wartości odstające w postaci punktów wystających poza wąsy (punkty te znajdują się w zakresach kwantyli 0-25 oraz 75-100). Jednak zawsze przed zakwalifikowaniem jakiś danych do wartości odstających należy się zastanowić, co powinno się z nimi zrobić i czy wpływają one w dużym stopniu negatywnie na modele predykcyjne. W naszym przypadku wykresy pudełkowe nie obrazują rozkładu zmiennych tak dokładnie, jak można było się tego spodziewać. Jest to spowodowane tym, że w każdym rekordzie danych, kolumna \"Sales\" ma inną wartość ilości sprzedanych gier. Poza wartościami 0 wszystkie można uznać jako odstające","metadata":{}},{"cell_type":"markdown","source":"Przeanalizujmy sprzedaż \"NA_Sales\"","metadata":{}},{"cell_type":"code","source":"data[data[\"NA_Sales\"]>=1]","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:54:19.266998Z","iopub.execute_input":"2021-06-18T18:54:19.267369Z","iopub.status.idle":"2021-06-18T18:54:19.29198Z","shell.execute_reply.started":"2021-06-18T18:54:19.267331Z","shell.execute_reply":"2021-06-18T18:54:19.291139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Zastąpmy te 910 rekordów wartościami sprzedaży między 0 a 1 milionem sztuk","metadata":{}},{"cell_type":"code","source":"import random\ndata.loc[data[\"NA_Sales\"] >= 1,\"NA_Sales\"] = data[\"NA_Sales\"].apply(lambda x: random.randrange(0,1))","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:54:19.293212Z","iopub.execute_input":"2021-06-18T18:54:19.293584Z","iopub.status.idle":"2021-06-18T18:54:19.321591Z","shell.execute_reply.started":"2021-06-18T18:54:19.293548Z","shell.execute_reply":"2021-06-18T18:54:19.320728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sprawdźmy jak zmienił się rozkład zmiennych","metadata":{}},{"cell_type":"code","source":"sns.boxplot(data = data, x=data[\"NA_Sales\"], orient = \"h\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:54:19.322516Z","iopub.execute_input":"2021-06-18T18:54:19.322756Z","iopub.status.idle":"2021-06-18T18:54:19.452561Z","shell.execute_reply.started":"2021-06-18T18:54:19.322733Z","shell.execute_reply":"2021-06-18T18:54:19.451928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Podobny zabieg można byłoby wykonać na każdej zmiennej sprzedażowej \"Sales\".","metadata":{}},{"cell_type":"markdown","source":"Do wykonania wykresów pudełkowych skategoryzowanych wybrano pary: Zmienna jakościowa: Year, Zmienna ilościowa: JP_Sales Zmienna jakościowa: Genre, Zmianna ilościowa: EU_Sales","metadata":{}},{"cell_type":"code","source":"sns.boxplot(data = data, x=\"Year\", y = \"JP_Sales\", orient=\"v\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:54:19.453381Z","iopub.execute_input":"2021-06-18T18:54:19.453703Z","iopub.status.idle":"2021-06-18T18:54:20.205562Z","shell.execute_reply.started":"2021-06-18T18:54:19.453679Z","shell.execute_reply":"2021-06-18T18:54:20.204718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(data = data, x=\"Genre\", y = \"EU_Sales\", orient=\"v\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:54:20.20666Z","iopub.execute_input":"2021-06-18T18:54:20.206911Z","iopub.status.idle":"2021-06-18T18:54:20.525715Z","shell.execute_reply.started":"2021-06-18T18:54:20.206885Z","shell.execute_reply":"2021-06-18T18:54:20.524659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test normalny dla zmiennych - wartości odstające","metadata":{}},{"cell_type":"markdown","source":"Na samym początku sporządzono wykresy rozkładu wartości zmiennych ilościowych, aby sprawdzić czy dana analiza jest potrzebna i przydatna do analizy.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(len(quali_cols), 1, figsize=(10,30))\nfor i, col in enumerate(quant_col):\n    axes[i].set_title(f\"Wykres rozkładu liczności dla {col}\")\n    sns.histplot(data[col], ax=axes[i], kde=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T18:54:20.527093Z","iopub.execute_input":"2021-06-18T18:54:20.527478Z","iopub.status.idle":"2021-06-18T18:54:36.458493Z","shell.execute_reply.started":"2021-06-18T18:54:20.52744Z","shell.execute_reply":"2021-06-18T18:54:36.456248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, col in enumerate(quant_col):\n    print(f\"Wartość p dla testu normalnego dla kolumny {col} wynosi {normaltest(data[col].values).pvalue}\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:13:21.993479Z","iopub.execute_input":"2021-06-18T19:13:21.993862Z","iopub.status.idle":"2021-06-18T19:13:22.007436Z","shell.execute_reply.started":"2021-06-18T19:13:21.99383Z","shell.execute_reply":"2021-06-18T19:13:22.006751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Jak widać dla każdej kolumny zmiennej ilościowej nie posiadamy rozkładu normalnego. Jest to cecha tego zbioru danych i nie powinno się zmieniać jego wartości, aby rozkład zmiennych był bliski do rozkładu normalnego. Niestety z tego powodu nie możemy stosować parametrycznych metod statystycznych w celu przewidywania zmiennych zależnych.","metadata":{}},{"cell_type":"markdown","source":"# Indukcja drzew decyzyjnych","metadata":{}},{"cell_type":"markdown","source":"Do sprawdzenia hipotezy pierwszej dotyczącej udziału sprzedanych gier w Północnej Ameryce względem całego globu wybrano zmienne niezależne:\n\nNA_Sales,\nGlobal_Sales\nZmienne zależne wybrano dzięki obserwacjom z podpunktu drugiego. W tym wypadku mamy doczynienia z przewidywania wartości dlatego, należy wykorzystać drzewo regresyjne","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text, DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\n\n# oddzielenie potrzebnych danych\nhipo_1_data = data[[\"Global_Sales\"]]\nhipo_1_y = data[\"NA_Sales\"]\n\n\n# rozdzielenie danych na sety treningowe i testowe\nX_train, X_test, y_train, y_test = train_test_split(hipo_1_data, hipo_1_y, train_size=0.7, random_state=1)\n\n# ładowanie modelu\ntree_model = DecisionTreeRegressor(min_samples_leaf=500)\n\n# trenowanie modelu\ntree_model.fit(X_train, y_train)\n\n# sprawdzanie dokłądności modelu na danych testowych\n\npreds = tree_model.predict(X_test)\nr2score = r2_score(y_test, preds)\nprint(f\"Współczynnik zbieżności wynosi {((1 - r2score) * 100):.2f}% - Dopasowanie modelu jest tym lepsze im bardziej współczynnik zbieżności jest bliżej 0%\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:13:25.665682Z","iopub.execute_input":"2021-06-18T19:13:25.666157Z","iopub.status.idle":"2021-06-18T19:13:25.683892Z","shell.execute_reply.started":"2021-06-18T19:13:25.666116Z","shell.execute_reply":"2021-06-18T19:13:25.682608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Określenie ważności predyktorów z użyciem wykresu.","metadata":{}},{"cell_type":"code","source":"var_importances = tree_model.feature_importances_\nstd = np.std(var_importances,axis=0)\nindices = np.argsort(var_importances)\nplt.figure()\nplt.title(\"Ważność predyktorów\")\nplt.barh(range(hipo_1_data.shape[1]), var_importances[indices],\n       color=\"r\", xerr=std, align=\"center\")\nplt.yticks(range(hipo_1_data.shape[1]), hipo_1_data.columns)\nplt.ylim([-1, hipo_1_data.shape[1]])\nplt.grid(b=True)\nplt.xlim(0, 1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:13:28.650221Z","iopub.execute_input":"2021-06-18T19:13:28.650549Z","iopub.status.idle":"2021-06-18T19:13:28.776673Z","shell.execute_reply.started":"2021-06-18T19:13:28.650521Z","shell.execute_reply":"2021-06-18T19:13:28.775743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Odczytanie i sformalizowanie na podstawie drzewa 3-5 reguł dla najbardziej wyrazistych klas lub dla liści o najmniejszej wariancji","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,20))\nplot_tree(tree_model, filled=True, rounded=True, feature_names=X_train.columns)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:13:31.711882Z","iopub.execute_input":"2021-06-18T19:13:31.712251Z","iopub.status.idle":"2021-06-18T19:13:33.377195Z","shell.execute_reply.started":"2021-06-18T19:13:31.712204Z","shell.execute_reply":"2021-06-18T19:13:33.376233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Zamiast wariancji sklearn wykorzystuje MSE (Mean Square Error) do określenia jakości drzewa. Jak widać prawa część drzewa wygląda całkiem dobrze (MSE w liściach na niskim poziomie ok 0.08) co wskazuje na bardzo dobre dopasowanie średniej wartości w liściach do wartości rzeczywistych (ilości sprzedanych gier w wyrażonych w milionach). Lewa strona drzewa natomiast posiada bardzo niskie wartośći MSE (na poziomie około 0.01) co powoduje, że drzewo jest bardzo mało wiarygodne. Potwierdza to wartośc dopasowania modelu (47.33%) wskazując, że model jest dobrze dopasowany tylko w połowie. Dlatego hipotezę tą można przyjąć tylko dla gier, których nakład wyniósł powyżej 0.08 miliona sprzedanych sztuk. Dla całej populacji należy odrzucić hipotezę.","metadata":{}},{"cell_type":"markdown","source":"Do sprawdzenia hipotezy drugiej dotyczącej sprzedaży gier na rynek japoński w kolejnych latach wybrano następujące zmienne niezależne:\n\nYear, JP_Sales, Publisher\n","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, plot_confusion_matrix, precision_recall_fscore_support\nfrom sklearn import preprocessing\npd.options.mode.chained_assignment = None\n\n# oddzielenie potrzebnych danych\nhipo_2_data = data[[\"Year\", \"Platform\"]]\nhipo_2_y = data[\"JP_Sales\"]\n\ndecode = {\n    \"2600\" : 1,\n    \"3DO\" : 2,\n    \"3DS\" : 3,\n    \"DC\" : 4,\n    \"DS\" : 5,\n    \"GB\" : 6,\n    \"GBA\" : 7,\n    \"GC\" : 8,\n    \"GEN\" : 9,\n    \"GG\" : 10,\n    \"N64\" : 11,\n    \"NES\" : 12,\n    \"NG\" : 13,\n    \"PC\" : 14,\n    \"PCFX\" : 15,\n    \"PS\" : 16,\n    \"PS2\" : 17,\n    \"PS3\" : 18,\n    \"PS4\" : 19,\n    \"PSP\" : 20,\n    \"PSV\" : 21,\n    \"SAT\" : 22,\n    \"SCD\" : 23,\n    \"SNES\" : 24,\n    \"TG16\" : 25,\n    \"WS\" : 26,\n    \"Wii\" : 27,\n    \"WiiU\" : 28,\n    \"X360\" : 29,\n    \"XB\" : 30,\n    \"XOne\" : 31\n}\n\nhipo_2_data.loc[:, \"Platform\"] = hipo_2_data.Platform.map(decode).values\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:13:37.335214Z","iopub.execute_input":"2021-06-18T19:13:37.335666Z","iopub.status.idle":"2021-06-18T19:13:37.34716Z","shell.execute_reply.started":"2021-06-18T19:13:37.335624Z","shell.execute_reply":"2021-06-18T19:13:37.346447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lab_enc = preprocessing.LabelEncoder()\nhipo_2_y = lab_enc.fit_transform(hipo_2_y)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:13:42.035571Z","iopub.execute_input":"2021-06-18T19:13:42.036063Z","iopub.status.idle":"2021-06-18T19:13:42.039995Z","shell.execute_reply.started":"2021-06-18T19:13:42.036016Z","shell.execute_reply":"2021-06-18T19:13:42.039409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rozdzielenie danych na sety treningowe i testowe\nX_train, X_test, y_train, y_test = train_test_split(hipo_2_data, hipo_2_y, train_size=0.7, random_state=1)\nX_train\n\n# ładowanie modelu\ntree_model = DecisionTreeClassifier(min_samples_leaf=200)\n\n# trenowanie modelu\ntree_model.fit(X_train, y_train)\n\n# sprawdzanie dokłądności modelu na danych testowych\n\npreds = tree_model.predict(X_test)\nacc = accuracy_score(y_test, preds)\nprint(f\"Dokładność modelu wynosi {100*acc:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:13:44.183002Z","iopub.execute_input":"2021-06-18T19:13:44.183442Z","iopub.status.idle":"2021-06-18T19:13:44.210792Z","shell.execute_reply.started":"2021-06-18T19:13:44.183396Z","shell.execute_reply":"2021-06-18T19:13:44.208442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Określenie ważności predykatów z użyciem wykresu","metadata":{}},{"cell_type":"code","source":"var_importances = tree_model.feature_importances_\nstd = np.std(var_importances,axis=0)\nindices = np.argsort(var_importances)\nplt.figure()\nplt.title(\"Ważność predyktorów\")\nplt.barh(range(hipo_2_data.shape[1]), var_importances[indices],\n       color=\"r\", xerr=std, align=\"center\")\nplt.yticks(range(hipo_2_data.shape[1]), hipo_2_data.columns)\nplt.ylim([-1, hipo_2_data.shape[1]])\nplt.grid(b=True)\nplt.xlim(0, 1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:13:46.686075Z","iopub.execute_input":"2021-06-18T19:13:46.686483Z","iopub.status.idle":"2021-06-18T19:13:46.822172Z","shell.execute_reply.started":"2021-06-18T19:13:46.686448Z","shell.execute_reply":"2021-06-18T19:13:46.82126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(25,20))\nplot_tree(tree_model, filled=True, rounded=True, feature_names=X_train.columns)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:13:49.53694Z","iopub.execute_input":"2021-06-18T19:13:49.537306Z","iopub.status.idle":"2021-06-18T19:13:59.880387Z","shell.execute_reply.started":"2021-06-18T19:13:49.537273Z","shell.execute_reply":"2021-06-18T19:13:59.87964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ze względu na ilość platform dostępnych w datasecie oraz ilość lat, w których gry były sprzedawane, powyższy wykres nie pozwala na odczytanie interesujących nas wartości. Po kolorach możemy tylko stwierdzić, że w prawej części drzewa jest dobre dopasowanie modelu. Jednak całościowo ma się to odpowiednio z obliczonym (63.45%) dokładnością modelu - drzewo też tylko w nieco większej połowie jest zakolorowane \"dobrymi\" kolorami.  ","metadata":{}},{"cell_type":"markdown","source":"Do sprawdzenia hipotezy trzeciej dotyczącej gier akcji na rynku europejskim wybrano następujące zmienne niezależne:\n\nGenre","metadata":{}},{"cell_type":"code","source":"\n# oddzielenie potrzebnych danych\nhipo_3_data = data[[\"Genre\"]]\nhipo_3_y = data[\"EU_Sales\"]\n\ndecode = {\n    \"Action\" : 1,\n    \"Adventure\" : 2,\n    \"Fighting\" : 3,\n    \"Misc\" : 4,\n    \"Platform\" : 5,\n    \"Puzzle\" : 6,\n    \"Racing\" : 7,\n    \"Role-Playing\" : 8,\n    \"Shooter\" : 9,\n    \"Simulation\" : 10,\n    \"Sports\" : 11,\n    \"Strategy\" : 12\n}\n\nhipo_3_data.loc[:, \"Genre\"] = hipo_3_data.Genre.map(decode).values","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:14:05.049859Z","iopub.execute_input":"2021-06-18T19:14:05.050371Z","iopub.status.idle":"2021-06-18T19:14:05.059963Z","shell.execute_reply.started":"2021-06-18T19:14:05.050325Z","shell.execute_reply":"2021-06-18T19:14:05.058975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lab_enc = preprocessing.LabelEncoder()\nhipo_3_y = lab_enc.fit_transform(hipo_3_y)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:14:08.012499Z","iopub.execute_input":"2021-06-18T19:14:08.012808Z","iopub.status.idle":"2021-06-18T19:14:08.017978Z","shell.execute_reply.started":"2021-06-18T19:14:08.01278Z","shell.execute_reply":"2021-06-18T19:14:08.017113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rozdzielenie danych na sety treningowe i testowe\nX_train, X_test, y_train, y_test = train_test_split(hipo_3_data, hipo_3_y, train_size=0.7, random_state=1)\n\n# ładowanie modelu\ntree_model = DecisionTreeClassifier(min_samples_leaf=500)\n\n# trenowanie modelu\ntree_model.fit(X_train, y_train)\n\n# sprawdzanie dokłądności modelu na danych testowych\n\npreds = tree_model.predict(X_test)\nacc = accuracy_score(y_test, preds)\nprint(f\"Dokładność modelu wynosi {100*acc:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:14:10.314347Z","iopub.execute_input":"2021-06-18T19:14:10.314672Z","iopub.status.idle":"2021-06-18T19:14:10.336602Z","shell.execute_reply.started":"2021-06-18T19:14:10.314642Z","shell.execute_reply":"2021-06-18T19:14:10.335367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Określenie ważności predykatów","metadata":{}},{"cell_type":"code","source":"var_importances = tree_model.feature_importances_\nstd = np.std(var_importances,axis=0)\nindices = np.argsort(var_importances)\nplt.figure()\nplt.title(\"Ważność predyktorów\")\nplt.barh(range(hipo_3_data.shape[1]), var_importances[indices],\n       color=\"r\", xerr=std, align=\"center\")\nplt.yticks(range(hipo_3_data.shape[1]), hipo_3_data.columns)\nplt.ylim([-1, hipo_3_data.shape[1]])\nplt.grid(b=True)\nplt.xlim(0, 1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:14:13.061843Z","iopub.execute_input":"2021-06-18T19:14:13.06257Z","iopub.status.idle":"2021-06-18T19:14:13.220385Z","shell.execute_reply.started":"2021-06-18T19:14:13.062519Z","shell.execute_reply":"2021-06-18T19:14:13.21936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(25,20))\nplot_tree(tree_model, filled=True, rounded=True, feature_names=X_train.columns, class_names=[\"1\",\"2\", \"3\", \"4\", \"5\", \"6\",\"7\", \"8\", \"9\", \"10\", \"11\", \"12\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:14:16.795419Z","iopub.execute_input":"2021-06-18T19:14:16.795734Z","iopub.status.idle":"2021-06-18T19:14:19.346106Z","shell.execute_reply.started":"2021-06-18T19:14:16.795706Z","shell.execute_reply":"2021-06-18T19:14:19.345156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analiza skupień","metadata":{}},{"cell_type":"markdown","source":"Do wykonania analizy skupień wybrano zmienne:\n\n* Genre\n* Year\n* Global_Sales\n\nCelem analizy jest sprawdzenie, czy jesteśmy w stanie wyznaczyć jakieś konkretne grupy gier kupowanych przez konsumentów w zależności od roku.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\npd.options.mode.chained_assignment = None\nfrom sklearn.decomposition import PCA\n\ndecode = {\n    \"Action\" : 1,\n    \"Adventure\" : 2,\n    \"Fighting\" : 3,\n    \"Misc\" : 4,\n    \"Platform\" : 5,\n    \"Puzzle\" : 6,\n    \"Racing\" : 7,\n    \"Role-Playing\" : 8,\n    \"Shooter\" : 9,\n    \"Simulation\" : 10,\n    \"Sports\" : 11,\n    \"Strategy\" : 12\n}\n\n# oddzielenie potrzebnych danych\ncluster_data = data[[\"Genre\", \"Year\", \"Global_Sales\"]]\ncluster_data.loc[:, \"Genre\"] = cluster_data.Genre.map(decode).values\n\nsse = []\n\n# ładowanie modelu\nfor k in range(1,11):\n    cluster_model = KMeans(n_clusters=k)\n    cluster_model.fit(cluster_data)\n    sse.append(cluster_model.inertia_)\n    \n# sprawdzanie jaką wartość k wybrać - metoda łokcia\n\nplt.style.use(\"fivethirtyeight\")\nplt.plot(range(1, 11), sse)\nplt.xticks(range(1, 11))\nplt.xlabel(\"Wartość k\")\nplt.ylabel(\"SSE\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:16:27.490082Z","iopub.execute_input":"2021-06-18T19:16:27.490425Z","iopub.status.idle":"2021-06-18T19:16:44.229697Z","shell.execute_reply.started":"2021-06-18T19:16:27.490394Z","shell.execute_reply":"2021-06-18T19:16:44.228612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Na powyższym wykresie  nie jest oczywista najrozsądniejsza ilość klastrów jaką powinno się wybrać. Podejrzewamy wartość 2 lub 3 lecz do automatycznego doboru ilości klastrów można wykorzystać silhouette coefficient score.\n\nSilhouette Coefficient jest obliczany przy użyciu średniej odległości wewnątrz klastra (a) i średniej odległości do najbliższego klastra (b) dla każdej próbki. Współczynnik sylwetki dla próbki to (b - a) / max (a, b). Aby wyjaśnić, b to odległość między próbką a najbliższą gromadą, której próbka nie jest częścią. Zwróć uwagę, że silhouette coefficient jest definiowany tylko wtedy, gdy liczba etykiet wynosi 2 <= n_labels <= n_samples -","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import silhouette_score\n\nsil_score_max = -1\nbest_n_clusters = 0\n\nfor k in range(2,11):\n  cluster_model = KMeans(n_clusters = k)\n  labels = cluster_model.fit_predict(cluster_data)\n  sil_score = silhouette_score(cluster_data, labels)\n  print(f\"Średnia wartość silhouette score dla {k} klastrów wynosi {sil_score}\")\n  if sil_score > sil_score_max:\n    sil_score_max = sil_score\n    best_n_clusters = k\n    \nprint(f\"Najlepsza ilość klastrów to: {best_n_clusters}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:16:52.081875Z","iopub.execute_input":"2021-06-18T19:16:52.082211Z","iopub.status.idle":"2021-06-18T19:17:47.144407Z","shell.execute_reply.started":"2021-06-18T19:16:52.082178Z","shell.execute_reply":"2021-06-18T19:17:47.14333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster_model = KMeans(n_clusters = best_n_clusters)\n# trenowanie modelu\nresult = cluster_model.fit_predict(cluster_data)\n\nlabels = cluster_model.labels_\n\nresult_data = cluster_data.copy()\nresult_data[\"labels\"] = labels\n\nresults_0 = cluster_data[result_data.labels == 0]\nresults_1 = cluster_data[result_data.labels == 1]","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:17:47.146049Z","iopub.execute_input":"2021-06-18T19:17:47.146476Z","iopub.status.idle":"2021-06-18T19:17:48.431131Z","shell.execute_reply.started":"2021-06-18T19:17:47.14643Z","shell.execute_reply":"2021-06-18T19:17:48.4302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analiza klastra 1:","metadata":{}},{"cell_type":"code","source":"results_0.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:17:48.432954Z","iopub.execute_input":"2021-06-18T19:17:48.433232Z","iopub.status.idle":"2021-06-18T19:17:48.457192Z","shell.execute_reply.started":"2021-06-18T19:17:48.433206Z","shell.execute_reply":"2021-06-18T19:17:48.456263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analiza klastra 2:","metadata":{}},{"cell_type":"code","source":"results_1.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:17:48.458476Z","iopub.execute_input":"2021-06-18T19:17:48.458722Z","iopub.status.idle":"2021-06-18T19:17:48.479076Z","shell.execute_reply.started":"2021-06-18T19:17:48.458698Z","shell.execute_reply":"2021-06-18T19:17:48.478154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Z powodu jednolitego rozkładu zmiennych badanie skupień nie przynosi dodatkowych informacji, ponieważ metody typu najniższych sąsiadów dzielą te zbiory na podobne klatry o podobnym wyglądzie. Jedyna różnica pomiędzy klastrami jest w sprzedaży globalnej, gdzie pierwszy klaster skupia w sobie gry, których sprzedano więcej, a drugi klaster te gry, których sprzedano mniej. Patrząc na typ gry, bądź rok publikacji, to nie ma ona większego wpływu na grupy gier.","metadata":{}},{"cell_type":"markdown","source":"Analiza skupień metodą EM","metadata":{}},{"cell_type":"code","source":"# załadowanie modułu do analizy metodą EM\nfrom sklearn.mixture import GaussianMixture\n\n# wczytanie modelu z dwoma klastrami\nem_model = GaussianMixture(n_components=2, random_state=0)\n\nlabels = em_model.fit_predict(cluster_data)\n\n#labels = em_model.labels_\n\nresult_data = cluster_data.copy()\nresult_data[\"labels\"] = labels\n\nresults_0 = cluster_data[result_data.labels == 0]\nresults_1 = cluster_data[result_data.labels == 1]","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:17:48.480438Z","iopub.execute_input":"2021-06-18T19:17:48.480779Z","iopub.status.idle":"2021-06-18T19:17:48.689017Z","shell.execute_reply.started":"2021-06-18T19:17:48.480749Z","shell.execute_reply":"2021-06-18T19:17:48.687921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analiza klastra 1","metadata":{}},{"cell_type":"code","source":"results_0.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:17:48.690144Z","iopub.execute_input":"2021-06-18T19:17:48.690446Z","iopub.status.idle":"2021-06-18T19:17:48.732207Z","shell.execute_reply.started":"2021-06-18T19:17:48.69042Z","shell.execute_reply":"2021-06-18T19:17:48.731125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analiza klastra 2","metadata":{}},{"cell_type":"code","source":"results_1.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:17:48.733847Z","iopub.execute_input":"2021-06-18T19:17:48.73447Z","iopub.status.idle":"2021-06-18T19:17:48.766947Z","shell.execute_reply.started":"2021-06-18T19:17:48.734422Z","shell.execute_reply":"2021-06-18T19:17:48.765533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Porównując wyniki pomiędzy analizą skupień metodą K-najbliższych sąsiadów a metodą EM, można zauważyć niewielkie różnice. Analiza metodą EM podzieliła zbiór danych głównie ze względu na globalną sprzedaż (dla klastra 1 gry sprzedane między 0.01 a 0.73 miliona sztuk, a dla klastra 2 od 0.02 do 82.74 miliona sztuk), ale w odróżnieniu od K-najbliższych sąsiadów wzięła również pod uwagę rok publikacji, gdzie klaster 2 jest nieco bardziej przesunięty w prawą stronę i średnia wartość wynosi dla niego 2004 rok sprzedaży, natomiast dla klastra 1 wynosi 2006 rok.","metadata":{}},{"cell_type":"markdown","source":"Wykres dla każdej zmiennej","metadata":{}},{"cell_type":"code","source":"import scipy.stats as stats\nimport math\n\nfor col in results_1:\n    mu_0 = results_0[col].mean()\n    variance_0 = results_0[col].var()\n    mu_1 = results_1[col].mean()\n    variance_1 = results_1[col].var()\n    sigma_0 = math.sqrt(variance_0)\n    sigma_1 = math.sqrt(variance_1)\n    x_0 = np.linspace(mu_0 - 3*sigma_0, mu_0 + 3*sigma_0, 100)\n    x_1 = np.linspace(mu_1 - 3*sigma_1, mu_1 + 3*sigma_1, 100)\n    plt.figure(figsize=(8,5))\n    plt.title(f\"Wykres dystrybucji zmiennych w klastrach dla zmiennej {col}\")\n    plt.plot(x_0, stats.norm.pdf(x_0, mu_0, sigma_0))\n    plt.plot(x_1, stats.norm.pdf(x_1, mu_1, sigma_1))\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T19:47:46.02849Z","iopub.execute_input":"2021-06-18T19:47:46.028874Z","iopub.status.idle":"2021-06-18T19:47:46.543538Z","shell.execute_reply.started":"2021-06-18T19:47:46.028844Z","shell.execute_reply":"2021-06-18T19:47:46.542431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Algorytm Data Mining","metadata":{}},{"cell_type":"markdown","source":"Jako dodatkowy algorytm wybrano sieci neuronowe implementowane przy pomocy biblioteki tensorflow. Model ten został wybrany, ponieważ sieci neuronowe są szeroko wykorzystywane w data science.","metadata":{}},{"cell_type":"code","source":"# zapisywanie danych w postaci tensora\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nhipo_1_data = data[[\"Year\", \"Genre\"]]\nhipo_1_y = data[\"Global_Sales\"]\n\ndecode = {\n    \"Action\" : 1,\n    \"Adventure\" : 2,\n    \"Fighting\" : 3,\n    \"Misc\" : 4,\n    \"Platform\" : 5,\n    \"Puzzle\" : 6,\n    \"Racing\" : 7,\n    \"Role-Playing\" : 8,\n    \"Shooter\" : 9,\n    \"Simulation\" : 10,\n    \"Sports\" : 11,\n    \"Strategy\" : 12\n}\n\nhipo_1_data.loc[:, \"Genre\"] = hipo_1_data.Genre.map(decode).values\n\n# rozdzielenie danych na sety treningowe i testowe\nX_train, X_test, y_train, y_test = train_test_split(hipo_1_data, hipo_1_y, train_size=0.7, random_state=1)\n\nmodel = keras.models.Sequential([\n    layers.Input(shape=(X_train.shape[1],)),\n    layers.Dense(10, activation=\"relu\"),\n    layers.Dropout(0.2),\n    layers.Dense(10, activation=\"relu\"),\n    layers.Dense(100, activation=\"softmax\")\n])\n\nmodel.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",\n             metrics=[\"accuracy\"])\n\nmodel.build()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T20:56:03.457436Z","iopub.execute_input":"2021-06-18T20:56:03.4578Z","iopub.status.idle":"2021-06-18T20:56:03.511808Z","shell.execute_reply.started":"2021-06-18T20:56:03.45777Z","shell.execute_reply":"2021-06-18T20:56:03.510733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model jaki zbudowałem składa się z 5 warstw:\n\n* Warstwa wejściowa - składa się z 3 neuronów (po jednym na każdą zmienną)\n* Pierwsza warstwa wewnętrzna - składa się z 10 neuronów, każdy połączony z neuronami z warstwy wejściowej. Zastosowano w nim funkcję aktywacyjną relu. Neuron z funkcją aktywacji ReLU przyjmuje dowolne wartości rzeczywiste jako swoje wejście (a), ale aktywuje się tylko wtedy, gdy te wejście (a) są większe niż 0.\n* Druga warstwa wewnętrzna - warstwa przejściowa, która losowo ustawia jednostki wejściowe na 0 z częstotliwością na każdym kroku podczas treningu, co pomaga zapobiegać nadmiernemu dopasowaniu. Wejścia nie ustawione na 0 są skalowane w górę o 1 / (1 - stawka) tak, że suma wszystkich wejść pozostaje niezmieniona.\n* Trzecia warstwa wewnętrzna - składa się z 10 neuronów, który każdy połączony jest z neuronem drugiej warstwy wewnętrznej. W warstwie tej zastosowaną funkcję aktywacyjną relu.\n* Warstwa wyjściowa - składa się z dwóch neuronów, który każdy jest połączony z trzecią warstwą wewnętrzną. Zastosowano tutaj funkcję aktywacyjną softmax, która przetwarza wartości na końcu neuronów i zamienia je na wartości między 0 a 1\n\nDodatkowo sieć została zbudowana z określeniem optymalizatora Adam, który implementuje wykładniczą średnią ruchomą gradientów, aby skalować tempo uczenia. Utrzymuje wykładniczo malejącą średnią poprzednich gradientów. Adam jest wydajny obliczeniowo i ma bardzo małe wymagania dotyczące pamięci. Adam Optimizer jest jednym z najpopularniejszych algorytmów optymalizacji zstępowania gradientu. Oprócz optymalizatora jako funkcję kosztów wybrano sparse_categorical_crossentropy, a do pomiarów jakości sieci wyprano parametr accuracy.","metadata":{}},{"cell_type":"code","source":"#trenowanie modelu\nhistory = model.fit(X_train, y_train, validation_split=0.33, epochs=20)\n# summarize history for accuracy\nplt.figure(figsize=(8,5))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Dokładność modelu')\nplt.ylabel('Dokładność')\nplt.xlabel('Epoka')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.figure(figsize=(8,5))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Funkcja kosztów modelu')\nplt.ylabel('Strata')\nplt.xlabel('Epoka')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T20:56:20.712985Z","iopub.execute_input":"2021-06-18T20:56:20.713463Z","iopub.status.idle":"2021-06-18T20:56:30.05235Z","shell.execute_reply.started":"2021-06-18T20:56:20.713431Z","shell.execute_reply":"2021-06-18T20:56:30.051371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Standaryzacja cech","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing","metadata":{"execution":{"iopub.status.busy":"2021-06-18T20:53:15.972101Z","iopub.execute_input":"2021-06-18T20:53:15.972584Z","iopub.status.idle":"2021-06-18T20:53:15.976138Z","shell.execute_reply.started":"2021-06-18T20:53:15.97255Z","shell.execute_reply":"2021-06-18T20:53:15.975458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = preprocessing.StandardScaler()\nX_train_scal = scaler.fit_transform(X_train)\nX_test_scal = scaler.transform(X_test)\n\n#trenowanie modelu\nhistory = model.fit(X_train, y_train, validation_split=0.33, epochs=20)\n# summarize history for accuracy\nplt.figure(figsize=(8,5))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Dokładność modelu')\nplt.ylabel('Dokładność')\nplt.xlabel('Epoka')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.figure(figsize=(8,5))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Funkcja kosztów modelu')\nplt.ylabel('Strata')\nplt.xlabel('Epoka')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T20:56:49.973085Z","iopub.execute_input":"2021-06-18T20:56:49.973657Z","iopub.status.idle":"2021-06-18T20:56:59.687312Z","shell.execute_reply.started":"2021-06-18T20:56:49.973619Z","shell.execute_reply":"2021-06-18T20:56:59.686329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T21:05:50.448032Z","iopub.execute_input":"2021-06-18T21:05:50.448388Z","iopub.status.idle":"2021-06-18T21:05:50.629942Z","shell.execute_reply.started":"2021-06-18T21:05:50.448359Z","shell.execute_reply":"2021-06-18T21:05:50.628774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Porównując modele drzewa decyzyjnego i sieci neuronowej w celu potwierdzenia hipotezy 1 model drzewa sprawdzał się dużo lepiej niż sieć neuronowa nie (różnica o 10%). Oznacza to, że nie zawsze skomplikowane rozwiązania są dobre dla każdego rozwiązania. W sytuacji, kiedy jest niewielka korelacja zmiennych, a rozkłady nie są normalne modele nieparametryczne mogą sprawdzać się lepiej niż modele parametryczne. Jednakże hipotezę można potwierdzić przy wykorzystaniu modelu sieci neuronowej","metadata":{}}]}