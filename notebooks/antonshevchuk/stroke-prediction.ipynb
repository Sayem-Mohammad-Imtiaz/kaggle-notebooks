{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Intro\n\nSo my first notebook, learning through doing.\nPlease, point out my mistakes and stupidity in comments, i do apprecciate it."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\ndf.head()\ndf.drop(['id'], axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.dpi'] = 100\nfig, (ax4, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4))\nsize = 0.4\n\n\ncmap = plt.get_cmap(\"tab20c\")\nouter_colors = cmap(np.arange(3)*4)\n\n\n#Stroke married\nax4.pie(df[df['ever_married']=='Yes']['stroke'].value_counts(), radius=1, colors=outer_colors,\n       wedgeprops=dict(width=size, edgecolor='w'), autopct='%1.1f%%', pctdistance=0.75)\nax4.pie(df[df['ever_married']=='No']['stroke'].value_counts(), radius=1-size, colors=outer_colors,\n       wedgeprops=dict(width=size, edgecolor='w'), autopct='%1.1f%%', pctdistance=0.75)\nax4.set(aspect=\"equal\", title='Married\\nOuter - 1, Inner -0')\n\n\n\n\n#Overall\nax2.text(0, 0, f'{df[\"stroke\"].count()} cases', color='black', va=\"center\", ha=\"center\")\nax2.pie(df['stroke'].value_counts(), radius=1, colors=outer_colors,\n       wedgeprops=dict(width=size, edgecolor='w'), autopct='%1.1f%%', pctdistance=0.75)\nax2.set(aspect=\"equal\", title='Stroke overall')\n\n\n\n#Hypertension\nax3.pie(df[df['hypertension']==1]['stroke'].value_counts(), radius=1, colors=outer_colors,\n       wedgeprops=dict(width=size, edgecolor='w'), autopct='%1.1f%%', pctdistance=0.75)\nax3.pie(df[df['hypertension']==0]['stroke'].value_counts(), radius=1-size, colors=outer_colors,\n       wedgeprops=dict(width=size, edgecolor='w'), autopct='%1.1f%%', pctdistance=0.75)\nax3.set(aspect=\"equal\", title='Hypertension\\nOuter - 1, Inner -0')\n\n\nfig.legend(['Stroke', 'No stroke'], loc='upper right')\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We've got a very imbalanced data. Let's make a model."},{"metadata":{},"cell_type":"markdown","source":"### Missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Options:\n1. Drop NaN values \n2. Drop whole column\n\nBMI feature doesn't seem to make some influence on stroke possibility. I decided to drop whole column."},{"metadata":{"trusted":true},"cell_type":"code","source":"#df.drop('bmi', axis=1, inplace=True)\ndf['bmi'].fillna(df['bmi'].mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Separate Data into numerical and categorical columns\ncat_cols = df.select_dtypes(['object', 'int']).columns\nnum_cols = df.select_dtypes(['float']).columns\n\nprint(f'Categorical columns: {cat_cols.values}. \\n Numerical columns: {num_cols.values}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Gender uniques\ndf['gender'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Just one value, dropped\ndf.drop(df[df['gender'] == 'Other'].index, axis=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Map binary categories to numerical\ndf['Residence_type'] = df['Residence_type'].map({'Rural': 0, 'Urban': 1})\ndf['ever_married'] = df['ever_married'].map({'No': 0, 'Yes': 1})\ndf['gender'] = df['gender'].map({'Male': 0, 'Female': 1})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2 multinominal categories left:\n* smoking_status - label encoding (Unknown = never smoked)\n* work_type - OHE"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['smoking_status'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['smoking_status'] = df['smoking_status'].map({'Unknown': 0, 'never smoked': 0, 'formerly smoked': 1, 'smokes': 2})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ohe = OneHotEncoder()\n\n#I used index=df.index, cuz I deleted row before and OHE indexing dont match with df.\n\ndf = pd.concat([df, pd.DataFrame(ohe.fit_transform(df[['work_type']]).toarray(), columns=ohe.categories_[0], index=df.index)], \n          axis=1)\ndf.drop('work_type', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take target column away and split X\ny = df['stroke']\nX = df.drop('stroke', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scaling\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import accuracy_score, auc, precision_score, recall_score\nimport datetime","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simple Models with default hyper-parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [\n    {'name': 'LogisticRegression', 'estimator': LogisticRegression(random_state=42)},\n    {'name': 'GradientBoostingClassifier', 'estimator': GradientBoostingClassifier(random_state=42)},\n    {'name': 'AdaBoostClassifier', 'estimator': AdaBoostClassifier(random_state=42)},\n    {'name': 'BaggingClassifier', 'estimator': BaggingClassifier(random_state=42)},\n    {'name': 'XGBClassifier', 'estimator': XGBClassifier(verbosity=0, random_state=42)},\n    {'name': 'DecisionTreeClassifier', 'estimator': DecisionTreeClassifier(random_state=42)},\n    \n]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Name           \\t Time          \\t Recall      ')\nprint('--------------------------------------------------------')\nfor model in models:\n    time_a = datetime.datetime.now()\n    \n    model['estimator'].fit(X_train, y_train)\n    model['preds'] =  model['estimator'].predict(X_test)\n    model['recall'] = recall_score(y_test, model['preds'], average='binary', pos_label=1)\n    \n    time_b = datetime.datetime.now()\n    print(f\"{model['name']}\\t {time_b-time_a}\\t {round(model['recall'],4)}\\t\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best estimator gives us 0.1348 recall. Test target proportion:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.sum() / y_test.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# WELL.....\n\n<img src= \"https://i.kym-cdn.com/entries/icons/original/000/028/021/work.jpg\">"},{"metadata":{},"cell_type":"markdown","source":"Data is heavy imbalanced, Ill try to improve results in next kernel versions. Stay in touch.\n\nBut I got one insight:\n\n *Dont get married if you dont want a stroke (6ix-time risk increased... yeah, let this dataset represenet the whole population*\n \n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}