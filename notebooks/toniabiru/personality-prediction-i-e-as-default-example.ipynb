{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This script only contains the code to train a model to predict (Extraversion or Intraversion) for an individual. \n\n### Similar code with slight changes can be used to predict (Sensing or Intuition), (Thinking or Feeling ) and (Judging or Perceiving)\n\n### This script was as a result of a collaboration with two other individuals as well","metadata":{}},{"cell_type":"code","source":"# Data Analysis\nimport pandas as pd\nimport numpy as np\nfrom numpy import asarray\nfrom numpy import savetxt\nfrom numpy import loadtxt\nimport pickle as pkl\nfrom scipy import sparse\n\n# Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n#Plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# Text Processing\nimport re\nimport itertools\nimport string\nimport collections\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\nimport nltk\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n\nimport spacy \n\n# Machine Learning packages\nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nimport sklearn.cluster as cluster\nfrom sklearn.manifold import TSNE\n\n# Model training and evaluation\nfrom sklearn.model_selection import train_test_split\n\n#Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\n\n#Metrics\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score, balanced_accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score, multilabel_confusion_matrix, confusion_matrix\nfrom sklearn.metrics import classification_report\n\n# Ignore noise warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:59:21.87968Z","iopub.execute_input":"2021-06-22T19:59:21.880136Z","iopub.status.idle":"2021-06-22T19:59:21.893357Z","shell.execute_reply.started":"2021-06-22T19:59:21.880097Z","shell.execute_reply":"2021-06-22T19:59:21.892534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the dataset\n\ndf = pd.read_csv('../input/original-mbti-data/mbti_1.csv')\nprint(df.shape, df.columns.to_list(),'\\n')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:59:25.937773Z","iopub.execute_input":"2021-06-22T19:59:25.938283Z","iopub.status.idle":"2021-06-22T19:59:26.595124Z","shell.execute_reply.started":"2021-06-22T19:59:25.938251Z","shell.execute_reply":"2021-06-22T19:59:26.594061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To install if necessary\n\n#conda install -c conda-forge wordcloud=1.6.0 \n#conda install -c conda-forge xgboost\n#pip install xgboost","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:44:38.894633Z","iopub.execute_input":"2021-06-22T19:44:38.894924Z","iopub.status.idle":"2021-06-22T19:44:38.899352Z","shell.execute_reply.started":"2021-06-22T19:44:38.894898Z","shell.execute_reply":"2021-06-22T19:44:38.898187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Exploratory Analysis ","metadata":{}},{"cell_type":"code","source":"#Check for any null values\n\ndf.isnull().any()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:59:40.628737Z","iopub.execute_input":"2021-06-22T19:59:40.629235Z","iopub.status.idle":"2021-06-22T19:59:40.642158Z","shell.execute_reply.started":"2021-06-22T19:59:40.629197Z","shell.execute_reply":"2021-06-22T19:59:40.640891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Size of the dataset\n\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:59:42.08964Z","iopub.execute_input":"2021-06-22T19:59:42.089997Z","iopub.status.idle":"2021-06-22T19:59:42.097051Z","shell.execute_reply.started":"2021-06-22T19:59:42.08997Z","shell.execute_reply":"2021-06-22T19:59:42.095497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:59:43.428319Z","iopub.execute_input":"2021-06-22T19:59:43.428714Z","iopub.status.idle":"2021-06-22T19:59:43.436409Z","shell.execute_reply.started":"2021-06-22T19:59:43.428681Z","shell.execute_reply":"2021-06-22T19:59:43.435592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:59:44.818196Z","iopub.execute_input":"2021-06-22T19:59:44.8189Z","iopub.status.idle":"2021-06-22T19:59:44.838804Z","shell.execute_reply.started":"2021-06-22T19:59:44.81886Z","shell.execute_reply":"2021-06-22T19:59:44.837231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* There are only 2 columns in the dataset\n* Total no. of rows are 8675\n* There are no null values present in the dataset\n* One Disadvantage is that all values are textual, hence they have to be converted to numerical form to train the ML model","metadata":{}},{"cell_type":"code","source":"df.describe(include=['object'])","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:59:46.787753Z","iopub.execute_input":"2021-06-22T19:59:46.788182Z","iopub.status.idle":"2021-06-22T19:59:46.869985Z","shell.execute_reply.started":"2021-06-22T19:59:46.788145Z","shell.execute_reply":"2021-06-22T19:59:46.868565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* There are 16 unique personality type indicators in the dataset\n* INFP is the most frequently occuring personality type in our dataset (no. of occurences is 1832)\n\n* Lastly, there are no repeating posts in the dataset","metadata":{}},{"cell_type":"code","source":"types = np.unique(np.array(df['ptype']))\ntypes","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:59:48.802511Z","iopub.execute_input":"2021-06-22T19:59:48.802905Z","iopub.status.idle":"2021-06-22T19:59:48.817143Z","shell.execute_reply.started":"2021-06-22T19:59:48.802874Z","shell.execute_reply":"2021-06-22T19:59:48.815902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total = df.groupby(['ptype']).count()\ntotal","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:59:49.725157Z","iopub.execute_input":"2021-06-22T19:59:49.725577Z","iopub.status.idle":"2021-06-22T19:59:49.743759Z","shell.execute_reply.started":"2021-06-22T19:59:49.725539Z","shell.execute_reply":"2021-06-22T19:59:49.742293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(df, x=\"ptype\",y=\"posts\",histfunc = \"count\",\n                   title='Total posts for each personality type',\n                   labels={'ptype':'Personality types','posts':'No. of posts available'}, # can specify one label per df column\n                   opacity=0.8,\n                   color_discrete_sequence=['navy'] # color of histogram bars\n                   )\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.histogram(df, x=\"ptype\",y=\"posts\",histfunc = \"count\", color=\"ptype\", \n             title='Total posts for each personality type',\n             labels={'ptype':'Personality types','posts':'of posts available'}, \n             color_discrete_sequence=px.colors.sequential.YlGnBu).update_xaxes(categoryorder=\"total descending\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this function counts the no of words in each post of a user\ndef var_row(row):\n    l = []\n    for i in row.split('|||'):\n        l.append(len(i.split()))\n    return np.var(l)\n\n#this function counts the no of words per post out of the total 50 posts in the whole row\ndf['words_per_comment'] = df['posts'].apply(lambda x: len(x.split())/50)\ndf['variance_of_word_counts'] = df['posts'].apply(lambda x: var_row(x))\n\nfig = px.strip(df, x='ptype', y='words_per_comment',hover_data=[\"variance_of_word_counts\"],\n              color=\"ptype\",color_discrete_sequence=px.colors.sequential.Plasma_r)\n\n# type 3 : boxplot with stripplot + color\n#fig = px.box(df, x='type', y='words_per_comment', color='type', points=\"all\",\n#             color_discrete_sequence=px.colors.sequential.Plasma_r).update_xaxes(categoryorder=\"total descending\")\n\n\nfig.update_layout(\n    hoverlabel=dict(\n        bgcolor=\"blue\",\n        font_size=12,\n        font_family=\"Rockwell\"\n    )\n)\nfig.show()\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-22T19:59:51.874738Z","iopub.execute_input":"2021-06-22T19:59:51.875108Z","iopub.status.idle":"2021-06-22T19:59:53.882087Z","shell.execute_reply.started":"2021-06-22T19:59:51.875077Z","shell.execute_reply":"2021-06-22T19:59:53.881023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This plot further shows clearly that there are a number of imbalances in our dataset, showing all the observations along with some representation of the underlying distribution using our added features.\n\nINFP has the most cluttered showing there are most number of comments of this type of personality.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(30,25))\nsns.set(style=\"white\", color_codes=True) # suitable theme for jointplot\nsns.jointplot(\"variance_of_word_counts\", \"words_per_comment\", data=df, alpha=0.7)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:59:59.116329Z","iopub.execute_input":"2021-06-22T19:59:59.116741Z","iopub.status.idle":"2021-06-22T19:59:59.854946Z","shell.execute_reply.started":"2021-06-22T19:59:59.116706Z","shell.execute_reply":"2021-06-22T19:59:59.854099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.density_heatmap(df, x=\"variance_of_word_counts\", y=\"words_per_comment\", marginal_x=\"box\", marginal_y=\"violin\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:00:04.811068Z","iopub.execute_input":"2021-06-22T20:00:04.811616Z","iopub.status.idle":"2021-06-22T20:00:04.995449Z","shell.execute_reply.started":"2021-06-22T20:00:04.811579Z","shell.execute_reply":"2021-06-22T20:00:04.994432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The 2 histogram plots represent Gaussian distribution of a sample space, which in our case comprises of no. of words per comment and associated variance of word counts from our dataset.\n* In the hexagonal plot, the hexagon with most number of points gets darker color. So if you look at the above plot, you can see that most of the posts have words between 100 and 150 and most of no. of words per comment by a user is between 25-30.\n* We can see that there is no correlation observed between variance of word count and the words per comment.\n* There is a strong relationship when there are 25-30 words per comment & the variance of word counts is 100-150\n* This is also visible by analyzing the histogram plots on both the axis.","metadata":{}},{"cell_type":"code","source":"def plot_jointplot(mbti_type, axs, titles):\n    df_1 = df[df['ptype'] == mbti_type]\n    sns.jointplot(\"variance_of_word_counts\", \"words_per_comment\", data=df_1, kind=\"hex\", ax = axs, title = titles)\n\nplt.figure(figsize=(24, 5))    \ni = df['ptype'].unique()\nk = 0\n\nfor m in range(1,3):\n  for n in range(1,7):\n    df_1 = df[df['ptype'] == i[k]]\n    sns.jointplot(\"variance_of_word_counts\", \"words_per_comment\", data=df_1, kind=\"hex\" )\n    plt.title(i[k])\n    k+=1\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:01:51.382223Z","iopub.execute_input":"2021-06-22T20:01:51.382862Z","iopub.status.idle":"2021-06-22T20:01:58.720639Z","shell.execute_reply.started":"2021-06-22T20:01:51.382809Z","shell.execute_reply":"2021-06-22T20:01:58.719767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"length_posts\"] = df[\"posts\"].apply(len)\n\nsns.distplot(df[\"length_posts\"]).set_title(\"Distribution of Lengths of all 50 Posts\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:02:03.662949Z","iopub.execute_input":"2021-06-22T20:02:03.6633Z","iopub.status.idle":"2021-06-22T20:02:04.006807Z","shell.execute_reply.started":"2021-06-22T20:02:03.663272Z","shell.execute_reply":"2021-06-22T20:02:04.005764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(df, x=\"length_posts\", hover_data=df.columns, barmode=\"overlay\",\n                  title='Length of posts')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:02:06.672417Z","iopub.execute_input":"2021-06-22T20:02:06.672817Z","iopub.status.idle":"2021-06-22T20:02:06.768262Z","shell.execute_reply.started":"2021-06-22T20:02:06.672782Z","shell.execute_reply":"2021-06-22T20:02:06.767292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#If you need to install tabulate\n\n#!pip install tabulate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Finding the most common words in all posts.\nwords = list(df[\"posts\"].apply(lambda x: x.split()))\nwords = [x for y in words for x in y]\n#print(Counter(words).most_common(40))\n\nfrom tabulate import tabulate\nprint(tabulate(Counter(words).most_common(40), headers=['Word', 'Frequency']))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:02:16.970925Z","iopub.execute_input":"2021-06-22T20:02:16.971348Z","iopub.status.idle":"2021-06-22T20:02:21.290786Z","shell.execute_reply.started":"2021-06-22T20:02:16.97131Z","shell.execute_reply":"2021-06-22T20:02:21.289791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lower max_font_size, change the maximum number of word and lighten the background:\nwordcloud = WordCloud(width=1200, height=500, background_color=\"white\").generate(\" \".join(words))\n# collocations to False  is set to ensure that the word cloud doesn't appear as if it contains any duplicate words\nplt.figure(figsize=(25,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:03:35.613314Z","iopub.execute_input":"2021-06-22T20:03:35.613764Z","iopub.status.idle":"2021-06-22T20:03:35.642527Z","shell.execute_reply.started":"2021-06-22T20:03:35.613726Z","shell.execute_reply":"2021-06-22T20:03:35.640734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(len(df['type'].unique()), sharex=True, figsize=(15,len(df['type'].unique())))\nk = 0\nfor i in df['type'].unique():\n    df_4 = df[df['type'] == i]\n    wordcloud = WordCloud(max_words=1628,relative_scaling=1,background_color=\"white\",normalize_plurals=False).generate(df_4['posts'].to_string())\n    plt.subplot(4,4,k+1)\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.title(i)\n    ax[k].axis(\"off\")\n    k+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* we can see there are a no. of irrelevant words present in the dataset (e.g. ha, ar, Ti etx.) which will need to be removed\n* Interestingly, among the most common words in the word clouds of individual personality types, is the names of MBTI personlity types themselves.\n\nIt would hence be necessary to clean our posts by removing these MBTI words from each of them as part of our pre-processing stage, before training the model for better evaluation results.\n\n#### Counting the no. of users and posts in the given MBTI Kaggle dataset","metadata":{}},{"cell_type":"code","source":"def extract(posts, new_posts):\n    for post in posts[1].split(\"|||\"):\n        new_posts.append((posts[0], post))\n\nposts = []\ndf.apply(lambda x: extract(x, posts), axis=1)\nprint(\"Number of users\", len(df))\nprint(\"Number of posts\", len(posts))\n#print(\"5 posts from start are:\")\n#posts[0:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* It is inferenced that a lot of hyperlinks are presnt in these posts\n* It is safe to assume that url links do not provide any real information about a user's personality, hence, we need to clean our dataset for these too.\n\nThis given sample dataset does not come from the entire Kaggle user population; rather, it comes from Kaggle users who leave comments; thus, our ML model's conclusion cannot be applied to all Kaggle users, only to those who leave comments.\n\nFurthermore, with more data, more accurate models could be obtained. As a result, the model may fail to classify a personality at the lower end.","metadata":{}},{"cell_type":"markdown","source":"### Pre-Processing Stage","metadata":{}},{"cell_type":"code","source":"def preprocess_text(df, remove_special=True):\n    texts = df['posts'].copy()\n    labels = df['ptype'].copy()\n\n    #Remove links \n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'https?:\\/\\/.*?[\\s+]', '', x.replace(\"|\",\" \") + \" \"))\n    \n    #Keep the End Of Sentence characters\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'\\.', ' EOSTokenDot ', x + \" \"))\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'\\?', ' EOSTokenQuest ', x + \" \"))\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'!', ' EOSTokenExs ', x + \" \"))\n    \n    #Strip Punctation\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[\\.+]', \".\",x))\n\n    #Remove multiple fullstops\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n\n    #Remove Non-words\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[^a-zA-Z\\s]','',x))\n\n    #Convert posts to lowercase\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: x.lower())\n\n    #Remove multiple letter repeating words\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'([a-z])\\1{2,}[\\s|\\w]*','',x)) \n\n    #Remove very short or long words\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'(\\b\\w{0,3})?\\b','',x)) \n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'(\\b\\w{30,1000})?\\b','',x))\n\n    #Remove MBTI Personality Words - crutial in order to get valid model accuracy estimation for unseen data. \n    if remove_special:\n        pers_types = ['INFP' ,'INFJ', 'INTP', 'INTJ', 'ENTP', 'ENFP', 'ISTP' ,'ISFP' ,'ENTJ', 'ISTJ','ENFJ', 'ISFJ' ,'ESTP', 'ESFP' ,'ESFJ' ,'ESTJ']\n        pers_types = [p.lower() for p in pers_types]\n        p = re.compile(\"(\" + \"|\".join(pers_types) + \")\")\n        \n        df[\"posts\"] = df[\"posts\"].apply(lambda x: p.sub('',x))\n    \n    return df\n\n#Preprocessing of entered Text\nnew_df = preprocess_text(df,remove_special=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df = new_df.drop(['words_per_comment','variance_of_word_counts', 'length_posts'],axis=1) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\ndef lemmatize_text(text):\n    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df['posts'] = new_df.posts.apply(lemmatize_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data = data.drop(columns=['words_per_comment','variance_of_word_counts','length_posts'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Converting MBTI personality (or target or Y feature) into numerical form using Label Encoding\n# encoding personality type\n# Don't think this column is necessary\n\nenc = LabelEncoder()\nnew_df['type of encoding'] = enc.fit_transform(new_df['ptype'])\n\ntarget = new_df['type of encoding'] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.head(16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Four Classifiers across MBTI axis","metadata":{}},{"cell_type":"code","source":"def get_types(row):\n    t=row['ptype']\n\n    I = 0; N = 0\n    T = 0; J = 0\n    \n    if t[0] == 'I': I = 1\n    elif t[0] == 'E': I = 0\n    else: print('I-E not found') \n        \n    if t[1] == 'N': N = 1\n    elif t[1] == 'S': N = 0\n    else: print('N-S not found')\n        \n    if t[2] == 'T': T = 1\n    elif t[2] == 'F': T = 0\n    else: print('T-F not found')\n        \n    if t[3] == 'J': J = 1\n    elif t[3] == 'P': J = 0\n    else: print('J-P not found')\n    return pd.Series( {'IE':I, 'NS':N , 'TF': T, 'JP': J }) \n\ndata = new_df.join(new_df.apply (lambda row: get_types (row),axis=1))\ndata.head(15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Using the above code, if a person has I, N, T and J, the value across the 4 axis of MBTI i.e. IE, NS, TF and JP respectively, will be 1. Else 0.\n\nThis will help us calculate for e.g. how many Introvert posts are present v/s how many Extrovert posts are presnt, out of all the given entries in our labelled Kaggle dataset. This is done in order to extplore the dataset for all the individual Personality Indices of MBTI\n\n###### Counting No. of posts in one class / Total no. of posts in the other class","metadata":{}},{"cell_type":"code","source":"print (\"Introversion (I) /  Extroversion (E):\\t\", data['IE'].value_counts()[0], \" / \", data['IE'].value_counts()[1])\nprint (\"Intuition (N) / Sensing (S):\\t\\t\", data['NS'].value_counts()[0], \" / \", data['NS'].value_counts()[1])\nprint (\"Thinking (T) / Feeling (F):\\t\\t\", data['TF'].value_counts()[0], \" / \", data['TF'].value_counts()[1])\nprint (\"Judging (J) / Perceiving (P):\\t\\t\", data['JP'].value_counts()[0], \" / \", data['JP'].value_counts()[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We infer that there is unequal distribution even among each of the 4 axis in the entries of out dataset. i.e. out of IE:E is the majority, in NS:S is the majority. While TF and JP have realtively less differnce between them.","metadata":{}},{"cell_type":"code","source":"#Plotting the distribution of each personality type indicator\nN = 4\nbottom = (data['IE'].value_counts()[0], data['NS'].value_counts()[0], data['TF'].value_counts()[0], data['JP'].value_counts()[0])\ntop = (data['IE'].value_counts()[1], data['NS'].value_counts()[1], data['TF'].value_counts()[1], data['JP'].value_counts()[1])\n\nind = np.arange(N)    # the x locations for the groups\n# the width of the bars\nwidth = 0.7           # or len(x) can also be used here\n\np1 = plt.bar(ind, bottom, width, label=\"I, N, T, F\")\np2 = plt.bar(ind, top, width, bottom=bottom, label=\"E, S, F, P\") \n\nplt.title('Distribution accoss types indicators')\nplt.ylabel('Count')\nplt.xticks(ind, ('I / E',  'N / S', 'T / F', 'J / P',))\nplt.legend()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"axis = data[[\"IE\", \"NS\",\"TF\",\"JP\"]].plot(kind=\"bar\", stacked=True)\n\nfig = axis.get_figure()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fun Fact : The above results match with real life findings by researchers across various personality and psycological studies like\n\nWe can compare this with the fact that Introverts are a minority, making up roughly 16 percent of people [1]. Eventhough among introverts, there are varying degrees, and Carl Jung said, “There is no such thing as a pure Extrovert or a pure introvert\" Hence it is tricky to classify a person with 1 type.\n\nWhile the population is split roughly 50/50 on the other dimensions, a full 70% of people show a preference for Sensing over Intuition when taking a personality test. Because Intuitives are the minority, the onus is on them to adjust to the Sensor way of thinking.\n\nThe differences between Judging and Perceiving are probably the most marked differences of all the four preferences. People with strong Judging preferences might have a hard time accepting people with strong Perceiving preferences, and vice-versa. On the other hand, a \"mixed\" couple (one Perceiving and one Judging) can complement each other very well, if they have developed themselves enough to be able to accept each other's differences.","metadata":{}},{"cell_type":"markdown","source":"##### Features Correlation Analysis","metadata":{}},{"cell_type":"code","source":"data[['IE','NS','TF','JP']].corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Stem the posts\n\nfrom nltk.stem.porter import *\nstemmer = PorterStemmer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['posts'] = data['posts'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Remove stopwords from the posts\n\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\ndata['posts'] = data['posts'].apply(lambda x: ' '.join([word for word in x if word not in (stop_words)]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Put the posts alone in a variable\nPostsAlone = data['posts']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PostsAlone","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Converting the posts into text features using sklearn’s TF-IDF\n#Might want to change the min_df and ngram_range\n#Fit first then transform\n#We will train our models on these transformations\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvec = TfidfVectorizer(min_df=0.2, ngram_range=(1,3))\nvec.fit(PostsAlone)\nfeatures = vec.transform(PostsAlone)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train-test split for the logisitc regression model\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(features, data['IE'], \n                                                    train_size=0.8, random_state=1)\nprint('training data:', X_train.shape)\n\nprint('test data:', X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train a logistic regression model to predict whether someone is a Judging or Perceiving\n\nlog_reg = LogisticRegression(solver='lbfgs', max_iter=3000)\n\n# fit the model to the training data\nclf = log_reg.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Accuracies of the model\n\nprint('\\ntraining accuracy: {}'.format(clf.score(X_train, y_train).round(3)))\nprint('test accuracy: {}'.format(clf.score(X_test, y_test).round(3)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Regularization w/ the C parameter (note: default C = 1)\n# Best c's for test accuracy appear to be 0.001, 0.01, 0.1\n\ncset = [.001, .01, .1, 1, 10]\nfor i in cset:\n    print('C =', i)\n    log_reg = LogisticRegression(solver='lbfgs', max_iter=1000, C=i)\n    clf = log_reg.fit(X_train, y_train)\n    print('training accuracy: {}'.format(clf.score(X_train, y_train).round(3)))\n    print('test accuracy: {}'.format(clf.score(X_test, y_test).round(3)), '\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Cross-validation w/ tuning regularization in logistic regression\n\nfor i in cset:\n    print('C =', i)\n    log_reg = LogisticRegression(solver='lbfgs', max_iter=1000, C=i)\n    scores = cross_val_score(log_reg, features, data['IE'], cv=5)\n    print(scores)\n    print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2), '\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Logistic Regression with Grid search\n#Had to use MaxAbScaler because I got an error w/MinMaxScaler. Not sure why\n#Best parameter here is \"C:0.1\"\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import MaxAbsScaler\n\n\n# define x_scaler\nscaler = MaxAbsScaler()\n\nscaled_X = scaler.fit_transform(X_train)\n\ntuned_parameters = {'C': [0.1, 0.5, 1, 5, 10, 50, 100]}\n\ngrid = GridSearchCV(LogisticRegression(solver='liblinear'), tuned_parameters, cv=3, scoring=\"accuracy\")\n\ngrid.fit(scaled_X, y_train)\n\nprint('mean of accuracies:', grid.cv_results_['mean_test_score'])\nprint('std dev of accuracies:', grid.cv_results_['std_test_score'])\n\n# print best parameter after tuning \nprint('best parameters:', grid.best_params_) \n\n# store the best estimator\nbest_logreg = grid.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SVM model\n#Didn't run this because it would take too long\n\nsvm = SVC(kernel = 'linear')\n\n#fit the model to the training data\nclf2 = svm.fit(X_train, y_train)\n\n# get accuracy stats\nprint('training accuracy: {}'.format(clf2.score(X_train, y_train).round(3)))\nprint('test accuracy: {}'.format(clf2.score(X_test, y_test).round(3)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Grid Search with SVM Model\nfrom sklearn.pipeline import Pipeline\n\n# add in a pipeline to control data leakage\nsteps = [('scaler', MaxAbsScaler()), ('SVM', SVC())]\n\n# define the pipeline object\npipeline = Pipeline(steps)\n\nparams = {'SVM__kernel': ['linear'], 'SVM__C': [0.1, 1, 10, 100]} \n\n# run grid search\ngrid = GridSearchCV(pipeline, param_grid=params, cv=3)\ngrid.fit(X_train, y_train)\n\n# print mean and standard deviation of scores by iteration\nprint('mean of accuracies:', grid.cv_results_['mean_test_score'])\nprint('std dev of accuracies:', grid.cv_results_['std_test_score'])\n\n# print best parameter after tuning \nprint(grid.best_params_) \n\n# store the best estimator\nbest_svm = grid.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Random Forests with grid search\n\n#set-up grid of parameters to search\nparam_grid = {'n_estimators': [10, 100, 250], 'max_samples': [.25, .5, 1]} \n\n# instantiate grid search object\ngrid = GridSearchCV(RandomForestClassifier(), param_grid, cv = 3)\n\n# fitting the model for grid search \ngrid.fit(X_train, y_train)\n\n# print parameters, mean, and standard deviation of scores by iteration\nfor z in range(0, len(grid.cv_results_['params'])):\n    print('\\nparams:', grid.cv_results_['params'][z])\n    print('mean of accuracies:', grid.cv_results_['mean_test_score'][z])\n    print('std dev of accuracies:', grid.cv_results_['std_test_score'][z])\n\n# print best parameter after tuning \nprint('best parameters:', grid.best_params_)\nprint('best score:', grid.best_score_)\n\n# store the best estimator\nbest_rf = grid.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Logistic Regression: 60.7 accuracy\n#SVM: Around 60.2 accuracy\n#Random Forests: 60.49\n\n#Logistic the Regression is the best but not by much at all","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Bringing in Trump's tweets\n\nmydata = pd.read_csv('../input/trump-tweets/Trump Tweets1.csv')\nprint(mydata.shape, mydata.columns.to_list(),'\\n')\nmydata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Pre-process the new dataset\n\ndef preprocess_text2(df, remove_special=True):\n\n    #Remove links \n    mydata[\"tweets\"] = mydata[\"tweets\"].apply(lambda x: re.sub(r'https?:\\/\\/.*?[\\s+]', '', x.replace(\"|\",\" \") + \" \"))\n    \n    #Keep the End Of Sentence characters\n    mydata[\"tweets\"] =  mydata[\"tweets\"].apply(lambda x: re.sub(r'\\.', ' EOSTokenDot ', x + \" \"))\n    mydata[\"tweets\"] =  mydata[\"tweets\"].apply(lambda x: re.sub(r'\\?', ' EOSTokenQuest ', x + \" \"))\n    mydata[\"tweets\"] =  mydata[\"tweets\"].apply(lambda x: re.sub(r'!', ' EOSTokenExs ', x + \" \"))\n    \n    #Strip Punctation\n    mydata[\"tweets\"] =  mydata[\"tweets\"].apply(lambda x: re.sub(r'[\\.+]', \".\",x))\n\n    #Remove multiple fullstops\n    mydata[\"tweets\"] =  mydata[\"tweets\"].apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n\n    #Remove Non-words\n    mydata[\"tweets\"] =  mydata[\"tweets\"].apply(lambda x: re.sub(r'[^a-zA-Z\\s]','',x))\n\n    #Convert posts to lowercase\n    mydata[\"tweets\"] =  mydata[\"tweets\"].apply(lambda x: x.lower())\n\n    #Remove multiple letter repeating words\n    mydata[\"tweets\"] =  mydata[\"tweets\"].apply(lambda x: re.sub(r'([a-z])\\1{2,}[\\s|\\w]*','',x)) \n\n    #Remove very short or long words\n    mydata[\"tweets\"] =  mydata[\"tweets\"].apply(lambda x: re.sub(r'(\\b\\w{0,3})?\\b','',x)) \n    mydata[\"tweets\"] =  mydata[\"tweets\"].apply(lambda x: re.sub(r'(\\b\\w{30,1000})?\\b','',x))\n\n    #Remove MBTI Personality Words - crutial in order to get valid model accuracy estimation for unseen data. \n    if remove_special:\n        pers_types = ['INFP' ,'INFJ', 'INTP', 'INTJ', 'ENTP', 'ENFP', 'ISTP' ,'ISFP' ,'ENTJ', 'ISTJ','ENFJ', 'ISFJ' ,'ESTP', 'ESFP' ,'ESFJ' ,'ESTJ']\n        pers_types = [p.lower() for p in pers_types]\n        p = re.compile(\"(\" + \"|\".join(pers_types) + \")\")\n        \n        mydata[\"tweets\"] = mydata[\"tweets\"].apply(lambda x: p.sub('',x))\n    \n    return df\n\n#Preprocessing of entered Text\ndf8 = preprocess_text2(mydata,remove_special=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df8","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lemmatize the new data\n\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\ndef lemmatize_text(text):\n    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lemmatize the new data\n\ndf8['tweets'] = df8.tweets.apply(lemmatize_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Stem the new data\n\nfrom nltk.stem.porter import *\nstemmer = PorterStemmer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Stem the new data\n\ndf8['tweets'] = df8['tweets'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Remove stopwords from the new data\n\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\ndf8['tweets']= df8['tweets'].apply(lambda x: ' '.join([word for word in x if word not in (stop_words)]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df8","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Put the tweets in a list\n\nList = df8['tweets'].to_list()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"List","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Join this information\n\nJoining = ' '.join(List)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Joining","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change this join back to a list\n\nJoining2 = [Joining]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Vectorize and fit all of the tweets combined\nfeatures2 = vec.transform(Joining2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(features2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict these features using our old logistic regression model (stored as clf) \n# This is for predicting [I] or [E] or [1] or [0]\n\npred = clf.predict(features2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pred)\n\n\n#Trump's personality trait is ESFP, Here, our model predicts that he's 1 or I.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Probability distribution of whether he's 0 or 1/E or I\n\npred2 = clf.predict_proba(features2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Probability distribution of whether he's 0 or 1/E or I\n\npred2","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}