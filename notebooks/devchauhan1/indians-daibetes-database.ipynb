{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.metrics import f1_score\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**IMPORTING DATASET**","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DATA PREPROCESSING**","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As Glucose,BloodPressure,SkinThickness,Insulin,BMI these columns have 0 as its values. We'll clean some of our dependent variables, replacing the 0's in the features with their mean.","metadata":{}},{"cell_type":"code","source":"cols_with_zero = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']\nfor col in cols_with_zero:\n    df[col] = df[col].replace(0, np.NaN)\n    median = int(df[col].median(skipna= True))\n    df[col] = df[col].replace(np.NaN, median)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DATA VISUALISATION**","metadata":{}},{"cell_type":"markdown","source":"Finding the correlations between the different features present in our dataset.","metadata":{}},{"cell_type":"code","source":"corr=df.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(corr,annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(30,10))\nsns.countplot(df[\"SkinThickness\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_columns=[\"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We use it forbreaking data variables up across multiple subplots and combining those subplots into a single figure.","metadata":{}},{"cell_type":"code","source":"for i in df_columns:\n    g = sns.FacetGrid(df, col='Outcome')\n    g = g.map(sns.kdeplot, i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style=\"whitegrid\"\nsns.pairplot(df,hue=\"Outcome\",palette=\"coolwarm\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.scatterplot(x=\"Insulin\",y=\"BMI\",hue=\"Outcome\",data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(111)\nsns.distplot(df['Age'], bins=10, kde=True)\nplt.show()\n\nplt.subplot(121)\nsns.distplot(df[\"Glucose\"],bins=10,kde=True)\nplt.show()\n\nplt.subplot(131)\nsns.distplot(df[\"BloodPressure\"],bins=10,kde=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.stripplot(x='Outcome', y=\"Age\" , data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.stripplot(x='Outcome', y=\"Glucose\" , data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting dataset into independent and dependent dataset","metadata":{}},{"cell_type":"code","source":"X=df.iloc[:,:-1].values\ny=df.iloc[:,-1].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SPLITTING DATASET INTO TRAIN TEST AND TEST SET**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**FEATURE SCALING**\n\nFor making all the features to come in a similar range","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TRAINING THE DATASET ON THE K-NN MODEL**","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 11, metric = 'euclidean', p = 2)\nclassifier.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**PREDICTING THE TEST SET RESULT**","metadata":{}},{"cell_type":"code","source":"y_pred = classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**MAKING THE CONFUSION MATRIX**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f1_score(y_test,y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**WE GOT AN ACCURACY OF 81% AND F1_SCORE 68% WHICH IS NOT VERY GOOD BUT SATISFACTORY**","metadata":{}}]}