{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Disclaimer & Credits\n\nIn this kernel, I have attempted to re-implement the code for the third chapter of **Aurélien Géron's** amazing book [Hands-on Machine Learning with Scikit-Learn, Keras and Tensorflow](https://github.com/ageron/handson-ml2). You can find his detailed jupyter notebooks for each chapter in the link mentioned before. This notebook is primarily a way for me to internalize the content shared in each chapter of the book, and I hope it is useful to you. \n\n\n**Note:** _The code and content here is contained in the notebooks linked above. I have done my best not to include anything present in his book but not present in the notebooks._"},{"metadata":{},"cell_type":"markdown","source":"## Previous Related Kernels:\n1. [End to End Machine Learning](https://www.kaggle.com/sairam6087/chpt-2-hands-on-ml2-end-to-end-ml-project)"},{"metadata":{},"cell_type":"markdown","source":"# Classification\nStudy classification using MNIST as an example dataset"},{"metadata":{},"cell_type":"markdown","source":"## Setup"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\ntest = pd.read_csv(\"../input/mnist-in-csv/mnist_test.csv\")\ntrain = pd.read_csv(\"../input/mnist-in-csv/mnist_train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, y_train = train.drop(labels=[\"label\"],axis=1),  train[\"label\"]\nX_test, y_test = test.drop(labels=[\"label\"],axis=1),  test[\"label\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.values.reshape(-1, 784)\nX_test = X_test.values.reshape(-1, 784)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport sklearn\nimport os\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"some_digit = X_train[0]\n\ndef plot_digit(data):\n    image = data.reshape(28,28)\n    plt.imshow(image, cmap=\"binary\")\n    plt.axis(\"off\")\n    plt.show()\n    \nplot_digit(some_digit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_digits(instances, images_per_row=10, **options):\n    size = 28\n    images_per_row = min(len(instances), images_per_row)\n    images = [instance.reshape(size,size) for instance in instances]\n    n_rows = (len(instances) - 1) // images_per_row + 1\n    row_images = []\n    n_empty = n_rows * images_per_row - len(instances)\n    images.append(np.zeros((size, size * n_empty)))\n    for row in range(n_rows):\n        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n        row_images.append(np.concatenate(rimages, axis=1))\n    image = np.concatenate(row_images, axis=0)\n    plt.imshow(image, cmap = mpl.cm.binary, **options)\n    plt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nexample_images = X_train[:100]\nplot_digits(example_images, images_per_row=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training a Binary Classifier\nFirst we look at training a simple binary classifier to distinguish between _'5's_ and other digits"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setup Training Set\ny_train_5 = (y_train == 5)\ny_test_5 = (y_test == 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SGD Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train_5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test on our example above\nsgd_clf.predict([some_digit])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performance Measures"},{"metadata":{},"cell_type":"markdown","source":"### Measuring Accuracy Using Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Implementing Cross Validation "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.base import clone\n\nskfolds = StratifiedKFold(n_splits=3, random_state=42)\n\nfor train_index, test_index in skfolds.split(X_train, y_train_5):\n    clone_clf = clone(sgd_clf) # Clone the model for each fold's run\n    X_train_fold = X_train[train_index]\n    y_train_fold= y_train_5[train_index]\n    X_test_fold = X_train[test_index]\n    y_test_fold = y_train_5[test_index]\n    \n    clone_clf.fit(X_train_fold, y_train_fold)\n    y_pred = clone_clf.predict(X_test_fold)\n    \n    num_correct = sum(y_test_fold == y_pred)\n    print(num_correct/len(y_test_fold))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen above, the numbers match the default Scikit learn implementation\n\n#### Limitations of Accuracy as a Performance Measure\nWhat if we had a naive classifier which said _**NOT 5**_ to every sample?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator\n\nclass Never5Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X),1), dtype=bool)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"never_5_clf = Never5Classifier()\ncross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\",n_jobs=-1) # Clearly something's up here! Just guessing not 5 results in > 90% accuracy!!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_5.value_counts() # There's only 10% True labels in the dataset => Accuracy isn't ideal for measuring performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For the confusion matrix, we need the actual predictions themselves\nfrom sklearn.model_selection import cross_val_predict\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,n_jobs=-1)\n\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_train_5, y_train_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In a perfect world, the confusion matrix of a perfect classifier would be\ny_perfect = y_train_5\nconfusion_matrix(y_train_5, y_perfect)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Precision and Recall \nHow does our binary classifier look when we inspect the Precision and Recall scores?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\n# Precision\nprecision_score(y_train_5, y_train_pred) # == 3530 / (3530 + 687) ; Look at the confusion matrix result above [[TN FN], [FP TP]] for these numbers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"3530/(3530 + 687)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Recall\nrecall_score(y_train_5, y_train_pred) # == 3530 / (3530 + 1891)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"3530/(3530 + 1891)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# F1 score is the harmonic mean of precision and recall \nfrom sklearn.metrics import f1_score\nf1_score(y_train_5, y_train_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"3530 / (3530 + (687 + 1891)/2) # Verify mathematically","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Precision Recall Tradeoff\n\nYou cannot have your cake and eat it too. (No idea why ;) )"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's threshold the scores\ny_score = sgd_clf.decision_function([some_digit])\ny_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clearly depending on the threshold set, the prediction changes\nthreshold_low, threshold_high = 0, 4000\npred_1, pred_2 = (y_score > threshold_low), (y_score > threshold_high)\nprint(pred_1, pred_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Q: How can we choose the threshold to get the best results for our use case? \n\n\nA: Plot a) The Precision, Recall curve vs thresholds OR b) Precision vs Recall"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\ny_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"decision_function\",n_jobs=-1) # Get the raw scores instead of the labels\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Option A\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1],\"b--\", label=\"Precision\", linewidth=2)\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\",linewidth=2)\n    plt.legend(loc=\"center right\", fontsize=16)\n    plt.xlabel(\"Threshold\", fontsize=16)\n    plt.grid(True)\n    plt.axis([-50000, 50000, 0, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_90_precision = recalls[np.argmax(precisions >= 0.90)]\nthreshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]\n\n\nplt.figure(figsize=(8, 4))                                                                  \nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.plot([threshold_90_precision, threshold_90_precision], [0., 0.9], \"r:\")                 \nplt.plot([-50000, threshold_90_precision], [0.9, 0.9], \"r:\")                                \nplt.plot([-50000, threshold_90_precision], [recall_90_precision, recall_90_precision], \"r:\")\nplt.plot([threshold_90_precision], [0.9], \"ro\")                                            \nplt.plot([threshold_90_precision], [recall_90_precision], \"ro\")                             \nplt.title(\"precision_recall_vs_threshold_plot\")                                              \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Option B\ndef plot_precision_vs_recall(precisions, recalls):\n    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n    plt.xlabel(\"Recall\", fontsize=16)\n    plt.ylabel(\"Precision\", fontsize=16)\n    plt.axis([0, 1, 0, 1])\n    plt.grid(True)\n\nplt.figure(figsize=(8, 6))\nplot_precision_vs_recall(precisions, recalls)\nplt.plot([0.4368, 0.4368], [0., 0.9], \"r:\")\nplt.plot([0.0, 0.4368], [0.9, 0.9], \"r:\")\nplt.plot([0.4368], [0.9], \"ro\")\nplt.title(\"precision_vs_recall_plot\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once you choose a threshold from the curve, you now make predictions as shown below"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_90 = (y_scores >= threshold_90_precision)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision_score(y_train_5, y_train_pred_90) # Good Precision","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_score(y_train_5, y_train_pred_90) # But at the cost of poor recall","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The ROC Curve\nAnother way to evaluate the classifier and choose the best operating point"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_train_5, y_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0,1], [0, 1], 'k--')\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel(\"False Positive Rate (1 - Specificity)\", fontsize=16)\n    plt.ylabel(\"True Positive Rate (Recall)\", fontsize=16)\n    plt.grid(True)\n    \n\nplt.figure(figsize=(8,6))\nplot_roc_curve(fpr, tpr)\nplt.plot([4.837e-3, 4.837e-3], [0., 0.4368], \"r:\") \nplt.plot([0.0, 4.837e-3], [0.4368, 0.4368], \"r:\") \nplt.plot([4.837e-3], [0.4368], \"ro\")            \nplt.title(\"ROC Curve\")                         \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_train_5, y_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Comparing Models using the ROC Curve\nThe more the model leans towards the top left of the ROC plot, the better it is. Let's look at how a Random Forest compares to the simple SGD model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nforest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\ny_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3, method=\"predict_proba\",n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_scores_forest = y_probas_forest[:, 1]\nfpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5, y_scores_forest) # Clearly the Random Forest is better :)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, \"b:\", linewidth=2, label=\"SGD\")\nplot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\nplt.plot([4.837e-3, 4.837e-3], [0., 0.4368], \"r:\")\nplt.plot([0.0, 4.837e-3], [0.4368, 0.4368], \"r:\")\nplt.plot([4.837e-3], [0.4368], \"ro\")\nplt.plot([4.837e-3, 4.837e-3], [0., 0.9487], \"r:\")\nplt.plot([4.837e-3], [0.9487], \"ro\")\nplt.grid(True)\nplt.legend(loc=\"lower right\", fontsize=16)\nplt.title(\"ROC Curve Comparison\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see how this lines up with the precision and recall values\nroc_auc_score(y_train_5, y_scores_forest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,n_jobs=-1)\nprecision_score(y_train_5, y_train_pred_forest) # Good Precision","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_score(y_train_5, y_train_pred_forest) # At much better recall than the SGD","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multiclass Classification\nClassification wherein there is more than 2 classes to distinguish. What if now, we used the entire dataset instead of **5** vs **NOT 5** ?"},{"metadata":{},"cell_type":"markdown","source":"### SVM Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvm_clf = SVC(gamma=\"auto\", random_state=42)\nsvm_clf.fit(X_train[:1000], y_train[:1000]) # Training with the whole dataset takes forever! :D\nsvm_clf.predict([some_digit]) # What does it predict for \"5\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What are the scores for this sample?\nsome_digit_scores = svm_clf.decision_function([some_digit])\nsome_digit_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What is the index with the max score?\nnp.argmax(some_digit_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What are the class labels for each index?\nsvm_clf.classes_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What is in the 5th index?\nsvm_clf.classes_[5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Typically the `SVC` class uses a **One versus One strategy** under the hood, but we can force it to use a **One versus Rest strategy** as below"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.multiclass import OneVsRestClassifier\novr_clf = OneVsRestClassifier(SVC())\novr_clf.fit(X_train[:1000], y_train[:1000])\novr_clf.predict([some_digit])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many estimators do we have?\nlen(ovr_clf.estimators_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SGD Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The same strategy of One vs One is used even for SGD by default\nsgd_clf.fit(X_train[:5000], y_train[:5000])\nsgd_clf.predict([some_digit])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd_clf.decision_function([some_digit])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What are the scores?\ncross_val_score(sgd_clf, X_train[:5000], y_train[:5000], cv=3, scoring=\"accuracy\", n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What if we used Scaling?\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\ncross_val_score(sgd_clf, X_train_scaled[:5000], y_train[:5000], cv=3, scoring=\"accuracy\", n_jobs=-1) # Scores improve :D","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Error Analysis\n\nAssuming that we have found the model we want to use and have some fine tuning done, how can we improve further?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix\ny_train_pred = cross_val_predict(sgd_clf, X_train_scaled[:5000], y_train[:5000], cv=3, n_jobs=-1)\nconf_mx = confusion_matrix(y_train[:5000], y_train_pred[:5000])\nconf_mx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How does it look like?\nplt.matshow(conf_mx, cmap=plt.cm.gray)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's make the matrix better for analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"row_sums = conf_mx.sum(axis=1, keepdims=True)\nnorm_conf_mx = conf_mx / row_sums","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.fill_diagonal(norm_conf_mx, 0)  # Let's keep only the errors\nplt.matshow(norm_conf_mx, cmap=plt.cm.gray)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like `3` and `5` are being confused"},{"metadata":{"trusted":true},"cell_type":"code","source":"cl_a, cl_b = 3, 5\nX_aa = X_train[:5000][(y_train[:5000] == cl_a) & (y_train_pred == cl_a)]\nX_ab = X_train[:5000][(y_train[:5000] == cl_a) & (y_train_pred == cl_b)]\nX_ba = X_train[:5000][(y_train[:5000] == cl_b) & (y_train_pred == cl_a)]\nX_bb = X_train[:5000][(y_train[:5000] == cl_b) & (y_train_pred == cl_b)]\n\nplt.figure(figsize=(8,8))\nplt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)\nplt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)\nplt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)\nplt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can clearly be seen why the confusion arises. "},{"metadata":{},"cell_type":"markdown","source":"## Multilabel Classification\nGiven a sample, produce more than one label as output for it"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\n# Problem Statement: Is the digit larger than 6. Also is it odd?\ny_train_large = (y_train >= 7)\ny_train_odd = (y_train % 2 == 1)\n\ny_multilabel = np.c_[y_train_large, y_train_odd]\n\nknn_clf = KNeighborsClassifier()\nknn_clf.fit(X_train[:5000], y_multilabel[:5000, :])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What is 5 ?\nknn_clf.predict([some_digit]) # It is not > 6 and it is odd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the **F1** score is a good way to evaluate a multilabel classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_knn_pred = cross_val_predict(knn_clf, X_train[:5000], y_multilabel[:5000, :], cv=3, n_jobs=-1)\nf1_score(y_multilabel[:5000, :], y_train_knn_pred, average=\"macro\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multioutput Classification\nA generalization of multilabel classification where each label can be multiclass"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Problem Statement : Image denoising\n\n# Add noise to the input images\nnoise = np.random.randint(0, 100, (len(X_train), 784))\nX_train_mod = X_train + noise\nnoise = np.random.randint(0, 100, (len(X_test), 784))\nX_test_mod = X_test + noise\ny_train_mod = X_train\ny_test_mod = X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"some_index = 108\nplt.subplot(121); plot_digit(X_test_mod[some_index])\nplt.subplot(122); plot_digit(y_test_mod[some_index])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_clf.fit(X_train_mod[:5000,:], y_train_mod[:5000,:])\nclean_digit = knn_clf.predict([X_test_mod[some_index]])\nplot_digit(clean_digit)\nplt.show() # Not bad :D","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exercises\n\n### 1. An MNIST Classifier With Over 97% Accuracy\nSince this takes a while to run, here is the solution as a separate kernel: https://www.kaggle.com/sairam6087/chpt-3-hands-on-ml2-exercise-solutions-1-2"},{"metadata":{},"cell_type":"markdown","source":"### 2. Write a function that can shift an MNIST image in any direction by one pixel and re-evaluate the model from 1. \n\nSee the kernel above"},{"metadata":{},"cell_type":"markdown","source":"### 3. Tackle the Titanic Dataset\nThis also is provided here as a separate kernel here: https://www.kaggle.com/sairam6087/chpt-3-hands-on-ml2-exercise-solutions-3"},{"metadata":{},"cell_type":"markdown","source":"### 4. Build a Spam Classifier\n\nHere is a kernel for this exercise: https://www.kaggle.com/sairam6087/chpt-3-hands-on-ml2-exercise-solutions-4"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}