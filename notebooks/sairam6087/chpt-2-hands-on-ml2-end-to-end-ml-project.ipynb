{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Disclaimer & Credits\n\nIn this kernel, I have attempted to re-implement the code for the second chapter of **Aurélien Géron's** amazing book [Hands-on Machine Learning with Scikit-Learn, Keras and Tensorflow](https://github.com/ageron/handson-ml2). You can find his detailed jupyter notebooks for each chapter in the link mentioned before. This notebook is primarily a way for me to internalize the content shared in each chapter of the book, and I hope it is useful to you. \n\n\n**Note:** _The code and content here is contained in the notebooks linked above. I have done my best not to include anything present in his book but not present in the notebooks._"},{"metadata":{},"cell_type":"markdown","source":"# End to End Machine Learning Project\nUse the California census to dataset to build a model of housing prices in the state"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# imports\nimport os\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"HOUSING_PATH=os.path.join(\"../input\",\"california-housing-prices\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_housing_data(housing_path=HOUSING_PATH):\n    csv_path = os.path.join(housing_path,\"housing.csv\")\n    return pd.read_csv(csv_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = load_housing_data()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Look at the structure of the data"},{"metadata":{},"cell_type":"markdown","source":"Let's use the `head` method to take a look at the first 5 rows of the dataframe. Herein, each row represents one district in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 10 attributes, namely longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, median_house_value and ocean_proximity. Alternatively, you can see them via the `columns` member of the dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the `info` method, we can get information about the total number of rows, each feature's type and the number of *non-null* values. Below, we can see that there are some null values for the `ocean_proximity` feature and that there are **20640** rows in the dataset. Further, the `ocean_proximity` feature is non-numeric"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the distribution of the `ocean-proximity` feature using the `value_counts` methods since it is non-numeric"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"ocean_proximity\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the other features, we can use the `describe()` method which will give statistical information for each feature such as *count,mean,std,min,max* and *percentile* information"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Additionally, plotting the histogram for each numerical attribute helps get a quick feel of the data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50,figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations from the plots:**\n1. Some of the attributes such as `median_income`, `housing_median_age` and `median_house_value` (our target variable!) are clipped to a range\n2. Attributes have different scales\n3. Many histograms are tail-heavy, i.e., the distribution extends more to the right of the median than the left\n"},{"metadata":{},"cell_type":"markdown","source":"## Creating a Test Set\nLet's split the total data into training and test sets using different methods and compare results"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n# Fix the random seed\nnp.random.seed(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standard Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Implementing basic train-test split\ndef split_train_test_data(total_data, test_frac=0.2):\n    # Shuffle the indices of the data randomly\n    shuffled_indices = np.random.permutation(len(total_data))\n    # Length of test set\n    test_set_len = int(test_frac*len(total_data))\n    # Test Indices\n    test_indices = shuffled_indices[:test_set_len]\n    # Train Indices\n    train_indices = shuffled_indices[test_set_len:]\n    \n    return total_data.iloc[train_indices], total_data.iloc[test_indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# My split data\ntrain_set, test_set = split_train_test_data(housing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split using Scikit-learn\nfrom sklearn.model_selection import train_test_split\n\nsk_train_set, sk_test_set = train_test_split(housing, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comparing data splits\ntrain_set.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sk_train_set.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stable Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# A more sophisticated split method: Keep the test set consistent even when new data is added to the total set\n# When there's new data added to the set, the test set will definitely have all of the original test data despite shuffling\nfrom zlib import crc32\n\ndef test_set_check(data_id, test_frac):\n    data_hash_value = crc32(np.int64(data_id)) & 0xffffffff\n    return data_hash_value < test_frac * 2**32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_by_id(total_data, test_frac, id_column):\n    ids = total_data[id_column]\n    ids_in_test_set = ids.apply(lambda id_: test_set_check(id_, test_frac))\n    \n    return total_data[~ids_in_test_set], total_data[ids_in_test_set]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create an id field for the dataset\n\n# Option 1) Add the index field\nhousing_with_id = housing.reset_index()\ntrain_set_stable, test_set_stable = split_by_id(housing_with_id, 0.2, \"index\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set_stable.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Option 2) Create a more stable id using latitude and longitude\nhousing_with_id = housing\nhousing_with_id[\"id\"] = housing[\"longitude\"]*1000 + housing[\"latitude\"]\ntrain_set_stable, test_set_stable = split_by_id(housing_with_id, 0.2, \"id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set_stable.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stratified Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a income category to see how the income values are distributed\nhousing[\"income_cat\"] = pd.cut(housing[\"median_income\"], \n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels = [1, 2, 3, 4, 5])\n\nhousing[\"income_cat\"].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To ensure that the test set is representative of all categories in the whole dataset, we can use stratified sampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's see if the data has been split according the true representation of categories in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Original data's distribution across categories\nhousing[\"income_cat\"].value_counts()/len(housing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stratified test set's distribution across categories\nstrat_test_set[\"income_cat\"].value_counts()/len(strat_test_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen above, the test set has roughly the same distribution of class categories as the training set. Now we can move on to inspecting the data for gaining insights"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the \"income_cat\" feature from the stratified datasets\n\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)\n    set_.drop(\"id\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Discover and Visualize the Data to Gain Insights"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a copy of the stratified data for analysis\nhousing = strat_train_set.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing Geographical Data\nLet's plot the data based on the latitude and longitude feature of each sample to see how the homes are distributed"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.plot(kind=\"scatter\", # Type of plot\n             x=\"longitude\",\n             y=\"latitude\",\n             alpha=0.4, # Transparency to visualize data easier\n             s=housing[\"population\"]/100, # Scale the points by the population; Gives us a sense of density\n             label=\"Population\",\n             figsize=(20,14),\n             c=\"median_house_value\", # Color code by the house value; Gives us a sense of where homes cost more\n             cmap=plt.get_cmap(\"jet\"), # Blue indicates low values and Red indicates high values\n             colorbar=True)\n\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preliminary insights:\n1. High density areas are Bay Area, Los Angeles and San Diego amongst others\n2. Housing prices seem to be related to the location of the home (near the ocean) and the density of population"},{"metadata":{},"cell_type":"markdown","source":"### Looking for Correlations"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the correlation of the target variable (median housing price) with all other features\ncorr_matrix = housing.corr()\n\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The house value seems to be heavily correlated to the median income and weakly negatively correlated with latitude"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\n\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n\n# The scatter matrix shows this relationship visually\nscatter_matrix(housing[attributes], figsize=(24,16))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Of these attributes, the correlation with median income is most interesting\nhousing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n             alpha=0.3,figsize=(12,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's a clear upward trend between the price and income."},{"metadata":{},"cell_type":"markdown","source":"### Adding a few features based on the correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# These are some features recommended by the author and as can be seen below, the bedrooms_per_room feature could be useful\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"] \nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\nhousing[\"population_per_household\"] = housing[\"population\"]/housing[\"households\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Recompute the Correlation\ncorr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare the Data for Machine Learning Algorithms"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Revert to the clean training set for the next steps\nhousing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels since this is our training set\nhousing_labels = strat_train_set[\"median_house_value\"].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Cleaning\nThere are three strategies commonly pursued for data cleaning as shown below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting the rows with missing info\nincomplete_rows = housing[housing.isnull().any(axis=1)]\nincomplete_rows.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `total_bedrooms` feature has some NaNs so let's use that to try out different cleaning strategies"},{"metadata":{"trusted":true},"cell_type":"code","source":"incomplete_rows.dropna(subset=[\"total_bedrooms\"]) # Option 1: Drop the rows which have NaNs for this feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"incomplete_rows.drop(\"total_bedrooms\",axis=1).head() # Option 2: Drop the feature completely","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"median_value = housing[\"total_bedrooms\"].median()\nincomplete_rows[\"total_bedrooms\"].fillna(median_value,inplace=True) # Option 3: Fill with something like the median value of the feature\nincomplete_rows.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we are filling missing values, `scikit-learn`'s `Imputer` can be used instead. However, it works only with numeric attributes so the text attributes have to be removed"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_numeric = housing.drop(\"ocean_proximity\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer.fit(housing_numeric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's look at the statistics computed by the imputer\nimputer.statistics_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Does it match the statistics of the original data?\nhousing_numeric.median().values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform the values in the training set\nhousing_numeric_transformed = imputer.transform(housing_numeric) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_tr = pd.DataFrame(housing_numeric_transformed, columns=housing_numeric.columns,\n                          index=housing_numeric.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_tr.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handling Text & Categorical Attributes\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_cat = housing[[\"ocean_proximity\"]]\nhousing_cat.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Ordinal Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\nhousing_cat_enc = ordinal_encoder.fit_transform(housing_cat)\nhousing_cat_enc[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ordinal_encoder.categories_ # labels run from 0 to 4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### One Hot Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder()\nhousing_cat_one_hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_one_hot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Custom Transformers in Scikit-Learn"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# index of columns we are interested in\nrooms_idx, bedrooms_idx, population_idx, households_idx = 3, 4, 5, 6\n\n# Needs 3 functions to be defined without *args or **kwargs\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    \n    # Function 1\n    def __init__(self, add_bedrooms_per_room=True): \n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    \n    # Function 2\n    def fit(self, X, y=None):\n        return self # nothing to do here\n    \n    # Function 3 : Adding the features we tried out earlier\n    def transform(self, X):\n        rooms_per_household = X[:,rooms_idx]/X[:,households_idx]\n        population_per_household = X[:, population_idx]/X[:, households_idx]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:,bedrooms_idx]/X[:,rooms_idx]\n            return np.c_[X, rooms_per_household,population_per_household,\n                         bedrooms_per_room]\n        else:\n            return np.c_[X,rooms_per_household,population_per_household]\n        \nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_extra_attribs = pd.DataFrame(housing_extra_attribs,\n                                     columns=list(housing.columns) + [\"rooms_per_household\", \"population_per_household\"],\n                                     index=housing.index)\n\nhousing_extra_attribs.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transformation Pipelines\nA pipeline allows us to put all the preprocessing steps in sequence and perform them in the right order"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Putting together all the steps of data processing into a pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler # For feature scaling so that all numeric attributes are in the same range\n\nnumeric_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy=\"median\")), # First Do this\n    ('attribs_adder', CombinedAttributesAdder()), # Next Do this\n    ('std_scaler', StandardScaler()), # Finally do this\n])\n\nhousing_numeric_tr = numeric_pipeline.fit_transform(housing_numeric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_numeric_tr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But what about the `ocean_proximity` which is non-numeric?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\nnumeric_attr = list(housing_numeric)\ncat_attr = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n    (\"num\", numeric_pipeline, numeric_attr), # One pipeline for numeric data\n    (\"cat\", OneHotEncoder(), cat_attr), # One pipeline for categorical data\n])\n\nhousing_prepared = full_pipeline.fit_transform(housing) # One pipeline to rule them all ;) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_prepared","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_prepared.shape, housing.shape # Note that the extra columns are the features we added plus 5 for one-hot encoding","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Select and Train a Model\nNow that the data's prepared, let's use it to train some models and check performance"},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate on a few training examples\nsome_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data) # Pre-process the sample in the same way \nprint(\"Predictions: \", lin_reg.predict(some_data_prepared))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Labels: \", list(some_labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluate the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\n# How good is the model on the training set? Look's like it's underfitting quite a bit\nlin_train_preds = lin_reg.predict(housing_prepared)\n\nlin_reg_mse = mean_squared_error(lin_train_preds, housing_labels)\nlin_reg_rmse = np.sqrt(lin_reg_mse)\nlin_reg_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluate the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_train_preds = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(tree_train_preds, housing_labels)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse # The Decision Tree is clearly overfitting!","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nforest_reg.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluate the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_train_preds = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(forest_train_preds, housing_labels)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse # Random Forest seems a lot better!","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fine Tune your Model"},{"metadata":{},"cell_type":"markdown","source":"### Cross-Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_scores(scores):\n    print(\"Scores: \", scores)\n    print(\"Mean: \", scores.mean())\n    print(\"Standard Deviation: \", scores.std())\n    \ndisplay_scores(tree_rmse_scores) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                              scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores) # Decision Tree performs worse than Linear Regression!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                                scoring=\"neg_mean_squared_error\", cv=10, n_jobs=-1)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Grid Search \nFind the best hyperparameters for the problem by trying out all combinations that you specify"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Specify the parameters you are interesting in trying out\nparam_grid = [\n    {'n_estimators': [3, 10, 30], 'max_features': [2,4,6,8]}, # Try a 3x4 combination first\n    {'bootstrap': [False], 'n_estimators': [3,10], 'max_features': [2,3,4]}, # Then try a 2x3 combination with bootstrapping set to False\n]\n\nforest_reg = RandomForestRegressor(random_state=42)\n\n# Train Across 5 folds so totally (12 + 6) * 5 = 90 rounds of training\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                          scoring=\"neg_mean_squared_error\",\n                          return_train_score=True,\n                          n_jobs=-1)\n\ngrid_search.fit(housing_prepared, housing_labels)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best hyperparameter combination can be found as:"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And the best estimator as:"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So what are the scores of each combination we tested?"},{"metadata":{"trusted":true},"cell_type":"code","source":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Randomized Search\nGive a range for each hyperparameter and it tries out a random combination of each hyperparameter by choosing random values for each one within the range"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n    'n_estimators': randint(low=1, high=200),\n    'max_features': randint(low=1, high=8 ),\n}\n\nforest_reg = RandomForestRegressor(random_state=42)\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n                               n_iter=10, cv=5, scoring=\"neg_mean_squared_error\", # Note that we've asked it to run 10 times\n                               random_state=42, n_jobs=-1)\nrnd_search.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarly, we can view the combination of cv scores and parameters that got them"},{"metadata":{"trusted":true},"cell_type":"code","source":"cvres = rnd_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analyze the Best Models and Their Errors\nLet's take a look at the feature importances of the best model so far, i.e., the Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\ncat_encoder = full_pipeline.named_transformers_[\"cat\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = numeric_attr + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True) # Let's look at the features in their order of importance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the Model on the Test Set\nLet's see how our optimized model does on the data that actually matters!"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\nY_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(final_predictions, Y_test)\nfinal_rmse = np.sqrt(final_mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's compute a 95% confidence interval on our result\nfrom scipy import stats\n\nconfidence=0.95\nsquared_errors = (final_predictions - Y_test)**2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, \n                         loc=squared_errors.mean(),\n                         scale=stats.sem(squared_errors)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exercises\n"},{"metadata":{},"cell_type":"markdown","source":"### 1. Try a Support Vector Machine Regressor (SVR) with various hyperparameters. How does the best SVR predictor perform?\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.svm import SVR\n\nparam_grid_svm = [\n    {'kernel': ['linear'], 'C': [10., 30., 100.]},\n    {'kernel': ['rbf'], 'C': [1.0, 3.0, 10., 30.], \n     'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},    \n]\n\nsvm_reg = SVR()\ngrid_search_svm = GridSearchCV(svm_reg, param_grid_svm, cv=5, scoring=\"neg_mean_squared_error\", verbose=2, n_jobs=-1)\ngrid_search_svm.fit(housing_prepared, housing_labels)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsvm_mse = grid_search_svm.best_score_\nsvm_rmse = np.sqrt(-svm_mse)\nsvm_rmse # Looks much worse than the Random Forest Regressor\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Best Parameters for SVM Regressor\ngrid_search_svm.best_params_\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Replace GridSearchCV with RandomizedSearchCV\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom scipy.stats import expon, reciprocal\n\nparam_distribs_svm = {\n    'kernel': ['linear', 'rbf'],\n    'C': reciprocal(20, 2000),\n    'gamma': expon(scale=1.0),\n}\n\nsvm_reg = SVR()\nrnd_search_svm = RandomizedSearchCV(svm_reg, param_distributions=param_distribs_svm,\n                                n_iter=26, cv=5, scoring='neg_mean_squared_error', \n                                verbose=2, random_state=42, n_jobs=-1)\nrnd_search_svm.fit(housing_prepared, housing_labels)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsvm_mse = rnd_search_svm.best_score_\nsvm_rmse = np.sqrt(-svm_mse)\nsvm_rmse # Seems a lot better right?\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrnd_search_svm.best_params_\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Try adding a transformer in the preparation pipeline to select only the most important attributes\nLet's first extract the \"Top K\" indices and then incorporate that into a custom transformer"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see if we still have this data from the Random Forest we ran a while back\nfeature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Method to extract indices of the top-K most important features\ndef extract_top_k_indices(importances, k):\n    # Get the indices we care about\n    return np.sort(np.argpartition(importances,-k)[-k:]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k=5\ntop_k_indices = extract_top_k_indices(feature_importances, k)\ntop_k_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = np.array(attributes)[top_k_indices]\nfeature_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(zip(feature_importances, attributes), reverse=True)[:k]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have the right features selected, so let's move on"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TopKFeatureSelector(BaseEstimator, TransformerMixin):\n    # function 1\n    def __init__(self, feature_importances, k):\n        self.k = k\n        self.feat_imp = feature_importances\n    # function 2\n    def fit(self, X, y=None):\n        self.top_k_indices = extract_top_k_indices(self.feat_imp, self.k)\n        return self\n    # function 3\n    def transform(self, X):\n        return X[:, self.top_k_indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_select_pipeline = Pipeline([\n    ('preparation', full_pipeline), # same one as before\n    ('feature_selection', TopKFeatureSelector(feature_importances, k)) # New block \n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_top_k_prepared = feat_select_pipeline.fit_transform(housing)\nhousing_top_k_prepared[:3] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_prepared[0:3, top_k_indices] # Check to see if we got it right","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Try creating a single pipeline that does the full data preparation plus the final prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"end_to_end_pipeline = Pipeline([\n    ('preparation', full_pipeline), # Prepare the Data\n    ('feature_selection', TopKFeatureSelector(feature_importances, k)), # Choose the best features\n    ('svm_reg', SVR(**rnd_search_svm.best_params_)) # Make Predictions\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"end_to_end_pipeline.fit(housing, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_data = housing.iloc[:5]\nsample_labels = housing_labels.iloc[:5]\n\nprint(\"Predictions: \", end_to_end_pipeline.predict(sample_data))\nprint(\"True Labels: \", list(sample_labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Automatically explore some preparation options using GridSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid_auto = [{\n    'preparation__num__imputer__strategy': ['median', 'most_frequent']\n}]\n\nauto_pipeline = Pipeline([\n    ('preparation', full_pipeline),\n    ('svm_reg', SVR(**rnd_search_svm.best_params_))\n])\n\ngrid_search_prep = GridSearchCV(auto_pipeline, param_grid_auto, cv=5,\n                                scoring='neg_mean_squared_error', verbose=2)\ngrid_search_prep.fit(housing, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search_prep.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}