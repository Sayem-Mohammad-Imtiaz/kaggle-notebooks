{"cells":[{"metadata":{"_uuid":"002a3721-3e7c-46e4-acd7-6398c6990593","_cell_guid":"ea81a299-8204-4d49-b655-fca36c3bfb0c","trusted":true},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Apr 29 15:53:02 2020\n\n@author: hp\n\"\"\"\n\n# Code by PRATHISH  MURUGAN\n# 29 April 2020\n\n#Pima-Indians-Diabetes\n\n#This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases.  \n#he datasets consist of several medical predictor (independent) variables and \n#one target (dependent) variable, Outcome. \n#Independent variables include the number of pregnancies the patient has had, their BMI, \n#insulin level, age, and so on.\n\n#This dataset can be found here -> https://www.kaggle.com/uciml/pima-indians-diabetes-database\n\n#Acknowledgements :-\n#Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). \n#Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. \n#In Proceedings of the Symposium on Computer Applications and \n#Medical Care (pp. 261--265).IEEE Computer Society Press.\n\n#This is a classifiaction problem and the dataset is not clean\n\n#This problem looks easy bit it gets hard because of the unclean and unclean dataset \n\nimport pandas as pd                     #data processing\nimport numpy as np                      #liner algebra\nimport matplotlib.pyplot as plt         #graphs\nimport seaborn as sns                   #visualizations\n\n#Imporing the CSV dataset\npima_main=pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv',na_values=['??','????'])\n\n#Making a copy of the original dataset for working\npima_work=pima_main.copy(deep=True)\n\nprint(pima_work.head())\nprint(pima_work.shape)\n\npima_work.describe()   # Gives the basic mean,meadian and quantas og each columns\n#Note atleast 25% of readings for insulin and skinthickness are 0. Min readings \n#for columns like Glucose, Bloodpressure and BMI are zero too which does not seem appropriate\n\npima_main.info()\npima_work.isnull().sum()     #to check any null values in any columns\n#There are no null values in the dataset\n\n#Now we have to check if there are any 0's in the set\npima_work.isin([0]).sum()\n# Seems like there are a lot of zeros and 0's in glucose,BP,BMI is not logical\n#We need to fill these 0's with some appropirate values\n\npima_work.Outcome.value_counts()\n#Seems like the Outcome is not Balanced\n\n# replace all zeros with NANs. I do this so that when means are calculated, zeros are not counted.\npima_nan = pima_work.replace({\n            'Glucose': 0,\n            'BloodPressure' : 0,\n            'SkinThickness' : 0,\n            'BMI' : 0 ,\n            'Insulin' : 0,\n        },np.NaN)                     \n\n\npima_nan.isin([0]).sum()\n\n#We have replaced all the unwanted 0's with NaN \n#Calcutating the mean of each columns\npima_nan.mean()\npima_nan.median()\n#Now we will try to replace this NaN with mean or median\npima_nan.isnull().sum()\npima_nan=pima_nan.fillna(pima_nan.mean())\npima_nan.isnull().sum()\n#Now we have filled the NaN values with the mean of each coloums\n\n#shuffle the dataset\nfrom sklearn.utils import shuffle\npima_nan = shuffle(pima_nan)\n\n# I want to see how the imputation process has affected these values. \n# I will follow this up with some visualizations.\n\npima_nan.groupby('Outcome').mean().transpose()\n\n#Now we see the correlation bettween the each of the features\npima_corr=pima_nan.corr()\n\n#Creating a Heatmap\nplt.figure(figsize=(9,9))\nsns.heatmap(pima_corr,cmap='coolwarm',annot=True,linewidths = 0.5)\n\n#From this heatmap we can see that Glucose is having the greatest\n#effect on outcome followed by BMI which is self-explanatory.\n\n# Histograms for imputed data.\n\nfor cols in pima_nan.columns:\n    x = pima_nan.loc[:,cols]\n    plt.hist(x)\n    plt.title('Histogram for Feature' +str(cols))\n    plt.show()\n# End of Visualizations\n    \n#Creating a test and train DF    \n    X = pima_nan.drop('Outcome',axis=1)\n    y = pima_nan['Outcome'].values\n    \n#Let's split the data randomly into training and test set\n    #importing train_test_split\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.2,random_state=42)\n\n#ML MODELS\n\n# MODEL 1 :- KNN CLASSIFICATION\n\n#Create-KNN-model\nfrom sklearn.neighbors import KNeighborsClassifier\nKNN = KNeighborsClassifier(n_neighbors = 60)       #n_neighbors = K value\n\n#K-fold CV\nfrom sklearn.model_selection import cross_val_score\naccuraccies = cross_val_score(estimator = KNN, X= X_train, y=y_train, cv=10)\nprint(\"Average Accuracies: \",np.mean(accuraccies))\nprint(\"Standart Deviation Accuracies: \",np.std(accuraccies))\n\n#The values I got for KNN at the time of coding is \n#Average Accuracies:  0.7664285714285715\n#Standard Deviation Accuracies:  0.09502140527437733 \n\nKNN.fit(X_train,y_train) #learning model\nKNN.score(X_test, y_test)\n\n#The score I got while writing the score \n#Score :-  0.751219512195122\n\ny_predict = KNN.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\nCM = confusion_matrix(y_test,y_predict)\n\n#CM visualization\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(CM,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytest\")\nplt.show()\n\n# MODEL 2 :- SVM\n\n#Create SVM Model\nfrom sklearn.svm import SVC\n\nSVM = SVC(random_state=42)\n\n#K-fold CV\nfrom sklearn.model_selection import cross_val_score\naccuraccies = cross_val_score(estimator = SVM, X= X_train, y=y_train, cv=10)\nprint(\"Average Accuracies: \",np.mean(accuraccies))\nprint(\"Standard Deviation Accuracies: \",np.std(accuraccies))\n\n#The values I got for SVM at the time of coding is \n#Average Accuracies:  0.6410714285714286\n#Standard Deviation Accuracies:  0.018064274887492272\n\nSVM.fit(X_train,y_train)  #learning \n#SVM Test \nprint (\"SVM Accuracy:\", SVM.score(X_test,y_test))\n\nSVMscore = SVM.score(X_test,y_test)\n# Score :-  0.6536585365853659\n\n#Confusion Matrix\n\nyprediciton3= SVM.predict(X_test)\nytrue = y_test\n\nfrom sklearn.metrics import confusion_matrix\nCM = confusion_matrix(ytrue,yprediciton3)\n\n#CM visualization\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(CM,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()\n\n# So this is a machine learning model to accurately predict whether or not the\n# patients in the dataset have diabetes or not\n\n# Hope you find it useful\n# Corections and suggestions are welcomed\n\n# BY PRATHISH MURUGAN\n# 29 - APRIL - 2020","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}