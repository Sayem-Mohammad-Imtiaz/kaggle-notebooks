{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Sentiment Analysis Using ML-Agorithms with Data Augumentation\n## What is NLP?\n> Natural Language Processing is also abbrevated as NLP.Whatever we speakor write is understandable to a human,which becomes very difficult for a computer to decode from the text/speech that we write/speak. To make it a computer understanable,we basically process the texts into a number, as we know computer can understand only the number,such that we can apply this in the machine learning Algorithms.  \n![NLP](https://venturebeat.com/wp-content/uploads/2018/09/natural-language-processing-e1572968977211.jpg?fit=578%2C289&strip=all)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![source](https://cdn-images-1.medium.com/max/1000/1*Uf_qQ0zF8G8y9zUhndA08w.png)\n> The above shows the venn diagram of NLP","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Import Library","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install nlpaug","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer,WordNetLemmatizer\nimport textblob\nfrom textblob import TextBlob, Word\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### read the dataset using read_csv() of pandas library","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"traindata=pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/train.csv')\ntestdata=pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testdata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindata['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it is unbalanced data as we could see there is only 7% which accounts positive class while rest are negative class","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nsns.countplot(traindata['label'])\nplt.title('Class-Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Augmentation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### What is Data Augumentation?\n\n> Data augumentation is a technique to overcome the imbalance in the target label of the dataset.Most of us must have augumented image data either by rotating the image,zooming,adding noises,etc, By doing these we basically increase the data.For images, there is a class in keras (ImageDatagenerator()) which helps to produce new images. For the text data, I have used nlpaug library.for better understandin you could read the blog\n>>https://towardsdatascience.com/data-augmentation-library-for-text-9661736b13ff\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nlpaug.augmenter.sentence as nas\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can give your own api key","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"WANDB_API_KEY='sonu'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug = nas.ContextualWordEmbsForSentenceAug(model_path='xlnet-base-cased')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text='am not interested in linguistics that does not address race racism is about power raciolinguistics brings'\naugmented_texts = aug.augment(text, n=3)\nprint(\"Original:\")\nprint(text)\nprint(\"Augmented Texts:\")\nprint(augmented_texts)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"augmented_texts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls=[]\ndef data_augument(df):\n    augmented_texts = aug.augment(df, n=10)\n    for i in augmented_texts:\n        ls.append(i)\n    return(augmented_texts)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm._tqdm_notebook import tqdm_notebook\ntqdm_notebook.pandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindata[traindata['label']==1]['tweet'].progress_apply(data_augument)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"array=np.array(ls)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save('array',array)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"augmented_texts=np.load('../input/array-file/array.npy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug_data=pd.DataFrame(augmented_texts,columns=['tweet'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug_data['label']=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindata=pd.concat([traindata.drop(columns=['id']), aug_data], join=\"outer\").sample(frac=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After augmnetation the dataset is now balanced","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.pie(traindata['label'].value_counts(), autopct='%1.1f%%', shadow=True,labels=['Negative Class','Positive Class'])\nplt.title('Class Distribution');\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindata['preclean_no_words']=  [len(t) for t in traindata.tweet]\nsns.boxplot(traindata.preclean_no_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#traindata['preclean_no_words']=  [len(t) for t in traindata.tweet]\nsns.boxplot(traindata[traindata['label']==0].preclean_no_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(traindata[traindata['label']==1].preclean_no_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindata['no_of_characters']=traindata['tweet'].str.len()\ntraindata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nplt.figure(figsize=(8,8))\nsns.distplot(traindata['no_of_characters'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindata['no_of_words']=traindata['tweet'].apply(lambda x: len(str(x).split(\" \")))\ntraindata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.distplot(traindata['no_of_words'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(traindata['no_of_characters'].max(),\"Max'm of all characters\")\nprint(traindata['no_of_words'].max(),\"Max'm of all words\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindata['no_of_hash']=traindata['tweet'].apply(lambda x:len([x for x in x.split() if x.startswith('#')]))\ntraindata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testdata['no_of_hash']=testdata['tweet'].apply(lambda x:len([x for x in x.split() if x.startswith('#')]))\ntestdata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.distplot(traindata['no_of_hash'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindata['no_of_digits']=traindata['tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\ntraindata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testdata['no_of_digits']=testdata['tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\ntestdata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.countplot(traindata['no_of_digits'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Text Pre-Processing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_process=traindata.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Lower Case","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_process['tweet_lowercase'] = train_process['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ntrain_process[['tweet', 'tweet_lowercase']].tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testdata['tweet_lowercase'] = testdata['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ntestdata[['tweet', 'tweet_lowercase']].tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Remove the stopwords","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = stopwords.words('english')+['0624',\n '07800',\n '07950',\n '08a',\n '100',\n '1000',\n '1000x',\n '100k',\n '101',\n '106',\n '10alltypespos',\n '10am',\n '10days',\n '10k',\n '10th',\n '1117',\n '11400',\n '11th',\n '1200',\n '123',\n '12313',\n '1299',\n '12mill',\n '13479',\n '13th',\n '13thdocumentary',\n '140',\n '14000',\n '14200',\n '142017',\n '148',\n '1499',\n '14th',\n '1500',\n '15000',\n '150516',\n '15thcentury',\n '160',\n '1600',\n '1625',\n '17th',\n '180',\n '18th',\n '1900',\n '190k',\n '1930s',\n '1960',\n '1968',\n '1970',\n '1980',\n '1996',\n '1999',\n '19th',\n '1gabba',\n '1pun',\n '1st',\n '1stammendment',\n '2',\n '200',\n '2000',\n '2001',\n '2002',\n '2003',\n '2004',\n '2006',\n '2008',\n '2009',\n '200k',\n '2010',\n '2011',\n '2012',\n '2013',\n '2014',\n '2015',\n '2016',\n '201617',\n '2016a',\n '2016election',\n '2016highlights',\n '2016ia',\n '2016in4a',\n '2016in4worda',\n '2016in4words',\n '2016in4wordsa',\n '2016in4worlds',\n '2016ina',\n '2016release',\n '2017',\n '2017fail',\n '2017in3words',\n '2017in3wordsa',\n '2017npr',\n '2018',\n '201a',\n '20days',\n '20th',\n '2100',\n '21st',\n '230pmet',\n '23rd',\n '247',\n '24h',\n '24hrs',\n '24th',\n '25th',\n '26th',\n '280',\n '299',\n '2a',\n '2day',\n '2days',\n '2i',\n '2ia1',\n '2k16',\n '2nd',\n '2nites',\n '2nnð3',\n '2pac',\n '2pm',\n '2raise',\n '2stand',\n '2the',\n '2ð',\n '2ðn',\n '2ðð',\n '2ðð1',\n '2ððð1',\n '2ðððð1',\n '30th',\n '342',\n '350',\n '35th',\n '360',\n '38billion',\n '399',\n '3rd',\n '4',\n '400',\n '400000',\n '40404',\n '41',\n '4a',\n '4a1',\n '4aa1',\n '4ai',\n '4aið',\n '4ejapan',\n '4i',\n '4i1',\n '4maps',\n '4nð3',\n '4o3',\n '4o4o4',\n '4pm',\n '4sa',\n '4th',\n '4wd',\n '4æa',\n '4ð',\n '4ð1',\n '4ð3',\n '4ðað',\n '4ðo3',\n '4ðð',\n '4ðð1',\n '4ðð3',\n '4ððð',\n '4ððð1',\n '4ððð3',\n '4ðððð',\n '4ðððμð1',\n '4μ',\n '4μo3',\n '50',\n '500',\n '50islamicinfo',\n '50th',\n '564943',\n '5hrs',\n '5sos',\n '5th',\n '5wordtrumplethinskin',\n '60',\n '600',\n '60minutes',\n '630',\n '6417153640',\n '642',\n '6pm',\n '6th',\n '6yearolds',\n '70',\n '700',\n '700000',\n '703',\n '799',\n '800',\n '80snostalgia',\n '80yrold',\n '8990',\n '8pm',\n '8th',\n '900',\n '90th',\n '911',\n '940pm',\n '946',\n '952',\n '999',\n '99c',\n '99c99p',\n '99c99pa',\n '99ca',\n '99cents',\n '99p',\n '9am',\n '9pm',\n '9th',\n '__luicalibre__s',\n '_animaladvocate',\n '_øuøu',\n 'a1',\n 'a15',\n 'a17',\n 'a1aaaa',\n 'a1i',\n 'aa1',\n 'aaa',\n 'aaaa',\n 'aaaaa',\n 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n 'aaahis',\n 'aaall',\n 'aaaplay',\n 'aampe',\n 'aand',\n 'aande',\n 'aanne',\n 'aantiislamista',\n 'aap',\n 'aape',\n 'aaron',\n 'abandoned',\n 'abba',\n 'abc',\n 'abd',\n 'abe',\n 'abeed']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_process['tweet_stopwords'] = train_process['tweet_lowercase'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\ntrain_process[['tweet_stopwords','tweet']].head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testdata['tweet_stopwords'] = testdata['tweet_lowercase'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\ntestdata[['tweet_stopwords','tweet']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Remove the punctuations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_process['tweet_punctuation'] = train_process['tweet_stopwords'].str.replace('[^\\w\\s]', '')\ntrain_process[['tweet', 'tweet_punctuation']].head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testdata['tweet_punctuation'] = testdata['tweet_stopwords'].str.replace('[^\\w\\s]', '')\ntestdata[['tweet', 'tweet_punctuation']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Remove the single letter present","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_process['tweet_single_letter']=train_process['tweet_punctuation'].apply(lambda words: ' '.join( [w for w in words.split() if len(w)>2] ))\ntrain_process[['tweet_single_letter','tweet']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testdata['tweet_single_letter']=testdata['tweet_punctuation'].apply(lambda words: ' '.join( [w for w in words.split() if len(w)>2] ))\ntestdata[['tweet_single_letter','tweet']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Top 25 and Least 25 words shown","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('top 25 used words')\nprint('-----------------')\nprint(pd.Series(''.join(train_process['tweet_single_letter']).split()).value_counts()[0:25])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('least 25 used words')\nprint('-------------------')\nprint(pd.Series(''.join(train_process['tweet_single_letter']).split()).value_counts()[-25:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##train_process['tweet_correct']=train_process['tweet_single_letter'].progress_apply(lambda x: str(TextBlob(x).correct()))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Lemmatize the sentences","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_process['tweet_lemma']=train_process['tweet_single_letter'].progress_apply(lambda words: ' '.join( [WordNetLemmatizer().lemmatize(w) for w in words.split()]))\ntrain_process[['tweet_lemma','tweet']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testdata['tweet_lemma']=testdata['tweet_single_letter'].progress_apply(lambda words: ' '.join( [WordNetLemmatizer().lemmatize(w) for w in words.split()]))\ntestdata[['tweet_lemma','tweet']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Train-Test Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(train_process['tweet_lemma'],train_process['label'],test_size=0.33,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### TF-IDF\n>> TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1,1), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature.fit(X_train) \nx_train=feature.transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testdata=feature.transform(testdata['tweet_lemma'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test=feature.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#feature.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### the evaluation metric is F-1 score","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\nfrom sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, classification_report\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'C': np.arange(20,30,2),\n              'max_iter': np.arange(100,1200,100),\n              'penalty': ['l1','l2']}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = StratifiedKFold(n_splits=10,random_state=1,shuffle=True)\n\ni=1\n\n\nfor train_index,test_index in kf.split(x_train,y_train):\n    print('\\n{} of kfold {}'.format(i,kf.n_splits))\n    xtr,xvl = x_train[train_index],x_train[test_index]\n    ytr,yvl = y_train.iloc[train_index],y_train.iloc[test_index]\n    #print(train_index)\n    model = RandomizedSearchCV(estimator=LogisticRegression(class_weight='balanced'),param_distributions=param_grid,verbose=1)\n    \n\n    model.fit(xtr, ytr)\n    #print (model.best_params_)\n    pred=model.predict(xvl)\n    print('roc_auc_score',roc_auc_score(yvl,pred))\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('best parameters',model.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_logistic = roc_auc_score(yvl,pred).mean()\nf1_logistic = f1_score(yvl,pred).mean()\nprint('Mean - ROC AUC', roc_auc_logistic)\nprint('F1 Score - ', f1_logistic)\nprint('Confusion Matrix \\n',confusion_matrix(yvl,pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#DecisionTree with tuned hyperparameters\nfrom sklearn.tree import DecisionTreeClassifier\nstart_time = time.time()\nparam_grid = {'criterion': ['gini','entropy'],\n             'min_samples_split':[50,70,100,150],\n             'max_features': ['sqrt','log2']}\n\n\ni=1\nkf = StratifiedKFold(n_splits=10,random_state=1,shuffle=True)\nfor train_index,test_index in kf.split(x_train,y_train):\n    print('\\n{} of kfold {}'.format(i,kf.n_splits))\n    xtr,xvl = x_train[train_index],x_train[test_index]\n    ytr,yvl = y_train.iloc[train_index],y_train.iloc[test_index]\n    \n    model = RandomizedSearchCV(estimator=DecisionTreeClassifier(class_weight={0:1,1:5}),param_distributions=param_grid,verbose=1)\n    \n\n    model.fit(xtr, ytr)\n    #print (model.best_params_)\n    pred=model.predict(xvl)\n    print('roc_auc_score',roc_auc_score(yvl,pred))\n    i+=1\n\nprint(\"Execution time: \" + str((time.time() - start_time)) + ' ms')\nprint ('best parameters',model.best_params_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model Accuracy\nroc_auc_dt = roc_auc_score(yvl,pred).mean()\nf1_dt = f1_score(yvl,pred).mean()\nprint('Mean - ROC AUC', roc_auc_dt)\nprint('F1 Score - ', f1_dt)\nprint('Confusion Matrix \\n',confusion_matrix(yvl,pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nstart_time = time.time()\nparam_grid = {'criterion': ['entropy'],\n             'min_samples_split':np.arange(10,100,20),\n             'max_features': ['sqrt'],\n             'n_estimators':[10,20,30]}\n\ni=1\nkf = StratifiedKFold(n_splits=10,random_state=1,shuffle=True)\nfor train_index,test_index in kf.split(x_train,y_train):\n    print('\\n{} of kfold {}'.format(i,kf.n_splits))\n    xtr,xvl = x_train[train_index],x_train[test_index]\n    ytr,yvl = y_train.iloc[train_index],y_train.iloc[test_index]\n    \n    model = RandomizedSearchCV(estimator=RandomForestClassifier(),param_distributions=param_grid,verbose=1)\n    \n\n    model.fit(xtr, ytr)\n    #print (model.best_params_)\n    pred=model.predict(xvl)\n    print('roc_auc_score',roc_auc_score(yvl,pred))\n    i+=1\n\nprint(\"Execution time: \" + str((time.time() - start_time)) + ' ms')\nprint ('best parameters',model.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model Accuracy\nroc_auc_rf = roc_auc_score(yvl,pred).mean()\nf1_rf = f1_score(yvl,pred).mean()\nprint('Mean - ROC AUC', roc_auc_rf)\nprint('F1 Score - ', f1_rf)\nprint('Confusion Matrix \\n',confusion_matrix(yvl,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest'],\n    'Mean - ROC AUC Score (Fold=10)': [roc_auc_logistic, roc_auc_dt, roc_auc_rf],\n    'Mean - F1 Score': [f1_logistic,f1_dt,f1_rf]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic=LogisticRegression(penalty='l2',max_iter=100,C=28)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testdata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic.fit(x_train,y_train)\npred=logistic.predict(testdata.toarray())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub=pd.DataFrame(testdata['id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['label']=pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('sub.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hope you liked it!!\nDO upvote it!!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}