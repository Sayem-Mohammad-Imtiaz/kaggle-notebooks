{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install bs4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# HTML Parsing NHANES Data Codebook with SAS label\n### Li-Chia Chen\n### 27Feb2021"},{"metadata":{},"cell_type":"markdown","source":"## Purpose\nWhile I am doing a project using the NHANES dataset, I find that it was hard to locate the desired variable and difficult to tell the meaning of each variable after feature selection. Therefore, I decided to parse the information from the NHANES website using **Beautiful Soup**. \n\nThere is already one codebook with detailed description here: https://www.kaggle.com/cdc/national-health-and-nutrition-examination-survey/discussion/47796\n\n\nIn this notebook the main purpose is to extract the sas labels from the data documentations websites in the 5 main categories:\n- Demographics: https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Demographics&CycleBeginYear=2013\n- Dietary: https://wwwn.cdc.gov/nchs/nhanes/Search/DataPage.aspx?Component=Dietary&CycleBeginYear=2013\n- Examination: https://wwwn.cdc.gov/nchs/nhanes/Search/DataPage.aspx?Component=Examination&CycleBeginYear=2013\n- Laboratory: https://wwwn.cdc.gov/nchs/nhanes/Search/DataPage.aspx?Component=Laboratory&CycleBeginYear=2013\n- Questionnaire: https://wwwn.cdc.gov/nchs/nhanes/Search/DataPage.aspx?Component=Questionnaire&CycleBeginYear=2013\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nimport regex as re\nimport urllib","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef parse_main(URL, links, category):\n    page = requests.get(URL)\n    soup = BeautifulSoup(page.content, 'html.parser')\n    table = soup.find('table')\n\n    for link in table.find_all('a'):\n        if str(link.get('href')).endswith('.htm') == True:\n            link_j = urllib.parse.urljoin('https://wwwn.cdc.gov/', link.get('href'))\n            links[category].append(link_j)\n\n\nurls = {'DM':'https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Demographics&CycleBeginYear=2013',\n        'DIET':'https://wwwn.cdc.gov/nchs/nhanes/Search/DataPage.aspx?Component=Dietary&CycleBeginYear=2013',\n        'EXAM':'https://wwwn.cdc.gov/nchs/nhanes/Search/DataPage.aspx?Component=Examination&CycleBeginYear=2013',\n        'LAB':'https://wwwn.cdc.gov/nchs/nhanes/Search/DataPage.aspx?Component=Laboratory&CycleBeginYear=2013',\n        'QUES':'https://wwwn.cdc.gov/nchs/nhanes/Search/DataPage.aspx?Component=Questionnaire&CycleBeginYear=2013'}\n\nlinks = {v:[] for v in urls.keys()}\n\nfor c, URL in urls.items():\n    print(c, URL)\n    parse_main(URL, links, c)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def parse_nhanes(links, codes):\n    for c, URLs in links.items():\n        for URL in URLs:\n            # access webs site\n            page = requests.get(URL)\n\n            # parse data\n            soup = BeautifulSoup(page.content, 'html.parser')\n            containers = soup.find_all('dl')\n            for i in containers:\n                try:\n                    varname = str(i.find(\"dt\",string=\"Variable Name: \").findNext(\"dd\").text)\n                    saslabel = str(i.find(\"dt\",string=\"SAS Label: \").findNext(\"dd\").text)\n#                     print(varname, saslabel)\n                    codes['category'].append(c)\n                    codes['variable'].append(varname.strip())\n                    codes['label'].append(saslabel.strip())\n                except:\n#                     print(f'error in {URL} {i}')\n                    pass\n    return codes\n\ncodes = {\"category\": [], \"variable\": [], \"label\": []}\n\n\nparse_nhanes(links, codes)\n\n\ncodebook = pd.DataFrame(codes)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"codebook.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the value_counts() above, you can see that there are several repeated varaibles due to the data design for the NHANES dataset. To easily match each variable I have list the unique variables separately."},{"metadata":{"trusted":true},"cell_type":"code","source":"code_unique = codebook[['variable', 'label']].drop_duplicates(subset=['variable'])\nprint(code_unique)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"code_unique.to_csv('nhanes_2013_2014_codebook.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}