{"cells":[{"metadata":{"collapsed":true,"_uuid":"21252e73e6e7cf8fda3195fff9ec9a3c0356765f"},"cell_type":"markdown","source":"# How to predict House Prices and hopefully become a Property Tycoon in New York"},{"metadata":{"_uuid":"23b8cb0fc7d4044b191970294e6a76403dd1c95e"},"cell_type":"markdown","source":"I am going to clean and visualise data and build a model to predict housing prices in New York. Everything is done obviously with the aim of becoming a property tycoon - one of the most important things to know in order to achieve that is the value of a property. So without further ado, let's go for it."},{"metadata":{"_uuid":"73a5e3d67ae6cf00f88a7854cc127691daa27b20"},"cell_type":"markdown","source":"# 1) Load Data and Clean it"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"49fa404a164775dabb34529b9c2241bc33b9e091"},"cell_type":"code","source":"# Import the modules\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport sklearn as sk\nimport itertools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings \nwarnings.filterwarnings('ignore')\n%matplotlib inline\nsns.set(style='white', context='notebook', palette='deep') \nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\n\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.model_selection import GridSearchCV\n\n# Data Scaler\nfrom sklearn.preprocessing import StandardScaler\n\n# Regression\nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import ElasticNet\n\n# Metrics\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score","execution_count":1,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"5785784df2a96bd005caf75ceb5cd4963b97a6e6"},"cell_type":"code","source":"# Read the data\ndata = pd.read_csv('../input/nyc-rolling-sales.csv')","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"79cc85a8fefeeff741fc2f8cda5963c3760974d8"},"cell_type":"markdown","source":"Let's update the **BOROUGH** names first according to the instructions found on Kaggle."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"ad488ed307fb189b06ff8e0012264c265cab9918"},"cell_type":"code","source":"# Renaming BOROUGHS\ndata['BOROUGH'][data['BOROUGH'] == 1] = 'Manhattan'\ndata['BOROUGH'][data['BOROUGH'] == 2] = 'Bronx'\ndata['BOROUGH'][data['BOROUGH'] == 3] = 'Brooklyn'\ndata['BOROUGH'][data['BOROUGH'] == 4] = 'Queens'\ndata['BOROUGH'][data['BOROUGH'] == 5] = 'Staten Island'","execution_count":3,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c38b637c35f9714bf97d6de5447486a7b798bdb7"},"cell_type":"code","source":"# Change the settings so that you can see all columns of the dataframe when calling df.head()\npd.set_option('display.max_columns',999)\ndata.head()","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"28f6142d37e55d8fa2ed7795e64b6942ee82f405"},"cell_type":"markdown","source":"What we can see by looking at the first few rows is that the column Unnamed: 0 is an artifact from the data load and is not needed. The column EASEMENT is completely empty and will be deleted. And there some missing Sale Prices."},{"metadata":{"_uuid":"58e018693906875c1943f3a122ac37593968688e"},"cell_type":"markdown","source":"**Update Data**"},{"metadata":{"_uuid":"5d0f49fe9ad58dbac58cd2f0d0a71f52477baec3"},"cell_type":"markdown","source":"I have already done some prior inspection of the data and am now updating the dataset by deleting columns and changing the data type of some of the variables."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"3d2a2ce24974085c1c1c7779c9dfb6fa495f7eee"},"cell_type":"code","source":"#EASE_MEANT is empty and can be dropped\ndel data['EASE-MENT']\n\n# Unnamed: 0 is an artifact from the data load and can be deleted\ndel data['Unnamed: 0']","execution_count":5,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"26d417c0830d60922574904de7bbfbe3b176251b"},"cell_type":"code","source":"#SALE PRICE is object but should be numeric\ndata['SALE PRICE'] = pd.to_numeric(data['SALE PRICE'], errors='coerce')\n\n#LAND and GROSS SQUARE FEET is object but should be numeric\ndata['LAND SQUARE FEET'] = pd.to_numeric(data['LAND SQUARE FEET'], errors='coerce')\ndata['GROSS SQUARE FEET']= pd.to_numeric(data['GROSS SQUARE FEET'], errors='coerce')\n\n#SALE DATE is object but should be datetime\ndata['SALE DATE'] = pd.to_datetime(data['SALE DATE'], errors='coerce')\n\n#Both TAX CLASS attributes should be categorical\ndata['TAX CLASS AT TIME OF SALE'] = data['TAX CLASS AT TIME OF SALE'].astype('category')\ndata['TAX CLASS AT PRESENT'] = data['TAX CLASS AT PRESENT'].astype('category')","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"a70631552fdcb638be790e2d31d33f46538404c5"},"cell_type":"markdown","source":"After updating the data let's check if there are any duplicate values in here."},{"metadata":{"trusted":false,"_uuid":"62ed59d02f6242546326e0c480da7abc055910c1"},"cell_type":"code","source":"sum(data.duplicated(data.columns))","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"cb19c6838049a6e642282399c0c886b99cd84dcb"},"cell_type":"markdown","source":"There are 765 duplicates. Let's remove them"},{"metadata":{"trusted":false,"_uuid":"eac7e61c945d93852f6c79deb7a1e92906596c37"},"cell_type":"code","source":"#Delete the duplicates and check that it worked\ndata = data.drop_duplicates(data.columns, keep='last')\nsum(data.duplicated(data.columns))","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"e33eead495025ec8abac3462383ecea12c298f9f"},"cell_type":"markdown","source":"**High-Level Data Inspection and Validation**"},{"metadata":{"trusted":false,"_uuid":"63243c6670bfc4e69b3ac6e748beb6e25d8c7e05"},"cell_type":"code","source":"# Check the number of rows and columns\ndata.shape","execution_count":9,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fae200b0aaf20dbb8069eef7eba2e330d6db0a56"},"cell_type":"code","source":"# Get a high-level overview of the data types, the amount of NULL values etc.\ndata.info()","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"37ba84fd2357dadbac7253185687ca6ce719fc42"},"cell_type":"markdown","source":"Let's show this visually. It's easier that way to see where the NULL values are and how many there are."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"ec58bd370b6db171767aaf1a1a2ac8ada9153f26"},"cell_type":"code","source":"# Capture the necessary data\nvariables = data.columns\n\ncount = []\n\nfor variable in variables:\n    length = data[variable].count()\n    count.append(length)\n    \ncount_pct = np.round(100 * pd.Series(count) / len(data), 2)","execution_count":11,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6fa9d6f89f91ed36ac4657b873b3cf8f1b9e64d1"},"cell_type":"code","source":"#Plot number of available data per variable\nplt.figure(figsize=(10,6))\nplt.barh(variables, count_pct)\nplt.title('Count of available data in percent', fontsize=15)\nplt.show()","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"a28a154130f54fd3f2c3af2d5ebf0b3b99e748be"},"cell_type":"markdown","source":"That's a shame. 20% of all Sale Prices are NULL, which is what I wanted to predict. I could still predict a price for those cases but there is no way of verifying the accuracy of the predictions. Those observations will have to be deleted.\n\nThere are also around a third of all observations with missing Square Feet data. There is potential to impute those values but we will have to see how well that will work."},{"metadata":{"trusted":false,"_uuid":"02b23367bd326a5521584c08d03ff923edce1a79"},"cell_type":"code","source":"# Remove observations with missing SALE PRICE\ndata = data[data['SALE PRICE'].notnull()]\nlen(data)","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"33ff89db3089413f9a9ba016fe32cd13ab1866e2"},"cell_type":"markdown","source":"After removing the missing SALE PRICES we are left with 70k observations, down from 85k at the very start. \n\nNow, let's get an overview of some descriptive stats of the numerical variables in the data set."},{"metadata":{"trusted":false,"_uuid":"a124c2e3fd71caa21e1aff3478e307b018b3c660"},"cell_type":"code","source":"data.describe()","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"726e8fb2d1257042843d7347283c05ecedeee533"},"cell_type":"markdown","source":"Some interesting observations to note:\n\n1) There are **ZIP CODES** with a value of 0, which is probably wrong.\n\n2) 75% of properties have no **COMMERCIAL UNITS**\n\n3) At least 50% of all properties have only 1 **TOTAL UNIT**. I am not quite sure what to make of that yet.\n\n4) There are properties have 0 **SQUARE FEET**, which shouldn't be possible unless they don't exist yet or the data is wrong.\n\n5) Some buildings were built in the **YEAR** 0, which again is wrong.\n\n6) Some properties have a **SALE PRICE** of 0, which is also wrong or a transfer but not actually a sale."},{"metadata":{"_uuid":"d55ad8a194e340aa8262a831a42e51ceff151993"},"cell_type":"markdown","source":"# 2) Data Inspection"},{"metadata":{"_uuid":"a607d0730f9eaba766a4cc8a23cc2dad29668539"},"cell_type":"markdown","source":"Again, I have already done some data inspection and I will not show those variables that aren't useful for the model. I will first look at the dependent variable **SALE PRICE**, which is the one I want to predict. After that I will look at the independent variables, which are the ones I use to predict the price."},{"metadata":{"_uuid":"0281e402677b9e7415ff3580851ae9fde5ba18df"},"cell_type":"markdown","source":"## 2.1) Dependent Variable Inspection "},{"metadata":{"_uuid":"5ddf86ff20a68770fff255400f71905a9e3b93ec"},"cell_type":"markdown","source":"**SALE PRICE**"},{"metadata":{"_uuid":"bb172ebc1a761b5248a17b75cf4c973b72d0b9f6"},"cell_type":"markdown","source":"Let's start with the dependent variable as this is the one I want to predict. First, some plots."},{"metadata":{"trusted":false,"_uuid":"00681f7f96d4dceb8e40a9fd691004df89f62587"},"cell_type":"code","source":"#Set the size of the plot\nplt.figure(figsize=(15,6))\n\n# Plot the data and configure the settings\nsns.boxplot(x='SALE PRICE', data=data)\nplt.ticklabel_format(style='plain', axis='x')\nplt.title('Boxplot of SALE PRICE in USD')\nplt.show()","execution_count":15,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c8f21a2ca92f2821ce32e4f7fbe5e1edafe55328"},"cell_type":"code","source":"#Set the size of the plot\nplt.figure(figsize=(15,6))\n\n# Plot the data and configure the settings\nsns.distplot(data['SALE PRICE'])\nplt.title('Histogram of SALE PRICE in USD')\nplt.ylabel('Normed Frequency')\nplt.show()","execution_count":16,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"05bd662cbcdbefa3c39bb8d8d47bee93cf2aa279"},"cell_type":"code","source":"#Set the size of the plot\nplt.figure(figsize=(15,6))\n\n#Get the data and format it\nx = data[['SALE PRICE']].sort_values(by='SALE PRICE').reset_index()\nx['PROPERTY PROPORTION'] = 1\nx['PROPERTY PROPORTION'] = x['PROPERTY PROPORTION'].cumsum()\nx['PROPERTY PROPORTION'] = 100* x['PROPERTY PROPORTION'] / len(x['PROPERTY PROPORTION'])\n\n# Plot the data and configure the settings\nplt.plot(x['PROPERTY PROPORTION'],x['SALE PRICE'], linestyle='None', marker='o')\nplt.title('Cumulative Distribution of Properties according to Price')\nplt.xlabel('Percentage of Properties in ascending order of Price')\nplt.ylabel('Sale Price')\nplt.ticklabel_format(style='plain', axis='y')\nplt.show()","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"6f55d18eb1d87972e3e9c8b97cbca08d2aa2d871"},"cell_type":"markdown","source":"What we see from the two graphs above is that there are a lot of outliers. Maybe this isn't all that surprising given that Manhattan is home to a lot of very expensive property. From the descriptive statistics we could also tell that 75% of the properties in this dataset are cheaper than 950,000 USD. There are also a fair number of properties cheaper than 100,000 USD, which seems too cheap in my opinion.\n\nI have done a few iterations of what a good cap on sales prices is and settled for \n\n1) property needs to be more expensive than 100,000 USD\n\n2) property needs to be cheaper than 5,000,000 USD\n\nEverything is else is a different animal and mixing all together in one model will decrease accuracy."},{"metadata":{"trusted":false,"_uuid":"cc012759b219bc4cae633afe3c23224a48af9ce0"},"cell_type":"code","source":"# Remove observations that fall outside those caps\ndata = data[(data['SALE PRICE'] > 100000) & (data['SALE PRICE'] < 5000000)]\nlen(data)","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"64674631563e008e47035945dedd5e87ba93a6c2"},"cell_type":"markdown","source":"And we're down from 70k observations to 55k, which is 79% remaining. That's a fair chunk of data. I've been toying with the idea of clustering properties on their SALE PRICE classifiying them as something like 'cheap', 'normal', 'expensive', and 'luxury' in order to avoid this issue. I will work on this in a future iteration.\n\nAnyways, let's check out the same graphs again."},{"metadata":{"trusted":false,"_uuid":"90024585aded9d0754f3c26b5cc1616ec9f68afa"},"cell_type":"code","source":"#Set the size of the plot\nplt.figure(figsize=(15,6))\n\n# Plot the data and configure the settings\nsns.boxplot(x='SALE PRICE', data=data)\nplt.ticklabel_format(style='plain', axis='x')\nplt.title('Boxplot of SALE PRICE in USD')\nplt.show()","execution_count":19,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cc3c8d85495cca6a056c0aaedb993d6a5125210d"},"cell_type":"code","source":"#Set the size of the plot\nplt.figure(figsize=(15,6))\n\n# Plot the data and configure the settings\nsns.distplot(data['SALE PRICE'])\nplt.title('Histogram of SALE PRICE in USD')\nplt.ylabel('Normed Frequency')\nplt.show()","execution_count":20,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"72cdb27511e7193950ac94daf3c1e38ab8412a40"},"cell_type":"code","source":"#Set the size of the plot\nplt.figure(figsize=(15,6))\n\n#Get the data and format it\nx = data[['SALE PRICE']].sort_values(by='SALE PRICE').reset_index()\nx['PROPERTY PROPORTION'] = 1\nx['PROPERTY PROPORTION'] = x['PROPERTY PROPORTION'].cumsum()\nx['PROPERTY PROPORTION'] = 100* x['PROPERTY PROPORTION'] / len(x['PROPERTY PROPORTION'])\n\n# Plot the data and configure the settings\nplt.plot(x['PROPERTY PROPORTION'],x['SALE PRICE'], linestyle='None', marker='o')\nplt.title('Cumulative Distribution of Properties according to Price')\nplt.xlabel('Percentage of Properties in ascending order of Price')\nplt.ylabel('Sale Price')\nplt.ticklabel_format(style='plain', axis='y')\nplt.show()","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"cb9e7317a0f03333e21d4e445de7fd9271d986f1"},"cell_type":"markdown","source":"**Recap and Revisit of Entire Data Set**"},{"metadata":{"_uuid":"423c05a7d4275c58da3c4de1064e0572e248be00"},"cell_type":"markdown","source":"After I've removed some observations due to their prices, which I have treated as outliers, how many NULL **SQUARE FEET** observations remain?\n\nUnfortunately, there are still a third of observations remaining that contain no **SQAURE FEET** data. It turns out - **SPOILER ALERT!** that this is the best predictor of **SALE PRICE** in this dataset, which means I want to keep as much data as possible instead of just throwing them away. I'll show the importance of **SQAURE FEET** in the result section at the end of the workbook."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"771f8bd0db226cf81ab7eaf343912d94f819c68b"},"cell_type":"code","source":"# Capture the necessary data\nvariables = data.columns\n\ncount = []\n\nfor variable in variables:\n    length = data[variable].count()\n    count.append(length)\n    \ncount_pct = np.round(100 * pd.Series(count) / len(data), 2)","execution_count":22,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a2f48f0a19fc11b4047bed16dc482272a1b24527"},"cell_type":"code","source":"#Plot number of available data per variable\nplt.figure(figsize=(10,6))\nplt.barh(variables, count_pct)\nplt.title('Count of available data in percent', fontsize=15)\nplt.show()","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"7f8c1292a66dcdbe3ec41d5d0e2fd6ff68b4fbff"},"cell_type":"markdown","source":"## 2.2) Independent Variables Inspection "},{"metadata":{"collapsed":true,"_uuid":"7675fe2fa192d5d240157b5609f9884f097c76a7"},"cell_type":"markdown","source":"I have already looked at each variable in more detail and only show the ones I am going to keep for the model. Currently I have removed all the NULL and outlier data of **SQUARE FEET** and **TOTAL UNITS** but in a future iteration I will try to impute some data points to keep as much as data as possible."},{"metadata":{"_uuid":"8eee195e4f17e6eedcc46eae9508bd4328c2c346"},"cell_type":"markdown","source":"**SQUARE FEET**"},{"metadata":{"_uuid":"74da3eb8e64b27493be50f4cd82b84aee75ab49d"},"cell_type":"markdown","source":"I need to get rid of the NULL values and a few outliers."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"72ac8293b0682af08d8fa7c148f589a9a6b2825e"},"cell_type":"code","source":"# Removes all NULL values\ndata = data[data['LAND SQUARE FEET'].notnull()] \ndata = data[data['GROSS SQUARE FEET'].notnull()] ","execution_count":24,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"defbb9e9f2fde939dad809653e77e3453126e98f"},"cell_type":"code","source":"# Keeps properties with fewer than 20,000 Square Feet, which is about 2,000 Square Metres\ndata = data[data['GROSS SQUARE FEET'] < 20000]\ndata = data[data['LAND SQUARE FEET'] < 20000]","execution_count":25,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1032d97a4ebbd0d590fe46439807f82f5442c8df"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.regplot(x='GROSS SQUARE FEET', y='SALE PRICE', data=data, fit_reg=False, scatter_kws={'alpha':0.3})\nplt.title('Gross Square Feet vs Sale Price')\nplt.show()","execution_count":26,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e83084ee6a97e719cca0e428e3bf917c6db3fa09"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.regplot(x='LAND SQUARE FEET', y='SALE PRICE', data=data, fit_reg=False, scatter_kws={'alpha':0.3})\nplt.title('Land Square Feet vs Sale Price')\nplt.show()","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"45f3f266c26380d854188af25f32ee3efb7b553c"},"cell_type":"markdown","source":"**Total Units**"},{"metadata":{"_uuid":"f0dbb4c4138d05bafd8ec6a0331ebd9eeda8b858"},"cell_type":"markdown","source":"I am deleting the outliers with very large numbers of **TOTAL UNITS** and those with 0 units. Those are only a handful of observations though. In addition, I get rid of the observations the sum of **COMMERCIAL UNITS** and **RESIDENTIAL UNITS** doesn't match **TOTAL UNITS**."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"bdbdb5c523a80533db296ef1fc80178b67c5facc"},"cell_type":"code","source":"# Only a handful of properties with 0 total units are remaining and they will now be deleted\ndata = data[(data['TOTAL UNITS'] > 0) & (data['TOTAL UNITS'] < 50)] ","execution_count":28,"outputs":[]},{"metadata":{"collapsed":true,"scrolled":true,"trusted":false,"_uuid":"4da918cd07dc99e13b94bbaf66837f4d648d5ded"},"cell_type":"code","source":"#Remove data where commercial + residential doesn't equal total units\ndata = data[data['TOTAL UNITS'] == data['COMMERCIAL UNITS'] + data['RESIDENTIAL UNITS']]","execution_count":29,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"abdc910d8ff486c6ce3e16c3066e767286c141e6"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.boxplot(x='COMMERCIAL UNITS', y='SALE PRICE', data=data)\nplt.title('Commercial Units vs Sale Price')\nplt.show()","execution_count":30,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ad5c48ff2847a46f97579981209c2e76f252705d"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.boxplot(x='RESIDENTIAL UNITS', y='SALE PRICE', data=data)\nplt.title('Residential Units vs Sale Price')\nplt.show()","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"3420fb98738fe4a60fed250f737b747cfcd6979a"},"cell_type":"markdown","source":"**YEAR BUILT**"},{"metadata":{"_uuid":"5f6cc4a9a8f7f3cfd48e1a4f2a1e3ace69159ce1"},"cell_type":"markdown","source":"Next one up is YEAR BUILT. There seem to be some buildings that were built in the year 0, which can't be correct. Let's remove those."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"e33ad920c22730b7900c3515cc27805566758669"},"cell_type":"code","source":"data = data[data['YEAR BUILT'] > 0]","execution_count":32,"outputs":[]},{"metadata":{"_uuid":"95448806b393584d4273a932605664b69398ea45"},"cell_type":"markdown","source":"What does the plot look like now? OK, that's a lot more realistic."},{"metadata":{"_uuid":"cf5a8839d54290784ad6555b56dcac3f9496d7e2"},"cell_type":"markdown","source":"However, **YEAR BUILT** isn't quite the variable we are looking for. What is more interesting is the **BUILDING AGE**. Both contain similar information, the second is however a bit more practical. So, let's create it."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"64c7b401d90c634523fcf19feebff5ab3d6b829b"},"cell_type":"code","source":"data['BUILDING AGE'] = 2017 - data['YEAR BUILT']","execution_count":33,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1a670f8a1f98bc0e4018f21c08403e3b4f6c2574"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.regplot(x='BUILDING AGE', y='SALE PRICE', data=data, fit_reg=False, scatter_kws={'alpha':0.1})\nplt.title('Sale Price Distribution by Building Age')\nplt.show()","execution_count":34,"outputs":[]},{"metadata":{"_uuid":"ecdf0ce658af9cf28287ac793bb0a626bbfdfd57"},"cell_type":"markdown","source":"**BOROUGH**"},{"metadata":{"_uuid":"d649edde58098f088e3c8f134745ef4a9bdd5268"},"cell_type":"markdown","source":"This is all in good shape and no surpises here."},{"metadata":{"trusted":false,"_uuid":"1ca7e360570969987ad032fa207c924f2cb72f39"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.boxplot(x='BOROUGH', y='SALE PRICE', data=data)\nplt.title('Sale Price Distribution by Borough')\nplt.show()","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"846d80e88f2c6ae56b0ede49ef46e4be95e311b6"},"cell_type":"markdown","source":"**Building Class Category**"},{"metadata":{"_uuid":"b27df5d19f8ba1c539f79e3367780bc632201698"},"cell_type":"markdown","source":"Some of the categories could potentially be merged in a future iteration."},{"metadata":{"trusted":false,"_uuid":"474d3ff6a8355ef9d84959dca409848a057b7057"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\norder = sorted(data['BUILDING CLASS CATEGORY'].unique())\nsns.boxplot(x='BUILDING CLASS CATEGORY', y='SALE PRICE', data=data, order=order)\nplt.xticks(rotation=90)\nplt.title('Sale Price Distribution by Bulding Class Category')\nplt.show()","execution_count":36,"outputs":[]},{"metadata":{"_uuid":"246227eac976f62b267d3aa02f642023ebc1472a"},"cell_type":"markdown","source":"**More Data Visualisations**"},{"metadata":{"trusted":false,"_uuid":"185c363dd5502a12f33c2dc50e7be1e163dd574d"},"cell_type":"code","source":"# Correlation Matrix\n\n# Compute the correlation matrix\nd= data[['TOTAL UNITS','GROSS SQUARE FEET','SALE PRICE', 'BUILDING AGE', 'LAND SQUARE FEET', 'RESIDENTIAL UNITS', \n         'COMMERCIAL UNITS']]\ncorr = d.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, \n            square=True, linewidths=.5, annot=True, cmap=cmap)\nplt.yticks(rotation=0)\nplt.title('Correlation Matrix of all Numerical Variables')\nplt.show()","execution_count":37,"outputs":[]},{"metadata":{"_uuid":"1686af63191f52bc9d35ec556bbf440d87064040"},"cell_type":"markdown","source":"# 3) Modelling"},{"metadata":{"_uuid":"3f705e1ed7501ed0d4c0f103e3979d90912faedc"},"cell_type":"markdown","source":"## 3.1) Data Preparation"},{"metadata":{"_uuid":"4ca83acb9cc55a44761f138d65677793e3f48a00"},"cell_type":"markdown","source":"scikit works best with normalized data, i.e. data that has a mean around 0 and a distribution around that. In the next section I will normalise/standardise the data and also take the log in order to get rid of the skewness and to allow for a more normal distribution.\n\nBut first I get all the relevant variables and one-hot encode the categrical variables, which is necessary for scikit to work. The numerical variables don't need one-hot encoding but will have to be normalised.\n"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"d5adbed7f356ca73b246ebcca574d0c922b4a8b3"},"cell_type":"code","source":"#Choose only the variables I want to use in the model\ncolumns = ['BOROUGH', 'BUILDING CLASS CATEGORY', 'COMMERCIAL UNITS','GROSS SQUARE FEET',\n       'SALE PRICE', 'BUILDING AGE', 'LAND SQUARE FEET', 'RESIDENTIAL UNITS']\ndata_model = data.loc[:,columns]","execution_count":38,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"36668a8aa6b9b7565953ade7295732dea3be26a3"},"cell_type":"code","source":"#Select the variables to be one-hot encoded\none_hot_features = ['BOROUGH', 'BUILDING CLASS CATEGORY']\n\n# For each categorical column, find the unique number of categories. This tells us how many columns we are adding to the dataset.\nlongest_str = max(one_hot_features, key=len)\ntotal_num_unique_categorical = 0\nfor feature in one_hot_features:\n    num_unique = len(data[feature].unique())\n    print('{col:<{fill_col}} : {num:d} unique categorical values.'.format(col=feature, \n                                                                          fill_col=len(longest_str),\n                                                                          num=num_unique))\n    total_num_unique_categorical += num_unique\nprint('{total:d} columns will be added during one-hot encoding.'.format(total=total_num_unique_categorical))","execution_count":39,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"13355e5c80f6da9ba9bb1021e2311653ba5cb684"},"cell_type":"code","source":"# Convert categorical variables into dummy/indicator variables (i.e. one-hot encoding).\none_hot_encoded = pd.get_dummies(data_model[one_hot_features])\none_hot_encoded.info(verbose=True, memory_usage=True, null_counts=True)","execution_count":40,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7039e134c1fbdc0c4ed4e25967c42c2e8a9a5eac"},"cell_type":"code","source":"#Delete the old columns...\ndata_model = data_model.drop(one_hot_features, axis=1)\n\n#...and add the new one-hot encoded variables\ndata_model = pd.concat([data_model, one_hot_encoded], axis=1)\ndata_model.head()","execution_count":41,"outputs":[]},{"metadata":{"_uuid":"79b83d707a21e2a2d1387a58a30d41ad51c5003c"},"cell_type":"markdown","source":"**Transforming the dependent variable SALE PRICE**"},{"metadata":{"_uuid":"bc5e0b3c125242916026aca352ddf3c93760c4c8"},"cell_type":"markdown","source":"This is what SALE PRICE looks before the transformation..."},{"metadata":{"trusted":false,"_uuid":"75b7b83448776b1d259606ce22dc89a4fdbe700d"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.distplot(data_model['SALE PRICE'])\nplt.title('Histogram of SALE PRICE')\nplt.show()","execution_count":42,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"c50dc5cfe8d6b36e95c465053a1628fdd2d97fc9"},"cell_type":"code","source":"# Take the log and normalise\ndata_model['SALE PRICE'] = StandardScaler().fit_transform(np.log(data_model['SALE PRICE']).reshape(-1,1))","execution_count":43,"outputs":[]},{"metadata":{"_uuid":"ca86c22fff460c38ce02d08310290adf95d15ab9"},"cell_type":"markdown","source":"...and  this what it looks after"},{"metadata":{"trusted":false,"_uuid":"57a483b24b0876dda620dfeb6d8498b3f21fc7db"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.distplot(data_model['SALE PRICE'])\nplt.title('Histogram of Normalised SALE PRICE')\nplt.show()","execution_count":44,"outputs":[]},{"metadata":{"_uuid":"64e7537da0f7a591939c73c79dc48740bdf8531d"},"cell_type":"markdown","source":"**Transforming the independent variables**"},{"metadata":{"_uuid":"c69b3f0436fa6cdcd5da4b657b896031ed9ee584"},"cell_type":"markdown","source":"Some of the variables contain zeroes, which is why I need to add 1 so that I can take the log before normalising it - you can see that in the table below. Using the log allows me to get rid of the skew in the data and have a more normal distribution. The reason why I need to add 1 is because I can't take the log of 0 - it is not defined. The log of 1 however is."},{"metadata":{"trusted":false,"_uuid":"30b8b73a6bb043886901b209d1209287a2d4d5eb"},"cell_type":"code","source":"data_model.describe()","execution_count":45,"outputs":[]},{"metadata":{"_uuid":"45e34a50e87085654f1e885b7968c43eb759e9aa"},"cell_type":"markdown","source":"**UNITS**"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"de5f0189a992403f96ec29234bff49fda87989c7"},"cell_type":"code","source":"# Add 1 to Units\ndata_model['COMMERCIAL UNITS'] = data_model['COMMERCIAL UNITS'] + 1\ndata_model['RESIDENTIAL UNITS'] = data_model['RESIDENTIAL UNITS'] + 1\n\n# Take the log and standardise\ndata_model['COMMERCIAL UNITS'] = StandardScaler().fit_transform(np.log(data_model['COMMERCIAL UNITS']).reshape(-1,1))\ndata_model['RESIDENTIAL UNITS'] = StandardScaler().fit_transform(np.log(data_model['RESIDENTIAL UNITS']).reshape(-1,1))","execution_count":46,"outputs":[]},{"metadata":{"_uuid":"591f275a551633da4bc252385d1f3a9164938aa3"},"cell_type":"markdown","source":"**SQUARE FEET**"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"9504facdd496726eea61c59925b6ea6d67f8a57a"},"cell_type":"code","source":"# Add 1 to Units\ndata_model['GROSS SQUARE FEET'] = data_model['GROSS SQUARE FEET'] + 1\ndata_model['LAND SQUARE FEET'] = data_model['LAND SQUARE FEET'] + 1\n\n# Take the log and standardise\ndata_model['GROSS SQUARE FEET'] = StandardScaler().fit_transform(np.log(data_model['GROSS SQUARE FEET']).reshape(-1,1))\ndata_model['LAND SQUARE FEET'] = StandardScaler().fit_transform(np.log(data_model['LAND SQUARE FEET']).reshape(-1,1))","execution_count":47,"outputs":[]},{"metadata":{"_uuid":"89d71dddb94b7e776527a3b81bec87b66523b37a"},"cell_type":"markdown","source":"**BUILDING AGE**"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"9c3e0a0215356eeeb30fb847bd3ee59c4811a14b"},"cell_type":"code","source":"# Add 1 to BUILDING AGE\ndata_model['BUILDING AGE'] = data_model['BUILDING AGE'] + 1\n\n# Take the log and standardise\ndata_model['BUILDING AGE'] = StandardScaler().fit_transform(np.log(data_model['BUILDING AGE']).reshape(-1,1))","execution_count":48,"outputs":[]},{"metadata":{"_uuid":"680b5ba40d304f79c8f50ec9028557297db9084c"},"cell_type":"markdown","source":"## 3.2) Split into Training/Testing Data"},{"metadata":{"_uuid":"39dade16151e8ea183d81eb2f53c10414322d6f2"},"cell_type":"markdown","source":"The step is necessary to ensure that the model is flexible and general enough so that can predict accurately unseen or new data. I train the model with the training data and then check how good it performs on the unseen testing data."},{"metadata":{"trusted":false,"_uuid":"85e7fcb8c3f3f6d0dea7e27f21fd766412e40a11"},"cell_type":"code","source":"#Split data into training and testing set with 80% of the data going into training\ntraining, testing = train_test_split(data_model, test_size=0.2, random_state=0)\nprint(\"Total sample size = %i; training sample size = %i, testing sample size = %i\"\\\n     %(data_model.shape[0],training.shape[0],testing.shape[0]))","execution_count":49,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"bed93aacef75e389f3f497fdcd79f421ea3e369b"},"cell_type":"code","source":"#X are the variables/features that help predict y, which tells us whether an employee left or stayed. This is done for both \n#training and testing\ndf_train_s = training.loc[:,data_model.columns]\nX_train_s = df_train_s.drop(['SALE PRICE'], axis=1)\ny_train_s = df_train_s.loc[:, ['SALE PRICE']]\n\ndf_test_s = testing.loc[:,data_model.columns]\nX_test_s = df_test_s.drop(['SALE PRICE'], axis=1)\ny_test_s = df_test_s.loc[:, ['SALE PRICE']]","execution_count":50,"outputs":[]},{"metadata":{"_uuid":"338db4b9ed4ef06e08af6015722edacf305159ac"},"cell_type":"markdown","source":"## 3.3) Running the Different Models"},{"metadata":{"_uuid":"d3a4955c8f029e807f69a183d25e9af31b693e24"},"cell_type":"markdown","source":"Finally, the moment we have all been waiting for. I will now test a few different models to see which one performs best. In a next iteration I will also fine tune these to get an even better result.\n\nI will use\n\n1) **Linear Regression**\n\n2) **Random Forest Regression**\n\n3) **Ridge Regression**\n\n4) **and ElasticNet**"},{"metadata":{"_uuid":"87fd9a417eb7018344d4e08355b0ffc07c734dbc"},"cell_type":"markdown","source":"**1) Linear Regression**"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"7c0095cc96b2f192fea0be7e1f8bfa69211edcb7"},"cell_type":"code","source":"# Create the regressor: linreg\nlinreg = LinearRegression()\n\n# Fit the regressor to the training data\nlinreg.fit(X_train_s, y_train_s)\n\n# Predict the labels of the test set: y_pred\ny_pred_s = linreg.predict(X_test_s)\n\n# Compute 5-fold cross-validation scores: cv_scores\ncv_scores_linreg = cross_val_score(linreg, X_train_s, y_train_s, cv=5)","execution_count":51,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c1f20994677550f3e77931294f9a2262397c2a38"},"cell_type":"code","source":"print(\"R^2: {}\".format(linreg.score(X_test_s, y_test_s)))\nrmse = np.sqrt(mean_squared_error(y_test_s, y_pred_s))\nprint(\"Root Mean Squared Error: {}\".format(rmse))\n\nprint(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores_linreg)))\n# Print the 5-fold cross-validation scores\nprint(cv_scores_linreg)","execution_count":52,"outputs":[]},{"metadata":{"_uuid":"275a2e0f876be6bb8f8ebc6439957b07e720b203"},"cell_type":"markdown","source":"**2) Random Forest**"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"8628a180879089c4821f0bc07355d182c8102eda"},"cell_type":"code","source":"rf_reg = RandomForestRegressor()\n\nrf_reg.fit(X_train_s, y_train_s)\n\ny_pred_s_rf = rf_reg.predict(X_test_s)\n\n# Compute 5-fold cross-validation scores: cv_scores\ncv_scores_rf = cross_val_score(rf_reg, X_train_s, y_train_s, cv=5)","execution_count":53,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8fbe43b96934f870a1f4b99541372663c6074473"},"cell_type":"code","source":"print(\"R^2: {}\".format(rf_reg.score(X_test_s, y_test_s)))\nrmse = np.sqrt(mean_squared_error(y_test_s, y_pred_s_rf))\nprint(\"Root Mean Squared Error: {}\".format(rmse))\n\nprint(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores_rf)))\n# Print the 5-fold cross-validation scores\nprint(cv_scores_rf)","execution_count":54,"outputs":[]},{"metadata":{"_uuid":"309b58b856ba5ff76619d1d945ee4a3e57916f3b"},"cell_type":"markdown","source":"Feature Importance of Random Forest"},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"7c8a624505ce2290addc7e10a310d9868cfce6f0"},"cell_type":"code","source":"importance = pd.DataFrame(list(zip(X_train_s.columns, np.transpose(rf_reg.feature_importances_))) \\\n            ).sort_values(1, ascending=False)\nimportance","execution_count":55,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"022734fbae4c53c5aab3883e125727140f3a0a92"},"cell_type":"code","source":"importances = rf_reg.feature_importances_\n\nstd = np.std([tree.feature_importances_ for tree in rf_reg.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X_train_s.shape[1]), importances[indices],  \n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X_train_s.shape[1]),X_train_s.columns[indices], rotation=90)\n#plt.xlim([-1, X_train_s.shape[1]])\nplt.xlim([-1, 10])\nplt.show()","execution_count":56,"outputs":[]},{"metadata":{"_uuid":"a3736d73cdf07aec92b1a7dae9238727e3aa8802"},"cell_type":"markdown","source":"**3) Ridge Regression**"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"38220cf36d01493029beb27e7f4b0e225cc20ec1"},"cell_type":"code","source":"def display_plot(cv_scores, cv_scores_std):\n    fig = plt.figure()\n    ax = fig.add_subplot(1,1,1)\n    ax.plot(alpha_space, cv_scores)\n\n    std_error = cv_scores_std / np.sqrt(10)\n\n    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\n    ax.set_ylabel('CV Score +/- Std Error')\n    ax.set_xlabel('Alpha')\n    ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n    ax.set_xlim([alpha_space[0], alpha_space[-1]])\n    ax.set_xscale('log')\n    plt.show()","execution_count":57,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"368c77befded092244f4c3b154989d253d8ee5cd"},"cell_type":"code","source":"# Import necessary modules\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\n\n# Setup the array of alphas and lists to store scores\nalpha_space = np.logspace(-4, 0, 50)\nridge_scores = []\nridge_scores_std = []\n\n# Create a ridge regressor: ridge\nridge = Ridge(normalize=True)\n\n# Compute scores over range of alphas\nfor alpha in alpha_space:\n\n    # Specify the alpha value to use: ridge.alpha\n    ridge.alpha = alpha\n    \n    # Perform 10-fold CV: ridge_cv_scores\n    ridge_cv_scores = cross_val_score(ridge, X_train_s, y_train_s, cv=10)\n    \n    # Append the mean of ridge_cv_scores to ridge_scores\n    ridge_scores.append(np.mean(ridge_cv_scores))\n    \n    # Append the std of ridge_cv_scores to ridge_scores_std\n    ridge_scores_std.append(np.std(ridge_cv_scores))\n\n# Display the plot\ndisplay_plot(ridge_scores, ridge_scores_std)","execution_count":58,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"feab901d310aab76288b5de0944c01920a05b8d9"},"cell_type":"code","source":"# Instantiate a ridge regressor: ridge\nridge = Ridge(alpha=0.01, normalize=True)\n\n#Fit the model\nridge.fit(X_train_s, y_train_s)\n\n#Predict\ny_pred_s_ridge = ridge.predict(X_test_s)\n\n# Perform 5-fold cross-validation: ridge_cv\nridge_cv = cross_val_score(ridge, X_train_s, y_train_s, cv=5)\n","execution_count":59,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b54752f1dc05929acad03e3260513a2062c1754d"},"cell_type":"code","source":"print(\"R^2: {}\".format(ridge.score(X_test_s, y_test_s)))\nrmse = np.sqrt(mean_squared_error(y_test_s, y_pred_s_ridge))\nprint(\"Root Mean Squared Error: {}\".format(rmse))\n\nprint(\"Average 5-Fold CV Score: {}\".format(np.mean(ridge_cv)))\n# Print the 5-fold cross-validation scores\nprint(ridge_cv)","execution_count":60,"outputs":[]},{"metadata":{"_uuid":"734c9aec3a878405b4e05501e2cfee0834e0629d"},"cell_type":"markdown","source":"**4) ElasticNet and GridSearch**"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"9771af0a050feb2797b1566ccef85ef78b54204d"},"cell_type":"code","source":"# Create the hyperparameter grid\nl1_space = np.linspace(0, 1, 30)\nparam_grid = {'l1_ratio': l1_space}\n\n# Instantiate the ElasticNet regressor: elastic_net\nelastic_net = ElasticNet()\n\n# Setup the GridSearchCV object: gm_cv\ngm_cv = GridSearchCV(elastic_net, param_grid, cv=5)\n\n# Fit it to the training data\ngm_cv.fit(X_train_s, y_train_s)\n\n# Predict on the test set and compute metrics\ny_pred_elas = gm_cv.predict(X_test_s)\nr2 = gm_cv.score(X_test_s, y_test_s)\nmse = mean_squared_error(y_test_s, y_pred_elas)\n","execution_count":61,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3a1f1ab4a44547954f9c2d87814bc94cffc26cfb"},"cell_type":"code","source":"print(\"Tuned ElasticNet l1 ratio: {}\".format(gm_cv.best_params_))\nprint(\"Tuned ElasticNet R squared: {}\".format(r2))\nprint(\"Tuned ElasticNet MSE: {}\".format(mse))","execution_count":62,"outputs":[]},{"metadata":{"_uuid":"4521adbecf51df678e552046e11ebde29ab6014e"},"cell_type":"markdown","source":"# 4) Conclusion and Next Steps"},{"metadata":{"collapsed":true,"_uuid":"fe7be88b3cc4a73a88b550ebbbb1779c1dc73967"},"cell_type":"markdown","source":"1) An untuned Random Forest Regression managed to get a R2 of 0.40, which is certainly not great but maybe not bad given the limited amount of data available. **SQUARE FEET**, **BUILDING AGE**, and **BOROUGH** were the most important features determining the **SALE PRICE**.\n\n2) We only used 30k observations out of a potential of 70k. Remember that there were a fair amount of NULL **SALE PRICE** observations and duplicates etc. So the original 85k don't count. In a next iteration I will therefore explore the possibilities to impute the **SQUARE FEET** data because I deleted a lot here. The same is true for **UNITS** data.\n\n3) Instead of throwing outlier **SALE PRICE** data I will try to use clustering to classify properties,w hich would allow me to keep the outlier data - maybe. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"}},"nbformat":4,"nbformat_minor":1}