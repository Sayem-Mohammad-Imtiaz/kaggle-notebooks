{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import classification_report \n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')\nX, y = data.drop(['target'], axis=1), data['target']\nfrom sklearn.preprocessing import StandardScaler\n\ndef scale_a_column(data, label_column='target'):\n    scaler = StandardScaler()\n    df1 = data.drop([label_column], axis=1)\n    res = scaler.fit_transform(df1)\n    df1 = pd.DataFrame(res, columns=df1.columns)\n    df1[label_column] = data[label_column]\n    return df1\nscaled_data = scale_a_column(data)\nX_scaled, y_scaled = scaled_data.drop(['target'], axis=1), scaled_data['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go\n\ndef plot_3d(data, f1, f2, f3, target='target'):\n    PLOT = go.Figure()\n    for C in list(data[target].unique()):    \n        PLOT.add_trace(go.Scatter3d(x = data[data[target] == C][f1],\n                                    y = data[data[target] == C][f2],\n                                    z = data[data[target] == C][f3],\n                                    mode = 'markers', marker_size = 8, marker_line_width = 1,\n                                    name = f'{target} ' + str(C)))\n\n    PLOT.update_layout(width = 800, height = 800, autosize = True, showlegend = True,\n                       scene = dict(xaxis=dict(title = f1, titlefont_color = 'black'),\n                                    yaxis=dict(title = f2, titlefont_color = 'black'),\n                                    zaxis=dict(title = f3, titlefont_color = 'black')),\n                       font = dict(family = \"Gilroy\", color  = 'black', size = 12))\n    PLOT.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_2d(data, x, y, figsize=(9, 5), hue='target'):\n    plt.figure(figsize = figsize)\n    sns.swarmplot(x = x, y = y, hue=hue,data=data)\n    plt.legend()\n    plt.title(f\"{x} -- {y} -- {hue}\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for i in range(X_scaled.shape[1]):\n#    for j in range(i+1, X_scaled.shape[1]):\n#            plot_2d(scaled_data, scaled_data.columns[i], scaled_data.columns[j])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_kmean(X_scaled, y, n_clusters=4, compare_cluster=False):\n    kmeans = KMeans(\n        init=\"random\",\n        n_clusters=n_clusters,\n        n_init=10,\n        max_iter=300,\n        random_state=42)\n\n    X_kmeans = X_scaled.copy()\n    kmeans.fit(X_kmeans)\n    \n    X_kmeans = X_scaled.copy()\n    X_kmeans['cluster'] = kmeans.labels_\n    \n    if compare_cluster:\n        d = pd.concat([X_kmeans, y], axis=1)\n        corr1 = d['cluster'].corr(d['target'])\n        d['cluster'] = 1 - d['cluster']\n        corr2 = d['cluster'].corr(d['target'])\n        print(corr1, corr2)\n    else:\n        print(pd.concat([X_kmeans, y], axis=1))\n    #print(X_kmeans)\n    return X_kmeans","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_em(X_scaled, y, n_components=4, compare_cluster=False):\n    X_GM = X_scaled.copy()\n    gm = GaussianMixture(n_components=n_components, random_state=0).fit(X_GM)\n    labels = gm.predict(X_GM)\n\n    X_GM = X_scaled.copy()\n    X_GM['cluster'] = labels\n    if compare_cluster:\n        d = pd.concat([X_GM, y], axis=1)\n        print(d['cluster'].corr(d['target']))\n    else:\n        print(pd.concat([X_GM, y], axis=1))\n    #print(X_GM)    \n    return X_GM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import decomposition\n\ndef run_pca(X, y):\n    pca = decomposition.PCA(n_components=int(X_scaled.shape[1]*0.7))\n    pca = decomposition.PCA(n_components=3)\n    pca.fit(X)\n    X_pca = pca.transform(X)\n\n    X_pca=pd.DataFrame(X_pca)\n    X_pca.index = X.index\n    X_pca.columns = [f'col{i+1}' for i in range(X_pca.shape[1])]\n    data_pca_for_plot = pd.concat([X_pca, y], axis=1)\n\n    return X_pca","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import decomposition\n\ndef run_ica(X, y):\n    ica = decomposition.FastICA(n_components=int(X_scaled.shape[1]*0.7))\n    ica.fit(X)\n    X_ica = ica.transform(X)\n\n    X_ica = pd.DataFrame(X_ica)\n    X_ica.index = X.index\n    X_ica.columns = [f'col{i+1}' for i in range(X_ica.shape[1])]\n    data_ica_for_plot=pd.concat([X_ica, y], axis=1)\n\n    return X_ica","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import random_projection\n\ndef run_random_project(X_scaled, y):\n    transformer = random_projection.GaussianRandomProjection(n_components=int(X_scaled.shape[1]*0.7))\n    #transformer = random_projection.GaussianRandomProjection()\n    X_rp = transformer.fit_transform(X_scaled)\n    X_rp = pd.DataFrame(X_rp)\n    X_rp.columns = [f'col{i+1}' for i in range(X_rp.shape[1])]\n\n    data_rp_for_plot=pd.concat([X_rp, y], axis=1)\n\n    for i in range(1, data_rp_for_plot.shape[1]):\n        for j in range(i+1, data_rp_for_plot.shape[1]):\n            for k in range(j+1, data_rp_for_plot.shape[1]):\n                break\n                plot_3d(data_rp_for_plot, f'col{i}', f'col{j}', f'col{k}', 'Drug')\n    return X_rp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\ndef run_tree_based_feature_selection(X, y):\n    clf = ExtraTreesClassifier(n_estimators=50)\n    clf = clf.fit(X, y)\n    #pd.DataFrame.from_dict(data, orient='index')\n    wieghts_df = pd.DataFrame.from_dict(dict(zip(X.columns, list(clf.feature_importances_))), orient='index', columns=['Weight'])\n    threshhold = min(clf.feature_importances_) * 1.01\n    threshhold = None\n    model = SelectFromModel(clf, max_features=4, threshold=threshhold, prefit=True)\n    X_selected = model.transform(X)\n\n    X_selected = pd.DataFrame(X_selected)\n    X_selected.index = X.index\n    X_selected.columns = [f'col{i+1}' for i in range(X_selected.shape[1])]\n    print(wieghts_df.transpose())\n    \n    data_rp_for_plot=pd.concat([X_selected, y], axis=1)\n    print(data_rp_for_plot)\n\n    for i in range(1, data_rp_for_plot.shape[1]):\n        for j in range(i+1, data_rp_for_plot.shape[1]):\n            for k in range(j+1, data_rp_for_plot.shape[1]):\n                break\n                plot_3d(data_rp_for_plot, f'col{i}', f'col{j}', f'col{k}', 'target')\n    \n    return X_selected","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_scaled_em = run_em(X_scaled, y)\nX_scaled_kmean = run_kmean(X_scaled, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_ica = run_ica(X_scaled, y)\nx_pca = run_pca(X_scaled, y)\nx_rp  = run_random_project(X_scaled, y)\nx_tbs = run_tree_based_feature_selection(X_scaled, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nn_max = 50\nn_start = 2\nscores = []\ninertia_list = np.empty(n_max)\n\nfor i in range(n_start,n_max):\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(x_pca)\n    inertia_list[i] = kmeans.inertia_\n    scores.append(silhouette_score(x_pca, kmeans.labels_))\n\nn_max_shift = 2  # find maximum after this index of score\nn_clusters = np.argmax(scores[n_max_shift:])+(n_start+n_max_shift) # it's my upgrade\nplt.plot(range(0,n_max),inertia_list,'-o')\nplt.xlabel('Number of cluster')\nplt.axvline(x=n_clusters, color='blue', linestyle='--')\nplt.ylabel('Inertia')\nplt.show()\n\nplt.plot(range(2,n_max), scores);\nplt.title('Results KMeans')\nplt.xlabel('n_clusters');\nplt.axvline(x=n_clusters, color='blue', linestyle='--')\nplt.ylabel('Silhouette Score');\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_clusters","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_tbs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_ica_with_kmean = run_kmean(x_ica, y)\nx_pca_with_kmean = run_kmean(x_pca, y)\nx_rp_with_kmean = run_kmean(x_rp, y)\nx_tbs_with_kmean = run_kmean(x_tbs, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_ica_with_em = run_em(x_ica, y)\nx_pca_with_em = run_em(x_pca, y)\nx_rp_with_em = run_em(x_rp, y)\nx_tbs_with_em = run_em(x_tbs, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_test(X, y ,clf_name, hidden_layers = 100):\n    clf = MLPClassifier(random_state=1, max_iter=2000, hidden_layer_sizes=hidden_layers)\n    X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=101)\n    clf=clf.fit(X_train, y_train)\n    return (clf_name, clf.score(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = [ run_test(X, y, 'Basic'), \nrun_test(x_ica, y, 'ICA'),\nrun_test(x_pca, y, 'PCA'), \nrun_test(x_rp, y, 'Randomized Projection'),\nrun_test(x_tbs, y, 'Tree based selection')]\n\npd.DataFrame.from_dict({item[0] : [item[1]] for item in res})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = [ run_test(X, y, 'Basic'), \nrun_test(x_ica_with_kmean, y, 'ICA'),\nrun_test(x_pca_with_kmean, y, 'PCA'), \nrun_test(x_rp_with_kmean, y, 'Randomized Projection'),\nrun_test(x_tbs_with_kmean, y, 'Tree based selection')]\n\npd.DataFrame.from_dict({item[0] : [item[1]] for item in res})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = [ run_test(X, y, 'Basic'), \nrun_test(x_ica_with_em, y, 'ICA'),\nrun_test(x_pca_with_em, y, 'PCA'), \nrun_test(x_rp_with_em, y, 'Randomized Projection'),\nrun_test(x_tbs_with_em, y, 'Tree based selection')]\n\npd.DataFrame.from_dict({item[0] : [item[1]] for item in res})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style('darkgrid')\naxes = pd.plotting.scatter_matrix(data, alpha = 0.3, figsize = (10,7), diagonal = 'kde' ,s=80)\ncorr = data.corr().values\n\nplt.xticks(fontsize =10,rotation =0)\nplt.yticks(fontsize =10)\nfor ax in axes.ravel():\n    ax.set_xlabel(ax.get_xlabel(),fontsize = 15, rotation = 60)\n    ax.set_ylabel(ax.get_ylabel(),fontsize = 15, rotation = 60)\n# put the correlation between each pair of variables on each graph\nfor i, j in zip(*np.triu_indices_from(axes, k=1)):\n    axes[i, j].annotate(\"%.3f\" %corr[i, j], (0.8, 0.8), xycoords=\"axes fraction\", ha=\"center\", va=\"center\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_ica_with_kmean = run_kmean(x_ica, y, 2, True)\nx_pca_with_kmean = run_kmean(x_pca, y, 2, True)\nx_rp_with_kmean = run_kmean(x_rp, y, 2, True)\nx_tbs_with_kmean = run_kmean(x_tbs, y, 2, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_ica_with_em = run_em(x_ica, y, 2, True)\nx_pca_with_em = run_em(x_pca, y, 2, True)\nx_rp_with_em = run_em(x_rp, y, 2, True)\nx_tbs_with_em = run_em(x_tbs, y, 2, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}