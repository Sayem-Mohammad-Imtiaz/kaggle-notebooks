{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -U spacy\n!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import unicode_literals, print_function\nfrom pathlib import Path\nfrom spacy.util import minibatch, compounding\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\nimport itertools\nimport json\nimport nltk.data\nimport numpy as np\nimport os\nimport pandas as pd\nimport random\nimport spacy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CONFIG\n\n# Data\nDIR_DATA_INPUT = os.path.join('/kaggle', 'input', 'CORD-19-research-challenge')\nDIR_BIORXIV = os.path.join(DIR_DATA_INPUT, 'biorxiv_medrxiv', 'biorxiv_medrxiv', 'pdf_json')\nDIR_COMM = os.path.join(DIR_DATA_INPUT, 'comm_use_subset', 'comm_use_subset', 'pdf_json')\nDIR_CUSTOM = os.path.join(DIR_DATA_INPUT, 'custom_license', 'custom_license', 'pdf_json')\nDIR_NONCUSTOM = os.path.join(DIR_DATA_INPUT, 'noncomm_use_subset', 'noncomm_use_subset', 'pdf_json')\n\nDIR_DATA_OUTPUT = os.path.join('/kaggle', 'working')\nPATH_AGG_JSON = os.path.join(DIR_DATA_OUTPUT, 'agg_data.json')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def extract_jsons_to_list(folder):\n    \"\"\"\n    Extracting 4 fields ('abstract', 'text', 'paper_id', 'title') from orginal Json file\n    :folder String, to location with Jsons\n    :return: Lists, with selected params\n    \"\"\"\n    results = []\n\n    files = os.listdir(folder)\n    for filename in tqdm(files, f'parsing {folder}'):\n        json_file = os.path.join(folder, filename)\n        file = json.load(open(json_file, 'rb'))\n        agg_abstract_file = ' '.join(\n            [abstract['text'] for abstract in file['abstract']])\n        text = ' '.join(\n            [text['text'] for text in file['body_text']])\n        results.append({\n            'abstract': agg_abstract_file,\n            'text': text,\n            'paper_id': file['paper_id'], \n            'title': file['metadata']['title']\n        })\n\n    return results\n\n\ndef save_json(file_to_save, path_to_save):\n    \"\"\"\n    Save in relevant Json format\n    :file_to_save DataFrame, file to save\n    :path_to_save String, lacation to save a file\n    \"\"\"\n    df = pd.DataFrame(file_to_save)\n    \n    df['json_output'] = df.apply(lambda x: {\n        'text': x.text, \"meta\":{'paper_id':x.paper_id, 'title': x.title}\n    }, axis=1)\n    df['json_output'].to_json(path_to_save, orient='records', lines=True)\n    \n\ndef filtr_covid_and_risk_factor(file_to_save, path_to_save):\n    \"\"\"\n    List filtering in abstact and text (filters: 'COVID-19' or 'SARS-CoV-2')\n    :file_to_save List, file to save\n    :path_to_save String, lacation to save a file\n    :return: DataFrame, valid data\n    \"\"\"\n    df = pd.DataFrame(file_to_save)\n    mask = df['abstract'].str.contains('COVID-19') | df['text'].str.contains('COVID-19') \\\n     | df['abstract'].str.contains('SARS-CoV-2') | df['text'].str.contains('SARS-CoV-2')\n    \n    abstracts = text_2_sentance(df[mask], 'abstract')\n    text = text_2_sentance(df[mask], 'text')\n    abstracts.extend(text)\n\n    save_json(abstracts, path_to_save)\n    \n    return df\n\n\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\ndef text_2_sentance(df, column):\n    \"\"\"\n    Save 5 senctance before and after sentance which contains `risk factor` expression\n    :df DataFrame, with text data\n    :column String, column name to process\n    :return: List, valid sentance\n    \"\"\"\n    df['sentances'] = df.apply(lambda x: tokenizer.tokenize(x[column]), axis = 1)\n    \n    valid_sentance = []\n    for _, row in tqdm(df.iterrows()):\n        for index, singiel_sentance in enumerate(row['sentances']):\n            if 'risk factor' in singiel_sentance.lower():\n                sentance_range = [valid_index for valid_index in range(index-5, index+6) if (valid_index >=0) and (valid_index < len(row['sentances']))]\n                valid_sentance.append({\n                    'text': row['sentances'][sentance_range[0]: (sentance_range[-1]+1)],\n                    'paper_id': row['paper_id'], \n                    'title': row['title']\n                })\n                \n    return valid_sentance\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate Json for Marek\n\nbio = extract_jsons_to_list(DIR_BIORXIV)\ncomm = extract_jsons_to_list(DIR_COMM)\ncus = extract_jsons_to_list(DIR_CUSTOM)\nnon = extract_jsons_to_list(DIR_NONCUSTOM)\n\nlist_agg = bio + comm + cus + non\nresults = filtr_covid_and_risk_factor(list_agg, PATH_AGG_JSON)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!head $PATH_AGG_JSON","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Download data for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://raw.githubusercontent.com/chopeen/CORD-19/master/data/annotated/cord_19_rf_sentences_merged.json\n!ls -1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split dataset for train and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nnew_list = []\nfile = json.load(open('cord_19_rf_sentences_merged.json', 'rb'))\n\ndf = pd.DataFrame(file)\n\nX_train, X_test = train_test_split(\n    df, test_size=0.2, random_state=42)\n\nX_train.to_json('train_abstract_teach.json', orient='records')\nX_test.to_json('test_abstract_teach.json', orient='records')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train NER model"},{"metadata":{"trusted":true},"cell_type":"code","source":"!spacy train en models/ train_abstract_teach.json test_abstract_teach.json --pipeline ner --base-model en_core_sci_lg  --replace-components","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}