{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport transformers\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/True.csv\")\nfake = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/Fake.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import DistilBertModel, DistilBertConfig\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nimport torch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\nfrom tqdm import tqdm\nimport time\nfrom transformers import AdamW,get_linear_schedule_with_warmup","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import glue_convert_examples_to_features as convert_examples_to_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true[\"Label\"] = 1\nfake[\"Label\"] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([true,fake],ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#data_final = data[[\"title\",\"Label\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"total_data\"] = data[\"title\"] + \" [SEP] \" + data[\"text\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Find Average Sequence Size"},{"metadata":{"trusted":true},"cell_type":"code","source":"#seq_len = [len(tokenizer.encode(data.iloc[i][\"total_data\"])) for i in range(len(data))]\n#print(np.median(seq_len))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Count no of fake and real headlines"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Label\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train , test = train_test_split(data,shuffle=True,test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_data_set(df,tokenizer):\n    input_ids = []\n    token_type_ids = []\n    attention_mask = []\n    labels = []\n    for i in range(len(df)):\n        text = df.iloc[i][\"total_data\"]\n        label = df.iloc[i][\"Label\"]\n        out = tokenizer.encode_plus(text,add_special_tokens=True,max_length=256,pad_to_max_length=True)\n        input_ids.append(out[\"input_ids\"])\n        token_type_ids.append(out[\"input_ids\"])\n        attention_mask.append(out[\"attention_mask\"])\n        labels.append(label)\n    all_input_ids = torch.tensor(input_ids,dtype=torch.long)\n    all_token_type_ids = torch.tensor(token_type_ids,dtype=torch.long)\n    all_attention_mask = torch.tensor(attention_mask,dtype= torch.long)\n    all_labels = torch.tensor(labels,dtype=torch.long)\n    tensor_dataset = TensorDataset(all_input_ids,all_token_type_ids,all_attention_mask,all_labels)\n    return tensor_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def do_train(train,test,epochs,model,tokenizer,device):\n    train_data = create_data_set(train,tokenizer)\n    train_data_loader = DataLoader(train_data,batch_size=64)\n    loss_per_epoch = []\n    test_accuracy_per_epoch = []\n    optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_data_loader))\n    for epoch in range(epochs):\n        tr_loss = 0\n        for ids,type_ids,att_mask,labels in tqdm(train_data_loader):\n            batch = tuple((ids.to(device),att_mask.to(device),type_ids.to(device),labels.to(device)))\n            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n            outputs = model(**inputs)\n            loss = outputs[0]\n            logits = outputs[1]\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            model.zero_grad()\n            tr_loss = tr_loss + loss.item() \n        loss_per_epoch.append(tr_loss/(epoch+1))\n        test_accuracy = do_test(test,model,tokenizer,device)\n        test_accuracy_per_epoch.append(test_accuracy)\n        print(epoch,test_accuracy)\n    \n    return loss_per_epoch,test_accuracy_per_epoch\n\ndef do_test(data,model,tokenizer,device):\n    eval_data = create_data_set(data,tokenizer)\n    eval_data_loader = DataLoader(eval_data,batch_size=128)\n    #print(\"Number of Batch:\",eval_data_loader.batch_size)\n    model.eval()\n    accuracy = 0.0\n    with torch.no_grad():\n        for ids,type_ids,att_mask,labels in tqdm(eval_data_loader):\n            batch = tuple((ids.to(device),att_mask.to(device),type_ids.to(device),labels.to(device)))\n            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n            outputs = model(**inputs)\n            logits = outputs[1]\n            preds = logits.detach().cpu().numpy()\n            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n            preds = np.argmax(preds,axis=1)\n            print(out_label_ids)\n            print(preds)\n            print(accuracy_score(out_label_ids,preds,normalize=False))\n            accuracy = accuracy + accuracy_score(out_label_ids,preds,normalize=False)\n    \n    #print(accuracy,len(data))\n    return (accuracy/len(data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def main():\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    #device = \"cpu\"\n    config = DistilBertConfig()\n    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n    #tokenizer.add_special_tokens\n    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased')\n    model.to(device)\n    loss_per_epoch, eval_per_epoch = do_train(train,test,1,model,tokenizer,device)\n    #return model\n    return (model,loss_per_epoch, eval_per_epoch)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train/Evaluate Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n#model,loss_per_epoch, eval_per_epoch  = main()\n#print(do_test(test,model,tokenizer,\"cuda\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test.iloc[2][\"title\"])\nprint(test.iloc[2][\"Label\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test Model on Some Hand Created Data to understand what model has learnt"},{"metadata":{"trusted":true},"cell_type":"code","source":"text = test.iloc[923][\"total_data\"]\n#text = \"Trump fails in democracy [END] Trump held a meeting today but it failed to produce proper outcome\"\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\ninput_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0).to(\"cuda\")  # Batch size 1\noutputs = model(input_ids)\noutputs = outputs[0].detach().cpu().numpy()\npreds = np.argmax(outputs,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The model has not learnt the language , it has only detected patterns in data Trump Never Very scary will be fake while Trump never very scary will be real\n\n\n# Similarly TRUMP is RACIST will be fake while trump is racist is real\n\n# I understood this fact once I trained the model purely on the titles of the news articles"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds,outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = 923\nprint(test.iloc[ids][\"total_data\"])\nprint(test.iloc[ids][\"Label\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}