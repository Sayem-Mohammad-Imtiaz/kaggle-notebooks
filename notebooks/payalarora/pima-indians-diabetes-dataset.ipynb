{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading the data\ndata = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking for missing values\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the data types\ndata.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# descriptive statistics\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, minimum value for \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\" is 0, but this can not be possible. Probably, 0 is for missing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking number of missing values for these columns\ncolumns = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']\nfor i in columns:\n    print(\"missing values in\",i,\" :\",(data[i] == 0).sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking outliers and distribution of these columns for imputation\nfor i in columns:\n    fig,axes = plt.subplots(1,2,figsize=(10,5))\n    sns.boxplot(x=data[i],orient = 'v',ax = axes[0])\n    sns.distplot(data[i],ax = axes[1])\n    fig.tight_layout()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the outliers and distribution, we can impute the missing values in \"Glucose\", \"BloodPressure\" and \"BMI\" with \"mean\" value and \"SkinThickness\" and \"Insulin\" with \"median\" value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# imputing missing values i.e. '0'\ndata['Glucose'].replace(0,data['Glucose'].mean(),inplace = True)\ndata['BloodPressure'].replace(0,data['BloodPressure'].mean(),inplace = True)\ndata['BMI'].replace(0,data['BMI'].mean(),inplace = True)\ndata['SkinThickness'].replace(0,data['SkinThickness'].median(),inplace = True)\ndata['Insulin'].replace(0,data['Insulin'].median(),inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have successfully imputed missing value.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now let's check the outliers in our data and how to handle them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,10))\nsns.boxplot(data=data, width= 0.5,ax=ax,  fliersize=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in data.columns:\n    plt.figure(figsize = (5,5))\n    sns.distplot(data[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the outliers in some specific columns and their distribution, outliers are removed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing outliers\ndata = data[data['SkinThickness']<80]\ndata = data[data['Insulin']<580]\ndata = data[data['BMI']<60]\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Looking at the distribution again \nfor i in data.columns:\n    plt.figure(figsize = (5,5))\n    sns.distplot(data[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, data distribution looks better than before.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# separating dependent and independent features\nX = data.drop(\"Outcome\",axis=1)\ny = data['Outcome']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# heatmap for checking correlation\ncorr_matrix = data.corr()\nsns.heatmap(corr_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's not much correlation in the data. Hence, no problem of multicollinearity","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Standard Scaling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# scaling the features\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_scaled = sc.fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled,columns = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting training and test data (80:20) ratio\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X_scaled,y,test_size = 0.20,random_state = 30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nclf_svm1 = SVC()\nclf_svm1.fit(X_train,y_train)\ny_pred = clf_svm1.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# accuracy of SVC\nfrom sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score\naccuracy = accuracy_score(y_test,y_pred)\nprint(\"SVC Accuracy:\",accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting the confusion matrix \nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(clf_svm1,X_test,y_test,values_format = 'd',display_labels = ['diabetic','not diabetic'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# selecting different parameters to use for improving the SVC Accuracy\nparam_grid = [\n    {'C' : [0.5,1,10,100],\n     'gamma' : ['scale','auto',1,0.1,0.01,0.001,0.0001],\n    'kernel' : ['linear','poly','rbf']}\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameter optimisation\nfrom sklearn.model_selection import GridSearchCV\n\noptimal_params = GridSearchCV(SVC(),param_grid,cv = 5,scoring = 'accuracy',verbose = 0)\noptimal_params.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(optimal_params.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_svm2 = SVC(C = 100, gamma = 0.0001,probability = True)\nclf_svm2.fit(X_train,y_train)\nsvm_y_pred = clf_svm2.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,accuracy_score\naccuracy = accuracy_score(y_test,svm_y_pred)\nprint(\"SVC Accuracy score:\",accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy is improved ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import recall_score,precision_score,f1_score\nrecall = recall_score(y_test,svm_y_pred)\nprecision = precision_score(y_test,svm_y_pred)\nf1 = f1_score(y_test,svm_y_pred)\nprint(\"SVC Recall:\",recall)\nprint(\"SVC Precision:\",precision)\nprint(\"SVC F1:\",f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,svm_y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting confusion matrix again\nplot_confusion_matrix(clf_svm2,X_test,y_test,values_format = 'd',display_labels = ['diabetic','not diabetic'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr_clf1 = LogisticRegression()\nlr_clf1.fit(X_train,y_train)\nlr_y_pred = lr_clf1.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_lr = accuracy_score(y_test,lr_y_pred)\nprint(\"Logistic Regression Accuracy:\",accuracy_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting confusion matrix\nplot_confusion_matrix(lr_clf1,X_test,y_test,values_format = 'd',display_labels = ['diabetic','not diabetic'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Hyperparameter optimisation\nfrom sklearn.model_selection import GridSearchCV\ngrid_values = {'penalty': ['l2'], 'C': [0.001,0.01,0.1,1,10,100,1000]}\nlr_optimal_params = GridSearchCV(LogisticRegression(),grid_values,cv =5,verbose = 0)\nlr_optimal_params.fit(X_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lr_optimal_params.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_clf2 = LogisticRegression(C = 1, penalty = 'l2')\nlr_clf2.fit(X_train,y_train)\nlr_y_pred = lr_clf2.predict(X_test)\naccuracy = accuracy_score(y_test,lr_y_pred)\nprint(\"Accuracy score:\",accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy remained same","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(lr_clf2,X_test,y_test,values_format = 'd',display_labels = ['diabetic','not diabetic'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import recall_score,precision_score,f1_score\nrecall = recall_score(y_test,lr_y_pred)\nprecision = precision_score(y_test,lr_y_pred)\nf1 = f1_score(y_test,lr_y_pred)\nprint(\"LR Recall:\",recall)\nprint(\"LR Precision:\",precision)\nprint(\"LR F1:\",f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,lr_y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# applying random forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf_clf = RandomForestClassifier(random_state = 32)\nmodel = rf_clf.fit(X_train,y_train)\nrf_y_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test,rf_y_pred)\nprint(\"Accuracy score:\",accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = [{\n    'n_estimators' : [10,20,50,100,200,300,400],\n    'criterion' : ['gini','entropy'],\n    'max_leaf_nodes' : range(8,32)\n}]\noptimal_params = GridSearchCV(RandomForestClassifier(random_state = 32),params,cv =5,verbose = 0)\noptimal_params.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(optimal_params.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = RandomForestClassifier(n_estimators=10,max_leaf_nodes = 29,criterion = 'entropy',random_state = 32 )\nmodel = rf_clf.fit(X_train,y_train)\nrf_y_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test,rf_y_pred)\nprint(\"Accuracy score:\",accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(rf_clf,X_test,y_test,values_format = 'd',display_labels = ['diabetic','not diabetic'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import recall_score,precision_score,f1_score\nrecall = recall_score(y_test,rf_y_pred)\nprecision = precision_score(y_test,rf_y_pred)\nf1 = f1_score(y_test,rf_y_pred)\nprint(\"RF Recall:\",recall)\nprint(\"RF Precision:\",precision)\nprint(\"RF F1:\",f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,rf_y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient Boosting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngb_clf = GradientBoostingClassifier()\ngb_clf.fit(X_train,y_train)\ngb_y_pred = gb_clf.predict(X_test)\naccuracy = accuracy_score(y_test,gb_y_pred)\nprint(\"Accuracy score:\",accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_params = [{\n    'learning_rate' : [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n    'criterion' : ['friedman_mse', 'mse', 'mae'],\n    'min_samples_leaf' : range(1,6)\n}]\ngb_optimal_params = GridSearchCV(GradientBoostingClassifier(),gb_params,cv =5,verbose = 0)\ngb_optimal_params.fit(X_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(gb_optimal_params.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_clf = GradientBoostingClassifier(criterion = 'mse',learning_rate = 0.1,min_samples_leaf = 4 )\ngb_clf.fit(X_train,y_train)\ngb_y_pred = gb_clf.predict(X_test)\naccuracy = accuracy_score(y_test,gb_y_pred)\nprint(\"Accuracy score:\",accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(gb_clf,X_test,y_test,values_format = 'd',display_labels = ['diabetic','not diabetic'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import recall_score,precision_score,f1_score\nrecall = recall_score(y_test,gb_y_pred)\nprecision = precision_score(y_test,gb_y_pred)\nf1 = f1_score(y_test,gb_y_pred)\nprint(\"GB Recall:\",recall)\nprint(\"GB Precision:\",precision)\nprint(\"GB F1:\",f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing the classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,gb_y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying Knn and finding the best value of 'k'\nfrom sklearn.neighbors import KNeighborsClassifier\n\ntest_scores = []\ntrain_scores = []\naccuracy_max = 0\nk_max = 0\nfor i in range(1,30):\n    knn = KNeighborsClassifier(i)\n    knn.fit(X_train,y_train)\n    y_pred = knn.predict(X_test)\n    \n    train_accuracy = knn.score(X_train,y_train)\n    train_scores.append(train_accuracy)\n    test_accuracy = accuracy_score(y_test,y_pred)\n    test_scores.append(test_accuracy)\n    if test_accuracy > accuracy_max:\n        accuracy_max = test_accuracy\n        k_max = i\nprint(\"maximum_test_accuracy:\",accuracy_max, \"is achieved at k:\",k_max)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn1 = KNeighborsClassifier(7)\nknn1.fit(X_train,y_train)\nknn_y_pred = knn1.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(knn1,X_test,y_test,values_format = 'd',display_labels = ['diabetic','not diabetic'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameter optimisation\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'n_neighbors':np.arange(1,50)}\nknn2 = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn2,param_grid,cv =5,verbose = 0)\nknn_cv.fit(X_train,y_train)\n\nprint(\"Best Parameters: \" + str(knn_cv.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn2 = KNeighborsClassifier(n_neighbors = 17)\nknn2.fit(X_train,y_train)\nknn_y_pred = knn2.predict(X_test)\nacc = accuracy_score(y_test,knn_y_pred)\nacc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import recall_score,precision_score,f1_score\nrecall = recall_score(y_test,knn_y_pred)\nprecision = precision_score(y_test,knn_y_pred)\nf1 = f1_score(y_test,knn_y_pred)\nprint(\"KNN Recall:\",recall)\nprint(\"KNN Precision:\",precision)\nprint(\"KNN F1:\",f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting training and test scores of KNN model\nplt.figure(figsize=(12,5))\np = sns.lineplot(range(1,30),train_scores,marker='*',label='Train Score')\np = sns.lineplot(range(1,30),test_scores,marker='o',label='Test Score')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that maximum value of k = 17 where the test score is maximum.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" We can observe that every model has almost same accuracy score.\n But Recall score is different for each model.\n Recall is the ratio of correctly predicted positive values out of all the actual positive values.\n Here, recall signifies the correctly identified diabetic patients out of all the diabetic patients. This value should be maximum.\n Random Forest has the maximum recall value.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can plot ROC curve to identify the best model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_y_pred_proba = clf_svm2.predict_proba(X_test)[:,1]\nlr_y_pred_proba = lr_clf2.predict_proba(X_test)[:,1]\nrf_y_pred_proba = rf_clf.predict_proba(X_test)[:,1]\ngb_y_pred_proba = gb_clf.predict_proba(X_test)[:,1]\nknn_y_pred_proba = knn1.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfpr_svc, tpr_svc, thresholds_svc = roc_curve(y_test, svc_y_pred_proba)\nfpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, lr_y_pred_proba)\nfpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, rf_y_pred_proba)\nfpr_gb, tpr_gb, thresholds_gb = roc_curve(y_test, gb_y_pred_proba)\nfpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test, knn_y_pred_proba)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the ROC Curve\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr_svc,tpr_svc, label='SVC')\nplt.plot(fpr_knn,tpr_knn, label='KNN')\nplt.plot(fpr_rf,tpr_rf, label='RF')\nplt.plot(fpr_gb,tpr_gb, label='GB')\nplt.plot(fpr_lr,tpr_lr, label='LR')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.legend()\nplt.title('ROC curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Area under ROC curve\nfrom sklearn.metrics import roc_auc_score\nknn_auc = roc_auc_score(y_test,knn_y_pred_proba)\nlr_auc = roc_auc_score(y_test,lr_y_pred_proba)\nsvc_auc = roc_auc_score(y_test,svc_y_pred_proba)\nrf_auc = roc_auc_score(y_test,rf_y_pred_proba)\ngb_auc = roc_auc_score(y_test,gb_y_pred_proba)\nprint(\"Area under KNN ROC curve:\",knn_auc)\nprint(\"Area under LR ROC curve:\",lr_auc)\nprint(\"Area under SVC ROC curve:\",svc_auc)\nprint(\"Area under RF ROC curve:\",rf_auc)\nprint(\"Area under GB ROC curve:\",gb_auc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Area under Logistic Regression,Support Vector Classifier and Random Forest is almost same and more than KNN and Gradient Boost.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If you like this Kernel, please upvote. I am new to kaggle and this is my first upload. \nAlso, suggestions for improvements and mistakes are welcome","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# End of the Notebook","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}