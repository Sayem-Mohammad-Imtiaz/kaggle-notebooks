{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Getting Started**\n\nTitle : Loan Status Prediction\n\nLoan Status :\n\n0 -- > Low Quality Wine\n\n1 -- > Good Quality Wine ","metadata":{}},{"cell_type":"code","source":"#Importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the dataset to a pandas DataFrame\n# Read .csv file into dataframe\ndata = pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Exploratory data analysis**","metadata":{}},{"cell_type":"code","source":"#Shape of data \nprint(data.shape)\n#dtypes of data \nprint(data.dtypes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Info of data\ndata.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# value_counts\ndata[\"quality\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mean value ofred wine\ndata.groupby(\"quality\").mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# describe the data\ndata.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# missing_values\ndata.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of values for each quality\nsns.catplot(x='quality', data = data, kind = 'count')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# volatile acidity vs Quality\nplot = plt.figure(figsize=(5,5))\nsns.barplot(x='quality', y = 'volatile acidity', data = data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# citric acid vs Quality\nplot = plt.figure(figsize=(5,5))\nsns.barplot(x='quality', y = 'citric acid', data = data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Correlation**\n\n1.0   -->  Positive Correlation\n\n-0.0  --> Negative Correlation","metadata":{}},{"cell_type":"code","source":"correlation = data.corr()\n# constructing a heatmap to understand the correlation between the columns\nplt.figure(figsize=(10,10))\nsns.heatmap(correlation, cbar=True, square=True, fmt = '.1f', annot = True, annot_kws={'size':8}, cmap = 'Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Transformation**","metadata":{}},{"cell_type":"code","source":"#label binarization\ntransform = data['quality'].apply(lambda y_value: 1 if y_value >= 7 else 0)\ntransform.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model Preparation**\n","metadata":{}},{"cell_type":"code","source":"# separating the data and label\nX = data.drop(['quality'], axis=1)\ny = transform\nprint(\"The shape of X is \" ,X.shape)\nprint(\"The shape of Y is \" ,y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking value counts again\ny.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_test_spilt\nX_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y , test_size=0.2, random_state=42)\nprint(\"The shape of X_train is\", X_train.shape )\nprint(\"The shape of X_test is\", X_test.shape)\nprint(\"The shape of y_train is\", y_train.shape)\nprint(\"The shape of y_test is\", y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking again for value_counts\ny_train.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking again for value_counts\ny_test.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **After stratify we have almost equal number of y_train & y_test values.**","metadata":{}},{"cell_type":"markdown","source":"# **Model Training**\n\nWe will train different model after the evaluation of model we will select out best model for production.\n\n1.   SVM Model\n2.   Logistic Regression\n3.   Decision Tree\n4.   Random Forest Regressor\n5.   KNeighborsClassifier\n6.   AdaBoost Classifier\n7.   Xgb Boost Classifier","metadata":{}},{"cell_type":"markdown","source":"## **SVM model**","metadata":{}},{"cell_type":"code","source":"from sklearn import svm\nclassifier_model = svm.SVC(kernel='linear')\nclassifier_model.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Logistic Regression Model**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogistic_model = LogisticRegression(max_iter=700)\nlogistic_model.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Feature Scaling for Decision tree, Random Forest & boosting**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nsc_X_train = sc_X.fit_transform(X_train)\nsc_X_test = sc_X.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Decision Tree**","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndecision_tree_model = DecisionTreeClassifier(random_state = 0)\ndecision_tree_model.fit(sc_X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Hyper Parameter Tuning For DTC**\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ngrid_params = {\n    'criterion' : ['gini', 'entropy'],\n    'max_depth' : [3, 5, 7, 10],\n    'min_samples_split' : range(2, 10, 1),\n    'min_samples_leaf' : range(2, 10, 1)\n}\n\ngrid_search = GridSearchCV(decision_tree_model, grid_params, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search.fit(sc_X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best parameters and best score\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **KNeighborsClassifier**","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nk_model = KNeighborsClassifier(n_neighbors=35)\nkfitModel = k_model.fit(sc_X_train, y_train)\nprint(kfitModel)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finding optimal values for k\nfrom sklearn.model_selection import cross_val_score\ncross_valid_scores = []\nfor k in range(1, 100):\n  knn = KNeighborsClassifier(n_neighbors = k)\n  scores = cross_val_score(knn,X, y, cv = 10, scoring = 'accuracy')\n  cross_valid_scores.append(scores.mean())    \n\nprint(\"Optimal k with cross-validation: \\t\",np.argmax(cross_valid_scores))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Random Forest model**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodelRF = RandomForestClassifier()\nmodelRF.fit(sc_X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model Evaluation**\n### **Model Evaluation Of SVM**","metadata":{}},{"cell_type":"code","source":"# accuracy score on training data\n\nX_train_prediction = classifier_model.predict(X_train)\ntraining_data_accuray = accuracy_score(X_train_prediction,y_train)\nprint('Accuracy of SVM model on training data : ', training_data_accuray)\n\n# accuracy score on testing data\n\nX_test_prediction = classifier_model.predict(X_test)\nsvm_test_data_accuray = accuracy_score(X_test_prediction,y_test)\nprint('Accuracy of SVM model on test data    : ', svm_test_data_accuray)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Model Evaluation of LGR**","metadata":{}},{"cell_type":"code","source":"# accuracy score on training data\n\nX_train_prediction = logistic_model.predict(sc_X_train)\ntraining_data_accuray = accuracy_score(X_train_prediction,y_train)\nprint('Accuracy of LGR model on training data  : ', training_data_accuray)\n\n# accuracy score on testing data\nX_test_prediction = logistic_model.predict(sc_X_test)\nlgr_test_data_accuray = accuracy_score(X_test_prediction,y_test)\nprint('Accuracy of LGR model on test data      : ', lgr_test_data_accuray)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model Evaluation of DTR after hypertuning**","metadata":{}},{"cell_type":"code","source":"dtc = grid_search.best_estimator_\ny_pred = dtc.predict(sc_X_test)\ndtc_train_acc = accuracy_score(y_train, dtc.predict(sc_X_train))\ndtc_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Decesion Tree Model  is {dtc_train_acc}\")\nprint(f\"Test Accuracy of Decesion Tree Model      is {dtc_test_acc}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualization for DTR trees**","metadata":{}},{"cell_type":"code","source":"from sklearn import tree\nplt.figure(figsize=(15,10))\ntree.plot_tree(dtc,filled=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model Evaluation of Random Forest**","metadata":{}},{"cell_type":"code","source":"# accuracy on test data\nX_test_prediction = modelRF.predict(X_test)\nkr_test_data_accuracy = accuracy_score(X_test_prediction, y_test)\nprint('Accuracy : ', kr_test_data_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model Evaluation of KNN**","metadata":{}},{"cell_type":"code","source":"kX_train_prediction = kfitModel.predict(sc_X_train)\ntraining_data_accuray = accuracy_score(kX_train_prediction,y_train)\nprint('Accuracy on training data  : ', training_data_accuray)\n\n# accuracy score on testing data\nkX_test_prediction = kfitModel.predict(sc_X_test)\nkx_lgr_test_data_accuray = accuracy_score(kX_test_prediction,y_test)\nprint('Accuracy on test data      : ', kx_lgr_test_data_accuray)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Boosting**","metadata":{}},{"cell_type":"code","source":"#Ada Boost Classifie\nfrom sklearn.ensemble import AdaBoostClassifier\n\nada = AdaBoostClassifier(base_estimator = dtc)\n\nparameters = {\n    'n_estimators' : [50, 70, 90, 120, 180, 200],\n    'learning_rate' : [0.001, 0.01, 0.1, 1, 10],\n    'algorithm' : ['SAMME', 'SAMME.R']\n}\n\ngrid_search = GridSearchCV(ada, parameters, n_jobs = -1, cv = 5, verbose = 1)\ngrid_search.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(grid_search.best_params_)\nprint(grid_search.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ada = AdaBoostClassifier(base_estimator = dtc, algorithm = 'SAMME', learning_rate = 1, n_estimators = 50)\nada.fit(sc_X_train, y_train)\n\nada_train_acc = accuracy_score(y_train, ada.predict(sc_X_train))\nada_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Ada Boost Model is {ada_train_acc}\")\nprint(f\"Test Accuracy of Ada Boost Model is {ada_test_acc}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix, classification_report\nconfusion_matrix(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classification report\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Xg Boost**","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(booster = 'gblinear', learning_rate = 1, max_depth = 3, n_estimators = 10)\nxgb.fit(sc_X_train, y_train)\n\ny_pred = xgb.predict(sc_X_test)\n\nxgb_train_acc = accuracy_score(y_train, xgb.predict(sc_X_train))\nxgb_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of XGB Model is {xgb_train_acc}\")\nprint(f\"Test Accuracy of XGB Model is {xgb_test_acc}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Models Best Scores : -**","metadata":{}},{"cell_type":"code","source":"models = ['xg Boost','Ada Boost Classifier','Logistic Regression','KNN','SVC', 'Decision Tree', 'Random Forest']\nscores = [xgb_test_acc,ada_test_acc,lgr_test_data_accuray,kx_lgr_test_data_accuray, svm_test_data_accuray, dtc_test_acc, kr_test_data_accuracy]\nmodels = pd.DataFrame({'Model' : models, 'Score' : scores})\nmodels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ***Xg Boost Ada boost classifier have almost same value but DTC & KNN give us the best result we will use KNN for production. lets visualize best score more.***","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize = (18, 8))\n\nsns.barplot(x = 'Model', y = 'Score', data = models)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Pridictive System for KNN.**","metadata":{}},{"cell_type":"code","source":"input_data = (7.5,0.5,0.36,6.1,0.071,17.0,102.0,0.9978,3.35,0.8,10.5)\n\n# changing the input data to a numpy array\ninput_data_as_numpy_array = np.asarray(input_data)\n\n# reshape the data as we are predicting the label for only one instance\ninput_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n\nprediction = kfitModel.predict(input_data_reshaped)\nprint(prediction)\n\nif (prediction[0]==1):\n  print('Good Quality Wine')\nelse:\n  print('Bad Quality Wine')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Pridictive System for DTC.**","metadata":{}},{"cell_type":"code","source":"input_data = (7.5,0.5,0.36,6.1,0.071,17.0,102.0,0.9978,3.35,0.8,10.5)\n\n# changing the input data to a numpy array\ninput_data_as_numpy_array = np.asarray(input_data)\n\n# reshape the data as we are predicting the label for only one instance\ninput_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n\nprediction = decision_tree_model.predict(input_data_reshaped)\nprint(prediction)\n\nif (prediction[0]==1):\n  print('Good Quality Wine')\nelse:\n  print('Bad Quality Wine')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **If you find this notebook usefull please upvote.** ❤️","metadata":{}}]}