{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This notebook is not on predicting if a tweet is true. It is to practise on topic modeling on tweets. Thanks","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib as plt\nimport seaborn as sns\nimport re\nimport string\nfrom sklearn.feature_extraction.text import CountVectorizer \nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import NMF\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read in the data set\ndata=pd.read_csv('../input/disaster-tweets/tweets.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preview the data set\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#A tweet example\ndata['text'][3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove, web link, numbers, punctuations, and to lower case\nwebsite= lambda x: re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \"\", x)\nalphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\npunc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n\ndata['reviews'] = data.text.map(website).map(alphanumeric).map(punc_lower)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Taking only the True events\nbag=data.loc[data['target']==1,'reviews']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the same tweet has been processed\nbag[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Count vectorizer with unigram\nvectorizer1 = CountVectorizer(stop_words='english')\ndoc_word1 = vectorizer1.fit_transform(bag)\ndoc_word1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check out the doc-word matrix\npd.DataFrame(doc_word1.toarray(), index=bag, columns=vectorizer1.get_feature_names()).head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a fucntion to display the topics and its word distributions\ndef display_topics(model, feature_names, no_top_words, topic_names=None):\n    for ix, topic in enumerate(model.components_):\n        if not topic_names or not topic_names[ix]:\n            print(\"\\nTopic \", ix)\n        else:\n            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n        print(\", \".join([feature_names[i]\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Latent Semantic Analysis with SVD\nlsa1 = TruncatedSVD(10,random_state=123)\ndoc_topic1 = lsa1.fit_transform(doc_word1)\n#lsa.explained_variance_ratio_,lsa.explained_variance_ratio_.sum()\ndisplay_topics(lsa1, vectorizer1.get_feature_names(), 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Count vectorizer with bigram\nvectorizer2 = CountVectorizer(ngram_range=(2,2), stop_words='english')\ndoc_word2 = vectorizer2.fit_transform(bag)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Latent Semantic Analysis with SVD\nlsa2 = TruncatedSVD(10,random_state=123)\ndoc_topics2 = lsa2.fit_transform(doc_word2)\n#lsa.explained_variance_ratio_,lsa.explained_variance_ratio_.sum()\ndisplay_topics(lsa2, vectorizer2.get_feature_names(), 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Explained variance ratio\nlsa2.explained_variance_ratio_,lsa2.explained_variance_ratio_.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#doc-term matrix\nVt2 = pd.DataFrame(doc_topics2.round(5),\n             index = bag,\n             columns = [\"Thunderstorm\",\"Tornado\",\"Train Accident\",\"Volcano Eruption\",\"Sinkhole\",\"Nuclear Meltdown\",\"Hail Storm\",\"Virus OutBreak\",\"Terrorist Bombing\",\"Electrocution\" ])\nVt2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tag each tweet with a topic\nVt2['Cat']=Vt2.idxmax(axis=1, skipna=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make a count of tweets in each topic\ncount=pd.DataFrame(Vt2.Cat.value_counts().sort_values(ascending=False))\ncount.rename(columns={'Cat':'Tweets Count'},inplace=True)\ncount","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot it\nsns.set_color_codes(\"pastel\")\nsns.barplot(x=count['Tweets Count'], y=count.index, data=count, orient='h', color=\"b\",)\n#plt.savefig('hbar.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Count vectorizer with trigram\nvectorizer3 = CountVectorizer(ngram_range=(3,3), stop_words='english')\ndoc_words3 = vectorizer3.fit_transform(bag)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Latent Semantic Analysis with SVD\nlsa3 = TruncatedSVD(8,random_state=123)\ndoc_topics3 = lsa3.fit_transform(doc_words3)\n#lsa.explained_variance_ratio_,lsa.explained_variance_ratio_.sum()\ndisplay_topics(lsa3, vectorizer3.get_feature_names(), 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Unigram and trigram do not give some distinction of topic, but bigram does, and with k = 10 because anything more are just repetive.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}