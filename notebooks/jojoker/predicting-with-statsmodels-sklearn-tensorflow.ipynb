{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Let's Start Our Journey"},{"metadata":{},"cell_type":"markdown","source":"In this Journey, We are going to try to predict houses prices"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nraw_data = pd.read_csv('../input/housesalesprediction/kc_house_data.csv')\nraw_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"there is no missing values"},{"metadata":{},"cell_type":"markdown","source":"# **Variable Definition**\n\n* *id*: unique ID for each home sold\n* *date*: date of the home sale\n* *price*: price of each home sold\n* *bedrooms*: number of berdrooms\n* *bathrooms*: number of bathrooms\n\n* *sqft_living*: square footage of the apartments interior living space\n* *sqft_lot*: square footage of the land space\n* *floors*: number of floors\n* *waterfront*: A dummy variable for whether the apartment was overlooking the waterfornt or not\n* *view*: an index from 0 to 4 of how good the view of the property was\n\n\n* *condition*: an index from 1 to 5 on the condition of the apartment\n* *grade*: an index from 1 to 13 where, 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design\n* *sqft_above*: the square footage of the interior housing space that is above ground level\n* *sqft_basement*: the square footage of the interior housing space that is below ground level\n* *yr_built*: the year the house was initially built\n\n\n* *yr_renovated*: the year of the house's last renovation\n* *zipcode*: what zipcode are the house is in\n* *lat*: lattitude\n* *long*: longitude\n* *sqft_living15*: the square footage of interior housing living space for the nearest 15 neighbors\n* *sqft_lot15*: the square footage of the land lots of the nearest 15 neighbors\n\nSource: https://www.slideshare.net/PawanShivhare1/predicting-king-county-house-prices"},{"metadata":{},"cell_type":"markdown","source":"# First thing First"},{"metadata":{},"cell_type":"markdown","source":"## Drop the unnecessary variable\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# I think We don't need ID. So we should remove it from our data\ndata = raw_data.drop('id', axis = 1)\n\n# I try to drop zipcode because We have had Latitude and Longitude data\ndata = data.drop('zipcode', axis = 1)\n\n# and actually I don't understand about sqft_living15 and sqft_lot15, so I decided to remove it\ndata = data.drop(['sqft_living15','sqft_lot15'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove Imposible House\n\nin this section, We try to remove data (row) which has these categories:\n\n* sqft_lot < sqft_living\n* sqft_lot < sqft_above\n* sqft_lot < sqft_basement\n\nbecause We can't build something that larger than our land. It's illegal"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[data['sqft_lot'] >= data['sqft_living']]\ndata = data[data['sqft_lot'] >= data['sqft_above']]\ndata = data[data['sqft_lot'] >= data['sqft_basement']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing each Variable"},{"metadata":{},"cell_type":"markdown","source":"## Date\n\nWe should turn this data into day, month, and year"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata['date'] = pd.to_datetime(data['date'], infer_datetime_format=True)\n\ndata['date'] = pd.to_datetime(data['date'], format = '%Y/%m/%d')\n\n# this code change value format into datetime format","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['day'] = data['date'].dt.day\ndata['month'] = data['date'].dt.month\n\n#I think We don't need year data, because it contains two values only (2014 and 2015)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we drop date columns, because We have split the data into day, month, and year\ndata = data.drop('date', axis=1)\n\n# reorder the columns\ndata = data[['price', 'day', 'month','bedrooms', 'bathrooms', 'sqft_living',\n       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated',\n       'lat', 'long']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Price"},{"metadata":{"trusted":true},"cell_type":"code","source":"#make a checkpoint\ndata_price  = data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_price['price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data_price['price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normal distribution check\nprice_kurtosis = data_price['price'].kurt()\nprice_skewness = data_price['price'].skew()\n\nprint ('Kurtosis and Skewness of Price \\n')\nprint ('Kurtosis: ' + str(price_kurtosis))\nprint ('Skewness: ' + str(price_skewness))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on [this Scientific journal](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3591587/), data can  be  assumed  to  be  normally  distributed if skewness < 2  and  kurtosis < 7.\n\nAs You can see, Our data is \"not normal\". We should do something"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Based on our boxplot, let's try to remove the outlier with threshold 6000000\ndata_price = data_price[data_price['price'] <= 6000000]\n\n#let's check kurtosis and skewness again\nprice_kurtosis = data_price['price'].kurt()\nprice_skewness = data_price['price'].skew()\nprint ('Kurtosis and Skewness of Price \\n')\nprint ('Kurtosis: ' + str(price_kurtosis))\nprint ('Skewness: ' + str(price_skewness))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"there is no significant change, so let's going back to our check point and transform it into natural log, \n\nbecause Our Kurtosis is more than 10. Which indicate that Our data has exponential data, which mean it's going to be better if We turn this data into natural log. let's try"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_price  = data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_price['log_price'] = np.log(data_price['price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_price['log_price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data_price['log_price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#recheck Our kurtosis and skewness score again\nprice_kurtosis = data_price['log_price'].kurt()\nprice_skewness = data_price['log_price'].skew()\n\nprint ('Final Our Kurtosis and Skewness \\n')\nprint ('Kurtosis: ' + str(price_kurtosis))\nprint ('Skewness: ' + str(price_skewness))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's better, it has significant change."},{"metadata":{"trusted":true},"cell_type":"code","source":"# before move on, I think We should drop price columns because We have log_price now\ndata_price = data_price.drop('price', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Bedroom\n \nnote: zero means the house doesn't have independent room for bedroom"},{"metadata":{"trusted":true},"cell_type":"code","source":"#make a checkpoint\ndata_bedroom = data_price.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_bedroom['bedrooms'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_bedroom['bedrooms'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data_bedroom['bedrooms'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normal distribution check\nbedroom_kurtosis = data_bedroom['bedrooms'].kurt()\nbedroom_skewness = data_bedroom['bedrooms'].skew()\n\nprint ('Kurtosis and Skewness of Bedroom \\n')\nprint ('Kurtosis: ' + str(bedroom_kurtosis))\nprint ('Skewness: ' + str(bedroom_skewness))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on [this Scientific journal](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3591587/), data can  be  assumed  to  be  normally  distributed if skewness < 2  and  kurtosis < 7. \n\nAs You can see, Our data is \"not normal\". We should do something"},{"metadata":{"trusted":true},"cell_type":"code","source":"#based on Our boxplot let's try to remove the outlier with threshold is 15\ndata_bedroom = data_bedroom[data_bedroom['bedrooms'] <= 15]\n\n#let's check kurtosis and skewness again\nbedroom_kurtosis = data_bedroom['bedrooms'].kurt()\nbedroom_skewness = data_bedroom['bedrooms'].skew()\nprint ('Final Our Kurtosis and Skewness \\n')\nprint ('Kurtosis: ' + str(bedroom_kurtosis))\nprint ('Skewness: ' + str(bedroom_skewness))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, there is significant changes. So We don't worry about this variable anymore"},{"metadata":{},"cell_type":"markdown","source":"## Bathrooms"},{"metadata":{},"cell_type":"markdown","source":"from [this article](https://www.badeloftusa.com/buying-guides/bathrooms/) \nWe know that there is 4 feature in toilet, sink, shower, toilet, and bathub.\n\n* 0.25 bathroom means 1 feature\n* 0.50 bathroom means 2 features\n* 0.75 bathroom means 3 features\n* 1.00 bathroom means 4 features\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#checkpoint\ndata_bathroom = data_bedroom.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_bathroom['bathrooms'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data_bathroom['bathrooms'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normal distribution check\nbathroom_kurtosis = data_bathroom['bathrooms'].kurt()\nbathroom_skewness = data_bathroom['bathrooms'].skew()\n\nprint ('Kurtosis and Skewness of Bathrooms \\n')\nprint ('Kurtosis: ' + str(bathroom_kurtosis))\nprint ('Skewness: ' + str(bathroom_skewness))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on [this Scientific journal](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3591587/), data can  be  assumed  to  be  normally  distributed if skewness < 2  and  kurtosis < 7. \n\nAs You can see, Our data is \"normal\". "},{"metadata":{},"cell_type":"markdown","source":"## sqft_living"},{"metadata":{"trusted":true},"cell_type":"code","source":"#checkpoint\ndata_sqft_living = data_bathroom.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_sqft_living['sqft_living'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data_sqft_living['sqft_living'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normal distribution check\n\nsqft_living_kurtosis = data_sqft_living['sqft_living'].kurt()\nsqft_living_skewness = data_sqft_living['sqft_living'].skew()\n\nprint ('Kurtosis and Skewness of Sqft Living \\n')\nprint ('Kurtosis: ' + str(sqft_living_kurtosis))\nprint ('Skewness: ' + str(sqft_living_skewness))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on [this Scientific journal](http://web.b.ebscohost.com/ehost/pdfviewer/pdfviewer?vid=0&sid=bdfdb145-c94f-4f92-bf57-9fd88ce4cd02%40sessionmgr103), data can  be  assumed  to  be  normally  distributed if skewness < 3  and  kurtosis < 10. \n\nAs You can see, Our data is \"normal\". "},{"metadata":{},"cell_type":"markdown","source":"## sqft_lot"},{"metadata":{"trusted":true},"cell_type":"code","source":"# checkpoint\ndata_sqft_lot = data_sqft_living.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_sqft_lot['sqft_lot'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data_sqft_lot['sqft_lot'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normal distribution check\nsqft_lot_kurtosis = data_sqft_lot['sqft_lot'].kurt()\nsqft_lot_skewness = data_sqft_lot['sqft_lot'].skew()\n\nprint ('Kurtosis and Skewness of Sqft Lot \\n')\nprint ('Kurtosis: ' + str(sqft_lot_kurtosis))\nprint ('Skewness: ' + str(sqft_lot_skewness))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on [this Scientific journal](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3591587/), data can  be  assumed  to  be  normally  distributed if skewness < 2  and  kurtosis < 7. \n\nAs You can see, Our data is \"not normal\" (it's really bad data). We should do something"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Based on our boxplot, let's try to remove the outlier with threshold 1250000\ndata_sqft_lot = data_sqft_lot[data_sqft_lot['sqft_lot'] <= 1250000]\n\n#let's check kurtosis and skewness again\nsqft_lot_kurtosis = data_sqft_lot['sqft_lot'].kurt()\nsqft_lot_skewness = data_sqft_lot['sqft_lot'].skew()\n\nprint ('Kurtosis and Skewness of Sqft Lot \\n')\nprint ('Kurtosis: ' + str(sqft_lot_kurtosis))\nprint ('Skewness: ' + str(sqft_lot_skewness))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"there is no significant change, so let's going back to our check point and transform it into natural log,\n\nbecause Our Kurtosis is more than 10. Which indicate that Our data has exponential data, which mean it's going to be better if We turn this data into natural log. let's try"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_sqft_lot = data_sqft_living.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_sqft_lot['log_sqft_lot'] = np.log(data_sqft_lot['sqft_lot'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_sqft_lot['log_sqft_lot'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data_sqft_lot['log_sqft_lot'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's check kurtosis and skewness again\nsqft_lot_kurtosis = data_sqft_lot['log_sqft_lot'].kurt()\nsqft_lot_skewness = data_sqft_lot['log_sqft_lot'].skew()\n\nprint ('Final Our Kurtosis and Skewness \\n')\nprint ('Kurtosis: ' + str(sqft_lot_kurtosis))\nprint ('Skewness: ' + str(sqft_lot_skewness))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's better, it has significant change"},{"metadata":{"trusted":true},"cell_type":"code","source":"# before move on, I think We should drop sqft_lot columns because We have log_sqft_lot now\ndata_sqft_lot= data_sqft_lot.drop('sqft_lot', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Floors"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_floor = data_sqft_lot.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_floor['floors'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data_floor['floors'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normal distribution check\nfloor_kurtosis = data_floor['floors'].kurt()\nfloor_skewness = data_floor['floors'].skew()\n\nprint ('Kurtosis and Skewness of Floors \\n')\nprint ('Kurtosis: ' + str(floor_kurtosis))\nprint ('Skewness: ' + str(floor_skewness))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on [this Scientific journal](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3591587/), data can  be  assumed  to  be  normally  distributed if skewness < 2  and  kurtosis < 7 (if it's negative skewness > -2 and kurtosis > - 7). \n\nAs You can see, Our data is \"normal\". "},{"metadata":{},"cell_type":"markdown","source":"## Waterfront"},{"metadata":{},"cell_type":"markdown","source":"waterfront: A dummy variable for whether the apartment was overlooking the waterfornt or not.\n\nit's categorical data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['waterfront'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This variable have 2 categores only (0 and 1), so We don't have a problem with this data"},{"metadata":{},"cell_type":"markdown","source":"## View"},{"metadata":{},"cell_type":"markdown","source":"view: an index from 0 to 4 of how good the view of the property was\n\nIt's categorical data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# checkpoint\ndata_view = data_floor.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_view['view'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should make the dummies for this variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"# one-hot encoding\ndata_view = pd.get_dummies(data_view, columns=['view'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_view.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Condition"},{"metadata":{},"cell_type":"markdown","source":"condition: an index from 1 to 5 on the condition of the apartment\n\nIt's categorical data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['condition'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should turn this variable data into dummies"},{"metadata":{"trusted":true},"cell_type":"code","source":"# a checkpoint\ndata_condition = data_view.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one-hot encoding\ndata_condition = pd.get_dummies(data_condition, columns=['condition'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_condition.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grade"},{"metadata":{"trusted":true},"cell_type":"code","source":"# checkpoint\ndata_grade = data_condition.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_grade['grade'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data_grade['grade'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normal distribution check\ngrade_kurtosis = data_grade['grade'].kurt()\ngrade_skewness = data_grade['grade'].skew()\n\nprint ('Kurtosis and Skewness of Grade \\n')\nprint ('Kurtosis: ' + str(grade_kurtosis))\nprint ('Skewness: ' + str(grade_skewness))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on [this Scientific journal](http://web.b.ebscohost.com/ehost/pdfviewer/pdfviewer?vid=0&sid=bdfdb145-c94f-4f92-bf57-9fd88ce4cd02%40sessionmgr103), data can  be  assumed  to  be  normally  distributed if skewness < 3  and  kurtosis < 10 \n\nAs You can see, Our data is \"normal\". "},{"metadata":{},"cell_type":"markdown","source":"## Sqft_Above"},{"metadata":{},"cell_type":"markdown","source":"Before We analyze the dat, make sure this variable has a meaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# checkpoint\ndata_above = data_grade.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_above['sqft_above'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data_above['sqft_above'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normal distribution check\nabove_kurtosis = data_above['sqft_above'].kurt()\nabove_skewness = data_above['sqft_above'].skew()\n\nprint ('Kurtosis and Skewness of Sqft_Above \\n')\nprint ('Kurtosis: ' + str(above_kurtosis))\nprint ('Skewness: ' + str(above_skewness))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on [this Scientific journal](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3591587/), data can  be  assumed  to  be  normally  distributed if skewness < 2  and  kurtosis < 7. \n\nAs You can see, Our data is \"normal\". "},{"metadata":{},"cell_type":"markdown","source":"## sqft_basement"},{"metadata":{"trusted":true},"cell_type":"code","source":"# checkpoint\ndata_basement = data_above.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_basement['sqft_basement'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a lot of houses don't have basement. I think that We must transform the data into categorical data, 0 means it doesn't have  basement and 1 means it has basement"},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform the data more than 0 into 1 which mean it has basement\ndata_basement.loc[data_basement.sqft_basement > 0, 'sqft_basement'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rename the columns\ndata_basement = data_basement.rename({'sqft_basement': 'basement'}, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Year Built"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_built = data_basement.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_built['yr_built'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data_built['yr_built'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normal distribution check\nbuilt_kurtosis = data_built['yr_built'].kurt()\nbuilt_skewness = data_built['yr_built'].skew()\n\nprint ('Kurtosis and Skewness of Year Built \\n')\nprint ('Kurtosis: ' + str(built_kurtosis))\nprint ('Skewness: ' + str(built_skewness))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on [this Scientific journal](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3591587/), data can  be  assumed  to  be  normally  distributed if skewness < 2  and  kurtosis < 7 (or if they are negative use this threshold skewness > -3 and kurtosis > -10). \n\nAs You can see, Our data is \"normal\". "},{"metadata":{},"cell_type":"markdown","source":"## Year Renovated"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_renovated = data_built.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_renovated['yr_renovated'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"a lot of houses hasn't been renovated since it was built. I think We should transform the data into categorical, 0 means hasn't been renovated and 1 means has been renovated"},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform the data more than 0 into 1 which mean it has been renovated\ndata_renovated.loc[data_renovated.yr_renovated > 0, 'yr_renovated'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rename the columns\ndata_renovated = data_renovated.rename({'yr_renovated': 'renovated'}, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Latitude"},{"metadata":{"trusted":true},"cell_type":"code","source":"# checkpoint\ndata_lat = data_renovated.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_lat['lat'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data_lat['lat'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normal distribution check\nlat_kurtosis = data_lat['lat'].kurt()\nlat_skewness = data_lat['lat'].skew()\n\nprint ('Kurtosis and Skewness of Latitude \\n')\nprint ('Kurtosis: ' + str(lat_kurtosis))\nprint ('Skewness: ' + str(lat_skewness))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on [this Scientific journal](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3591587/), data can  be  assumed  to  be  normally  distributed if skewness < 2  and  kurtosis < 7 (or if they are negative use this threshold skewness > -3 and kurtosis > -10). \n\nAs You can see, Our data is \"normal\". Let's move on"},{"metadata":{},"cell_type":"markdown","source":"## Longitude"},{"metadata":{"trusted":true},"cell_type":"code","source":"# checkpoint\ndata_long = data_lat.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_lat['long'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data_lat['long'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normal distribution check\nlat_kurtosis = data_lat['lat'].kurt()\nlat_skewness = data_lat['lat'].skew()\n\nprint ('Kurtosis and Skewness of Longitude \\n')\nprint ('Kurtosis: ' + str(lat_kurtosis))\nprint ('Skewness: ' + str(lat_skewness))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on [this Scientific journal](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3591587/), data can  be  assumed  to  be  normally  distributed if skewness < 2  and  kurtosis < 7 (or if they are negative use this threshold skewness > -3 and kurtosis > -10). \n\nAs You can see, Our data is \"normal\".\n\nWe almost done cleaning Our data Yeeaaa\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's make a checkpoint and reset the index\ndata_cleaned = data_long.reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Standardizing"},{"metadata":{},"cell_type":"markdown","source":"We have a lot of type of data, in this section We try to standardizing the quantitaive data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cleaned.head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cleaned.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, We must split the data which one has quantitative data and which one has qualitative data\n\nquantitative_data = data_cleaned[['day', 'month','bedrooms', 'bathrooms', 'sqft_living', 'floors', 'grade', 'sqft_above', 'yr_built','lat', 'long', 'log_sqft_lot']]\n\n# I move the log_price to this, because It doesn't need to be standardize\nqualitative_data = data_cleaned[['waterfront', 'basement','renovated','view_0','view_1','view_2', 'view_3', 'view_4', 'condition_1','condition_2', 'condition_3','condition_4','condition_5', 'log_price',]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Warmup the engine\nfrom sklearn.preprocessing import StandardScaler\n\ndata_scaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# standardize the data\nscaled = data_scaler.fit_transform(quantitative_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# turn into pandas\nscaled_quan = pd.DataFrame(scaled, columns=quantitative_data.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine with categorical data\nscaled_data = pd.concat([scaled_quan,qualitative_data], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_data.head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our data is ready to be analyzed"},{"metadata":{},"cell_type":"markdown","source":"# Regression"},{"metadata":{},"cell_type":"markdown","source":"Before We do regression, We should care about [multicollinearity](https://www.analyticsvidhya.com/blog/2020/03/what-is-multicollinearity/). I try to used VIF (Variable Inflation Factors) to analyze multicollinearity."},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, We must declare which one is dependent variable, and which one is independet variables\ny = scaled_data['log_price']\nx = scaled_data.drop('log_price', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import library for VIF\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calc_vif(X):\n\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n    return(vif)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calc_vif(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n* VIF starts at 1 and has no upper limit\n* VIF = 1, no correlation between the independent variable and the other variables\n* VIF exceeding 5 or 10 indicates high multicollinearity between this independent variable and the others\n\nSource: https://www.analyticsvidhya.com/blog/2020/03/what-is-multicollinearity/"},{"metadata":{},"cell_type":"markdown","source":" Based on Our VIF result, We should do something with sqft_living and sqft_above because Their VIF is more than 5.\n \n And also, We should do something with view and condition variable because Their VIF is in infinite number which is more than 5"},{"metadata":{},"cell_type":"markdown","source":"I think We should drop sqft_above and keep sqft_living variable because It has meaningful data than sqft_above\n\nand We should drop one of view variable and condition variable  because [this reason](https://www.quora.com/How-and-why-having-the-same-number-of-dummy-variables-as-categories-is-problematic-in-linear-regression-Dummy-variable-trap-Im-looking-for-a-purely-mathematical-not-intuitive-explanation-Also-please-avoid-using-the/answer/Iliya-Valchanov?share=9494e990&srid=uX7Kg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_regression = scaled_data.drop(['sqft_above', 'view_0', 'condition_5'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Big Checkpoint\npre_regression.to_csv('pre_regression.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, We must declare which one is dependent variable, and which one is independet variables\ny = pre_regression['log_price']\nx1 = pre_regression.drop('log_price', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = sm.add_constant(x1)\nresults = sm.OLS(y,x).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Summary\n\n* R-Squared         :     0.766  -> Really good model\n* Prob (F-statistic): \t0.00   -> Which mean Our Regression is significantly can predict the houses price\n* and each variable has p value less than 0.05, which mean variablest that We used in Our model is important for Our Model"},{"metadata":{},"cell_type":"markdown","source":"Yeaah We did it guys :)\n\nBut I want to try to build a model using sklearn and Tensorflow."},{"metadata":{},"cell_type":"markdown","source":"# Bonus"},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning with Sklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the module\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define input and target\nindepedent_vr = pre_regression['log_price']\ndependent_vr = pre_regression.drop('log_price', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the data into 80% train and 20% test, as a default It will shuffle the data before the split \nx_train, x_test, y_train, y_test = train_test_split(dependent_vr, indepedent_vr, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# regression\nfrom sklearn import linear_model\nreg = linear_model.LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the model based on training data\nreg.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test the accuracy with test data\nreg.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy of Our model is 77%, is like what We did before with statsmodels module"},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning with Tensorflow"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We must shuffle the data first, because tensorflow doesn't have shuffle function\ndata_tf = pre_regression.sample(frac=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split the data into 80% train, 10% validation, and 10 %test data\nrow_count = data_tf.shape[0]\n\ntrain_data_count = int(0.8*row_count)\nvalidation_data_count = int(0.1*row_count)\ntest_data_count = row_count - train_data_count - validation_data_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#divide the data into targets and inputs\ntargets = data_tf['log_price']\ninputs = data_tf.drop('log_price', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_inputs = inputs[:train_data_count]\ntrain_targets = targets[:train_data_count]\n\nvalidation_inputs = inputs[train_data_count:train_data_count+validation_data_count]\nvalidation_targets = targets[train_data_count:train_data_count+validation_data_count]\n\ntest_inputs = inputs[train_data_count+validation_data_count:]\ntest_targets = targets[train_data_count+validation_data_count:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's make Our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import our module\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The number of inputs\ninputs_size = len(inputs.columns)\n\n# The number of targets\ntargets_size = 1\n\n# The number of hidden layers\nhidden_layer_size = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n    tf.keras.layers.Dense(targets_size)\n    ])\n\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae','mse'])\n\nbatch_size = 100\nmax_epochs = 100\n\n# to prevent overfitting\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n\n# start the engine\nmodel.fit(train_inputs, \n          train_targets, \n          batch_size = batch_size, \n          epochs = max_epochs, \n          callbacks = [early_stopping],\n          validation_data = (validation_inputs, validation_targets), \n          verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate Our model with testing data\nloss, mae, mse= model.evaluate(test_inputs, test_targets, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Testing set Mean Abs Error: \" + str(mae))\nprint(\"Testing set Mean Sq Error: \" + str(mse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's mean that Our model can predict the price very well, Because It has small error"},{"metadata":{},"cell_type":"markdown","source":"We did it guys, **Thank You** so much for Your time. Maybe next time, We should try to another journey. Hahaha\n\n:)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}