{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\n### Problem context\n*A manager at the bank is disturbed with more and more customers leaving their credit card services. They would really appreciate if one could predict for them who is gonna get churned so they can proactively go to the customer to provide them better services and turn customers' decisions in the opposite direction.*\n\n### Goal\nThis notebook aims to try to predict customers who are likely to get churned through Machine Learning algorithms such as Random Forest, AdaBoost and Gradient Boosting.\n\n### Dataset\nThis dataset consists of 10,000 customers mentioning their age, salary, marital_status, credit card limit, credit card category, etc. There are nearly 18 features. It is an unbalanced dataset where only abouy 16% of customers who have churned.\n\n\n\n## Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.ticker as mtick\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport plotly.express as ex\nimport plotly.figure_factory as ff\nimport plotly.offline as offline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom imblearn.over_sampling import SMOTE \nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Loading\n\nWe load the dataset and have a first look at it."},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_data = pd.read_csv(\"../input/credit-card-customers/BankChurners.csv\")\n\n# Deleting columns that we do not need (according to \"detail\" of the data in kaggle.com)\nbank_data = bank_data.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2', 'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\"CLIENTNUM\"], axis = 1)\n\nbank_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_data.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no null values, which is good!"},{"metadata":{},"cell_type":"markdown","source":"## Converting Categorical Features\n\nNext step we convert categorical features to numerical; I decided to use .map() instead of pd.get_dummies().\nAlso, I decided to group some features as equals, for example, in Marital_Status was either is Married (1) or not (0). Same with Education_Level.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a copy of the original data to use the original one later\nnum_data = bank_data.copy()\n\nnum_data['Attrition_Flag'] = num_data['Attrition_Flag'].map({'Existing Customer':0, 'Attrited Customer':1})\nnum_data['Income_Category'] = num_data['Income_Category'].map({'Unknown':0, 'Less than $40K':0, '$40K - $60K':1, '$60K - $80K':2, '$80K - $120K':3, '$120K +':4})\nnum_data['Marital_Status'] = num_data['Marital_Status'].map({'Divorced':0, 'Unknown':0, 'Single':0, 'Married':1})\nnum_data['Education_Level'] = num_data['Education_Level'].map({'Unknown':0, 'Uneducated':0, 'High School':1, 'College':2,'Graduate':3,'Post-Graduate':4,'Doctorate':5})\nnum_data['Card_Category'] = num_data['Card_Category'].map({'Blue':0, 'Silver':1, 'Gold':2, 'Platinum':3})\n\n#Just playing with different ways to do the same thing\nnum_data = pd.get_dummies(num_data) #Gender\nnum_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation Heatmap\n\nGetting the correlation of target variable with all the other features."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nnum_data.corr()[\"Attrition_Flag\"].sort_values(ascending=False).plot(kind=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ploting Attrition_Flag\n\nWe ca clearly see in this plot the difference between Current Customers and Attritied Customers."},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.catplot(y=\"Attrition_Flag\", kind=\"count\", data=bank_data, height=2.6, aspect=2.5, orient='h')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Attrited Customers vs Existing Customers\n\n### By Marital Status"},{"metadata":{"trusted":true},"cell_type":"code","source":"def barplot_percentages(feature, orient='v', axis_name=\"Percentage of customers\"):\n    ratios = pd.DataFrame()\n    g = bank_data.groupby(feature)[\"Attrition_Flag\"].value_counts().to_frame()\n    g = g.rename({\"Attrition_Flag\": axis_name}, axis=1).reset_index()\n    g[axis_name] = g[axis_name]/len(bank_data)\n    if orient == 'v':\n        ax = sns.barplot(x=feature, y= axis_name, hue='Attrition_Flag', data=g, orient=orient)\n        ax.set_yticklabels(['{:,.0%}'.format(y) for y in ax.get_yticks()])\n    else:\n        ax = sns.barplot(x= axis_name, y=feature, hue='Attrition_Flag', data=g, orient=orient)\n        ax.set_xticklabels(['{:,.0%}'.format(x) for x in ax.get_xticks()])\n    ax.plot()\nbarplot_percentages(\"Marital_Status\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By Dependant Count"},{"metadata":{"trusted":true},"cell_type":"code","source":"def barplot_percentages(feature, orient='v', axis_name=\"Percentage of customers\"):\n    ratios = pd.DataFrame()\n    g = bank_data.groupby(feature)[\"Attrition_Flag\"].value_counts().to_frame()\n    g = g.rename({\"Attrition_Flag\": axis_name}, axis=1).reset_index()\n    g[axis_name] = g[axis_name]/len(bank_data)\n    if orient == 'v':\n        ax = sns.barplot(x=feature, y= axis_name, hue='Attrition_Flag', data=g, orient=orient)\n        ax.set_yticklabels(['{:,.0%}'.format(y) for y in ax.get_yticks()])\n    else:\n        ax = sns.barplot(x= axis_name, y=feature, hue='Attrition_Flag', data=g, orient=orient)\n        ax.set_xticklabels(['{:,.0%}'.format(x) for x in ax.get_xticks()])\n    ax.plot()\nbarplot_percentages(\"Dependent_count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By Education Level"},{"metadata":{"trusted":true},"cell_type":"code","source":"def barplot_percentages(feature, orient='v', axis_name=\"Percentage of customers\"):\n    ratios = pd.DataFrame()\n    g = bank_data.groupby(feature)[\"Attrition_Flag\"].value_counts().to_frame()\n    g = g.rename({\"Attrition_Flag\": axis_name}, axis=1).reset_index()\n    g[axis_name] = g[axis_name]/len(bank_data)\n    if orient == 'v':\n        ax = sns.barplot(x=feature, y= axis_name, hue='Attrition_Flag', data=g, orient=orient)\n        ax.set_yticklabels(['{:,.0%}'.format(y) for y in ax.get_yticks()])\n    else:\n        ax = sns.barplot(x= axis_name, y=feature, hue='Attrition_Flag', data=g, orient=orient)\n        ax.set_xticklabels(['{:,.0%}'.format(x) for x in ax.get_xticks()])\n    ax.plot()\nbarplot_percentages(\"Education_Level\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation Heatmap "},{"metadata":{"trusted":true},"cell_type":"code","source":"corrs = num_data.corr()\nfigure = ff.create_annotated_heatmap(\n    z=corrs.values,\n    x=list(corrs.columns),\n    y=list(corrs.index),\n    annotation_text=corrs.round(2).values,\n    showscale=True)\noffline.iplot(figure,filename='corrheatmap')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features Importances\n\nNext, I will plot the importance of every feature.\n\nFor this, I will first remove redundant columns; which means remove columns with 2 unique values.\nThen will set Attrition_Flag as my Target Variable (y), to after that use Ranfom Forest to find this features importances."},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Drop redundant columns and set target variable\n# drop = ['Attrition_Flag', 'Gender_F']\n# x, y = num_data.drop(drop,axis=1), num_data['Attrition_Flag']\n\n# # Fit RandomForest Classifier\n# clf = RandomForestClassifier(n_estimators=350, criterion='entropy', max_features='auto', random_state=1)\n# clf = clf.fit(x, y)\n\n# # Plot features importances\n# imp = pd.Series(data=clf.feature_importances_, index=x.columns).sort_values(ascending=False)\n# plt.figure(figsize=(10,12))\n# plt.title(\"Feature importance\")\n# ax = sns.barplot(y=imp.index, x=imp.values, palette=\"Blues_d\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing\n\n### Dividing Dataset\nWe divide into label and feature sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = num_data.drop(['Attrition_Flag'], axis = 1) # Features\nY = num_data['Attrition_Flag'] # Labels\nprint(type(X))\nprint(type(Y))\nprint(X.shape)\nprint(Y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalizing\nNext, we normalize numerical so that each feature has mean 0 and variance 1 using Standar Scaler."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_scaler = StandardScaler()\nX_scaled = feature_scaler.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dividing Dataset \nNow we divide into training and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split( X_scaled, Y, test_size = 0.3, random_state = 100)\n\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Implementing Oversampling \nTo balance the dataset we use SMOTE which stands for Synthetic Minority Oversampling Technique."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of observations in each class before oversampling (training data): \\n\", pd.Series(Y_train).value_counts())\n\nsmote = SMOTE(random_state = 101)\nX_train,Y_train = smote.fit_sample(X_train,Y_train)\n\nprint(\"Number of observations in each class after oversampling (training data): \\n\", pd.Series(Y_train).value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier\n\n### Tuning Random Forest\n\nNow, we will find the best parameters to run Random Forest and implementing cross-validation using Grid Search.\n\nIn the GridSearch parameters we use \"recall\" as in this case I will be trying to reduce False Negatives.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(criterion='entropy', max_features='auto', random_state=1)\ngrid_param = {'n_estimators': [200, 250, 300, 350, 400, 450]}\n\ngd_sr = GridSearchCV(estimator=rfc, param_grid=grid_param, scoring='recall', cv=5)\n\ngd_sr.fit(X_train, Y_train)\n\nbest_parameters = gd_sr.best_params_\nprint(best_parameters)\n\nbest_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator\nprint(best_result)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building Random Forest\n\nWe build now our Random Forest using the tuned parameter (350 number of trees in the forest).\nAlso, we plot a Confusion Matrix to appreciate better the results.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=350, criterion='entropy', max_features='auto', random_state=1)\nrfc.fit(X_train,Y_train)\n\nY_pred = rfc.predict(X_test)\n\nconf_mat = metrics.confusion_matrix(Y_test, Y_pred)\nplt.figure(figsize=(8,6))\nsns.heatmap(conf_mat,annot=True)\nplt.title(\"Confusion_matrix\")\nplt.xlabel(\"Predicted Class\")\nplt.ylabel(\"Actual class\")\nplt.show()\nprint('Confusion matrix: \\n', conf_mat)\nprint('TP: ', conf_mat[1,1])\nprint('TN: ', conf_mat[0,0])\nprint('FP: ', conf_mat[0,1])\nprint('FN: ', conf_mat[1,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(Y_test,rfc.predict(X_test)))\nprint('Accuracy_Score:',accuracy_score(Y_test,Y_pred)*100,'%')\nprint('Recall:',metrics.recall_score(Y_test,Y_pred)*100,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finding Feature Importances\n\nNow we look for the features with higher importance, to run a new Random Forest using only some of the most important ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"featimp = pd.Series(rfc.feature_importances_, index=list(X)).sort_values(ascending=False)\nprint(featimp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Selecting features with higher significance\n\nNow we will create a new subset (X1), using only some features, and run a new Random Forest to compare them."},{"metadata":{"trusted":true},"cell_type":"code","source":"X1 = num_data[['Total_Trans_Ct', 'Total_Trans_Amt', 'Total_Revolving_Bal', 'Total_Ct_Chng_Q4_Q1', 'Total_Relationship_Count',\n                 'Months_Inactive_12_mon','Total_Amt_Chng_Q4_Q1']]\n\nfeature_scaler = StandardScaler()\nX1_scaled = feature_scaler.fit_transform(X1)\n\nX1_train, X1_test, Y1_train, Y1_test = train_test_split( X1_scaled, Y, test_size = 0.3, random_state = 100)\n\nsmote = SMOTE(random_state = 101)\nX1_train,Y1_train = smote.fit_sample(X1_train,Y1_train)\n\nrfc = RandomForestClassifier(n_estimators=350, criterion='entropy', max_features='auto', random_state=1)\nrfc.fit(X1_train,Y1_train)\n\nY_pred = rfc.predict(X1_test)\n\nconf_mat = metrics.confusion_matrix(Y1_test, Y_pred)\nplt.figure(figsize=(8,6))\nsns.heatmap(conf_mat,annot=True)\nplt.title(\"Confusion_matrix\")\nplt.xlabel(\"Predicted Class\")\nplt.ylabel(\"Actual class\")\nplt.show()\nprint('Confusion matrix: \\n', conf_mat)\nprint('TP: ', conf_mat[1,1])\nprint('TN: ', conf_mat[0,0])\nprint('FP: ', conf_mat[0,1])\nprint('FN: ', conf_mat[1,0])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(Y1_test,Y_pred))\nprint('Accuracy_Score:',accuracy_score(Y1_test,Y_pred)*100,'%')\nprint('Recall:',metrics.recall_score(Y1_test,Y_pred)*100,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# AdaBoost Classifier\n\n\n### Tuning AdaBoost\n\nSame as with Random Forest, we will try to find the best parameters to run now AdaBoost, implementing cross-validation using Grid Search."},{"metadata":{"trusted":true},"cell_type":"code","source":"abc = AdaBoostClassifier(random_state=1)\ngrid_param = {'n_estimators': [5,10,20,30,40,50]}\n\ngd_sr = GridSearchCV(estimator=abc, param_grid=grid_param, scoring='recall', cv=5)\n\ngd_sr.fit(X_train, Y_train)\n\nbest_parameters = gd_sr.best_params_\nprint(best_parameters)\n\nbest_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator\nprint(best_result)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building AdaBoost\n\nAfter running GridSearch we found that the ideal maximum number of estimators at which boosting is terminatedo is 50 so we now build our AdaBoost Classifier using the this parameter and then we plot a Confusion Matrix to appreciate better the results.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"abc = AdaBoostClassifier(n_estimators=50, random_state=1)\nabc.fit(X_train,Y_train)\n\nY_pred = abc.predict(X_test)\n\nconf_mat = metrics.confusion_matrix(Y_test, Y_pred)\nplt.figure(figsize=(8,6))\nsns.heatmap(conf_mat,annot=True)\nplt.title(\"Confusion_matrix\")\nplt.xlabel(\"Predicted Class\")\nplt.ylabel(\"Actual class\")\nplt.show()\nprint('Confusion matrix: \\n', conf_mat)\nprint('TP: ', conf_mat[1,1])\nprint('TN: ', conf_mat[0,0])\nprint('FP: ', conf_mat[0,1])\nprint('FN: ', conf_mat[1,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Classification report: \\n', metrics.classification_report(Y_test, Y_pred))\nprint('Accuracy_Score:',accuracy_score(Y_test, Y_pred)*100,'%')\nprint('Recall:',metrics.recall_score(Y_test, Y_pred)*100,'%')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importances"},{"metadata":{"trusted":true},"cell_type":"code","source":"featimp = pd.Series(abc.feature_importances_, index=list(X)).sort_values(ascending=False)\nprint(featimp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient Boosting Classifier\n\n### Tuning Gradient Boosting\n\nLast, we do the same thing to find the best parameters to run now Gradient Boosting, implementing cross-validation using Grid Search."},{"metadata":{"trusted":true},"cell_type":"code","source":"gbc = GradientBoostingClassifier(random_state=1)\ngrid_param = {'n_estimators': [10,20,30,40,50], 'max_depth': [5,6,7,8,9,10,11,12], 'max_leaf_nodes': [8,12,16,20,24,28,32]}\n\ngd_sr = GridSearchCV(estimator=gbc, param_grid=grid_param, scoring='recall', cv=5)\n\ngd_sr.fit(X_train, Y_train)\n\nbest_parameters = gd_sr.best_params_\nprint(best_parameters)\n\nbest_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator\nprint(best_result)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building Gradient Boosting\n\nNow we run Gradient Boosting Classifier using the tuned parameters which are n_estimators=40, max_depth=9 and max_leaf_nodes=32 and we plot a Confusion Matrix.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building Gradient Boost using the tuned parameter\ngbc = GradientBoostingClassifier(n_estimators=40, max_depth=9, max_leaf_nodes=32, random_state=1)\ngbc.fit(X_train,Y_train)\n\nY_pred = gbc.predict(X_test)\n# print('Classification report: \\n', metrics.classification_report(Y_test, Y_pred))\n\nconf_mat = metrics.confusion_matrix(Y_test, Y_pred)\nplt.figure(figsize=(8,6))\nsns.heatmap(conf_mat,annot=True)\nplt.title(\"Confusion_matrix\")\nplt.xlabel(\"Predicted Class\")\nplt.ylabel(\"Actual class\")\nplt.show()\nprint('Confusion matrix: \\n', conf_mat)\nprint('TP: ', conf_mat[1,1])\nprint('TN: ', conf_mat[0,0])\nprint('FP: ', conf_mat[0,1])\nprint('FN: ', conf_mat[1,0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Classification report: \\n', metrics.classification_report(Y_test, Y_pred))\nprint('Accuracy_Score:',accuracy_score(Y_test, Y_pred)*100,'%')\nprint('Recall:',metrics.recall_score(Y_test, Y_pred)*100,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importances"},{"metadata":{"trusted":true},"cell_type":"code","source":"featimp = pd.Series(gbc.feature_importances_, index=list(X)).sort_values(ascending=False)\nprint(featimp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nI decided to work keeping the classifier with the highest possible recall score, this in order to get a minimum number of False Negatives, the reason of this is because those False Negatives are basically customers that our model is predicting that are still \"Existing Customers\" but the reality is that they are \"Attrited Customers\". \nIn other words, this are customers that we think they still are with us, but the actually have already churned. In this kind of situations it is better to try to reduce this Falses Predictions, because the other ones, the False Positives, are customers we are not expecting to be with us and however they are, which means we have more customers than expected.\n__________\n\nWe could see that AdaBoost Classifier was the one which has performed better; even though the accuracy score was a little lower than the other models it got a recall score of 90.82 (higher than all the other models) which helps in having only 48 False Negatives.\n\n"},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}