{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <font size=\"6\"><div style=\"border-radius:5px;color:white;background-color:#FA8072;\">Heart Disease Prediction‚ù§ </div> </font>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://www.wthub.org/wp-content/uploads/2019/11/heart_glow.gif\" height=\"500\" width=\"500\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:#E74C3C\"> Summary </span>\n###                     This Kernel presents the story of Predicting Heart Diseases in a super simplified yet interactive manner to spellbound the reader! The motive to the writter is to give a smooth ride of the machine learning pipeline with the best learning experience ever. So tighten up your seat belts and get grooving!!\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## <span style=\"color:#E74C3C\"> ML Pipeline Contents </span>\n > * [Data Exploration](#1) \n > * [Data Description](#2)\n > * [EDA](#3)\n > * [Feature Engineering](#4) \n > * [Data Processing](#5)\n > * [Methodology](#6)\n   > > * NAive Bayes\n   > > * KNN\n   > > * Decision Tree\n   > > * Random Forest\n > * [Metricies Evaluation ](#7)\n    >> * Accuracy \n    >> * Confusion Matrix\n    > >* ROC Score\n    >> * Recall \n    >> * Precision\n    >> * F1 score\n > * [Plot of all Metrices](#9)\n > * [References](#8)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"1\"><font size=\"6\"><div style=\"border-radius:5px;color:white;background-color:#FA8072;\"> üìú 1. Data Exploration </div> </font></a>\nThis is the first part of the kernel wherein we have explored the dataset given and made it avaliable for use.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data = pd.read_csv('../input/heart-disease-uci/heart.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"data.describe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since all the data values were present as described in the dataset description I didn't applied any pre processing steps for the dataset niether checked for NAN values.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"2\"><font size=\"6\"><div style=\"border-radius:5px;color:white;background-color:#FA8072;\"> üìå2. Data Description : </div></font></a>\n2. sex : 1 - male , 0- female\n3. cp : chest pain type --> 1,2,3,0 (essentially of 4 types)\n4. trestbps:  resting blood pressure ---> integers\n5. chol : serum cholestoral --> int\n6. fbs : fasting blood sugar \n7. restecg : resting electrocardiographic result\n8. thalach : maximum heart rate achieved\n9. exang : exercise induced angina 1-Y ,0 - N\n10. Oldpeak  : ST depression induced by exercise relative to rest\n11. slope - the slope of the peak exercise ST segment\n12. ca - number of major vessels (0-3) colored by flourosopy\n13. thal - 3 = normal; 6 = fixed defect; 7 = reversable defect\n14. target : Actual values ie. have the disease(1) or not(0)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# target is the output and the final conclusion if a person has heart disease or not \ndata.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-------------------------------------\n------------------------------\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"3\"><font size=\"6\"><div style=\"border-radius:5px;color:white;background-color:#FA8072;\"> üèãÔ∏è‚Äç‚ôÄÔ∏èPower pact EDA in seconds</div></font></a>\n# <span style=\"color:#E74C3C\"> üêºPandas Profiling </span>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install pandas-profiling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"from pandas_profiling import ProfileReport\nprof = ProfileReport(data)\nprof.to_file(output_file='output.html')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prof","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:#E74C3C\"> üçß Sweetviz </spam>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"!pip install sweetviz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"import sweetviz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"report = sweetviz.analyze(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#display the report\nreport.show_html('Heart_EDA.html')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"4\"><font size=\"6\"><div style=\"border-radius:5px;color:white;background-color:#FA8072;\"> 2. Feature Engineeringüòé</div></font></a>\n### this is the second part of the kernel wherein relationship between each column of the dataset and 2 generalized attributes , in our case sex‚úÖ and age‚úÖ , is made to derive insights form the figures and find which feature is relatively more important for the model and will have a greater impact on the model accuracy and provide relevent predictions .","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:#E74C3C\"> 2.1 chest pain types : ‚úÖ </span>\n### It is essentially of 4 types and coded in the dataset as 0,1,2,3 .\n### The firat graph prescribes the relation between the 4 types of pains and most number of males and females suffer from type 0 chest pain while the 3rd kind of chest pain is least common. We can easily infer from the chart that type 0 and type 2 chest pains can be important in determining a heart disease.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ax = sns.countplot(x=\"cp\",hue=\"sex\", data=data)\nplt.title('Heart Disease count According To Chest Pain Type')\nplt.xlabel('Chest Pain Type')\nplt.ylabel('Count ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The below plot clearly prooves that people of the age 55-59 are most likely to have chest pains of types 0 and 2.\nPeople of the age 29 - 34 years are less likely to have chest pains. We can infer age and sex to be important factors along with chest pain types","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:#E74C3C\"> 2.2 Resting heart Beat‚ùå\n resting heart rate, the target is between 60 and 100 beats per minute (BPM)\n The heatmap density reveals that most cases ","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig_dims = (6,6)\nfig, ax = plt.subplots(figsize=fig_dims)\nax = sns.barplot(x=\"target\",y=\"trestbps\", hue=\"sex\", data=data)\nplt.title('Heart Disease Frequency According To resting blood pressure  ')\nplt.xlabel('Target : Disease or not ')\nplt.ylabel('trst beats per second ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" resting blood pressure is not an important parameter rather misleading","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.catplot(x=\"target\", y=\"trestbps\", data=data, kind=\"box\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The resting heart beat plot is also misleading and alost the same for the person who has a heart disease or not ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:#E74C3C\">2.3 Cholestrol‚ùå\n## Choslestrol levels is alomst same for both plots hence non relevent\"\n","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig_dims = (6,6)\nfig, ax = plt.subplots(figsize=fig_dims)\nfig = sns.violinplot(x=data['target'], y=data['chol'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:#E74C3C\"> 2.4 Fasting Blood Sugar\nA fasting blood sugar level less than 100 mg/dL (5.6 mmol/L) is normal. A fasting blood sugar level from 100 to 125 mg/dL (5.6 to 6.9 mmol/L) is considered prediabetes. If it's 126 mg/dL (7 mmol/L) or higher on two separate tests, you have diabetes.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"data.fbs.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.countplot(data=data, x=\"fbs\", hue=\"target\")\nplt.title('Heart Disease Frequency According To FBS')\nplt.xlabel('FBS - (Fasting Blood Sugar > 120 mg/dl) (1 = true; 0 = false)')\nplt.xticks(rotation = 0)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.ylabel('Frequency of Disease or Not')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we are not provided with the exact values of fbs the people have a detailed exploration can't be made but having fasting bs is a prooven fact of heart disease and an important parameter to judge the model it will be considered.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:#E74C3C\"> 2.5 restecg : resting electrocardiographic result‚ùå","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sequential_colors = sns.color_palette(\"PuRd\", 2)\nsns.palplot(sequential_colors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.set_palette(sequential_colors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.countplot( x=data['restecg'], hue=data['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not much valuable inferences can be made over categorical values ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:#E74C3C\"> 2.6 thalach : maximum heart rate achieved ‚úÖ\nYou can calculate your maximum heart rate by subtracting your age from 220. For example, if you're 45 years old, subtract 45 from 220 to get a maximum heart rate of 175. This is the average maximum number of times your heart should beat per minute during exercise.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"220-80\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.boxplot(y=data['thalach'], x=data['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows people having heart disease usually tend to have a high max heart rate","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import plotly.express as px","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = px.scatter(data, x=\"thalach\", y=\"age\", color=\"target\")\nfig.add_shape(\n        # Line Vertical\n        dict(\n            type=\"line\",\n            x0=140,\n            y0=0,\n            x1=140,\n            y1=80,\n            line=dict(\n                color=\"RoyalBlue\",\n                width=3\n            )\n))\nfig.add_shape(\n        # Line Vertical\n        dict(\n            type=\"line\",\n            x0=190,\n            y0=0,\n            x1=190,\n            y1=80,\n            line=dict(\n                color=\"RoyalBlue\",\n                width=3\n            )\n))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"most heart patient lies in the active thalach range.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:#E74C3C\"> 2.7 exang : exercise induced angina  ‚ùå","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"sns.set_palette(\"Paired\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.violinplot(x=data[\"exang\"], y=data[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although exercise induced Angima could be a valuable attribute but relative to the dataset people having exang do not face heart diesases and ppl not having exang go through heart diseases, so we drop this feature.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:#E74C3C\">2.8 Oldpeak : ST depression induced by exercise relative to rest‚ùå\nSTD is an important factor in determining a heart disease and a ","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"sequential_colors = sns.color_palette(\"summer\", 2)\nsns.palplot(sequential_colors)\nsns.set_palette(sequential_colors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.barplot(x=data[\"target\"], y=data[\"oldpeak\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"--------------------------------------------\n-----------------------------------------","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"5\"><font size=\"6\"><div style=\"border-radius:5px;color:white;background-color:#FA8072;\">‚èÆ‚è≠ Data Processing</div></font></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"So now that we have filtered out 4 features that will be most important for our model for this particular dataset we will manipulate them in order to add them to our ml model. theses features are:\n1. Age              -------------------------> Discrete Variable\n2. Sex              -------------------------> Nominal/Categorical Variable\n3. Chest Pain Type  -------------------------> Ordinal Variable\n4. thalach          -------------------------> \nNow since age and sex attributes do not have \nCategorical variables :\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"a = pd.get_dummies(data['cp'], prefix = \"cp\")\nb = pd.get_dummies(data['thal'], prefix = \"thal\")\nc = pd.get_dummies(data['slope'], prefix = \"slope\")\nd = pd.get_dummies(data['sex'], prefix = \"sex\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"updated_clms = [data, a,b,c,d]\ndata = pd.concat(updated_clms, axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(columns = ['cp','thal', 'slope', 'sex'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span style=\"color:#E74C3C\"> üìå Scalling / Normalization </span>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nstandardScaler = StandardScaler()\ncolumns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\ndata[columns_to_scale] = standardScaler.fit_transform(data[columns_to_scale])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:#E74C3C\"> üïó Approach </span>\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### <span style=\"color:\">1. Selecting The \"Prediction\" </span>\nWe are very sure of what we want to predict the target values hence:\n","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"y = data.target\ny","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Selecting the \"Features\"\nSelecting the x attributes can be very daunting in case of multiple number of features in the dataset as a Data Scientist needs to be aware of what each column wants to convey and how much inpact will it have on the prediction and model accuracy, so that we can drop the less important features and simplify our problem.\nSo there are 2 ways to it :\n\n1. Include all Features\n2. Include only most impactful Features\n\nFor the simplicity of the project we would like to include all the features while the difference in incliuding important ones will be assigning a X var for them and repeting the same steps.\n","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Just for demonstration purposes we will be implementing with all features\nX_important_features = data[['cp_1', 'cp_2','cp_3','thal_0','thal_1','thal_2','thal_3','slope_0','slope_1','slope_2']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# \nX = data.drop(['target'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Splitting the data into train test\n For comparing and testing the viability of our model we need to split the heart.csv file into test and train data so that we can have a clear picture of how well our model is peforming and the overfitting stats.\n \n ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#transpose matrices\nX_train = X_train.T\ny_train = y_train.T\nX_test = X_test.T\ny_test = y_test.T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"6\"><font size=\"5\"><div style=\"border-radius:10px;color:white;background-color:#FA8072;padding: 10px;\">üß† Building the Model</div></font></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Our problem is a type of Binnary Classification and we need to figure out if the person is suffering from a heart disease or not","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" <font size=\"4\"><div style=\"border-radius:10px;color:white;background-color:#FA8072;padding: 10px;\"> 1. Naive Bayes</div></font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train.T, y_train.T)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### <div class=\"alert alert-block alert-info\">üìå Accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies = {}\nacc = nb.score(X_test.T,y_test.T)*100\naccuracies['Naive Bayes'] = acc\nprint(\"Accuracy of Naive Bayes: {:.2f}%\".format(acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"y_pred=nb.predict(X_test.T)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"precisions={}\nrecalls={}\nf1_scores={}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### <div class=\"alert alert-block alert-info\">üìå Confusion Matrix ","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"cf_matrix = confusion_matrix(y_test.T, y_pred)\nprint(cf_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n            fmt='.2%', cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <div class=\"alert alert-block alert-info\"> üìå ROC scores","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import scikitplot as skplt\nimport matplotlib.pyplot as plt\n\ny_pred_proba = nb.predict_proba(X_test.T)\ny_true = y_test.T\nskplt.metrics.plot_roc_curve(y_true, y_pred_proba)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#recall\nfrom sklearn.metrics import recall_score\nrecall = recall_score(y_test.T, y_pred)\nprint('Recall: %.3f' % recall)\n\nrecalls['Naive Bayes'] = recall","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# precision\nfrom sklearn.metrics import precision_score\nprecision = precision_score(y_test.T, y_pred)\nprint('Precision: %.3f' % precision)\n\nprecisions['Naive Bayes'] = precision","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fi score\nf1 = 2*((precision*recall)/(precision+recall))\nf1_scores['Naive Bayes'] = f1\nprint(f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nfilename = 'naive_bayes.h5'\npickle.dump(nb, open(filename, 'wb'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <font size=\"6\"><div style=\"border-radius:5px;color:white;background-color:#FA8072;\">  2. KNN</div></font>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 2)  # n_neighbors means k\nknn.fit(X_train.T, y_train.T)\nprediction = knn.predict(X_test.T)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"{} NN Score: {:.2f}%\".format(2, knn.score(X_test.T, y_test.T)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 18)  # n_neighbors means k\nknn.fit(X_train.T, y_train.T)\nprediction = knn.predict(X_test.T)\nprint(\"{} NN Score: {:.2f}%\".format(2, knn.score(X_test.T, y_test.T)*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <div class=\"alert alert-block alert-info\"> üìå Accuracy </div>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = knn.score(X_test.T,y_test.T)*100\naccuracies['KNN'] = acc\nprint(\"Accuracy of KNN: {:.2f}%\".format(acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"y_pred=knn.predict(X_test.T)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <div class=\"alert alert-block alert-info\"> üìå Confusion Matrix</div>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cf_matrix = confusion_matrix(y_test.T, y_pred)\n\nsns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n            fmt='.2%', cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <div class=\"alert alert-block alert-info\"> üìå ROC Score</div>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import scikitplot as skplt\nimport matplotlib.pyplot as plt\n\ny_pred_proba = knn.predict_proba(X_test.T)\ny_true = y_test.T\nskplt.metrics.plot_roc_curve(y_true, y_pred_proba)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import recall_score\nrecall = recall_score(y_test.T, y_pred)\nprint('Recall: %.3f' % recall)\nrecalls['KNN'] = recall","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# precision\nfrom sklearn.metrics import precision_score\nprecision = precision_score(y_test.T, y_pred)\nprint('Precision: %.3f' % precision)\nprecisions['KNN'] = precision","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fi score\nf1 =2*((precision*recall)/(precision+recall))\nprint(\"F1 Score \",f1)\nf1_scores['KNN'] = f1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nfilename = 'knn_model.sav'\npickle.dump(knn, open(filename, 'wb'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <font size=\"6\"><div style=\"border-radius:5px;color:white;background-color:#FA8072;\"> 3. Decision Tree </div></font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n# Define model. Specify a number for random_state to ensure same results each run\ndt = DecisionTreeClassifier(random_state=1)\n\n# Fit model\ndt.fit(X_train.T, y_train.T)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <div class=\"alert alert-block alert-info\"> üìå Accuracy</div>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = dt.score(X_test.T,y_test.T)*100\naccuracies['Decision Tree'] = acc\nprint(\"Accuracy of Decision Tree: {:.2f}%\".format(acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=dt.predict(X_test.T)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <div class=\"alert alert-block alert-info\"> üìå Confusion Matrix</div>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cf_matrix = confusion_matrix(y_test.T, y_pred)\n\nsns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n            fmt='.2%', cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <div class=\"alert alert-block alert-info\"> üìå ROC Score</div>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import scikitplot as skplt\nimport matplotlib.pyplot as plt\n\ny_pred_proba = dt.predict_proba(X_test.T)\ny_true = y_test.T\nskplt.metrics.plot_roc_curve(y_true, y_pred_proba)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import recall_score\nrecall = recall_score(y_test, y_pred)\nprint('Recall: %.3f' % recall)\nrecalls['Decision Tree'] = recall","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# precision\nfrom sklearn.metrics import precision_score\nprecision = precision_score(y_test, y_pred, labels=[1,2], average='micro')\nprint('Precision: %.3f' % precision)\nprecisions['Decision Tree'] = precision","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fi score\nf1 = 2*((precision*recall)/(precision+recall))\nprint(\"F1 Score \",f1)\nf1_scores['Decision Tree']= f1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nfilename = 'dt_model.sav'\npickle.dump(dt, open(filename, 'wb'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <font size=\"6\"><div style=\"border-radius:5px;color:white;background-color:#FA8072;\"> 4. Random Forest</div></font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=1,random_state=1)\nrf.fit(X_test.T,y_test.T)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <div class=\"alert alert-block alert-info\"> üìå Accuracy</div>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = rf.score(X_test.T,y_test.T)*100\naccuracies['Random Forest'] = acc\nprint(\"Accuracy of Random Forest: {:.2f}%\".format(acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=rf.predict(X_test.T)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <div class=\"alert alert-block alert-info\"> üìå Confusion Matrix</div>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cf_matrix = confusion_matrix(y_test.T, y_pred)\n\nsns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n            fmt='.2%', cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <div class=\"alert alert-block alert-info\"> üìå ROC Score</div>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import scikitplot as skplt\nimport matplotlib.pyplot as plt\n\ny_pred_proba = rf.predict_proba(X_test.T)\ny_true = y_test.T\nskplt.metrics.plot_roc_curve(y_true, y_pred_proba)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import recall_score\nrecall = recall_score(y_test, y_pred)\nprint('Recall: %.3f' % recall)\nrecalls['Random Forest'] = recall","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# precision\nfrom sklearn.metrics import precision_score\nprecision = precision_score(y_test, y_pred, labels=[1,2], average='micro')\nprint('Precision: %.3f' % precision)\nprecisions['Random Forest'] = precision","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#f1 score\nf1 = 2*((precision*recall)/(precision+recall))\nprint(\"F1 Score \",f1)\nf1_scores['Random Forest']= f1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nfilename = 'rf_model.sav'\npickle.dump(rf, open(filename, 'wb'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"--------------------------------------------------------------------\n---------------------------------------------------------------------","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"9\"><font size=\"6\"><div style=\"border-radius:5px;color:white;background-color:#FA8072;\"> Comparing all the models </div></font></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = accuracies.values()\nx = accuracies.keys()\nplt.bar(x,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Recall","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"recalls\nprecisions\nf1_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = recalls.values()\nx = recalls.keys()\nplt.bar(x,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Precision","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = precisions.values()\nx = precisions.keys()\nplt.bar(x,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# F1 Score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = f1_scores.values()\nx = f1_scores.keys()\nplt.bar(x,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"8\"><font size=\"6\"><div style=\"border-radius:5px;color:white;background-color:#FA8072;\"> Referencesüí¨</div></font></a>\nI believe human mind is not self sufficient it needs the community to help it. Here are a few of the community gemsüîπ that made it possible for me! \n\n1. [**Inspiration**](https://www.kaggle.com/thedatabeast) \n2. [Machine Learning Mastery](https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/)\n3. [Insights](https://www.kaggle.com/cdabakoglu/heart-disease-classifications-machine-learning)\n4. [Dataset](https://www.kaggle.com/ronitf/heart-disease-uci\\)\n5. [Format](https://matplotlib.org/2.1.1/api/_as_gen/matplotlib.pyplot.plot.html)\n6. [Style](https://www.kaggle.com/shubhamksingh/create-beautiful-notebooks-formatting-tutorial/comments)\n7. [Design](https://www.kaggle.com/chrisbow/formatting-notebooks-with-markdown-tutorial) \n8. [Documentation](https://www.kaggle.com/getting-started/40799) \n9. [My_Previous_Kernel](https://www.kaggle.com/bhanvimenghani/folium-chai-eda)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"8\"><font size=\"6\"><div style=\"border-radius:5px;color:white;background-color:#FA8072;\"> Recomendation / Future Worksüí¨ </div></font></a>\n    A Quick Google search for the following topics might be of use to the readers \n    1. Research papers (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5863635/#:~:text=In%20this%20study%2C%20an%20effective,cholesterol%2C%20and%20obesity%20for%20prediction.)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src =\"https://i.pinimg.com/originals/6b/d3/e9/6bd3e97cbf5c7d873b75548d6515a6e0.gif\">","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}