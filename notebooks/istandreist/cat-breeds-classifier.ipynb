{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nfrom glob import glob\n#import workspace_utils\n#from workspace_utils import active_session\nimport cv2                \nimport matplotlib.pyplot as plt                        \nfrom tqdm import tqdm\n\nfrom PIL import ImageFile\nfrom PIL import Image\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\n\nimport torch\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport os \nclasses_list = os.listdir('../input/cat-breeds-dataset/images')\nprint (classes_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Creating Train / Val / Test folders (One time use)\nimport os\nimport numpy as np\nimport shutil\nimport random\nroot_dir = '../input/cat-breeds-dataset/images/' # data root path (data origin)\nworking_dir = '../working/cat-breeds-dataset/images/'\nclasses_dir = classes_list #total labels\n\nnum_classes = len(classes_list)\n\nval_ratio = 0.15\ntest_ratio = 0.05\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if (os.path.isdir('../working/') == False):\n    #shutil.rmtree('../working/')\n    os.listdir('../working/')\n    !os.mkdir(working_dir)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if (os.path.isdir('../working/cat-breeds-dataset') == False):\n    for cls in classes_dir:\n        os.makedirs(working_dir +'train/' + cls)\n        os.makedirs(working_dir +'val/' + cls)\n        os.makedirs(working_dir +'test/' + cls)\n\n\n        # Creating partitions of the data after shuffling\n        src = root_dir + cls # Folder to copy images from\n\n        allFileNames = os.listdir(src)\n        #np.random.shuffle(allFileNames)\n        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),\n                                                                  [int(len(allFileNames)* (1 - (val_ratio + test_ratio))), \n                                                                   int(len(allFileNames)* (1 - test_ratio))])\n\n\n        train_FileNames = [src+ '/' + name for name in train_FileNames.tolist()]\n        val_FileNames = [src + '/' +name for name in val_FileNames.tolist()]\n        test_FileNames = [src + '/' +name for name in test_FileNames.tolist()]\n\n\n        # Copy-pasting images\n        print(cls)\n        for name in train_FileNames:\n            shutil.copy(name, working_dir +'train/' + cls)\n\n        for name in val_FileNames:\n            shutil.copy(name, working_dir +'val/' + cls)\n\n        for name in test_FileNames:\n            shutil.copy(name, working_dir +'test/' + cls)\n\n        print(cls, ' complete.')\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torchvision.models as models\nimport torch.nn as nn\nimport torch\n\n## TODO: Specify model architecture \n# Load the pretrained model from pytorch\n\n\nmodel_resnet50 = models.resnet50(pretrained=True)\n\n# Freeze training for all \"features\" layers\n#for param in model_transfer.features.parameters():\nfor param in model_resnet50.parameters():\n    param.requires_grad = False    \n\nmodel_resnet50.fc = nn.Linear(2048, num_classes, bias=True) \n    \nfor param in model_resnet50.fc.parameters():\n    param.requires_grad = True\n\n# print out the model structure\nprint(model_resnet50)\n    \n    \n\n\nmodel_vgg16 = models.vgg16(pretrained=True)\n\nfor param in model_vgg16.parameters():\n    param.requires_grad = False    \n    \nmodel_vgg16.classifier[6] = nn.Linear(model_vgg16.classifier[3].out_features, num_classes)\nmodel_vgg16.classifier[6].requires_grad = True\n\n# print out the model structure\n\nprint(model_vgg16)\n\n\nif torch.cuda.is_available():\n    model_vgg16 = model_vgg16.cuda()\n    model_resnet50 = model_resnet50.cuda()\n    use_cuda = True\nelse:\n    use_cuda = False\nprint(use_cuda)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Specify appropriate transforms, and batch_sizes\n\nstandard_normalization = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                              std=[0.229, 0.224, 0.225])\n\ndata_transform = {'train': transforms.Compose([transforms.RandomResizedCrop(224),\n                                     transforms.RandomHorizontalFlip(),\n                                     transforms.ToTensor(),\n                                     standard_normalization]),\n                   'valid': transforms.Compose([transforms.Resize(256),\n                                     transforms.CenterCrop(224),\n                                     transforms.ToTensor(),\n                                     standard_normalization]),\n                   'test': transforms.Compose([transforms.Resize(size=(224,224)),\n                                     transforms.ToTensor(), \n                                     standard_normalization])\n                  }\n\ntrain_dir = working_dir + 'train'\nvalid_dir = working_dir + 'val'\ntest_dir = working_dir + 'test'\n\ntrain_data = datasets.ImageFolder(train_dir, transform=data_transform['train'])\nvalidation_data = datasets.ImageFolder(valid_dir, transform=data_transform['valid'])\ntest_data = datasets.ImageFolder(test_dir, transform=data_transform['test'])\n\n# define dataloader parameters\nbatch_size = 20\nnum_workers = 0\n\n# prepare data loaders\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n                                           num_workers=num_workers, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n                                          num_workers=num_workers, shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size, \n                                          num_workers=num_workers, shuffle=True)\n\n\nloaders = {\n    'train': train_loader,\n    'valid': valid_loader,\n    'test': test_loader\n}\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n### TODO: select loss function\ncriterion = nn.CrossEntropyLoss()\n\n### TODO: select optimizer\noptimizer = optim.SGD(model_resnet50.parameters(), lr=0.001, momentum=0.5)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## TODO complete epoch and training batch loops\n## These loops should update the classifier-weights of this model\n## And track (and print out) the training loss over time\n\ndef train(n_epochs, loaders, model, use_cuda, optimizer, criterion, save_path):\n    valid_loss_min = np.Inf\n    for epoch in range(1, n_epochs+1):\n      # keep track of training and validation loss\n        train_loss = 0.0\n        valid_loss = 0.0\n       \n\n        ###################\n        # train the model #\n        ###################\n        model.train()\n        #for data, target in loaders_scratch:\n        for batch_idx, (data, target) in enumerate(loaders['train']):\n            # move tensors to GPU if CUDA is available\n            if use_cuda:\n                data, target = data.cuda(), target.cuda()\n            # clear the gradients of all optimized variables\n            optimizer.zero_grad()\n            # forward pass: compute predicted outputs by passing inputs to the model\n           \n            output = model(data)\n           \n            # calculate the batch loss\n            loss = criterion(output, target)\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            optimizer.step()\n            # update training loss\n            #train_loss += loss.item()*data.size(0)\n            ## record the average training loss, using something like\n            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n                \n        ######################    \n        # validate the model #\n        ######################\n        model.eval()\n        #for data, target in valid_loader:\n        for batch_idx, (data, target) in enumerate(loaders['valid']):\n            # move tensors to GPU if CUDA is available\n            if use_cuda:\n                data, target = data.cuda(), target.cuda()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(data)\n            # calculate the batch loss\n            loss = criterion(output, target)\n            # update average validation loss \n            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n           \n\n        # calculate average losses\n        train_loss = train_loss/len(train_loader.dataset)\n        valid_loss = valid_loss/len(valid_loader.dataset)\n\n        # print training/validation statistics \n        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n            epoch, train_loss, valid_loss))\n\n        # save model if validation loss has decreased\n        if valid_loss <= valid_loss_min:\n            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n            valid_loss_min,\n            valid_loss))\n            torch.save(model.state_dict(), save_path)\n            valid_loss_min = valid_loss\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = train(30, loaders, model_resnet50, use_cuda, optimizer, \n                      criterion, '../working/model.pt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(loaders, model, criterion, use_cuda):\n\n    # monitor test loss and accuracy\n    test_loss = 0.\n    correct = 0.\n    total = 0.\n\n    for batch_idx, (data, target) in enumerate(loaders['test']):\n        # move to GPU\n        if use_cuda:\n            data, target = data.cuda(), target.cuda()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the loss\n        loss = criterion(output, target)\n        # update average test loss \n        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n        # convert output probabilities to predicted class\n        pred = output.data.max(1, keepdim=True)[1]\n        # compare predictions to true label\n        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n        total += data.size(0)\n            \n    print('Test Loss: {:.6f}\\n'.format(test_loss))\n\n    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n        100. * correct / total, correct, total))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test(loaders, model, criterion, use_cuda)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}