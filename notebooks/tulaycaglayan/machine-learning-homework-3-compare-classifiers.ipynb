{"cells":[{"metadata":{"_uuid":"1076f16f91d17afeda0b7d126706d6ae12af2e33"},"cell_type":"markdown","source":"<a id=\"0\"> <a/>\n# Compare Classifiers <br>\nIn this program, we will read file \"column-2C-weka.csv\"  . This file has class property and 6 numeric features  . Class property has 2 values named \"Abnormal\" and \"Normal\" . We will fit 3 different classfiers for this file and compare 3 models and find which model is most suitable for the data <br>\n\n1. [Read Data](#1)<br>\n2. [Train Test Split](#2)<br>\n3. [Fit Classifiers](#3)<br>\n    3.1. [K Neighbors Classifier ](#3.1)<br>\n    3.2. [Random Forest Classifier](#3.2)<br>\n    3.3. [Logistic Regression ](#3.3)<br>\n4. [Confusion matrixes ](#4)<br>\n5. [ROC Curve](#5)<br>\n6. [Conclusion](#6)<br>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# import models \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# metrics \nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import roc_curve\n\n# graphs \nimport matplotlib.pyplot as plt \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## <div id=\"1\">1. Read Data <div/>"},{"metadata":{"trusted":true,"_uuid":"9fbf1e5d91971c851ac76f09652e228c40481dbd"},"cell_type":"code","source":"data = pd.read_csv('../input/column_2C_weka.csv')\nprint ( 'distinct class values : ' , data['class'].unique())\n\ndata['class'] = [1 if each == 'Normal' else 0 for each in data['class'] ]\nx = data.loc[:,data.columns != 'class'] # veya data.drop(['class'], axis = 1) \ny = data.loc[:,'class']\n\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8585a10f29eb14ce776c306866ef8ba378b26c9"},"cell_type":"markdown","source":"## <div id=\"2\">2. Train Test Split<div/>"},{"metadata":{"trusted":true,"_uuid":"3cdc0e8fa523eac37cbff878006c2e253f69b2e3"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, random_state = 1)\n\nprint ('x_train shape: {} '.format(x_train.shape))\nprint ('y_train shape: {} '.format(y_train.shape))\nprint ('x_test shape: {} '.format(x_test.shape))\nprint ('y_test shape: {} '.format(y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c5faf0f9c22b9ae7b39cd2255efd73b9792c43b"},"cell_type":"markdown","source":"## <div id=\"3\">3. Fit Classifiers<div/>\n### <div id=\"3.1\">3.1.K Neighbors Classifier<div/>"},{"metadata":{"trusted":true,"_uuid":"92374f5b6102cf4fed0b384590031792b1cc9c7b"},"cell_type":"code","source":"\n# find  best k value \nknn_accuracy_list =[]\nfor k in  range (1,25):\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(x_train, y_train)\n    knn_accuracy_list.append(knn.score(x_test, y_test))\n    \nprint ('best k is :{} , best acccuracy is :{} '.format(  knn_accuracy_list.index (np.max(knn_accuracy_list))+1, np.max(knn_accuracy_list)))\n\n#knn classifier \nknn = KNeighborsClassifier(n_neighbors = knn_accuracy_list.index (np.max(knn_accuracy_list))+1 )\nknn.fit(x_train, y_train)\ny_pred_knn = knn.predict(x_test)\nprint ('KNeighborsClassifier test accuracy ' , knn.score(x_test, y_test) )\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05b95e4aa457a9d044e0b9796f20c2598faec73b"},"cell_type":"markdown","source":"### <div id=\"3.2\">3.2. Random Forest Classifier<div/>"},{"metadata":{"trusted":true,"_uuid":"f80208d46a0b55ba6722ac6b0ea65c6621aed13c"},"cell_type":"code","source":"rfc_accuracy_list =[]\nfor r in  range (1,25):\n    rfc = RandomForestClassifier(random_state = r)\n    rfc.fit(x_train, y_train)\n    rfc_accuracy_list.append(rfc.score(x_test, y_test))\n    \nprint ('best r is :{} , best acccuracy is :{} '.format(  rfc_accuracy_list.index (np.max(rfc_accuracy_list))+1, np.max(rfc_accuracy_list)))\n    \nrfc = RandomForestClassifier(random_state = rfc_accuracy_list.index (np.max(rfc_accuracy_list))+1)\nrfc.fit(x_train,y_train)\ny_pred_rfc = rfc.predict(x_test)\nprint ('RandomForestClassifier test accuracy ' , rfc.score(x_test, y_test) )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abb15378686c28a5e0ed98dde51ed0b919bae8c2"},"cell_type":"markdown","source":"### <div id=\"3.3\">3.3. Logistic Regression<div/>"},{"metadata":{"trusted":true,"_uuid":"34525b75573f6b4d7e7678b80ea3562eb351eb01"},"cell_type":"code","source":"logreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred_logreg = logreg.predict(x_test)\nprint ('LogisticRegression test accuracy ' , logreg.score(x_test, y_test) )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db8d64a6d0b439f2f8347cd5b36917f984279a9a"},"cell_type":"markdown","source":"KNeighborsClassifier test accuracy  **0.8817204301075269**<br>\nRandomForestClassifier test accuracy  **0.8924731182795699**<br>\nLogisticRegression test accuracy  **0.8602150537634409**<br>\n\nAccording to accuracy RandomForestClassifier > KNeighborsClassifier > LogisticRegression"},{"metadata":{"_uuid":"176cafd09f1e86b1574bd0a15875efa5a4d66968"},"cell_type":"markdown","source":"## <div id=\"4\">4. Confusion matrixes<div/>\n\nA confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known<br><br>\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Pred No** &nbsp;&nbsp;&nbsp;&nbsp;**Pred Yes**<br>\n**Actual No**  &nbsp;TN&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FP <br>\n**Actual Yes** &nbsp;FN&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TP<br>\n\nThis is a list of rates that are often computed from a confusion matrix for a binary classifier: <br>\n\n**Accuracy:** Overall, how often is the classifier correct?<br>\n    * (TP+TN)/total\n**Misclassification Rate: **Overall, how often is it wrong?<br>\n    * (FP+FN)/total\n    * 1 - Accuracy\n    * also known as \"Error Rate\"\n**True Positive Rate: **When it's actually yes, how often does it predict yes?<br>\n    * TP/actual yes\n    * also known as \"Sensitivity\" or \"Recall\"\n**False Positive Rate:** When it's actually no, how often does it predict yes?<br>\n    * FP/actual no \n**True Negative Rate: **When it's actually no, how often does it predict no?<br>\n    * TN/actual no\n    * 1 - False Positive Rate\n    * also known as \"Specificity\"\n**Precision: **When it predicts yes, how often is it correct?<br>\n    * TP/predicted yes\n**Prevalence:** How often does the yes condition actually occur in our sample?<br>\n    * Actual yes/total"},{"metadata":{"trusted":true,"_uuid":"5d17ca77a72aeed73f51ef8737057b763cc5cef7"},"cell_type":"code","source":"cm = confusion_matrix(y_test,y_pred_knn)\nprint('KNN Confusion matrix: \\n',cm)\nprint('KNN Classification report: \\n',classification_report(y_test,y_pred_knn))\n\ncm = confusion_matrix(y_test,y_pred_rfc)\nprint('RandomForestClassifier Confusion matrix: \\n',cm)\nprint('RandomForestClassifier Classification report: \\n',classification_report(y_test,y_pred_rfc))\n\ncm = confusion_matrix(y_test,y_pred_logreg)\nprint('LogisticRegression Confusion matrix: \\n',cm)\nprint('LogisticRegression Classification report: \\n',classification_report(y_test,y_pred_logreg))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7ccaa4b54bc10d1713ae3351e89b9052dc7d536"},"cell_type":"markdown","source":"## <div id=\"5\">5.ROC Curve<div/>\n    \nThe ROC curve is created by plotting the **true positive rate (TPR) ** against the ** false positive rate (FPR)** at various threshold settings. The** true-positive rate** is also known as **sensitivity, recall or probability of detection **in machine learning. "},{"metadata":{"trusted":true,"_uuid":"2cfaab6835c65f0d278781d0636fd0446e48481d"},"cell_type":"code","source":"\ny_pred_knn_prob = knn.predict_proba(x_test)[:,1]\ny_pred_rfc_prob = rfc.predict_proba(x_test)[:,1]\ny_pred_lr_prob = logreg.predict_proba(x_test)[:,1]\n\nfpr_knn, tpr_knn, thresholds = roc_curve(y_test, y_pred_knn_prob) \nfpr_rfc, tpr_rfc, thresholds = roc_curve(y_test, y_pred_rfc_prob) \nfpr_lr, tpr_lr, thresholds = roc_curve(y_test, y_pred_lr_prob) \n\nplt.figure (figsize=[13 ,8])\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_knn, tpr_knn, label='KNN')\nplt.plot(fpr_rfc, tpr_rfc, label='Random Forest')\nplt.plot(fpr_lr, tpr_lr, label='Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.title('ROC')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a60c5dd9eff7aca944e32560b6263f258beb2128"},"cell_type":"markdown","source":"## <div id=\"6\">6. Conclusion<div/>\n\nAccording to accurancy and precision , for this data , below are classifiers from best to worst :\n\n1. Random Forest Classifier\n2. K Neighbors Classifier\n3. Logistic Regression"},{"metadata":{"_uuid":"4c62b4e09d93b8ea64fb1032aba56624ba66edcb"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}