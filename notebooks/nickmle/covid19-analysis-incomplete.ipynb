{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n# data collection and data load and extractuib part will take places --#\n# data extraction process is more likely be automatic that people can load the data from --> \n# there are some external process plus some random processes -- that can have several random processe\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\nfrom bs4 import BeautifulSoup\nimport requests\nimport sklearn \nimport datetime\nfrom matplotlib import pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# to affect the configuration of the data all the world have in order to build the better performing models --> \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n# Please don't use API_key, it is free trial version with  500 maximum perday.\n# this should be enough only for this notebook.\n# please get one for you\napi_key = \"0ebe4425fbad41d8aaf25354200504\"\n\n\"\"\"\nbasics statistical analysis\nforecasting of the general trends on cases\nMain Ideas:\n    - Prediction of deaths based on the values of confirmed cases.\n    - Speed of recovery <--> confirmed cases.\n    - association between tempeture and COVID-19 spread\n        - For this, I am open for suggestions --> since the very good historical temperature data provider api is key to accomplish.\n        \nDashboard build:\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\"\"\"\nProject goal to study to about the  association between tempeture and COVID-19 spread\n\n\n\"\"\"\n\n\nconfirmed =  pd.read_csv(\"../input/novel-corona-virus-2019-dataset/time_series_covid_19_confirmed.csv\")\ndeaths =  pd.read_csv(\"../input/novel-corona-virus-2019-dataset/time_series_covid_19_recovered.csv\")\nrecovered =  pd.read_csv(\"../input/novel-corona-virus-2019-dataset/time_series_covid_19_deaths.csv\")\n#there are several goals to achieve in the conservations those are coming to with the actions that is taking shape -->  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confirmed.shape, deaths.shape, recovered.shape)#  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed.head()\nlen(confirmed['Country/Region'].unique())# total of 178 countries have infected with the desease --> that can run in several different runs --> ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef getcoronatable():\n    \n    \n    \"\"\"\n    There is somepanddemica that can used to change several items -- making tmore reports of the people \n    This function will get the data from https://www.worldometers.info/coronavirus/ sites -- and synchronize it in the terms --.\n    \n    managing the  loading cycle of the  table -- and update the analyze for the data --> running service. some works needs to be done .. with thoses\n    \n    this is the particular event point -- that program that is running the returning the dashboard that deals with basic information -->  \n    There are some minimal elements for the cluster centers and belong with some more tasks with several step procedures --> \n    can use the training in and out --> that will have cluster with the shorter distance .. \n    there are syntatic kmeans -- that can \n    \n    \"\"\"\n    response = requests.get(\"https://www.worldometers.info/coronavirus/\")\n    print(response.status_code)\n    namepart = str(datetime.datetime.now())\n    if response.status_code == 200:\n        print(\"Successful load of the website.\")# there is the successful load from the website -- will get printed --> \n    bs_content = BeautifulSoup(response.content, \"html\")\n    \n    data_in_html = bs_content.find_all(\"table\")# this is the data file --# it is possible to do automatic synchronazation every 30 mins to load the data -->\n\n    rows = data_in_html[0].find_all(\"tr\")# all the features those are columns -\n    #bs_content bs contents will be loaded in the many frames  that can run on other platforms --\n    columns = []\n    for feature in rows[0].text.split(\"\\n\"):\n        if feature!= \"\":\n            columns.append(feature)\n    #data = pd.DataFrame(columns = columns)\n    #data.head()\n    print(columns, len(columns))\n    data_rows=  []\n    rows = rows[1:]\n    # there could be some values those are hard to extract valuable data from --> needs have particular runs -- that cannot change the values -- \n    \n    # there are 13 columns passed but --> 11 columns has been created as the data frame -->  \n    \n    # \n    print(\"Can run also can't run the data -- mapping stages\") \n    \n    for row in rows:\n        row = row.text.split(\"\\n\")[1:]\n        row = row[:-1]# there is also nonetype elements that needs to be downloaded to run on the rest of the cases .. that can run --> higher performance algorithm --> \n        date_data = row[-1]\n        \n        row = row[:-2]\n        row.append(date_data) # if does the extend comes into the version then it is more likely that  --> there could last string that is converted in the  list --by each character\n        \n        if len(row) == len(columns):\n            \n            data_rows.append(row)\n    print(len(data_rows))\n    # this  live -->  coding from --\n    print(\"The length of the each row that is extracted: \", len(data_rows[0]))\n    print(\"Data inside the first row is : \", data_rows[0])\n    print(\"Data inside the second row is : \", data_rows[1])\n    print(\"Data inside the third row is : \", data_rows[2])\n    data = pd.DataFrame(data_rows, columns = columns)\n    data.to_csv(\"data\"+namepart+\".csv\", index = False, encoding = \"utf-8\")# \n    # encode the data by  utf-8 code and run  rest of the model -- that can run --.\n    return data\n# there could be several runs\n# values needs to be run on different services --> \n# CORR VALUE table creation is important right -- with several different activation functions ... \n# vals and running examples -- that can be changed --> in several dimensional matrix, and there can have several dimensional matrix --> very wide matrix -->  \n# go two vectors and stack them together --> \n# exactly having in the neural network with several elements -- that can run with more complex models --> \n# can it be worth to spend even though I can earn the amound I want to earn????  this \n# is the question \n# there could be some runs needs to deployed \n# there could be some run that can taken -->  load the rest --> \ndata = getcoronatable()#\n# some runs that\n# \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# there are different cases and different general values also some conditions that can be shown and derived from the general approaches --> fixing the general values --> \n# there can be several values -->  very basica and general -- use cases -->  \nindex = data[\"Country,Other\"].values\nprint(len(index), data.shape)# 20 items as the index of the data  frame that is going to run --> \ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# interactive dahsboards build with bokeh python\n# \nfrom bokeh.io import show\nfrom bokeh.models.grids import Grid\nfrom bokeh.models.plots import Plot\nfrom bokeh.models.axes import LinearAxis\nfrom bokeh.models.ranges import Range1d\nfrom bokeh.models.glyphs import Line, Cross\nfrom bokeh.models.sources import ColumnDataSource\nfrom bokeh.models.tickers import SingleIntervalTicker, YearsTicker\nfrom bokeh.models.renderers import GlyphRenderer\nfrom bokeh.models.annotations import Title, Legend, LegendItem","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# every important steps that will take seriously int the cases -- that can be significant --> building the requ\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed.tail()# There could be several runs -- those are very critical to run the better performing models -- ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed[confirmed[\"Country/Region\"]  == \"US\"]#in some countries there state data included and for some there are no state or sub regional data included.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total number of geographical regional data collected: \", confirmed.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed.head(2)# sample rows from the top","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed.tail(2)# sample rows from bottom ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_total(cols):\n    \n    \"\"\"\n    cols has two values inside it.\n    1st -> Country/Region column\n    2nd -> 3/31/20 that particular days data point.\n    \n    for some regions there are sub regional data points. so for particular day in order to get the total need to sum\n    filtered values in the column\n    \n    \"\"\"\n    confirmed\n\ncols = ['Country/Region','3/31/20']\nconfirmed[confirmed['Country/Region'] == \"China\"][cols].sum()\nplt.show()# with no address == directly show th graph --> ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first element and the last element from the row --> \n#rows = rows[1:]# first element row of the data will be removed since there is the  header of the columns \ndeaths[deaths['Country/Region'] == \"China\"][cols].plot()\nplt.show()#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['1/22/20', '1/23/20', '1/24/20', '1/25/20', '1/26/20', '1/27/20', '1/28/20', '1/29/20',\n       '1/30/20', '1/31/20', '2/1/20', '2/2/20', '2/3/20', '2/4/20', '2/5/20',\n       '2/6/20', '2/7/20', '2/8/20', '2/9/20', '2/10/20', '2/11/20', '2/12/20',\n       '2/13/20', '2/14/20', '2/15/20', '2/16/20', '2/17/20', '2/18/20',\n       '2/19/20', '2/20/20', '2/21/20', '2/22/20', '2/23/20', '2/24/20',\n       '2/25/20', '2/26/20', '2/27/20', '2/28/20', '2/29/20', '3/1/20',\n       '3/2/20', '3/3/20', '3/4/20', '3/5/20', '3/6/20', '3/7/20', '3/8/20',\n       '3/9/20', '3/10/20', '3/11/20', '3/12/20', '3/13/20', '3/14/20',\n       '3/15/20', '3/16/20', '3/17/20', '3/18/20', '3/19/20', '3/20/20',\n       '3/21/20', '3/22/20']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"current_deaths = deaths[deaths['Country/Region'] == \"China\"][cols].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"current_deaths.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n# there are some risk managements --> that can work with several sectors -- that will work out with \n# potentially high observeable results --> that can be changed --> \n# there was an entry or no entry --> to go in t that country -->needs to be update with several values- \n# meret similar clusters -- and convergence  of the clusters --> bottom up or greedy algorithsms --> that can be update -- several values that is going to show the the final objective --> ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"../input/novel-corona-virus-2019-dataset/\")# need to build currrent version of data  set that will not be updated with several values or that can be changed withs everal values-\n# that needs to be updated with several options that is not going to work with al\n# all the dedicated version of the software that is currently working on with --> ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# can deploy dl model to run on those -->  \nconfirmed = pd.read_csv(\"../input/novel-corona-virus-2019-dataset/time_series_covid_19_confirmed.csv\")\nrecovered = pd.read_csv(\"../input/novel-corona-virus-2019-dataset/time_series_covid_19_recovered.csv\")\ndeaths = pd.read_csv(\"../input/novel-corona-virus-2019-dataset/time_series_covid_19_deaths.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confirmed.shape, recovered.shape, deaths.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed.columns# information inside the state that will be load ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# geographics -- mapping\n# \ndates = ['1/22/20', '1/23/20',\n       '1/24/20', '1/25/20', '1/26/20', '1/27/20', '1/28/20', '1/29/20',\n       '1/30/20', '1/31/20', '2/1/20', '2/2/20', '2/3/20', '2/4/20', '2/5/20',\n       '2/6/20', '2/7/20', '2/8/20', '2/9/20', '2/10/20', '2/11/20', '2/12/20',\n       '2/13/20', '2/14/20', '2/15/20', '2/16/20', '2/17/20', '2/18/20',\n       '2/19/20', '2/20/20', '2/21/20', '2/22/20', '2/23/20', '2/24/20',\n       '2/25/20', '2/26/20', '2/27/20', '2/28/20', '2/29/20', '3/1/20',\n       '3/2/20', '3/3/20', '3/4/20', '3/5/20', '3/6/20', '3/7/20', '3/8/20',\n       '3/9/20', '3/10/20', '3/11/20', '3/12/20', '3/13/20', '3/14/20',\n       '3/15/20', '3/16/20', '3/17/20', '3/18/20', '3/19/20', '3/20/20',\n       '3/21/20', '3/22/20', '3/23/20', '3/24/20', '3/25/20', '3/26/20']\ngeo_data = ['Province/State', 'Country/Region', 'Lat', 'Long']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(confirmed[dates].sum().values)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rate function\nrates = [0]# supposing that starts from 100 from first day of the year \ndaily_change = [100]\nseries = confirmed[dates].sum().values\nfor i in range(1, len(confirmed[dates].sum().values)):\n    current = (series[i]-series[i-1])/series[i]\n    temp = series[i]-series[i-1]\n    rates.append(current)\n    daily_change.append(temp)\nplt.figure(figsize = (15, 10))#need to do grid enabler in the dataset -- visualization \nplt.plot(rates)\nplt.show()# we can define that the growth precential rate  is still exponent\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 10))\nplt.plot(daily_change)\nplt.xlabel(\"Day started from January 22nd 2020\")\nplt.ylabel(\"Number of impected people daily\")\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nThere has to be triple measure -- \nOn comfirmed cases\nOn Recovered cases\nOn deaths\n\n\"\"\"\nseries[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in confirmed['Country/Region'].values:\n    print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = pd.DataFrame(columns = confirmed.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = data1.append(confirmed.iloc[0], ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = data1.drop(index = 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data= confirmed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data[\"Lat\"]!=33.0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Here I am trying to create data points for the weather data will be connected to the my analysis. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"#'%.3f'%geo_point0[0]# wuth 3 data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_geomapping(geo_arr):\n    # this will get the particular geolocation array with lenght of 2 \n    lat = geo_arr[0] # \n    lon = geo_arr[1] # \n    lat = '%.3f'%lat\n    lon = '%.3f'%lon\n    return lat+\",\"+lon\ndef lat_lon_str(confirmed):\n    lat_lon = []\n    for i in range(confirmed.shape[0]):\n        geo_point = confirmed[[\"Lat\", \"Long\"]].iloc[i].values\n        lat_lon.append(get_geomapping(geo_point))\n    return lat_lon\nlat_lon = lat_lon_str(confirmed)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lat_lon[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from bs4 import BeautifulSoup\nimport json\nimport requests\nimport datetime\n\nprint(\"webpage renderer loaded!\")\n#today\n\ndef  get_weather_data(particular_lat_lon):\n    api_key = \"0ebe4425fbad41d8aaf25354200504\"\n    today = str(datetime.datetime.today().date())\n    start = str(datetime.datetime.today().date()-datetime.timedelta(35)) # try to load the last 30 days of weather data --> \n    print(\"Start date : \", start, \"End date : \", today)\n    # I can get he more data points but, the limit is that historical data will return 5 weeks of the full data points with 24hrs  of temperature\n    \"\"\"\n    start is the start date\n    today is the enddate\n    difference between today  and start date is 30 days with no more time and seconds delta by timedelta element\n    particular_lat_lon is the  latitude and longitude.\n    api_key is provided api_key from the platform that is using to get the weather data. \n    \n    \n    \"\"\"\n    \n    # on q key in the dictionary it has to be modified based on the values it is getting from the outside data points\n    params = {\"key\":api_key, \"q\":particular_lat_lon, \"format\":\"json\", \"tp\":24, \"date\":start, \"enddate\":today}\n\n    print(params) # the filter the data from  here to load the it is possible  to make star t and end points.\n    # \n    response = requests.get(\"https://api.worldweatheronline.com/premium/v1/past-weather.ashx\", params = params)\n    historical_weather = json.loads(response.content)\n    dates = []\n    maxtempC = []\n    mintempC = []\n    avgtempC = []\n    daylighthours = []\n    totalSnow = [] # basically the rain or snow that will drop  when it is \n    df = pd.DataFrame(columns = [\"date\", \"maxtempC\", \"mintempC\", \"avgtempC\", \"sunHour\", \"totalSnow_cm\"])\n    for weather in historical_weather[\"data\"][\"weather\"]:\n        dates.append(weather['date'])\n        maxtempC.append(weather['maxtempC'])\n        mintempC.append(weather[\"mintempC\"])\n        avgtempC.append(weather[\"avgtempC\"])\n        daylighthours.append(weather[\"sunHour\"])\n        totalSnow.append(weather[\"totalSnow_cm\"])\n    df[\"date\"] = dates\n    df[\"maxtempC\"] = maxtempC\n    df[\"mintempC\"] = mintempC\n    df[\"avgtempC\"] = avgtempC\n    df[\"sunHour\"] = daylighthours\n    df[\"totalSnow_cm\"] = totalSnow\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = str(datetime.datetime.today().date()-datetime.timedelta(35)) # try to load the last 30 days of weather data --> \nstart","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ll = lat_lon[0]\ndf = get_weather_data(ll)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# it is starts from where it needs to be started -->\ndf.shape  # significant -->","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#top_30 countries will be included in the study","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = [\"Country/Region\", \"4/3/20\"]\ncol = confirmed.columns[-1]\n# try to condside\nprint(col)\nlastday = confirmed[confirmed['4/3/20'] > 1000][cols].sort_values(\"4/3/20\", ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(lastday[\"Country/Region\"].unique())# there are several regions in that are not unique -- since included sub regions  are in the data points","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_pie(confirmed, pdaycol):\n    \"\"\"\n    pdaycol is the name of the column that is getting updated when the data runs with the higher efficiency --> \n    \n    \"\"\"\n    #pdaycol = '3/31/20'\n    print(pdaycol, \"the  CoronaPie Chart\")# for some people showing this would\n    # slicing  data by the pandemic epicenters\n    #confirmed['3/31/20'].sum()# there are some pandemic related data extraction methods that are very prevalent to run for entire cases --> only for the serious cases it will be harder to easer to load and better to perform the machine learning methods --.\n    # setting 5 percent cutline \n    cutline  = confirmed[pdaycol].sum()*.01\n    # cutline countries with more 1000\n    col = [\"Country/Region\", pdaycol]\n    morethan10k = confirmed[confirmed[col][pdaycol]>cutline][col]\n\n\n    rest_sum = confirmed[confirmed[col][pdaycol]<=cutline][col].sum().values[1]\n    print(rest_sum)\n    Countries = list(morethan10k[\"Country/Region\"].values)\n    Countries.append(\"Other\")\n    TotalCases = list(morethan10k[pdaycol])# pday is the particular day that frame that is going to be created in order to run in der\n    TotalCases.append(rest_sum)\n    data  = []\n    for i in range(len(Countries)):\n        temp = []\n        temp.append(Countries[i])\n        temp.append(TotalCases[i])\n        data.append(temp)\n    #data\n\n    data = pd.DataFrame(data =data, columns = [\"Countries\", \"p_day\"]) # p_day is the particular day\n    data = data.sort_values(\"p_day\", ascending= False)\n    explosion = []\n    # try to explode countries with more than 10% percent of the total case in particary days has been exploded\n\n    for i in data[\"p_day\"].values:\n        if i/data[\"p_day\"].sum()> .1:\n            explosion.append(0.1)\n        else:\n            explosion.append(0)\n    print(explosion)\n    plt.figure(figsize=(15, 15))\n    plt.pie(\"p_day\", labels = \"Countries\", explode = explosion, data = data, autopct = \"%.0f%%\")\n    plt.title(pdaycol)\n    plt.show()\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"build_pie(confirmed, '4/7/20')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_sevendays():\n    last_sevendays = confirmed.columns[-7:]\n    \n    for day in last_sevendays:\n        build_pie(confirmed[last_sevendays], day)\n#build_sevendays()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed['4/7/20'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_sevendays = confirmed.columns[-7:]\n    \n#for day in last_sevendays:\n#    build_pie(day)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 8)\naxes = axes.ravel()\nfor i, ax in enumerate(axes):\n    ax.plot()\n    print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns# seaborn plotting library for  --> # --> predicted -- point __. \n# fix the algorithm to run the model.\n# ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import requests\n\n# using open weather data - to create the data base with the several  load able points \nweather_url = \"http://bulk.openweathermap.org/snapshot/weather_14.json.gz?appid=6c3b580b086824dc7ce8152b4aec2c67\"\nprint(weather_url)\n# there are som end points for the api key -->\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting threads -- small useable and workable with whole plans that can work with those all the case -- \nresponse = requests.get(weather_url)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"response.content# lockdown on state --> ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for the confirmed cases >> summation of the values has to be performeddata\ncountries = confirmed[\"Country/Region\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_series_cols = ['1/22/20', '1/23/20',\n       '1/24/20', '1/25/20', '1/26/20', '1/27/20', '1/28/20', '1/29/20',\n       '1/30/20', '1/31/20', '2/1/20', '2/2/20', '2/3/20', '2/4/20', '2/5/20',\n       '2/6/20', '2/7/20', '2/8/20', '2/9/20', '2/10/20', '2/11/20', '2/12/20',\n       '2/13/20', '2/14/20', '2/15/20', '2/16/20', '2/17/20', '2/18/20',\n       '2/19/20', '2/20/20', '2/21/20', '2/22/20', '2/23/20', '2/24/20',\n       '2/25/20', '2/26/20', '2/27/20', '2/28/20', '2/29/20', '3/1/20',\n       '3/2/20', '3/3/20', '3/4/20', '3/5/20', '3/6/20', '3/7/20', '3/8/20',\n       '3/9/20', '3/10/20', '3/11/20', '3/12/20', '3/13/20', '3/14/20',\n       '3/15/20', '3/16/20', '3/17/20', '3/18/20', '3/19/20', '3/20/20',\n       '3/21/20', '3/22/20', '3/23/20', '3/24/20', '3/25/20', '3/26/20',\n       '3/27/20', '3/28/20', '3/29/20', '3/30/20', '3/31/20', '4/1/20',\n       '4/2/20', '4/3/20']\ntotals = confirmed[time_series_cols].sum().values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(totals, color = \"red\")\nplt.legend(\"Upper left\")# there legends has be special more likely\nplt.xlabel(\"time points\")\nplt.ylabel(\"number of cases\")\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sums_each_country = []\nfor country in countries:\n    sums_each_country.append(confirmed[confirmed[\"Country/Region\"] == country][time_series_cols].sum().values)\n    \nall_df = pd.DataFrame(data = sums_each_country,  columns = time_series_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df[\"Country/Region\"]  = countries\nall_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df[all_df['4/3/20']>1000].sort_values('4/3/20', ascending = False)\n# there are several frames\n# need to create the ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import seaborn as sns\n\n#sns.set(style = \"ticks\")\n#sns.relplot(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"build_pie(all_df, '4/3/20')# now it is going into the perfect,  that can be loaded --> ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport matplotlib# not only a single plot that needs to build there are several data points\n# there can be particular countries with data speculations -->\ndef get_ts_from_str(time_series_cols):\n    time_series_cols_dt = []\n    for i in time_series_cols:\n        time_series_cols_dt.append(datetime.datetime.strptime(i, \"%m/%d/%y\"))\n    len(time_series_cols_dt)\n    return time_series_cols_dt\n\ndef build_top_profiles(all_df, pday):\n    # there are several acceptable loads for the data \n    #pday = \"4/3/20\"  this particular date is thrown for the building of te graph\n    \n    all_df = all_df.sort_values(pday, ascending = False)\n    \n    top_countries = all_df[\"Country/Region\"].values[:10]\n    particular_country = all_df[all_df[\"Country/Region\"] == top_countries[0]][time_series_cols].values \n    new_particular_country1 = np.reshape(particular_country,  (particular_country.shape[1],))\n    particular_country = all_df[all_df[\"Country/Region\"] == top_countries[1]][time_series_cols].values \n    new_particular_country2 = np.reshape(particular_country,  (particular_country.shape[1],))\n    particular_country = all_df[all_df[\"Country/Region\"] == top_countries[2]][time_series_cols].values \n    new_particular_country3 = np.reshape(particular_country,  (particular_country.shape[1],))\n    particular_country = all_df[all_df[\"Country/Region\"] == top_countries[3]][time_series_cols].values \n    new_particular_country4 = np.reshape(particular_country,  (particular_country.shape[1],))\n    particular_country = all_df[all_df[\"Country/Region\"] == top_countries[4]][time_series_cols].values \n    new_particular_country5 = np.reshape(particular_country,  (particular_country.shape[1],))\n    particular_country = all_df[all_df[\"Country/Region\"] == top_countries[5]][time_series_cols].values \n    new_particular_country6 = np.reshape(particular_country,  (particular_country.shape[1],))\n    particular_country = all_df[all_df[\"Country/Region\"] == top_countries[6]][time_series_cols].values \n    new_particular_country7 = np.reshape(particular_country,  (particular_country.shape[1],))\n    particular_country = all_df[all_df[\"Country/Region\"] == top_countries[7]][time_series_cols].values \n    new_particular_country8 = np.reshape(particular_country,  (particular_country.shape[1],))\n    particular_country = all_df[all_df[\"Country/Region\"] == top_countries[8]][time_series_cols].values \n    new_particular_country9 = np.reshape(particular_country,  (particular_country.shape[1],))\n    particular_country = all_df[all_df[\"Country/Region\"] == top_countries[9]][time_series_cols].values \n    new_particular_country10 = np.reshape(particular_country,  (particular_country.shape[1],))\n    ts_cols = get_ts_from_str(time_series_cols)\n    x_label_display = []\n    for i in range(len(time_series_cols)):\n        if  i%5 == 0:\n            x_label_display.append(time_series_cols[i])\n        else:\n            x_label_display.append(\"\")\n    if x_label_display[-1] == \"\":\n        x_label_display = x_label_display[:-1]\n        x_label_display.append(time_series_cols[-1])\n    print(len(x_label_display))\n    plt.figure(figsize=(10,15))\n    gs = matplotlib.gridspec.GridSpec(5, 5)\n    #plt.title(\"Total\")\n    # there is country mapping that is taking place\n    top1 = top_countries[0]\n    top2 = top_countries[1]\n    top3 = top_countries[2]\n    top4 = top_countries[3]\n    top5 = top_countries[4]\n    top6 = top_countries[5]\n    top7 = top_countries[6]\n    top8 = top_countries[7]\n    top9 = top_countries[8]\n    top10 = top_countries[9]\n    \n    ax1 = plt.subplot(gs[:4, :3], title = top1)# up to 3 0,1, 2 x 0, 1, 2\n    ax2 = plt.subplot(gs[0, 3:], title = top2) # last column by the graph\n    ax3 = plt.subplot(gs[1, 3:], title = top3) # \n    ax4 = plt.subplot(gs[2, 3:], title = top4)\n    ax5 = plt.subplot(gs[3, 3:], title = top5)\n    ax6 = plt.subplot(gs[4, 0], title = top6)\n    ax7 = plt.subplot(gs[4, 1], title = top7)\n    ax8 = plt.subplot(gs[4, 2], title = top8)\n    ax9 = plt.subplot(gs[4, 3], title = top9)\n    ax10 = plt.subplot(gs[4, 4], title = top10)\n    tempdf =  pd.DataFrame(columns = [\"cases\"])\n    tempdf[\"cases\"] = new_particular_country1\n    tempdf.index = ts_cols\n    tempdf = tempdf.to_period(freq =\"W\")\n    print(tempdf.shape)\n    ax1.plot(new_particular_country1)\n    ax2.plot(new_particular_country2)\n    ax3.plot(new_particular_country3)\n    ax4.plot(new_particular_country4)\n    ax5.plot(new_particular_country5)\n    ax6.plot( new_particular_country6)\n    ax7.plot( new_particular_country7)\n    ax8.plot(new_particular_country8)\n    ax9.plot(new_particular_country9)\n    ax10.plot(new_particular_country10)\n    #plt.title(\"Top 5 profiles in number of cases\")\n    plt.show()\n    \n    top_profiles = [new_particular_country1, new_particular_country2, new_particular_country3, new_particular_country4, new_particular_country5, new_particular_country6, new_particular_country7, new_particular_country8, new_particular_country9, new_particular_country10]\n    daily_ups_tp = [] # daily ups for the top profiles--> \n    for current_list in top_profiles:\n        temp = []\n        index = 1\n        for value in current_list[:-1]:\n            temp.append(current_list[index]-value)\n            index+=1\n        daily_ups_tp.append(temp)\n    # there is an inside variable no --> s global --> \n    plt.figure(figsize= (10, 15))\n    gs = matplotlib.gridspec.GridSpec(5, 5)# two graphs that has to get build \n    #plt.title(\"Daily\")\n    ax1 = plt.subplot(gs[:4, :3], title = top1)# up to 3 0,1, 2 x 0, 1, 2\n    ax2 = plt.subplot(gs[0, 3:], title = top2) # last column by the graph\n    ax3 = plt.subplot(gs[1, 3:], title = top3) # \n    ax4 = plt.subplot(gs[2, 3:], title = top4)\n    ax5 = plt.subplot(gs[3, 3:], title = top5)\n    # next5 profiles of the countries will be included here to run --> \n    ax6 = plt.subplot(gs[4, 0], title = top6)\n    ax7 = plt.subplot(gs[4, 1], title = top7)\n    ax8 = plt.subplot(gs[4, 2], title = top8)\n    ax9 = plt.subplot(gs[4, 3], title = top9)\n    ax10 = plt.subplot(gs[4, 4], title = top10)\n    ax1.plot(daily_ups_tp[0])\n    ax2.plot(daily_ups_tp[1])\n    ax3.plot(daily_ups_tp[2])\n    ax4.plot(daily_ups_tp[3])\n    ax5.plot(daily_ups_tp[4])\n    ax6.plot(daily_ups_tp[5])\n    ax7.plot(daily_ups_tp[6])\n    ax8.plot(daily_ups_tp[7])\n    ax9.plot(daily_ups_tp[8])\n    ax10.plot(daily_ups_tp[9])\n    plt.show()\nbuild_top_profiles(all_df, \"4/3/20\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Working with deaths table combine --> deaths and confirmed case table"},{"metadata":{"trusted":true},"cell_type":"code","source":"deaths.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"deaths.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Trends on the death 1: general trend 2: daily new deaths\")\ntotal_deaths_by_country = []\nfor country in deaths[\"Country/Region\"].unique():\n    temp = deaths[deaths[\"Country/Region\"] == country][time_series_cols].sum().values\n    total_deaths_by_country.append(temp)\ndeaths_df = pd.DataFrame(total_deaths_by_country, columns = time_series_cols)\ndeaths_df[\"Country/Region\"] = deaths[\"Country/Region\"].unique()\ndeaths_df = deaths_df.sort_values(\"4/3/20\", ascending = False)\nbuild_top_profiles(deaths_df, time_series_cols[-1])# build the last ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exdate = datetime.datetime.strptime(time_series_cols[0], \"%m/%d/%y\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#time_series_cols_dt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Recovered cases"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Trends on the death 1: general trend 2: daily new deaths\")\ntotal_recovered_by_country = []\ncol = time_series_cols[-1]\nfor country in recovered[\"Country/Region\"].unique():\n    temp = recovered[recovered[\"Country/Region\"] == country][time_series_cols].sum().values\n    total_recovered_by_country.append(temp)\nrecovered_df = pd.DataFrame(total_recovered_by_country, columns = time_series_cols)\nrecovered_df[\"Country/Region\"] = recovered[\"Country/Region\"].unique()\nrecovered_df = recovered_df.sort_values(col, ascending = False)\nbuild_top_profiles(recovered_df, time_series_cols[-1])# build the last ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Combined plotting"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}