{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Objective: Classify people with diabetes or not using  different features","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd \nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport missingno\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, metrics, preprocessing\n\n#Machine Learning\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\n#preprocessing\nfrom sklearn.impute import KNNImputer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis\n\nReferences:\n1. https://github.com/dformoso/sklearn-classification/blob/master/Data%20Science%20Workbook%20-%20Census%20Income%20Dataset.ipynb\n2. https://github.com/mrdbourke/your-first-kaggle-submission/blob/master/kaggle-titanic-dataset-example-submission-workflow.ipynb\nAssociated video: https://www.youtube.com/watch?v=f1y9wDDxWnA&feature=youtu.be","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"df = pd.read_csv('../input/diabetes-dataset/diabetes2.csv')\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shows that there are no null values in the ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missingno.matrix(df) #nice way of visualizing missing values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Feature: Pregnancies**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nax = sns.distplot(df['Pregnancies'][df.Outcome == 1], color =\"darkturquoise\", rug = True)\nsns.distplot(df['Pregnancies'][df.Outcome == 0], color =\"lightcoral\",rug = True)\nplt.legend(['Diabetes', 'No Diabetes'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distplot shows both the histogram and the KDE together. From the graph, number of pregnancies show some distinguishability(?) ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Pregnancies'].value_counts()\n#df['Pregnancies'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x = df['Pregnancies'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature: Glucose","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nax = sns.distplot(df['Glucose'][df.Outcome == 1], color =\"darkturquoise\", rug = True)\nsns.distplot(df['Glucose'][df.Outcome == 0], color =\"lightcoral\", rug = True)\nplt.legend(['Diabetes', 'No Diabetes'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min(df['Glucose']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['Glucose'] == 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normal Glucose should be between 60 to 140, 0 seems absurd","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['Glucose'].lt(60)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Glucose'].value_counts() #not significant","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x = df['Glucose'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since only five rows have Glucose 0, it can be removed\n\n# Feature: BloodPressure","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nax = sns.distplot(df['BloodPressure'][df.Outcome == 1], color =\"darkturquoise\", rug=True)\nsns.distplot(df['BloodPressure'][df.Outcome == 0], color =\"lightcoral\", rug=True)\nplt.legend(['Diabetes', 'No Diabetes'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min(df['BloodPressure']) #Again, seems absurd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.loc[df['BloodPressure'] == 0].shape[0])\nprint(df.loc[df['BloodPressure'] == 0].shape[0]/df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['BloodPressure'].lt(40)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['BloodPressure'].lt(40)].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['BloodPressure'].gt(120)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x = df['BloodPressure'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since 35 rows have Blood pressure = 0, it would not be wise to drop them \n\n# Feature: SkinThickness","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nax = sns.distplot(df['SkinThickness'][df.Outcome == 1], color =\"darkturquoise\", rug=True)\nsns.distplot(df['SkinThickness'][df.Outcome == 0], color =\"lightcoral\", rug=True)\nplt.legend(['Diabetes', 'No Diabetes'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x = df['SkinThickness'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['SkinThickness'] == 0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['SkinThickness'].lt(2)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can't drop 227 rows as well\n\n# Feature: Insulin","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nax = sns.distplot(df['Insulin'][df.Outcome == 1], color =\"darkturquoise\", rug=True)\nsns.distplot(df['Insulin'][df.Outcome == 0], color =\"lightcoral\", rug=True)\nplt.legend(['Diabetes', 'No Diabetes'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x = df['Insulin'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['Insulin'].lt(16)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['Insulin'] == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['Insulin'] == 0].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature: BMI","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nax = sns.distplot(df['BMI'][df.Outcome == 1], color =\"darkturquoise\", rug=True)\nsns.distplot(df['BMI'][df.Outcome == 0], color =\"lightcoral\", rug=True)\nplt.legend(['Diabetes', 'No Diabetes'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x = df['BMI'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.BMI == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.BMI == 0].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature: DiabetesPedigreeFunction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nax = sns.distplot(df['DiabetesPedigreeFunction'][df.Outcome == 1], color =\"darkturquoise\", rug=True)\nsns.distplot(df['DiabetesPedigreeFunction'][df.Outcome == 0], color =\"lightcoral\", rug=True)\nplt.legend(['Diabetes', 'No Diabetes'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x = df['DiabetesPedigreeFunction'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature: Age","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nax = sns.distplot(df['Age'][df.Outcome == 1], color =\"darkturquoise\", rug=True)\nsns.distplot(df['Age'][df.Outcome == 0], color =\"lightcoral\", rug=True)\nsns.distplot(df['Age'], color =\"green\", rug=True)\nplt.legend(['Diabetes', 'No Diabetes', 'all'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x = df['Age'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We saw that there are many missing values in the columns, but they have been imputed with zero. Let's remove them and visualize ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_with_na = df.copy(deep = True)\ndf_with_na['Insulin'] = df['Insulin'].map(lambda i: np.nan if i==0 else i)\ndf_with_na['SkinThickness'] = df['SkinThickness'].map(lambda i: np.nan if i==0 else i)\ndf_with_na['BloodPressure'] = df['BloodPressure'].map(lambda i: np.nan if i==0 else i)\ndf_with_na['BMI'] = df['BMI'].map(lambda i: np.nan if i==0 else i)\ndf_with_na['Glucose'] = df['Glucose'].map(lambda i: np.nan if i==0 else i)\n\nmissingno.matrix(df_with_na) #nice way of visualizing missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.corr(), annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df_with_na.corr(), annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that there's high correlation between BMI and SkinThickness. Since Skinthickness has many missing values, maybe we can drop it. \n\n# Bivariate Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df, hue='Outcome')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No two features have very good distinguishing property\n\n# Feature importance analysis\n\nWe could see there are many missing values for many columns. Let's see how we can handle it[](http://)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier()\nclf.fit(df.drop('Outcome', axis = 1), df['Outcome'])\nplt.figure()\nimportance = clf.feature_importances_\nprint(df.drop('Outcome', axis=1).columns)\nprint(clf.feature_importances_)\nimportance = pd.DataFrame(importance, index=df.drop('Outcome', axis=1).columns, columns=[\"Importance\"])\nimportance.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(20,len(importance)/2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We know BMI has 11 zero values. Let's see if the feature importance increases if we remove those values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_no_BMI0 = df[df.BMI != 0]\n\nclf = RandomForestClassifier()\nclf.fit(df_no_BMI0.drop('Outcome', axis = 1), df_no_BMI0['Outcome'])\nplt.figure()\nimportance = clf.feature_importances_\nprint(df_no_BMI0.drop('Outcome', axis=1).columns)\nprint(clf.feature_importances_)\nimportance = pd.DataFrame(importance, index=df_no_BMI0.drop('Outcome', axis=1).columns, columns=[\"Importance\"])\nimportance.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(20,len(importance)/2))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remains more or less the same. Let's check it with Blood pressure (35 zero values)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_no_BP0 = df[df.BloodPressure != 0]\n\nclf = RandomForestClassifier()\nclf.fit(df_no_BP0.drop('Outcome', axis = 1), df_no_BP0['Outcome'])\nplt.figure()\nimportance = clf.feature_importances_\nprint(df_no_BP0.drop('Outcome', axis=1).columns)\nprint(clf.feature_importances_)\nimportance = pd.DataFrame(importance, index=df_no_BP0.drop('Outcome', axis=1).columns, columns=[\"Importance\"])\nimportance.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(20,len(importance)/2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the insulin data (370 missing)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_no_IN0 = df[df.Insulin != 0]\n\nclf = RandomForestClassifier()\nclf.fit(df_no_IN0.drop('Outcome', axis = 1), df_no_IN0['Outcome'])\nplt.figure()\nimportance = clf.feature_importances_\nprint(df_no_IN0.drop('Outcome', axis=1).columns)\nprint(clf.feature_importances_)\nimportance = pd.DataFrame(importance, index=df_no_IN0.drop('Outcome', axis=1).columns, columns=[\"Importance\"])\nimportance.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(20,len(importance)/2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping the 0 values show significant increase in the feature importance of Insulin. Shows we need to handle it somehow","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_no_SK0 = df[df.SkinThickness != 0]\n\nclf = RandomForestClassifier()\nclf.fit(df_no_SK0.drop('Outcome', axis = 1), df_no_SK0['Outcome'])\nplt.figure()\nimportance = clf.feature_importances_\nprint(df_no_SK0.drop('Outcome', axis=1).columns)\nprint(clf.feature_importances_)\nimportance = pd.DataFrame(importance, index=df_no_SK0.drop('Outcome', axis=1).columns, columns=[\"Importance\"])\nimportance.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(20,len(importance)/2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Changes the importances of other features, but skin thickness feature importance remains low","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_no_missing = df_with_na.dropna()\nprint(df_no_missing.shape)\n\nclf = RandomForestClassifier()\nclf.fit(df_no_missing.drop('Outcome', axis = 1), df_no_missing['Outcome'])\nplt.figure()\nimportance = clf.feature_importances_\nprint(df_no_missing.drop('Outcome', axis=1).columns)\nprint(clf.feature_importances_)\nimportance = pd.DataFrame(importance, index=df_no_missing.drop('Outcome', axis=1).columns, columns=[\"Importance\"])\nimportance.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(20,len(importance)/2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we see after removing the missing values, the feature importance list looks like this. Glucose, insulin and age seem to be the three most important features.\n\n## Data preprocessing:\n\nWe saw missing values in the following columns:\n1. Glucose: 5\n2. BMI: 11\n3. Insulin: 370\n4. SkinThickness: 227\n5. Blood Pressure: 35\n\nSteps:\n1. Remove the missing values of Glucose and BMI (both very important features) [DATASET 1]\n2. Drop SkinThickness because the feature importance is low [DATASET 2]\n3. Create a train test split (test split should not contain any missing value)\n4. Use data imputation \n\nReferences:\n1. General overview of popular methods: https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779\n> > \n2. Imputation using KNN: https://datascienceplus.com/knnimputer-for-missing-value-imputation-in-python-using-scikit-learn/\nhttps://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html\n\n\n\n## DATASET 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X, X_test, y, y_test = train_test_split(df.drop('Outcome', axis= 1), df['Outcome'], test_size=0.20, random_state=42)\nX, X_test_with_na, y, _ = train_test_split(df_with_na.drop('Outcome', axis= 1), df_with_na['Outcome'], test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.concat([X, y], axis = 1)\ndf_train_without_missing = df_train.dropna()\n\ny_wo_missing_train = df_train_without_missing['Outcome']\nX_wo_missing_train = df_train_without_missing.drop(columns = ['Outcome'])\n\nscaler = preprocessing.StandardScaler().fit(X_wo_missing_train)\n\n#DATASET 1\nX_wo_missing_train = scaler.transform(X_wo_missing_train)\nX_wo_missing_test = scaler.transform(X_test)\nX_wo_missing_test_with_na = scaler.transform(X_test_with_na)\n\nprint(X.shape)\nprint(df_train_without_missing.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DATASET 2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer = KNNImputer(n_neighbors=5)\nX_knn_imp = imputer.fit_transform(X)\ndf_knn_imp = pd.DataFrame(X_knn_imp, columns = df.drop('Outcome', axis = 1).columns) #DATASET 2\n\nclf = RandomForestClassifier()\nclf.fit(df_knn_imp, y)\nplt.figure()\nimportance = clf.feature_importances_\nprint(df_knn_imp.columns)\nprint(clf.feature_importances_)\nimportance = pd.DataFrame(importance, index=df_knn_imp.columns, columns=[\"Importance\"])\nimportance.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(20,len(importance)/2))\n\nscaler = preprocessing.StandardScaler().fit(df_knn_imp)\nX_knn_imp_train = scaler.transform(df_knn_imp)\nX_knn_imp_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DATASET 3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nimp = IterativeImputer(max_iter = 10, random_state = 42)\nimp.fit(X)\nX_iter_imp = imp.transform(X)\nX_iter_imp = pd.DataFrame(X_iter_imp, columns = X.columns)\n\n\nclf = RandomForestClassifier()\nclf.fit(X_iter_imp, y)\nplt.figure()\nimportance = clf.feature_importances_\nprint(X_iter_imp.columns)\nprint(clf.feature_importances_)\nimportance = pd.DataFrame(importance, index=X_iter_imp.columns, columns=[\"Importance\"])\nimportance.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(20,len(importance)/2))\n\nscaler = preprocessing.StandardScaler().fit(X_iter_imp)\nX_iter_imp_train = scaler.transform(X_iter_imp)\nX_iter_imp_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model training: \n\nSteps: \n1. Choose dataset\n2. Scale train and test data\n3. Do gridsearch (if applicable) for best hyperparamters\n\nReferences: \n1. Scaling: https://scikit-learn.org/stable/modules/preprocessing.html\n2. Cross validation: \nhttps://stats.stackexchange.com/questions/411290/how-to-use-a-cross-validated-model-for-prediction\nhttps://scikit-learn.org/stable/modules/cross_validation.html\n3. Grid Search:\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV\n4. Randomized Search vs Grid Search: \n    https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html\n    https://stackoverflow.com/questions/57426633/what-is-randomsearchcv-and-gridsearchcv\n5. ROC Curves:\nhttps://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n\n6. Logistic regression:\n    a. Documentation : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n    b. Which solver to use: \n    https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451\n    https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-defintions/52388406#52388406\n\n## Utils:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# \ndef fit_ml_algo(algo, X_train, y_train, X_test, y_test, cv):\n    model = algo.fit(X_train, y_train)\n    test_prediction = model.predict(X_test)\n    test_probs = model.predict_proba(X_test)[:,1]\n    train_accuracy = model.score(X_train, y_train)*100\n    test_accuracy = model.score(X_test, y_test)*100\n    train_prediction = model_selection.cross_val_predict(algo, X_train, y_train, cv = 10, n_jobs = -1)\n    acc_cv = metrics.accuracy_score(y_train, train_prediction)*100\n    model_scores = model_selection.cross_val_score(LogisticRegression(), X_train, y_train, cv = 10, n_jobs = -1)\n\n    print(\"Cross Validation accuracy: (%0.2f) %0.4f (+/- %0.4f)\" % (acc_cv, model_scores.mean(), model_scores.std() * 2))\n    print('Model Test Accuracy: %0.2f   Model Train Accuracy: %0.2f'%(test_accuracy, train_accuracy))\n    print(metrics.classification_report(y_test, test_prediction))\n    print(\"Confusion matrix\")\n    print(metrics.confusion_matrix(y_test,test_prediction))\n    \n    return train_prediction, test_prediction, test_probs\n    \n    \n# calculate the fpr and tpr for all thresholds of the classification\ndef plot_roc_curve(y_test, preds):\n    fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n    roc_auc = metrics.auc(fpr, tpr)\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([-0.01, 1.01])\n    plt.ylim([-0.01, 1.01])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n    \n    \n# Adding gridsearch report creating code\ndef report(results, n_top = 5):\n    for i in range(1, n_top +1 ):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset 1: data without missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"c = [0.01, 0.1, 1, 5, 10]\nparam_grid = [{'C': c, 'penalty': ['l2'], 'solver': ['liblinear', 'newton-cg','saga','lbfgs']}, {'C': c, 'penalty': ['l1'], 'solver': ['liblinear', 'saga']}]\n\nlg_grid = model_selection.GridSearchCV(LogisticRegression(), param_grid, n_jobs = -1)\nlg_grid.fit(X_train_wo_msng, y_train_wo_msng)\n\nreport(lg_grid.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_prediction_1, test_prediction_1, test_probs_1 = fit_ml_algo(LogisticRegression(C = 0.1, penalty = 'l1', solver = 'liblinear'), X_train_without_missing_tf, y_train_without_missing, X_test_tf, y_test, cv = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(y_test, test_probs_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset 2: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"c = [0.01, 0.1, 1, 5, 10]\nparam_grid = [{'C': c, 'penalty': ['l2'], 'solver': ['liblinear', 'newton-cg','saga','lbfgs']}, {'C': c, 'penalty': ['l1'], 'solver': ['liblinear', 'saga']}]\n\nlg_grid = model_selection.GridSearchCV(LogisticRegression(), param_grid, n_jobs = -1)\nlg_grid.fit(X_knn_imp_train, y)\n\nreport(lg_grid.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_prediction_2, test_prediction_2, test_probs_2 = fit_ml_algo(LogisticRegression(C = 0.1, penalty = 'l1', solver = 'liblinear'), X_knn_imp_train, y, X_knn_imp_test, y_test, cv = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(y_test, test_probs_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Dataset 3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"c = [0.01, 0.1, 1, 5, 10]\nparam_grid = [{'C': c, 'penalty': ['l2'], 'solver': ['liblinear', 'newton-cg','saga','lbfgs']}, {'C': c, 'penalty': ['l1'], 'solver': ['liblinear', 'saga']}]\n\nlg_grid = model_selection.GridSearchCV(LogisticRegression(), param_grid, n_jobs = -1)\nlg_grid.fit(X_iter_imp_train, y)\n\nreport(lg_grid.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_prediction_3, test_prediction_3, test_probs_3 = fit_ml_algo(LogisticRegression(C = 0.1, penalty = 'l1', solver = 'saga'), X_iter_imp_train, y, X_iter_imp_test, y_test, cv = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(y_test, test_probs_3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVM\n\n1. Documentation:\nhttps://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n\n2. ROC for SVC:\nhttps://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#gridsearch params\nc = [0.01, 0.1, 1, 5, 10]\nparam_grid = [{'C': c, 'kernel': ['linear', 'poly','rbf','sigmoid']}]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVM: Dataset 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_grid = model_selection.GridSearchCV(SVC(), param_grid, n_jobs = -1)\nsvc_grid.fit(X_train_wo_msng, y_train_wo_msng)\nreport(svc_grid.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_prediction, test_prediction, test_probs = fit_ml_algo(SVC(C =5, kernel = 'linear', probability = True), X_train_wo_msng, y_train_wo_msng, X_test_tf, y_test, cv = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(y_test, test_probs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVM: Dataset 2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_grid = model_selection.GridSearchCV(SVC(), param_grid, n_jobs = -1)\nsvc_grid.fit(X_knn_imp_train, y)\nreport(svc_grid.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_prediction, test_prediction, test_probs = fit_ml_algo(SVC(C = 0.01, kernel = 'linear', probability = True), X_knn_imp_train, y, X_knn_imp_test, y_test, cv = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(y_test, test_probs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVM: Dataset 3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_grid = model_selection.GridSearchCV(SVC(), param_grid, n_jobs = -1)\nsvc_grid.fit(X_iter_imp_train, y)\nreport(svc_grid.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_prediction, test_prediction, test_probs = fit_ml_algo(SVC(C = 0.01, kernel = 'linear', probability = True), X_iter_imp_train, y, X_iter_imp_test, y_test, cv = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(y_test, test_probs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost\n\n1. Documentation: https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n\n2. Hyperparameter grid search in XGBoost: https://www.kaggle.com/tilii7/hyperparameter-grid-search-with-xgboost\n\n3. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'min_child_weight': [1, 5, 10], 'gamma': [0.5, 1, 1.5, 2, 5], 'subsample': [0.6, 0.8, 1.0], 'colsample_bytree': [0.6, 0.8, 1.0], 'max_depth': [3, 4, 5]}\nestimator = XGBClassifier(objective= 'binary:logistic', nthread=4, seed=42)\nxg_grid = model_selection.GridSearchCV(estimator=estimator, param_grid=params, scoring = 'roc_auc', n_jobs = 10, cv = 10, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost: Dataset 1","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"xg_grid.fit(X_wo_missing_train, y_wo_missing_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_estimator = xg_grid.best_estimator_\nreport(xg_grid.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_prediction, test_prediction, test_probs = fit_ml_algo(best_estimator, X_wo_missing_train, y_wo_missing_train, X_wo_missing_test_with_na, y_test, cv = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(y_test, test_probs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost: Dataset 2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_grid.fit(X_knn_imp_train, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_estimator = xg_grid.best_estimator_\nreport(xg_grid.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_prediction, test_prediction, test_probs = fit_ml_algo(best_estimator, X_knn_imp_train, y, X_knn_imp_test, y_test, cv = 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost: Dataset 3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_grid.fit(X_iter_imp_train, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_estimator = xg_grid.best_estimator_\nreport(xg_grid.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_prediction, test_prediction, test_probs = fit_ml_algo(best_estimator, X_iter_imp_train, y, X_iter_imp_test, y_test, cv = 10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}