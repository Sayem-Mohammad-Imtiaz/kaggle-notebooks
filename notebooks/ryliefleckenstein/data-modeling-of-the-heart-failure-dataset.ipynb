{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Data Visualization and Modeling of the Heart Failure Dataset\n\n\n\n\n\n## Outline\n\n- Data Visualization\n\n- Recursive feature elimination with cross validation\n\n- Tree based feature selection methods with random forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization library  \nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import RFECV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Analysis\n\n### Heart Failure Prediction Dataset\n\nWe are creating a model to predict the mortality caused by heart failure. \n\n### About this dataset\n\n\"Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worlwide.\nHeart failure is a common event caused by CVDs and this dataset contains 12 features that can be used to predict mortality by heart failure.\n\nMost cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies.\n\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.\" \n\nDavide Chicco, Giuseppe Jurman: Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone. BMC Medical Informatics and Decision Making 20, 16 (2020). (link)\n\nhttps://www.kaggle.com/andrewmvd/heart-failure-clinical-data\n\n### Feature Information\n\n1) age - age of the patient\n\n2) anaemie - decrease of red blood cells or hemoglobin\n\n3) creatinine phosphokinase - Level of the CPK enzyme in the blood (mcg/L)\n\n4) diabetes - if they have it or not (boolean, 0 - no / 1 - yes)\n\n5) ejection fraction - Percentage of blood leaving the heart at each contraction (percentage)\n\n6) high blood pressure - if they have it or not (boolean, 0 - no / 1 - yes)\n\n7) platelets - Platelets in the blood (kiloplatelets/mL)\n\n8) serum creatine - Level of serum creatinine in the blood (mg/dL)\n\n9) serum sodium - Level of serum sodium in the blood (mEq/L)\n\n10) sex - (0 - woman / 1 - man)\n\n11) smoking - (0 - no / 1 - yes)\n\n12) time - Follow-up period (days)\n\n13) DEATH EVENT - If the patient deceased during the follow-up period (boolean) (0 - no / 1 - yes)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_dat = pd.read_csv('/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\nheart_dat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_dat.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y includes our labels and x includes our features\ndlist = ['DEATH_EVENT']\nx = heart_dat.drop(dlist, axis=1)\ny = heart_dat['DEATH_EVENT']\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(y,label=\"Count\")       # Deaths = 96, Alive = 203\nN, Y = y.value_counts()                   # 1 - Deaths, 0 - Alive\nprint('Number of Deaths: ',Y)\nprint('Number Alive : ',N)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization\n\n### Standarization\n\nIn order to see how each feature compares to another we must make sure they are all on the same \"playing field\" and standardize/ normalize the data througgh the following formula:\n\nthe formula $$ Z = \\frac{x - \\mu}{\\sigma} $$\n\nThis just puts every feature on the same scale for cleaner more informative data vizualization.\n\n### 0  -  Alive / 1 - Death\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_dat_dia = y\nheart_dat_n_2 = (x - x.mean()) / (x.std())              # standardization\ndata = pd.concat([y,heart_dat_n_2.iloc[:,0:12]],axis=1)\ndata = pd.melt(data,id_vars=\"DEATH_EVENT\",\n                    var_name=\"features\",\n                    value_name='value') \nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"DEATH_EVENT\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that *age, creatine_phosphokinase, ejection_fraction, serum_creatine, serum_sodium, and time* all appear to be seperated enough to be useful in our calculations. This is just face value interpretation and must be explored further.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"DEATH_EVENT\", data=data)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see *age and time* seem to be seperated the best out of all of the features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_1 = heart_dat.drop(['anaemia',\n       'diabetes', 'high_blood_pressure', 'sex', 'smoking', 'age', 'creatinine_phosphokinase','ejection_fraction'],axis=1)\npd.plotting.scatter_matrix(plot_1, c=y, figsize=(15, 15),\n marker='o', hist_kwds={'bins': 20}, s=60,\n alpha=.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_2 = heart_dat.drop(['anaemia',\n       'diabetes', 'high_blood_pressure', 'sex', 'smoking','platelets', 'serum_creatinine', 'serum_sodium','time'],axis=1)\npd.plotting.scatter_matrix(plot_2, c=y, figsize=(15, 15),\n marker='o', hist_kwds={'bins': 20}, s=60,\n alpha=.8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recursive feature elimination with cross validation and random forest classification\n\nHere I created a Random Forest Classifier using all of the features to start with.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a Random Forest Classifier with all features\n\nX_train, X_test, y_train, y_test = train_test_split(\n x, y, test_size=0.2, random_state=10110)\n\n#random forest classifier with n_estimators=10\nclf_rf = RandomForestClassifier(random_state=10110)      \nclr_rf = clf_rf.fit(X_train,y_train)\n\nac = accuracy_score(y_test,clf_rf.predict(X_test))\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(y_test,clf_rf.predict(X_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we implement the recursive feature elimination with cross validation to determine the optimal number of features to be used in our random forest classifier and which features should be used.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# The \"accuracy\" scoring is proportional to the number of correct classifications\nclf_rf_2 = RandomForestClassifier(random_state=10110, n_estimators=30) \nrfecv = RFECV(estimator=clf_rf_2, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\nrfecv = rfecv.fit(X_train, y_train)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', X_train.columns[rfecv.support_])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score of number of selected features\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I created another model using the determined 4 best features and we can see that the model performed just as well with an accuracy of 86.6%.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a Random Forest Classifier with the 4 best features and n_estimators = 30\n\n# y includes our labels and x includes our features\ndlist = ['DEATH_EVENT', 'age', 'anaemia', 'creatinine_phosphokinase', 'diabetes',\n        'high_blood_pressure','serum_sodium', 'sex', 'smoking']\nx_2 = heart_dat.drop(dlist,axis=1)\ny_2 = heart_dat['DEATH_EVENT']\n\nX_train, X_test, y_train, y_test = train_test_split(\n x_2, y_2, test_size=0.2, random_state=10110)\n\n#random forest classifier with n_estimators=30\nclf_rf_3 = RandomForestClassifier(random_state=10110, n_estimators=30)      \nclr_rf_3 = clf_rf_3.fit(X_train,y_train)\n\nac = accuracy_score(y_test,clf_rf_3.predict(X_test))\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(y_test,clf_rf_3.predict(X_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final Model\n\nHere we use all of the data to train the model to get the most use out of the data we are given. It is common practice to create the final model using all of the data after the training and validation has been completed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the final model, a Random Forest Classifier with the 4 best features and n_estimators = 30\n\n# y includes our labels and x includes our features\ndlist = ['DEATH_EVENT', 'age', 'anaemia', 'creatinine_phosphokinase', 'diabetes',\n        'high_blood_pressure','serum_sodium', 'sex', 'smoking']\nx_2 = heart_dat.drop(dlist,axis=1)\ny_2 = heart_dat['DEATH_EVENT']\n\n\n#random forest classifier with n_estimators=30\nclf_rf_final = RandomForestClassifier(random_state=10110, n_estimators=30)      \nclr_rf_final = clf_rf_final.fit(x_2,y_2)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}