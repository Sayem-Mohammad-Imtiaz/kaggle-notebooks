{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the csv file\ndata = pd.read_csv('/kaggle/input/pakistans-largest-ecommerce-dataset/Pakistan Largest Ecommerce Dataset.csv')\ndf = data.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 1: Data Pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for missing / NaN values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Doing a visual inspection of all columns\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observations\n- Out of 26 columns, last 5 columns in the dataset contain NaN values for all records\n- Records at 464051 indices (from the bottom) contain NaN values for all columns\n- ' MV ' is an ambiguous column name with extra spaces\n- Some of the columns have incorrect data types\n\n##### Actions\n- Last 5 columns need to be dropped from the dataset\n- 464051 rows, containing NaN values need to be dropped from the dataset\n- Renamed the columns ' MV ' and 'category_name_1' to 'MV' and 'category_name'"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([\"Unnamed: 21\", \"Unnamed: 22\", \"Unnamed: 23\", \"Unnamed: 24\", \"Unnamed: 25\"], axis = 1, inplace=True)\ndf.dropna(subset=[\"item_id\"], axis=0, inplace=True)\ndf.rename(columns={\" MV \": \"MV\", \"category_name_1\": \"category_name\"}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Dropping duplicate entries, if any, from the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Basic data quality and integrity checks"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The number of rows with negative or zero Quantity:\",sum(n <= 0 for n in df.qty_ordered))\nprint(\"The number of rows with negative Price:\",sum(n < 0 for n in df.price))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Convert all values in 'sku' column to upper case for uniformity"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sku']=df['sku'].str.upper()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exploring all columns, finding and Imputing Null Values\n#### Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['status'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observations\n- There are a lot of labels for 'status' column.\n- Need to check if any relationship exists between 'status' and 'BI Status' columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('BI Status')['status'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observations\n- All transactions marked as either **'complete' or 'closed'**, fall in the **'Net' category** for 'BI Status'\n- All transactions marked as **'received','paid','cod','exchanged' or something related to refund** are marked in **'Valid' category**\n- All transactions marked as **either 'canceled' or something to do with incomplete transation** are marked in **'Gross' category**\n- '#REF!' looks an erroneus label.\n\n##### Actions\n**Replace values inside the 'status' column by creating new labels**\n\n- **'complete','closed','received','paid','cod'** will belong to category **'Completed'**\n- **'order_refunded','refund', 'exchange'** will belong to category **'Refund'**\n- **'pending','payment_review','processing','holded','pending_paypal','\\N'** will beling to **'Pending'**\n- **'canceled'** will belong to **'Cancelled'**\n- **'fraud'** will belong to **'Fraud'**\n**Also replace the '#REF!'' entry to 'Net' in 'BI status'**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['status'] = df['status'].replace('complete', 'Completed')\ndf['status'] = df['status'].replace('closed', 'Completed')\ndf['status'] = df['status'].replace('received', 'Completed')\ndf['status'] = df['status'].replace('paid', 'Completed')\ndf['status'] = df['status'].replace('cod', 'Completed')\ndf['status'] = df['status'].replace('order_refunded', 'Refund')\ndf['status'] = df['status'].replace('refund', 'Refund')\ndf['status'] = df['status'].replace('exchange', 'Refund')\ndf['status'] = df['status'].replace('pending', 'Pending')\ndf['status'] = df['status'].replace('payment_review', 'Pending')\ndf['status'] = df['status'].replace('processing', 'Pending')\ndf['status'] = df['status'].replace('holded', 'Pending')\ndf['status'] = df['status'].replace('pending_paypal', 'Pending')\ndf['status'] = df['status'].replace(r'\\\\N', 'Pending', regex=True)\ndf['status'] = df['status'].replace('fraud', 'Fraud')\ndf['status'] = df['status'].replace('canceled', 'Cancelled')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['status'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['BI Status'] = df['BI Status'].replace('#REF!', 'Net')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['BI Status'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Handling Null values in 'status' column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['status'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observation\n- 15 NaN values in 'status' column have 'Gross' in the BI column meaning all these transactions are not valid\n\n##### Actions\n- Replacing NaN values with label **'Cancelled'** in line with our understanding of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['status'].fillna(\"Cancelled\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Handling NaN values in 'category_name' column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['category_name'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observations\n- There are 164 NaN values in the **'category_name'** column that can be filled using some information from **'sku'** column. Not doing it right now\n- 7850 transactions have a unicode label associated with them.\n- 164 transactions have NaN values.\n\n##### Actions\n- Replacing the unicode label and NaN values with label 'Unknown'"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['category_name'] = df['category_name'].replace(r'\\\\N', 'Unknown', regex=True)\ndf['category_name'].fillna(\"Unknown\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Handling NaN values in 'sku' column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['sku'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Obsevations\n- 20 NaN values for **'sku'** exist in the dataset and these values can be replaced.\n\n##### Action\n- Replace NaN values with a new sku code **'Missing'**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sku'].fillna(\"Missing\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Handling missing values in 'Sales_commission_code' column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sales_commission_code'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['sales_commission_code'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observations\n- The column has a large number of NaN values and there are more than 7000 types of values in this column\n- The column does not seem to add any value for further analysis and can be dropped at a later stage\n- At this stage, NaN values as well as unicode labels can be replaced with 'Missing'\n\n##### Actions\n- Replacing NaN and unicode values with **'Missing'**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sales_commission_code'].fillna(\"Missing\",inplace=True)\ndf['sales_commission_code'] = df['sales_commission_code'].replace(r'\\\\N', 'Missing', regex=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Handling missing values in 'Customer ID' and 'Customer Since' columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['Customer ID'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observations\n- There are a total of 11 rows where the 'Customer ID' column is NaN and exactly the same rows in 'Customer since' are also NaN, which makes sense and shows that these columns have a relationship.\n- All 11 records are from FY18, with the first record from 01-2018.\n- For keeping the records in dataset for analysis, a fake 'Customer ID' value of '0' can be assigned with '01-2018' assigned to all records in 'Customer Since' column\n\n##### Actions\n- Replaced 'Customer ID' with value **'0'** and 'Customer Since' with value **'01-2018'** for all NaN values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Customer ID'].fillna(\"0\",inplace=True)\ndf['Customer Since'].fillna(\"1-2018\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking for Null values again and setting appropriate datatypes"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Convert the datatypes of columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[[\"item_id\"]] = df[[\"item_id\"]].astype(\"str\")\ndf[[\"Month\"]] = df[[\"Month\"]].astype(\"int\")\ndf[[\"Year\"]] = df[[\"Year\"]].astype(\"int\")\ndf['created_at'] = pd.to_datetime(df['created_at'])\ndf[[\"qty_ordered\"]] = df[[\"qty_ordered\"]].astype(\"int\")\ndf[[\"Customer ID\"]] = df[[\"Customer ID\"]].astype(\"str\")\ndf[[\"increment_id\"]] = df[[\"increment_id\"]].astype(\"str\")\n\n## creating new columns to drill down the time dimension\ndf['day_of_week'] = df['created_at'].dt.dayofweek # 0 = monday.\ndf['weekday_flag'] = (df['day_of_week'] // 5 != 1).astype(str)\ndf['date_of_month'] = df['created_at'].dt.day","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"### Is there a correlation between Order Status and Payment Methods\n\n#### From the notebook on relationship between Order Status and Payment Methods, it was concluded that\n\n- E-commerce store users used Cash and voucher based transactions as the preferred method for FY17 and FY18 in terms of revenue generation through Completed transactions, but the cod payments %age saw a downward trend in FY18\n- Digital or E-payment methods were mainly responsible for making the revenue earned in FY18 double than it was in FY17. However, due to a large number of cancelled transactions associated with these methods, there is a strong possibility that the web portal faced integration challenges and resulted in many cancelled transactions\n- Digital / E-payment have been a driver in revenue growth but at the same time resulted in more cancellations and potential revenue lost."},{"metadata":{},"cell_type":"markdown","source":"#### Let's try and reduce some of the column labels for both columns, especially labels having few entries so see if some kind of relationship / correlation can be explored"},{"metadata":{},"cell_type":"markdown","source":"#### Exploring the 'payment_methods' feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['payment_method'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observations\n\n- **'Easypay' and 'Easypay_MA'** can be combined under the label 'Easypay'\n- **'cod' and 'cashatdoorstep'** can be combined under 'cod'\n- **'marketingexpense', 'financesettlement', 'productcredit', 'internetbanking', 'mygateway', 'mcblite', 'ublcreditcard', 'apg'** can be combined under 'Others' as all of these have very few entries in dataset\n\n##### Actions\n- Combine 'Easypay' and 'Easypay_MA'\n- Combine 'cod' and 'cashatdoorstep'\n- Combine 'marketingexpense', 'financesettlement', 'productcredit', 'internetbanking', 'mygateway', 'mcblite', 'ublcreditcard', 'apg' under 'Others'"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['payment_method'] = df['payment_method'].replace('Easypay_MA', 'Easypay')\ndf['payment_method'] = df['payment_method'].replace('cashatdoorstep', 'cod')\ndf['payment_method'] = df['payment_method'].replace(['marketingexpense','financesettlement','productcredit', 'internetbanking', 'mygateway', 'mcblite', 'ublcreditcard', 'apg'], 'Others')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['payment_method'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exploring the 'status' feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['status'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observations\n\n- 'Pending' and 'Fraud' can also be combined under 'Cancelled' as these are very less in number and do not contribute to the revenue.\n\n##### Actions\n- Combine 'Pending' and 'Fraud' under 'Cancelled'"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['status'] = df['status'].replace(['Pending','Fraud'], 'Cancelled')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['status'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Now performing Chi-squared test to examine the relationship between Order Status and Payment Method\n\nTo understand what is Chi-Squared Test and how it is used for statistical evaluation, check the link\n\nhttps://www.statisticshowto.com/probability-and-statistics/chi-square/"},{"metadata":{},"cell_type":"markdown","source":"#### Chi-Squared Test\n\nIf Statistic >= Critical Value: significant result, categorical variables are dependent. If Statistic < Critical Value: not significant result, categorical variables are independent."},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.stats as stats\nimport plotly.express as px\ndf1 = pd.crosstab(df['payment_method'], df['status'])\nobserved = df1.values\nval=stats.chi2_contingency(df1)\nexpected = val[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(df1, x=df1.index.values, y=df1.columns.values)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import chi2\nchi_square=sum([(o-e)**2./e for o,e in zip(observed,expected)])\nchi_square_statistic=chi_square[0]+chi_square[1]\n\n# Specifying alpha as 0.05 or p-value criteria as 95%\nalpha = 0.05\nno_of_rows=df1.shape[0]\nno_of_columns=df1.shape[1]\nddof=(no_of_rows-1)*(no_of_columns-1)\n\ncritical_value=chi2.ppf(q=1-alpha,df=ddof)\np_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if chi_square_statistic>=critical_value:\n    print(\"There is a relationship between Payment Method and Order Status\")\nelse:\n    print(\"There is no relationship between Payment Method and Order Status\")\n    \nif p_value<=alpha:\n    print(\"There is a relationship between Payment Method and Order Status\")\nelse:\n    print(\"There is no relationship between Payment Method and Order Status\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observations\n- Both parameters for chi-squared test result validation indicate that there is a statistical relationship between Payment Method and Order Status\n- However, still we cannot say anything in terms of any quantitative measure that how strong is the correlation between Payment Method and Order Status. As both are categorical variables, so Pearson's correlation coefficient cannot be used. \n- A Python library dython gives a set of data analysis tools that calculates categorical-categorical relationship between features and can be used in this case to provide an answer to our question. The link to the library and associated documentation can be seen here\n\nhttps://pypi.org/project/dython/#description  "},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install dython","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from dython.nominal import associations\n\ndf=df[['status','category_name','payment_method']]\nassociations(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion\n\n- The result in the heatmap above has been plotted using 3 categorical columns from the dataset. The library uses Cramer's V or Cramer's phi as the underlying measure which gives a measure of association between the categorical variables. Details on this statistical measure can be read on the following link\nhttps://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V\n- The heatmap above gives a value of **0.40** between **Payment Method** and **Order Status** where 0 corresponds to no association between the variables and 1 corresponds to complete association.\n- The line plot for the contingency table (crosstab) also validates the heatmap result as a clear trend can be seen between few labels but not so clear trend between other labels.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}