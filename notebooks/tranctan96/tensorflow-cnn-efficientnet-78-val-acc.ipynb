{"cells":[{"metadata":{},"cell_type":"markdown","source":"# GTZAN - Baseline CNN and Transfer learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport glob\nimport os\nimport PIL\nimport tensorflow as tf\n\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.applications import EfficientNetB0, EfficientNetB3, MobileNetV2, InceptionV3\nfrom tensorflow.keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_data = '../input/gtzan-dataset-music-genre-classification/Data/images_original/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Loader"},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE=8\nTARGET_SIZE=224 # Based on EfficientNetB0\nNUM_CLASSES=10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds = image_dataset_from_directory(\n  img_data,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  image_size=(TARGET_SIZE, TARGET_SIZE),\n  batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_ds = image_dataset_from_directory(\n  img_data,\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=123,\n  image_size=(TARGET_SIZE, TARGET_SIZE),\n  batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names = train_ds.class_names\nprint(class_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 20))\nfor images, labels in train_ds.take(1):\n    for i in range(8):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Callbacks and Helper Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_save = tf.keras.callbacks.ModelCheckpoint('./best_weights.h5', \n                             save_best_only = True, \n                             save_weights_only = True,\n                             monitor = 'val_loss', \n                             mode = 'min', verbose = 1)\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', min_delta = 0.001, \n                           patience = 10, mode = 'min', verbose = 1,\n                           restore_best_weights = True)\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.3, \n                              patience = 2, min_delta = 0.001, \n                              mode = 'min', verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_hist(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    plt.figure(figsize=(10, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(acc, label='Training Accuracy')\n    plt.plot(val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.title('Training and Validation Accuracy')\n    plt.grid()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(loss, label='Training Loss')\n    plt.plot(val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.title('Training and Validation Loss')\n    plt.grid()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential([\n  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(TARGET_SIZE, TARGET_SIZE, 3)),\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(NUM_CLASSES)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=Adam(lr = 0.001),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs=30\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs,\n  callbacks=[model_save, early_stop, reduce_lr],\n  verbose=2\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hist(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CNN with Dropout"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential([\n  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(TARGET_SIZE, TARGET_SIZE, 3)),\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Dropout(0.2),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(NUM_CLASSES)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=Adam(lr = 0.001),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 30\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs,\n  callbacks=[model_save, early_stop, reduce_lr],\n  verbose=2\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hist(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EfficientNet train from scratch"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    conv_base = EfficientNetB0(include_top = False, weights = None,\n                               input_shape = (TARGET_SIZE, TARGET_SIZE, 3))\n    model = conv_base.output\n    model = layers.GlobalAveragePooling2D()(model)\n    model = layers.Dense(NUM_CLASSES, activation = \"softmax\")(model)\n    model = models.Model(conv_base.input, model)\n\n    model.compile(optimizer = Adam(lr = 0.001),\n                  loss = \"sparse_categorical_crossentropy\",\n                  metrics = [\"accuracy\"])\n    return model\nmodel = create_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 30\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs,\n  callbacks=[model_save, early_stop, reduce_lr],\n  verbose=2\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hist(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Transfer learning - EfficientNet (mostly retrain)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    conv_base = EfficientNetB0(include_top = False, weights = \"imagenet\", drop_connect_rate=0.6,\n                               input_shape = (TARGET_SIZE, TARGET_SIZE, 3))\n    model = conv_base.output\n    model = layers.GlobalAveragePooling2D()(model)\n    model = layers.Dense(NUM_CLASSES, activation = \"softmax\")(model)\n    model = models.Model(conv_base.input, model)\n\n    model.compile(optimizer = Adam(lr = 0.001),\n                  loss = \"sparse_categorical_crossentropy\",\n                  metrics = [\"accuracy\"])\n    return model\nmodel = create_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 30\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs,\n  callbacks=[model_save, early_stop, reduce_lr],\n  verbose=2\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hist(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Transfer learning - EfficientNet (the usual way of transfer learning)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    conv_base = EfficientNetB0(include_top = False, weights = \"imagenet\", drop_connect_rate=0.6,\n                               input_shape = (TARGET_SIZE, TARGET_SIZE, 3))\n    # Freeze pre-trained layers\n    conv_base.trainable = False\n    \n    # Re-build top layers\n    model = conv_base.output\n    model = layers.GlobalAveragePooling2D()(model)\n    model = layers.BatchNormalization()(model)\n    \n    dropout_rate=0.2\n    model = layers.Dropout(dropout_rate, name=\"top_dropout\")(model)\n    model = layers.Dense(NUM_CLASSES, activation = \"softmax\")(model)\n    model = models.Model(conv_base.input, model)\n\n    model.compile(optimizer = Adam(lr = 0.01),\n                  loss = \"sparse_categorical_crossentropy\",\n                  metrics = [\"accuracy\"])\n    return model\n\nmodel = create_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 30\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs,\n  callbacks=[model_save, early_stop, reduce_lr],\n  verbose=2\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hist(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights_path = './first_finetune_weights.h5'\nmodel.save_weights(weights_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Unfreeze some of the layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_UNFREEZE_LAYERS = 100\n\ncont_model = tf.keras.models.clone_model(model)\ncont_model.load_weights(weights_path)\n\ndef unfreeze_model(model):\n    # We unfreeze the top NUM_UNFREEZE_LAYERS layers while leaving BatchNorm layers frozen\n    for layer in model.layers[-NUM_UNFREEZE_LAYERS:]:\n        if not isinstance(layer, layers.BatchNormalization):\n            layer.trainable = True\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n    model.compile(\n        optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", \n        metrics=[\"accuracy\"]\n    )\n\n\nunfreeze_model(cont_model)\ncont_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 30  # @param {type: \"slider\", min:8, max:50}\nhistory = cont_model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs,\n  callbacks=[model_save, early_stop, reduce_lr],\n  verbose=2\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hist(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Unfreeze all the layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_model = tf.keras.models.clone_model(model)\ncont_model.load_weights(weights_path)\n\ndef unfreeze_whole_model(model):\n    # We unfreeze the whole layers while leaving BatchNorm layers frozen\n    for layer in model.layers:\n        if not isinstance(layer, layers.BatchNormalization):\n            layer.trainable = True\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n    model.compile(\n        optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", \n        metrics=[\"accuracy\"]\n    )\n\n\nunfreeze_whole_model(cont_model)\ncont_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 30  # @param {type: \"slider\", min:8, max:50}\nhistory = cont_model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs,\n  callbacks=[model_save, early_stop, reduce_lr],\n  verbose=2\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hist(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Observations\n- Results:\n    - Baseline CNN - 0.5 val\n    - CNN with Dropout - 0.6 val\n    - EfficientNetB0 transfer learning - 0.78 val\n- All models show overfitting, this may be due to the lack of training data for each class.\n- Ordinary data augmentation may not be feasible for song data like GTZAN, because:\n    - Cannot use typical transformations like rotation, zoom, flipping because spectrogram would be non-sense\n    - Cannot use audio transformation because this will distort the original song.\n    - Solution: Research specific methods of data augmentation for song data.\n- Why usual way of transfer learning does not perform better than fine-tuning the whole EfficientNet model?"},{"metadata":{},"cell_type":"markdown","source":"### Reference\n- https://www.tensorflow.org/tutorials/images/classification\n- https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}