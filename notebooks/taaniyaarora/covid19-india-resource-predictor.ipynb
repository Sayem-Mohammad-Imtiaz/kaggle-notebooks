{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID-19 India : Intelligent Resource Predictor\n\n### Finding gap between the requirement of beds for Covid care and their availability in every state's medical facility.\n\n# Flow \n1. Predict the estimated No. of covid cases on a day for a state.\n2. Derive the number of cases that require hospitalization out of the total count obtained in the previous step.\n3. Find the gap between the figure obtained above and from the recent data of vacant beds for a state.\n\n# Contents\n1. Datasets\n2. Data Preparation - Train/test\n3. Model Training\n4. Predicting the number of COVID cases 7 days from present day\n5. Backtesting\n6. Model Evaluation\n7. Finding No. of patients to be hospitalised\n8. Finding Gap in availabity of No. of beds for COVID cases\n\n### Training data -\nData span - 2nd April to 15th May\nUsing past 30 days' data, predict estimated postive COVID cases for 7th day from a present day.\n\n### Test data -\nUsing 30 day rolling window of past data from 15th April onwards, predict count of positive cases for 23rd May to 4th June for each state in India.\n\n\n### Assumptions for deriving requirement of bed resources\nThe distribution of demographics w.r.t age of positive COVID cases -\n\nCategory 1 - 25% - young children \n\nCategory 2 - 45% - Youth/Middle aged\n\nCategory 3 - 30% - Senior  Citizens\n\nWe include the 1st and 3rd category (65% of total positive cases) as cases that require hospitalization with priority over the 2nd category.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nnp.random.seed(1)\n\nimport tensorflow as tf\ntf.random.set_seed(2)\n\nimport math\nfrom datetime import datetime\nimport pandas as pd\nimport time\nimport warnings\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings('ignore')\nimport os\n\nfrom tensorflow import keras\nfrom tensorflow.keras import backend\nfrom tensorflow.keras.models import Sequential,load_model\nfrom tensorflow.keras.layers import Dense,LSTM,Dropout,Flatten\nfrom tensorflow.keras import optimizers\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features in the dataset below are at daily level derived from the COVID19 dataset containing cumulative statewise counts of new, recovered & deceased cases.\n","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"covid_features = pd.read_csv(\"/kaggle/input/features-covid/statewise_features.csv\")\nbeds_dataset = pd.read_csv(\"/kaggle/input/covid19-in-india/HospitalBedsIndia.csv\", index_col=0)\ncontainment_zones = pd.read_csv(\"/kaggle/input/district-containment-zones/district-containment-zones-2020-04-30.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_features['Date'] = pd.to_datetime(covid_features['Date'], format=\"%Y-%m-%d\")\n\ncovid_features['Date'].min(), covid_features['Date'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_features.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_features['State'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# No. of steps for LSTM to look back for making predictions\n\ntimesteps = 30","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def transformDF(df,state,date_start,date_end,date_col=\"Date\"):\n    # fetch selective columns\n    df_columns = [\"State\",date_col,\"scaled_pop_density\",\"new_cases\",\"samples_tested\"]\n    df = df[df_columns]\n    state_df = df[df['State'] == state]\n    state_df[date_col] = pd.to_datetime(state_df[date_col])\n    # Generate valid dates in the specified data range\n    all_dates_df = pd.DataFrame(pd.date_range(start=date_start,end=date_end,freq='D'),columns=['date'])\n    all_days_state_df = all_dates_df.merge(state_df,\n                                                how='left',\n                                                left_on='date',\n                                                right_on=date_col)\n    pop_density = all_days_state_df[~all_days_state_df['scaled_pop_density'].isnull()][\"scaled_pop_density\"].unique()[0]\n    values = {\"State\":state,\n             \"new_cases\":0,\n             \"samples_tested\":0,\n             \"scaled_pop_density\":pop_density}\n    final_df = all_days_state_df.fillna(value=values)\\\n                               .drop(date_col,axis=1)\n    return final_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getPreparedData(timesteps,df):\n    # Preparing the data\n    input_data = []              # historic input values \n    actual_data = []             # actual values at current time instance as target values\n    scalers = []  \n    N_STEPS = timesteps          # No. of timesteps back in time that the LSTM model sees\n    # df = df[['pop_density','new_cases','samples_tested']]\n    df = df[['scaled_pop_density','new_cases','samples_tested']]\n    lookahead_day = 7            # The Nth day in future for which to make prediction\n    nfeatures = len(df.columns)\n    \n    feature_index = 1            #  index of feature - new_cases\n    ## At each prediction timestep, we provide values from n_steps historical timesteps\n    for i in range(N_STEPS,len(df) - lookahead_day):  \n        historical_x_val = df.values[i-N_STEPS:i]\n        y_val = df.values[i+lookahead_day][feature_index]\n        minmax_scaler = MinMaxScaler(feature_range=(0,1))\n        scaled_x = historical_x_val[:]   \n        \n        # 1st feature is scaled already. Skipping this feature in scaling step below\n        scaled_x[:,1:] = minmax_scaler.fit_transform(historical_x_val[:,1:])     \n        \n        # Scaling target values\n        scaling_shape = (1, nfeatures - 1)                        # subtract 1 from num features scaled\n        reshaped_y = np.broadcast_to(y_val,scaling_shape)         # Make its shape compatible for minmax scale transformation\n        scaled_y = max(0, minmax_scaler.transform(reshaped_y)[0][0])\n    \n        input_data.append(scaled_x) \n        actual_data.append(scaled_y)\n        scalers.append(minmax_scaler)\n    train_x = np.array(input_data)\n    train_y = np.array(actual_data)\n    assert len(train_x.shape) == 3 , \"x_train is expected to be 3-Dimensional for LSTM\"\n    return train_x,train_y,scalers\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### Training data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling population density using data from all states\ndensity_scaler = MinMaxScaler((0,1))\ncovid_features['scaled_pop_density'] = density_scaler.fit_transform(covid_features['pop_density'].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use data from 1st April to 15th May for training\n# Combining data from multiple state to prepare training dataset 30 days long\n\nstart_time = time.time()\ntrainX = trainY = TrainScalers = None\n\nfor i,state in enumerate(covid_features['State'].unique()):\n    state_df = transformDF(covid_features,state,date_start='2020-04-02',date_end='2020-05-15',date_col=\"Date\")\n    # print(state_df)\n    train_x,train_y,train_scaler = getPreparedData(timesteps=timesteps\\\n                                                 ,df=state_df)\n    \n    if trainX is None:\n            trainX = train_x\n            trainY = train_y\n            TrainScalers = train_scaler\n\n    else:\n        trainX = np.vstack((trainX,train_x))\n        trainY = np.hstack((trainY,train_y))\n        TrainScalers = TrainScalers + train_scaler\n\nprint(\"Training data prepared in {} seconds\".format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainX.shape, trainY.shape, len(TrainScalers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use data from 1st April to 15th May for training\n# Combining data from multiple states to prepare training dataset 30 days long\n\nnrecords = 13   # No. of test data points per state\nstart_time = time.time()\ntestX  = testY = testScalers = testStateDf = target_y = None\n\nfor i,state in enumerate(covid_features['State'].unique()):\n    state_df = transformDF(covid_features,state,date_start='2020-04-15',date_end='2020-06-03',date_col=\"Date\")\n    \n    test_X,test_Y,test_scalers = getPreparedData(timesteps=timesteps\\\n                                                 ,df=state_df)\n    target_Y = state_df['new_cases'].values[-nrecords: ]\n    if testX is None:\n        testX = test_X\n        testY = test_Y\n        testScalers = test_scalers \n        target_y = target_Y\n    else:\n        testX = np.vstack((testX,test_X))\n        testY = np.hstack((testY,test_Y))\n        testScalers = testScalers + test_scalers\n        testStateDf = pd.concat([testStateDf,state_df])\n        target_y = np.hstack((target_y,target_Y))\n        \nprint(\"Test data prepared in {} seconds\".format(time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With data span from 15th April to 14th May, considering 30 back time steps, and to predict for 7th day from 15th May onwards,we will test predictions from 23rd May to 4th June. This way, we have 13 test data points for each of the 35 states.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Model training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bs = 16\nepoch = 400\nlr = 0.001\nlstm_dropout = 0.2\nlstm_nodes = 8\noptimizer = keras.optimizers.Adam(learning_rate=lr)\nnum_features = 3     # no. of positive cases, total samples tested on a day & population density of state\ntraining_loss = tf.keras.losses.MAE\n\ncheckpoint_path = \"./covid_adam_tanh_relu_checkpoint\"\nlogdir = \"./logs/covid_adam_tanh_relu\" + datetime.now().strftime(format=\"&Y%m%d-%H-%M-%S\")\n\n# for visualizing training metrics on tensorboard\ntensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir,update_freq='epoch')\ncheckpoint_callback = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                        verbose=2,\n                                                        period=20,\n                                                        save_best_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Setting up the forecasting model using LSTM\nmodel = Sequential()\nmodel.add(LSTM(units=lstm_nodes, input_shape=(timesteps, num_features), return_sequences=True,dropout=0.0))\nmodel.add(LSTM(units=lstm_nodes, return_sequences=False,dropout=0.0))\nmodel.add(Dropout(rate=lstm_dropout))\nmodel.add(Dense(units=20,activation='tanh',name=\"dense_1\"))\nmodel.add(Dense(units=1,activation='relu',name=\"dense_2_output\"))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=optimizer,loss=training_loss,metrics=['mae','mse'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time history = model.fit(x=trainX\\\n                        ,y=trainY\\\n                        ,batch_size=bs\\\n                        ,epochs=epoch\\\n                        ,verbose=2\\\n                        ,callbacks=[tensorboard_callback,checkpoint_callback]\\\n                        ,shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting loss function during training phase\n\nplt.plot(history.history['loss'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting the number of COVID cases 7 days from present day","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"training_orginal_scaled_predictions = []\ntraining_predictions = model.predict(trainX,batch_size=bs)\n\n# Making shape compatible for scaling back into original range\ntrain_size = trainX.shape[0]\nreshaped_tr_predictions = np.broadcast_to(training_predictions, (train_size, num_features - 1))\nreshaped_tr_predictions[:10]\n\n# scaling the predictions back to orginal scale\nfor i in range(len(trainX)):\n    training_inverse_scaled_predictions = TrainScalers[i].inverse_transform(reshaped_tr_predictions[i].reshape(1,-1))[:,0]\n    training_orginal_scaled_predictions.append(training_inverse_scaled_predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Backtesting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_inverse_scaled_predictions = []\ntest_original_scaled_predictions = []\n\ntest_predictions = model.predict(testX, batch_size=bs)\n\ntest_size = testX.shape[0]\nreshaped_test_predictions = np.broadcast_to(test_predictions, (test_size, num_features - 1 ))\n\n# scaling the predictions back to orginal scale\nfor i in range(len(testScalers)):\n    prediction = testScalers[i].inverse_transform(reshaped_test_predictions[i].reshape(1,-1))[:,0]\n    test_original_scaled_predictions.append(prediction)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rounding off the actual and predicted values upto 3 decimal places","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"target_y = np.round(target_y,0)\npredictions_all_states = np.round(test_original_scaled_predictions,0)\npredictions_all_states[predictions_all_states < 0] = 0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of dates for which the predictions are made\n\ndates = covid_features.sort_values('Date')['Date'].astype(str).unique()[-nrecords:].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_df = None\n\nindex = 0\nstates = covid_features['State'].unique()\nfor i in range(0,len(testScalers),nrecords):\n    test_state_df = pd.DataFrame(predictions_all_states[i:i+nrecords], columns=['predictions'])\n    test_state_df['State'] = states[index]\n    test_state_df['Date'] = dates\n    test_state_df['target_values'] = target_y[i:i+nrecords]\n    test_state_df[[\"predictions\",\"target_values\"]].plot(kind='bar',figsize=(6,4),grid=True)\n    plt.xlabel(\"State: {}\".format(states[index]))\n    plt.ylabel(\"No. of Positive COVID cases \")\n    plt.xticks(range(nrecords),dates)\n    index += 1\n    predictions_df = pd.concat([predictions_df,test_state_df])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"High deviation between model predictions and actual count of confirmed cases from above visualizations for states Nagaland, Punjaba and Andaman and Nicobar Islands can be justified by the past trend of counts in these states","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_features.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_df['Date'] = pd.to_datetime(predictions_df['Date'], format=\"%Y-%m-%d\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combining predictions with original features\n\ncovid_features['Date'] = covid_features.Date.astype(str)\npredictions_df['Date'] = predictions_df.Date.astype(str)\nfeatures_with_predictions = covid_features.merge(predictions_df[['predictions','Date','State']], \n                     how='left', \n                    on=['Date','State'])\nfeatures_with_predictions.drop(\"scaled_pop_density\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state = \"Nagaland\"\nplt.figure(figsize=(10,4))\nfeatures_with_predictions[features_with_predictions['State']==state].set_index('Date')[['new_cases','predictions']][-30:].plot(kind=\"bar\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state = \"Punjab\"\nplt.figure(figsize=(10,4))\nfeatures_with_predictions[features_with_predictions['State']==state].set_index('Date')[['new_cases','predictions']][-30:].plot(kind=\"bar\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state = \"Andaman and Nicobar Islands\"\nplt.figure(figsize=(10,4))\nfeatures_with_predictions[features_with_predictions['State']==state].set_index('Date')[['new_cases','predictions']][-30:].plot(kind=\"bar\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean absolute error\n\nmae = mean_absolute_error(target_y, predictions_all_states)\nmae","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean squared error\nmse = mean_squared_error(target_y, predictions_all_states)\nrmse = math.sqrt(mse)\nrmse\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Number of patients to be hospitalized","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Assuming 65% of the predicted positive cases require hospitalization. This group of patients may belong to the population of senior citizens or poeple with pre-morbid health conditions.\nThen the total number of hospitalizations that is expected to be done is - ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"statewise_hosp_requirement = predictions_df[['Date','State','predictions']]\nstatewise_hosp_requirement.columns = ['Date', 'State', 'predicted_case_count']\nstatewise_hosp_requirement['num_hosp_required'] = np.round(statewise_hosp_requirement['predicted_case_count']*0.65, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"statewise_hosp_requirement = statewise_hosp_requirement.groupby('State').sum().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"statewise_hosp_requirement.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gap in availabity of No. of beds for COVID cases","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"beds_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assuming 5% of total Beds are reserved for COVID care\n# Assuming only 2% of those beds are vacant for hospitalizing newly diagnozed patients\n# The available bed count is - \n\nbeds_dataset['total_beds'] = beds_dataset['NumPublicBeds_HMIS'] + beds_dataset['NumRuralBeds_NHP18'] + beds_dataset['NumUrbanBeds_NHP18']\nbeds_dataset['beds_for_covid'] = np.round(beds_dataset['total_beds']*0.05, 0)\n# beds_dataset['vacant_beds_for_covid'] = np.round(beds_dataset['beds_for_covid']*0.02, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_beds_data = beds_dataset[['State/UT','beds_for_covid']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data cleaning for consistent data before joining\n\ncovid_beds_data['State/UT'] = covid_beds_data['State/UT'].str.replace('&', 'and')\ncovid_beds_data.loc[covid_beds_data['State/UT'].str.contains('Diu'), 'State/UT'] = \"Dadra and Nagar Haveli and Daman and Diu\"\ncovid_beds_data.loc[covid_beds_data['State/UT'].str.contains('Dadra'), 'State/UT'] = \"Dadra and Nagar Haveli and Daman and Diu\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_beds_data = covid_beds_data.groupby('State/UT').sum().reset_index()\ncovid_beds_data.columns = ['State', 'beds_for_covid']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merging the details of requirements per state and available hospital beds","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"statewise_avail_req = statewise_hosp_requirement.merge(covid_beds_data, \n                                on='State',\n                                how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"statewise_avail_req.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For 13 days of span, the difference between the beds availaible for COVID and total No. of hospitalizations required will give us the gap in bed resource. A positive value of the difference denotes the shortage of the resource.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"statewise_avail_req['gap_in_req_beds'] = statewise_avail_req['num_hosp_required'] - statewise_avail_req['beds_for_covid']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Top 4 states falling short of bed resources","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"statewise_avail_req.sort_values('gap_in_req_beds', ascending=False).head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}