{"cells":[{"metadata":{"_uuid":"2a3a729e04e1e54ff461e1ce7ee78670785d0a48"},"cell_type":"markdown","source":"# MACHINE LEARNING\n\nThis tutorial created for observe the result of algorithms."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#import library\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"../input\"))\nimport pandas as pd\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#load dataset\ndata = pd.read_csv('../input/indian_liver_patient.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"beaf78cd162c667ec84c47c7a55993ab4ec32aed"},"cell_type":"code","source":"#view dataset\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cbe4b2180c4a3e0c5cfde34d166f5e3ac25b62d"},"cell_type":"code","source":"#visualization age-albumin and globulin ratio\nplt.scatter(data.Age, data.Albumin_and_Globulin_Ratio, color = 'navy',alpha = 0.3)\nplt.xlabel('Age')\nplt.ylabel('Albumin_and_Globulin_Ratio')\nplt.title('Visualization for Age-Albumin_and_Globulin_Ratio')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5b7951aa5baaeaa6841da170f04a808f8adb30e"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8889b3f9087b63581233c24abd5fb85b137fe9dc"},"cell_type":"code","source":"corr = data.corr()\ncorr.style.background_gradient()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81909b7022f5077b995862557916b46f35eae7f9"},"cell_type":"markdown","source":"# Linear Regression\n\nLinear Regression is a way of predicting a response Y on the basis of a single predictor variable X. It is assumed that there is approximately a linear relationship between X and Y.\n\ny = b0 + b1*x"},{"metadata":{"trusted":true,"_uuid":"3be3802f5e6f0b7d059799b8da86d5a4f7a6f641"},"cell_type":"code","source":"# we created x and y array. So we understand very well.\ny = np.array([2,5,10,14,15,16,20,25,30,35,36,38,40,45,50,52,55,60,61,62]).reshape(-1,1)\nx = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]).reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39d8958ba4f5ff513dec32b258780c698e01688f"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlinear_reg = LinearRegression()\nlinear_reg.fit(x,y)\nb_zero = linear_reg.intercept_\nb_one = linear_reg.coef_\nprint('b_zero: {}, b_one: {}'.format(b_zero,b_one))\n#visualization\nplt.scatter(x,y)\nx_predict = linear_reg.predict(x)\nplt.plot(x,x_predict,color='red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5aa572dd9b5a03ddbcc0c8e407e821375b98d2c2"},"cell_type":"code","source":"#Now we visualization for dataset\nfrom sklearn.linear_model import LinearRegression\nlinear_reg = LinearRegression()\nx = data.Total_Bilirubin.values.reshape(-1,1)\ny = data.Direct_Bilirubin.values.reshape(-1,1)\nlinear_reg.fit(x,y)\nb_zero = linear_reg.intercept_\nb_one = linear_reg.coef_\nprint('b_zero: {}, b_one: {}'.format(b_zero,b_one))\n#visualization\nplt.scatter(x,y)\nx_predict = linear_reg.predict(x)\nplt.plot(x,x_predict,color='red')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75d4c93a011d7d460b3ace33b4b6dbc63cbe2c47"},"cell_type":"markdown","source":"# Polynomial Regression"},{"metadata":{"trusted":true,"_uuid":"a9e8a4982057302b655addf97a308e53f5c7da7a"},"cell_type":"code","source":"# This regression may involve x^2,x^3,....\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\ny = np.array([100,95,93,90,86,85,80,82,75,70,65,55,40,45]).reshape(-1,1)\nx = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14]).reshape(-1,1)\nplt.scatter(x,y)\npolynomial_regression = PolynomialFeatures(degree=4)\nx_polynomial = polynomial_regression.fit_transform(x)\nlinear_regressionn = LinearRegression()\nlinear_regressionn.fit(x_polynomial,y)\ny_head2 = linear_regressionn.predict(x_polynomial)\nplt.plot(x,y_head2,color = 'purple',label=\"ploy\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cc6b45aed7aab32d9f41d221ed9281a1ecda433"},"cell_type":"markdown","source":"# Logistic Regression\n\nLike all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables."},{"metadata":{"trusted":true,"_uuid":"dc8a4c0b7840037ca08e8c9168ad84ce1c15c266"},"cell_type":"code","source":"y = data.Dataset.values.reshape(-1,1)\nx_data = data[['Age','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase','Alamine_Aminotransferase','Aspartate_Aminotransferase','Total_Protiens','Albumin']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5aa7cfcaff30fe6273c237f1bad0e61c5d232363"},"cell_type":"code","source":"#normalization\nx = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data)).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f716041d44e85908b5613cfd6d2419c755004c1"},"cell_type":"code","source":"#train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.15,random_state=42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e6bb317d5550faea4acba99eb8b62269faf48fc"},"cell_type":"code","source":"# create initialize weights and bias values\ndef initialize_weights_and_bias(dimension):\n    #create weights\n    w = np.full((dimension,1),0.01)\n    #create bias\n    b = 0.0\n    return w,b\n#w,b = initialize_weights_and_bias(5)\n#w,b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f16bd41c17d620c6d9ddd0875a67dbbc480148e"},"cell_type":"code","source":"# create activation function\ndef sigmoid(z):\n    #sigmoid function\n    y_head = 1/(1+np.exp(-z))\n    return y_head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a78e9c1577d2b86a444d18284e118ab1a904e70f"},"cell_type":"code","source":"#create forward backward propagation\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3780e964dfaa3065b311872153163ccf4becd634"},"cell_type":"code","source":"# for values update\ndef update(w,b,x_train,y_train,learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    for i in range(number_of_iterarion):\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        \n        w = w-learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n       # if block created for visualization\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iteration %i: %f\" %(i,cost))\n            \n    parameters = {\"weight\":w, \"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index, rotation = 'vertical')\n    plt.xlabel(\"number of iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db9bd5915b48b82a359624061854e71d7afd959d"},"cell_type":"code","source":"#prediction\ndef predict(w,b,x_test):\n    t = np.dot(w.T,x_test)+b\n    z = sigmoid(t)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if z[0,i]<=0.5:\n            Y_prediction[0,i] = 2\n        else:\n            Y_prediction[0,i] = 1\n    return Y_prediction\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ccc815f6a030217ba12b878ab2887a574034a1e"},"cell_type":"code","source":"# %% logistic_regression\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  number_of_iterarion):\n    # initialize\n    dimension =  x_train.shape[0]  \n    w,b = initialize_weights_and_bias(dimension)\n    \n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,number_of_iterarion)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    \n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, number_of_iterarion = 30)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"743d846a64614519fd740be30ce15f0e736fd0dd"},"cell_type":"markdown","source":"# KNN Algorithm\n\nA case is classified by a majority vote of its neighbors, with the case being assigned to the class most common amongst its K nearest neighbors measured by a distance function. If K = 1, then the case is simply assigned to the class of its nearest neighbor. "},{"metadata":{"trusted":true,"_uuid":"b8cab73c9fbaeef355f92ef09e7715a9cb42287d"},"cell_type":"code","source":"data1 = data[['Age','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase','Alamine_Aminotransferase','Aspartate_Aminotransferase','Total_Protiens','Albumin','Dataset']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1adb941eddde56b2486752592dd855fff904a8e0"},"cell_type":"code","source":"data1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"697dfc0c8a457877a85495510f62dfd6acd0e3e1"},"cell_type":"code","source":"#patient has liver disease or not. One =liver disease, two =liver disease not.\none = data1[data1.Dataset == 1]\ntwo = data1[data1.Dataset == 2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04b37916ad08e2cbfb12c27f0b03bd283be73807"},"cell_type":"code","source":"plt.scatter(one.Age, one.Total_Protiens, color = \"purple\", label = \"one\", alpha = 0.4)\nplt.scatter(two.Age, two.Total_Protiens, color = \"orange\", label = \"two\", alpha = 0.4)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Total_Protiens\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cefcdb339d9e1ce37ff2513e18f4bb5157593a11"},"cell_type":"code","source":"y = data.Dataset.values.reshape(-1,1)\nx_data = data[['Age','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase','Alamine_Aminotransferase','Aspartate_Aminotransferase','Total_Protiens','Albumin']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bbb8d35051300ced0589786a31ffe5eab606fa1"},"cell_type":"code","source":"# normalization\nx = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))\n# (x-minx)/(maxx-minx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4379dd53a8c6e070b38c2ffcd20aebb2ed4affd4"},"cell_type":"code","source":"#train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2437c0391c3d7bb67f982f734c20a5920b28513d"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=2)\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8beb81211f5d1219a9a4271e504274b095c0e3aa"},"cell_type":"code","source":"print(\"{} nın score: {}\".format(2,knn.score(x_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fade35d24a438ee4ffbb07a35df17a1d04bc830"},"cell_type":"code","source":"score_list = []\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors=each)\n    knn2.fit(x_train, y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,15),score_list,color='red')\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57f51c41cf9169d4effcb56db3c0d50cd8e5c4de"},"cell_type":"markdown","source":"# Support Vector Machine\n\nSupport vector machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for both classification and regression.\n"},{"metadata":{"trusted":true,"_uuid":"c68e825b61b77b591e60e180490bd6b42c02453e"},"cell_type":"code","source":"y = data.Dataset.values.reshape(-1,1)\nx_data = data[['Age','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase','Alamine_Aminotransferase','Aspartate_Aminotransferase','Total_Protiens','Albumin']]\nx = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=1)\n#svm\nfrom sklearn.svm import SVC\nsvm = SVC(random_state = 1)\nsvm.fit(x_train,y_train)\nprint(\"svm algoritmasının doğruluğu: \",svm.score(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c67a871689b470198fe2396fd5bc1014979409a4"},"cell_type":"markdown","source":"# Navie Bayes Classifications"},{"metadata":{"trusted":true,"_uuid":"4f434263f9ff40c3219ea1f00e79126b42c470fa"},"cell_type":"code","source":"y = data.Dataset.values.reshape(-1,1)\nx_data = data[['Age','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase','Alamine_Aminotransferase','Aspartate_Aminotransferase','Total_Protiens','Albumin']]\n#normalization\nx = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))\n#train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=1)\n#navie bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train, y_train)\nprint(\"bayes algoritmasının sonucu: \", nb.score(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8e8400e6820c0f09dc3da68cdd9d8bc37053566"},"cell_type":"markdown","source":"# Decision Tree"},{"metadata":{"trusted":true,"_uuid":"74a397b6df5cc3b5c925c000ca22e9acd2d18b65"},"cell_type":"code","source":"y = data.Dataset.values.reshape(-1,1)\nx_data = data[['Age','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase','Alamine_Aminotransferase','Aspartate_Aminotransferase','Total_Protiens','Albumin']]\n#normalization\nx = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))\n#train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=1)\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train, y_train)\nprint(\"score: \", dt.score(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07286f5ee905f127739605f2fa2b3ed22570f3cf"},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true,"_uuid":"cf003499dd6babad307a1c1fd4189f869db5acc3"},"cell_type":"code","source":"y = data.Dataset.values.reshape(-1,1)\nx_data = data[['Age','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase','Alamine_Aminotransferase','Aspartate_Aminotransferase','Total_Protiens','Albumin']]\n#normalization\nx = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))\n#train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=1)\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 10, random_state = 1)\nrf.fit(x_train,y_train)\nprint(\"random forest result: \", rf.score(x_test,y_test))\n#confusion matrix\ny_pred = rf.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true, y_pred)\n#visualization\nf,ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"prediction values\")\nplt.ylabel(\"original values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48f048a919ec5cb1f7449b9d03014b701d45468d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}