{"cells":[{"metadata":{},"cell_type":"markdown","source":"# What you will Learn\nNLP is one of the most important areas of data science. This notebook focuses on the detection of `Fake News`. Most people tend to use deep learning approaches based on word embeddings, word sequences and other elaborate methods to detect them. In this notebook you will learn, that you can already achieve extremely good results by focusing on easily extracted features. Even though these methods are far from perfect, they are easy to perform and they allow us to get some valuable insights into the structure of Fake News."},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-danger\" role=\"alert\">\n    <h3>This notebook is work in progress. Feel free to <span style=\"color:red\">comment</span> if you have any suggestions   |   motivate me with an <span style=\"color:red\">upvote</span> if you like this project.</h3>\n</div>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import STOPWORDS\nimport nltk\nimport string","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get an overview of both CSVs and merge them together!\nThe data is provided in two separated csv files. One file contains real articles, the other file contains fake news. Let's get an idea of both files."},{"metadata":{"trusted":true},"cell_type":"code","source":"fake = pd.read_csv('../input/fake-and-real-news-dataset/Fake.csv')\nnews = pd.read_csv('../input/fake-and-real-news-dataset/True.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's take a look at Real News"},{"metadata":{"trusted":true},"cell_type":"code","source":"news","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"are there any missing values in `News.csv`?"},{"metadata":{"trusted":true},"cell_type":"code","source":"news.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"not a single column contains missing values!"},{"metadata":{},"cell_type":"markdown","source":"Which `subjects` are contained in this file?"},{"metadata":{"trusted":true},"cell_type":"code","source":"news['subject'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's take a look at Fake News"},{"metadata":{"trusted":true},"cell_type":"code","source":"fake","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"are there any missing values in `Fake.csv`?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fake.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"not a single column contains missing values!"},{"metadata":{},"cell_type":"markdown","source":"Which `subjects` are contained in this file?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fake['subject'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-> Both CSV files contain the same variables, `title`, `text`, `subject`, and `date`. We have roughly as many real news as fake news. THis might be very supportive for fututre predictions. We don't need to oversample or undersample our dataset, because the dataset is balanced w.r.t. the target. Unfortunately, none of the files contains an explicit column for the target variable `is_fake`. Let's create such a column! Afterwards we can merge them together into one file!"},{"metadata":{"trusted":true},"cell_type":"code","source":"news['is_fake'] = 0 # contains only news\nfake['is_fake'] = 1 # contains only fakes\n\n# merge them into one file\ndata = pd.concat([news, fake])\ndata = data.reset_index()\n# don't forget to shuffle them. \n# Otherwise all news are on top and all fakes are on the bottom\ndata = data.sample(frac=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For this notebook, I will ignore the `dates` and `subjects`(because they are disjoint). I will probably take a look at them in a later project. Let's drop them for now."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Are there any duplicates in the data? If yes, we would have to remove them"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(['date', 'subject'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before taking a closer look at the data, perform a `train_test_split` to evade **Data Snooping**!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(data, test_size=0.2, random_state=42)\nX_train = train.drop('is_fake', axis=1)\ny_train = train['is_fake']\nX_test = test.drop('is_fake', axis=1)\ny_test = test['is_fake']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"how are Fake News distributed among the train and the test data?"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inspect the Training data to get further insights\nTherefore we have to split the Training set into News and Fake News"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_news = X_train.loc[(y_train==0),:]\ntrain_fake = X_train.loc[(y_train==1),:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lexical Diversity of Fake News\nLet's define a measure for lexical diversity to find out how many unique vocabs are used in Fake News articles.\n\nLet's define the lexical diversity measure as $\\frac{\\text{number of unique words in one (target)category}}{\\text{number of words in both (target)categories}}$"},{"metadata":{"trusted":true},"cell_type":"code","source":"# a very simple measure for lexical diversity\ndef lexical_diversity(data, other_data, feature):\n    # ignoare capital letters\n    column = data[feature].str.lower() \n    # create one text from the column\n    text = ' '.join(column)\n    # drop punctuation\n    exclude = set(string.punctuation)\n    words = ''.join(char for char in text if char not in exclude)\n    # create a list of wordsw instead of one huge text\n    words_splitted = words.split()\n    \n    # analogously\n    other_column = other_data[feature].str.lower() \n    other_text = ' '.join(other_column)\n    other_words = ''.join(char for char in other_text if char not in exclude)\n    other_words_splitted = other_words.split()\n    # lexical diversity measure\n    return len(set(words_splitted)) / (len(words_splitted) + len(other_words_splitted))\n\nprint(f'Real News: {lexical_diversity(data=train_news, other_data=train_fake, feature=\"text\")}')\nprint(f'Fake News: {lexical_diversity(data=train_fake, other_data=train_news, feature=\"text\")}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interestingly it seems like `Fake News` contain almost two times greater lexical diversity than real news. Their authors seem to have a wide vocabulary."},{"metadata":{},"cell_type":"markdown","source":"## Most frequently used words in Titles\nis there a difference between the most frequently used words in the titles of Real News and Fake News? This question might already reveal the topics of the Fake News articles."},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = set(STOPWORDS)\n\ndef common_tokens_title(data, feature, name):\n    column = data[feature].str.lower() \n    text = ' '.join(column)\n    exclude = set(string.punctuation)\n    words = ''.join(char for char in text if char not in exclude)\n    words_splitted = words.split()\n    words_stopped = [word for word in words_splitted if not word in stopwords]\n    print(f'{name}:\\n{pd.DataFrame(nltk.FreqDist(words_stopped).most_common(10))[0]}')\n    \ncommon_tokens_title(train_news, 'title', 'Most common descriptive words in Real News Titles')\nprint('\\n')\ncommon_tokens_title(train_fake, 'title', 'Most common descriptive words in Fake News Titles')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like most Fake News Articles in our Training data are about US Presidents and candidates for the latter. Besides `trump`, the real news seem to focus on more general political topics."},{"metadata":{},"cell_type":"markdown","source":"# Punctutation\nPunctuation might provide some information for predictions. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import regex as re\n\n# let's begin with a helper function to count punctuation\ndef count_punctuation(text):\n    peri = re.subn(r\"\\.\", '', text)[1]\n    comm = re.subn(r\"\\,\", '', text)[1]\n    ques = re.subn(r\"\\?\", '', text)[1]\n    excl = re.subn(r\"\\!\", '', text)[1]\n    return [peri,comm, ques, excl]\n    \ncount_punctuation('...alph!a.beta.gamma...??')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count the usage of puntuations per row in a specified feature (title and text)\n# and store the data in a dataframe\ndef create_punctuation_df(dataset, feature):\n    return dataset.apply(lambda row: pd.Series({'peri_' + feature:count_punctuation(row[feature])[0], \n                                                       'comm_' + feature:count_punctuation(row[feature])[1],\n                                                      'ques_' + feature:count_punctuation(row[feature])[2],\n                                                      'excl_' + feature:count_punctuation(row[feature])[3]}), axis=1)\n\npunctuation_train_title = create_punctuation_df(train, 'title')\npunctuation_test_title = create_punctuation_df(test, 'title')\npunctuation_train_text = create_punctuation_df(train, 'text')\npunctuation_test_text = create_punctuation_df(test, 'text')\npunctuation_train_text ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Length"},{"metadata":{"trusted":true},"cell_type":"code","source":"# count the text length per row for both features (title and text)\n# and store the information in a dataframe\ndef create_len_df(dataset):\n    return dataset.apply(lambda row: pd.Series({'length_title':len(row['title']),\n                                               'length_text':len(row['text'])}), axis=1)\n\nlen_train = create_len_df(train)\nlen_test = create_len_df(test)\n\nlen_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Number of Words in the Title\nLet's create a feature which counts the number of words in the Title. In my **Digital Markeeting Courses** I learned, that titles are extremely important when it comes to grabbing Attention. A rule of thumbs tells us that titles should have about 5 words."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_num_words_df(dataset):\n    return dataset.apply(lambda row: pd.Series({'num_words_title':len(row['title'].split())}), axis=1)\n\nnum_words_train = create_num_words_df(train)\nnum_words_test = create_num_words_df(test)\n\nnum_words_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lenght of Title Relative to the Length of the Article + Title\nOne could assume that some attention grabbing Fake News have a long title and very short texts. Is that true?"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_title_ratio_df(dataset):\n    return dataset.apply(lambda row: pd.Series({'title_ratio':len(row['title'])/(len(row['title']) + len(row['text']))}), axis=1)\n\ntitle_ratio_train = create_title_ratio_df(train)\ntitle_ratio_test = create_title_ratio_df(test)\n\ntitle_ratio_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Concatenate the new Datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_punct_len = pd.concat([punctuation_train_title, \n                               punctuation_train_text, \n                               len_train, \n                               num_words_train, \n                               title_ratio_train], \n                              axis=1)\n\nX_test_punct_len = pd.concat([punctuation_test_title, \n                              punctuation_test_text, \n                              len_test, \n                              num_words_test, \n                              title_ratio_test], \n                             axis=1)\n\nX_train_punct_len","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Investigate the Engineered Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_punct_len.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Many Titles don't contain any form of `punctuation`!\n\n* Some texts contain suspiciously small amounts of characters. At least one of them contains only 1 chars\n\nLet's remove some extreme outliers!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nX_train_punct_len_zscore = X_train_punct_len.apply(stats.zscore, axis=0)\nmask_outliers = np.logical_not(((X_train_punct_len_zscore>5).any(axis=1)).values + ((X_train_punct_len_zscore<-5).any(axis=1)).values)\nX_train_punct_len = X_train_punct_len.loc[mask_outliers,:]\ny_train = y_train[X_train_punct_len.index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How does our data look like now?"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_punct_len.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## How do they interact with each other?"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfig, ax = plt.subplots(figsize=(8,6))\nsns.heatmap(np.abs(X_train_punct_len.corr()), annot=True)\nplt.savefig('correlation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* huge correlation between `peri_text` and `length_text`\n\n* huge correlation between `comm_text` and `length_text`\n\n* huge correlation between `comm_text` and `peri_text`\n\n* huge correlation between `peri_test` and `num_words_text`\n\n...and so on\n\nWe have to find out how they interact with the target to determine what to do with them."},{"metadata":{},"cell_type":"markdown","source":"## How do they Interact with the Target?"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([X_train_punct_len, y_train], axis=1).groupby('is_fake').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Main Findings:**\n* Titles of Fake News contain ~4 times less `periods`\n\n* Titles of Fake News contain ~10 times more `question marks`\n\n* Titles of Fake News contain ~130 times more `exclamation marks` **(wow!)**\n\n* Texts of Fake News contain ~10 times more `question marks` \n\n* Texts of Fake News contain ~10 times more `exclamation marks`\n\n* Titles of Fake News are 50% `longer`. The number of words seems to reflect that fact as well. -> The length of words in the titles of Fake News and real News don't seem to vary a lot.\n\n* $\\frac{len(title)}{len(title) + len(text)}$ is ~40% longer in Fake News\n\n* Let's ignore the high correlations we found above. A more elaborate approach would be to eliminate correlating features by dropping them or using dimensionality reduction like PCA"},{"metadata":{},"cell_type":"markdown","source":"# A Simple Model based on Punctuation and Text Length\nLet's use a default RandomForest for our classification!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(random_state=42)\npreds = cross_val_predict(rf, X_train_punct_len, y_train, cv=5)\nf1 = f1_score(y_true=y_train, y_pred=preds)\nacc = accuracy_score(y_true=y_train, y_pred=preds)\nprint(f'f1: {f1}\\nacc: {acc}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are already pretty nice results on train! What are the most `important features`?"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.fit(X_train_punct_len, y_train)\npd.Series(rf.feature_importances_, \n          index=X_train_punct_len.columns).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interestingly, the length of the title is way more important than e.g. `excl_title`, even though the relation between the target and `excl_title` is way stronger. This is most likely the case because only a few titles contain exclamation marks."},{"metadata":{},"cell_type":"markdown","source":"## Finetune the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparams= {'n_estimators':[200, 300],\n        'criterion':['gini', 'entropy'],\n        'max_depth':[5, None],\n        'max_features':[2,5]}\ngscv = GridSearchCV(rf, params)\ngscv.fit(X_train_punct_len, y_train)\ngscv.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_tuned = RandomForestClassifier(criterion='entropy', max_depth=None, max_features=2, n_estimators=200, random_state=42)\npreds = cross_val_predict(rf_tuned, X_train_punct_len, y_train, cv=5)\nf1 = f1_score(y_true=y_train, y_pred=preds)\nacc = accuracy_score(y_true=y_train, y_pred=preds)\nprint(f'f1: {f1}\\nacc: {acc}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_tuned.fit(X_train_punct_len, y_train)\npd.Series(rf_tuned.feature_importances_, \n          index=X_train_punct_len.columns).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final Prediction on Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_test = rf_tuned.predict(X_test_punct_len) \nf1 = f1_score(y_true=y_test, y_pred=preds_test)\nacc = accuracy_score(y_true=y_test, y_pred=preds_test)\nprint(f'f1: {f1}\\nacc: {acc}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`f1-score` of about 93%! Our model is already very powerful, even though it solely focuses on punctuation and text length. Moreover, it performs even better than on train!"},{"metadata":{},"cell_type":"markdown","source":"# How well can we predict on subsets of the Features?"},{"metadata":{"trusted":true},"cell_type":"code","source":"forest = RandomForestClassifier(random_state=42)\n\nbest_column = 'all'\nbest_score = 0\nfor column in X_train_punct_len.columns:\n    forest.fit(X_train_punct_len[column].values.reshape((-1,1)), y_train)\n    preds = cross_val_predict(forest, X_train_punct_len[column].values.reshape((-1,1)), y_train, cv=5)\n    f1 = f1_score(y_true=y_train, y_pred=preds)\n    \n    if(f1>best_score):\n        best_score = f1\n        best_column = column\n\nprint(f'A default RandomForest already obtains an f1-score of {f1} on validation data when trained solely on the column {column}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest = RandomForestClassifier(random_state=42)\npunctuation = ['peri_title', 'comm_title', 'ques_title', 'excl_title', 'peri_text', 'comm_text', 'ques_text', 'excl_text']\nlenght = ['length_title', 'length_text','num_words_title', 'title_ratio']\n\n\nforest.fit(X_train_punct_len[punctuation], y_train)\npreds = cross_val_predict(forest, X_train_punct_len[punctuation], y_train, cv=5)\nf1 = f1_score(y_true=y_train, y_pred=preds)\nprint(f'A default RandomForest already obtains an f1-score of {f1} when trained solely on punctuation-based Features')\nprint('The Feature Inportances in that approach are:')\nprint(pd.Series(forest.feature_importances_, \n          index=punctuation).sort_values(ascending=False))\n\nforest.fit(X_train_punct_len[lenght], y_train)\npreds = cross_val_predict(forest, X_train_punct_len[lenght], y_train, cv=5)\nf1 = f1_score(y_true=y_train, y_pred=preds)\nprint(f'A default RandomForest already obtains an f1-score of {f1} when trained solely on length-based Features')\nprint('The Feature Importances in that approach are:')\nprint(pd.Series(forest.feature_importances_, \n          index=lenght).sort_values(ascending=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we saw, even some basic feature engineering might reveal powerful features for our machine learning models. I am pretty sure that adding `Word Embeddings` will improve the performance by a lot.\n\nThank you for reading this notebook!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}