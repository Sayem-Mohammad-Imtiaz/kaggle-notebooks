{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"background-color:SteelBlue; color:white\" >-> Content:</h1>\nHi all, this notebook covers the most important concepts of deep learning on structured (tabular) data. \n\nThis project is still work in progress. Feel free to leave a comment to suggest further improvements.\n\n## 0. [Prerequisits](#sec0)\n\n## 1. [Tensor Handling](#sec1)\n* Rank And Size\n* Reshaping Tensors\n* Dtypes\n* Tensor Broadcasting\n* Use Tensors On GPUs\n\n## 2. [Data Loading](#sec2)\n* Custom Datasets\n* Samplers And Data Loaders\n* Demonstration\n\n## 3. [Modules, Linear Layers And FNN](#sec3)\n* Linear Layers From Scratch\n* Create A Custom FNN\n* Model Inspection\n* Demonstration\n\n## 4. [Autograd and Training](#sec4)\n* Autograd\n* Training Neural Networks\n* Training Loop And Evaluation\n* Perform Training\n\n## 5. [Regularization](#sec5)\n* Overfitting And Underfitting\n* Vanishing And Exploding Gradients\n* Weight Decay\n* Dropout\n\n## 6. [Gradient Accumulation](#sec6)\n\n## 7. [(Batch) Normalization](#sec7)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sec0\"></a>\n***\n<h1 style=\"background-color:SteelBlue; color:white\" >-> 0. Prerequisits</h1>\n\n\n1. **python fundamentals:** [This simple & free Kaggle Course](https://www.kaggle.com/learn/python) is already enough!\n2. **notebooks & numpy:** [Chapter 1&2 of this free book](https://jakevdp.github.io/PythonDataScienceHandbook/) is probably the best way to learn it! \n\nFrom now on I expect you all to be familiar with the concepts used in the named sources. I don't expect any further python skills.","metadata":{}},{"cell_type":"code","source":"#!pip install tabulate\n\n# all imports we will need\nfrom wand.image import Image as WImage\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import RandomSampler, SequentialSampler, DataLoader\nfrom tabulate import tabulate\nfrom graphviz import Digraph\nfrom torch.autograd import Variable\nfrom torch.optim import Adam\nfrom sklearn.metrics import accuracy_score\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-12T18:37:05.027633Z","iopub.execute_input":"2021-09-12T18:37:05.028465Z","iopub.status.idle":"2021-09-12T18:37:08.02758Z","shell.execute_reply.started":"2021-09-12T18:37:05.028313Z","shell.execute_reply":"2021-09-12T18:37:08.026257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"sec1\"></a>\n***\n<h1 style=\"background-color:SteelBlue; color:white\" >-> 1. Tensor Handling</h1>\n\nThere are multiple ways of interpreting tensors. From a python perspective it's best to interpret them as regular arrays.\nWe can simply create them from existing lists or numpy arrays.","metadata":{}},{"cell_type":"markdown","source":"## Rank And Size\nDescribing tensors we need some terminology. The most important property of a tensor is its **rank** and **size**.\n\nTensor of...\n\n* **rank 0**: a number / scalar\n* **rank 1**: an array of rank 0 tensors\n* **rank 2**: an array of rank 1 tensors\n* **rank 3**: an array of rank 2 tensors\n...\n\nThe **size** of a tensor describes how many tensors of smaller ranks are stored in it.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"array = np.array([[[1, 2, 3, 4],\n                   [5, 6, 7, 8],\n                   [0, 0, 0, 1]],\n\n                  [[4, 3, 2, 1],\n                   [8, 7, 6, 5],\n                   [1, 0, 0, 0]]\n                  ])\n\n# bridge from np to torch\ntensor = torch.tensor(array)\n# bridge from torch to np\narray = tensor.numpy()\n\nprint(tensor, \"\\n\\n\")\nprint(\"size =\", tensor.size())\nprint(\"\\ntype:\\n\", type(tensor.numpy()))","metadata":{"execution":{"iopub.status.busy":"2021-09-12T18:37:08.029894Z","iopub.execute_input":"2021-09-12T18:37:08.030364Z","iopub.status.idle":"2021-09-12T18:37:08.07252Z","shell.execute_reply.started":"2021-09-12T18:37:08.030317Z","shell.execute_reply":"2021-09-12T18:37:08.071136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the **size** of this tensor tells us that it is a tensor of rank 3 consisting of 2 tensors of rank 2, in which 3 tensors of rank 1 are stored. In each of these tensors of rank 1 are 4 tensors of rank 0.","metadata":{}},{"cell_type":"markdown","source":"## Reshaping Tensors\nWe can simply reshape tensors to change their size.\nHaving tensor of rank n, reshaping it recursively fills the desired shape beginning with the first element of the first tensor of rank n-1.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"tensor = torch.rand(size=(3, 4, 2))\nprint(\"original tensor:\\n\", tensor)\n\nnew_size = [4, 6]\ntensor_reshaped = tensor.reshape(new_size)\nprint(\"reshaped:\\n\", tensor_reshaped)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-12T18:37:08.076575Z","iopub.execute_input":"2021-09-12T18:37:08.076958Z","iopub.status.idle":"2021-09-12T18:37:08.13781Z","shell.execute_reply.started":"2021-09-12T18:37:08.07687Z","shell.execute_reply":"2021-09-12T18:37:08.136473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dtypes\nEach tensor stores values of a single type only.\nYou will see later on that some core functionalities of PyTorch expect tensors of a certain **dtype**.\n\nStandard **dtypes**:\n* **int64 aka long**: tensors storing integers only\n* **float32 aka float**: tensors storing at least one non-integer","metadata":{}},{"cell_type":"code","source":"tensor = torch.rand(size=(3, 3))\n\n# change dtype of a tensor\ntensor_long = tensor.long()\nprint(\"dtype of a long tensor:\", tensor_long.dtype)\n\ntensor_float = tensor.float()\nprint(\"dtype of a float tensor:\", tensor_float.dtype)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T18:37:08.139974Z","iopub.execute_input":"2021-09-12T18:37:08.140376Z","iopub.status.idle":"2021-09-12T18:37:08.150119Z","shell.execute_reply.started":"2021-09-12T18:37:08.140337Z","shell.execute_reply":"2021-09-12T18:37:08.149295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tensor Broadcasting\n\nsimilar to numpy arrays, we can add, subtract, multiply ... 2 tensors. \n\nThe following example shows that tensor operations stick to the rules of **numpy broadcasting** as explained in [this chapter](https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html) of the book listed in the prerequisits:\n\nif we add a tensor **a of rank 2** to a tensor **b of rank 1**, b is will be added to each tensor of rank 1 stored in a","metadata":{}},{"cell_type":"code","source":"tensor_a = torch.tensor([[1, 2, 3],\n                         [4, 5, 6]])\n\ntensor_b = torch.tensor([7, 8, 9])\n\ntensor_a + tensor_b","metadata":{"execution":{"iopub.status.busy":"2021-09-12T18:37:08.151318Z","iopub.execute_input":"2021-09-12T18:37:08.151608Z","iopub.status.idle":"2021-09-12T18:37:08.164625Z","shell.execute_reply.started":"2021-09-12T18:37:08.151578Z","shell.execute_reply":"2021-09-12T18:37:08.163788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Use Tensors On GPUs\n\nOne of the most important benefits of tensors is that you can perform tensor operations on the GPU (if you have one).\n\nBoth tensors have to be on the same device if you want to perfom an operation on them.","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\n# move them to the gpu (if one is available),\ntensor_a.to(device)\ntensor_b.to(device)\n\ntensor_a + tensor_b","metadata":{"execution":{"iopub.status.busy":"2021-09-12T18:37:08.16605Z","iopub.execute_input":"2021-09-12T18:37:08.166585Z","iopub.status.idle":"2021-09-12T18:37:08.174616Z","shell.execute_reply.started":"2021-09-12T18:37:08.166547Z","shell.execute_reply":"2021-09-12T18:37:08.173868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"sec2\"></a>\n***\n<h1 style=\"background-color:SteelBlue; color:white\" >-> 2. Data Loading</h1>\n\n## Custom Datasets\nEach entry of a TensorDataset contains the independend and the dependend variable(s) of one observation.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    \"\"\"\n    contains all the mandatory methods\n    to load it using a pytorch dataloader\n    \"\"\"\n\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, item):\n        obs = self.data.iloc[item, :]\n        x = torch.tensor(obs.drop(\"label\").values).float()\n        y = torch.tensor(obs[\"label\"]).long()\n        return x, y","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-12T18:37:08.17576Z","iopub.execute_input":"2021-09-12T18:37:08.176237Z","iopub.status.idle":"2021-09-12T18:37:08.189297Z","shell.execute_reply.started":"2021-09-12T18:37:08.176205Z","shell.execute_reply":"2021-09-12T18:37:08.188124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Samplers And Data Loaders\n* samplers define data drawing policies\n* Data Loaders are used to draw batches of data from Datasets using drawing policies","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def create_loader(df: pd.DataFrame, sampler_class, batch_size: int):\n    dataset = CustomDataset(data=df)\n    sampler = sampler_class(data_source=dataset)\n    return DataLoader(dataset=dataset, \n                      sampler=sampler, \n                      batch_size=batch_size)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-12T18:37:08.192073Z","iopub.execute_input":"2021-09-12T18:37:08.192617Z","iopub.status.idle":"2021-09-12T18:37:08.209646Z","shell.execute_reply.started":"2021-09-12T18:37:08.192586Z","shell.execute_reply":"2021-09-12T18:37:08.208514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Demonstration","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def get_demo_batch():\n    # parameters\n    batch_size = 16\n\n    # read data\n    demo_df = pd.read_csv(\"../input/mnist-in-csv/mnist_train.csv\")\n\n    # create loader\n    demo_loader = create_loader(df=demo_df,\n                                sampler_class=RandomSampler,\n                                batch_size=batch_size)\n\n    # draw first batch from loader\n    demo_x_batch, demo_y_batch = next(iter(demo_loader))\n    return demo_x_batch, demo_y_batch","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-12T18:37:08.212391Z","iopub.execute_input":"2021-09-12T18:37:08.21313Z","iopub.status.idle":"2021-09-12T18:37:08.223872Z","shell.execute_reply.started":"2021-09-12T18:37:08.213076Z","shell.execute_reply":"2021-09-12T18:37:08.222906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def demo_dataloaders():\n    demo_x_batch, demo_y_batch = get_demo_batch()\n    print(\"each batch is stored in a list.\")\n    print(\"\\nthe first entry is of type\", type(demo_x_batch))\n    print(\"and has a size of\", demo_x_batch.size())\n    print(\"\\nthe second entry is of type\", type(demo_y_batch))\n    print(\"and has a size of\", demo_y_batch.size())\n\n\ndemo_dataloaders()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-12T18:37:08.225581Z","iopub.execute_input":"2021-09-12T18:37:08.226407Z","iopub.status.idle":"2021-09-12T18:37:14.061411Z","shell.execute_reply.started":"2021-09-12T18:37:08.226356Z","shell.execute_reply":"2021-09-12T18:37:14.059345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"sec3\"></a>\n***\n<h1 style=\"background-color:SteelBlue; color:white\" >-> 3. Linear Layers And FNN</h1>\n\n## Modules, Linear Layers From Scratch\n\nPyTorch combines tensors and tensor operations to modules. These modules are building blocks which can be used to construct neural networks. They can contain single layers or large neural networks. \nThe probably most simple module is the [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer.\n\nGiven some input $x$, the linear layer computes its output $y$ via $y=xA^T+b$. Both, the weight matrix $A$ and the bias $b$ will be adapted during the training process so that the output $y$ becomes as close as possible to the ground truth.","metadata":{}},{"cell_type":"code","source":"class CustomLinear(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        A = torch.randn(size=(out_features, in_features))\n        self.weight = nn.Parameter(data=A, requires_grad=True)\n        b = torch.randn(size=(out_features,))\n        self.bias = nn.Parameter(data=b, requires_grad=True) \n\n    def forward(self, x):\n        return x @ self.weight.t() + self.bias","metadata":{"execution":{"iopub.status.busy":"2021-09-12T18:37:14.063762Z","iopub.execute_input":"2021-09-12T18:37:14.064283Z","iopub.status.idle":"2021-09-12T18:37:14.072853Z","shell.execute_reply.started":"2021-09-12T18:37:14.064217Z","shell.execute_reply":"2021-09-12T18:37:14.071902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `CustomLinear` module is a simplified implementiation of the [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) module.\nThe fundamental functionalities are the same.\nNote that this layer is nothing else but a combination of tensors with `requires_grad=True`, and tensor operations.\nNote that `A` and `b` are `nn.Parameters`, and thus are iteratively altered during the training process to improve the performance of the module.\nLater on we will see how this works in detail.\nI renamed `A` and `b` to `weight` and `bias` respectively. to match the pattern of the original linear layer.","metadata":{}},{"cell_type":"markdown","source":"## Create A Custom FNN\nNote: The first linear layer is builtin, the last linear layer is our custom module\n\nAs we can see, the whole neural network is based on the module-abstraction as well.","metadata":{"execution":{"iopub.status.busy":"2021-09-10T14:09:39.720711Z","iopub.execute_input":"2021-09-10T14:09:39.721257Z","iopub.status.idle":"2021-09-10T14:09:39.758773Z","shell.execute_reply.started":"2021-09-10T14:09:39.721217Z","shell.execute_reply":"2021-09-10T14:09:39.757606Z"},"pycharm":{"name":"#%% md\n"},"trusted":true}},{"cell_type":"code","source":"class ExampleFNN(nn.Module):\n    def __init__(self, num_feats, num_classes):\n        super(ExampleFNN, self).__init__()\n\n        # hidden layer 1\n        self.linear1 = nn.Linear(in_features=num_feats, \n                                 out_features=256)\n        self.relu1 = nn.ReLU()\n\n        # output layer \n        self.linear2 = CustomLinear(in_features=256, \n                                    out_features=num_classes)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu1(x)\n        return self.linear2(x)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T18:37:14.074235Z","iopub.execute_input":"2021-09-12T18:37:14.074602Z","iopub.status.idle":"2021-09-12T18:37:14.089834Z","shell.execute_reply.started":"2021-09-12T18:37:14.074571Z","shell.execute_reply":"2021-09-12T18:37:14.0889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Inspection\nLet's investigate our model a little. Therefore, we can either use a predefined model summarizer like [torchsummery](https://pypi.org/project/torch-summary/https://pypi.org/project/torch-summary/) or write our own summarizer from scratch. The latter provides some further insights into handling the model.\n\nLet's investigate how the model parameters are initialized:","metadata":{}},{"cell_type":"code","source":"def show_weights(module):\n    if (type(module) == nn.Linear) or (type(module) == CustomLinear):\n        print(module)\n        print(type(module))\n        print(module.weight)\n        print(\"\\n\")","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-12T18:37:14.091145Z","iopub.execute_input":"2021-09-12T18:37:14.091709Z","iopub.status.idle":"2021-09-12T18:37:14.104147Z","shell.execute_reply.started":"2021-09-12T18:37:14.091674Z","shell.execute_reply":"2021-09-12T18:37:14.102637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that we can use the apply method to iterate over the whole model. We can exploit this functionality to select certain module types and transform them. I.e., we can perform our own parameter initialization before training the model.\n\nThe last element contains the whole model.","metadata":{}},{"cell_type":"code","source":"def summary(module, x):\n    \"\"\"\n    Iterates over all inner module childs,\n    calculates number of parameters per child,\n\n    :param module: module to summarize\n    :param x: demo module input\n    \"\"\"\n    print_list = []\n    total_params = 0\n\n    # iterate over all inner module childs\n    for child in module.named_children():\n        x = child[1](x)\n        param_string = \"\"\n        child_params = 0\n        for param in child[1].named_parameters():\n            shape = list(param[1].size())\n            params = 1\n            for ax in shape:\n                params *= ax\n            child_params += params\n            param_string = param_string+f\"'{param[0]}'\"+\" shape: \"+str(shape)+\" \"\n        total_params += child_params\n        print_list.append([child[0], list(x.size()), param_string, child_params])\n\n    print(f\"Using a Batch Size of {x.size(0)}:\\n\")\n    headers = [\"Name\", \"Out Shape\", \"Weights\", \"Trainable Parameters\"]\n    print(tabulate(print_list, headers=headers))\n    print(\"\\nTrainable Model Parameters:\", total_params)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T18:37:14.105812Z","iopub.execute_input":"2021-09-12T18:37:14.106313Z","iopub.status.idle":"2021-09-12T18:37:14.118125Z","shell.execute_reply.started":"2021-09-12T18:37:14.106277Z","shell.execute_reply":"2021-09-12T18:37:14.117277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"visualize the model as a computational graph:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# source; https://gist.github.com/wangg12/f11258583ffcc4728eb71adc0f38e832\ndef make_dot(var, params=None):\n    if params is not None:\n        assert isinstance(params.values()[0], Variable)\n        param_map = {id(v): k for k, v in params.items()}\n\n    node_attr = dict(style=\"filled\", \n                     shape=\"box\", \n                     align=\"left\", \n                     fontsize=\"12\", \n                     ranksep=\"0.1\", \n                     height=\"0.2\")\n    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n    seen = set()\n\n    def size_to_str(size):\n        return \"(\" + (\", \").join([\"%d\" % v for v in size]) + \")\"\n\n    def add_nodes(var):\n        if var not in seen:\n            if torch.is_tensor(var):\n                dot.node(str(id(var)), \n                         size_to_str(var.size()), \n                         fillcolor=\"orange\")\n                dot.edge(str(id(var.grad_fn)), str(id(var)))\n                var = var.grad_fn\n            if hasattr(var, \"variable\"):\n                u = var.variable\n                name = param_map[id(u)] if params is not None else \"\"\n                node_name = \"%s\\n %s\" % (name, size_to_str(u.size()))\n                dot.node(str(id(var)), \n                         node_name, \n                         fillcolor=\"lightblue\")\n            else:\n                dot.node(str(id(var)), str(type(var).__name__))\n            seen.add(var)\n            if hasattr(var, \"next_functions\"):\n                for u in var.next_functions:\n                    if u[0] is not None:\n                        dot.edge(str(id(u[0])), str(id(var)))\n                        add_nodes(u[0])\n            if hasattr(var, \"saved_tensors\"):\n                for t in var.saved_tensors:\n                    dot.edge(str(id(t)), str(id(var)))\n                    add_nodes(t)\n\n    add_nodes(var)\n    return dot","metadata":{"execution":{"iopub.status.busy":"2021-09-12T18:37:14.119471Z","iopub.execute_input":"2021-09-12T18:37:14.119966Z","iopub.status.idle":"2021-09-12T18:37:14.135126Z","shell.execute_reply.started":"2021-09-12T18:37:14.119932Z","shell.execute_reply":"2021-09-12T18:37:14.134343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Demonstration","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def get_demo_model():\n    return ExampleFNN(num_feats=784, num_classes=10)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-12T18:37:14.136556Z","iopub.execute_input":"2021-09-12T18:37:14.137102Z","iopub.status.idle":"2021-09-12T18:37:14.153402Z","shell.execute_reply.started":"2021-09-12T18:37:14.137068Z","shell.execute_reply":"2021-09-12T18:37:14.152435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def demo_fnn():\n    demo_model = get_demo_model()\n\n    demo_x_batch, _ = get_demo_batch()\n    demo_pred = demo_model(demo_x_batch)\n\n    # show weight initiation\n    demo_model.apply(show_weights)\n\n    # model summary\n    summary(module=demo_model, x=demo_x_batch)\n\n    # create computational graph\n    graph = make_dot(demo_pred)\n    graph.view()\n    img = WImage(filename='../input/computational-graph/Digraph.gv (2).pdf')\n    return img\n\n\ndemo_fnn()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n","is_executing":true},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-12T18:37:14.154968Z","iopub.execute_input":"2021-09-12T18:37:14.155594Z","iopub.status.idle":"2021-09-12T18:37:19.355772Z","shell.execute_reply.started":"2021-09-12T18:37:14.155555Z","shell.execute_reply":"2021-09-12T18:37:19.354482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each layer $\\mathscr{l}$ of a FNN has $M_{\\mathscr{l}-1} \\cdot M_{\\mathscr{l}}$ many weights plus $M_{\\mathscr{l}}$ many bias terms. $M_{\\mathscr{l}}$ is the number of nodes (i.e. output size) of layer ${\\mathscr{l}}$.","metadata":{}},{"cell_type":"markdown","source":"The computational graph displays:\n\n* orange = model output is of size `batch_size x n_classes` (leaf of the forward pass tree, root of the backward pass tree)\n* blue = trainable model parameters (root of the forward pass tree, leaf of the backward pass tree)\n\nnote: the edges within the backward pass direct into the opposite directions. This results in the roots becoming leafs, and vice versa. Thus, we often say that the gradient can be calculated wrt the leafs, we actually refer to the model parameters.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sec4\"></a>\n***\n<h1 style=\"background-color:SteelBlue; color:white\" >-> 4. Autograd And Training</h1>","metadata":{}},{"cell_type":"markdown","source":"## Autograd\n\nWe can describe every tensor operation as a function. PyTorch Autograd allows us to calculate the Gradient of such a function with respect to each individual input. Let me give you an example. If you are not familiar with matrix calculus, you can always refer to [this](https://en.wikipedia.org/wiki/Matrix_calculus):\n\n$$f:\\mathbb{R}^2\\rightarrow \\mathbb{R}, f(x) = x^Tx+1 = x_1^2+x_2^2 +1$$\n\nHas the following partial derivatives:\n$$\\frac{\\partial f(x)}{\\partial x_1}=2x_1$$\n\n$$\\frac{\\partial f(x)}{\\partial x_2}=2x_2$$\n\nSo we would obtain at $x=\\left(\\begin{array}{c} 2 \\\\ 3 \\end{array}\\right)$:\n\n\n$$f(x)=2^2+3^2+1=4+9+1=14$$\n\n$$\\frac{\\partial f(x)}{\\partial x_1}=4$$\n\n$$\\frac{\\partial f(x)}{\\partial x_2}=6$$\n\nAs we can see, Pytorch autograd provides the same results:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def f(x):\n    return x.t() @ x + 1\n\n\nx = torch.tensor([[2],\n                  [3]], dtype=torch.float32)\n\n# enable autograd for that tensor\n# underscore in the end denotes inplace operations\nx.requires_grad_()\n\ny = f(x)\n\nprint(y)\n\ny.backward()  # populate gradients\nx.grad","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n","is_executing":true},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-12T18:37:19.357951Z","iopub.execute_input":"2021-09-12T18:37:19.358344Z","iopub.status.idle":"2021-09-12T18:37:19.381127Z","shell.execute_reply.started":"2021-09-12T18:37:19.358305Z","shell.execute_reply":"2021-09-12T18:37:19.379152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This way, we can automatically calculate the gradient of a function (e.g. a loss function) with respect to all parameters, even if the computation of the function contains loops and conditionals.\nLet's exploit this for training a neural network!","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"## Training Neural Networks\n\nOne update step is performed as follows:\n1. fetch a batch\n2. perform the forward pass\n3. calculate the loss\n4. calculate the gradient of the loss wrt all model parameters\n5. perform a variant of gradient descent to update the model parameters\n6. clear the gradient to perform further updates\n\nLet's perform such a step to get an idea of its mechanics!","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def demo_update_step():\n    # select a model to train\n    demo_model = get_demo_model()\n\n    # set the model into training mode if not done yet\n    demo_model.train()\n\n    # an elaborate variant of gradient descent\n    optimizer = Adam(demo_model.parameters())\n\n    # a loss function to determine the \"goodness\" of the model\n    loss_func = nn.CrossEntropyLoss()\n\n    # fetch a batch\n    x, y = get_demo_batch()\n\n    # forward\n    probas = demo_model(x)\n\n    # calculate loss\n    loss = loss_func(probas, y)\n\n    print(\"weight gradient before backward:\\n\", \n          demo_model.linear2.weight.grad)\n    print(\"x gradient before backward:\\n\", \n          x.grad)\n\n    # calculate gradient wrt all model parameters\n    loss.backward()\n\n    # gradient descent to update the model parameters\n    optimizer.step()\n\n    print(\"\\nweight gradient after backward:\\n\", \n          demo_model.linear2.weight.grad)\n    print(\"x gradient after backward:\\n\", \n          x.grad)\n\n    # clear gradient\n    optimizer.zero_grad()\n\n    print(\"\\nweight gradient after clearing:\\n\", \n          demo_model.linear2.weight.grad)\n    print(\"x gradient after clearing:\\n\", \n          x.grad)\n\n\ndemo_update_step()","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-09-12T18:37:19.383076Z","iopub.execute_input":"2021-09-12T18:37:19.383444Z","iopub.status.idle":"2021-09-12T18:37:23.910869Z","shell.execute_reply.started":"2021-09-12T18:37:19.383414Z","shell.execute_reply":"2021-09-12T18:37:23.909839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, `loss.backward()` populates the gradient tensors of the parameters, not of the input.\nMoreover, we can see that we can clear them up (setting them to 0) manually using `optimizer.zero_grad()`. Otherwise the gradients would end up being added together (which has some benefits in more complex scenarios we will talk about later on).","metadata":{"execution":{"iopub.status.busy":"2021-09-09T13:34:45.833666Z","iopub.execute_input":"2021-09-09T13:34:45.834084Z","iopub.status.idle":"2021-09-09T13:34:45.839122Z","shell.execute_reply.started":"2021-09-09T13:34:45.83404Z","shell.execute_reply":"2021-09-09T13:34:45.838107Z"}}},{"cell_type":"markdown","source":"## Training Loop And Evaluation","metadata":{}},{"cell_type":"code","source":"def calculate_loss(batch, model, loss_func, device):\n    x_batch = batch[0].to(device)\n    y_batch = batch[1].to(device)\n\n    optimizer.zero_grad()  # clear gradient\n    model.train()  # training mode  \n    out = model(x_batch)  # forward = make predictions\n    loss = loss_func(out, y_batch)  # calculate error\n    return loss\n\n\ndef train(batch, model, optimizer, loss_func, device):\n    loss = calculate_loss(batch=batch, \n                          model=model, \n                          loss_func=loss_func, \n                          device=device)\n    loss.backward()  # populate gradient wrt model parameters\n    optimizer.step()  # update model parameters\n    return loss\n\n\ndef train_loop(model, optimizer, loss_func, train_loader, device, n_loss_prints):\n    print_interval = int(len(train_loader) / n_loss_prints)\n    next_print_iter = print_interval\n    loss_summed = 0\n    for i, batch in enumerate(train_loader):\n        loss_summed += train(batch=batch,\n                             model=model,\n                             optimizer=optimizer,\n                             loss_func=loss_func,\n                             device=device)\n        if i == next_print_iter:\n            print(f\"iteration {i+1}/{len(train_loader)}, loss:{loss_summed/print_interval}\")\n            next_print_iter += print_interval\n            loss_summed = 0\n\n\ndef predict(model, sequential_loader, device):\n    model.eval()\n    y_proba = []\n    y_true = []\n    for batch in sequential_loader:\n        x_batch = batch[0].to(device)\n        y_batch = batch[1].to(device)\n        with torch.no_grad():\n            out = model(x_batch)\n            y_proba.append(out)\n            y_true.append(y_batch)\n    y_proba_tensor = torch.cat(y_proba)\n    y_true_tensor = torch.cat(y_true).cpu().numpy()\n    y_pred = np.argmax(y_proba_tensor.cpu().numpy(), axis=1)\n    return [y_pred, y_true_tensor]","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-09-12T18:37:23.912318Z","iopub.execute_input":"2021-09-12T18:37:23.912707Z","iopub.status.idle":"2021-09-12T18:37:23.930828Z","shell.execute_reply.started":"2021-09-12T18:37:23.912673Z","shell.execute_reply":"2021-09-12T18:37:23.92782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"sec5\"></a>\n***\n<h1 style=\"background-color:SteelBlue; color:white\" >-> 5. Regularization</h1>","metadata":{}},{"cell_type":"markdown","source":"## Overfitting And Underfitting\ntodo","metadata":{}},{"cell_type":"markdown","source":"## Vanishing And Exploding Gradients\ntodo","metadata":{}},{"cell_type":"markdown","source":"## Weight Decay\n* weight decay (l2 regularization) is a regularization technique\n* weight decay flattens the distribution of weights within the model (parameters are more likely to have similar values)\n* model is less sensitive and focuses more on underlying patterns instead of noise\n* gradient flow is improved, which lowers the risk of vanishing/exploding gradients (more on that later)\n\nLet's perform training with some weight decay:","metadata":{}},{"cell_type":"code","source":"# parameters\ntotal_epochs = 2\nbatch_size = 16\n\n# create loaders\ntrain_df = pd.read_csv(\"../input/mnist-in-csv/mnist_train.csv\")\nval_df = pd.read_csv(\"../input/mnist-in-csv/mnist_test.csv\")\ntrain_sampler = RandomSampler\neval_sampler = SequentialSampler\ntrain_loader = create_loader(df=train_df,\n                             sampler_class=train_sampler,\n                             batch_size=batch_size)\ntrain_loader_eval = create_loader(df=train_df,\n                                  sampler_class=eval_sampler,\n                                  batch_size=batch_size)\nval_loader_eval = create_loader(df=val_df,\n                                sampler_class=eval_sampler,\n                                batch_size=batch_size)","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-09-12T18:37:23.93258Z","iopub.execute_input":"2021-09-12T18:37:23.933218Z","iopub.status.idle":"2021-09-12T18:37:29.030525Z","shell.execute_reply.started":"2021-09-12T18:37:23.933172Z","shell.execute_reply":"2021-09-12T18:37:29.029179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ExampleFNN(num_feats=784, num_classes=10).to(device)\n\n# access all parameters\n# perform weight decay on selected ones\noptimizer = Adam([{\"params\": model.linear1.bias},\n                  {\"params\": model.linear2.bias},\n                  {\"params\": model.linear1.weight, \"weigth_decay\": 1},\n                  {\"params\": model.linear2.weight, \"weight_decay\": 1}], lr=0.0001)\nloss_func = nn.CrossEntropyLoss()\n\n# train\nfor epoch in range(1, total_epochs + 1):\n    print(f\"Epoch {epoch} / {total_epochs}:\")\n    train_loop(model=model,\n               optimizer=optimizer,\n               loss_func=loss_func,\n               train_loader=train_loader,\n               device=device,\n               n_loss_prints=10)\n    y_pred_train, y_true_train = predict(model=model,\n                                         sequential_loader=train_loader_eval,\n                                         device=device)\n\n    y_pred_val, y_true_val = predict(model=model,\n                                       sequential_loader=val_loader_eval,\n                                       device=device)\n    acc_train = accuracy_score(y_true=y_true_train, y_pred=y_pred_train)\n    acc_val = accuracy_score(y_true=y_true_val, y_pred=y_pred_val)\n    print(\"Train Accuracy:\", acc_train)\n    print(\"Validation Accuracy:\", acc_val)\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-09-12T18:37:29.033492Z","iopub.execute_input":"2021-09-12T18:37:29.033968Z","iopub.status.idle":"2021-09-12T18:41:39.577606Z","shell.execute_reply.started":"2021-09-12T18:37:29.033924Z","shell.execute_reply":"2021-09-12T18:41:39.576492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dropout\ntodo","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sec6\"></a>\n***\n<h1 style=\"background-color:SteelBlue; color:white\" >-> 6. Gradient Accumulation</h1>\nGradient Accumulation might be beneficial:\n\n* The optimal batch size depends on both data and algorithm. \n\n* Too small values lead to less stable learning behavior and giving more importance to outliers. The calculated Gradient per batch is just a predicted gradient for the whole dataset.\n\n* GPU memory is limited, thus we might be forced to use batch sizes smaller than optimal.\n\n* GPU memory consumption mostly depends on the `model complexity` (#parameters), the `data size` (e.g. large images vs. small imagtes)\n\n* We can reduce the memory consumption depending on data size by using Gradient Accumulation\n\n* `Gradient Accumulation` sequentially creates gradients for pseudo batches that are larger than the actual batches.","metadata":{}},{"cell_type":"code","source":"def train_loop_ga(model, optimizer, loss_func, train_loader, device, accumulation):\n    loss_accumulated = 0\n    for i, batch in enumerate(train_loader):\n        loss_accumulated += calculate_loss(batch=batch, \n                              model=model, \n                              loss_func=loss_func, \n                              device=device)\n        if ((i+1)%accumulation==0):\n            loss_accumulated /= accumulation\n            loss_accumulated.backward()\n            optimizer.step()\n            optimizer.zero_grad() \n            print(f\"iteration {i+1}/{len(train_loader)}, loss:{loss_accumulated}\")\n            loss_accumulated = 0","metadata":{"execution":{"iopub.status.busy":"2021-09-12T18:41:39.581673Z","iopub.execute_input":"2021-09-12T18:41:39.582045Z","iopub.status.idle":"2021-09-12T18:41:39.590505Z","shell.execute_reply.started":"2021-09-12T18:41:39.582007Z","shell.execute_reply":"2021-09-12T18:41:39.588721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ExampleFNN(num_feats=784, num_classes=10).to(device)\noptimizer = Adam([{\"params\": model.linear1.bias},\n                  {\"params\": model.linear2.bias},\n                  {\"params\": model.linear1.weight, \"weigth_decay\": 50},\n                  {\"params\": model.linear2.weight, \"weight_decay\": 50}], lr=0.0001)\nloss_func = nn.CrossEntropyLoss()\n\n# train\nfor epoch in range(1, total_epochs + 1):\n    print(f\"Epoch {epoch} / {total_epochs}:\")\n    train_loop_ga(model=model,\n                  optimizer=optimizer,\n                  loss_func=loss_func,\n                  train_loader=train_loader,\n                  device=device,\n                  accumulation=150)\n    y_pred_train, y_true_train = predict(model=model,\n                                         sequential_loader=train_loader_eval,\n                                         device=device)\n\n    y_pred_val, y_true_val = predict(model=model,\n                                       sequential_loader=val_loader_eval,\n                                       device=device)\n    acc_train = accuracy_score(y_true=y_true_train, y_pred=y_pred_train)\n    acc_val = accuracy_score(y_true=y_true_val, y_pred=y_pred_val)\n    print(\"Train Accuracy:\", acc_train)\n    print(\"Validation Accuracy:\", acc_val)\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-09-12T18:41:39.593095Z","iopub.execute_input":"2021-09-12T18:41:39.593581Z","iopub.status.idle":"2021-09-12T18:45:28.464271Z","shell.execute_reply.started":"2021-09-12T18:41:39.593534Z","shell.execute_reply":"2021-09-12T18:45:28.46349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, a larger batch size (aka a bigger accumulation range) doesn't automatically result in a better training behavior. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sec7\"></a>\n***\n<h1 style=\"background-color:SteelBlue; color:white\" >-> 7. (Batch) Normalization</h1>\ntodo","metadata":{}},{"cell_type":"markdown","source":"Helpful Videos and Blogs:\n* [Elliot Waite: Autograd](https://www.youtube.com/watch?v=MswxJw-8PvE&t=75s)\n\nThis Project is still in progress =)","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-danger\" role=\"alert\">\n    <h3>Feel free to <span style=\"color:red\">comment</span> if you have any suggestions   |   motivate me with an <span style=\"color:red\">upvote</span> if you like this project.</h3>\n</div>","metadata":{}}]}