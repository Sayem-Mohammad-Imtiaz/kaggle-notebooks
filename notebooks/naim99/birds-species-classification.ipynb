{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow\nimport keras\n# If you get a FutureWarning from running this c","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To serve images to our model during training we will use a python Generator. Given that our images are structured as previously stated, a folder with the training set and a folder with the validation set, both with subfolders for each category of birds, we can use a prebuilt keras generator called ImageDataGenerator. The generator acts as a list and serves what is known as batches of tuples, where the first element of the tuple contains the images and of the batch and the second element contains the corresponding labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\ngenerator = ImageDataGenerator()\nbatches = generator.flow_from_directory('../input/100-bird-species/train', batch_size=4)\n\nbatches","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point it is usually a good idea to do a sanity check. This typically includes verifying that our images are served on the correct format, and that the images and labels are still correctly matched. We can first fetch the reverse encoding of the generator to be able to decode the onehot encoded labels given in the batches"},{"metadata":{"trusted":true},"cell_type":"code","source":"indices = batches.class_indices\nlabels = [None] * 225\n\nfor key in indices:\n    labels[indices[key]] = key\n\nlabels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can then use matplotlib to visualize the first batch"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfor X, y in batches:\n    fig, ax = plt.subplots(1, 4, figsize=(10, 10))\n    \n    for i in range(len(X)):\n        img = X[i].astype(np.uint8)\n        label = labels[np.argmax(y[i])]\n\n        ax[i].imshow(img)\n        ax[i].set_title(label)\n        ax[i].set_xticks([])\n        ax[i].set_yticks([])\n    \n    plt.show()\n    break # We only need the first batch","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setting up the base model"},{"metadata":{},"cell_type":"markdown","source":"Once we know our dataset is being served correctly we can start setting up the base model that will be the core of our classification model. As previously mentioned this will be a model called VGG19, a small model which yields relatively good results. Like the generator, this also exists as a prebuilt module in Keras, namely in the applications-module."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.vgg19 import VGG19","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When initializing the model we need to specify that we want to use the weights trained on ImageNet, that we want the entire model including top layers, and we also specify the image size we are going to use for verbosity."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = VGG19(weights='imagenet', include_top=True, \n              input_shape=(224, 224, 3))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can sanity check this step by running predicting the label for an image from our generator. Note that the predictions we are doing now will be using the labels from ImageNet, as this is what the model currently recognizes, not the labels from our dataset. We start by reinitializing the generator with the correct size and a batch size of 1. We also set the seed for the random library to control the order of the images"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(1234)\ngenerator = ImageDataGenerator()\nbatches = generator.flow_from_directory('../input/100-bird-species/train', \n                                        target_size=(224, 224), \n                                        batch_size=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can then run predictions on the first batch containing a single image. To decode the prediction using imagenet labels we can use a predefined function found in the same module as the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.vgg19 import decode_predictions\n\nfor X, y in batches:\n    preds = model.predict(X)\n    decoded_preds = decode_predictions(preds, top=1)\n    fig = plt.figure()\n    \n    img = X[0].astype(np.uint8)\n    label = labels[np.argmax(y[0])]\n    predicted = decoded_preds[0]\n    \n    plt.imshow(img)\n    fig.suptitle('Truth: {}, Predicted: {}'.format(label, predicted))\n    plt.show()\n    \n    break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"We now know both our generator and model are set up, and we are able to make predictions. The predictions, however, does not necessarily look very good. This is because of a process called preprocessing: A set of transformations applied to the images before training to give the model the best possible foundation to learn what it needs. Typical preprocessing includes rescaling the values of the data, shifting the range, and other numerical operations. Luckily, in Keras, the module which contains a model also contains the preprocessing function used for training the model. We can fetch this function and feed it to our generator to ensure all images are preprocessed before they are served to the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.vgg19 import preprocess_input\n\nnp.random.seed(1234)\ngenerator = ImageDataGenerator(preprocessing_function=preprocess_input)\nbatches = generator.flow_from_directory('../input/100-bird-species/train', \n                                        target_size=(224, 224),\n                                        batch_size=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once we have reinitialized the generator correctly we can rerun our predictions to see if they improve"},{"metadata":{"trusted":true},"cell_type":"code","source":"for X, y in batches:\n    preds = model.predict(X)\n    decoded_preds = decode_predictions(preds, top=1)\n    fig = plt.figure()\n    \n    img = X[0].astype(np.uint8)\n    label = labels[np.argmax(y[0])]\n    predicted = decoded_preds[0]\n    \n    plt.imshow(img)\n    fig.suptitle('Truth: {}, Predicted: {}'.format(label, predicted))\n    plt.show()\n    \n    break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**It is improved from 0.64 to 0.74 !! ******üòçüòéüòã"},{"metadata":{},"cell_type":"markdown","source":"Note that seeing improvements in the predictions is not a given even though the images are now preprocessed correctly. The label we are looking for might not be a part of the original dataset the model was trained on, or it might simply be a case of a bad prediction where the model misses. However, running a sanity check (preferably over more images) is usually a good habit to achieve the best results."},{"metadata":{},"cell_type":"markdown","source":"# Configuring the bird classification model"},{"metadata":{},"cell_type":"markdown","source":"Once we are happy with the interaction of our base model, we can start setting up our own custom model for solving the problem we are interested in, in this case classifying bird species. The first step is to be a bit more restrictive with what we use from the pretrained model, only picking out the parts we need. We do this by dropping the top layers used for predictions, and instead perform a pooling operation on the final convolutional layer. Once we have initialized it we can fetch the input and the output of the pretrained model using properties found in keras' model class."},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrained = VGG19(include_top=False, input_shape=(224, 224, 3), \n                   weights='imagenet', pooling='max')\ninputs = pretrained.input\noutputs = pretrained.output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we do not want the weights in this part of the final model to change, we can freeze them"},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in pretrained.layers:\n    layer.trainable = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can then create our own custom layers for performing our own task. We will use a hidden fully connected layer with 128 neurons, and a final prediction layer with 225 neurons, one per specie in our dataset. Note that the hidden layer takes the output from the pretrained model as its input."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense\n\nhidden = Dense(128, activation='relu')(outputs)\npreds = Dense(225, activation='softmax')(hidden)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once we have all our layers set up we can wrap them in a Model, and compile the model using a pretty standardized set of hyperparameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.engine import Model\nfrom keras.optimizers import Adam\n\nmodel = Model(inputs, preds)\nmodel.compile(loss='categorical_crossentropy', \n              optimizer=Adam(lr=1e-4),\n              metrics=['acc'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can then set up generators like we did before, one for the training data and one for validation, and train the model using fit_generator. Note that training here is set to a single epoch simply for example purposes"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(1234)\n\n# If you run into memory errors, try reducing this\nbatch_size = 32\n\ntrain_generator = ImageDataGenerator(\n    preprocessing_function=preprocess_input)\ntrain_batches = train_generator.flow_from_directory('../input/100-bird-species/train',\n                                                    target_size=(224, 224), \n                                                    batch_size=batch_size)\n\nval_generator = ImageDataGenerator(\n    preprocessing_function=preprocess_input)\nval_batches = val_generator.flow_from_directory('../input/100-bird-species/valid',\n                                                target_size=(224, 224),\n                                                batch_size=batch_size)\n\n# Note that training is set to 1 epoch, \n# to avoid unintentionally locking up computers\nmodel.fit_generator(train_batches, \n                    epochs=1, \n                    validation_data=val_batches, \n                    steps_per_epoch=len(train_batches), \n                    validation_steps=len(val_batches))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Regularization"},{"metadata":{},"cell_type":"markdown","source":"If you run the training above for a larger number of epochs, you will typically achieve a very decent result on the training data and a considerably worse outcome on the validation data. *This is an example of overfitting: The model starts remembering specifics from the training set instead of learning the general features we are interested in.*\nWe handle this by introducing regularization, trying to force the model to generalize. In image recognition this is typically done using dropout-layers, which during training randomly sets the firing of a subset of neurons to 0. We can introduce dropout to our model by inserting a Dropout layer which drops 30% of the neurons between the two final layers of our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dropout\n\nhidden = Dense(128, activation='relu')(outputs)\ndropout = Dropout(.3)(hidden)\npreds = Dense(225, activation='softmax')(dropout)\n\nmodel = Model(inputs, preds)\nmodel.compile(loss='categorical_crossentropy', \n              optimizer=Adam(lr=1e-4),\n              metrics=['acc'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can recompile the model and restart training to achieve what should be a better result"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(1234)\n\n# If you run into memory errors, try reducing this\nbatch_size = 32\n\ntrain_generator = ImageDataGenerator(\n    preprocessing_function=preprocess_input)\ntrain_batches = train_generator.flow_from_directory('../input/100-bird-species/train',\n                                                    target_size=(224, 224),\n                                                    batch_size=batch_size)\n\nval_generator = ImageDataGenerator(\n    preprocessing_function=preprocess_input)\nval_batches = val_generator.flow_from_directory('../input/100-bird-species/train',\n                                                target_size=(224, 224),\n                                                batch_size=batch_size)\n\n# Note that training is set to 1 epoch, \n# to avoid unintentionally locking up computers\nmodel.fit_generator(train_batches, \n                    epochs=1, \n                    validation_data=val_batches,\n                    steps_per_epoch=len(train_batches), \n                    validation_steps=len(val_batches))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Augmentations"},{"metadata":{},"cell_type":"markdown","source":"A second technique for avoiding **overfitting** is augmenting the images, which goal it is to take the existing data points in our dataset and create brand new samples. It works by somehow modifying an image in a way which changes it, while maintaining the thruthfulness of the corresponding label. An example in our case is mirroring the images vertically, which can be implemented directly in the keras generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(1234)\n\ngenerator = ImageDataGenerator(horizontal_flip=True)\nbatches = generator.flow_from_directory('../input/100-bird-species/train',\n                                        batch_size=1,\n                                        shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using this functionality will randomly decide whether to flip the image or not each time the image is presented, theoretically yielding two samples from the single data point we started with. We can see this by visualizing the same image served from multiple batches"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 5, figsize=(15, 10))\n\nfor i in range(5):\n    batches = generator.flow_from_directory('../input/100-bird-species/train', \n                                            batch_size=1,\n                                            shuffle=False)\n    for X, y in batches:\n        ax[i].imshow(X[0].astype(np.uint8))\n        ax[i].set_title('Run {}'.format(i + 1))\n        ax[i].set_xticks([])\n        ax[i].set_yticks([])\n        break\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}