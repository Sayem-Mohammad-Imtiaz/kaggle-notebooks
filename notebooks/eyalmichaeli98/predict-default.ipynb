{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\nThis notebook was created as part of a Machine Learning academic course. Any feedback is more than welcome."},{"metadata":{},"cell_type":"markdown","source":"## Variables\nThere are 25 variables:\n\n* ID: ID of each client\n* LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit\n* SEX: Gender (1=male, 2=female)\n* EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n* MARRIAGE: Marital status (1=married, 2=single, 3=others)\n* AGE: Age in years\n* PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)\n* PAY_2: Repayment status in August, 2005 (scale same as above)\n* PAY_3: Repayment status in July, 2005 (scale same as above)\n* PAY_4: Repayment status in June, 2005 (scale same as above)\n* PAY_5: Repayment status in May, 2005 (scale same as above)\n* PAY_6: Repayment status in April, 2005 (scale same as above)\n* BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n* BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n* BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n* BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n* BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n* BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n* PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n* PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n* PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n* PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n* PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n* PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n* default.payment.next.month: Default payment (1=yes, 0=no)\n"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport sklearn as sk","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Some personal settings"},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.rcParams.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nplt.style.use('seaborn')\nsns.set(style=\"darkgrid\")\nplt.rcParams['figure.figsize'] = (10.0, 8.0)\nplt.rcParams['xtick.labelsize'] = 14 \nplt.rcParams['ytick.labelsize'] = 14 \nplt.rcParams['axes.labelsize'] = 18\n\nsns.set(font_scale=1.8)\nsns.set(style=\"darkgrid\")\n\npd.options.display.max_columns = 100\npd.options.display.max_rows = 100\npd.options.mode.chained_assignment = None\npd.set_option('display.float_format', lambda x: '%.4f' % x)\nnp.set_printoptions(formatter={'float_kind':'{:f}'.format})\n\nclass color:\n   PURPLE = '\\033[95m'\n   CYAN = '\\033[96m'\n   DARKCYAN = '\\033[36m'\n   BLUE = '\\033[94m'\n   GREEN = '\\033[92m'\n   YELLOW = '\\033[93m'\n   RED = '\\033[91m'\n   BOLD = '\\033[1m'\n   UNDERLINE = '\\033[4m'\n   END = '\\033[0m'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Default of credit card clients: predict DEFAULT\ndf = pd.read_csv('../input/default/default of credit card clients.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check how many examples and how many features are in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.shape)\ndf.shape[0] - df.dropna().shape[0]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 30000 examples and 25 columns (24 features and one label). 68 rows with NA values. Let's look at the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning & Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"Change column names to be more convenient "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.rename(columns=str.lower)\ndf = df.rename(columns={'education': 'educ', 'marriage': 'status', 'pay_0': 'pay_1'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop unneeded columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(columns=['id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns.to_list()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look for missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the number of null values is really small, we'll simply drop them."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna()\ndf.default.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's make sure all columns are as documented."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to our documentation, the PAY_n variables indicate the number of months of delay and indicates \"pay duly\" with -1. Then what is -2 and 0? It seems to me that the label has to be adjusted to 0 for pay duly. Let's fix this."},{"metadata":{"trusted":true},"cell_type":"code","source":"pay_cols = ['pay_1', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6']\npay_cols = [col.lower() for col in pay_cols]\n\nfor col in pay_cols:\n    fil = (df[col] == -2) | (df[col] == -1) | (df[col] == 0)\n    df.loc[fil, col] = 0\n    \ndf.pay_1.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Categorical values to 1-hot"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_no_dummies = df\nnon_dummy_df = df \ndf = pd.get_dummies(df)\ncols = [col.replace(' ', '_') for col in df.columns]\ndf.columns = cols\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check all values are indeed numeric"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All features are numeric, that's great."},{"metadata":{},"cell_type":"markdown","source":"# Data exploration"},{"metadata":{},"cell_type":"markdown","source":"What are the statistics of the data?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the some features distributions"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = df.sex_female.value_counts(normalize=True).plot(kind='bar')\nax.set_xlabel(\"Gender\", size=18)\nax.set_xticklabels(['Female', 'Male'], rotation = 0)\nax.tick_params(axis='both', which='major', labelsize=14)\ndf.sex_female.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = df_no_dummies.educ.value_counts(normalize=True).plot(kind='bar')\nax.set_xlabel(\"Education\")\nax.set_xticklabels(ax.get_xticklabels(), rotation = 0)#, ha=\"right\")\ndf_no_dummies.educ.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = df_no_dummies.status.value_counts(normalize=True).plot(kind='bar')\nax.set_xlabel(\"Status\")\nax.set_xticklabels(ax.get_xticklabels(), rotation = 0)  #, ha=\"right\")\ndf_no_dummies.status.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1.6)\n\nax = df.default.value_counts(normalize=True).plot(kind='bar')\nax.set_xticklabels(['Not_defaulted', 'defaulted'], rotation = 0)\nax.tick_params(axis='both', which='major', labelsize=14)\ndf.default.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"About ~22% of the clients defaulted. A small unbalanced dataset, we'll remember that."},{"metadata":{},"cell_type":"markdown","source":"Distribution of the age in our data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1.6)\n\nplt.title('Age distribution')\ndf.age.hist(bins=17)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This histogram seems reasonable(based on the fact that the minimum age is 21).\n"},{"metadata":{},"cell_type":"markdown","source":"Here we can see the distribution of the credit that was given"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1.25)\n\nplt.figure(figsize=(14,8))\nax = df.limit_bal.hist(bins=50)\nax.set_xticks(np.linspace(0, 1000000, 11))\nax.set_xlim(0,750000)\nplt.title('Credit(Limit_Bal) distribution')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1, 7):\n    col = 'pay_' + str(i)\n    print('Column', col, ':\\n', df[col].value_counts().sort_index()\\\n          .plot(kind='bar', figsize=(7,4)), '\\n\\n')\n    plt.title('value counts for column: {}'.format(col), fontsize=18)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1, 7):\n    col = 'bill_amt' + str(i)\n    print(df[col].plot(kind='hist', figsize=(7,4), bins=40), '\\n\\n')\n    plt.title('Histogram for column: {}'.format(col), fontsize=18)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the credit limit distribution VS sex."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1.8)\n\nfig, ax1 = plt.subplots(figsize=(14,8))\ns = sns.boxplot(ax = ax1, x=\"sex\", y=\"limit_bal\", hue=\"sex\",data=non_dummy_df,\\\n                palette=\"PRGn\",showfliers=True)\nplt.title(\"Limit Balance Box plot by Gender\", fontsize=24)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1.8)\n\nfig, ax1 = plt.subplots(figsize=(14, 8))\nsns.boxplot(x=\"educ\", y=\"limit_bal\", hue='sex', data=df_no_dummies)\nplt.title(\"Limit Balance Box plot by Gender & Edcuation\", fontsize=24)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1.8)\n\nfig, ax1 = plt.subplots(figsize=(14,8))\ns = sns.boxplot(ax = ax1, x=\"status\", y=\"limit_bal\", hue=\"sex\",data=non_dummy_df,\\\n                palette=\"PRGn\",showfliers=True)\nplt.title(\"Limit Balance Box plot by Status\", fontsize=24)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's see how default differs with respect to other features."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,10))\nsns.set(font_scale=2)\nsns.barplot(x=\"sex\", y=\"default\", data=non_dummy_df, hue=\"educ\", capsize=.05)\nplt.legend(loc = 'best', bbox_to_anchor=(0, 1), fontsize=18)\nplt.title(\"Default average & confidence intervals by Education level & Gender\", fontsize=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,10))\nsns.set(font_scale=2)\nsns.barplot(x=\"sex\", y=\"default\", data=non_dummy_df, hue=\"status\", capsize=.05)\nplt.legend(loc = 'upper left', bbox_to_anchor=(0, 1), fontsize=18)\nplt.title(\"Default average & confidence intervals by Status & Gender\", fontsize=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['pay_amt' + str(i) for i in range(1,7)]\ndf[cols].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's see a correlation matrix heat map, and try to find interesting relations"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"white\")\n\ncorr = df.corr()\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(10, 8))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.9, vmin=-0.9, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.title(\"Correlation Matrix\", fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now plot correlations to default"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn')\nsns.set(font_scale=1.4)\nplt.figure(figsize=(10,8))\nplt.title('Default correlation with features')\n\ndf.corr()['default'].drop('default').plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that the strongest relationships are regarding repayment status(pay_x) the link is positive, and it seems that the link decreases with the number of months before the current month.\nSame goes for the payment amount(pay_amt_x), just with negative and less significant correlations. \nAnother notable correlation is with the amount of given credit(limit_bal)"},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"Let's scale the x values"},{"metadata":{},"cell_type":"markdown","source":"Because there are different scales in our data, we'll use standard scalar which is less sensitive to outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\ndf = df.astype(float)\ndf = df.dropna()\nscale = MinMaxScaler()\ndf_to_scale = df\nscaled = scale.fit_transform(df_to_scale)\nscaled_df = pd.DataFrame(scaled, columns=df_to_scale.columns)\nscaled_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train and test split"},{"metadata":{},"cell_type":"markdown","source":"We have about 30k obs', so let's use 80% for train and 20% for test."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntest_size = round(0.2 * len(scaled_df))\ntrain, test = train_test_split(scaled_df, test_size=test_size, random_state=0, shuffle=True)\n\nlabel = 'default'\n\nx_train, y_train = train.drop(label, axis=1), train[label]\nx_test, y_test = test.drop(label, axis=1), test[label]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And let's split for NOT scaled data as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_not_scaled, test_not_scaled = train_test_split(df, test_size=test_size,\\\n                                                     random_state=0, shuffle=True)\n\nlabel = 'default'\n\nx_train_not_scaled, y_train_not_scaled = train_not_scaled.drop(label, axis=1), train_not_scaled[label]\nx_test_not_scaled, y_test_not_scaled = test_not_scaled.drop(label, axis=1), test_not_scaled[label]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape, y_train.shape, x_test.shape, y_test.shape\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation + Benchmark"},{"metadata":{},"cell_type":"markdown","source":"As this is a classification problem that is a little unbalanced in its labels, we'll use F1 & accuracy as our evaluation metric."},{"metadata":{},"cell_type":"markdown","source":"The benchmark would be the most common label in the train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.default.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case it's 0 (not survived), let's check its performance on both train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = len(df[df.default==0])/len(df)\npred = np.zeros(len(df))\nf1 = sk.metrics.f1_score(df.default, pred)\nprint('Beanchmark Accuracy:', acc)\nprint('Beanchmark F1:', f1) # it'll be 0..","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The performance on train and test is almost equal, our best algorithm should beat this performance!"},{"metadata":{},"cell_type":"markdown","source":"# Running KNN - on NOT scaled data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\ntrain_acc = []\ntest_acc = []\n\ntrain_f1 = []\ntest_f1 = []\n\nk_vals = list(range(5, 41, 5))\n\nvals = k_vals\n\nfor k in vals:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(x_train_not_scaled, y_train_not_scaled)\n    \n    train_acc.append(knn.score(x_train_not_scaled, y_train_not_scaled))\n    test_acc.append(knn.score(x_test_not_scaled, y_test_not_scaled))\n    \n    y_pred_train = knn.predict(x_train_not_scaled)\n    y_pred_test = knn.predict(x_test_not_scaled)\n\n    train_f1.append(sk.metrics.f1_score(y_train, y_pred_train))\n    test_f1.append(sk.metrics.f1_score(y_test, y_pred_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1.5)\n# This will plot the accuracies as a function of k.\n\nfig = plt.figure(figsize=(10, 8))\nax1 = fig.add_subplot()\nax1.plot(vals, train_acc, '-o', label='Training Accuracy')\nax1.plot(vals ,test_acc, '-o', label='Testing Accuracy')\nax1.set_ylabel(\"Accuracy\")\nax1.set_xlabel(\"k\")\nplt.title('KNN accuracy by K - NOT scaled data')\nplt.legend(fontsize=14)\nplt.show()\n\n# This will plot the f1 score as a function of k.\n\nfig = plt.figure(figsize=(10, 8))\nax1 = fig.add_subplot()\nax1.plot(vals, train_f1, '-o', label='Training Accuracy')\nax1.plot(vals ,test_f1, '-o', label='Testing Accuracy')\nax1.set_ylabel(\"F1 score\")\nax1.set_xlabel(\"k\")\nplt.title('KNN F1 score by K - NOT scaled data')\nplt.legend(fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It doesn't seem better than the benchmark even when when it's performing best on the test set(k = 30). Also, we know that KNN can highly suffer from features that are in different scales. So let's run it on scaled data"},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 30\nknn = KNeighborsClassifier(n_neighbors=k)\nknn.fit(x_train_not_scaled, y_train_not_scaled)\ny_pred_test = knn.predict(x_test_not_scaled)\npred_df = pd.DataFrame({'KNN_not_scaled': y_pred_test})\npred_df['Beanchmark'] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN - on SCALED data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\ntrain_acc = []\ntest_acc = []\n\ntrain_f1 = []\ntest_f1 = []\n\nk_vals = list(range(5, 41, 5))\nvals = k_vals\n\nfor k in vals:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(x_train, y_train)\n    \n    test_acc.append(knn.score(x_test, y_test))\n    train_acc.append(knn.score(x_train, y_train))\n    \n    y_pred_train = knn.predict(x_train)\n    y_pred_test = knn.predict(x_test)\n\n    train_f1.append(sk.metrics.f1_score(y_train, y_pred_train))\n    test_f1.append(sk.metrics.f1_score(y_test, y_pred_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# This will plot the accuracies as a function of k.\n\nfig = plt.figure(figsize=(10, 8))\nax1 = fig.add_subplot()\nax1.plot(vals, train_acc, '-o', label='Training Accuracy')\nax1.plot(vals ,test_acc, '-o', label='Testing Accuracy')\nax1.set_ylabel(\"Accuracy\")\nax1.set_xlabel(\"k\")\nplt.title('KNN accuracy by K - scaled data')\nplt.legend(fontsize=14)\nplt.show()\n\n\n# This will plot the f1 score as a function of k.\n\nfig = plt.figure(figsize=(10, 8))\nax1 = fig.add_subplot()\nax1.plot(vals, train_f1, '-o', label='Training F1 Score')\nax1.plot(vals ,test_f1, '-o', label='Testing F1 Score')\nax1.set_ylabel(\"F1 Score\")\nax1.set_xlabel(\"k\")\nplt.title('KNN F1 Score by K - scaled data')\nplt.legend(fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy improved. and the F1 score improved by a lot. Based on this graphs we'll choose k = 25."},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 25\nknn = KNeighborsClassifier(n_neighbors=k)\nknn.fit(x_train_not_scaled, y_train_not_scaled)\ny_pred_test = knn.predict(x_test_not_scaled)\npred_df['KNN_scaled'] = y_pred_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nmax_depth_vals = range(3, 19, 3)\n\nmin_samples_vals = range(1, 101, 10)\n\nfor depth in max_depth_vals:\n    \n    train_acc = []\n    test_acc = []\n    \n    train_f1 = []\n    test_f1 = []\n    \n    for min_sample in min_samples_vals:\n        classifier = DecisionTreeClassifier(random_state=0, \\\n                                            max_depth=depth, min_samples_leaf=min_sample)\n        classifier.fit(x_train_not_scaled, y_train_not_scaled)\n        \n        train_acc.append(classifier.score(x_train_not_scaled, y_train_not_scaled))\n        test_acc.append(classifier.score(x_test_not_scaled, y_test_not_scaled))\n        \n        y_pred_train = classifier.predict(x_train_not_scaled)\n        y_pred_test = classifier.predict(x_test_not_scaled)\n\n        train_f1.append(sk.metrics.f1_score(y_train, y_pred_train))\n        test_f1.append(sk.metrics.f1_score(y_test, y_pred_test))\n\n    # This will plot the Accuracy Scores as a function of k.\n  \n    fig, ax = plt.subplots(figsize=(7, 5))\n    plt.plot(min_samples_vals, train_acc, '-o', label = 'Training Accuracy')\n    plt.plot(min_samples_vals, test_acc, '-o', label = 'Test Accuracy')\n    ax.set_xlabel('Min Samples Leaf', fontsize=16)\n    ax.set_ylabel('Accuracy', fontsize=16)\n    plt.title('Accuracy for Decision Tree with Maximum depth = {} by min_samples leaf'.format(depth)\\\n              , fontsize=18)\n    plt.legend(fontsize=14)\n    plt.show()\n    print('\\n')\n\n    # This will plot the F1 Scores as a function of k.\n\n    fig = plt.figure(figsize=(7, 5))\n    ax1 = fig.add_subplot()\n    ax1.plot(min_samples_vals, train_f1, '-o', label = 'Training F1 Score')\n    ax1.plot(min_samples_vals, test_f1, '-o', label = 'Test F1 Score')\n    ax1.set_ylabel(\"F1 Score\", fontsize=16)\n    ax1.set_xlabel('Min Samples Leaf', fontsize=16)\n    plt.title('F1 Score for Decision Tree with Maximum depth = {} by min_samples leaf'.format(depth)\\\n              , fontsize=18)\n    plt.legend(fontsize=14)\n    plt.show()\n    print('\\n\\n\\n\\n')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on this graphs we decided to go with min_samples_leaf=60 and max_tree_depth=15"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth = 15\nmin_samples_leaf = 60\nclassifier = DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf)\nclassifier.fit(x_train_not_scaled, y_train_not_scaled)\ny_pred_test = classifier.predict(x_test_not_scaled)\npred_df['Decision_tree'] = y_pred_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's plot the Decision Tree"},{"metadata":{},"cell_type":"markdown","source":"We plot will plot this with max_depth = 6 just so we'll be able to keep track on the tree."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import export_graphviz\nfrom IPython.display import SVG\nfrom graphviz import Source\n\n\ndef plot_tree(tree, features, labels):\n    graph = Source(export_graphviz(tree, feature_names=features, class_names=labels, filled = True))\n    display(SVG(graph.pipe(format='svg')))\n\ntree = DecisionTreeClassifier(max_depth=6, min_samples_leaf=60, random_state=0)\ntree.fit(x_train, y_train)\nplot_tree(tree, features=x_train.columns, labels=['Not Default', 'Default'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nmax_depth_vals = range(3, 19, 3)\n\nn_estimators_values = range(10, 200, 20)\n\nfor depth in max_depth_vals:\n    \n    train_acc = []\n    test_acc = []\n    \n    train_f1 = []\n    test_f1 = []\n    \n    for n in n_estimators_values:\n        classifier = RandomForestClassifier(random_state=0, n_estimators=n, \\\n                                        max_depth=depth, min_samples_leaf=60) \n                                    # The paramters we decided on earlier\n        classifier.fit(x_train_not_scaled, y_train_not_scaled)\n        \n        train_acc.append(classifier.score(x_train_not_scaled, y_train_not_scaled))\n        test_acc.append(classifier.score(x_test_not_scaled, y_test_not_scaled))\n        \n        y_pred_train = classifier.predict(x_train_not_scaled)\n        y_pred_test = classifier.predict(x_test_not_scaled)\n\n        train_f1.append(sk.metrics.f1_score(y_train, y_pred_train))\n        test_f1.append(sk.metrics.f1_score(y_test, y_pred_test))\n\n    # This will plot the Accuracy Scores as a function of k.\n\n    fig, ax = plt.subplots(figsize=(7, 5))\n    plt.plot(n_estimators_values, train_acc, '-o', label = 'Training Accuracy')\n    plt.plot(n_estimators_values, test_acc, '-o', label = 'Test Accuracy')\n    ax.set_xlabel('n_estimators', fontsize=16)\n    ax.set_ylabel('Accuracy', fontsize=16)\n    plt.title('Accuracy for Random Forest with Maximum depth = {} by n_estimators'.format(depth)\\\n              , fontsize=18)\n    plt.legend(fontsize=14)\n    plt.show()\n    print('\\n')\n\n    # This will plot the F1 Scores as a function of k.\n\n    fig = plt.figure(figsize=(7, 5))\n    ax1 = fig.add_subplot()\n    ax1.plot(n_estimators_values, train_f1, '-o', label = 'Training F1 Score')\n    ax1.plot(n_estimators_values, test_f1, '-o', label = 'Test F1 Score')\n    ax1.set_ylabel(\"F1 Score\", fontsize=16)\n    ax1.set_xlabel('n_estimators', fontsize=16)\n    plt.title('F1 Score for Random Forest with Maximum depth = {} by n_estimators'.format(depth)\\\n              , fontsize=18)\n    plt.legend(fontsize=14)\n    plt.show()\n    print('\\n\\n\\n\\n')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can notice that the y axes has really small differences both on F1 and Accuracy(diff<1%).\nIt seems that 100 is the best number of trees. Let's see what is the feature importance:"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_of_trees = 100\nclassifier = RandomForestClassifier(random_state=0, n_estimators=n_of_trees, \\\n                                        max_depth=15, min_samples_leaf=60) \n                                    # The paramters we decided on earlier\nclassifier.fit(x_train_not_scaled, y_train_not_scaled)\npred_df['random_forest_pred'] = classifier.predict(x_test_not_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1.6)\n\ntmp = pd.DataFrame({'Feature': x_train.columns, 'Feature importance': classifier.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance', ascending=False)\n\nplt.figure(figsize = (10, 8))\nplt.title('Features importance of Random Forest')\ns = sns.barplot(x='Feature', y='Feature importance', data=tmp)\ns.set_xticklabels(s.get_xticklabels(), rotation=90)\nplt.show()   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# AdaBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\nmax_depth_vals = range(5, 18, 4)\n\nn_estimators_values = range(40, 161, 40)\n\nfor depth in max_depth_vals:\n    \n    train_acc = []\n    test_acc = []\n    \n    train_f1 = []\n    test_f1 = []\n    \n    for n in n_estimators_values:\n        base_estimator = DecisionTreeClassifier(max_depth=depth, min_samples_leaf=120)\n                                        # AdaBoost tends to oferfit, so we'll use more suited paramters.\n        classifier = AdaBoostClassifier(random_state=0, n_estimators=n, \\\n                                        base_estimator=base_estimator)\n        classifier.fit(x_train_not_scaled, y_train_not_scaled)\n        \n        train_acc.append(classifier.score(x_train_not_scaled, y_train_not_scaled))\n        test_acc.append(classifier.score(x_test_not_scaled, y_test_not_scaled))\n        \n        y_pred_train = classifier.predict(x_train_not_scaled)\n        y_pred_test = classifier.predict(x_test_not_scaled)\n\n        train_f1.append(sk.metrics.f1_score(y_train, y_pred_train))\n        test_f1.append(sk.metrics.f1_score(y_test, y_pred_test))\n\n    # This will plot the Accuracy Scores as a function of k.\n\n    fig, ax = plt.subplots(figsize=(7, 5))\n    plt.plot(n_estimators_values, train_acc, '-o', label = 'Training Accuracy')\n    plt.plot(n_estimators_values, test_acc, '-o', label = 'Test Accuracy')\n    ax.set_xlabel('n_estimators', fontsize=16)\n    ax.set_ylabel('Accuracy', fontsize=16)\n    plt.title('Accuracy for AdaBoost with Maximum depth = {} by n_estimators'.format(depth)\\\n              , fontsize=18)\n    plt.legend(fontsize=14)\n    plt.show()\n    print('\\n')\n\n    # This will plot the F1 Scores as a function of k.\n\n    fig = plt.figure(figsize=(7, 5))\n    ax1 = fig.add_subplot()\n    ax1.plot(n_estimators_values, train_f1, '-o', label = 'Training F1 Score')\n    ax1.plot(n_estimators_values, test_f1, '-o', label = 'Test F1 Score')\n    ax1.set_ylabel(\"F1 Score\", fontsize=16)\n    ax1.set_xlabel('n_estimators', fontsize=16)\n    plt.title('F1 Score for AdaBoost with Maximum depth = {} by n_estimators'.format(depth)\\\n              , fontsize=18)\n    plt.legend(fontsize=14)\n    plt.show()\n    print('\\n\\n\\n\\n')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can notice that there is a major overfitting. We can also notice that the y axes has really small differences both on F1 and Accuracy(usually diff<1%).\nIt seems that 40 is the best number of trees and max_depth = 5 is the best max_depth. Let's see what is the feature importance:"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_of_trees = 40\nbase_estimator = DecisionTreeClassifier(max_depth=15, min_samples_leaf=60)\n                                # The paramters we decided on earlier\nclassifier = AdaBoostClassifier(random_state=0, n_estimators=n_of_trees, base_estimator=base_estimator)\nclassifier.fit(x_train_not_scaled, y_train_not_scaled)\npred_df['AdaBoost'] = classifier.predict(x_test_not_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1.6)\n\ntmp = pd.DataFrame({'Feature': x_train.columns, 'Feature importance': classifier.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance', ascending=False)\n\nplt.figure(figsize = (10, 8))\nplt.title('Features importance of AdaBoost')\ns = sns.barplot(x='Feature', y='Feature importance', data=tmp)\ns.set_xticklabels(s.get_xticklabels(), rotation=90)\nplt.show()   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclassifier = LogisticRegression(penalty='l1', solver='liblinear')\nclassifier.fit(x_train, y_train)\n\ny_pred_test = classifier.predict(x_test)\ny_pred_train = classifier.predict(x_train)\n\nacc_train = classifier.score(x_train, y_train)\nacc_test = classifier.score(x_test, y_test)\n\nf1_train = sk.metrics.f1_score(y_train, y_pred_train)\nf1_test = sk.metrics.f1_score(y_test, y_pred_test)\n\n\npred_df['logistic_regression'] = y_pred_test\n\nprint('Accuracy for train:', acc_train)\nprint('F1 Score for train:', f1_train, '\\n')\n\nprint('Accuracy for test:', acc_test)\nprint('F1 Score for test:', f1_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Networks with pyTorch"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## train data\nclass trainData(Dataset):\n    \n    def __init__(self, X_data, y_data):\n        self.X_data = X_data\n        self.y_data = y_data\n        \n    def __getitem__(self, index):\n        return self.X_data[index], self.y_data[index]\n        \n    def __len__ (self):\n        return len(self.X_data)\n\n\ntrain_data = trainData(torch.FloatTensor(x_train.values), \n                       torch.FloatTensor(y_train))\n## test data    \nclass testData(Dataset):\n    \n    def __init__(self, X_data):\n        self.X_data = X_data\n        \n    def __getitem__(self, index):\n        return self.X_data[index]\n        \n    def __len__ (self):\n        return len(self.X_data)\n    \n\ntest_data = testData(torch.FloatTensor(x_test.values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 64\ntrain_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(dataset=test_data, batch_size=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class binaryClassification(nn.Module):\n    def __init__(self):\n        super(binaryClassification, self).__init__()\n        # Number of input features is 12.\n        self.layer_1 = nn.Linear(30, 64) \n        self.layer_2 = nn.Linear(64, 64)\n        self.layer_out = nn.Linear(64, 1) \n        \n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.1)\n        self.batchnorm1 = nn.BatchNorm1d(64)\n        self.batchnorm2 = nn.BatchNorm1d(64)\n        \n    def forward(self, inputs):\n        x = self.relu(self.layer_1(inputs))\n        x = self.batchnorm1(x)\n        x = self.relu(self.layer_2(x))\n        x = self.batchnorm2(x)\n        x = self.dropout(x)\n        x = self.layer_out(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LEARNING_RATE = 0.001\nmodel = binaryClassification()\nmodel.to(device)\nprint(model)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def binary_acc(y_pred, y_test):\n    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n\n    correct_results_sum = (y_pred_tag == y_test).sum().float()\n    acc = correct_results_sum/y_test.shape[0]\n    acc = torch.round(acc * 100)\n    \n    return acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 50\nmodel.train()\nfor e in range(1, EPOCHS+1):\n    epoch_loss = 0\n    epoch_acc = 0\n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n        \n        y_pred = model(X_batch)\n        \n        loss = criterion(y_pred, y_batch.unsqueeze(1))\n        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n        \n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n\n    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} |\\\n    Acc: {epoch_acc/len(train_loader):.3f}')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_list = []\nmodel.eval()\nwith torch.no_grad():\n    for X_batch in test_loader:\n        X_batch = X_batch.to(device)\n        y_test_pred = model(X_batch)\n        y_test_pred = torch.sigmoid(y_test_pred)\n        y_pred_tag = torch.round(y_test_pred)\n        y_pred_list.append(y_pred_tag.cpu().numpy())\n\ny_pred_list = [a.squeeze().tolist() for a in y_pred_list]\npred_df['NN'] = y_pred_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, y_pred_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from string import ascii_uppercase\nfrom sklearn.metrics import confusion_matrix\n\nplt.figure(figsize=(12, 10))\nconfm = confusion_matrix(y_test, y_pred_list)\ndf_cm = pd.DataFrame(confm, index=['Predicted No Default', 'Predicted Default'], columns=['Not Default', 'Default'])\n\nax = sns.heatmap(df_cm, cmap='Oranges', annot=True, fmt='g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Averaging models"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df['default'] = test.default.reset_index(drop=True)\np = pred_df\npred_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df = p\npred_df['model_avg'] = pred_df[['random_forest_pred', 'AdaBoost', 'NN']].mean(axis=1).round(decimals=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = []\nf1 = []\nmodels = pred_df.drop('default', axis=1).columns\n\nfor model in models:\n    acc.append(sk.metrics.accuracy_score(pred_df.default, pred_df[model]))\n    f1.append(sk.metrics.f1_score(pred_df.default, pred_df[model]))\n\ncomparison = pd.DataFrame({'Model': models, 'Accuracy': acc, 'F1 Score': f1})\ncomparison","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = []\nf1 = []\nmodels = pred_df.drop('default', axis=1).columns\n\nfor model in models:\n    acc.append(sk.metrics.accuracy_score(pred_df.default, pred_df[model]))\n    f1.append(sk.metrics.f1_score(pred_df.default, pred_df[model]))\n\ncomparison = pd.DataFrame({'Model': models, 'Accuracy': acc, 'F1 Score': f1})\ncomparison","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(18,10))\nsns.set(font_scale=1.3)\n\nvalues = comparison.Accuracy.values\nclrs = ['grey' if (x < max(values)) else 'red' for x in values ]\n\ngraph = sns.barplot(x=\"Model\", y=\"Accuracy\", data=comparison, palette= clrs)\n\nfor p in graph.patches:\n        graph.annotate('{:.0f}%'.format(p.get_height()*100), (p.get_x()+0.42, p.get_height()),\n                    ha='center', va='bottom')\nplt.title(\"Accuracy Comparison by Model\", fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(18,10))\nsns.set(font_scale=1.2)\nsns.barplot(x=\"Model\", y=\"F1 Score\", data=comparison, capsize=.05)\nplt.title(\"F1 Score Comparison by Model\", fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Performance vs. amount of data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_acc = []\ntest_acc = []\n\ntrain_f1 = []\ntest_f1 = []\n\npercents = [0.1, 0.3, 0.5, 0.7, 1]\n\nx_train_not_scaled = x_train_not_scaled.reset_index(drop=True)\ny_train_not_scaled = y_train_not_scaled.reset_index(drop=True)\n\nfor p in percents:\n    x_train_to_use = x_train_not_scaled.iloc[: int(p * len(x_train_not_scaled))]\n    y_train_to_use = y_train_not_scaled.iloc[: int(p * len(y_train_not_scaled))]\n\n    n_of_trees = 100\n    classifier = RandomForestClassifier(random_state=0, n_estimators=n_of_trees, \\\n                                            max_depth=15, min_samples_leaf=60) \n                                        # The paramters we decided on earlier\n    classifier.fit(x_train_to_use, y_train_to_use)\n    \n    test_acc.append(classifier.score(x_test_not_scaled, y_test_not_scaled))\n    train_acc.append(classifier.score(x_train_to_use, y_train_to_use))\n    \n    y_pred_train = classifier.predict(x_train_to_use)\n    y_pred_test = classifier.predict(x_test_not_scaled)\n\n    train_f1.append(sk.metrics.f1_score(y_train_to_use, y_pred_train))\n    test_f1.append(sk.metrics.f1_score(y_test_not_scaled, y_pred_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1.5)\n# This will plot the accuracies as a function of % of data.\n\nfig = plt.figure(figsize=(10, 8))\nax1 = fig.add_subplot()\nax1.plot(percents, train_acc, '-o', label='Training Accuracy')\nax1.plot(percents ,test_acc, '-o', label='Testing Accuracy')\nax1.set_ylabel(\"Accuracy\")\nax1.set_xlabel(\"% Of Data\")\nplt.title('Random Forest Accuracy by % of data')\nplt.legend(fontsize=14)\nplt.show()\n\n\n# This will plot the f1 score as a function of % of data.\n\nfig = plt.figure(figsize=(10, 8))\nax1 = fig.add_subplot()\nax1.plot(percents, train_f1, '-o', label='Training F1 Score')\nax1.plot(percents ,test_f1, '-o', label='Testing F1 Score')\nax1.set_ylabel(\"F1 Score\")\nax1.set_xlabel(\"% Of Data\")\nplt.title('Random Forest F1 Score by % of data')\nplt.legend(fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that the score doesn't improve when we go from 70% to 100%. Hence, we wouldn't use more data."},{"metadata":{},"cell_type":"markdown","source":"# Stacking models - normal average"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df = pred_df.drop('model_avg', axis=1)\npred_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.regression.linear_model import OLS\n\nx_cols = ['ln_min_wage', 'arab', 'gender', 'constant']\n\nols = OLS(pred_df.default, pred_df.drop(['default', 'Beanchmark'], axis=1)).fit()\n\nols.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dic = dict(ols.params)\nsum_params = sum(dic.values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare = pd.DataFrame(columns=['Model', 'weight'])\ncompare['Model'] = dic.keys()\ncompare['weight'] = dic.values()\nnorm_weight = [param/sum_params for param in ols.params]\ncompare['norm_weight'] = norm_weight\nc = compare\ncompare","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare = c\ncompare_t = compare.T\ncompare_t = compare_t.rename(columns=compare_t.iloc[0])\ncompare_t = compare_t.iloc[1:]\ncompare = compare_t.T\ncompare_t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df['final_model'] = compare_t.iloc[0]['KNN_not_scaled']*pred_df.KNN_not_scaled\\\n    + compare_t.iloc[0]['KNN_scaled']*pred_df.KNN_scaled\\\n    + compare_t.iloc[0]['Decision_tree']*pred_df.Decision_tree\\\n    + compare_t.iloc[0]['random_forest_pred']*pred_df.Decision_tree\\\n    + compare_t.iloc[0]['AdaBoost']*pred_df.AdaBoost\\\n    + compare_t.iloc[0]['logistic_regression']*pred_df.logistic_regression\\\n    + compare_t.iloc[0]['NN']*pred_df.NN\n\n\npred_df.final_model = pred_df.final_model.round()\npred_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df['final_norm_model'] = compare_t.iloc[1]['KNN_not_scaled']*pred_df.KNN_not_scaled\\\n    + compare_t.iloc[1]['KNN_scaled']*pred_df.KNN_scaled\\\n    + compare_t.iloc[1]['Decision_tree']*pred_df.Decision_tree\\\n    + compare_t.iloc[1]['random_forest_pred']*pred_df.Decision_tree\\\n    + compare_t.iloc[1]['AdaBoost']*pred_df.AdaBoost\\\n    + compare_t.iloc[1]['logistic_regression']*pred_df.logistic_regression\\\n    + compare_t.iloc[1]['NN']*pred_df.NN\n\n\npred_df.final_norm_model = pred_df.final_norm_model.round()\npred_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = []\nf1 = []\nmodels = pred_df.drop('default', axis=1).columns\n\nfor model in models:\n    acc.append(sk.metrics.accuracy_score(pred_df.default, pred_df[model]))\n    f1.append(sk.metrics.f1_score(pred_df.default, pred_df[model]))\n\ncomparison = pd.DataFrame({'Model': models, 'Accuracy': acc, 'F1 Score': f1})\ncomparison","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(18,10))\nsns.set(font_scale=1.3)\n\nvalues = comparison.Accuracy.values\nclrs = ['grey' if (x < max(values)) else 'red' for x in values ]\n\ngraph = sns.barplot(x=\"Model\", y=\"Accuracy\", data=comparison, palette=clrs)\n\nfor p in graph.patches:\n        graph.annotate('{:.0f}%'.format(p.get_height()*100), (p.get_x()+0.42, p.get_height()),\n                    ha='center', va='bottom')\nplt.title(\"Accuracy Comparison by Model\", fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stacking models - with Sklearn function"},{"metadata":{"trusted":true},"cell_type":"code","source":"# compare standalone models for binary classification\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom matplotlib import pyplot\n\n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    models['lr'] = LogisticRegression()\n    models['knn'] = KNeighborsClassifier()\n    models['cart'] = DecisionTreeClassifier()\n    models['svm'] = SVC()\n    models['bayes'] = GaussianNB()\n    return models\n\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n    return scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = scaled_df.drop('default', axis=1), scaled_df.default\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\n\n\ndef get_stacking():\n    # define the base models\n    level0 = list()\n    level0.append(('lr', LogisticRegression()))\n    level0.append(('knn', KNeighborsClassifier()))\n    level0.append(('cart', DecisionTreeClassifier()))\n    level0.append(('svm', SVC()))\n    level0.append(('bayes', GaussianNB()))\n    # define meta learner model\n    level1 = LogisticRegression()\n    # define the stacking ensemble\n    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_models():\n    models = dict()\n    models['lr'] = LogisticRegression()\n    models['knn'] = KNeighborsClassifier()\n    models['cart'] = DecisionTreeClassifier()\n    models['svm'] = SVC()\n    models['bayes'] = GaussianNB()\n    models['stacking'] = get_stacking()\n    return models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = scaled_df.drop('default', axis=1), scaled_df.default\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Thank you for your time!"},{"metadata":{},"cell_type":"markdown","source":"Again, any comments will be welcomed"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}