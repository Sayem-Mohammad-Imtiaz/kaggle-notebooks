{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Diagnosing Heart Disease","metadata":{}},{"cell_type":"markdown","source":"Data contains;\n\n* age - age in years\n* sex - (1 = male; 0 = female)\n* cp - chest pain type\n* trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n* chol - serum cholestoral in mg/dl\n* fbs - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n* restecg - resting electrocardiographic results\n* thalach - maximum heart rate achieved\n* exang - exercise induced angina (1 = yes; 0 = no)\n* oldpeak - ST depression induced by exercise relative to rest\n* slope - the slope of the peak exercise ST segment\n* ca - number of major vessels (0-3) colored by flourosopy\n* thal - 3 = normal; 6 = fixed defect; 7 = reversable defect\n* target - have disease or not (1=yes, 0=no)","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-13T09:25:47.155847Z","iopub.execute_input":"2021-09-13T09:25:47.156196Z","iopub.status.idle":"2021-09-13T09:25:47.160401Z","shell.execute_reply.started":"2021-09-13T09:25:47.156166Z","shell.execute_reply":"2021-09-13T09:25:47.159634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-13T09:25:47.170512Z","iopub.execute_input":"2021-09-13T09:25:47.17096Z","iopub.status.idle":"2021-09-13T09:25:47.180791Z","shell.execute_reply.started":"2021-09-13T09:25:47.170929Z","shell.execute_reply":"2021-09-13T09:25:47.179679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\nplt.style.use('seaborn')\nsns.set(style=\"darkgrid\")\nplt.rcParams['figure.figsize'] = (10.0, 8.0)\nplt.rcParams['xtick.labelsize'] = 14 \nplt.rcParams['ytick.labelsize'] = 14 \nplt.rcParams['axes.labelsize'] = 18\n\nsns.set(font_scale=1.8)\nsns.set(style=\"darkgrid\")\n\npd.options.display.max_columns = 100\npd.options.display.max_rows = 100\npd.options.mode.chained_assignment = None\npd.set_option('display.float_format', lambda x: '%.4f' % x)\nnp.set_printoptions(formatter={'float_kind':'{:f}'.format})","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:47.187439Z","iopub.execute_input":"2021-09-13T09:25:47.187987Z","iopub.status.idle":"2021-09-13T09:25:47.200088Z","shell.execute_reply.started":"2021-09-13T09:25:47.187953Z","shell.execute_reply":"2021-09-13T09:25:47.199407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import the Data","metadata":{}},{"cell_type":"code","source":"raw_df = pd.read_csv(\"../input/heart-disease-uci/heart.csv\")\n\nraw_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:47.2043Z","iopub.execute_input":"2021-09-13T09:25:47.204742Z","iopub.status.idle":"2021-09-13T09:25:47.221161Z","shell.execute_reply.started":"2021-09-13T09:25:47.204711Z","shell.execute_reply":"2021-09-13T09:25:47.220177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploring the data + Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Let's change the column names to be a bit clearer\n","metadata":{}},{"cell_type":"code","source":"raw_df.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',\n       'exercise_induced_angina', 'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'target']","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:47.22259Z","iopub.execute_input":"2021-09-13T09:25:47.222845Z","iopub.status.idle":"2021-09-13T09:25:47.226736Z","shell.execute_reply.started":"2021-09-13T09:25:47.22282Z","shell.execute_reply":"2021-09-13T09:25:47.225874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Get some basic feel for the data","metadata":{}},{"cell_type":"code","source":"raw_df.sample(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:47.2444Z","iopub.execute_input":"2021-09-13T09:25:47.24493Z","iopub.status.idle":"2021-09-13T09:25:47.264167Z","shell.execute_reply.started":"2021-09-13T09:25:47.244889Z","shell.execute_reply":"2021-09-13T09:25:47.263166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:47.265801Z","iopub.execute_input":"2021-09-13T09:25:47.266158Z","iopub.status.idle":"2021-09-13T09:25:47.285598Z","shell.execute_reply.started":"2021-09-13T09:25:47.266127Z","shell.execute_reply":"2021-09-13T09:25:47.284568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:47.287129Z","iopub.execute_input":"2021-09-13T09:25:47.287446Z","iopub.status.idle":"2021-09-13T09:25:47.337085Z","shell.execute_reply.started":"2021-09-13T09:25:47.287407Z","shell.execute_reply":"2021-09-13T09:25:47.336137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since 54% of the dataset had a heart disease, we can't infer anything by the target feature frequency in other features(such as age), becuase it has a strong bias. </br>\nThat is, we can't say that (for example) for ages 25-35 has higher probability of having a heart disease than 35-45.","metadata":{}},{"cell_type":"markdown","source":"### Understanding resting electrocardiographic results","metadata":{}},{"cell_type":"markdown","source":"From looking at the features description, I don't understand some of the features nature. <br/>\nThose features are: \n<br/>resting electrocardiographic results (values 0,1,2)\n\n<br/> Is this ordinal? <br/> I need to know this in order to decide whether or not I should perform one-hot-encoding on it (if it's ordinal, it's better to leave it as-is).","metadata":{}},{"cell_type":"markdown","source":"So, let's try to make sense of the not-understood features","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14,8))\nsns.set(font_scale=1.5)\nsns.barplot(x=\"rest_ecg\", y=\"target\", data=raw_df, ci=None)\nax.set_xlabel('rest_ecg')\nax.set_ylabel('target')\nplt.title(\"target distribution for each rest ECG value\".title(), fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:47.338536Z","iopub.execute_input":"2021-09-13T09:25:47.338807Z","iopub.status.idle":"2021-09-13T09:25:47.491874Z","shell.execute_reply.started":"2021-09-13T09:25:47.33878Z","shell.execute_reply":"2021-09-13T09:25:47.491083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Doesnt seem ordinal to me.\nLet's one-hot-encode!","metadata":{}},{"cell_type":"markdown","source":"### One-Hot-Encoding","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n\ndf = pd.get_dummies(raw_df, drop_first=True, columns=[\"chest_pain_type\", \"rest_ecg\", \"thalassemia\", \"st_slope\"])\n\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:47.493405Z","iopub.execute_input":"2021-09-13T09:25:47.493651Z","iopub.status.idle":"2021-09-13T09:25:47.720867Z","shell.execute_reply.started":"2021-09-13T09:25:47.493627Z","shell.execute_reply":"2021-09-13T09:25:47.720099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Outliers","metadata":{}},{"cell_type":"markdown","source":"Since the dataset is not big, outliers will have a significant impact. Let's boxlot some outlier suspected columns","metadata":{}},{"cell_type":"markdown","source":"Reminder: the box is 25th-75th percentiles (aka IQR). The bottom and top lines are defined by: \n</br>\ntop: 75th percentile + 1.5 * IQR\n</br>\nbottom:25th percentile - 1.5 * IQR\n","metadata":{}},{"cell_type":"code","source":"cols_to_box_plot = [\"cholesterol\", \"max_heart_rate_achieved\"]\n\nfig, ax = plt.subplots(len(cols_to_box_plot), 1, figsize=(12, 10*len(cols_to_box_plot)))\nfor i, col in enumerate(cols_to_box_plot):\n    sns.boxplot(y=col, data=df, ax=ax[i])\n    ax[i].set_title(f\"Box Plot for {col} column\".title())\n    print()\n","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:47.722353Z","iopub.execute_input":"2021-09-13T09:25:47.722603Z","iopub.status.idle":"2021-09-13T09:25:47.983417Z","shell.execute_reply.started":"2021-09-13T09:25:47.722579Z","shell.execute_reply":"2021-09-13T09:25:47.982552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's remove the outliers","metadata":{}},{"cell_type":"code","source":"def remove_outliers(df, col_names, how=\"IQR\", what_to_do=\"drop\"):\n    for col_name in col_names:\n        Q1 = df[col_name].quantile(0.25)\n        Q3 = df[col_name].quantile(0.75)\n        IQR = Q3 - Q1  # IQR is interquartile range. \n\n        filter1 = (df[col_name] >= Q1 - 1.5 * IQR) & (df[col_name] <= Q3 + 1.5 *IQR)\n\n        if what_to_do == \"drop\":\n            df = df[filter1]\n\n        if what_to_do == \"median\":\n            df.loc[filter1, col_name] = df.col_name.median()\n\n        if what_to_do == \"mean\":\n            df.loc[filter1, col_name] = df.col_name.mean()\n        break\n        \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:47.984533Z","iopub.execute_input":"2021-09-13T09:25:47.984804Z","iopub.status.idle":"2021-09-13T09:25:47.991754Z","shell.execute_reply.started":"2021-09-13T09:25:47.984777Z","shell.execute_reply":"2021-09-13T09:25:47.99079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_with_outliers = df.copy()  # to keep track of whether or not the ouliers removal improved models. And if so, which.\ndf = remove_outliers(df, [\"cholesterol\", \"max_heart_rate_achieved\"])\ndf","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:47.993458Z","iopub.execute_input":"2021-09-13T09:25:47.993839Z","iopub.status.idle":"2021-09-13T09:25:48.028001Z","shell.execute_reply.started":"2021-09-13T09:25:47.993792Z","shell.execute_reply":"2021-09-13T09:25:48.027038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"check for NAs","metadata":{}},{"cell_type":"code","source":"df.isna().any()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:48.029579Z","iopub.execute_input":"2021-09-13T09:25:48.029976Z","iopub.status.idle":"2021-09-13T09:25:48.03987Z","shell.execute_reply.started":"2021-09-13T09:25:48.029937Z","shell.execute_reply":"2021-09-13T09:25:48.038608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great! no NAs at all                     ","metadata":{}},{"cell_type":"markdown","source":"Let's explore some more","metadata":{}},{"cell_type":"code","source":"df.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:48.042438Z","iopub.execute_input":"2021-09-13T09:25:48.042786Z","iopub.status.idle":"2021-09-13T09:25:48.052681Z","shell.execute_reply.started":"2021-09-13T09:25:48.042751Z","shell.execute_reply":"2021-09-13T09:25:48.051978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's well known that older people tend to have higher possibilty for heart disease, I wonder in what age does this start to reflect, and by how much. Let's check","metadata":{}},{"cell_type":"markdown","source":"Let's create a temp df with one-hot-encoding, without throwing the first categorical value, </br>\nto better understand the corrleation matrix\n","metadata":{}},{"cell_type":"code","source":"temp_df = pd.get_dummies(raw_df, drop_first=False, columns=[\"chest_pain_type\", \"rest_ecg\", \"thalassemia\", \"st_slope\"])\n","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:48.054011Z","iopub.execute_input":"2021-09-13T09:25:48.054326Z","iopub.status.idle":"2021-09-13T09:25:48.06837Z","shell.execute_reply.started":"2021-09-13T09:25:48.05429Z","shell.execute_reply":"2021-09-13T09:25:48.067611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlations","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(22, 16))\nsns.heatmap(temp_df.corr(), annot=True, fmt='.1f', cmap='BrBG', vmax=1, vmin=-1)\nplt.title(\"DataFrame Correlation Matrix\\n\", fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:48.069443Z","iopub.execute_input":"2021-09-13T09:25:48.069934Z","iopub.status.idle":"2021-09-13T09:25:50.688488Z","shell.execute_reply.started":"2021-09-13T09:25:48.06989Z","shell.execute_reply":"2021-09-13T09:25:50.687186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It doesn't seem there's a strong Multicollinearity in the data. </br>\nPerhaps st_slope and st_depression (0.6) are strognly correlated and hence, worth considiration in terms of whether or not to remove one of them.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8, 12))\nheatmap = sns.heatmap(temp_df.corr()[['target']].sort_values(by='target', ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Features Correlating with the Target feature'.title(), fontdict={'fontsize': 18}, pad=16);","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:50.690133Z","iopub.execute_input":"2021-09-13T09:25:50.690558Z","iopub.status.idle":"2021-09-13T09:25:51.276376Z","shell.execute_reply.started":"2021-09-13T09:25:50.690515Z","shell.execute_reply":"2021-09-13T09:25:51.275423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"code","source":"df[\"age_sq\"] = df.age ** 2\ndf[\"age_sex\"] = df.age * df.sex","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:51.277552Z","iopub.execute_input":"2021-09-13T09:25:51.277833Z","iopub.status.idle":"2021-09-13T09:25:51.284425Z","shell.execute_reply.started":"2021-09-13T09:25:51.277806Z","shell.execute_reply":"2021-09-13T09:25:51.283519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Scalling","metadata":{}},{"cell_type":"markdown","source":"We'll try both Normalization & standadization, and see which one performes better.","metadata":{}},{"cell_type":"markdown","source":"## Standardization","metadata":{}},{"cell_type":"markdown","source":"Standardization: Will scale the input to have mean of 0 and variance of 1. $$X_{stand} = \\frac{X - \\mu}{\\sigma}$$\n","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler \n\ndesc_df = df.describe()\ncols = [col for col in df.columns if desc_df[col][\"max\"] != 1.0]  # if the max is 1, then it's a dummy var (I checked)\n\nscaler = StandardScaler()\n\nstndrd_df = df.copy()\nstndrd_df[cols] = scaler.fit_transform(df[cols])\n\nstndrd_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:51.285577Z","iopub.execute_input":"2021-09-13T09:25:51.28602Z","iopub.status.idle":"2021-09-13T09:25:51.403534Z","shell.execute_reply.started":"2021-09-13T09:25:51.285994Z","shell.execute_reply":"2021-09-13T09:25:51.40284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normalization","metadata":{}},{"cell_type":"markdown","source":"Min Max Scaling: Will scale the input to have minimum of 0 and maximum of 1. </br> That is, it scales the data in the range of [0, 1] This is useful when the parameters have to be on same positive scale. But in this case, the outliers are lost. $$X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\n\ncols = df.columns\nscaler = MinMaxScaler()\nnorm_df = df.copy()\nnorm_df[cols] = scaler.fit_transform(df[cols])\n\nnorm_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:51.404444Z","iopub.execute_input":"2021-09-13T09:25:51.404795Z","iopub.status.idle":"2021-09-13T09:25:51.474538Z","shell.execute_reply.started":"2021-09-13T09:25:51.404768Z","shell.execute_reply":"2021-09-13T09:25:51.473625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dummy df (for Bernoulli Naive bayes)","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\n\ncontinues_cols = [\"age\", \"age_sex\", \"age_sq\", \"resting_blood_pressure\", \"cholesterol\", \"max_heart_rate_achieved\", \"st_depression\"]\ncategory_cols = [\"num_major_vessels\"]\ndummy_cols = [col for col in df.columns if col not in continues_cols + category_cols + [\"target\"]]\n\ndummy_df = df.copy()\n\ndummy_df = pd.get_dummies(dummy_df, drop_first=True, columns=category_cols)\n\nfor col in continues_cols:\n    try:\n        dummy_df[col] = pd.qcut(dummy_df[col], 3, labels=[1, 2, 3], duplicates='drop')\n    except ValueError as e:\n        print(col, e, sep=\"\\n\")\n        dummy_df[col] = pd.qcut(dummy_df[col], q=[0.1, 0.5, 0.8], labels=[1, 2], duplicates='drop')\n\nenc = OrdinalEncoder()\ndummy_df_ = enc.fit_transform(dummy_df)\n\ndummy_df = pd.DataFrame(dummy_df_, columns=dummy_df.columns)\ndummy_df = pd.get_dummies(dummy_df, drop_first=True, columns=continues_cols)\n\n\n[col for col in dummy_df.columns if max(dummy_df[col]) != 1.0]  # make sure all cols are binary","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:51.475867Z","iopub.execute_input":"2021-09-13T09:25:51.476151Z","iopub.status.idle":"2021-09-13T09:25:51.524921Z","shell.execute_reply.started":"2021-09-13T09:25:51.476124Z","shell.execute_reply":"2021-09-13T09:25:51.523949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ML models","metadata":{}},{"cell_type":"markdown","source":"## First, Benchmark","metadata":{}},{"cell_type":"markdown","source":"The benchmark would be the most common label in the train set","metadata":{}},{"cell_type":"code","source":"counts = df.target.value_counts(dropna=False)\ncounts","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:51.526163Z","iopub.execute_input":"2021-09-13T09:25:51.526419Z","iopub.status.idle":"2021-09-13T09:25:51.532573Z","shell.execute_reply.started":"2021-09-13T09:25:51.526387Z","shell.execute_reply":"2021-09-13T09:25:51.53196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset is balanced, so it'll probably be easy to get better accuracy than the beanchmark.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score \n\n\nacc = len(df[df.target==counts.sort_values(ascending=False).index[0]])/len(df)\npred = np.ones(len(df))\nf1 = f1_score(df.target, pred)\n\nprint('Beanchmark Accuracy:', acc)\nprint('Beanchmark F1:', f1) # it'll be 0..","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:51.533686Z","iopub.execute_input":"2021-09-13T09:25:51.534198Z","iopub.status.idle":"2021-09-13T09:25:51.54893Z","shell.execute_reply.started":"2021-09-13T09:25:51.534167Z","shell.execute_reply":"2021-09-13T09:25:51.547979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's create some variables and dfs that will help with the help functions","metadata":{}},{"cell_type":"code","source":"dfs = {\"Normalized_df\": norm_df, \"Standadized_df\": stndrd_df, \"not_scaled_df\": df, \"not_scaled_with_outliers_df\": df_with_outliers}\ndfs_w_dummy = {\"Normalized_df\": norm_df, \"Standadized_df\": stndrd_df, \"not_scaled_df\": df, \"not_scaled_with_outliers_df\": df_with_outliers, \"dummy_df\": dummy_df}  \n# perhaps I won't want to try all models with dummy as well. It's mainly for NB\n\ntarget_var = \"target\"\n\nevaluations = [\"avg_test_accuracy\", \"avg_test_f1\"]\n\nindex = [\n    np.repeat(list(dfs_w_dummy.keys()), len(evaluations)),\n    evaluations * len(dfs_w_dummy)\n        ]\n\nbeanchmark_vals = np.array([acc, f1])\nbeanchmark_vals = np.tile(beanchmark_vals, len(dfs_w_dummy))\n\nevaluation_df = pd.DataFrame(index=index)\nevaluation_df[\"beanchmark\"] = beanchmark_vals\n\nevaluation_df","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:51.550371Z","iopub.execute_input":"2021-09-13T09:25:51.550882Z","iopub.status.idle":"2021-09-13T09:25:51.567462Z","shell.execute_reply.started":"2021-09-13T09:25:51.550795Z","shell.execute_reply":"2021-09-13T09:25:51.566576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Helper function, to evaluate a model","metadata":{"execution":{"iopub.status.busy":"2021-09-04T12:54:12.181164Z","iopub.execute_input":"2021-09-04T12:54:12.181525Z","iopub.status.idle":"2021-09-04T12:54:12.185898Z","shell.execute_reply.started":"2021-09-04T12:54:12.181496Z","shell.execute_reply":"2021-09-04T12:54:12.184937Z"}}},{"cell_type":"code","source":"from numpy import mean\nfrom numpy import std\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_validate\n\n\n\ndef evaluate(classifier, dfs=dfs, k=4):\n    \"\"\"\n    Description: Given a classifier, list of dataframes and k, computes the accuracy and f1 scores with KFOLD, and stores it in evaluation_df for further analysis and comparison.\n    params:\n    classifier: classifier object\n    dfs: list of dataframes. The rational for more than 1 df, is in case you'll want to compare different preprocessed datasets, such as: different features, saclling, etc.\n    k: number of k in KFOLD\n    returns: a DataFrame object\n    \"\"\"\n    clf_name = classifier.__class__.__name__\n    print(f\"For model: {clf_name}\")\n    \n    for i, df_item in enumerate(dfs.items()):\n        df_name = df_item[0]\n        df_ = df_item[1]\n        \n        X = df_.loc[:, df_.columns != target_var]\n\n        y = df_[target_var]\n        \n        scores = cross_validate(classifier, X, y, cv=k, scoring=('accuracy', 'f1'), return_train_score=True)\n        avg_test_accuracy = scores['test_accuracy'].mean()\n        avg_test_f1 = scores['test_f1'].mean()\n        \n        print(f\"\\nfor dataframe {df_name}:\")\n        print(f\"{avg_test_accuracy:.3f} accuracy with a standard deviation of {scores['test_accuracy'].std():.3f}\")\n        print(f\"{avg_test_f1:.3f} f1 with a standard deviation of {scores['test_f1'].std():.3f}\")\n        print(\"Train accuracy:\", scores[\"train_accuracy\"])\n        print((df_name, evaluations), clf_name)\n        evaluation_df.loc[(df_name, evaluations), clf_name] =  [avg_test_accuracy, avg_test_f1]\n\n    \n    print(\"\\n\")\n    \n    return evaluation_df\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:51.568586Z","iopub.execute_input":"2021-09-13T09:25:51.569007Z","iopub.status.idle":"2021-09-13T09:25:51.583763Z","shell.execute_reply.started":"2021-09-13T09:25:51.568968Z","shell.execute_reply":"2021-09-13T09:25:51.582447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Helper function, to perform a grid search","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n\ndef grid_search(model, grid: dict, dfs=dfs, k=4, to_print=True):\n    \"\"\"\n    Description: Performing a GridSearch given model, dfs and paramters\n    params:\n    model: model object that is working with GridSearchCV\n    grid: a dict object(we can iterate over it: list, tuple, numpy array) grid which has param name and param values\n    dfs: list of dataframes. The rational for more than 1 df, is in case you'll want to compare different preprocessed datasets, such as: different features, saclling, etc.\n    k: number of k in KFOLD\n    returns: a dict object, best_params ***on one of the given datasets (if its performed way better than others, we'll see it in evaluation_df)***\n    \"\"\"\n    max_acc = -1\n    for df_name, df_ in dfs.items():\n        X = df_.loc[:, df_.columns != target_var]\n        y = df_[target_var]\n\n        grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=k, scoring='accuracy',error_score=0)\n        grid_result = grid_search.fit(X, y)\n    \n        print(f\"\\nFor {df_name}:\")\n        print(\"Best: %.3f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n        \n        if grid_result.best_score_ > max_acc: # if this accuracy is higher than the last one, then change the best_params to this one and the max_acc var\n            best_params = grid_result.best_params_ \n            max_acc = grid_result.best_score_\n            \n        means = grid_result.cv_results_['mean_test_score']\n        stds = grid_result.cv_results_['std_test_score']\n        params = grid_result.cv_results_['params']\n        \n        if to_print == True:\n            for mean, stdev, param in zip(means, stds, params):\n                print(\"%.3f (%.3f) with: %r\" % (mean, stdev, param))\n        \n    return best_params # NOTE: That's returning the best performed params, that performed best on a given dataset","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:51.585046Z","iopub.execute_input":"2021-09-13T09:25:51.585361Z","iopub.status.idle":"2021-09-13T09:25:51.598191Z","shell.execute_reply.started":"2021-09-13T09:25:51.585326Z","shell.execute_reply":"2021-09-13T09:25:51.597153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Naive Bayes","metadata":{}},{"cell_type":"markdown","source":"### There are deifferent kinds of naive bayes classifiers. </br>\nall bayes classifiers obey to this equation: p(y | x1; x2...; xn) = p (x1; x2...; xn | y) * p(y) / p(x).  </br>\n* Multinomial - the most common one, assumes discrete features. simple conditional probability with the probability being simply the frequency of each feature in each class.  </br>\n* Gaussian - assumes the likelihood probabilities follow Gaussian distribution.  </br>\n* Bernoully - assumes bernoully distribution of the features(binary).\n* Categorical - assumes features are categorical","metadata":{}},{"cell_type":"markdown","source":"For the Bernoully naive bayes, we need to binarize values, so we will only use the standardized df(has both negative and positive values, approx in the 50 percentile) with different binarize values (ex: binarize=0 is to cut it to approx half 0 and half 1). ","metadata":{}},{"cell_type":"code","source":"stndrd_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:51.599535Z","iopub.execute_input":"2021-09-13T09:25:51.599841Z","iopub.status.idle":"2021-09-13T09:25:51.672272Z","shell.execute_reply.started":"2021-09-13T09:25:51.599811Z","shell.execute_reply":"2021-09-13T09:25:51.671348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nfrom sklearn.naive_bayes import BernoulliNB\n\n\nbinarize = [-0.5, 0, 0.5]\nstandard_dfs = {\"Standadized_df\": stndrd_df}\ngrid = dict(binarize=binarize)\nmodel = BernoulliNB()\nbest_params = grid_search(model, grid, to_print=False, dfs=dfs_w_dummy)\n\n\n##########################################################################################\n\n\nmodel = BernoulliNB(**best_params)\n\nevaluate(model, k=4, dfs=dfs_w_dummy)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:51.673747Z","iopub.execute_input":"2021-09-13T09:25:51.67416Z","iopub.status.idle":"2021-09-13T09:25:54.129981Z","shell.execute_reply.started":"2021-09-13T09:25:51.674116Z","shell.execute_reply":"2021-09-13T09:25:54.129142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Makes sense that BernoulliNB will work best on the standardized df(with binary=0 or close to it). </br> However, it doesn't seem siginificant, which probably means that the already-binary(those the model didn't binarize) features has the most impact on the model.","metadata":{}},{"cell_type":"markdown","source":"Now, let's try with the dummy df (binarize paramter in BernoulliNB makes it 2 categories features). </br>\nNote: BernoulliNB on standadized df is only 2 categories while the dummy df has 3 categories for the continues values. it shouldn't behave too different.</br>\nLet's see if it's better that way. </br>","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\n\nmodel = GaussianNB()\n\nevaluate(model, k=4, dfs=dfs_w_dummy)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:54.133393Z","iopub.execute_input":"2021-09-13T09:25:54.133659Z","iopub.status.idle":"2021-09-13T09:25:54.35678Z","shell.execute_reply.started":"2021-09-13T09:25:54.133631Z","shell.execute_reply":"2021-09-13T09:25:54.356088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note to self: no need for categorical NB because bernoully NB takes its place.\n</br>\nAlso, multinominal is not relavent beacuse it's not suited for both continues and binary data. It needs discrite values.","metadata":{}},{"cell_type":"markdown","source":"# Logistic regression","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom sklearn.linear_model import LogisticRegression\n\n\nmodel = LogisticRegression()\n\nsolvers = ['newton-cg', 'liblinear']\npenalty = ['l1', 'l2']\nc_values = (0.1, 1, 2, 5, 10, 20, 30, 40, 50)\nn_jobs = [-1]\n\ngrid = dict(solver=solvers, penalty=penalty, C=c_values)\n\nbest_params = grid_search(model, grid, to_print=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:54.358251Z","iopub.execute_input":"2021-09-13T09:25:54.358511Z","iopub.status.idle":"2021-09-13T09:25:59.636278Z","shell.execute_reply.started":"2021-09-13T09:25:54.358486Z","shell.execute_reply":"2021-09-13T09:25:59.635236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n\nmodel = LogisticRegression(**best_params)\n\nevaluate(model, k=4, dfs=dfs_w_dummy)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:25:59.637434Z","iopub.execute_input":"2021-09-13T09:25:59.637685Z","iopub.status.idle":"2021-09-13T09:26:00.399881Z","shell.execute_reply.started":"2021-09-13T09:25:59.637661Z","shell.execute_reply":"2021-09-13T09:26:00.398876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Support Vector Machines","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom sklearn.svm import SVC\n\n\nmodel = SVC()\n\nkernel = ['poly', 'rbf', 'sigmoid']\nC = np.linspace(0.1, 20, 4)\ndegree = [2, 3]\ncoef0 = [1, 5]\n\ngrid = dict(kernel=kernel, C=C, degree=degree, coef0=coef0)\n\nbest_params = grid_search(model, grid, to_print=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:26:00.400995Z","iopub.execute_input":"2021-09-13T09:26:00.401281Z","iopub.status.idle":"2021-09-13T09:26:03.661457Z","shell.execute_reply.started":"2021-09-13T09:26:00.401252Z","shell.execute_reply":"2021-09-13T09:26:03.660482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n\nmodel = SVC(**best_params)\n\nevaluate(model, k=4)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:26:03.662971Z","iopub.execute_input":"2021-09-13T09:26:03.663372Z","iopub.status.idle":"2021-09-13T09:26:04.006291Z","shell.execute_reply.started":"2021-09-13T09:26:03.663319Z","shell.execute_reply":"2021-09-13T09:26:04.005283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KNN","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nmodel = KNeighborsClassifier()\n\nn_neighbors = range(3, 24, 3)\nweights = ['uniform', 'distance']\nmetric = ['minkowski']  # remember that minkowski can be manhattan or euclidean (p=1, p=2, respectively)\np = [1, 2, 3, 4]  # for minkowski\n\n\ngrid = dict(n_neighbors=n_neighbors, weights=weights, metric=metric, p=p, n_jobs=n_jobs)\n\nbest_params = grid_search(model, grid)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:26:04.007776Z","iopub.execute_input":"2021-09-13T09:26:04.008183Z","iopub.status.idle":"2021-09-13T09:26:29.949653Z","shell.execute_reply.started":"2021-09-13T09:26:04.008144Z","shell.execute_reply":"2021-09-13T09:26:29.94862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n\nmodel = KNeighborsClassifier(**best_params)\n\nevaluate(model, k=4)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:26:29.951398Z","iopub.execute_input":"2021-09-13T09:26:29.951695Z","iopub.status.idle":"2021-09-13T09:26:30.62778Z","shell.execute_reply.started":"2021-09-13T09:26:29.951667Z","shell.execute_reply":"2021-09-13T09:26:30.626682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nmodel = DecisionTreeClassifier()\n\nmin_samples_leaf = np.linspace(5, 20, 4).astype(int)\nmax_depth = [3, 7, 9]\n\n\ngrid = dict(max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n\n\nbest_params = grid_search(model, grid)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:26:30.629332Z","iopub.execute_input":"2021-09-13T09:26:30.629733Z","iopub.status.idle":"2021-09-13T09:26:31.373798Z","shell.execute_reply.started":"2021-09-13T09:26:30.62969Z","shell.execute_reply":"2021-09-13T09:26:31.372826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n\nmodel = DecisionTreeClassifier(**best_params)\n\nevaluate(model, k=4, dfs=dfs_w_dummy)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:26:31.374954Z","iopub.execute_input":"2021-09-13T09:26:31.37525Z","iopub.status.idle":"2021-09-13T09:26:31.598938Z","shell.execute_reply.started":"2021-09-13T09:26:31.375221Z","shell.execute_reply":"2021-09-13T09:26:31.598045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nmodel = RandomForestClassifier()\n\n\nn_estimators = [10, 100, 500, 1000]\nmin_samples_leaf = np.linspace(5, 20, 4).astype(int)\nmax_depth = [3, 7, 9]\n\n\ngrid = dict(n_estimators=n_estimators, max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n\n\nbest_params = grid_search(model, grid, to_print=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:26:31.600096Z","iopub.execute_input":"2021-09-13T09:26:31.600354Z","iopub.status.idle":"2021-09-13T09:29:50.32646Z","shell.execute_reply.started":"2021-09-13T09:26:31.600328Z","shell.execute_reply":"2021-09-13T09:29:50.325423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n\nmodel = RandomForestClassifier(**best_params)\n\nevaluate(model, k=4)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:29:50.32793Z","iopub.execute_input":"2021-09-13T09:29:50.32827Z","iopub.status.idle":"2021-09-13T09:29:50.774177Z","shell.execute_reply.started":"2021-09-13T09:29:50.328239Z","shell.execute_reply":"2021-09-13T09:29:50.773195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AdaBoost","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\nmodel = AdaBoostClassifier()\n\n\nn_estimators = [10, 100, 500, 700, 1000]\nlearning_rate = [0.001, 0.01, 0.1]\n\n\ngrid = dict(learning_rate=learning_rate, n_estimators=n_estimators)\n\n\nbest_params = grid_search(model, grid)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:29:50.775691Z","iopub.execute_input":"2021-09-13T09:29:50.776068Z","iopub.status.idle":"2021-09-13T09:31:11.502237Z","shell.execute_reply.started":"2021-09-13T09:29:50.776028Z","shell.execute_reply":"2021-09-13T09:31:11.5013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n\nmodel = AdaBoostClassifier(**best_params)\n\nevaluate(model, k=4)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:31:11.503767Z","iopub.execute_input":"2021-09-13T09:31:11.504049Z","iopub.status.idle":"2021-09-13T09:31:26.037048Z","shell.execute_reply.started":"2021-09-13T09:31:11.504021Z","shell.execute_reply":"2021-09-13T09:31:26.036115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n\nmodel = GradientBoostingClassifier()\n\nn_estimators = [10, 100, 1000]\nlearning_rate = [0.001, 0.01, 0.1]\nsubsample = [0.5, 0.7, 1.0]\nmax_depth = [3, 7, 9]\n\n\ngrid = dict(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, max_depth=max_depth)\n\nbest_params = grid_search(model, grid, to_print=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:31:26.038231Z","iopub.execute_input":"2021-09-13T09:31:26.038512Z","iopub.status.idle":"2021-09-13T09:38:38.312608Z","shell.execute_reply.started":"2021-09-13T09:31:26.038486Z","shell.execute_reply":"2021-09-13T09:38:38.311374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n\nmodel = GradientBoostingClassifier(**best_params)\n\nevaluate(model, k=4)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:38:38.314399Z","iopub.execute_input":"2021-09-13T09:38:38.314807Z","iopub.status.idle":"2021-09-13T09:38:39.915132Z","shell.execute_reply.started":"2021-09-13T09:38:38.314763Z","shell.execute_reply":"2021-09-13T09:38:39.914058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Networks","metadata":{}},{"cell_type":"markdown","source":"To be continued...","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"evaluation_df","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:38:39.916404Z","iopub.execute_input":"2021-09-13T09:38:39.916693Z","iopub.status.idle":"2021-09-13T09:38:39.933848Z","shell.execute_reply.started":"2021-09-13T09:38:39.916664Z","shell.execute_reply":"2021-09-13T09:38:39.932752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluation_df1 = evaluation_df.groupby(level=[1]).max() * 100 \nevaluation_df1 = evaluation_df1.T.reset_index(col_fill=\"model\")\nevaluation_df1.rename(columns={'index': 'model'}, inplace=True)\nevaluation_df1","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:38:39.935128Z","iopub.execute_input":"2021-09-13T09:38:39.935428Z","iopub.status.idle":"2021-09-13T09:38:39.956841Z","shell.execute_reply.started":"2021-09-13T09:38:39.93539Z","shell.execute_reply":"2021-09-13T09:38:39.955651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = evaluation_df1.avg_test_accuracy\n\n\n\nfig, ax = plt.subplots(figsize=(20, 10))\nplt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\nplot = sns.barplot(x=evaluation_df1.model, y=y)\nplt.title(\"Test set/s Mean accuracy %\".title())\n\n# annotate the accuracy\nfor bar in plot.patches:\n    plot.annotate(format(bar.get_height(), '.2f'),\n                   (bar.get_x() + bar.get_width() / 2,\n                    bar.get_height()), ha='center', va='center',\n                   size=15, xytext=(0, 5),\n                   textcoords='offset points')\n    \nfor bar in ax.patches:\n    bar.set_facecolor('#888888')\n\nhighlight = evaluation_df1.iloc[y.idxmax()].model\n\npos = y.idxmax()\n\nax.patches[pos].set_facecolor('#aa3333')","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:38:39.958046Z","iopub.execute_input":"2021-09-13T09:38:39.958353Z","iopub.status.idle":"2021-09-13T09:38:40.342899Z","shell.execute_reply.started":"2021-09-13T09:38:39.958325Z","shell.execute_reply":"2021-09-13T09:38:40.342235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that this accuracies are a result of the best-performed hyperparamters & data type.","metadata":{}},{"cell_type":"code","source":"y = evaluation_df1.avg_test_f1\n\n\nfig, ax = plt.subplots(figsize=(20, 10))\nplt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\nplot = sns.barplot(x=evaluation_df1.model, y=y)\nplt.title(\"Test set/s Mean f1 score %\".title())\n\n# annotate the accuracy\nfor bar in plot.patches:\n    plot.annotate(format(bar.get_height(), '.2f'),\n                   (bar.get_x() + bar.get_width() / 2,\n                    bar.get_height()), ha='center', va='center',\n                   size=15, xytext=(0, 5),\n                   textcoords='offset points')\n    \nfor bar in ax.patches:\n    bar.set_facecolor('#888888')\n\nhighlight = evaluation_df1.iloc[y.idxmax()].model\n\npos = y.idxmax()\n\nax.patches[pos].set_facecolor('#aa3333')","metadata":{"execution":{"iopub.status.busy":"2021-09-13T09:38:40.343803Z","iopub.execute_input":"2021-09-13T09:38:40.34404Z","iopub.status.idle":"2021-09-13T09:38:40.723038Z","shell.execute_reply.started":"2021-09-13T09:38:40.344016Z","shell.execute_reply":"2021-09-13T09:38:40.722262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## And the winner is ... (Drumroll...) ...","metadata":{}},{"cell_type":"markdown","source":"# Logistic Regression!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}