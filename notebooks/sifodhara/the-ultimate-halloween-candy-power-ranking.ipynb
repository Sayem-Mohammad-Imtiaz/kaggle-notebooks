{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"candy_data = pd.read_csv('/kaggle/input/the-ultimate-halloween-candy-power-ranking/candy-data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"candy_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets check shape of the dataframe\ncandy_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets check some info about candy data \ncandy_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"there is no null data in the dataset\n*our task is to predict whether a candy has chocolate in it or not*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets visualise the data\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5), dpi=80)\nplt.subplot(1,4,1)\nsns.countplot(x=candy_data['chocolate'])\nplt.title(\"candy contains chocolate\")\n\nplt.subplot(1,4,2)\nsns.countplot(x=candy_data['fruity'])\n\nplt.title(\"candy contains fruity\")\n\nplt.subplot(1,4,3)\nsns.countplot(x=candy_data['caramel'])\nplt.title(\"candy contains caramel\")\n\n\nplt.subplot(1,4,4)\nsns.countplot(x=candy_data['peanutyalmondy'])\nplt.title(\"candy contains peanutyalmondy\")\n#ax.set_yscale('log')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def map_type(x,y):\n    if x == 1 and y==0:\n        return(\"hard\")\n    elif x==0 and y==1:\n        return (\"bar\")\n    elif x==0 and y==0:\n        return(\"soft\")\n    elif x==1 and y==1:\n        return(\"soft\")\n\ncandy_data['type'] = candy_data[['hard','bar']].apply(lambda x: map_type(x['hard'],x['bar']) , axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5), dpi=80)\nplt.subplot(1,4,1)\nsns.countplot(x=candy_data['nougat'])\nplt.title(\"candy contains nougat\")\n\nplt.subplot(1,4,2)\nsns.countplot(x=candy_data['crispedricewafer'])\n\nplt.title(\"candy contains crispedricewafer\")\n\nplt.subplot(1,4,3)\nsns.countplot(x=candy_data['type'])\nplt.title(\"type of candy\")\n\n\nplt.subplot(1,4,4)\nsns.countplot(x=candy_data['pluribus'])\nplt.title(\"pluribus\")\n#ax.set_yscale('log')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"maximum candies are soft type and more candies are sold in box rather than single","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5), dpi=80)\nplt.subplot(1,3,1)\nsns.distplot(candy_data['sugarpercent'])\nplt.title(\"distribution of sugarpercent\")\n\nplt.subplot(1,3,2)\nsns.distplot(candy_data['pricepercent'])\nplt.title(\"distribution of pricepercent\")\n\n\n\n\nplt.subplot(1,3,3)\nsns.distplot(candy_data['winpercent'])\nplt.title(\"distribution of winpercent\")\n#ax.set_yscale('log')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"sugarpercent,pricepercent and winpercent are normally distributed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## creating a feature that tells us one certain candy what type of features contains\ncandy_data['features'] = candy_data['chocolate']+candy_data['fruity']+candy_data['caramel']+candy_data['peanutyalmondy']+candy_data['nougat']+candy_data['crispedricewafer']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,20), dpi=80)\n#plt.subplot(1,4,1)\n\nsns.barplot(x=\"features\",y=\"competitorname\",data=candy_data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"some candies like snickers,snickes crisper and baby ruth has lots of features and flavors","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Let's Answer some Questions Regarding Ranking Among Candies","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* top voted candies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"top_candies_win = candy_data.sort_values(by='winpercent',ascending=False)\ntop_candies_win.head(10)\n## top 10 voted candies","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"all top voted ten candies has chocolate as their ingredient","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## let's look at least voted candies\ntop_candies_win.tail(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* top sugary candies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"top_candies_sugary= candy_data.sort_values(by='sugarpercent',ascending=False)\ntop_candies_sugary.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"sugary candies are not much popular as per votes","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* top costly candies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"top_candies_costly = candy_data.sort_values(by='pricepercent',ascending = False)\ntop_candies_costly.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"costly candies are not much popular as per votes and maximum costly canides contain chocolates","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Let's Create A Logistic Regression Model To Predict Whether A Candy Contains Chocolate Or Not","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## divided the data set into train and test\nfrom sklearn.model_selection import train_test_split\ndf_train,df_test = train_test_split(candy_data,train_size=0.7,test_size=0.3,random_state=100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## checking heat map of variables\ncandy_data_corr = df_train[['chocolate','fruity','caramel','peanutyalmondy','nougat','crispedricewafer','hard','bar','pluribus','sugarpercent','pricepercent','winpercent','features']]\nsns.heatmap(candy_data_corr.corr(),annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets scale some variables for use in our predictive model\nimport sklearn\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscale_var = ['winpercent','features']\ncandy_data_corr[scale_var] = scaler.fit_transform(candy_data_corr[scale_var])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets check the head once \ncandy_data_corr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## divided train set into x and y \n##x - predictors\n##y - we are going to predict this variable\ny_train = candy_data_corr.pop('chocolate')\nX_train = candy_data_corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## using RFE for feature selection in our model\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg,7)\nrfe = rfe.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = X_train.columns[rfe.support_]\ncols\n## feature selected by rfe ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets run one logistic regression model using features selected by rfe\nimport statsmodels.api as sm \nmodel1 = sm.GLM(y_train,sm.add_constant(X_train[cols]),family=sm.families.Binomial())\nres = model1.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"some feature's p value is really high need to drop some feature after checking vif of each","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets look for variance inflation factor of features \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['features'] = X_train[cols].columns\nvif['vif'] = [variance_inflation_factor(X_train[cols].values,i) for i in range(X_train[cols].shape[1])]\nvif['vif'] = round(vif['vif'], 2)\nvif = vif.sort_values(by='vif',ascending=False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's remove 'bar' and re run the model.As per our model this feature is statistically unfit in the data  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train[cols]\nX_train_new.drop('crispedricewafer',axis=1,inplace=True)\n## drop crispedricewafer due to its high p value that means its not statistically fit in our data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets check our new feature set\nX_train_new.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = sm.GLM(y_train,sm.add_constant(X_train_new),family=sm.families.Binomial())\nmodel2 = model2.fit()\nmodel2.summary()\n## fit our model to new data set after removing one feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets look for variance inflation factor of features \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['features'] = X_train_new.columns\nvif['vif'] = [variance_inflation_factor(X_train_new.values,i) for i in range(X_train_new.shape[1])]\nvif['vif'] = round(vif['vif'], 2)\nvif = vif.sort_values(by='vif',ascending=False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## remove feature hard\nX_train_new.drop('hard',axis=1,inplace=True)\n## drop hard due to its high p value that means its not statistically fit in our data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model3 = sm.GLM(y_train,sm.add_constant(X_train_new),family=sm.families.Binomial())\nmodel3 = model3.fit()\nmodel3.summary()\n## fit model to our revised dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets look for variance inflation factor of features \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['features'] = X_train_new.columns\nvif['vif'] = [variance_inflation_factor(X_train_new.values,i) for i in range(X_train_new.shape[1])]\nvif['vif'] = round(vif['vif'], 2)\nvif = vif.sort_values(by='vif',ascending=False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new.drop('winpercent',axis=1,inplace=True)\n## drop winpercent due to its high p value that means its not statistically fit in our data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model4 = sm.GLM(y_train,sm.add_constant(X_train_new),family=sm.families.Binomial())\nmodel4 = model4.fit()\nmodel4.summary()\n## fit our model in revised data set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets look for variance inflation factor of features \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['features'] = X_train_new.columns\nvif['vif'] = [variance_inflation_factor(X_train_new.values,i) for i in range(X_train_new.shape[1])]\nvif['vif'] = round(vif['vif'], 2)\nvif = vif.sort_values(by='vif',ascending=False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new.drop('pricepercent',axis=1,inplace=True)\n## drop pricepercent due to its high p value that means its not statistically fit in our data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model5 = sm.GLM(y_train,sm.add_constant(X_train_new),family=sm.families.Binomial())\nmodel5 = model5.fit()\nmodel5.summary()\n## again fit our model to revised data set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets look for variance inflation factor of features \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['features'] = X_train_new.columns\nvif['vif'] = [variance_inflation_factor(X_train_new.values,i) for i in range(X_train_new.shape[1])]\nvif['vif'] = round(vif['vif'], 2)\nvif = vif.sort_values(by='vif',ascending=False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new.drop('bar',axis=1,inplace=True)\n## drop bar due to its high p value that means its not statistically fit in our data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model6 = sm.GLM(y_train,sm.add_constant(X_train_new),family=sm.families.Binomial())\nmodel6 = model6.fit()\nmodel6.summary()\n## again fit our model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets look for variance inflation factor of features \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['features'] = X_train_new.columns\nvif['vif'] = [variance_inflation_factor(X_train_new.values,i) for i in range(X_train_new.shape[1])]\nvif['vif'] = round(vif['vif'], 2)\nvif = vif.sort_values(by='vif',ascending=False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"finally we achived our desired model p values less than 0.05 and vif for all features is also less than 5 ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets predict some candies whether they have chocolates or not\ny_train_pred = model6.predict(sm.add_constant(X_train_new)).values.reshape(-1)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_pred = pd.DataFrame({'competitorname':df_train['competitorname'].values,'chocolate':y_train.values,'pred':y_train_pred})\nfinal_pred.head()\n## created a new dataframe with candies name and their predictio values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lets find an optimal cutoff to decide basis on which we will decide whether a candy contains a choclate or not","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n# lets plot the ROC curve \n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"An ROC curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve( final_pred.chocolate, final_pred.pred, drop_intermediate = False )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_roc(final_pred.chocolate, final_pred.pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# lets find the optimal cutoff point for predict ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numbers = [float(x/10) for x in range(10)]\nfor i in numbers:\n    final_pred[i] = final_pred['pred'].map(lambda x:1 if x>i else 0)\nfinal_pred.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"created a data frame with all the values for different cut offs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(final_pred.chocolate, final_pred[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])/total1\n    \n    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as per accuracy,specificity and sensitivity curve for different optimal cutoff points chose our optimal cutoff point as 0.5","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_pred['result'] = final_pred['pred'].apply(lambda x:1 if x>0.5 else 0)\nfinal_pred.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion = confusion_matrix(final_pred.chocolate,final_pred.result)\nconfusion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the sensitivity of our logistic regression model\nTP / float(TP+FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us calculate specificity\nTN / float(TN+FP)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP/ float(TN+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Positive predictive value \nprint (TP / float(TP+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Negative predictive value\nprint (TN / float(TN+ FN))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"in our train data, we have achieved an accuracy of 89% with a positive predictive value of 80% and a negative predictive value of 96%\nour model successfully able to predict whether candies contain chocolate or not (that implies it is more successfully predict chocolate values which are 1 rather than which are 0s)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}