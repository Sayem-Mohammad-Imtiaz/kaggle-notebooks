{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Context\nAlthough this dataset was originally contributed to the UCI Machine Learning repository nearly 30 years ago, mushroom hunting (otherwise known as \"shrooming\") is enjoying new peaks in popularity. Learn which features spell certain death and which are most palatable in this dataset of mushroom characteristics. And how certain can your model be?\n\nContent\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide to North American Mushrooms (1981). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like \"leaflets three, let it be'' for Poisonous Oak and Ivy.\n\nTime period: Donated to UCI ML 27 April 1987\nInspiration\nWhat types of machine learning models perform best on this dataset?\n\nWhich features are most indicative of a poisonous mushroom?\n\nAcknowledgements\nThis dataset was originally donated to the UCI Machine Learning repository. You can learn more about past research using the data here.\n\n\n# Objective:\n\nOur objective is to predict whether a mushroom is edible or poisoneous.\n\n# About this file\n Attribute Information: (classes: edible=e, poisonous=p)\n\ncap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n\ncap-surface: fibrous=f,grooves=g,scaly=y,smooth=s\n\ncap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n\nbruises: bruises=t,no=f\n\nodor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n\ngill-attachment: attached=a,descending=d,free=f,notched=n\n\ngill-spacing: close=c,crowded=w,distant=d\n\ngill-size: broad=b,narrow=n\n\ngill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n\nstalk-shape: enlarging=e,tapering=t\n\nstalk-root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?\n\nstalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\n\nstalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\n\nstalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n\nstalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n\nveil-type: partial=p,universal=u\n\nveil-color: brown=n,orange=o,white=w,yellow=y\n\nring-number: none=n,one=o,two=t\n\nring-type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n\nspore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n\npopulation: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n\nhabitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d"},{"metadata":{},"cell_type":"markdown","source":"# Data Inspection"},{"metadata":{"trusted":true},"cell_type":"code","source":"## import libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"mushroom_df = pd.read_csv('../input/mushroom-classification/mushrooms.csv')\n## read our data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mushroom_df.head() ## check head of our dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mushroom_df.shape ## check shape of our dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mushroom_df.info() ## check info about our dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check for null values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"mushroom_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No null values in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"mushroom_df.describe(include='all') ## check description of the dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mushroom_df.columns\n\n## all variables of the dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(mushroom_df.columns) ## total 23 variables are there in the dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"Using pie plot to visualise our classes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# ----------------------------------------------------------------------------------------------------\n# prepare the data for plotting\n# create a dictionary of classes and their totals\nd = mushroom_df[\"class\"].value_counts().to_dict()\n\n# ----------------------------------------------------------------------------------------------------\n# instanciate the figure\nfig = plt.figure(figsize = (18, 6))\nax = fig.add_subplot()\n\n# ----------------------------------------------------------------------------------------------------\n# plot the data using matplotlib\nax.pie(d.values(), # pass the values from our dictionary\n       labels = d.keys(), # pass the labels from our dictonary\n       autopct = '%1.1f%%', # specify the format to be plotted\n       textprops = {'fontsize': 10, 'color' : \"white\"} # change the font size and the color of the numbers inside the pie\n      )\n# ----------------------------------------------------------------------------------------------------\n# prettify the plot\n\n# set the title\nax.set_title(\"Pie chart\")\n\n# set the legend and add a title to the legend\nax.legend(loc = \"upper left\", bbox_to_anchor = (1, 0, 0.5, 1), fontsize = 10, title = \"mushroom Class\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Approximately 52% of the total mushrooms are edible"},{"metadata":{},"cell_type":"markdown","source":"Check all variable in respect of classes."},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = list(mushroom_df.columns)\nplt.figure(figsize=(40,20))\n\nfor i in enumerate(cols):\n    plt.subplot(5,5,i[0]+1)\n    ax = sns.countplot(x=i[1],hue='class',data=mushroom_df)\n    ax.set_xlabel(i[1],fontsize=20)\nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Comments:**\n* There are no grooves type cap-surface mushrooms at all.\n* Mushrooms with bruises are mainly edible and mushrooms with no bruises are mainly poisoneous.\n* creosote,fishy,foul,musty and spicy odor mushrooms are poisoneous.\n* Maximum mushrooms are free gill-attached , close gill-spacing (max poisoneous) and narrow gill - size (max edible).\n* Buff and green gill colored mushrooms are edible.\n* bulbous stalk root mushrroms are maximum\n* silky stalk-surface-above-ring mushrooms are mainly poisoneous and smooth stalk-surface-above-ring mushrooms are mainly edible.\n* silky stalk-surface-below-ring mushrooms are mainly poisoneous and smooth stalk-surface-below-ring mushrooms are mainly edible.\n* orange and red stalk color above ring mushrooms are poisoneous.\n* brown and orange veil color mushrooms are poisoneous.\n* Buff , green , yellow , brown , orange spore print color mushrooms are mainly poisoneous.\n* numerous and abandant mushrooms are mainly poisoneous."},{"metadata":{"trusted":true},"cell_type":"code","source":"## let's check some variable with imbalance lebels\nmushroom_df['veil-type'].value_counts()/mushroom_df.shape[0]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" As variable 'vail-type' has only one value hence remove this variable from the dataframe and all mushrooms are partial type."},{"metadata":{"trusted":true},"cell_type":"code","source":"## hence remove this variable \nmushroom_df.drop('veil-type',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mushroom_df.shape ## check final shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preproccessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"## import libraries for data preproccessing\nimport sklearn \nfrom sklearn.preprocessing import OneHotEncoder\nimport category_encoders as ce\nimport sklearn\nfrom sklearn.model_selection import train_test_split\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train,df_test = train_test_split(mushroom_df,train_size=0.7,random_state=5) ## split data in train and test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = df_train.pop('class') ## x and y split of train data\nX_train = df_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = df_test.pop('class') ## x and y split of test data\nX_test = df_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this strategy, each category value is converted into a new column and assigned a 1 or 0 (notation for true/false) value to the column"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = ce.OneHotEncoder(cols=list(X_train.columns))\n\nX_train = encoder.fit_transform(X_train) ## one hot encoding on all variables\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head() ## check head of x ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = encoder.transform(X_test) ## ebcoding done on x of test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head() ## check x test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = y_train.apply(lambda x:0 if x=='e' else 1) ## convert target variable into 0 and 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = y_test.apply(lambda x:0 if x=='e' else 1) ## convert target variable into 0 and 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Remove Constant Features**\n* Constant features are those that show the same value, just one value, for all the observations of the dataset. This is, the same value for all the rows of the dataset. These features provide no information that allows a machine learning model to discriminate or predict a target.\n* Variance threshold from sklearn is a simple baseline approach to feature selection. It removes all features which variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e., features that have the same value in all samples.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\nsel = VarianceThreshold(threshold=0)\nsel.fit(X_train)  # fit finds the features with zero variance\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if we sum over get_support, we get the number of features that are not constant\nsum(sel.get_support())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no constant variable hence increase threshold  bit"},{"metadata":{"trusted":true},"cell_type":"code","source":"sel1 = VarianceThreshold(threshold=0.1)\nsel1.fit(X_train)  # fit finds the features with 90% variance\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if we sum over get_support, we get the number of features that are not constant\nsum(sel1.get_support())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train[X_train.columns[sel1.get_support()]] ## select variables with proper distribution of values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = X_test[X_test.columns[sel1.get_support()]] ## select variables with proper distribution of values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier ## import libraries for randomforest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Created a helper function to evaluate models."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\ndef evaluate_model(rf):\n    print(\"confusion matrix for training set: \",confusion_matrix(y_train,rf.predict(X_train)))\n    print(\"accuracy score of training set: \",accuracy_score(y_train,rf.predict(X_train)))\n    print(\"--\"*50)\n    print(\"confusion matrix for test set: \",confusion_matrix(y_test,rf.predict(X_test)))\n    print(\"accuracy score of test set: \",accuracy_score(y_test,rf.predict(X_test)))\n    print(\"**\"*50)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(random_state = 50)\nrfc.fit(X_train,y_train) ## train our first model with default parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_model(rfc) ## evaluate the model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check Feature Importance"},{"metadata":{},"cell_type":"markdown","source":"Create one dataframe with feature score to identify least important feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_score = pd.DataFrame({'features':X_train.columns,'feature score':rfc.feature_importances_}) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_score.sort_values(by='feature score',ascending=False).head(10) ## check top 10 features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_score.sort_values(by='feature score',ascending=False).tail(10) ## check least top 10 features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Least important feature is 'stalk-color-below-ring_3' . Hence removing it. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop('stalk-color-below-ring_3',axis=1,inplace=True) ## remove from train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.drop('stalk-color-below-ring_3',axis=1,inplace=True) ## remove from test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc1 = RandomForestClassifier(random_state=10)\nrfc1.fit(X_train,y_train) ## fit our secoend model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_model(rfc1) ## evaluate model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy not changed due to one variable."},{"metadata":{},"cell_type":"markdown","source":"Again check feature importance."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_score = pd.DataFrame({'features':X_train.columns,'feature score':rfc1.feature_importances_})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_score.sort_values(by='feature score',ascending=False).tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop('cap-color_6',axis=1,inplace=True) ## checking by removing cap-color_6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.drop('cap-color_6',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc2 = RandomForestClassifier(random_state=20)\nrfc2.fit(X_train,y_train) ## again fit the model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_model(rfc2) ## accuracy score still not changed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_train.columns) ## final list of features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\nplt.figure(figsize=(30,15))\ntree.plot_tree(rfc2.estimators_[0],filled=True)\nplt.show()\n\n## plot one decision tree of the random forest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**\n\nWe achieved 100 % accuracy for train and test hence there is no need of hyper parameter tuning but we can check and remove more features from the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}