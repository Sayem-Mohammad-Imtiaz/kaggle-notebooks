{"cells":[{"metadata":{"_uuid":"5c3bbdc3be74060c22782bb704f8761a68efeff7"},"cell_type":"markdown","source":"\n**垃圾邮件分类**\n\nhttps://www.kaggle.com/uciml/sms-spam-collection-dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np \n\ndata_dir = \"../input/\"\n\ndf = pd.read_csv(data_dir + '/spam.csv', encoding='latin-1')  \n# 编码相关阅读http://blog.csdn.net/robertcpp/article/details/7837712 \n\n# 查看数据\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c4b4ecf4a9b030625b5fe4e2148987eecebeef0"},"cell_type":"code","source":"# 查看v2的样本\ndf.v2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d783b8b5e0ce235f08e719e020f59166ffe07ee"},"cell_type":"code","source":"# 查看v1的样本\ndf.v1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1197dfd37bed16d3b2ce050267c08a4f835762e5"},"cell_type":"code","source":"# 查看数据的纬度\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c74d6fe15009bfaddad7a3cc5874518bb9df40e9"},"cell_type":"markdown","source":"**把数据拆分成为训练集和测试集**"},{"metadata":{"trusted":true,"_uuid":"e165e25232a24ba46862154fe2ae3f4cae204f9e"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# 把数据拆分成训练集和测试集\n# train_test_split (X, Y, test_size=0.2, random_state=0)\ndata_train, data_test, labels_train, labels_test = train_test_split(\n    df.v2,\n    df.v1, \n    test_size=0.2, \n    random_state=0) \n\n# 查看训练集样本\nprint (data_train.head())\n# 查看训练集标注\nprint (labels_train.head())\n# 查看训练集的样本个数\nprint(data_train.shape)\n# 查看测试机的样本个数\nprint(data_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5cf3f8b3709214a1f85c4413060b6de6fc627387"},"cell_type":"markdown","source":"**构建模型所需要的数据格式：一个词汇表，以及训练及测试数据的计数信息：(句子id,单词id)->计数**"},{"metadata":{"trusted":true,"_uuid":"ee320866d3ad5b9ffd6e1b5b7b1286f3deae4f71"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n# 调用库来构造分类器所需的输入数据\nvectorizer = CountVectorizer()\n\n#fit_transform一共完成了两件事. fit: build dict (i.e. word->wordID)  transform: convert document (i.e. each line in the file) to word vector \n#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform\n#fit: 统计单词的总个数，建成一个表，每个单词给一个标号 (这个库内部实现有一个缺陷，会把长度为1的单词给过滤掉了)\n#transform:统计每句话每个单词出现的次数\n\n# 用训练集的单词来建立词库，因为测试集的数据在现实场景中属于未知数据，且把训练集每句话词(也就是input doc中的每一行)变成向量形态\ndata_train_count = vectorizer.fit_transform(data_train)\n# 把测试集每句话变成向量形态\ndata_test_count  = vectorizer.transform(data_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8dbdf03b0f86866873b9ebf1600ee9085dafc707"},"cell_type":"code","source":"# 训练数据纬度 \n# 7612 也就是fit过程中建立的词汇表的size\nprint (data_train_count.shape)\n# 测试数据纬度\nprint (data_test_count.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7724c259bd784c4917bd5c13281f92a786501b6a"},"cell_type":"code","source":"# 看看这些数据长什么样\n# 词汇表 （太长了，我这里注释掉）\nprint (len(vectorizer.vocabulary_))\n#print (vectorizer.vocabulary_)\n# print first 3 lines. \n# each line: represent the word verctor for a setence/email content which is just the each line in the input file i.e. spam.csv\n# each line: e.g. [0,0,1, 2.....0,0,0] 1表示词典中index 为2的单词，在这一份doc/邮件，中出现1次，2表示典中index 3的单词，在这一份doc/邮件中出现2次\nprint(data_train_count.toarray()[0:4])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecc054e22b5078199398de8e17b1be55c8311f71"},"cell_type":"markdown","source":"**图形化展示一些数据，获得更直观的理解**"},{"metadata":{"trusted":true,"_uuid":"81a9861d52c15722a4f5a8adfe35a3dd258cf28b"},"cell_type":"code","source":"import matplotlib.pyplot as plt # 画图常用库\n\n# 我们来看看单词的分布. 统计每个单词出现的次数\n# 因为matrix每一行就代表一个句子里的单词分布, 每个位置上（i.e. column）的数值即表示，dict里对应的index的单词出现的次数. 所以矩阵按列求和就可以求出每个单词在总的spam.csv中出现的次数\noccurrence = data_train_count.toarray().sum(axis=0) #把矩阵按列求和\nplt.plot(occurrence)\nplt.show() # 显示图形， x 轴表示单词的index, y轴表示，dict中对应index的单词出现的次数\n\n# 按照每个词出现的次数从高到低进行排序. get_feature_names 即是dict里面的word.\n# build dataframe\nword_freq_df = pd.DataFrame({'term': vectorizer.get_feature_names(), 'occurrence':occurrence})\nword_freq_df_sort = word_freq_df.sort_values(by=['occurrence'], ascending=False)\nword_freq_df_sort.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93f48ba9b18c5d3706a6f545165c8234021a6250"},"cell_type":"markdown","source":"**进行模型训练以及预测**"},{"metadata":{"trusted":true,"_uuid":"fba75345a53f7e9accc98fc5af92ae4765d7f476"},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nclf = MultinomialNB()\nclf.fit(data_train_count, labels_train)\npredictions = clf.predict(data_test_count)\nprint(predictions)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e26517f214849ad9a639011735c8eb4087d8f09"},"cell_type":"markdown","source":"**计算模型的准确率**"},{"metadata":{"trusted":true,"_uuid":"7c2e812428d52fc4edc44ed91fc6ac9ac90775c2"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nprint (accuracy_score(labels_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44e920838ed5273f3b64493b5de0379d74a56877"},"cell_type":"markdown","source":"**其他常用指标: （Precision, Recall, F1-score, confusion_matrix）**"},{"metadata":{"trusted":true,"_uuid":"7dfffe0b9b94f42f0229b557aab96f2c82ad1f1b"},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix\nprint (confusion_matrix(labels_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0178591dd2f3b6acb9ee7c2eeec7b4314e17261"},"cell_type":"code","source":"print (classification_report(labels_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9878f5a1b2397691762792ffc9dfc77b105813e"},"cell_type":"markdown","source":"**交叉验证的示范:**"},{"metadata":{"trusted":true,"_uuid":"9dce948c8ac9a77c4dd0200745d1f0e5b00e79fb"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n# 从df获得全部邮件内容和标注\ndata_content = df.v2\ndata_label = df.v1\nvect = CountVectorizer()\n# 在整体数据集上构建词汇表以及转化成计数格式 Note: 这里不需要train, validation split. cross_val_score will handle this split\ndata_count = vect.fit_transform(data_content)\n# 交叉验证 clf = MultinomialNB()\n# cross_val_score(model, X, Y, cv=20, scoring=\"accuracy\")\ncross_val = cross_val_score(clf, data_count, data_label, cv=20, scoring='accuracy')\n# 打印每组实验测试集的准确率\nprint (cross_val)\n# 求平均值\nprint (np.mean(cross_val))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}