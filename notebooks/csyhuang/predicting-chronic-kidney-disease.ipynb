{"nbformat_minor":1,"nbformat":4,"cells":[{"source":"# Predicting Chronic Kidney Disease based on health records\nGiven 24 health related attributes taken in 2-month period of 400 patients, using the information of the 158 patients with complete records to predict the outcome (i.e. whether one has chronic kidney disease) of the remaining 242 patients (with missing values in their records).\n\n## Summary of Results\nWith proper tuning of parameters using cross-validation in the training set, the Random Forest Classfier achieves an accuracy of 88.8% and an ROC AUC of 99.2%. Lesson learnt: It happens that some pruning helps improve the performance of RF a lot.[](http://)","metadata":{"_cell_guid":"ef22745a-3680-441c-952a-7ce07e28ef3a","_uuid":"78003d4f6d7b9dc5c6b298422dfe6e0e7601a24f"},"cell_type":"markdown"},{"source":"## Load Modules and helper functions","metadata":{"_cell_guid":"b0692cea-7400-461d-a8af-c29231368f02","_uuid":"92c521a749de6c280b92828423b40db416fe1241"},"cell_type":"markdown"},{"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report,accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\ndef auc_scorer(clf, X, y, model): # Helper function to plot the ROC curve\n    if model=='RF':\n        fpr, tpr, _ = roc_curve(y, clf.predict_proba(X)[:,1])\n    elif model=='SVM':\n        fpr, tpr, _ = roc_curve(y, clf.decision_function(X))\n    roc_auc = auc(fpr, tpr)\n\n    plt.figure()    # Plot the ROC curve\n    plt.plot(fpr, tpr, label='ROC curve from '+model+' model (area = %0.3f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return fpr,tpr,roc_auc\n\n# from subprocess import check_output\n# print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","metadata":{"_cell_guid":"1a35d647-79b7-40a8-86aa-adc4e5bb33cf","collapsed":true,"_uuid":"e80657e1dba9c8011e3c89ff215e50b2793bf637"},"execution_count":null,"cell_type":"code"},{"source":"## Load files","metadata":{"_cell_guid":"68be8886-4e23-4f5d-a35b-e9c4ab3df8de","_uuid":"7c0340215ef5df788ab038b09acb0d4a0478ca9e"},"cell_type":"markdown"},{"outputs":[],"source":"df = pd.read_csv('../input/kidney_disease.csv')","metadata":{"_cell_guid":"2795e5b4-a0e0-4ca6-9374-37b554a6115b","collapsed":true,"_uuid":"7411d96d0f393dd253f6dc6aa8d6a24a220647c4"},"execution_count":null,"cell_type":"code"},{"source":"## Cleaning and preprocessing of data for training a classifier","metadata":{"_cell_guid":"7d1d9de5-6b08-4317-ac17-b3d6da8379cf","_uuid":"5be7dfb54962791ae37d47cf089cc699b54fbae2"},"cell_type":"markdown"},{"outputs":[],"source":"# Map text to 1/0 and do some cleaning\ndf[['htn','dm','cad','pe','ane']] = df[['htn','dm','cad','pe','ane']].replace(to_replace={'yes':1,'no':0})\ndf[['rbc','pc']] = df[['rbc','pc']].replace(to_replace={'abnormal':1,'normal':0})\ndf[['pcc','ba']] = df[['pcc','ba']].replace(to_replace={'present':1,'notpresent':0})\ndf[['appet']] = df[['appet']].replace(to_replace={'good':1,'poor':0,'no':np.nan})\ndf['classification'] = df['classification'].replace(to_replace={'ckd':1.0,'ckd\\t':1.0,'notckd':0.0,'no':0.0})\ndf.rename(columns={'classification':'class'},inplace=True)","metadata":{"_cell_guid":"d26cdb91-8988-455a-9c16-cf98b901a82a","collapsed":true,"_uuid":"d673c87769a249511ff4143f186d74482e201516"},"execution_count":null,"cell_type":"code"},{"outputs":[],"source":"# Further cleaning\ndf['pe'] = df['pe'].replace(to_replace='good',value=0) # Not having pedal edema is good\ndf['appet'] = df['appet'].replace(to_replace='no',value=0)\ndf['cad'] = df['cad'].replace(to_replace='\\tno',value=0)\ndf['dm'] = df['dm'].replace(to_replace={'\\tno':0,'\\tyes':1,' yes':1, '':np.nan})\ndf.drop('id',axis=1,inplace=True)","metadata":{"_cell_guid":"ad80743d-6471-4871-a817-c6cf96f73f3b","collapsed":true,"_uuid":"c40044cea7ae9b4ab210b54ccc2c3c8f37783287"},"execution_count":null,"cell_type":"code"},{"outputs":[],"source":"df.head()","metadata":{"_cell_guid":"10731633-cd50-41ce-9c95-a60dc8ac8e8f","_uuid":"775c0d2c2c49b847ab698964a033e5f124fa42d3"},"execution_count":null,"cell_type":"code"},{"source":"## Check the portion of rows with NaN\n- Now the data is cleaned with improper values labelled NaN. Let's see how many NaNs are there.\n- Drop all the rows with NaN values, and build a model out of this dataset (i.e. df2)","metadata":{"_cell_guid":"042af755-5ec1-4e4d-8923-c14e743195c7","_uuid":"cd05059199eacedc1d7fdaa950c85189977a8914"},"cell_type":"markdown"},{"outputs":[],"source":"df2 = df.dropna(axis=0)\ndf2['class'].value_counts()","metadata":{"_cell_guid":"e4242e89-614a-4a6f-bd45-2f82351b5d04","_uuid":"3d98f3adfcf97d96b3bc9ff37b4ab8c4c13d3dbb"},"execution_count":null,"cell_type":"code"},{"source":"## Examine correlations between different features","metadata":{"_cell_guid":"f6ad991c-eb2a-4910-9809-642635149e04","_uuid":"a0d03c254efeec4bcf94ad55c854a83ca85dfe27"},"cell_type":"markdown"},{"outputs":[],"source":"corr_df = df2.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr_df, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_df, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.title('Correlations between different predictors')\nplt.show()","metadata":{"_cell_guid":"19b64021-bcdf-4f88-913b-962390628f8d","_uuid":"4943b12c51e4561e285a6272ce2180d0601bb6ae"},"execution_count":null,"cell_type":"code"},{"source":"## Split the set for training models further into a (sub-)training set and testing set.","metadata":{"_cell_guid":"288204a8-f8d8-4ceb-9a7d-d17d36c484b0","_uuid":"de87bc336ca22e901bd0ee2c58d1160a8e7332fa"},"cell_type":"markdown"},{"outputs":[],"source":"X_train, X_test, y_train, y_test = train_test_split(df2.iloc[:,:-1], df2['class'], \n                                                    test_size = 0.33, random_state=44,\n                                                   stratify= df2['class'] )\n","metadata":{"_cell_guid":"5eb0de02-55b0-447d-9a14-7ec9915d7ffc","collapsed":true,"_uuid":"71f45a3d6c98d57ac8f1592388f5700316de502a"},"execution_count":null,"cell_type":"code"},{"outputs":[],"source":"print(X_train.shape)\nprint(X_test.shape)","metadata":{"_cell_guid":"a44df0d8-f87e-4f17-b7fc-5dc74aeb9af0","_uuid":"fe32d3246677ac2ea2edb679f8bdeb036a12f24f"},"execution_count":null,"cell_type":"code"},{"outputs":[],"source":"y_train.value_counts()","metadata":{"_cell_guid":"a91caed3-dab3-4151-99c4-5d7d6853f745","_uuid":"8c2d9a6eacd3d3f27be1c3c4bc042d839c778b69"},"execution_count":null,"cell_type":"code"},{"source":"## Choosing parameters with GridSearchCV with 10-fold cross validations.\n(Suggestion for next time: try using Bayesian model selection method)","metadata":{"_cell_guid":"4f9fa8e7-cc41-4776-86ba-654d8f225d0a","_uuid":"13430eedffd5ecf5a1d70ab2cc86271d91a00d47"},"cell_type":"markdown"},{"outputs":[],"source":"tuned_parameters = [{'n_estimators':[7,8,9,10,11,12,13,14,15,16],'max_depth':[2,3,4,5,6,None],\n                     'class_weight':[None,{0: 0.33,1:0.67},'balanced'],'random_state':[42]}]\nclf = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv=10,scoring='f1')\nclf.fit(X_train, y_train)\n\nprint(\"Detailed classification report:\")\ny_true, lr_pred = y_test, clf.predict(X_test)\nprint(classification_report(y_true, lr_pred))\n\nconfusion = confusion_matrix(y_test, lr_pred)\nprint('Confusion Matrix:')\nprint(confusion)\n\n# Determine the false positive and true positive rates\nfpr,tpr,roc_auc = auc_scorer(clf, X_test, y_test, 'RF')\n\nprint('Best parameters:')\nprint(clf.best_params_)\nclf_best = clf.best_estimator_\n","metadata":{"_cell_guid":"53b832b7-b6ed-4efc-93fe-e1c8d312bdc1","_uuid":"068ddf02dceed8fc7f18c6d7bba431bfbd7de056"},"execution_count":null,"cell_type":"code"},{"source":"## Examine feature importance\nSince I pruned the forest (*max_depth*=2) and decrease the number of trees (*n_estimators*=8), not all features are used.","metadata":{"_cell_guid":"fcb927c2-e0d6-40bd-afd9-5ff2e340f1e6","_uuid":"f6eaae9019e5a7838bd85b2f927687f41a5eec7c"},"cell_type":"markdown"},{"outputs":[],"source":"plt.figure(figsize=(12,3))\nfeatures = X_test.columns.values.tolist()\nimportance = clf_best.feature_importances_.tolist()\nfeature_series = pd.Series(data=importance,index=features)\nfeature_series.plot.bar()\nplt.title('Feature Importance')","metadata":{"_cell_guid":"c389c50a-3437-474b-a02f-7fdc7a5607fa","_uuid":"78fa6089f36ed50c7888b6d32ae94e5e7e0f7bf1"},"execution_count":null,"cell_type":"code"},{"outputs":[],"source":"list_to_fill = X_test.columns[feature_series>0]\nprint(list_to_fill)","metadata":{"_cell_guid":"6cb8a976-2759-4977-90a6-ff4620cdcad0","_uuid":"7adf7c33dee3385773492ecb9b3d837d1e018667"},"execution_count":null,"cell_type":"code"},{"source":"## Next, I examine the rest of the dataset (with missing values across the rows)\nAre there correlations between occurence of missing values in a row? The plot suggests, seems no.","metadata":{"_cell_guid":"81c680cf-3cfd-4a38-a0d5-04c57df2118d","_uuid":"c13981412605422f6120faba5721309fd52864d6"},"cell_type":"markdown"},{"outputs":[],"source":"# Are there correlation in missing values?\ncorr_df = pd.isnull(df).corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr_df, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_df, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","metadata":{"_cell_guid":"f9075981-ffbc-4650-aa48-f88f2329af28","_uuid":"02c51e5a3b53c0812ec4402a29b4220c49193867"},"execution_count":null,"cell_type":"code"},{"source":"## Make predictions with the best model selected above\nI filled in all NaN with 0 and pass it to the trained classifier. The results are as follows:\n- True positive = 180\n- True negative = 35\n- False positive = 0\n- False negative = 27\n----\n- Accuracy = 88.8%\n- ROC AUC = 99.2%","metadata":{"_cell_guid":"51c717c3-2605-4d24-a284-c700a4ed6b81","_uuid":"a37311774ca129c95271d09cecf7797f7b69e6f7"},"cell_type":"markdown"},{"outputs":[],"source":"df2 = df.dropna(axis=0)\nno_na = df2.index.tolist()\nsome_na = df.drop(no_na).apply(lambda x: pd.to_numeric(x,errors='coerce'))\nsome_na = some_na.fillna(0) # Fill up all Nan by zero.\n\nX_test = some_na.iloc[:,:-1]\ny_test = some_na['class']\ny_true = y_test\nlr_pred = clf_best.predict(X_test)\nprint(classification_report(y_true, lr_pred))\n\nconfusion = confusion_matrix(y_test, lr_pred)\nprint('Confusion Matrix:')\nprint(confusion)\n\nprint('Accuracy: %3f' % accuracy_score(y_true, lr_pred))\n# Determine the false positive and true positive rates\nfpr,tpr,roc_auc = auc_scorer(clf_best, X_test, y_test, 'RF')\n \n","metadata":{"_cell_guid":"6d5371a7-4a8f-47fc-8151-d659cf3e3494","_uuid":"2cec0c3b125d51d54162e02ecf0a50173250d16e"},"execution_count":null,"cell_type":"code"}],"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","version":"3.6.3","pygments_lexer":"ipython3","file_extension":".py","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python"}}}