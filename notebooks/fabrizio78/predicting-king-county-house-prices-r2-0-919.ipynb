{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **KING COUNTY HOUSE PRICE**\n**[by Fabrizio Basso](https://www.linkedin.com/in/fabrizio-basso-4543463b/)**\n\n## Dataset\n<hr/>\n\n* This dataset contains house sale prices for King County, which includes Seattle. \n* It includes homes sold between May 2014 and May 2015.\n* 21 columns. (features)\n* 21613 rows.\n\n***Feature Columns***\n    \n* **id:** Unique ID for each home sold\n* **date:** Date of the home sale --> This is the target feature\n* **price:** Price of each home sold\n* **bedrooms:** Number of bedrooms\n* **bathrooms:** Number of bathrooms, where .5 accounts for a room with a toilet but no shower\n* **sqft_living:** Square footage of the apartments interior living space\n* **sqft_lot:** Square footage of the land space\n* **floors:** Number of floors\n* **waterfront:** - A dummy variable for whether the apartment was overlooking the waterfront or not\n* **view:** An index from 0 to 4 of how good the view of the property was\n* **condition:** - An index from 1 to 5 on the condition of the apartment,\n* **grade:** An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design.\n* **sqft_above:** The square footage of the interior housing space that is above ground level\n* **sqft_basement:** The square footage of the interior housing space that is below ground level\n* **yr_built:** The year the house was initially built\n* **yr_renovated:** The year of the houseâ€™s last renovation\n* **zipcode:** What zipcode area the house is in\n* **lat:** Lattitude\n* **long:** Longitude\n* **sqft_living15:** The square footage of interior housing living space for the nearest 15 neighbors\n* **sqft_lot15:** The square footage of the land lots of the nearest 15 neighbors"},{"metadata":{},"cell_type":"markdown","source":"### Import of Main Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Regular Imports\nimport os\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.patches as patches\nfrom tabulate import tabulate\nimport missingno as msno \nimport warnings\nfrom joblib import dump, load\nwarnings.filterwarnings(\"ignore\")\n\n!pip install -U scikit-learn==0.24.1\n\nimport sklearn\nsklearn.__version__\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Set Color Palettes for the notebook\ncustom_colors = ['#74a09e','#86c1b2','#98e2c6','#f3c969','#f2a553', '#d96548', '#c14953']\nsns.palplot(sns.color_palette(custom_colors))\n\n# Set Style\nsns.set_style(\"whitegrid\",{\"grid.linestyle\":\"--\"})\nsns.despine(left=True, bottom=True)\nmpl.rcParams['figure.dpi'] = 250\nmpl.rc('axes', labelsize=10)\nplt.rc('xtick',labelsize=10)\nplt.rc('ytick',labelsize=10)\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **2.0 Exploraty Data Analysis**\n\nIn this section the main goal is to get familiar with the dataset. Among all the topics covered, it will address the following questions.\n\n### Which features contain blank, null or empty values?\n\nWe can check for missing values with pandas isna(). This indicates whether values are missing or not. Then we can sum all the values to check every column.\n\n### Which features are categorical?\n\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\n\n* **Categorical**: id, waterfront, zipcode.\n\n### Which features are numerical? \nThese values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\n\n* **Continous**: price, bathrooms, floors, lat, long.\n* **Discrete**: date, bedrooms, sqft_living, sqft_lot, view, condition, grade, sqft_above, sqft_basement, yr_built, yr_renovated, sqft_living15, sqft_lot15."},{"metadata":{"trusted":true},"cell_type":"code","source":"#import the dataset\nhouse_df = pd.read_csv('../input/housesalesprediction/kc_house_data.csv',parse_dates=['date'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1 **Assess the precence of missing values** "},{"metadata":{"trusted":true},"cell_type":"code","source":"house_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Or graphically:"},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.matrix(house_df, figsize=(12.5,5), fontsize=10, color=(0.8, 0.25, 0.25));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there is no missing value in the dataset there is no need for imputation.\n\nNow the number of unique values for each feature is assessed:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in house_df.columns:\n    print(f'Unique values for {column}: {len(house_df[column].unique())}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"id\" feature has basically an unique values for each transaction to identify it. Therefore, it can be eliminated from the dataset as not informative."},{"metadata":{"trusted":true},"cell_type":"code","source":"house_df.drop('id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Moreover the data appear to be in the right format:"},{"metadata":{"trusted":true},"cell_type":"code","source":"house_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **2.2 Pearson correlation matrix**\n\nThe Pearson correlation coefficient evaluates the strength and direction of the linear relationship between two variables. The coefficient ranges between -1 and +1. The greater the value in absolute term, the stonger is the relationship between two features."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"whitegrid\", font_scale=1)\n\nplt.figure(figsize=(13,13))\nplt.title('Pearson Correlation Matrix',fontsize=25)\nsns.heatmap(house_df.corr(),linewidths=0.45,vmax=0.7,square=True,cmap=\"autumn_r\",linecolor='w',\n            annot=True, annot_kws={\"size\":7}, cbar_kws={\"shrink\": .8});\n\nsns.set_style(\"whitegrid\",{\"grid.linestyle\":\"--\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As next step in the EDA process, some of the features will be analyzed in details to assess their nature and what kind of information they have about the target feature, the house price. The first feature is the \"Zipcode\"\n\n## - **ZipCode**\n\nTaken at face value, the *zipcode* does not appear to capture much information about the house prices. Correlation is -0.05. However, this is highly misleading. All things equal, \"Zipcodes\" connected to posh, well-off areas identify proprieties with higher prices or values. In total there are 70 zipcodes in King County: "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of Zipcodes:\nlen(house_df['zipcode'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of proprierty transaction is  highly un-envenly distribuited across the postal codes. It ranges from a top in the area of 600 circa to minimun of about 50. Moreover, proprieties facing the waterfront are only located in specific zipcodes covering areas closed to the seaside. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(13,6))\n\ng = sns.countplot(x='zipcode', hue='waterfront', data=house_df, ax=ax, )\ng.set_xticklabels(labels = house_df['zipcode'].unique(), rotation=90, fontsize=10);\ng.grid(linestyle='--')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(15,6))\nsns.boxplot(x='zipcode',y='price',data=house_df,ax=ax, palette='Reds');\nax.set_xticklabels(labels = house_df['zipcode'].unique(), rotation=90, fontsize=10);\nax.set_title('Boxplot: Price Distribution by Zipcodes');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**: The graph above shows that some postcode-areas have significantly different price distribution than others. Therefore, zipcode is a relevant features. However, since it is a categorical feature, it has no direct correspondance between its intrinsic value and a specific variation in the price. Its value is only a convention: a number is used but it could be also a sequence of letter. For instance increasing the zipcode from 98001 to 98100 does not produce a specific change in the prices just because its value has been increased by 100. To adress this issue, **a different dummy needs to create afor each zipcode. This operation will be performed later on within a data pre-processing step within a pipeline.**\n\n### - **Feature Analysis: Bedrooms, Floors and Bathrooms:**\n\nThe Graph below confirms the general intuition deriving from the correlation matrix. These features has on average a positive correlation with the proprietis prices."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1, 2,figsize=(15,5))\nsns.boxplot(x=house_df['bedrooms'],y=house_df['price'], ax=axes[0], palette = 'autumn_r')\nsns.boxplot(x=house_df['floors'],y=house_df['price'], ax=axes[1], palette = 'autumn_r')\nsns.despine(left=True, bottom=True)\naxes[0].set(xlabel='Bedrooms', ylabel='Price')\naxes[0].yaxis.tick_left()\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\naxes[1].set(xlabel='Floors', ylabel='Price')\n\nf, axe = plt.subplots(1, 1,figsize=(15,5))\nsns.despine(left=True, bottom=True)\nsns.boxplot(x=house_df['bathrooms'],y=house_df['price'], ax=axe, palette = 'autumn_r')\naxe.yaxis.tick_left()\naxe.set(xlabel='Bathrooms / Bedrooms', ylabel='Price');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### - **Feature Analysis: WaterFront, View and Grade:**\nA similar consideration as above can be made for these three features. In particular, \"waterfront\" location seems to provide quite a boost to the propriety price. The same applies to the propriety's view and building quality (grade). It must be noted that in this case it is not necessary to create dummies out of \"view\" and \"grade\" since to a higher values in these feature also corresponds better qualities of the propriety. "},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1, 2,figsize=(15,5))\nsns.boxplot(x=house_df['waterfront'],y=house_df['price'], ax=axes[0], palette = 'viridis')\nsns.boxplot(x=house_df['view'],y=house_df['price'], ax=axes[1], palette = 'viridis')\nsns.despine(left=True, bottom=True)\naxes[0].set(xlabel='Waterfront', ylabel='Price')\naxes[0].yaxis.tick_left()\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\naxes[1].set(xlabel='View', ylabel='Price')\n\n\nf, axe = plt.subplots(1, 1,figsize=(15,5))\nsns.boxplot(x=house_df['grade'],y=house_df['price'], ax=axe, palette = 'viridis')\nsns.despine(left=True, bottom=True)\naxe.yaxis.tick_left()\naxe.set(xlabel='Grade', ylabel='Price');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Construnction Year and Renovations: Binning**\n\nData binning is a preprocessing technique used to reduce the effects of minor observation errors. It is worthwhile applying this transformation to some columns of this dataset. Binning is applied to yr_built and yr_renovated. Ages and renovation ages of the houses are calculated in relation to the date the propriety is sold. The original feature is dropped. The distribuitions of these features is shown in the following graphs:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# just take the year from the date column\nhouse_df['sales_yr']=pd.DatetimeIndex(house_df['date']).year\nhouse_df['sales_mth']=pd.DatetimeIndex(house_df['date']).month\n\n# add the age of the buildings when the houses were sold as a new column\nhouse_df['age']=house_df['sales_yr']-house_df['yr_built']\n# add the age of the renovation when the houses were sold as a new column\nhouse_df['age_rnv']=0\nhouse_df['age_rnv']=house_df['sales_yr'][house_df['yr_renovated']!=0].astype(int)-house_df['yr_renovated'][house_df['yr_renovated']!=0]\nhouse_df['age_rnv'][house_df['age_rnv'].isnull()]=house_df['age']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# partition the age into bins\nbins_age = [-2,1,5,10,20,30,60,100,100000]\nlabels = [0,5,10,20,30,60,80,100]\nhouse_df['age_binned'] = pd.cut(house_df['age'], bins=bins_age, labels=labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# partition the age_rnv into bins\nbins_ren = [-2,1,5,10,20,30,50,100000]\nlabels = [0,5,10,20,30,60,100]\nhouse_df['age_rnv_binned'] = pd.cut(house_df['age_rnv'], bins=bins_ren, labels=labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize=(15,5))\n\nsns.countplot(house_df['age_binned'], palette='Reds', ax=ax[0], alpha=0.85);\nsns.countplot(house_df['age_rnv_binned'], palette='Blues', ax=ax[1], alpha=0.85)\nax[0].set_title('Years since Construction')\nax[1].set_title('Years since Renovation');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Year and Month of Transaction - Information Extraction**\n\nThe transactions range from May-2014 to May-2015. There is about a fluctuation of circa 10% between the min and max at monthly levels.\nThe date is split in two different features: Year and Month of the transaction, while the original feature is dropped.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"house_df.groupby([\"sales_yr\",\"sales_mth\"])[\"price\"].agg(['mean','median']).plot(figsize=(15,6), marker='*', markersize = 12)\nplt.title('Price Evolution over Time', fontsize=17);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house_df.drop(['date'], inplace=True, axis=1)\nhouse_df.drop(['yr_built','yr_renovated'], inplace=True, axis=1)\n\nhouse_df_bin = house_df[['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n                         'waterfront', 'view', 'condition', 'grade', 'sqft_above','sqft_basement',\n                         'zipcode', 'lat', 'long', 'sqft_living15','sqft_lot15', 'sales_yr',\n                         'sales_mth','age_binned','age_rnv_binned']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Features to Normalize**\n\nSome Features distribution go through a log transformation, to make their distribution more Normal-like. The original features are then dropped.\n\na) **Price**:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.distplot(house_df_bin[\"price\"], color='r');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house_df_bin[\"log_price\"] = np.log(house_df_bin[\"price\"])\n\nplt.figure(figsize=(10,6))\nsns.distplot(house_df_bin[\"log_price\"], color='r');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The original price feature is then dropped:"},{"metadata":{"trusted":true},"cell_type":"code","source":"house_df_bin.drop(['price'], inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Addtional Feature to Normalize**:\n\nThe same procedure is applied to these features:\n\n- sqft_living\n- sqft_lot\n- sqft_basement \n- sqft_living15 \n- sqft_lot15\n\nSince the value \"0\" cannot go through a log transformation, a +1 is added to those features that can assume that value:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = [\"sqft_living\",\"sqft_lot\",\"sqft_basement\",\"sqft_living15\",\"sqft_lot15\"]\nhouse_df_bin[cols].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Basement squared feet size is the only features showing 0 in this subset of features."},{"metadata":{"trusted":true},"cell_type":"code","source":"house_df_bin.loc[:,'sqft_basement_log'] = np.log(house_df_bin.loc[:,'sqft_basement']+1)\nhouse_df_bin.loc[:,'sqft_living_log'] = np.log(house_df_bin.loc[:,'sqft_living'])\nhouse_df_bin.loc[:,'sqft_lot_log'] = np.log(house_df_bin.loc[:,'sqft_lot'])\nhouse_df_bin.loc[:,'sqft_living15_log'] = np.log(house_df_bin.loc[:,'sqft_living15'])\nhouse_df_bin.loc[:,'sqft_lot15_log'] = np.log(house_df_bin.loc[:,'sqft_lot15'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The original and the new distribution of these features is shown in the graphs below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_cols = [\"sqft_living_log\",\"sqft_lot_log\",\"sqft_basement_log\",\"sqft_living15_log\",\"sqft_lot15_log\"]\n\nfig, axes = plt.subplots(2,5,figsize=(13,5))\naxes = np.ravel(axes)\nfor num, ax in enumerate(axes):\n  if num<5:\n    sns.distplot(house_df_bin[cols[num]],ax=ax, color=custom_colors[num])\n  else:\n    sns.distplot(house_df_bin[log_cols[num-5]],ax=ax, color=custom_colors[num-5])\n    \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As before, original values are then dropped:"},{"metadata":{"trusted":true},"cell_type":"code","source":"house_df_bin.drop(cols, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **<font color='green'>3 Data Preprocessing/Feature Engineering</font>:**\n\n### 3.1 Pre-Processing Data\nApply pre-processing steps to your training and testing datasets separately in order to avoid data leakage.\n\n**Data normalization**\n\nIf the dataset has numerical features with different scales, standardize the data to create a dataset within the same scale.\nStandardScaler and MinMaxScaler are very popular data normalization methods.\n\n**OneHotEncoder()**\n\nIf the dataset that will be used for the regression model includes categorical and/or boolean-type columns, use OneHotEncoder to transform them into numeric arrays.\nBelow, categorical columns are determined and OneHotEncoder is initiated:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n\nhouse_df_bin['age_binned'] = house_df_bin['age_binned'].astype('int64') \nhouse_df_bin['age_rnv_binned'] = house_df_bin['age_rnv_binned'].astype('int64') \n\nnumerical_columns = house_df_bin.drop(['log_price','zipcode'], axis=1).columns\nscaler = MinMaxScaler()\n\ncategorical_columns = ['zipcode']\nohe = OneHotEncoder(handle_unknown='error', drop='first', sparse=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Features** and **Target** variables are then separated and defined:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_bin = house_df_bin.drop(['log_price'], axis=1)\ny = house_df_bin['log_price']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features dataset has 20 different features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Total number of Features: {len(X_bin.columns)}')\nX_bin.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Train-Validation-Test dataset:\n\nThe dataset is now divided in a training, validation and test dataset. Random_state is set to 1703... I need to pay my respects to St. Patrick!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\n\nX_train, X_test, y_train, y_test = train_test_split(X_bin, y,  test_size=.15, random_state=170378)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train,  test_size=.18, random_state=170378)\n\nprint(f\"Train Data Shape: {X_train.shape}\")\nprint(f\"Valid Data Shape: {X_valid.shape}\")\nprint(f\"Test Data Shape: {X_test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since later on the models will be evaluated using the price in level, the target features, y, are also stored in levels."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_lev = np.exp(y_train)\ny_valid_lev = np.exp(y_valid)\ny_test_lev = np.exp(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train dataset will be used to train the models, while test and validation to test the model generalization proprieties. IN particular the test dataset is created to make the submission forecast.\n\n**ColumnTransformer()**\n\nThis applies transformers to columns of an array or a pandas DataFrame. This is will be the first step of the pipeline to fit the models."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers =[('num', scaler, numerical_columns),('cat', ohe, categorical_columns)],remainder='drop')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Modeling\n**Instantiate Regression algorithm**\n\nTwo main regression algortithms will be tested. \n\n1. Random Forest Regression;\n2. Extra Tree Regression;\n3. XGBRegressor;\n4. Artificial Neural Networks.\n\nGridsearchCV is applied, using the default setting of 5 cross-validation folders, to fine tune the models hyperparameters for the first three models. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nfrom sklearn import set_config\nset_config(display='diagram',)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **4.1 Random Forest Regression**\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#rf_reg = RandomForestRegressor(n_jobs=-1,random_state= 1703,criterion= 'mse')\n#rf_params = {'max_depth': [15,17,20,22,25],\n#             'max_features':[18,20,22,25,30,35],             \n#             'n_estimators': [300,400,500,700,900]}\n#rf_gridsearch = GridSearchCV(estimator=rf_reg,\n#                              param_grid=rf_params,\n#                              cv=5,\n#                              return_train_score=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Create pipeline**\n\nAfter instantiating GridSearchCV, a pipeline is created. This pipeline will allow to perform data transformation (one hot encoder and data standardization) and GridSearchCV at the same time."},{"metadata":{"trusted":true},"cell_type":"code","source":"#pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n#                           ('m', rf_gridsearch)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fit model**\n\nThe model is fit on the training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%time\n#model = pipeline.fit(X_train, y_train)\n#model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the best hyperparameters selected for the Random Forest Model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#model['m'].best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cells above have been silenced, as running them would take to much time. However, the optimal hyperparameter selected are:\n\n- max_depth: 25,\n- max_features: 35,             \n- n_estimators: 900"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = 900 #model['m'].best_params_['n_estimators']\nmax_depth = 25 #model['m'].best_params_['max_depth']\nmax_features = 35 #model['m'].best_params_['max_features']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Model is now fitted using the Selected Hyperparameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfrom sklearn.model_selection import cross_val_score\n\nrf_opt = RandomForestRegressor(n_estimators=n_estimators,\n                               max_depth=max_depth,\n                               max_features=max_features,\n                               n_jobs=-1,\n                               random_state= 1703,\n                               criterion= 'mse')\n\nbest_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                                ('m', rf_opt)])\n\nprint(cross_val_score(best_pipeline,X_train, y_train,cv=5))\n\nbest_model = best_pipeline.fit(X_train, y_train)\n\n\n#dump(best_model, 'rf_1_best_model.joblib') \n#best_model = load('rf_1_best_model.joblib') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the results from the cross validation analysis we expect the Random Forest model to achieve an R2 score on the validation set around 0.89. \n\nThis result is confirmed in the next cell:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train Score: \nprint(f'Score on Training set: {best_model.score(X_train, y_train)}')\n#Validation Score:\nprint(f'Score on Valuation set: {best_model.score(X_valid, y_valid)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The models seems to overfit the training data. This is also confirmed by the following graphs, where the models visuaaly performas better on the training dataset in comparison to the validation set. This is clear from both the prediction on the price in log and in levels:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat_train = best_model.predict(X_train)\ny_hat_valid = best_model.predict(X_valid)\n\ny_hat_train_lev = np.exp(y_hat_train)\ny_hat_valid_lev = np.exp(y_hat_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize=(11,5), sharey=True, sharex=True)\n\nax[0].scatter(y_hat_train,y_train)\nax[1].scatter(y_hat_valid,y_valid, c='r')\nax[0].set_title('Train Dataset')\nax[1].set_title('Validation Dataset');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize=(11,5), sharey=True, sharex=True)\n\nax[0].scatter(y_hat_train_lev,y_train_lev)\nax[1].scatter(y_hat_valid_lev,y_valid_lev, c='r')\nax[0].set_title('Train Dataset')\nax[1].set_title('Validation Dataset');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A DataFRame to store the results is created:"},{"metadata":{"trusted":true},"cell_type":"code","source":"index = ['RandomForest','ExtraTree','XGBRegressor']\ncol = ['R2 Train', 'RMSE Train','R2 Valid', 'RMSE Valid']\n\nresults_df_log = pd.DataFrame(index=index, columns=col)\nresults_df_lev = pd.DataFrame(index=index, columns=col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The two key metrics (R2 and RMSE) are now calculated for the price in level and in log and then stored:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, r2_score\n\nmse_train_rf = mean_squared_error(y_train, y_hat_train, squared=False)\nmse_valid_rf = mean_squared_error(y_valid, y_hat_valid, squared=False)\n\nr2_train_rf = r2_score(y_train, y_hat_train)\nr2_valid_rf = r2_score(y_valid, y_hat_valid)\n\nprint(f'MSE Score on Training set: {mse_train_rf}')\nprint(f'MSE Score on Validation set: {mse_valid_rf}')\nprint('\\n')\nprint(f'R2 Score on Training set: {r2_train_rf}')\nprint(f'R2 Score on Validation set: {r2_valid_rf}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df_log.loc['RandomForest','R2 Train'] = r2_train_rf\nresults_df_log.loc['RandomForest','R2 Valid'] = r2_valid_rf\nresults_df_log.loc['RandomForest','RMSE Train'] = mse_train_rf\nresults_df_log.loc['RandomForest','RMSE Valid'] = mse_valid_rf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mse_train_rf = mean_squared_error(y_train_lev, y_hat_train_lev, squared=False)\nmse_valid_rf = mean_squared_error(y_valid_lev, y_hat_valid_lev, squared=False)\n\nr2_train_rf = r2_score(y_train_lev, y_hat_train_lev)\nr2_valid_rf = r2_score(y_valid_lev, y_hat_valid_lev)\n\nprint(f'MSE Score on Training set: {mse_train_rf}')\nprint(f'MSE Score on Valid set: {mse_valid_rf}')\nprint('\\n')\nprint(f'R2 Score on Training set: {r2_train_rf}')\nprint(f'R2 Score on Valid set: {r2_valid_rf}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = ['RandomForest','ExtraTree','XGBRegressor','Art Neural Net']\ncol = ['R2 Train', 'RMSE Train','R2 Valid', 'RMSE Valid']\n\nresults_df_lev.loc['RandomForest','R2 Train'] = r2_train_rf\nresults_df_lev.loc['RandomForest','R2 Valid'] = r2_valid_rf\nresults_df_lev.loc['RandomForest','RMSE Train'] = mse_train_rf\nresults_df_lev.loc['RandomForest','RMSE Valid'] = mse_valid_rf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On average, the RandomForest model error in evaluating the price of a propriety in the validation set is 134K. The average price in the sample is 545k USD, meaning an evaluation error of 23.5%. "},{"metadata":{},"cell_type":"markdown","source":"### 4.2 **Extra Tree Regressor**\n\nThe same approach used for the Random Forest Model is now used for the ExtraTrees:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#et_reg = ExtraTreesRegressor(n_jobs=-1,random_state= 1703,criterion= 'mse')\n#et_params = {'max_depth': [15,17,20,22,25],\n#             'max_features':[18,20,22,25,30,35],             \n#             'n_estimators': [300,400,500,700,900]}\n#et_gridsearch = GridSearchCV(estimator=et_reg,\n#                             param_grid=et_params,\n#                             cv=5,\n#                             return_train_score=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n#                           ('m', et_gridsearch)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = pipeline.fit(X_train, y_train)\n#model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model['m'].best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cells above have been silenced, as running them would take to much time. However, the optimal hyperparameter selected are:\n\n- max_depth: 25,\n- max_features: 35,             \n- n_estimators: 900"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = 900 #model['m'].best_params_['n_estimators']\nmax_depth = 25 #model['m'].best_params_['max_depth']\nmax_features = 35 #model['m'].best_params_['max_features']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfrom sklearn.model_selection import cross_val_score\n\net_opt = ExtraTreesRegressor(n_estimators=n_estimators,\n                               max_depth=max_depth,\n                               max_features=max_features,\n                               n_jobs=-1,\n                               random_state= 1703,\n                               criterion= 'mse')\n\net_best_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                                ('m', et_opt)])\n\nprint(cross_val_score(et_best_pipeline,X_train, y_train,cv=5))\n\net_best_model = et_best_pipeline.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the results from the cross validation analysis we expect the ExtraTree model to achieve an R2 score on the validation set around 0.89. \n\nThis result is confirmed in the next cell:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train Score: \nprint(f'Score on Training set: {et_best_model.score(X_train, y_train)}')\n#Validation Score:\nprint(f'Score on Valuation set: {et_best_model.score(X_valid, y_valid)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, the models seems to overfit the training data. Although the model achieves a slight improvement on the validation dataset, the gap with the train dataset is even wider. This is also confirmed by the following graphs, where the models visuaaly performs almost perfectly on the training dataset in comparison to the validation set. This is clear from both the prediction on the price in log and in levels:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat_train = et_best_model.predict(X_train)\ny_hat_valid = et_best_model.predict(X_valid)\n\ny_hat_train_lev = np.exp(y_hat_train)\ny_hat_valid_lev = np.exp(y_hat_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize=(11,5), sharey=True, sharex=True)\n\nax[0].scatter(y_hat_train,y_train)\nax[1].scatter(y_hat_valid,y_valid, c='r')\nax[0].set_title('Train Dataset')\nax[1].set_title('Validation Dataset')\n\nplt.suptitle('ExtraTree Regressor');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mse_train_et = mean_squared_error(y_train, y_hat_train, squared=False)\nmse_valid_et = mean_squared_error(y_valid, y_hat_valid, squared=False)\n\nr2_train_et = r2_score(y_train, y_hat_train)\nr2_valid_et = r2_score(y_valid, y_hat_valid)\n\nprint(f'MSE Score on Training set: {mse_train_et}')\nprint(f'MSE Score on Validation set: {mse_valid_et}')\nprint('\\n')\nprint(f'R2 Score on Training set: {r2_train_et}')\nprint(f'R2 Score on Training set: {r2_valid_et}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df_log.loc['ExtraTree','R2 Train'] = r2_train_et\nresults_df_log.loc['ExtraTree','R2 Valid'] = r2_valid_et\nresults_df_log.loc['ExtraTree','RMSE Train'] = mse_train_et\nresults_df_log.loc['ExtraTree','RMSE Valid'] = mse_valid_et","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mse_train_et = mean_squared_error(y_train_lev, y_hat_train_lev, squared=False)\nmse_valid_et = mean_squared_error(y_valid_lev, y_hat_valid_lev, squared=False)\n\nr2_train_et = r2_score(y_train_lev, y_hat_train_lev)\nr2_valid_et = r2_score(y_valid_lev, y_hat_valid_lev)\n\nprint(f'MSE Score on Training set: {mse_train_et}')\nprint(f'MSE Score on Valid set: {mse_valid_et}')\nprint('\\n')\nprint(f'R2 Score on Training set: {r2_train_et}')\nprint(f'R2 Score on Valid set: {r2_valid_et}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df_lev.loc['ExtraTree','R2 Train'] = r2_train_et\nresults_df_lev.loc['ExtraTree','R2 Valid'] = r2_valid_et\nresults_df_lev.loc['ExtraTree','RMSE Train'] = mse_train_et\nresults_df_lev.loc['ExtraTree','RMSE Valid'] = mse_valid_et","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df_lev","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On average, the ExtraTrees model error in evaluating the price of a propriety in the validation set is 121'088, slightly better than the RandomForest. The average price in the sample is 545k USD, meaning an evaluation error of 22.2%.\n\n### 4.3 **XGBRegressor**\n\nThe same approach used for the Random Forest/Extra Tree Model is now used for the XGBRegressor Model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyper-parameters to tune\n#xgb1 = XGBRegressor(nthread=4,subsample=0.9,colsample_bytree=0.7,min_child_weight=4,silent=1,objective='reg:squarederror',verbosity=0)\n\n#xg_param = {'learning_rate': [0.01, 0.03, 0.05, 0.1],\n#              'max_depth': [7, 8, 9, 10],\n#              'n_estimators': [200, 300, 500, 700, 900]}\n\n#Xb_gridsearch = GridSearchCV(estimator=xgb1,\n#                              param_grid=xg_param,\n#                              cv=5,\n#                              return_train_score=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#xg_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n#                              ('m', Xb_gridsearch)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%time\n#xg_best_model = xg_pipeline.fit(X_train, y_train)\n#xg_best_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#xg_best_model['m'].best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cells above have been silenced, as running them would take to much time. However, the optimal hyperparameter selected are:\n\n- Learning Rate: 0.03,\n- max_depth:8,             \n- n_estimators: 700"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_rate = 0.03 #xg_best_model['m'].best_params_.get('learning_rate')\nn_est = 700 #xg_best_model['m'].best_params_.get('n_estimators')\ntree_md = 8 #xg_best_model['m'].best_params_.get('max_depth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import cross_val_score\n# Various hyper-parameters to tune\nxgb_opt = XGBRegressor(learning_rate=learn_rate,\n                       n_estimators=n_est,\n                       max_depth=tree_md,\n                       nthread=4,\n                       subsample=0.9,\n                       colsample_bytree=0.7,\n                       min_child_weight=4,\n                       objective='reg:squarederror')\n\nbest_xg_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                                    ('m', xgb_opt)])\n\nprint(cross_val_score(best_xg_pipeline,X_train, y_train,cv=5))\n\nbest_xg_model = best_xg_pipeline.fit(X_train, y_train)\nbest_xg_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train Score: \nprint(f'Score on Training set: {best_xg_model.score(X_train, y_train)}')\n#Validation Score:\nprint(f'Score on Valuation set: {best_xg_model.score(X_valid, y_valid)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The models overfit affecting former model is partially reduced in XGBRegressor. Cross validation results and the score achieved on the validation set mark a significant improvements in comparison to the preceeding two models. In general we should expect a R2 score on unseen data around 0.91, marking an improvement in comparison to the two preceeding model in the range of 2%."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat_train = best_xg_model.predict(X_train)\ny_hat_valid = best_xg_model.predict(X_valid)\n\ny_hat_train_lev = np.exp(y_hat_train)\ny_hat_valid_lev = np.exp(y_hat_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize=(11,5), sharey=True, sharex=True)\n\nax[0].scatter(y_hat_train,y_train)\nax[1].scatter(y_hat_valid,y_valid, c='r')\nax[0].set_title('Train Dataset')\nax[1].set_title('Validation Dataset')\n\nplt.suptitle('XGBRegressor');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize=(11,5), sharey=True, sharex=True)\n\nax[0].scatter(y_hat_train_lev,y_train_lev)\nax[1].scatter(y_hat_valid_lev,y_valid_lev, c='r')\nax[0].set_title('Train Dataset')\nax[1].set_title('Validation Dataset');\n\nplt.suptitle('XGBRegressor');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mse_train_xgb = mean_squared_error(y_train, y_hat_train, squared=False)\nmse_valid_xgb = mean_squared_error(y_valid, y_hat_valid, squared=False)\n\nr2_train_xgb = r2_score(y_train, y_hat_train)\nr2_valid_xgb = r2_score(y_valid, y_hat_valid)\n\nprint(f'MSE Score on Training set: {mse_train_xgb}')\nprint(f'MSE Score on Validation set: {mse_valid_xgb}')\nprint('\\n')\nprint(f'R2 Score on Training set: {r2_train_xgb}')\nprint(f'R2 Score on Training set: {r2_valid_xgb}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df_log.loc['XGBRegressor','R2 Train'] = r2_train_xgb\nresults_df_log.loc['XGBRegressor','R2 Valid'] = r2_valid_xgb\nresults_df_log.loc['XGBRegressor','RMSE Train'] = mse_train_xgb\nresults_df_log.loc['XGBRegressor','RMSE Valid'] = mse_valid_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mse_train_xgb = mean_squared_error(y_train_lev, y_hat_train_lev, squared=False)\nmse_valid_xgb = mean_squared_error(y_valid_lev, y_hat_valid_lev, squared=False)\n\nr2_train_xgb = r2_score(y_train_lev, y_hat_train_lev)\nr2_valid_xgb = r2_score(y_valid_lev, y_hat_valid_lev)\n\nprint(f'MSE Score on Training set: {mse_train_xgb}')\nprint(f'MSE Score on Valid set: {mse_valid_xgb}')\nprint('\\n')\nprint(f'R2 Score on Training set: {r2_train_xgb}')\nprint(f'R2 Score on Valid set: {r2_valid_xgb}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df_lev.loc['XGBRegressor','R2 Train'] = r2_train_xgb\nresults_df_lev.loc['XGBRegressor','R2 Valid'] = r2_valid_xgb\nresults_df_lev.loc['XGBRegressor','RMSE Train'] = mse_train_xgb\nresults_df_lev.loc['XGBRegressor','RMSE Valid'] = mse_valid_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df_lev","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On average, the XGBRegressor model error in evaluating the price of a propriety in the validation set is 114K, the lowest value so far scored. The average price in the sample is 545k USD, meaning an evaluation error of 20.9%, more than 2.5% lower than the ExtraTrees Model."},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nXGBRagressor is the model delivering the best results on the validation dataset. The table below summarizes the overall results using the target feature in log:"},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df_log","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And in levels:"},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df_lev","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBRegressor comes at the top of the contest followed by the ExtraTreesRegressor. The Root-MeanSquaredError is 114k USD on the validation set. The model is now tested on the test dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat_test = best_xg_model.predict(X_test)\ny_hat_test_lev = np.exp(y_hat_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mse_test_xgb = mean_squared_error(y_test, y_hat_test, squared=False)\nr2_test_xgb = r2_score(y_test, y_hat_test)\n\nprint(f'MSE Score on Test set: {mse_test_xgb}')\nprint('\\n')\nprint(f'R2 Score on Test set: {r2_test_xgb}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mse_test_xgb = mean_squared_error(y_test_lev, y_hat_test_lev, squared=False)\nr2_test_xgb = r2_score(y_test_lev, y_hat_test_lev)\n\nprint(f'MSE Score on Test set: {mse_test_xgb}')\nprint('\\n')\nprint(f'R2 Score on Test set: {r2_test_xgb}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1,figsize=(6,5), sharey=True, sharex=True)\n\nax.scatter(y_hat_test_lev,y_test_lev);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results on the test dataset confirm the data collected from the validation set. The Root mean squared error on the test set is 103K and R2 of 0.919.  "},{"metadata":{},"cell_type":"markdown","source":"### Save prediction in a csv file "},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = pd.DataFrame(index=y_test.index,columns=['Real Value','Prediction'])\n\nprediction['Real Value'] = y_test_lev\nprediction['Prediction'] = y_hat_test_lev\n\n\n#Convert DataFrame to a csv file that can be uploaded\n#This is saved in the same directory as your notebook\nfilename = 'King County House Prediction.csv'\n\nprediction.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}