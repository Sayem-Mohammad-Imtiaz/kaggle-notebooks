{"cells":[{"metadata":{"_uuid":"94ed3a8a35b21c8725c59dc6a590d8cee3354387"},"cell_type":"markdown","source":"# **In this notebook, we will attempt to build various classification models to predict customer churn in a Telco company. **\n\n**Dataset taken from: https://www.kaggle.com/blastchar/telco-customer-churn**\n\n**1. Importing Libraries and Dataset**\n\n**2. Exploratory Analysis**\n\n**3. Data cleaning and preprocessing**\n\n**4. Feature Engineering**\n\n**5. Building Classification Models**\n\n**6. Hypertuning Parameters**\n\n**7. Conclusion**"},{"metadata":{"_uuid":"b72f25a65c9e4c21bc103191c48fd4278219898e"},"cell_type":"markdown","source":"**1. Importing Libraries and Dataset**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport plotly.offline as py\nimport plotly.tools as tls\npy.init_notebook_mode(connected=True)\nimport cufflinks as cf\nimport itertools\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.formula.api as sm\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn import feature_selection, model_selection\nimport scikitplot as skplt\nfrom sklearn.metrics import confusion_matrix,accuracy_score,average_precision_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score, scorer, roc_curve\nfrom sklearn import preprocessing\nfrom sklearn.metrics import f1_score\nfrom yellowbrick.classifier import DiscriminationThreshold\nimport time\nimport tensorflow as tf\nimport warnings\n#plt.style.use(['dark_background'])\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"../input\"))\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndata = data.drop([\"customerID\"],axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aec39d242f1cb4a75125ae1f7351424c05b7e302"},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4a55336590d346d93bf46cf523d375dff644734"},"cell_type":"markdown","source":"**2. Exploratory Analysis**"},{"metadata":{"_uuid":"ec23f48b3c3a85d7c8dc7b2aae92825df583df86"},"cell_type":"markdown","source":"Let's visualise the different features."},{"metadata":{"trusted":true,"_uuid":"59a69bc9c69142a8f4e6c5b17650e1a08a026633"},"cell_type":"code","source":"trace1 = go.Bar(\n            x=['No Churn','Churn'],\n            y=[sum(data[\"Churn\"]=='No'),sum(data[\"Churn\"]=='Yes')],\n            marker=dict(color=[\"red\",\"red\"]),\n            name=\"Churn Rate\",\n    )\n\ntrace2 = go.Bar(\n            x=['Males','Females'],\n            y=[sum(data[\"gender\"]=='Male'),sum(data[\"gender\"]=='Female')],\n            marker=dict(color=[\"yellow\",\"yellow\"]),\n            name='Gender'\n    )\n\n\ntrace3 = go.Bar(\n            x=['Single','Has Partner'],\n            y=[sum(data[\"Partner\"]=='No'),sum(data[\"Partner\"]=='Yes')],\n            marker=dict(color=[\"green\",\"green\"]),\n            name='R/ship Status'\n    \n    )\n\ntrace4 = go.Bar(\n            x=['Non-Senior Citizen','Senior Citizen'],\n            y=[sum(data[\"SeniorCitizen\"]==0),sum(data[\"SeniorCitizen\"]==1)],\n            marker=dict(color=[\"blue\",\"blue\"]),\n            name='Senior Citizen Rate'\n    )\ntrace5 = go.Bar(\n            x=['No Dependents','Dependents'],\n            y=[sum(data[\"Dependents\"]=='No'),sum(data[\"Dependents\"]=='Yes')],\n            marker=dict(color=[\"lime\",\"lime\"]),\n            name='Dependents Rate'\n    )\n\ntrace6 = go.Bar(\n            x=['DSL', 'Fiber optic', 'No'],\n            y=[sum(data[\"InternetService\"]=='DSL'),sum(data[\"InternetService\"]=='Fiber optic'),sum(data[\"InternetService\"]=='No')],\n            marker=dict(color=[\"green\",\"green\",\"green\"]),\n            name='Internet Service Distribution'\n    )\n\ntrace7 = go.Bar(\n            x=['Month-to-month', 'One year', 'Two year'],\n            y=[sum(data[\"Contract\"]=='Month-to-month'),sum(data[\"Contract\"]=='One year'),sum(data[\"Contract\"]=='Two year')],\n            marker=dict(color=[\"green\",\"green\",\"green\"]),\n            name='Contract Type Distribution'\n    )\n\ntrace8 = go.Histogram(x=data['TotalCharges'],name=\"Distribution of Total Charges\")\n\ntrace9 = go.Histogram(x=data['MonthlyCharges'],name=\"Distribution of Monthly Charges\")\n\nfig = tools.make_subplots(rows=3, cols=3,\n                          subplot_titles=[\"Churn Rate (%): \" + str(round(100*(sum(data[\"Churn\"]=='Yes')/data.shape[0]),2)) +\"%\",\n                                          \"Male: \" + str(round(100*(sum(data[\"gender\"]==\"Male\")/data.shape[0]),2)) + \"%, Female: \" + str(round(100-100*(sum(data[\"gender\"]==\"Male\")/data.shape[0]),2))+\"%\",\n                                          \"Single: \" + str(round(100*(sum(data[\"Partner\"]==\"No\")/data.shape[0]),2)) + \"%, Attached: \" + str(round(100-100*(sum(data[\"Partner\"]==\"No\")/data.shape[0]),2))+\"%\",\n                                          \"Senior Citizen: \" + str(round(100-100*(sum(data[\"SeniorCitizen\"]==0)/data.shape[0]),2))+\"%\",\n                                          \"Dependents: \" + str(round(100*(sum(data[\"Dependents\"]==\"Yes\")/data.shape[0]),2)) + \"%\",\n                                          \"Internet Service Distribution\",\n                                          \"Contract Type Distribution\",\n                                          \"Distribution of Total Charges\",\n                                          \"Distribution of Monthly Charges\"\n                                          ])\nfor i in fig['layout']['annotations']:\n    i['font'] = dict(size=12,color='black')\nfig.append_trace(trace1, 1,1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 3)\nfig.append_trace(trace4, 2, 1)\nfig.append_trace(trace5, 2, 2)\nfig.append_trace(trace6, 2, 3)\nfig.append_trace(trace7, 3, 1)\nfig.append_trace(trace8, 3, 2)\nfig.append_trace(trace9, 3, 3)\nfig['layout'].update(height=800, width=1000, title=\"<b>Distribution of Features<b>\")\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37b1ddc00385209758ce25881ced87ddfb63a8a3"},"cell_type":"markdown","source":"Let's view the rates of churn among different variables."},{"metadata":{"trusted":true,"_uuid":"607250c9c94247d53e2ec4396d2ed1ec88bc4c1b"},"cell_type":"code","source":"def churn_distribution(variable):\n    num = len(list(train[variable].unique()))\n    x=[]\n    for i in range(num):\n        x.append(go.Bar(\n            x=['Churn','No Churn'],\n            y=[data[data['Churn']=='Yes'][variable].value_counts()[i],data[data['Churn']=='No'][variable].value_counts()[i]],\n            name=str(data[data['Churn']==1][variable].value_counts().index[i])))\n    layout = go.Layout(\n        width=500,\n        height=400,\n        barmode='stack',\n        title = \"Churn rate among \" + str(variable)\n    )\n\n    fig = go.Figure(data=x, layout=layout)\n    py.iplot(fig, filename='stacked-bar')\n    \n    \ndef churn_stacked_bar(variable):\n    x1=list(data[variable].unique())\n    trace1 = go.Bar(\n        x=x1,\n        y=[data[data[variable]==x1[i]][\"Churn\"].value_counts()[0] for i in range(len(x1))],\n        name='No Churn'\n    )\n    trace2 = go.Bar(\n        x=x1,\n        y=[data[data[variable]==x1[i]][\"Churn\"].value_counts()[1] for i in range(len(x1))],\n        name='Churn'\n    )\n    layout = go.Layout(\n        width=500,\n        height=400,\n        barmode='stack',\n        title = \"Churn rate among \" + str(variable)\n    )\n    t=[trace1,trace2]\n\n    fig = go.Figure(data=t, layout=layout)\n    py.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"559300026e8a9e772e65bc3f0defc3f5e74e9415"},"cell_type":"code","source":"bin_cols   = ['gender',\n  'SeniorCitizen',\n  'Partner',\n  'Dependents',\n  'PhoneService',\n  'MultipleLines',\n  'InternetService',\n  'OnlineSecurity',\n  'OnlineBackup',\n  'DeviceProtection',\n  'TechSupport',\n  'StreamingTV',\n  'StreamingMovies',\n  'Contract',\n  'PaperlessBilling',\n  'PaymentMethod']\nfor i in bin_cols:\n    churn_stacked_bar(i)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3dd7d73d92cad30031b76f034f27678674fffa2d"},"cell_type":"markdown","source":"We can see that:\n\n- **Senior Citizens tend to have a higher rate of churn (about 45%) as compared to non-senior citizens (about 25%).**\n\n- **Customers that have do not have dependents tend to have a higher churn rate (about 30%)  as compared to those that have dependents (about 15%).**\n\n- **Customers that uses fiber optic has a much higher churn rate (about 40%) as compared to the rest of the internet services.**\n\n- **Customers that do not have online security, online backup, device protection, tech support all have a higher churn rate respectively**\n\n- **Customers that have short contracts (Month-to-Month) have a higher churn rate (about 40%) as compared to those with longer contracts**\n\n- **Customers that pay by electronic check have a higher churn rate (about 45%) compared to the rest of the methods**\n\n- **Customers that have paperless billing have higher churn rate (33%) as compared to those that do not (15%).**"},{"metadata":{"_uuid":"85d0e521c3b11feba9e1a66fcf5705ca2b7d6810"},"cell_type":"markdown","source":"Let's view the correlation among different variables."},{"metadata":{"trusted":true,"_uuid":"7e505e91a8ebf7b56ddbd5d6b4ebc26d381a575b"},"cell_type":"code","source":"cols = data.columns\nnumer_cols = data._get_numeric_data().columns\ncat_cols = list(set(cols) - set(numer_cols))\nfor i in cat_cols:\n    data[i] = data[i].astype('category')\n    data[i] = data[i].cat.codes\nf,ax = plt.subplots(figsize=(12,12))\nsns.heatmap(data.corr(), annot=True, linewidths=0.5, fmt= '.2f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56773c4eb3bd0e637526617662c45c06c513eeb1"},"cell_type":"code","source":"for i in data.columns:\n    print (i + \": \"+str(sum(data[i].isnull()))+\" missing values\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65020cadb5dc31aaecccbe9fa3456f0a5ae6f0ea"},"cell_type":"markdown","source":"**3. Data Cleaning and Preprocessing**"},{"metadata":{"trusted":true,"_uuid":"5b02ac50bda3336d0b49405ec86b0090c3960504"},"cell_type":"code","source":"#Target columns\nTarget = [\"Churn\"]\n#categorical columns\ncat_cols   = data.nunique()[data.nunique() < 6].keys().tolist()\ncat_cols   = [x for x in cat_cols if x not in Target]\n#numerical columns\nnum_cols   = [x for x in data.columns if x not in cat_cols + Target]\n#Binary columns with 2 values\nbin_cols   = data.nunique()[data.nunique() == 2].keys().tolist()\n#Columns more than 2 values\nmulti_cols = [i for i in cat_cols if i not in bin_cols]\n\n#Label encoding Binary columns\nle = LabelEncoder()\nfor i in bin_cols :\n    data[i] = le.fit_transform(data[i])\n    \n#Duplicating columns for multi value columns\ndata = pd.get_dummies(data = data,columns = multi_cols )\n\n#Scaling Numerical columns\nstd = StandardScaler()\nscaled = std.fit_transform(data[num_cols])\nscaled = pd.DataFrame(scaled,columns=num_cols)\n\n#dropping original values merging scaled values for numerical columns\ndata2 = data.copy()\ndata = data.drop(columns = num_cols,axis = 1)\ndata = data.merge(scaled,left_index=True,right_index=True,how = \"left\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4997e1fecdf243e7989af0efba4ac91cfa7d4747"},"cell_type":"markdown","source":"**4. Feature Engineering**"},{"metadata":{"_uuid":"6f71756083396869af53ece8e16c496fe07515a1"},"cell_type":"markdown","source":"We shall add 3 features based on our exploratory analysis:\n\n- **Whether a customer is both a senior citizens and have dependents**\n- **If a customer do not have any online security, backup, device protection and tech support**\n- **Whether a customer has both short contract and electronic check payment and paperless billing**"},{"metadata":{"trusted":true,"_uuid":"4bde81efabc7f13ea3c5102a21ae4c4a8d370c96"},"cell_type":"code","source":"data[\"Senior+Depend\"] = np.where(((data[\"SeniorCitizen\"]==1) & (data[\"Dependents\"]==1)),1,0)\ndata[\"no_extra_services\"] = np.where(((data[\"OnlineSecurity_0\"]==1) & (data[\"OnlineBackup_0\"]==1) & (data[\"DeviceProtection_0\"]==1) & (data[\"TechSupport_0\"]==1)),1,0)\ndata[\"ShortContract+electronic+paperless\"] = np.where(((data[\"Contract_0\"]==1) & (data[\"PaymentMethod_2\"]==1) & (data[\"PaperlessBilling\"]==1)),1,0)  #Month-to-month contract and electronic payment and paperless billing\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7717154a7dc6543846c026418b1252d4c8d6c7c"},"cell_type":"markdown","source":"**5. Building Classification Models**"},{"metadata":{"trusted":true,"_uuid":"0c07d19b7ceb62f4850bf85c7fdb3f2e64d231ff","scrolled":false},"cell_type":"code","source":"y = data.loc[:,\"Churn\"]\n#splitting train and test data \ntrain,test = train_test_split(data,test_size = .2 ,random_state = 0,stratify=y)\n##seperating dependent and independent variables\ncols    = [i for i in data.columns if i not in Target]\nx_train = train[cols]\ny_train = train[Target]\nx_test  = test[cols]\ny_test = test[Target]\n#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    ensemble.AdaBoostClassifier(learning_rate = 0.1, n_estimators= 100, random_state= 0),\n    ensemble.BaggingClassifier(max_samples= 0.5, n_estimators= 300, random_state= 0),\n    ensemble.ExtraTreesClassifier(criterion= 'gini', max_depth= 10, n_estimators= 10, random_state= 0),\n    ensemble.GradientBoostingClassifier(learning_rate= 0.05, max_depth= 4, n_estimators= 300, random_state= 0),\n    ensemble.RandomForestClassifier(criterion= 'gini', max_depth= 10, n_estimators= 300, oob_score= True, random_state= 0),\n    \n    linear_model.LogisticRegressionCV(fit_intercept= False, random_state= 0, solver= 'lbfgs'),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n\n    naive_bayes.BernoulliNB(alpha= 0.75),\n    naive_bayes.GaussianNB(),\n    \n\n    neighbors.KNeighborsClassifier(algorithm= 'brute', weights= 'uniform'),\n    \n\n    svm.SVC(C= 5, decision_function_shape= 'ovo', gamma= 0.1, probability= True, random_state= 0),\n\n    tree.DecisionTreeClassifier(),\n\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    XGBClassifier(learning_rate= 0.001, seed= 0)    \n    ]\n\ndata_x_bin = list(data.columns)\ndata_x_bin.remove('Churn')\n\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .2, train_size = .8, random_state = 0 ) \n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean','MLA Test Accuracy Mean','MLA Train Precision Mean','MLA Test Precision Mean','MLA Train Recall Mean', 'MLA Test Recall Mean', 'MLA Train F1 Mean','MLA Test F1 Mean',\"MLA Train AUROC Mean\",\"MLA Test AUROC Mean\",'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = data[Target]\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.at[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.at[row_index, 'MLA Parameters'] = str(alg.get_params())\n    alg.fit(x_train, y_train)\n    cv_results_recall = model_selection.cross_validate(alg, x_train, y_train, cv  = cv_split,scoring = 'recall')\n    cv_results_acc = model_selection.cross_validate(alg, x_train, y_train, cv  = cv_split,scoring = 'accuracy')\n    cv_results_f1 = model_selection.cross_validate(alg, x_train, y_train, cv  = cv_split,scoring = 'f1')\n    cv_results_auroc = model_selection.cross_validate(alg, x_train, y_train, cv  = cv_split,scoring = 'roc_auc')\n    cv_results_prec = model_selection.cross_validate(alg,x_train, y_train, cv  = cv_split,scoring = 'precision')\n    MLA_compare.at[row_index, 'MLA Time'] = round(cv_results_acc['fit_time'].mean(),2)\n    MLA_compare.at[row_index, 'MLA Train Accuracy Mean'] = round(cv_results_acc['train_score'].mean(),3)\n    MLA_compare.at[row_index, 'MLA Test Accuracy Mean'] = round(accuracy_score(y_test, alg.predict(x_test)),3) \n    MLA_compare.at[row_index, 'MLA Train Precision Mean'] = round(cv_results_prec['train_score'].mean(),3)\n    MLA_compare.at[row_index, 'MLA Test Precision Mean'] = round(precision_score(y_test, alg.predict(x_test)),3) \n    MLA_compare.at[row_index, 'MLA Train Recall Mean'] = round(cv_results_recall['train_score'].mean(),3)\n    MLA_compare.at[row_index, 'MLA Test Recall Mean'] = round(recall_score(y_test, alg.predict(x_test)),3) \n    MLA_compare.at[row_index, 'MLA Train F1 Mean'] = round(cv_results_f1['train_score'].mean(),3)\n    MLA_compare.at[row_index, 'MLA Test F1 Mean'] = round(f1_score(y_test, alg.predict(x_test)),3) \n    MLA_compare.at[row_index, 'MLA Train AUROC Mean'] = round(cv_results_auroc['train_score'].mean(),3)   \n    MLA_compare.at[row_index, 'MLA Test AUROC Mean'] = round(roc_auc_score(y_test, alg.predict(x_test)),3) \n    \n    \n    MLA_predict[MLA_name] = alg.predict(data[data_x_bin])\n    print(MLA_name + \" done\")\n    row_index+=1\n\nMLA_compare.sort_values(by = ['MLA Test Recall Mean'], ascending = False, inplace = True)\ndef highlight_max(s):\n    '''\n    highlight the maximum in a Series yellow.\n    '''\n    is_max = s == s.max()\n    return ['background-color: yellow' if v else '' for v in is_max]\nMLA_compare_copy = MLA_compare.copy()\nMLA_compare_copy= MLA_compare_copy.style.apply(highlight_max,subset=['MLA Test Accuracy Mean', 'MLA Test Precision Mean','MLA Test Recall Mean', 'MLA Test F1 Mean','MLA Test AUROC Mean'])\nMLA_compare_copy\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44fed79dddcd3366cadb4a5c0545d019fce49397"},"cell_type":"code","source":"def model_comparison(table,metric,color) :\n    tracer = go.Bar(y = table[\"MLA Name\"] ,\n                    x = table[metric],\n                    orientation = \"h\",name = metric ,\n                    marker = dict(line = dict(width =.7),\n                                  color = color)\n                   )\n    return tracer\n\nlayout = go.Layout(dict(title = \"Model performances\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"metric\",\n                                     zerolinewidth=1,\n                                     ticklen=5,gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        margin = dict(l = 250),\n                        height = 1200\n                       )\n                  )\n\n\ntrace1  = model_comparison(MLA_compare,\"MLA Test Accuracy Mean\",\"#6699FF\")\ntrace2  = model_comparison(MLA_compare,'MLA Test Recall Mean',\"red\")\ntrace3  = model_comparison(MLA_compare,'MLA Test F1 Mean',\"#33CC99\")\ntrace4  = model_comparison(MLA_compare,'MLA Test AUROC Mean',\"lightgrey\")\ntrace5  = model_comparison(MLA_compare,'MLA Test Precision Mean',\"yellow\")\ntraces = [trace1,trace2,trace3,trace4,trace5]\nfig = go.Figure(data=traces,layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"996ba0b9f5491891adfff2bdb9978d1da8bcffbe"},"cell_type":"code","source":"adaboost = ensemble.AdaBoostClassifier(learning_rate = 0.1, n_estimators= 100, random_state= 0).fit(x_train,y_train)\nbagging = ensemble.BaggingClassifier(max_samples= 0.5, n_estimators= 300, random_state= 0).fit(x_train,y_train)\nexttree = ensemble.ExtraTreesClassifier(criterion= 'gini', max_depth= 10, n_estimators= 10, random_state= 0).fit(x_train,y_train)\ngbc = ensemble.GradientBoostingClassifier(learning_rate= 0.05, max_depth= 4, n_estimators= 300, random_state= 0).fit(x_train,y_train)\nrfc = ensemble.RandomForestClassifier(criterion= 'gini', max_depth= 10, n_estimators= 300, oob_score= True, random_state= 0).fit(x_train,y_train)\nlogit = linear_model.LogisticRegressionCV(fit_intercept= False, random_state= 0, solver= 'lbfgs').fit(x_train,y_train)\nridge = linear_model.RidgeClassifierCV().fit(x_train,y_train)\nsgdc = linear_model.SGDClassifier().fit(x_train,y_train)\nperc = linear_model.Perceptron().fit(x_train,y_train)\nbnb = naive_bayes.BernoulliNB(alpha= 0.75).fit(x_train,y_train)\ngnb = naive_bayes.GaussianNB().fit(x_train,y_train).fit(x_train,y_train)\nknn = neighbors.KNeighborsClassifier(algorithm= 'brute', weights= 'uniform').fit(x_train,y_train)\nsvc_rbf = svm.SVC(C= 5, decision_function_shape= 'ovo', gamma= 0.1, probability= True, random_state= 0).fit(x_train,y_train)\ndtc = tree.DecisionTreeClassifier().fit(x_train,y_train)\nqda = discriminant_analysis.QuadraticDiscriminantAnalysis().fit(x_train,y_train)\nxgc = XGBClassifier(learning_rate= 0.001, seed= 0).fit(x_train,y_train)\n\nlst    = [adaboost,bagging,exttree,gbc,rfc,logit,ridge,sgdc,perc,bnb,gnb,knn,svc_rbf,dtc,qda,xgc]\n\nlength = len(lst)\n\nmods   = ['Ada Boost','Bagging Classifier','Extra Trees Classifier','Gradient Boosting Classifier',\n          'Random Forest Classifier','Logistic Regression','Ridge Classifier','SGD Classifier',\n          'Perceptron','Bernoulli Naive Bayes','Gaussian Naive Bayes', 'KNN Classifier','SVM Classifier RBF',\n          'Decision Tree','QDA','XGBoost Classifier']\n\nfig = plt.figure(figsize=(36,36))\nfig.set_facecolor(\"#F3F3F3\")\nfor i,j,k in itertools.zip_longest(lst,range(length),mods) :\n    plt.subplot(4,4,j+1)\n    predictions = i.predict(x_test)\n    conf_matrix = confusion_matrix(predictions,y_test)\n    sns.heatmap(conf_matrix,annot=True,fmt = \"d\",square = True,\n                xticklabels=[\"Actual not churn\",\"Actual churn\"],\n                yticklabels=[\"Predicted not churn\",\"Predicted churn\"],\n                linewidths = 3,linecolor = \"w\",cmap = \"Set2\",cbar=False)\n\n    plt.title(k,color = \"b\",fontsize=12)\n    sns.set(font_scale=1.1)\n    #plt.tight_layout()\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12,rotation=45)\n    plt.subplots_adjust(wspace = .8,hspace = .005)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3f29681adc4900f2a1c46044fcc3bc8159fd8c0"},"cell_type":"markdown","source":"We will now apply Synthetic Minority Oversampling Technique and train/test our model."},{"metadata":{"trusted":true,"_uuid":"edf49be4afec7be23fed6f9af3bb26c893ce901a"},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsmote_X = data[cols]\nsmote_Y = data[Target]\n\n#Split train and test data\nsmote_train_X,smote_test_X,smote_train_Y,smote_test_Y = train_test_split(smote_X,smote_Y,\n                                                                         test_size = .2 ,\n                                                                         stratify = smote_Y,\n                                                                         random_state = 0)\n\n#oversampling \nos = SMOTE(random_state = 0)\nos_smote_X,os_smote_Y = os.fit_sample(smote_train_X,smote_train_Y)\nos_smote_X = pd.DataFrame(data = os_smote_X,columns=cols)\nos_smote_Y = pd.DataFrame(data = os_smote_Y,columns=Target)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28198c4cba63e3c3208a227f8fc15f6be9991722"},"cell_type":"code","source":"os_smote_X.shape,os_smote_Y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db78e610587a1175d683ac0ed4e1af2e76b4dff5"},"cell_type":"code","source":"MLA = [\n    ensemble.AdaBoostClassifier(learning_rate = 0.1, n_estimators= 100, random_state= 0),\n    ensemble.BaggingClassifier(max_samples= 0.5, n_estimators= 300, random_state= 0),\n    ensemble.ExtraTreesClassifier(criterion= 'gini', max_depth= 10, n_estimators= 10, random_state= 0),\n    ensemble.GradientBoostingClassifier(learning_rate= 0.05, max_depth= 4, n_estimators= 300, random_state= 0),\n    ensemble.RandomForestClassifier(criterion= 'gini', max_depth= 10, n_estimators= 300, oob_score= True, random_state= 0),\n\n    linear_model.LogisticRegressionCV(fit_intercept= False, random_state= 0, solver= 'lbfgs'),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    naive_bayes.BernoulliNB(alpha= 0.75),\n    naive_bayes.GaussianNB(),\n    \n\n    neighbors.KNeighborsClassifier(algorithm= 'brute', weights= 'uniform'),\n    \n\n    svm.SVC(C= 5, decision_function_shape= 'ovo', gamma= 0.1, probability= True, random_state= 0),\n \n    tree.DecisionTreeClassifier(),\n    \n\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n\n    XGBClassifier(learning_rate= 0.001, seed= 0)    \n    ]\n\ndata_x_bin = list(data.columns)\ndata_x_bin.remove('Churn')\n\n\n\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .2, train_size = .8, random_state = 0 )\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean','MLA Test Accuracy Mean','MLA Train Precision Mean','MLA Test Precision Mean','MLA Train Recall Mean', 'MLA Test Recall Mean', 'MLA Train F1 Mean','MLA Test F1 Mean',\"MLA Train AUROC Mean\",\"MLA Test AUROC Mean\",'MLA Time']\nMLA_compare_smote = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict_smote = smote_test_Y\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare_smote.at[row_index, 'MLA Name'] = MLA_name\n    MLA_compare_smote.at[row_index, 'MLA Parameters'] = str(alg.get_params())\n    alg.fit(smote_train_X,smote_train_Y)\n    cv_results_recall = model_selection.cross_validate(alg, os_smote_X, os_smote_Y, cv  = cv_split,scoring = 'recall')\n    cv_results_acc = model_selection.cross_validate(alg, os_smote_X, os_smote_Y, cv  = cv_split,scoring = 'accuracy')\n    cv_results_f1 = model_selection.cross_validate(alg, os_smote_X, os_smote_Y, cv  = cv_split,scoring = 'f1')\n    cv_results_auroc = model_selection.cross_validate(alg, os_smote_X, os_smote_Y, cv  = cv_split,scoring = 'roc_auc')\n    cv_results_prec = model_selection.cross_validate(alg, os_smote_X, os_smote_Y, cv  = cv_split,scoring = 'precision')\n    MLA_compare_smote.at[row_index, 'MLA Time'] = round(cv_results_acc['fit_time'].mean(),2)\n    MLA_compare_smote.at[row_index, 'MLA Train Accuracy Mean'] = round(cv_results_acc['train_score'].mean(),3)\n    MLA_compare_smote.at[row_index, 'MLA Test Accuracy Mean'] = round(accuracy_score(y_test, alg.predict(x_test)),3)\n    MLA_compare_smote.at[row_index, 'MLA Train Precision Mean'] = round(cv_results_prec['train_score'].mean(),3)\n    MLA_compare_smote.at[row_index, 'MLA Test Precision Mean'] = round(precision_score(y_test, alg.predict(x_test)),3)\n    MLA_compare_smote.at[row_index, 'MLA Train Recall Mean'] = round(cv_results_recall['train_score'].mean(),3)\n    MLA_compare_smote.at[row_index, 'MLA Test Recall Mean'] = round(recall_score(y_test, alg.predict(x_test)),3)\n    MLA_compare_smote.at[row_index, 'MLA Train F1 Mean'] = round(cv_results_f1['train_score'].mean(),3)\n    MLA_compare_smote.at[row_index, 'MLA Test F1 Mean'] = round(f1_score(y_test, alg.predict(x_test)),3)   \n    MLA_compare_smote.at[row_index, 'MLA Train AUROC Mean'] = round(cv_results_auroc['train_score'].mean(),3)   \n    MLA_compare_smote.at[row_index, 'MLA Test AUROC Mean'] = round(roc_auc_score(y_test, alg.predict(x_test)),3) \n    \n    \n    MLA_predict_smote[MLA_name] = alg.predict(smote_test_X)\n    print(MLA_name + \" done\")\n    row_index+=1\n\n    \nMLA_compare_smote.sort_values(by = ['MLA Test Recall Mean'], ascending = False, inplace = True)\ndef highlight_max(s):\n    '''\n    highlight the maximum in a Series yellow.\n    '''\n    is_max = s == s.max()\n    return ['background-color: yellow' if v else '' for v in is_max]\nMLA_compare_smote_copy = MLA_compare_smote.copy()\nMLA_compare_smote_copy= MLA_compare_smote_copy.style.apply(highlight_max,subset=['MLA Test Accuracy Mean', 'MLA Test Precision Mean','MLA Test Recall Mean', 'MLA Test F1 Mean','MLA Test AUROC Mean'])\nMLA_compare_smote_copy\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e80ec1a9b3b70b7a82808aa107a336b018d8bc07"},"cell_type":"code","source":"trace1  = model_comparison(MLA_compare_smote,\"MLA Test Accuracy Mean\",\"#6699FF\")\ntrace2  = model_comparison(MLA_compare_smote,'MLA Test Recall Mean',\"red\")\ntrace3  = model_comparison(MLA_compare_smote,'MLA Test F1 Mean',\"#33CC99\")\ntrace4  = model_comparison(MLA_compare_smote,'MLA Test AUROC Mean',\"lightgrey\")\ntrace5  = model_comparison(MLA_compare_smote,'MLA Test Precision Mean',\"yellow\")\ntraces = [trace1,trace2,trace3,trace4,trace5]\nfig = go.Figure(data=traces,layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"101205299034aaa1d036042bcb39acda2c1d3a71"},"cell_type":"code","source":"adaboost = ensemble.AdaBoostClassifier(learning_rate = 0.1, n_estimators= 100, random_state= 0).fit(smote_train_X,smote_train_Y)\nbagging = ensemble.BaggingClassifier(max_samples= 0.5, n_estimators= 300, random_state= 0).fit(smote_train_X,smote_train_Y)\nexttree = ensemble.ExtraTreesClassifier(criterion= 'gini', max_depth= 10, n_estimators= 10, random_state= 0).fit(smote_train_X,smote_train_Y)\ngbc = ensemble.GradientBoostingClassifier(learning_rate= 0.05, max_depth= 4, n_estimators= 300, random_state= 0).fit(smote_train_X,smote_train_Y)\nrfc = ensemble.RandomForestClassifier(criterion= 'gini', max_depth= 10, n_estimators= 300, oob_score= True, random_state= 0).fit(smote_train_X,smote_train_Y)\nlogit = linear_model.LogisticRegressionCV(fit_intercept= False, random_state= 0, solver= 'lbfgs').fit(smote_train_X,smote_train_Y)\nridge = linear_model.RidgeClassifierCV().fit(smote_train_X,smote_train_Y)\nsgdc = linear_model.SGDClassifier().fit(smote_train_X,smote_train_Y)\nperc = linear_model.Perceptron().fit(smote_train_X,smote_train_Y)\nbnb = naive_bayes.BernoulliNB(alpha= 0.75).fit(smote_train_X,smote_train_Y)\ngnb = naive_bayes.GaussianNB().fit(x_train,y_train).fit(smote_train_X,smote_train_Y)\nknn = neighbors.KNeighborsClassifier(algorithm= 'brute', weights= 'uniform').fit(smote_train_X,smote_train_Y)\nsvc_rbf = svm.SVC(C= 5, decision_function_shape= 'ovo', gamma= 0.1, probability= True, random_state= 0).fit(smote_train_X,smote_train_Y)\ndtc = tree.DecisionTreeClassifier().fit(smote_train_X,smote_train_Y)\nqda = discriminant_analysis.QuadraticDiscriminantAnalysis().fit(smote_train_X,smote_train_Y)\nxgc = XGBClassifier(learning_rate= 0.001, seed= 0).fit(smote_train_X,smote_train_Y)\n\nlst    = [adaboost,bagging,exttree,gbc,rfc,logit,ridge,sgdc,perc,bnb,gnb,knn,svc_rbf,dtc,qda,xgc]\n\nlength = len(lst)\n\nmods   = ['Ada Boost','Bagging Classifier','Extra Trees Classifier','Gradient Boosting Classifier',\n          'Random Forest Classifier','Logistic Regression','Ridge Classifier','SGD Classifier',\n          'Perceptron','Bernoulli Naive Bayes','Gaussian Naive Bayes', 'KNN Classifier','SVM Classifier RBF',\n          'Decision Tree','QDA','XGBoost Classifier']\n\nfig = plt.figure(figsize=(36,36))\nfig.set_facecolor(\"#F3F3F3\")\nfor i,j,k in itertools.zip_longest(lst,range(length),mods) :\n    plt.subplot(4,4,j+1)\n    predictions = i.predict(x_test)\n    conf_matrix = confusion_matrix(predictions,y_test)\n    sns.heatmap(conf_matrix,annot=True,fmt = \"d\",square = True,\n                xticklabels=[\"Actual not churn\",\"Actual churn\"],\n                yticklabels=[\"Predicted not churn\",\"Predicted churn\"],\n                linewidths = 3,linecolor = \"w\",cmap = \"Set2\",cbar=False)\n\n    plt.title(k,color = \"b\",fontsize=12)\n    sns.set(font_scale=1.1)\n    #plt.tight_layout()\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12,rotation=45)\n    plt.subplots_adjust(wspace = .8,hspace = .005)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8e042b264fed915e364401695b6a2e6b2018ad3"},"cell_type":"markdown","source":"**6. Hypertuning Parameters**"},{"metadata":{"trusted":true,"_uuid":"db89208a2c6577a5b56b606e4e43b0765255504d"},"cell_type":"markdown","source":"Based on the above, I would choose Bernoulli NB method due to the highest. Let's now hypertune its parameters."},{"metadata":{"trusted":true,"_uuid":"cfd5d27692c51a974badaa35f9373f4592676266"},"cell_type":"code","source":"param_grid = dict(alpha = (0.001,0.01,0.05,0.1,0.5,1))\noptim_Berno_NB = GridSearchCV(naive_bayes.BernoulliNB(),param_grid = param_grid, scoring = 'f1_macro')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a41152d869bd03af38cf0e722de9005518d09617"},"cell_type":"code","source":"def alg_report(algorithm,training_x,testing_x,\n                                 training_y,testing_y,threshold_plot = True) :\n    \n    #model\n    algorithm.fit(training_x,training_y)\n    predictions   = algorithm.predict(testing_x)\n    probabilities = algorithm.predict_proba(testing_x)\n    \n    print (algorithm)\n    print (\"\\n Classification report : \\n\",classification_report(testing_y,predictions))\n    print (\"Accuracy Score   : \",accuracy_score(testing_y,predictions))\n    #confusion matrix\n    conf_matrix = confusion_matrix(testing_y,predictions)\n    #roc_auc_score\n    model_roc_auc = roc_auc_score(testing_y,predictions) \n    print (\"Area under curve : \",model_roc_auc)\n    fpr,tpr,thresholds = roc_curve(testing_y,probabilities[:,1])\n     \n    #plot roc curve\n    trace1 = go.Scatter(x = fpr,y = tpr,\n                        name = \"Roc : \" + str(model_roc_auc),\n                        line = dict(color = ('rgb(22, 96, 167)'),width = 2),\n                       )\n    trace2 = go.Scatter(x = [0,1],y=[0,1],\n                        line = dict(color = ('rgb(205, 12, 24)'),width = 2,\n                        dash = 'dot'))\n    \n    #plot confusion matrix\n    trace3 = go.Heatmap(z = conf_matrix ,x = [\"Predict Not churn\", \"Predict Churn\"],\n                        y = [\"Actual Not churn\",\"Actual Churn\"],\n                        showscale  = False,colorscale = \"Blues\",name = \"matrix\",\n                        xaxis = \"x2\",yaxis = \"y2\"\n                       )\n    \n    layout = go.Layout(dict(title=\"Model performance\" ,\n                            autosize = False,height = 500,width = 800,\n                            showlegend = False,\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            xaxis = dict(title = \"false positive rate\",\n                                         gridcolor = 'rgb(255, 255, 255)',\n                                         domain=[0, 0.6],\n                                         ticklen=5,gridwidth=2),\n                            yaxis = dict(title = \"true positive rate\",\n                                         gridcolor = 'rgb(255, 255, 255)',\n                                         zerolinewidth=1,\n                                         ticklen=5,gridwidth=2),\n                            margin = dict(b=200),\n                            xaxis2=dict(domain=[0.7, 1],tickangle = 90,\n                                        gridcolor = 'rgb(255, 255, 255)'),\n                            yaxis2=dict(anchor='x2',gridcolor = 'rgb(255, 255, 255)')\n                           )\n                  )\n    data = [trace1,trace2,trace3]\n    fig = go.Figure(data=data,layout=layout)\n    \n    py.iplot(fig)\n    \n    if threshold_plot == True : \n        visualizer = DiscriminationThreshold(algorithm)\n        visualizer.fit(training_x,training_y)\n        visualizer.poof()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2879d396906f2e4e622827789d194f64266fa673"},"cell_type":"code","source":"cols    = [i for i in data.columns if i not in Target]\nalg_report(optim_Berno_NB,x_train.values,x_test.values,y_train[\"Churn\"].values,y_test.values,cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a8af1d58e59d684dd42b7fc08f36814a3095c91"},"cell_type":"markdown","source":"**7. Conclusion**"},{"metadata":{"trusted":true,"_uuid":"cf6cdc87092b03529ea8f16e2521be3889e93e21"},"cell_type":"markdown","source":"Bernoulli Naive Bayes Model is selected as our final model to predict customer churn due to the F1-score, which balances precision and recall. "},{"metadata":{"trusted":true,"_uuid":"10c9597a212ae4ee41b297537de4b53802ab2ba6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}