{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-15T15:41:07.0788Z","iopub.execute_input":"2021-06-15T15:41:07.07913Z","iopub.status.idle":"2021-06-15T15:41:07.091849Z","shell.execute_reply.started":"2021-06-15T15:41:07.079102Z","shell.execute_reply":"2021-06-15T15:41:07.090556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"load the dataset and present examples","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv (r'/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv')\ndataset.head(3)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-15T15:41:07.093545Z","iopub.execute_input":"2021-06-15T15:41:07.094137Z","iopub.status.idle":"2021-06-15T15:41:07.132392Z","shell.execute_reply.started":"2021-06-15T15:41:07.094094Z","shell.execute_reply":"2021-06-15T15:41:07.131102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.output.value_counts()\nprint('\\n\\n')\ndataset.thall.value_counts()\n\n# Get one hot encoding of columns cp\none_hot = pd.get_dummies(dataset['cp'])\none_hot.columns = [str(col) + ' cp' for col in one_hot.columns]\n    \n# Drop column cp as it is now encoded\ndataset = dataset.drop('cp',axis = 1)\n# Join the encoded df\ndataset = dataset.join(one_hot)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T15:41:07.13417Z","iopub.execute_input":"2021-06-15T15:41:07.134485Z","iopub.status.idle":"2021-06-15T15:41:07.14698Z","shell.execute_reply.started":"2021-06-15T15:41:07.134452Z","shell.execute_reply":"2021-06-15T15:41:07.146121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T15:41:07.148312Z","iopub.execute_input":"2021-06-15T15:41:07.148722Z","iopub.status.idle":"2021-06-15T15:41:07.182233Z","shell.execute_reply.started":"2021-06-15T15:41:07.148647Z","shell.execute_reply":"2021-06-15T15:41:07.180913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Normalization of the data to range 0-1","metadata":{}},{"cell_type":"code","source":"# no division by 0 : \n# print(dataset_Norm.max())\nnormalization_factors = dataset.max()\n# normalize\ndataset_Norm = dataset.div(dataset.max(), axis='columns', level=None, fill_value=None)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-15T15:41:07.183327Z","iopub.execute_input":"2021-06-15T15:41:07.183579Z","iopub.status.idle":"2021-06-15T15:41:07.201311Z","shell.execute_reply.started":"2021-06-15T15:41:07.183556Z","shell.execute_reply":"2021-06-15T15:41:07.200182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('the data frame info:')\ndataset.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T15:41:07.202401Z","iopub.execute_input":"2021-06-15T15:41:07.202816Z","iopub.status.idle":"2021-06-15T15:41:07.231619Z","shell.execute_reply.started":"2021-06-15T15:41:07.202777Z","shell.execute_reply":"2021-06-15T15:41:07.23057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"separate the data to train and validation\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split as splt\ntrain_db, test_db = splt(dataset_Norm, test_size=0.25, train_size=None, random_state=0, shuffle=True, stratify=None)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-15T15:41:07.233327Z","iopub.execute_input":"2021-06-15T15:41:07.233914Z","iopub.status.idle":"2021-06-15T15:41:07.246233Z","shell.execute_reply.started":"2021-06-15T15:41:07.233874Z","shell.execute_reply":"2021-06-15T15:41:07.245192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_db.output.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T15:41:07.249304Z","iopub.execute_input":"2021-06-15T15:41:07.249849Z","iopub.status.idle":"2021-06-15T15:41:07.268365Z","shell.execute_reply.started":"2021-06-15T15:41:07.249817Z","shell.execute_reply":"2021-06-15T15:41:07.267309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_db.output.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T15:41:07.269892Z","iopub.execute_input":"2021-06-15T15:41:07.270217Z","iopub.status.idle":"2021-06-15T15:41:07.2887Z","shell.execute_reply.started":"2021-06-15T15:41:07.270184Z","shell.execute_reply":"2021-06-15T15:41:07.287781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"check the spliting ","metadata":{}},{"cell_type":"code","source":"test_db.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T15:41:07.289844Z","iopub.execute_input":"2021-06-15T15:41:07.290249Z","iopub.status.idle":"2021-06-15T15:41:07.318653Z","shell.execute_reply.started":"2021-06-15T15:41:07.290216Z","shell.execute_reply":"2021-06-15T15:41:07.317699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_db.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T15:41:07.320335Z","iopub.execute_input":"2021-06-15T15:41:07.320762Z","iopub.status.idle":"2021-06-15T15:41:07.352762Z","shell.execute_reply.started":"2021-06-15T15:41:07.320724Z","shell.execute_reply":"2021-06-15T15:41:07.351536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The train set info: \\n')\ntrain_db.info()\n\nprint('\\n\\nAnd the test set info: \\n')\ntest_db.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T15:41:07.3542Z","iopub.execute_input":"2021-06-15T15:41:07.354483Z","iopub.status.idle":"2021-06-15T15:41:07.375201Z","shell.execute_reply.started":"2021-06-15T15:41:07.354454Z","shell.execute_reply":"2021-06-15T15:41:07.374724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !conda install pytorch torchvision -c pytorch","metadata":{"execution":{"iopub.status.busy":"2021-06-15T15:41:07.376054Z","iopub.execute_input":"2021-06-15T15:41:07.376371Z","iopub.status.idle":"2021-06-15T15:41:07.390488Z","shell.execute_reply.started":"2021-06-15T15:41:07.37634Z","shell.execute_reply":"2021-06-15T15:41:07.388936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data_utils","metadata":{"execution":{"iopub.status.busy":"2021-06-15T15:41:07.392201Z","iopub.execute_input":"2021-06-15T15:41:07.392557Z","iopub.status.idle":"2021-06-15T15:41:07.409152Z","shell.execute_reply.started":"2021-06-15T15:41:07.392523Z","shell.execute_reply":"2021-06-15T15:41:07.40785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creat a data loader for pytorch","metadata":{}},{"cell_type":"code","source":"batch_size = 10\n\ntrain_y = torch.tensor(train_db['output'].values.astype(np.float32))\ntrain = torch.tensor(train_db.drop('output', axis = 1).values.astype(np.float32)) \n\ntest_y = torch.tensor(test_db['output'].values.astype(np.float32))\ntest = torch.tensor(test_db.drop('output', axis = 1).values.astype(np.float32)) \n\n\ntrain_tensor = data_utils.TensorDataset(train, train_y) \ntrain_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\n\ntest_tensor = data_utils.TensorDataset(test, test_y) \ntest_loader = data_utils.DataLoader(dataset = test_tensor, batch_size = batch_size, shuffle = True)\n\n# print(train_tensor[:5])\n\n# print(np.asarray(train_tensor[0][0])) # the first example\n# print(np.asarray(train_tensor[0][1])) # the first examples output","metadata":{"execution":{"iopub.status.busy":"2021-06-15T15:41:07.410361Z","iopub.execute_input":"2021-06-15T15:41:07.410594Z","iopub.status.idle":"2021-06-15T15:41:07.428415Z","shell.execute_reply.started":"2021-06-15T15:41:07.410571Z","shell.execute_reply":"2021-06-15T15:41:07.426711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the neural network architecture","metadata":{}},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n\n        # input_size = 13\n\n        hid_layer_1 = 2 ** 6\n        #hid_layer_2 = 2 ** 7\n        #hid_layer_3 = 2 ** 9\n        #hid_layer_4 = 2 ** 7\n        #hid_layer_5 = 2 ** 4\n\n        #self.fc1 = nn.Linear(13, hid_layer_1)\n        #self.fc2 = nn.Linear(hid_layer_1, hid_layer_2)\n        #self.fc3 = nn.Linear(hid_layer_2, hid_layer_3)\n        #self.fc4 = nn.Linear(hid_layer_3, hid_layer_4)\n        #self.fc5 = nn.Linear(hid_layer_4, hid_layer_5)\n        #self.fc6 = nn.Linear(hid_layer_5, 1)\n\n        self.fc1 = nn.Linear(len(dataset.columns) - 1, hid_layer_1)\n        self.fc2 = nn.Linear(hid_layer_1, hid_layer_1)\n        self.fc3 = nn.Linear(hid_layer_1, hid_layer_1)\n        self.fc4 = nn.Linear(hid_layer_1, hid_layer_1)\n        self.fc5 = nn.Linear(hid_layer_1, hid_layer_1)\n        self.fc6 = nn.Linear(hid_layer_1, 1)\n        \n        # dropout for the small and big matrixes\n        self.dropout1 = nn.Dropout(0.5)  # 1 -> 2 ; 4 -> 5\n        self.dropout2 = nn.Dropout(0.5)  # 2 -> 3 ; 3 -> 4\n\n    def forward(self, x):\n        # input through first HL. size change: 16 -> 2 ** 6\n        x.type(torch.LongTensor)\n        x = F.relu(self.fc1(x))\n        x = self.dropout1(x)\n\n        # HL1 output through 2nd HL. size change: 2 ** 6 -> 2 ** 7\n        #x = F.relu(self.fc2(x))\n        #x = self.dropout2(x)\n\n        # HL2 output through 3ed HL. size change: 2 ** 6 -> 2 ** 7\n        #x = F.relu(self.fc3(x))\n        #x = self.dropout2(x)                            ## lets simplify the model by avoiding a Hidden Layer\n\n        # HL3 output through 4th HL. size change: 2 ** 6 -> 2 ** 7\n        #x = F.relu(self.fc4(x))\n        #x = self.dropout1(x)                           ## lets simplify the model by avoiding a Hidden Layer\n\n        # HL4 output through 5th HL. size change: 2 ** 6 -> 2 ** 7\n        #x = F.relu(self.fc5(x))\n        #x = self.dropout1(x)\n\n        # HL5 output through the output unit. size change: 2 ** 6 -> 2 ** 7\n        x = torch.sigmoid(self.fc6(x))\n        return x\n    \n# initialize the NN\nmodel = Net()\nprint(model)\ndevice = 'cpu'\n\ntesting_example = torch.tensor([n*0.1 for n in range(len(dataset.columns) - 1)])\ntesting_example = testing_example.to(device=device)\nprint(model.forward(testing_example))","metadata":{"execution":{"iopub.status.busy":"2021-06-15T15:41:07.429933Z","iopub.execute_input":"2021-06-15T15:41:07.430227Z","iopub.status.idle":"2021-06-15T15:41:07.447711Z","shell.execute_reply.started":"2021-06-15T15:41:07.430202Z","shell.execute_reply":"2021-06-15T15:41:07.446051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/dansbecker/running-kaggle-kernels-with-a-gpu\nUSE_GPU = True\n\nif USE_GPU and torch.cuda.is_available():\n    print('using device: cuda')\nelse:\n    print('using device: cpu')\ntorch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T15:41:07.449749Z","iopub.execute_input":"2021-06-15T15:41:07.450196Z","iopub.status.idle":"2021-06-15T15:41:07.468446Z","shell.execute_reply.started":"2021-06-15T15:41:07.450162Z","shell.execute_reply":"2021-06-15T15:41:07.467647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## TODO: Specify loss and optimization functions\nfrom torch import optim\n# specify loss function\ncriterion = nn.BCELoss() # binary cross entropy loss function\n\n# specify optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay = 0.002)\n# number of epochs to train the model\nn_epochs = 500  # suggest training between 20-50 epochs\nrate_check = 10  # number of epochs between tests\n\nn_tests = n_epochs // rate_check # number of tests\naccuracy = np.zeros(n_tests)\nprint_flag = True\n\nloss_vec = np.zeros(n_epochs)\ntest_loss_vec = np.zeros(n_epochs//rate_check)\n\nfor epoch in range(n_epochs):\n    \n    # test performance on the training every 10 epochs\n    if (epoch//rate_check) * rate_check == epoch:\n        with torch.no_grad():\n            \n            model.eval()\n            test_loss = 0.0\n            print(test_db.shape[0])\n            predict = torch.zeros(test_db.shape[0])\n            g_truth = torch.zeros(test_db.shape[0])\n            idx = 0\n            \n            test_loss = 0.0\n            \n            for data, target in test_loader:\n                output = model(data)\n                batch_size = len(target)\n                predict[idx:idx + batch_size] = torch.round( torch.reshape(output, (-1,)) )\n                g_truth[idx:idx + batch_size] = target\n                idx += batch_size\n                \n                loss = criterion(torch.reshape(output, (-1,)), target)\n                test_loss += 1 * loss.item() * data.size(0)\n                if print_flag: \n                    print(1*loss.item()*data.size(0))\n                    \n                    print('data size = '+ str(data.size(0)))\n                    print_flag = False\n                \n            test_loss = test_loss / len(test_loader.dataset)\n            test_loss_vec[epoch//rate_check] = test_loss\n            \n            \n            correct = torch.sum(predict == g_truth)\n            if print_flag:\n                print('prediction:\\n')\n                # print(predict)\n                \n                print('\\ng_truth:\\n')\n                # print(g_truth)\n            test_size = len(predict)\n            accuracy[epoch//rate_check] = (correct/test_size)\n            print('The train accuracy is {:.3} of {} / {}'.format(accuracy[epoch//rate_check], correct, test_size))\n    \n    model.train()\n    # monitor training loss\n    train_loss = 0.0\n    \n    ###################\n    # train the model #\n    ###################\n\n    for data, target in train_loader:\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # uncomment to show first output and target\n        if print_flag: \n            # print(output)\n            # print(target)\n            # print_flag = False\n            pass\n        \n        # calculate the loss    \n        loss = criterion(torch.reshape(output, (-1,)), target)\n        if print_flag: \n            print(1*loss.item()*data.size(0))\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update running training loss\n        if print_flag: \n            print(1*loss.item()*data.size(0))\n            print_flag = False\n        train_loss += 1*loss.item()*data.size(0)\n    \n    \n    \n    # print training statistics \n    # calculate average loss over an epoch\n    train_loss = train_loss/len(train_loader.dataset)\n    \n    loss_vec[epoch] = train_loss\n    # print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n    #     epoch+1, \n    #     train_loss\n    #     ))\n","metadata":{"execution":{"iopub.status.busy":"2021-06-15T15:41:07.469779Z","iopub.execute_input":"2021-06-15T15:41:07.470305Z","iopub.status.idle":"2021-06-15T15:41:21.699697Z","shell.execute_reply.started":"2021-06-15T15:41:07.470255Z","shell.execute_reply":"2021-06-15T15:41:21.698782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"visualization of the accuracy during the training","metadata":{}},{"cell_type":"code","source":"from matplotlib.pyplot import plot as plt\nfrom matplotlib.pyplot import legend\n# plt(accuracy)\n# print (accuracy)\nprint('\\n\\n')\nplt([x/len(loss_vec) for x in range(len(loss_vec))],loss_vec)\nplt([x/len(test_loss_vec) for x in range(len(test_loss_vec))],test_loss_vec)\nlegend(['train loss','test loss'])\n","metadata":{"execution":{"iopub.status.busy":"2021-06-15T15:41:21.700995Z","iopub.execute_input":"2021-06-15T15:41:21.70128Z","iopub.status.idle":"2021-06-15T15:41:21.86263Z","shell.execute_reply.started":"2021-06-15T15:41:21.701251Z","shell.execute_reply":"2021-06-15T15:41:21.861298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}