{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# to do OHE labels\nfrom keras.utils import to_categorical\n# the only python lib we really need\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# read data\nimport pandas as pd\nmnist_test = pd.read_csv(\"../input/mnist-in-csv/mnist_test.csv\")\nmnist_train = pd.read_csv(\"../input/mnist-in-csv/mnist_train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# do numpy arrays\nXtrain = mnist_train.drop(['label'], axis=1).values\nYtrain =  mnist_train.loc[:, 'label'].values\nXtest = mnist_test.drop(['label'], axis=1).values\nYtest =  mnist_test.loc[:, 'label'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Xtrain.shape)\nprint(Ytrain.shape)\nprint(Xtest.shape)\nprint(Ytest.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot encoded Y\nYtrain_ohe = to_categorical(Ytrain) \nYtest_ohe = to_categorical(Ytest) \nprint(Ytrain.shape)\nprint(Ytrain_ohe.shape)\nprint(Xtest.shape)\nprint(Ytest_ohe.shape)\nprint(Ytrain[0])\nprint(Ytrain_ohe[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# functions\n# activations functions\ndef tanh(x):\n    #TODO: write tanh function\n\n# for tyhe last layer (output)\ndef softmax(x):\n    exp_scores = np.exp(x)\n    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True) \n\n# foraward propagation\ndef forward(X):\n    #TODO: write forward propagation layer by layer\n\ndef loss(y, X):\n    N = len(y)\n    yhat = forward(X)\n    logs = np.sum(np.log(yhat[range(N), y]))\n    return -1.0/N * logs\n\n# tanh derivative\ndef tanh_dev(x):\n    return 1.0-np.tanh(x)**2\n\n# predict and score\ndef predict(X):\n    return np.argmax(forward(X), axis=1)\n\ndef score(X, Y):\n    return np.mean(predict(X)==Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# layers size\ninput_layer = #TODO: What should be first layer size\nhidden_layer = 100\noutput_layer = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initial weights and bias\nW1 = np.random.randn( #TODO: what should be shape of hidden weights tesnsor ) \nb1 = np.random.randn( #TODO: what should be shape of hidden bias tensor ) \nW2 = np.random.randn( #TODO: what should be shape of output weights tesnsor )\nb2 = np.random.randn( #TODO: what should be shape of output bias tesnsor ) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = 0.1\nepochs = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(epochs):\n    \n    # Forward propagation\n    # TODO: code forward propagation\n    \n    # Backpropagation\n    # TODO: code backward error propagation\n    \n    # update weights and bias\n    # TODO: code waights and bias updating\n    \n    print(\"Epoch done: {:4d}, loss: {:6.4f}, score: {:.2f}\" .format(epoch+1, loss(Ytrain, Xtrain), score(Xtrain, Ytrain)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}