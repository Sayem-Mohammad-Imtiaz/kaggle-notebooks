{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Polynomial Regression ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"\n\n**For knowing more about Linear Regression on this Dataset I invite you to refer to my previous notebook about [Linear Regression Model for Real Estate](https://www.kaggle.com/amirkonjkav/linear-regression-model-for-real-estate).**\n","metadata":{}},{"cell_type":"markdown","source":"## Linear regression\n#### requires the relation between the dependent variable and the independent variable to be linear. What if the distribution of the data was more complex as shown in the below figure? Can linear models be used to fit non-linear data? How can we generate a curve that best captures the data as shown below?","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/imagefolder/A.png\")","metadata":{"execution":{"iopub.status.busy":"2021-09-12T06:41:45.249849Z","iopub.execute_input":"2021-09-12T06:41:45.250236Z","iopub.status.idle":"2021-09-12T06:41:45.264528Z","shell.execute_reply.started":"2021-09-12T06:41:45.250198Z","shell.execute_reply":"2021-09-12T06:41:45.263365Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### It is very difficult to fit a linear regression line in the above graph with a low value of error. Hence we can try to use the polynomial regression to fit a polynomial line so that we can achieve a minimum error or minimum cost function","metadata":{}},{"cell_type":"markdown","source":"# Polynomial","metadata":{}},{"cell_type":"markdown","source":"Polynomial regression is a form of Linear regression where only due to the Non-linear relationship between dependent and independent variables we add some polynomial terms to linear regression to convert it into Polynomial regression.","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-09-12T10:01:03.923843Z","iopub.execute_input":"2021-09-12T10:01:03.924229Z","iopub.status.idle":"2021-09-12T10:01:05.170219Z","shell.execute_reply.started":"2021-09-12T10:01:03.92419Z","shell.execute_reply":"2021-09-12T10:01:05.16946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/real-estate-price-prediction/Real estate.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-12T10:01:15.645542Z","iopub.execute_input":"2021-09-12T10:01:15.646467Z","iopub.status.idle":"2021-09-12T10:01:15.694601Z","shell.execute_reply.started":"2021-09-12T10:01:15.646414Z","shell.execute_reply":"2021-09-12T10:01:15.693977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-12T10:01:18.182018Z","iopub.execute_input":"2021-09-12T10:01:18.182779Z","iopub.status.idle":"2021-09-12T10:01:18.232404Z","shell.execute_reply.started":"2021-09-12T10:01:18.182719Z","shell.execute_reply":"2021-09-12T10:01:18.231298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define X and y","metadata":{}},{"cell_type":"code","source":"X = df.drop('Y house price of unit area', axis=1)\n\ny = df['Y house price of unit area']","metadata":{"execution":{"iopub.status.busy":"2021-09-12T10:01:19.604355Z","iopub.execute_input":"2021-09-12T10:01:19.604712Z","iopub.status.idle":"2021-09-12T10:01:19.612299Z","shell.execute_reply.started":"2021-09-12T10:01:19.604677Z","shell.execute_reply":"2021-09-12T10:01:19.61156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"markdown","source":"The sklearn.preprocessing package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators","metadata":{}},{"cell_type":"code","source":"poly_converter=PolynomialFeatures(degree=2, include_bias=True)\n\npoly_features= poly_converter.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T10:01:21.174052Z","iopub.execute_input":"2021-09-12T10:01:21.17448Z","iopub.status.idle":"2021-09-12T10:01:21.182173Z","shell.execute_reply.started":"2021-09-12T10:01:21.174447Z","shell.execute_reply":"2021-09-12T10:01:21.181521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('shape of X is :',X.shape)\nprint('shape of X after using polynomial :',poly_features.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T10:01:22.929641Z","iopub.execute_input":"2021-09-12T10:01:22.93005Z","iopub.status.idle":"2021-09-12T10:01:22.934774Z","shell.execute_reply.started":"2021-09-12T10:01:22.930019Z","shell.execute_reply":"2021-09-12T10:01:22.934192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### compare of these shapes show us that our features expand from 7 to 36!\n","metadata":{}},{"cell_type":"markdown","source":"## split data for Train and Test","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T10:01:25.846797Z","iopub.execute_input":"2021-09-12T10:01:25.847314Z","iopub.status.idle":"2021-09-12T10:01:25.853949Z","shell.execute_reply.started":"2021-09-12T10:01:25.847278Z","shell.execute_reply":"2021-09-12T10:01:25.852909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Polynomial Regression Model","metadata":{}},{"cell_type":"code","source":"model = LinearRegression()\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T10:01:28.255731Z","iopub.execute_input":"2021-09-12T10:01:28.256271Z","iopub.status.idle":"2021-09-12T10:01:28.286205Z","shell.execute_reply.started":"2021-09-12T10:01:28.256236Z","shell.execute_reply":"2021-09-12T10:01:28.285285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Prediction","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(X_test)\npd.DataFrame({'Y_Test': y_test,'Y_Pred':y_pred, 'Residuals':(y_test-y_pred) }).head()","metadata":{"execution":{"iopub.status.busy":"2021-09-12T10:01:30.375512Z","iopub.execute_input":"2021-09-12T10:01:30.37584Z","iopub.status.idle":"2021-09-12T10:01:30.390602Z","shell.execute_reply.started":"2021-09-12T10:01:30.375807Z","shell.execute_reply":"2021-09-12T10:01:30.389644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAE_Poly = metrics.mean_absolute_error(y_test, y_pred)\nMSE_Poly = metrics.mean_squared_error(y_test, y_pred)\nRMSE_Poly = np.sqrt(MSE_Poly)\n\npd.DataFrame([MAE_Poly, MSE_Poly, RMSE_Poly], index=['MAE', 'MSE', 'RMSE'], columns=['metrics'])","metadata":{"execution":{"iopub.status.busy":"2021-09-12T10:01:32.436299Z","iopub.execute_input":"2021-09-12T10:01:32.437254Z","iopub.status.idle":"2021-09-12T10:01:32.451911Z","shell.execute_reply.started":"2021-09-12T10:01:32.437198Z","shell.execute_reply":"2021-09-12T10:01:32.450859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compare to the simple linear regression","metadata":{}},{"cell_type":"code","source":"XS_train, XS_test, ys_train, ys_test = train_test_split(X, y, test_size=0.3, random_state=101)\n\nsimplemodel = LinearRegression()\nsimplemodel.fit(XS_train, ys_train)\nys_pred = simplemodel.predict(XS_test)\n\nMAE_simple  = metrics.mean_absolute_error(ys_test,ys_pred)\nMSE_simple  = metrics.mean_squared_error(ys_test,ys_pred)\nRMSE_simple = np.sqrt(MSE_simple)\n\n\npd.DataFrame({'Poly Metrics': [MAE_Poly, MSE_Poly, RMSE_Poly], \n              'Simple Metrics':[MAE_simple, MSE_simple, RMSE_simple]}, \n               index=['MAE', 'MSE', 'RMSE'])","metadata":{"execution":{"iopub.status.busy":"2021-09-12T10:03:47.675512Z","iopub.execute_input":"2021-09-12T10:03:47.675913Z","iopub.status.idle":"2021-09-12T10:03:47.703205Z","shell.execute_reply.started":"2021-09-12T10:03:47.675879Z","shell.execute_reply":"2021-09-12T10:03:47.702517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **We see there is no significant difference between simple and polynomial regression**","metadata":{}},{"cell_type":"markdown","source":"## Choose the best degree ","metadata":{}},{"cell_type":"markdown","source":"**We make a loop for surveying polynomial from two degree to 10 degree**","metadata":{}},{"cell_type":"code","source":"# Train List of RMSE per degree\ntrain_RMSE_list=[]\n\n#Test List of RMSE per degree\ntest_RMSE_list=[]\n\nfor d in range(1,8):\n\n    #create poly data set for degree (d)\n    polynomial_converter= PolynomialFeatures(degree=d, include_bias=True)\n    poly_features= polynomial_converter.fit_transform(X)\n\n    \n    #Split the dataset\n    X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3,random_state=101)\n    \n    #Train the Polynomial Model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    #Predicting on both Train & Test Data\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n    \n    #Evaluating the Model\n    train_RMSE = np.sqrt(metrics.mean_squared_error(y_train, y_train_pred))\n    test_RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))\n    \n    #Append the RMSE to the Train and Test List \n    train_RMSE_list.append(train_RMSE)\n    test_RMSE_list.append(test_RMSE)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-12T10:18:57.325901Z","iopub.execute_input":"2021-09-12T10:18:57.326242Z","iopub.status.idle":"2021-09-12T10:18:57.686454Z","shell.execute_reply.started":"2021-09-12T10:18:57.326212Z","shell.execute_reply":"2021-09-12T10:18:57.685457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(1,8), train_RMSE_list, label='Train RMSE')\nplt.plot(range(1,8), test_RMSE_list, label='Test RMSE')\n\nplt.xlabel('Polynomial Degree')\nplt.ylabel('RMSE')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-09-12T10:18:58.996269Z","iopub.execute_input":"2021-09-12T10:18:58.996546Z","iopub.status.idle":"2021-09-12T10:18:59.264374Z","shell.execute_reply.started":"2021-09-12T10:18:58.99652Z","shell.execute_reply":"2021-09-12T10:18:59.263357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **The graph shows that with increasing of degree our errors increasing too**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}