{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"age: age in years \n\nsex: (1 = male; 0 = female) \n\ncp: chest pain type \n\ntrestbps: resting blood pressure (in mm Hg on admission to the hospital) \n\nchol: serum cholestoral in mg/dl \n\nfbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) \n\nrestecg: resting electrocardiographic results \n\nthalach: maximum heart rate achieved \n\nexang: exercise induced angina (1 = yes; 0 = no) \n\noldpeak: ST depression induced by exercise relative to rest \n\nslope: the slope of the peak exercise ST segment \n\nca: number of major vessels (0-3) colored by flourosopy \n\nthal: 3 = normal; 6 = fixed defect; 7 = reversable defect \n\ntarget: 1 or 0"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib as mpl\nmpl.rcParams['figure.dpi'] = 400\nimport seaborn as sns #plotting package\nimport graphviz #to visualize decision trees\nfrom sklearn import datasets\ndf = pd.read_csv(\"../input/heart-disease-uci/heart.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns = ['age','sex', 'chest_pain', 'resting_blood_pressure', 'cholestoral', 'fasting_blood_sugar', 'resting_ECG', 'max_heart_rate', 'angina_from_exercise',\n          'st_depression', 'st_slope','major_vessels_with_flourosopy','thalassemia','target']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, col in enumerate(df.columns):\n    plt.figure(i)\n    sns.distplot(df[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_response = df.columns.tolist()\nfeatures_response","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df[features_response].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(corr,xticklabels = corr.columns.values,\n           yticklabels = corr.columns.values, center = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[features_response].iloc[:,:-1].values\ny = df[features_response].iloc[:,-1].values\nprint(X.shape, y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import f_classif\n[f_stat, f_p_value] = f_classif(X,y)\nf_test_df = pd.DataFrame({'Feature': features_response[:-1],\n                         'F statistic': f_stat,\n                         'p value': f_p_value})\nf_test_df.sort_values('p value')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Determining the best features from statistics using 80th percentile and above\nfrom sklearn.feature_selection import SelectPercentile\nselector = SelectPercentile(f_classif, percentile = 20)\nselector.fit(X,y)\nbest_feature_ix = selector.get_support()\nbest_feature_ix\nfeatures = features_response[:-1]\nbest_features = [features[counter] for counter in range(len(features))\n                if best_feature_ix[counter]]\nbest_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = np.array(df['target'])\nfeatures = df.drop('target', axis = 1)\nfeature_list = list(features.columns)\nfeatures = np.array(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using logisitc regression with 'angina' to determine a heart disease patient or not\nfrom sklearn.linear_model import LogisticRegression\nmy_lr = LogisticRegression() #Binary classifier\nmy_lr.C = 0.1\nmy_lr.solver = 'liblinear'\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\ndf['angina_from_exercise'].values.reshape(-1,1), df['target'].values, test_size =0.2, random_state=24)\nprint('The mean values of the train predictions are {}, \\nThe mean values of the testing predictions are {}.'.format(np.mean(y_train), np.mean(y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_lr.fit(X_train, y_train)\ny_pred = my_lr.predict(X_test)\nfrom sklearn import metrics\nmetrics.accuracy_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_proba = my_lr.predict_proba(X_test) #obtaining predicted probabilites\npos_proba = y_pred_proba[:,1] #putting second column of predicted probabilites into an array\n#plot an roc auc curve\nfpr,tpr, thresholds = metrics.roc_curve(y_test, pos_proba)\nplt.plot(fpr,tpr,'*-')\nplt.plot([0,1],[0,1],'r--')\nplt.legend(['Logistic regression','Random chance'])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC curve')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.roc_auc_score(y_test, pos_proba)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating the decision tree**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using a decision tree classifier to decide variable importance\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nXA_train, XA_test, ya_train, ya_test = \\\ntrain_test_split(df[features_response[:-1]].values, df['target'].values, \n                test_size=0.2, random_state = 24)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = tree.DecisionTreeClassifier(max_depth = 4)\ndt.fit(XA_train, ya_train)\ndot_data = tree.export_graphviz(dt, out_file = None, filled = True, \n                               rounded= True, feature_names = features_response[:-1],\n                                proportion=True, class_names = ['Not Heart Disease', 'Heart Disease'])\ngraph = graphviz.Source(dot_data)\ngraph","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ya_pred = dt.predict(XA_test)\nconfusionmatrix = metrics.confusion_matrix(ya_test, ya_pred)\nconfusionmatrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot an roc auc curve\nfpr,tpr, thresholds = metrics.roc_curve(ya_test, ya_pred)\nplt.plot(fpr,tpr,'*-')\nplt.plot([0,1],[0,1],'r--')\nplt.legend(['Logistic regression','Random chance'])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC curve')\nacc = metrics.accuracy_score(ya_test,ya_pred)\nprint('The Decision Trees accuracy is {}'.format(acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"P = sum(ya_test)\nTP = sum((ya_test==1) & ya_pred==1)\nFN = sum((ya_test==1) & (ya_pred==0))\nN = sum(ya_test ==0)\nTN = sum((ya_test==0)&(ya_pred==0))\nFP = sum((ya_test==0) & (ya_pred==1))\nSE = TP/(TP+FN)\nSP= TN/(TN+FP)\nprint('The sensitivity is {} and the the specificity is {}'.format(SE,SP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import export_graphviz\n\nvar_importance = list(dt.feature_importances_)\nfeature_importances1 = [(features, round(importance,2)) for features, importance in zip(feature_list, var_importance)]\n\n\nfeature_importances1 = sorted(feature_importances1, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances1];","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting variable importance\nplt.style.use('fivethirtyeight')\n# list of x locations for plotting\nx_values = list(range(len(var_importance)))\n# Make a bar chart\nplt.bar(x_values, var_importance, orientation = 'vertical')\n# Tick labels for x axis\nplt.xticks(x_values, feature_list, rotation='vertical')\n# Axis labels and title\nplt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using a random forest classifier for the decision tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a random forest Classifier. By convention, clf means 'Classifier'\nclf = RandomForestClassifier(n_jobs=2, random_state=0)\n\n# Train the Classifier to take the training features and learn how they relate\n# to the training y (the species)\nclf.fit(XA_train, ya_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_pred = clf.predict(XA_test)\n\nacc2 = metrics.accuracy_score(ya_test,rf_pred)\nprint('The Decision Trees accuracy is {}'.format(acc2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_nodes = []\nmax_depths = []\n\n# Stats about the trees in random forest\nfor ind_tree in clf.estimators_:\n    n_nodes.append(ind_tree.tree_.node_count)\n    max_depths.append(ind_tree.tree_.max_depth)\n    \nprint(f'Average number of nodes {int(np.mean(n_nodes))}')\nprint(f'Average maximum depth {int(np.mean(max_depths))}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import export_graphviz\n\nvar_importance1 = list(clf.feature_importances_)\nfeature_importances2 = [(features, round(importance,2)) for features, importance in zip(feature_list, var_importance1)]\n\n\nfeature_importances2 = sorted(feature_importances2, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances2];","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting variable importance\nplt.style.use('fivethirtyeight')\n# list of x locations for plotting\nx_values = list(range(len(var_importance1)))\n# Make a bar chart\nplt.bar(x_values, var_importance1, orientation = 'vertical')\n# Tick labels for x axis\nplt.xticks(x_values, feature_list, rotation='vertical')\n# Axis labels and title\nplt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_response = df.columns.tolist()\n\nitems_to_remove = ['age', 'cholestoral', 'st_slope','max_heart_rate','sex', 'resting_blood_pressure', 'fasting_blood_sugar', 'resting_ECG','angina_from_exercise','target']\n\nfeatures_response = [item for item in features_response if item not in items_to_remove]\nfeatures_response\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"newXA_train, newXA_test, newya_train, newya_test = \\\ntrain_test_split(df[features_response[:-1]].values, df['target'].values, \n                test_size=0.2, random_state = 24)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}