{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Goal\n\nGoal is to get familiar with implementing a DNN classifier\n\n# General approach\n1. Load data\n2. Visualise data in a meaningful manner, cleaning it up if necessary\n3. Create a basic DNN classifier\n4. See if I can tune it ?\n\n# 1. Load Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Configure Seaborn plot style\nsns.set_style('dark')\nsns.set_palette('viridis')\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nprint(\"Available files:\")\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"raw_data = pd.read_csv(\"../input/breast-cancer-wisconsin-data/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying a overview of data in each column\nfor c in raw_data.columns:\n    na = raw_data[c].isna().sum()\n    print(f\"{raw_data[c].dtype} [{c}] ->  - {na} NaN values ({100*na/raw_data.shape[0]:.2f}%)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**First impressions:**\n* Columns that can be dropped (useless): 'id' & 'Unnamed: 32'\n* Column used for labelling : 'diagnosis'\n* 30 remaining columns are to be numerical data and have no missing values\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting raw data into features (X) and labels (y)\ncols_to_drop = ['id','Unnamed: 32']\ndata = raw_data.drop(cols_to_drop,axis=1)\nX_0 = data.drop(['diagnosis'],axis=1)\ny_0 = data['diagnosis']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Visualisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look for imbalance in data Labels that could skew our model\nmalignant = y_0.value_counts()['M']\nbenign = y_0.value_counts()['B']\ntotal = y_0.shape[0]\nprint(f\"Malignant entries: {malignant} ({100*malignant/total:.2f}%)\")\nprint(f\"Benign entries: {benign} ({100*benign/total:.2f}%)\")\nprint(f\"Others: {total-malignant-benign}\")\n\nax = sns.countplot(y_0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting all features individually separated by diagnosis on a 6*5 grid, because why not ?\nplot_rows = 6\nplots_per_row = 5\ntotal_plots = plot_rows * plots_per_row\n\nfig, axs = plt.subplots(plot_rows,plots_per_row,figsize=(18,plot_rows*4))\naxs = axs.ravel()\n\ni=0\nfor feature in X_0.columns[:total_plots]:\n    axs[i].set_title(feature)\n    for d in y_0.unique():\n        sns.kdeplot(ax=axs[i], data=data.loc[data.diagnosis == d, feature],\n                   label=d,\n                   shade=True)\n    i+=1\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I see at least 7 features that don't seem useful for our classification :\n* 'fractal_dimension_mean'\n* 'texture_se'\n* 'smoothness_se'\n* 'symmetry_se'\n* 'fractal_dimension_se'\n* 'symmetry_worst'\n* 'fractal_dimension_worst'\n\nThere are most likely others, but less obvious so i'll leave them in for now"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Updating the list of columns that can be dropped\ncols_to_drop = ['fractal_dimension_mean',\n                'texture_se','smoothness_se','symmetry_se',\n                'fractal_dimension_se','symmetry_worst',\n                'fractal_dimension_worst']\n\nX = X_0.drop(cols_to_drop, axis=1)\n\n# We are now left with 23 columns\nX.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoding = {'M':1,\n            'B':0}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Basic Deep Neural Network\n\nI'll start by making a DNN with :\n* 23 inputs scaled down to a range of 0 - 1\n* 2 hidden layers with 128 nodes each, 10% dropout, ReLU activation function\n* 1 output, sigmoid activation function\n\nThis is a wild guess, I have no idea what I'm doing at this point..."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\n\n\nle = LabelEncoder()\ny = le.fit_transform(y_0)\n\n\nX_train,X_valid,y_train,y_valid = train_test_split(X,y,test_size=0.2,random_state=42)\n\ninput_shape = [X.shape[1]]\n\n\n# Layer configuration\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=input_shape),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.1),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.1),\n    layers.Dense(1, activation='sigmoid')\n])\n\n\n# Putting an early stopper so I don't need to worry to much about epochs\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.01,\n    patience=10,\n    restore_best_weights=True)\n\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy']\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid,y_valid),\n    batch_size=50,\n    epochs=500,\n    callbacks=[early_stopping]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\n# Start the plot at epoch 10\nhistory_df.loc[10:, ['loss', 'val_loss']].plot()\nhistory_df.loc[10:, ['binary_accuracy', 'val_binary_accuracy']].plot()\n\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df['val_loss'].min(), \n              history_df['val_binary_accuracy'].max()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"...97% accuracy :)\n\n# 4. Tuning\n\nI don't think this is worth tuning, so i'll try to make it worse instead\n\n# 4.1. Narrower, very deep DNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\n\n\nle = LabelEncoder()\ny = le.fit_transform(y_0)\n\n\nX_train,X_valid,y_train,y_valid = train_test_split(X,y,test_size=0.2,random_state=42)\n\ninput_shape = [X.shape[1]]\n\n\n# Layer configuration\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=input_shape),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.1),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.1),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.1),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.1),\n    layers.Dense(1, activation='sigmoid')\n])\n\n\n# Putting an early stopper so I don't need to worry to much about epochs\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.01,\n    patience=10,\n    restore_best_weights=True)\n\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy']\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid,y_valid),\n    batch_size=50,\n    epochs=500,\n    callbacks=[early_stopping],\n    verbose=False\n)\n\nhistory_df = pd.DataFrame(history.history)\n# Start the plot at epoch 0\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot()\n\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df['val_loss'].min(), \n              history_df['val_binary_accuracy'].max()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This one is aweful, perfect :)\n\n# 4.2. Very wide & shallow DNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\n\n\nle = LabelEncoder()\ny = le.fit_transform(y_0)\n\n\nX_train,X_valid,y_train,y_valid = train_test_split(X,y,test_size=0.2,random_state=42)\n\ninput_shape = [X.shape[1]]\n\n\n# Layer configuration\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=input_shape),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.1),\n    layers.Dense(1, activation='sigmoid')\n])\n\n\n# Putting an early stopper so I don't need to worry to much about epochs\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.01,\n    patience=10,\n    restore_best_weights=True)\n\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy']\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid,y_valid),\n    batch_size=50,\n    epochs=500,\n    callbacks=[early_stopping],\n    verbose=False\n)\n\nhistory_df = pd.DataFrame(history.history)\n# Start the plot at epoch 0\nhistory_df.loc[5:, ['loss', 'val_loss']].plot()\nhistory_df.loc[5:, ['binary_accuracy', 'val_binary_accuracy']].plot()\n\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df['val_loss'].min(), \n              history_df['val_binary_accuracy'].max()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nDon't go too deep"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}