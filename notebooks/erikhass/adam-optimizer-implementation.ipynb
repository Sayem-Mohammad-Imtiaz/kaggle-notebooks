{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Workbook to test Adam optimizer implementation**\n\n*Goal*\n\nLearn how to implement the Adam optimizer with regularization and start hyper-parameter tuning on a linear regression problem.\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport sys\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/electric-motor-temperature/pmsm_temperature_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = pd.DataFrame(data, columns= [\"pm\"])\ndata = data.drop([\"profile_id\", \"stator_yoke\", \"stator_winding\", \"stator_tooth\", \"pm\"],1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target.isna().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_split = 0.8\nm = data.shape[0]\ntraining_size = int(training_split*m)\n\ntraining_data = data.iloc[:training_size, :]\ntraining_data.insert(0, \"Bias\", 1)\ntraining_label = target.iloc[:training_size]\n\ntest_data = data.iloc[:training_size, :]\ntest_data.insert(0,\"Bias\",1)\ntest_label = target.iloc[:training_size]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def computeCost(data, target, theta, lamb = 0):\n    m = data.shape[0]\n    J = 2/m * np.sum(np.subtract(np.dot(data, theta), target)**2) + lamb * np.linalg.norm(theta)\n    return J\n\ndef calculateGradient(data, target, theta, optParams):\n    m = optParams[\"m\"]\n    lambd = optParams[\"lambd\"]\n    theta_grad = np.zeros(theta.shape)\n    #don't apply the regularization to b (theta[0])\n    theta_grad = 1/m * np.transpose(np.dot(np.transpose(np.subtract(np.dot(data, theta), target)),data)) + np.r_[np.zeros((1,1)), np.multiply(lambd/m, theta[1:,:])]\n    return theta_grad\n    \ndef calcBatch(data, target, theta, batchStart, batch_size, optParams):\n    theta_grad = calculateGradient(data.iloc[batchStart:batchStart+batch_size,:], target.iloc[batchStart:batchStart+batch_size], theta, optParams)\n    theta = updateTheta(theta, theta_grad, optParams)\n    return theta\n    \ndef updateTheta(theta, theta_grad, optParams):\n    if optParams[\"optimizer\"] == \"sgd\":\n        learning_rate = optParams[\"lr\"]\n        theta = theta - learning_rate * theta_grad\n    elif optParams[\"optimizer\"] == \"adam\":\n        beta1 = optParams[\"beta1\"]\n        beta2 = optParams[\"beta2\"]\n        epsilon = optParams[\"epsilon\"]\n        mt = optParams[\"mt\"]\n        vt = optParams[\"vt\"]\n        learning_rate = optParams[\"lr\"]\n        t = optParams[\"t\"]\n        m = beta1 * mt + (1.0 - beta1) * theta_grad\n        v = beta2 * vt + (1.0 - beta2) * theta_grad**2\n        #m_hat = np.divide(m, 1.0 - np.power(beta1, t))\n        #v_hat = np.divide(m, 1.0 - np.power(beta2, t))\n        lrt = learning_rate * np.sqrt(1.0 - beta2**t)/(1.0 - beta1**t)\n        theta = theta - lrt * np.divide(m, (np.sqrt(v) + epsilon))\n    else:\n        theta = 0\n        \n    optParams[\"mt\"] = m\n    optParams[\"vt\"] = v\n    return theta","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(data, target, epochs, batch_size, learning_rate = 0.01, lambd = 0, beta1 = 0.9, beta2=0.999, epsilon = 1e-8, optimizer=\"sgd\"):\n\n    theta = np.random.randn(data.shape[1],1)\n    J_hist = np.zeros((epochs, 1))\n    m = data.shape[0]\n    numBatches = int(m/batch_size)\n    calcLastBatch = m%batch_size > 0\n    \n    optParams = {}\n    optParams[\"m\"] = m\n    optParams[\"lr\"] = learning_rate\n    optParams[\"lambd\"] = lambd\n    optParams[\"beta1\"] = beta1\n    optParams[\"beta2\"] = beta2\n    optParams[\"epsilon\"] = epsilon\n    optParams[\"mt\"] = np.zeros((data.shape[1],1))\n    optParams[\"vt\"] = np.zeros((data.shape[1],1))\n    optParams[\"optimizer\"] = optimizer\n    \n    for i in range(epochs):\n        J_hist[i] = computeCost(data, target, theta, lambd)\n        optParams[\"t\"] = float(i) + 1\n        for currBatch in range(0):\n            theta = calcBatch(data, target, theta, currBatch*batch_size, batch_size, optParams)  \n        if calcLastBatch:\n            theta = calcBatch(data, target, theta, numBatches*batch_size, m - numBatches*batch_size, optParams)\n    \n    return theta, J_hist\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"theta, J_hist = train_model(training_data, training_label, epochs = 5000, batch_size = 512, learning_rate= 0.0001, lambd = 1, optimizer = \"adam\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(np.subtract(np.dot(test_data, theta),test_label)**2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(J_hist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.plot(np.dot(test_data,theta))\nplt.plot(test_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(theta)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}