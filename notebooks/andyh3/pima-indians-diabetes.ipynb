{"cells":[{"metadata":{"_uuid":"54854fd6-ec89-4e64-91c1-0e225032dd00","_cell_guid":"dcd94460-2baf-4629-aa3a-708bd9338183","trusted":true},"cell_type":"markdown","source":"# Setting up imports"},{"metadata":{"_uuid":"8dcf667a-e08d-497f-865a-bd1e0dcdc952","_cell_guid":"da5d70f4-ee95-43f2-958e-0c96f873a9ef","trusted":true},"cell_type":"code","source":"import warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom fancyimpute.knn import KNN\nfrom sklearn import metrics, svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import (GridSearchCV, KFold, StratifiedKFold,\n                                     cross_val_predict, cross_val_score,\n                                     train_test_split)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\n\nwarnings.filterwarnings('ignore')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6867154c-86e3-4ddb-8834-159fdb1ec790","_cell_guid":"f3ff113c-5192-494d-a47a-9b675c72b70a","trusted":true},"cell_type":"markdown","source":"# Begin Basic Exploratory Data Analysis (EDA)"},{"metadata":{"_uuid":"c4a14803-32cf-4a8d-8334-649aa81b1bc2","_cell_guid":"32465228-5f7d-4d04-8468-1829f6164fa9","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe8395f8-4953-41dd-a728-e813d2afbff8","_cell_guid":"056f5c02-2e3a-46f4-a3bd-1ac7fe82fc73","trusted":true},"cell_type":"code","source":"df.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6d5f720-a9af-492b-88e2-c428378866a7","_cell_guid":"83b0ec1c-ff20-4513-a52e-8802aeb4719c","trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"312bd467-bfc9-4e2c-be8a-dbfc31d9c4e1","_cell_guid":"6b7033e3-6c68-430d-9c13-f3d21ebdd4ff","trusted":true},"cell_type":"markdown","source":"From `df.describe()`, we notice that the minimum values for `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI` are `0`, which probably does not make sense. Given that, we should replace the replace the `0`s with a more suitable value in a process called _imputation_. We would also imputate data if any of the values are NULL (i.e. `NaN`), but `df.isnull().sum()` above shows that there are no null values. \n \n Generally metrics such as the _mean_, or _median_ are used to imputate data. One might choose to use the _mean_ to imputate data if the column(s) are more of less normally distributed as skewness can impact the value. If there is some skewness, the _median_ is probably a better choice. \n\nOne issue with using the _mean_ or _median_ to imputate data is that it does not consider any correlations among the various features. There exists advanced techniques out there that imputate data by way of machine learning algorithms that predict the missing values. For our dataset we choose to imputate our data with k-nearest neighbor (kNN) by way of a package called `fancyimpute`. `fancyimpute` will only work if the values are actual NULLs, so before imputing, we need to replace the `0` values with `np.nan`\n"},{"metadata":{"_uuid":"de4d78c2-8c82-4b95-bc8a-4a4df4133b67","_cell_guid":"3a7fa75b-b1ff-41b6-88fe-f985607c3ff3","trusted":true},"cell_type":"code","source":"features_with_zero_values = ['BMI', 'BloodPressure', 'Glucose', 'Insulin', 'SkinThickness']\ndf[features_with_zero_values] = df[features_with_zero_values].replace(0, np.nan)\ndata = KNN(k=5).fit_transform(df.values)\ndf = pd.DataFrame(data, columns=df.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"939db5d2-b333-4458-8cc7-d53d85609f04","_cell_guid":"22a166e8-495f-4cbb-abd3-524621de3447","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc611000-9ec7-44d6-be21-0d9c2e07a360","_cell_guid":"c2dfda60-febb-4ceb-9301-938d9c0e31ed","trusted":true},"cell_type":"markdown","source":"Notice the `0` values in the `Insulin` and `SkinThickness` columns are now replaced with an imputated value. Now let's continue on with our data exploration"},{"metadata":{"_uuid":"70140433-a1c7-42d9-8443-ce5c30e1e61b","_cell_guid":"1322a5d8-d5ed-4660-a99a-939c9289b814","trusted":true},"cell_type":"code","source":"df.hist(figsize=(12, 12))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b41fc6f7-655e-45b8-a3ef-ca1a6d60397f","_cell_guid":"fdf18cce-6f21-490e-8310-b7b0b5785891","trusted":true},"cell_type":"code","source":"sns.pairplot(data=df, diag_kind='kde', hue='Outcome', vars=df.columns[:-1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e51549a-192b-4b7b-90d4-18ed1aa67e29","_cell_guid":"e2eb8455-b35a-42d6-a440-cd379d6ccb72","trusted":true},"cell_type":"code","source":"sns.heatmap(df.corr(), annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Fitting and Analysis\n\nOne thing to note here is that the values of our various features are not on the same scale or magnitude. For example, `BloodPressure` and `Glucose` values can go into the hundreds, but features such as `Age` and `BMI` never go that high. If left alone, features with higher magnitudes will contribute a greater weight to the machine learning algorith of choice. In order to deal with this, it is preferred to scale the fatures down to the same magnitude by a process known as _feature scaling_. Standardization is one method of feature scaling and does so by replacing the values with their _z_ scores. We will apply standardization to our features, but before doing that, we will split our `df` into feature (`X`) and outcome (`y`) then scale the features."},{"metadata":{"_uuid":"7d858fb8-99f2-4775-a114-9d472241cf9b","_cell_guid":"0f81eb5f-bf50-4fa1-854d-05e3da8ba764","trusted":true},"cell_type":"code","source":"X = df.drop('Outcome', axis=1)\ny = df['Outcome']\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX = pd.DataFrame(X_scaled, columns=X.columns)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89ae86e6-1897-412b-a641-c8ee9b5c0f18","_cell_guid":"c25610d7-5279-4e7d-8234-1d4d0e93e4c6","trusted":true},"cell_type":"code","source":"X.hist(figsize=(12, 12))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2463b4f-624b-4352-afa8-8a49e8f3de09","_cell_guid":"c24e91ec-07ee-4549-8660-373720b5af80","trusted":true},"cell_type":"markdown","source":"We now see that everything is more or less on the same scale. Now we can begin spltting our data into training and test sets. We choose to _stratify_ our data so that the the training and test sets have the same proportions of class labels as the input dataset. For example, if `y` were 25% zeroes and 75% ones, then setting `stratify=y` would make the splits have 25% of `0`s and 75% of `1`s\n#"},{"metadata":{"_uuid":"c137e2d1-e721-47ad-b58a-5bdf2d3d8e07","_cell_guid":"7ecf151e-1ddf-4532-a774-754ada870ba4","trusted":true},"cell_type":"code","source":"random_state = 0\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=random_state, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8b33475-8abc-4d10-92b5-1683e1a5e170","_cell_guid":"ec1a0458-6f0e-4679-9b45-4936efb7ff21","trusted":true},"cell_type":"markdown","source":"Now we can begin fitting our data to various models. In fitting our model, we will also apply k-fold cross validation with *k* = 10. We build up a DataFrame of metrics per model that we will use to determine the best model as well as later use to plot figures."},{"metadata":{"_uuid":"3d3b1e28-a867-4e22-b04d-ca44c9a5381d","_cell_guid":"ceb2d0bf-5936-429f-8d5d-de8866d4f32a","trusted":true},"cell_type":"code","source":"models = [\n    ('SVC (Linear)', svm.SVC(kernel='linear', probability=True)),\n    ('LR', LogisticRegression()),\n    ('SVC (RBF)', svm.SVC(kernel='rbf', probability=True)),\n    ('RFC', RandomForestClassifier())\n]\n\nmodels.extend(\n    [\n        (f'KNN (n={i})', KNeighborsClassifier(i))\n        for i in range(1, 9)\n    ]\n)\n\ncv = StratifiedKFold(n_splits=10, random_state=random_state)\n\nmodel_metrics_map = {}\n    \nfor model_descriptor, model in models:\n    print(f\"Computing metrics for model {model_descriptor}\")\n    # Fit mode\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    # Cross val predict class\n    cv_y_pred = cross_val_predict(model, X_train, y_train, cv=cv)\n    model_metrics_map.setdefault(model_descriptor, {})['cv_y_pred'] = cv_y_pred\n    # Cross val predict probability\n    cv_y_prob_pred = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')\n    model_metrics_map.setdefault(model_descriptor, {})['cv_y_prob_pred'] = cv_y_prob_pred\n    # Train accuracy\n    train_accuracy = metrics.accuracy_score(y_test, y_pred)\n    model_metrics_map.setdefault(model_descriptor, {})['train_accuracy'] = train_accuracy\n    # Test accuracy using cross validation\n    test_cv_accuracies = cross_val_score(model, X_train, y_train, cv=cv)\n    model_metrics_map.setdefault(model_descriptor, {})['test_cv_accuracies'] = test_cv_accuracies\n    test_mean_cv_accuracy = test_cv_accuracies.mean()\n    model_metrics_map.setdefault(model_descriptor, {})['test_mean_cv_accuracy'] = test_mean_cv_accuracy\n    test_std_cv_accuracy = test_cv_accuracies.std()\n    model_metrics_map.setdefault(model_descriptor, {})['test_std_cv_accuracy'] = test_std_cv_accuracy\n    # Train ROC AUC\n    train_roc_auc = metrics.roc_auc_score(y_test, y_pred)\n    model_metrics_map.setdefault(model_descriptor, {})['train_roc_auc'] = train_roc_auc\n    # Test ROC AUC using cross validation\n    test_roc_aucs = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')\n    model_metrics_map.setdefault(model_descriptor, {})['test_roc_aucs'] = test_roc_aucs\n    test_mean_auc = test_roc_aucs.mean()\n    model_metrics_map.setdefault(model_descriptor, {})['test_mean_auc'] = test_mean_auc\n    test_std_auc = test_roc_aucs.std()\n    model_metrics_map.setdefault(model_descriptor, {})['test_std_auc'] = test_std_auc\n    # Confusion matrix\n    confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n    model_metrics_map.setdefault(model_descriptor, {})['confusion_matrix'] = confusion_matrix\n    # Test Precision\n    test_precisions = cross_val_score(model, X_train, y_train, cv=cv, scoring='precision')\n    model_metrics_map.setdefault(model_descriptor, {})['test_precisions'] = test_precisions\n    test_mean_precision = test_precisions.mean()\n    model_metrics_map.setdefault(model_descriptor, {})['test_mean_precision'] = test_mean_precision\n    test_std_precision = test_precisions.std()\n    model_metrics_map.setdefault(model_descriptor, {})['test_std_precision'] = test_std_precision\n    # Train Precision\n    train_precision = metrics.precision_score(y_train, cv_y_pred)\n    model_metrics_map.setdefault(model_descriptor, {})['train_precision'] = train_precision\n    # Test Recall\n    test_recalls = cross_val_score(model, X_train, y_train, cv=cv, scoring='recall')\n    model_metrics_map.setdefault(model_descriptor, {})['test_recalls'] = test_recalls\n    test_mean_recall = test_recalls.mean()\n    model_metrics_map.setdefault(model_descriptor, {})['test_mean_recall'] = test_mean_recall\n    test_std_recall = test_recalls.std()\n    model_metrics_map.setdefault(model_descriptor, {})['test_std_recall'] = test_std_recall\n    # Train Recall\n    train_recall = metrics.recall_score(y_train, cv_y_pred)\n    model_metrics_map.setdefault(model_descriptor, {})['train_recall'] = train_recall\n    # Test F1 Score\n    test_f1_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1')\n    model_metrics_map.setdefault(model_descriptor, {})['test_f1_scores'] = test_f1_scores\n    test_mean_f1_score = test_f1_scores.mean()\n    model_metrics_map.setdefault(model_descriptor, {})['test_mean_f1_score'] = test_mean_f1_score\n    test_std_f1_score = test_f1_scores.std()\n    model_metrics_map.setdefault(model_descriptor, {})['test_std_f1_score'] = test_std_f1_score\n    # Train F1 Score\n    train_f1_score = metrics.f1_score(y_train, cv_y_pred)\n    model_metrics_map.setdefault(model_descriptor, {})['train_f1_score'] = train_f1_score\n    \nmodel_metrics_df = pd.DataFrame(model_metrics_map).T\nmodel_metrics_df.head()\n\ncore_metrics = [\n    'train_accuracy', \n    'train_roc_auc',\n    'train_precision',\n    'train_recall',\n    'train_f1_score',\n    'test_mean_cv_accuracy', \n    'test_mean_auc',\n    'test_mean_precision',\n    'test_mean_recall',\n    'test_mean_f1_score'\n]\ncore_metrics_df = model_metrics_df[core_metrics]\ncore_metrics_df\n\ndef highlight_max(s):\n    is_max = s == s.max()\n    return ['background-color: green' if v else '' for v in is_max]\n\ncore_metrics_df.style.apply(highlight_max)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above DataFrame, we see that a RFC classifier had the higher accuracy and ROC AUC on the training set. SVC (Linear) had the highest precision. A KNN classifier with seven neighbors had the highest recall and F1 score. \n\nBelow we plot the various confusion matrices for the various models."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrices(df):\n    fig, ax = plt.subplots(2, 6, figsize=(24, 8))\n    index_ax_map = {\n        6 * i + j: ax[i, j]\n        for i in range(2)\n        for j in range(6)\n    }\n    confusion_matrices = df['confusion_matrix'].values\n    model_descriptors = df.index.values\n    for i, o in enumerate(zip(model_descriptors, confusion_matrices)):\n        model_descriptor, confusion_matrix = o\n        sns.heatmap(pd.DataFrame(confusion_matrix), ax=index_ax_map[i], annot=True, fmt='g')\n        index_ax_map[i].set_title(model_descriptor)\n        if i > 5:\n            index_ax_map[i].set_xlabel(\"Predicted\")\n        if i in (0, 6):\n            index_ax_map[i].set_ylabel(\"Actual\")\n    plt.show()\n\n\nplot_confusion_matrices(model_metrics_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The goal of our model should be to minimize fale negatives (FN) because we do not want to predict no diabetes when in reality the person does. One way to minimize FN is to have a higher recall. Our KNN classifier with *n* = 7 has the highest recall, so will conduct further analysis on that."},{"metadata":{"trusted":true},"cell_type":"code","source":"best_models_df = model_metrics_df.loc[['RFC', 'KNN (n=7)'], :]\nbest_models_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_auc_curve(df, y_true):\n    fig, ax = plt.subplots(figsize=(12, 8))\n    y_scores = df['cv_y_prob_pred'].values\n    auc_scores = df['train_roc_auc'].values\n    model_descriptors = df.index.values\n    for model_descriptor, y_score, auc_score in zip(model_descriptors, y_scores, auc_scores):\n        fpr, tpr, _ = metrics.roc_curve(y_true, y_score[:, 1])\n        ax.plot(fpr, tpr, label=f\"{model_descriptor} (AUC={auc_score:.3f})\")\n    ax.legend()\n    ax.plot([0, 1], 'k--')\n    ax.set_xlim([0.0, 1.0])\n    ax.set_ylim([0.0, 1.0])\n    plt.show()\n    \n\nplot_roc_auc_curve(best_models_df, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_precision_recall_vs_threshold(df, y_true):\n    fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n    index_ax_map = {\n        0: ax[0],\n        1: ax[1]\n    }\n    proba_preds = df['cv_y_prob_pred'].values\n    model_descriptors = df.index.values\n    for i, o in enumerate(zip(model_descriptors, proba_preds)):\n        model_descriptor, proba_pred = o\n        precisions, recalls, thresholds = metrics.precision_recall_curve(y_true, proba_pred[:, 1])\n        index_ax_map[i].plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n        index_ax_map[i].plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n        index_ax_map[i].set_ylim([0, 1])\n        index_ax_map[i].set_title(model_descriptor)\n        index_ax_map[i].set_xlabel('Recall')\n        index_ax_map[i].set_ylabel('Precision')\n        \n        \nplot_precision_recall_vs_threshold(best_models_df, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c595967-bc4c-4615-8482-5e908f59680a","_cell_guid":"0c81eee8-f5e7-4292-a603-f2b60e91a21d","trusted":true},"cell_type":"markdown","source":"# Conclusion and Future Steps\n\nIn conclusion, a KNN model with n=7 ended up being our best model that maximized recall. Although the recall isn't super high, there is still a lot of work that can be done to maximize this score. As this was my first project and exposure to machine learning, there are various things that we can try out to help our scores including more feature engineering, hyperparameter tuning, and using an ensemble model. Please feel free to provide and suggestion or guidance, as that will be truly appreciated!"}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}