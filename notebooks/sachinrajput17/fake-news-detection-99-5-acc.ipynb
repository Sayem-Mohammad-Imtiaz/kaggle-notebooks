{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nimport matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom keras.preprocessing import text, sequence\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Dropout\nfrom keras.callbacks import ReduceLROnPlateau\nimport tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Loading","metadata":{}},{"cell_type":"code","source":"true = pd.read_csv(\"../input/fake-and-real-news-dataset/True.csv\")\ntrue.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake = pd.read_csv(\"../input/fake-and-real-news-dataset/Fake.csv\")\nfake.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true.shape,fake.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Null Values in Real News Data = \",true.isna().any().sum())\nprint(\"Null Values in Fake News Data = \",fake.isna().any().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true['category'] = 1\nfake['category'] = 0\n\ndf = pd.concat([true,fake])\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Final Shape of the data","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(include=\"object\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df['category'].value_counts())\nsns.countplot(df['category'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='subject',hue='category',data=df,)\nplt.xticks(rotation=90)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"text\"] =df[\"title\"]+df[\"text\"]+df['subject']\ndf=df[[\"text\",\"category\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### STOPWORDS\n\nStopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have etc. Such words are already captured this in corpus named corpus. We first download it to our python environment.","metadata":{}},{"cell_type":"code","source":"import spacy\nnlp = spacy.load('en_core_web_sm')\nlist1 = nlp.Defaults.stop_words","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list2 = stopwords.words('english')\npunctuation = list(string.punctuation)\nStopwords = set((set(list1)|set(list2)|set(punctuation)))\nlen(Stopwords)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Cleaning","metadata":{}},{"cell_type":"code","source":"#creating instance\nlemma=WordNetLemmatizer()\n\n#text cleaning function\ndef clean_text(text):\n    \n    \"\"\"\n    It takes text as an input and clean it by applying several methods\n    \n    \"\"\"\n    \n    string = \"\"\n    \n    #lower casing\n    text=text.lower()\n    \n    #simplifying text\n    text=re.sub(r\"i'm\",\"i am\",text)\n    text=re.sub(r\"he's\",\"he is\",text)\n    text=re.sub(r\"she's\",\"she is\",text)\n    text=re.sub(r\"that's\",\"that is\",text)\n    text=re.sub(r\"what's\",\"what is\",text)\n    text=re.sub(r\"where's\",\"where is\",text)\n    text=re.sub(r\"\\'ll\",\" will\",text)\n    text=re.sub(r\"\\'ve\",\" have\",text)\n    text=re.sub(r\"\\'re\",\" are\",text)\n    text=re.sub(r\"\\'d\",\" would\",text)\n    text=re.sub(r\"won't\",\"will not\",text)\n    text=re.sub(r\"can't\",\"cannot\",text)\n    \n    #removing any special character\n    text=re.sub(r\"[-()\\\"#!@$%^&*{}?.,:]\",\" \",text)\n    text=re.sub(r\"\\s+\",\" \",text)\n    text=re.sub('[^A-Za-z0-9]+',' ', text)\n    \n    for word in text.split():\n        if word not in Stopwords:\n            string+=lemma.lemmatize(word)+\" \"\n    \n    return string","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cleaning the whole data\ndf[\"text\"]=df[\"text\"].apply(clean_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Word Cloud","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Word Cloud for Real News","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (20,20))\nwc = WordCloud(max_words = 2000 , width = 1000 , height = 500 , stopwords = Stopwords).generate(\" \".join(df[df.category == 1].text))\nplt.axis(\"off\")\nplt.imshow(wc , interpolation = 'bilinear')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Word Cloud for Fake News","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1000 , height = 500 , stopwords = Stopwords).generate(\" \".join(df[df.category == 0].text))\nplt.axis(\"off\")\nplt.imshow(wc , interpolation = 'bilinear')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Word Cloud for Whole data","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1000 , height = 500 , stopwords = Stopwords,background_color='white').generate(\" \".join(df.text))\nplt.axis(\"off\")\nplt.imshow(wc , interpolation = 'bilinear')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Classification Model","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Split the dataset into Train And Test Dataset.\nX=df[\"text\"] #feature \ny=df[\"category\"] # traget\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline","metadata":{}},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"#pipe line will take the text and vectorise it , and then TF-IDF, then fitting the model\n\nclf_text=Pipeline([(\"tfidf\",TfidfVectorizer()),(\"clf\",LogisticRegression())])\nclf_text.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#making prediction using the model\npredictions=clf_text.predict(X_test)\n\nprint(metrics.classification_report(y_test,predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#overall acuracy\nprint(metrics.accuracy_score(y_test,predictions))\nprint(metrics.f1_score(y_test,predictions))\n\n#confusion matrix\nprint(metrics.confusion_matrix(y_test,predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Naive Bayes","metadata":{}},{"cell_type":"code","source":"#pipe line will take the text and vectorise it , and then TF-IDF, then fitting the model\n\nclf_text=Pipeline([(\"tfidf\",TfidfVectorizer()),(\"clf\",MultinomialNB(alpha=0.5))])\nclf_text.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#making prediction using the model\npredictions=clf_text.predict(X_test)\n\nprint(metrics.classification_report(y_test,predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#overall acuracy\nprint(metrics.accuracy_score(y_test,predictions))\nprint(metrics.f1_score(y_test,predictions))\n\n#confusion matrix\nprint(metrics.confusion_matrix(y_test,predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SVC","metadata":{}},{"cell_type":"code","source":"#pipe line will take the text and vectorise it , and then TF-IDF, then fitting the model\n\nclf_text=Pipeline([(\"tfidf\",TfidfVectorizer()),(\"clf\",LinearSVC())])\nclf_text.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#making prediction using the model\npredictions=clf_text.predict(X_test)\n\nprint(metrics.classification_report(y_test,predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#overall acuracy\nprint(metrics.accuracy_score(y_test,predictions))\nprint(metrics.f1_score(y_test,predictions))\n\n#confusion matrix\nprint(metrics.confusion_matrix(y_test,predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest Cassifier","metadata":{}},{"cell_type":"code","source":"#pipe line will take the text and vectorise it , and then TF-IDF, then fitting the model\n\nclf_rf=Pipeline([(\"tfidf\",TfidfVectorizer()),(\"clf\",RandomForestClassifier(random_state=0))])\nclf_rf.fit(X_train,y_train)\n\n#making prediction using the model\npredictions=clf_rf.predict(X_test)\n\n\nprint(metrics.classification_report(y_test,predictions))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#overall acuracy\nprint(metrics.accuracy_score(y_test,predictions))\nprint(metrics.f1_score(y_test,predictions))\n\n#confusion matrix\nprint(metrics.confusion_matrix(y_test,predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"#pipe line will take the text and vectorise it , and then TF-IDF, then fitting the model\n\nclf_dt=Pipeline([(\"tfidf\",TfidfVectorizer()),(\"clf\",DecisionTreeClassifier(random_state=2))])\nclf_dt.fit(X_train,y_train)\n\n#making prediction using the model\npredictions=clf_dt.predict(X_test)\n\n\nprint(metrics.classification_report(y_test,predictions))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#overall acuracy\nprint(metrics.accuracy_score(y_test,predictions))\nprint(metrics.f1_score(y_test,predictions))\n\n#confusion matrix\nprint(metrics.confusion_matrix(y_test,predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Updating......","metadata":{}},{"cell_type":"markdown","source":"Please upvote the notebook if you find it useful. Your comments are also requested for improvement of the notebook.","metadata":{}}]}