{"cells":[{"metadata":{"_uuid":"67d1a83ea91b3a1c01cd1ab59449aabb5ee0ef76"},"cell_type":"markdown","source":"# Uber market demand prediction using seasonal ARIMA with grid seach"},{"metadata":{"_uuid":"884f3d9b037a8efd9d8c61d3102680622bc7a229"},"cell_type":"markdown","source":"Market demand plays a crucial part in the marketing strategy of any company. Forecasting such demand becomes crucial when the market is filled with competition, and a small mismatch in supply and demand can lead to a customer switching to another service provider.\n\nIn this notebook we look at a classical algorithm(ARIMA) which can be used to predict the demand for user trips in the upcoming week for a particular location. Particularly, we will be utilizing Uber's 2014 user trips data of New York city, to accomplish the same.\n\nThe dataset can be found on kaggle.com(https://www.kaggle.com/fivethirtyeight/uber-pickups-in-new-york-city)"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"d0c9f5b82e46d0161486270c707dec5d529d220b"},"cell_type":"code","source":"#Importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.tsa.stattools import adfuller\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt","execution_count":133,"outputs":[]},{"metadata":{"_uuid":"5b54536ca7a04126b7ed0f4e4a7b762753f14426"},"cell_type":"markdown","source":"First we upload our data-set into a single data-frame:"},{"metadata":{"trusted":false,"_uuid":"a9f8c3f8e0ea3c7cd28adc9fdcc1392e7ff121f0"},"cell_type":"code","source":"#Preparing the uber 2014 main dataset\ndef prepare_2014_df():\n    \n    #Loading datasets\n    uber_2014_apr=pd.read_csv('../input/uber-raw-data-apr14.csv',header=0)\n    uber_2014_may=pd.read_csv('../input/uber-raw-data-may14.csv',header=0)\n    uber_2014_jun=pd.read_csv('../input/uber-raw-data-jun14.csv',header=0)\n    uber_2014_jul=pd.read_csv('../input/uber-raw-data-jul14.csv',header=0)\n    uber_2014_aug=pd.read_csv('../input/uber-raw-data-aug14.csv',header=0)\n    uber_2014_sep=pd.read_csv('../input/uber-raw-data-sep14.csv',header=0)\n\n    \n    #Merging\n    df = uber_2014_apr.append([uber_2014_may,uber_2014_jun,uber_2014_jul,uber_2014_aug,uber_2014_sep], ignore_index=True)\n    \n    #returning merged dataframe\n    return df\n\n#Uber 2014 dataset\nuber_2014_master = prepare_2014_df()\nuber_2014_master.head()","execution_count":134,"outputs":[]},{"metadata":{"_uuid":"70823a779b1fd01712f441eb8cff43356ff0cbf9"},"cell_type":"markdown","source":"## Feature Engineering\n\nNext, we prepare the data-frame so that it is in a time-series format, which can then be utilized for modelling.\nSince we are only looking at a basic time-series forecast model, we will only be utilizing the Date/Time column for now.\n\nI plan to predict the demand at a day level and hence we will be resampling the data at a day level. However deping on the need, we can sample the data at different levels(Hour,Month,Year etc.)"},{"metadata":{"trusted":false,"_uuid":"dd954657e16ac4fd4178efbf4a6d14fe04202043"},"cell_type":"code","source":"# Feature Engineering\ndef create_day_series(df):\n    \n    # Grouping by Date/Time to calculate number of trips\n    day_df = pd.Series(df.groupby(['Date/Time']).size())\n    # setting Date/Time as index\n    day_df.index = pd.DatetimeIndex(day_df.index)\n    # Resampling to daily trips\n    day_df = day_df.resample('1D').apply(np.sum)\n    \n    return day_df\n\nday_df_2014 = create_day_series(uber_2014_master)\nday_df_2014.head()","execution_count":135,"outputs":[]},{"metadata":{"_uuid":"f05e748476cf20c1d803e1da72884411a9c90582"},"cell_type":"markdown","source":"Now that we have the time-series data-frame ready, we can look into some initial visualizations of the data to decide our parameters for the ARIMA model"},{"metadata":{"trusted":false,"_uuid":"03ce10e897b9120e617d1ef704fa803f10e970e5"},"cell_type":"code","source":"#Checking trend and autocorrelation\ndef initial_plots(time_series, num_lag):\n\n    #Original timeseries plot\n    plt.figure(1)\n    plt.plot(time_series)\n    plt.title('Original data across time')\n    plt.figure(2)\n    plot_acf(time_series, lags = num_lag)\n    plt.title('Autocorrelation plot')\n    plot_pacf(time_series, lags = num_lag)\n    plt.title('Partial autocorrelation plot')\n    \n    plt.show()\n\n    \n#Augmented Dickey-Fuller test for stationarity\n#checking p-value\nprint('p-value: {}'.format(adfuller(day_df_2014)[1]))\n\n#plotting\ninitial_plots(day_df_2014, 45)","execution_count":136,"outputs":[]},{"metadata":{"_uuid":"bbe7ddeb1faebf9734bf616de1bafd6e8ff36157"},"cell_type":"markdown","source":"Looking at the ADF test we see that clearly the time-series is not stationary(p-value>0.05 i.e for a confidence level of 95%), hence differencing is required.\n\nBefore we even analyse the ACF and PACF plots we need to difference and test for stationarity"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"34efd86d6482acd7c3ce592799d82d2bd1ec818f"},"cell_type":"code","source":"#storing differenced series\ndiff_series = day_df_2014.diff(periods=1)\n\n#Augmented Dickey-Fuller test for stationarity\n#checking p-value\nprint('p-value: {}'.format(adfuller(diff_series.dropna())[1]))","execution_count":137,"outputs":[]},{"metadata":{"_uuid":"06aba98d36c6f3f12c23ae00d026477ac365f211"},"cell_type":"markdown","source":"Looks like the series is stationary now (as p-value < 0.05, we can assume stationarity with a confidence level of 95%, even higher actually). So a differencing of 1 should be perfect!\n\nNow lets look at the ACF and PACF plots:"},{"metadata":{"trusted":false,"_uuid":"a24b65f41bf0282f2b0976f45c42967283f81015"},"cell_type":"code","source":"initial_plots(diff_series.dropna(), 30)","execution_count":138,"outputs":[]},{"metadata":{"_uuid":"d9fe953d178a3d711e335010145d8360b7e3f98b"},"cell_type":"markdown","source":"From the ACF and PACF plots we can see a clear spike at every 7 day interval. And since this appears clearly in the ACF plot this whows a seasonal MA component of 1.\n\n## Fitting SARIMAX models\n\nAlthough at this point the components can be guessed at ARIMA(0,1,0)(0,0,1)[7], we will implement a grid search to find the best fitting values, using RMSE as the deciding factor:"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"6354c979f7faf950904d8bc9f876fca34d8f4b90"},"cell_type":"code","source":"#Defining RMSE\ndef rmse(x,y):\n    return sqrt(mean_squared_error(x,y))\n\n#fitting ARIMA model on dataset\ndef SARIMAX_call(time_series,p_list,d_list,q_list,P_list,D_list,Q_list,s_list,test_period):    \n    \n    #Splitting into training and testing\n    training_ts = time_series[:-test_period]\n    \n    testing_ts = time_series[len(time_series)-test_period:]\n    \n    error_table = pd.DataFrame(columns = ['p','d','q','P','D','Q','s','AIC','BIC','RMSE'],\\\n                                                           index = range(len(ns_ar)*len(ns_diff)*len(ns_ma)*len(s_ar)\\\n                                                                         *len(s_diff)*len(s_ma)*len(s_list)))\n    count = 0\n    \n    for p in p_list:\n        for d in d_list:\n            for q in q_list:\n                for P in P_list:\n                    for D in D_list:\n                        for Q in Q_list:\n                            for s in s_list:\n                                #fitting the model\n                                SARIMAX_model = SARIMAX(training_ts.astype(float),\\\n                                                        order=(p,d,q),\\\n                                                        seasonal_order=(P,D,Q,s),\\\n                                                        enforce_invertibility=False)\n                                SARIMAX_model_fit = SARIMAX_model.fit(disp=0)\n                                AIC = np.round(SARIMAX_model_fit.aic,2)\n                                BIC = np.round(SARIMAX_model_fit.bic,2)\n                                predictions = SARIMAX_model_fit.forecast(steps=test_period,typ='levels')\n                                RMSE = rmse(testing_ts.values,predictions.values)                                \n\n                                #populating error table\n                                error_table['p'][count] = p\n                                error_table['d'][count] = d\n                                error_table['q'][count] = q\n                                error_table['P'][count] = P\n                                error_table['D'][count] = D\n                                error_table['Q'][count] = Q\n                                error_table['s'][count] = s\n                                error_table['AIC'][count] = AIC\n                                error_table['BIC'][count] = BIC\n                                error_table['RMSE'][count] = RMSE\n                                \n                                count+=1 #incrementing count        \n    \n    #returning the fitted model and values\n    return error_table\n\nns_ar = [0,1,2]\nns_diff = [1]\nns_ma = [0,1,2]\ns_ar = [0,1]\ns_diff = [0,1] \ns_ma = [1,2]\ns_list = [7]\n\nerror_table = SARIMAX_call(day_df_2014,ns_ar,ns_diff,ns_ma,s_ar,s_diff,s_ma,s_list,30)","execution_count":139,"outputs":[]},{"metadata":{"_uuid":"7fa79736e9543e2d9d42f16e91ce75725ec413fe"},"cell_type":"markdown","source":"Now that we have obtained the RMSE values for different combinations, we can take a look at the lowest 5 RMSE values:"},{"metadata":{"trusted":false,"_uuid":"1ee1001a90ef9168bb39e98e1dcee1d8bcc41cf6"},"cell_type":"code","source":"# printing top 5 lowest RMSE from error table\nerror_table.sort_values(by='RMSE').head(5)","execution_count":140,"outputs":[]},{"metadata":{"_uuid":"712c6b7ed3ebb16aa31af97eafdefec0a02c08bd"},"cell_type":"markdown","source":"We see that ARIMA(0,1,0)(1,0,2)[7] gives the lowest RMSE. However, next best RMSE is of ARIMA(0,1,0)(0,1,2)[7] has lower AIC and BIC scores, which tells us that it fits the data much better than the lowest RMSE ARIMA fit.\n\nHere we can chose the second ARIMA if we want a more robust solution, which can generalize well to other situations as well, however this ensures that we do not get the best RMSE for this testing data.\n\n## Forecasting\n\nI will forecast using ARIMA(0,1,0)(0,1,2)[7] as I want the model to fit the data better along with giving me a lower RMSE:"},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"2744048e60b08abb2d63abc722ff46dff4701dbe"},"cell_type":"code","source":"#Predicting values using the fitted model\ndef predict(time_series,p,d,q,P,D,Q,s,n_days,conf):\n    \n    #Splitting into training and testing\n    training_ts = time_series[:-n_days]\n    \n    testing_ts = time_series[len(time_series)-n_days:]\n    \n    #fitting the model\n    SARIMAX_model = SARIMAX(training_ts.astype(float),\\\n                            order=(p,d,q),\\\n                            seasonal_order=(P,D,Q,s),\\\n                            enforce_invertibility=False)\n    SARIMAX_model_fit = SARIMAX_model.fit(disp=0)\n    \n    #Predicting\n    SARIMAX_prediction = pd.DataFrame(SARIMAX_model_fit.forecast(steps=n_days,alpha=(1-conf)).values,\\\n                          columns=['Prediction'])\n    SARIMAX_prediction.index = pd.date_range(training_ts.index.max()+1,periods=n_days)\n    \n    #Plotting\n    plt.figure(4)\n    plt.title('Plot of original data and predicted values using the ARIMA model')\n    plt.xlabel('Time')\n    plt.ylabel('Number of Trips')\n    plt.plot(time_series[1:],'k-', label='Original data')\n    plt.plot(SARIMAX_prediction,'b--', label='Next {}days predicted values'.format(n_days))\n    plt.legend()\n    plt.show()\n    \n    #Returning predicitons\n    return SARIMAX_prediction\n\n#Predicting the values and builing an 80% confidence interval\nprediction = predict(day_df_2014,0,1,0,0,1,2,7,7,0.80)","execution_count":141,"outputs":[]},{"metadata":{"_uuid":"e1d22c2bea6edcbc8b0b5978be10eb2bccfc2f07"},"cell_type":"markdown","source":"Thus using this simple classical Seasonal ARIMA model we can utilize the forecasted data to plan and alter our marketing strategy for the upcoming week(or any timeframe upon which predictions are to be made) in any location.\n\nIn this example, I have just used the complete dataset to forecast the demand, however we need not restrict ourselves to this level. We can split the data-set based on location and obtain forecasts for different pick-up zones in NYC(which would add even more value towards our predictions)."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}