{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2fc1e25a-6155-c020-9acd-6a9462ef1761"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datetime as dt\nimport re\nimport matplotlib as plt\nimport seaborn as sns\nimport string\n\nimport sklearn.model_selection as skcv\nfrom sklearn import preprocessing as pp\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\nfrom nltk.stem import SnowballStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\ndef parser(x):\n    t = re.findall(r\"[\\w]+\",x)\n    t[1] = t[1][:3]\n    t.pop(0)\n    t1 = \" \".join(t)\n    return dt.datetime.strptime(t1, \"%b %d %Y\")\n    \nbotsDF = pd.read_csv(\"H:\\Desktop\\Python\\Kaggle\\Deep NLP\\Sheet_1.csv\", usecols=['response_id','class', 'response_text'])#, encoding=\"latin1\", parse_dates=['Date'], date_parser = parser, dayfirst = True, index_col=[0], converters={'Longitute':np.float64})\nresumeDF = pd.read_csv(\"H:\\Desktop\\Python\\Kaggle\\Deep NLP\\Sheet_2.csv\" , encoding=\"latin1\") #, parse_dates=['Date'], date_parser = parser, dayfirst = True, index_col=[0], converters={'Longitute':np.float64})\n\nstopwords = pd.read_csv(\"H:\\Desktop\\Python\\edx AI\\P5 - NLP\\stopwords.en.txt\", names=['words'])\nstops = set(stopwords['words'].tolist())\n\ndef makeCorpus(txt):\n    responses = txt.str.lower().str.split()\n    \n    refined = []\n    \n    for sentence in responses:\n        refined.append(\" \".join([word.strip(string.punctuation) for word in sentence if word not in stops]))\n    \n    sno = SnowballStemmer('english')\n    r = [\" \".join([sno.stem(x) for x in sentence.split(\" \")]) for sentence in refined]\n    return r\n\nbotsDF['corpus'] = makeCorpus(botsDF['response_text'])\nresumeDF['corpus'] = makeCorpus(resumeDF['resume_text'])\n\nbotsDF.groupby('class').corpus.count().plot.bar()\n      \ncnt = CountVectorizer()\nX = cnt.fit_transform(botsDF['corpus'])\n\nX_train, X_test, Y_train, Y_test = skcv.train_test_split(X,botsDF['class'],test_size=0.4, random_state=1)\ncv = skcv.KFold(n_splits=5, random_state=0)\n\ndef EvaluateModel(model, params, m_desc):\n    \"\"\"\n    print(\"\")\n    print(\"Working for %s. Please wait ....\" % m_desc)\n    sTime = time.clock()\n    \"\"\"    \n    gs = GridSearchCV(estimator=model, param_grid=params, cv=cv, n_jobs=1)\n    \n    gs.fit(X_train, Y_train)\n    \"\"\"\n    print(\"Best parameters for %s is %r\" % (m_desc, gs.best_params_))\n    \"\"\"\n    print(\"Best parameters for %s are %r \" % (m_desc, gs.best_params_))\n    \n    tr_score = gs.best_score_\n    \"\"\"\n    print(\"Best training score is %f\" % tr_score)\n    \"\"\"\n    y_true, y_pred = Y_test, gs.predict(X_test)\n    ts_score = accuracy_score(y_true, y_pred)\n    \"\"\"    \n    print(\"Test score for %s is %f \" %(m_desc, ts_score))\n    eTime = time.clock() - sTime\n    print(\"Paramter(s) tuned for %s in %d min(s) %d sec(s)\" % (m_desc, int(eTime/60), int(eTime%60) ))   \n    print(\"\")\n    \"\"\"\n    return m_desc, tr_score, ts_score\n\nc = [0.1, 1, 3]\nd = [4,5,6]\ng = [0.1,1]\n\nmodels = {}\n\nc = [0.1, 0.5, 1, 5, 10, 50, 100]\nm, tr, ts = EvaluateModel(SVC(kernel='linear'), dict(C=c),\"svm_linear\")    \nmodels[m] = [m, tr, ts]\n\n\n# SVC with Polynomial Kernel\n# Params\nc = [0.1, 1, 3]\nd = [4,5,6]\ng = [0.1,1]\nm, tr, ts = EvaluateModel(SVC(kernel='poly'), dict(C=c,degree=d,gamma=g),\"svm_polynomial\")    \nmodels[m] = [m, tr, ts]\n\n\n# SVC with RBF Kernel\n# Params\nC = [0.1, 0.5, 1, 5, 10, 50, 100] \ng = [0.1, 0.5, 1, 3, 6, 10]\nm, tr, ts = EvaluateModel(SVC(kernel='rbf'), dict(C=c,gamma=g),\"svm_rbf\")    \nmodels[m] = [m, tr, ts]\n\n\n#Logistic Regression\n# Params\nC = [0.1, 0.5, 1, 5, 10, 50, 100]\nm, tr, ts = EvaluateModel(LogisticRegression(), dict(C=C),\"logistic\")    \nmodels[m] = [m, tr, ts]\n\n\n#K Nearest Neighbors\n# Params\nn_neighbors = np.linspace(1, 25, 1)\nleaf_size = np.linspace(5, 60, 5)\nm, tr, ts = EvaluateModel(KNeighborsClassifier(), dict(n_neighbors=n_neighbors,leaf_size=leaf_size),\"knn\")    \nmodels[m] = [m, tr, ts]\n\n\n#Decision Tree Classifier\n# Params\nmax_depth = np.linspace(1, 50, 1)\nmin_samples_split = np.linspace(2, 10, 5, dtype=int)\nm , tr, ts = EvaluateModel(DecisionTreeClassifier(), dict(max_depth=max_depth),\"decision_tree\")    \nmodels[m] = [m, tr, ts]\n\n\n#Random Forest Classifier\n# Params\nmax_depth = np.linspace(1, 50, 50) \nmax_features = np.linspace(5,15,10,dtype=int)\nmin_samples_split = np.linspace(2, 10, 5, dtype=int)\nm , tr, ts = EvaluateModel(RandomForestClassifier(), dict(max_depth=max_depth, max_features=max_features, min_samples_split=min_samples_split),\"random_forest\")    \nmodels[m] = [m, tr, ts]\n"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}