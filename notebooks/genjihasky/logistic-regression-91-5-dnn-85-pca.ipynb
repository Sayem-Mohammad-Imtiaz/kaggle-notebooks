{"cells":[{"metadata":{},"cell_type":"markdown","source":"# DSA5102 Project: Are You Likely to Suffer a Heart Attack?<a id='top'></a>\n## Contents <a id='top'></a>\n1. <a href=#intro>Introduction</a>\n    1. <a href=#back>Background</a>\n    1. <a href=#object>Objective</a>\n    2. <a href=#data>Data description</a>\n1. <a href=#model>Modeling</a>\n    1. <a href=#sl>Supervised Learning</a>\n    1. <a href=#ul>Unsupervised Learning</a>\n1. <a href=#conclusion>Conclusion</a>\n1. <a href=#ref>Links</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nfrom sklearn.metrics import silhouette_score\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import PCA\nfrom scipy.cluster import hierarchy\nimport plotly.express as px","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='intro'></a>\n# 1. Introduction\n<a href=#top>(back to top)</a>"},{"metadata":{},"cell_type":"markdown","source":"<a id='back'></a>\n### 1.1 Background\nNowadays with the development of remote equipments and  communication technologies, the pace of life has become more compact, which caused people are working longer, and the pressure of workers is also increasing fast.At the same time, in such an environment, people usually do not take good care of their bodies, leading to the onset of various diseases.Heart disease as one of  the most common circulatory system diseases, because of its sudden onset and unpredictability, often can not be prevented to cause a fatal blow to patients. It is a huge threat to health."},{"metadata":{},"cell_type":"markdown","source":"<a id='object'></a>\n### 1.2 Objective\nTherefore, in this project, my interest is to classify the high-risk and low-risk population by using machine learning (supervised learning and unsupervised learning) through 14 attributes that may be associated with cardiac disease, such as patient's age, gender, type of chest pain, resting blood pressure, fasting blood sugar, etc.In the comparison of each model, we want to select the most accurate and most robust model."},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_disease=pd.read_csv('../input/health-care-data-set-on-heart-attack-possibility/heart.csv')#import data as dataframe\nheart_disease.columns          #chcek columns name","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_disease.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_disease.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax=plt.subplots(5,3,figsize=(20,28))\nsns.distplot(heart_disease['age'],bins=10,ax=ax[0,0],axlabel='Age Distribution')\nsns.countplot(x=\"sex\", data=heart_disease,ax=ax[0,1])\nsns.countplot(x=\"cp\", data=heart_disease,ax=ax[0,2])\nsns.distplot(heart_disease['trestbps'],bins=10,ax=ax[1,0],axlabel='resting blood pressure')\nsns.distplot(heart_disease['chol'],bins=10,ax=ax[1,1],axlabel='serum cholestoral in mg/dl')\nsns.countplot(x=\"fbs\", data=heart_disease,ax=ax[1,2])\nsns.countplot(x=\"restecg\", data=heart_disease,ax=ax[2,0])\nsns.distplot(heart_disease['thalach'],bins=10,ax=ax[2,1],axlabel='maximum heart rate achieved')\nsns.countplot(x=\"exang\", data=heart_disease,ax=ax[2,2])\nsns.distplot(heart_disease['oldpeak'],bins=10,ax=ax[3,0],axlabel='ST depression induced by exercise relative to rest')\nsns.countplot(x='slope',data=heart_disease,ax=ax[3,1])\nsns.countplot(x='ca',data=heart_disease,ax=ax[3,2])\nsns.countplot(x='thal',data=heart_disease,ax=ax[4,0])\nsns.countplot(x='target',data=heart_disease,ax=ax[4,1])\nax[4,1].set_title(' target: 0= less chance of heart attack 1= more chance of heart attack')\nax[4,0].set_title('thal: 0 = normal; 1 = fixed defect; 2 = reversable defect')\nax[3,2].set_title('number of major vessels (0-3) colored by flourosopy')\nax[3,1].set_title('the slope of the peak exercise ST segment')\nax[2,2].set_title('exercise induced angina')\nax[1,2].set_title(\"fasting blood sugar > 120 mg/dl\")\nax[0,2].set_title(\"chest pain type\")\nax[2,0].set_title('resting electrocardiographic results')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize= (10, 6))\ncorrMatrix = heart_disease.corr()\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='model'></a>\n# 2. Modeling\n<a href=#top>(back to top)</a>"},{"metadata":{},"cell_type":"markdown","source":"### Data cleaning\nCheck for missing value"},{"metadata":{},"cell_type":"markdown","source":"At a glance of the table, we know there is not any `null/NAN` value in the DataFrame, which is good."},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_disease.replace(to_replace= r'^\\s*$', value=np.nan,regex=True, inplace=True ) #replace any unit value that only contains \" \", space\nheart_disease.isnull().any() #check whether each column contains a missing value","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then if there is any unit that only contains \" \" or space value, it will be replaced by a `NAN value`. After the replacement, we check again if there is any `null value`. Fortunately, the data are clean."},{"metadata":{},"cell_type":"markdown","source":"### 2.1.1 Deep Neural Network Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# cut the Dataframe into two parts, one for features, another for target\nX=heart_disease.drop(['target'],axis=1)\ny=heart_disease['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For `DNN`, we need to convert *categorical feature* columns(cp, fbs, exang,slope and so on) into `one-hot code`. To do that first we need to change those columns type from integer to object. As follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X[['sex','cp','fbs','restecg','exang','slope','ca','thal']]=(X[['sex','cp','fbs','restecg','exang','slope','ca','thal']].astype(object))\nX.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert all the categorical columns into onehot code, \n#and drop the the first onehot code from each conversion.\nX_encode=pd.get_dummies(X,drop_first=True)\nfrom keras.utils.np_utils import to_categorical \ny_encode=to_categorical(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_cols=X_encode.shape[1] #find number of node in input layer\nn_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Normalize all the data (train and test) "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_encode_scaled=scale(X_encode)\npd.DataFrame(X_encode_scaled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Split data into train set and test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_train1,features_test1,target_train1,target_test1 = train_test_split(X_encode_scaled,y_encode,test_size=0.2,random_state=42,stratify=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### DNN Model construction:\n\nsequential model;adam optimizer; loss:categorical crossentropy; metrics: categorical crossentropy"},{"metadata":{},"cell_type":"markdown","source":"When building `DNN`, we first build a overfited model that can ensure it has enough capacity to pass information. Then we use dropout or regularization methods to reduce overfitting, and finally we do hyperparameter tunning(nodes number). Here is the final stage model that already has gone through this process"},{"metadata":{"trusted":true},"cell_type":"code","source":"model1=Sequential()\nmodel1.add(Dense(40,activation='relu',input_shape=(n_cols,)))\nmodel1.add(Dropout(0.25))\nmodel1.add(Dense(40,activation='relu'))\nmodel1.add(Dropout(0.25))\nmodel1.add(Dense(2,activation='softmax'))                                             #using softmax as activation function(for classfication)\nmodel1.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['categorical_accuracy']) #using categorical_crossentropy as loss function","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use early stop method to prevent overfitting from training too many times."},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping_monitor=EarlyStopping(patience=2)        #set a early stop to prevent overfitting\nrecord=model1.fit(features_train1,target_train1,validation_split=0.2,epochs=50,callbacks=[early_stopping_monitor])   #10% of data would be used for validation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluate model"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss,accuracy=model1.evaluate(features_test1,target_test1)\nprint('loss is ', loss, '\\nDNN accuracy on test data is ' ,accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot accuracy change vs validation accuracy change based on epochs\nplt.plot(record.epoch, record.history.get('categorical_accuracy'),color='orange')\nplt.plot(record.epoch, record.history.get('val_categorical_accuracy'),color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the trained model\nfrom keras.models import load_model\nmodel1.save('model1.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion Matrix and classification report"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_pred_label1=np.argmax(model1.predict(features_test1),axis=1) # use DNN to predict labels on test data\nC_dnn=confusion_matrix(\n    target_test1[:,1],   # array, Gound true (correct) target values\n    target_pred_label1,  # array, Estimated targets as returned by a classifier\n    labels=[0,1],        # array, List of labels to index the matrix.\n    sample_weight=None  # array-like of shape = [n_samples], Optional sample weights\n)\n\ncol_name=pd.MultiIndex.from_product([['Predicted label'], ['low risk','high risk']])\nrow_name=pd.MultiIndex.from_product([['True label'], ['low risk','high risk']])\npd.DataFrame(C_dnn,columns=col_name,index=row_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, labels_name, title):\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]    # normalization\n    plt.imshow(cm, interpolation='nearest')    \n    plt.title(title)    \n    plt.colorbar()\n    num_local = np.array(range(len(labels_name)))    \n    plt.xticks(num_local, labels_name, rotation=90)    \n    plt.yticks(num_local, labels_name)    \n    plt.ylabel('True label')    \n    plt.xlabel('Predicted label')\n\nplot_confusion_matrix(C_dnn,['low risk','high risk'], \"DNN_pred Confusion Matrix\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(target_test1[:,1],target_pred_label1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above we can see the *model accuracy* is 87%, and it also perform well on the confusion matrix. What is more, from the plot we can see there seems to be no overfitting as the difference of two lines are not so big."},{"metadata":{},"cell_type":"markdown","source":"#### Probabilistic Calculation from DNN model:\n\nInstead of just knowing a binary result that whether a patient with given features is likely to have heart disease, we want to know the specific probability of heart disease with gien features."},{"metadata":{"trusted":true},"cell_type":"code","source":"def Probability1(features,model):\n    prediction_array=model.predict(features)\n    probability_array =prediction_array[:,1]\n    probability_table=pd.DataFrame(probability_array)\n    return probability_table\n# we define a function that calculate the probability of heart disease with gien features.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Probability1(features_test1,model1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this way we can not only find the binary result, but also find the actual predicted probability of heart disease for the given feature."},{"metadata":{},"cell_type":"markdown","source":"### 2.1.2 Logistic Regression\n\nApart from DNN, `logistic regression` model is also suitable for binary classification problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_train2,features_test2,target_train2,target_test2 = train_test_split(X_encode_scaled,y,test_size=0.2,random_state=42,stratify=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model construction"},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg=LogisticRegression(max_iter=3000) # set the max iteration to be 3000 otherwise the process can't be finished\nlogreg.fit(features_train2,target_train2)\ntarget_pred2=logreg.predict(features_test2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluate model\nUsing test dataset to plot ROC curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\ny_pred_prob = logreg.predict_proba(features_test2)[:,1]\nfpr, tpr, thresholds = roc_curve(target_test2,y_pred_prob)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr,label='Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Logistic Regression ROC Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_scores=cross_val_score(logreg,X_encode_scaled,y,cv=5,scoring='roc_auc')\nprint('AUC of logistic model is ',cv_scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion Matrix and classification report"},{"metadata":{"trusted":true},"cell_type":"code","source":"C_logistic=confusion_matrix(\n    target_test2,   # array, Gound true (correct) target values\n    target_pred2,  # array, Estimated targets as returned by a classifier\n    labels=[0,1],        # array, List of labels to index the matrix.\n    sample_weight=None  # array-like of shape = [n_samples], Optional sample weights\n)\npd.DataFrame(C_logistic,columns=col_name,index=row_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(C_logistic,['low risk','high risk'], \"Logistic Regression Confusion Matrix\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(target_test2,target_pred2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above we can see the model performance is 91.5% with *accuracy* 0.87, and it also perform well on the `confusion matrix`. What is more, it also performs well on the ROC curve, which means it has high true positive rate and also has low false positive rate. It indicates that model can perform well on classification and filter."},{"metadata":{},"cell_type":"markdown","source":"#### Probability calculation from logistic regression model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Probability2(features):\n    y_pred_prob = logreg.predict_proba(features)[:,1]\n    return pd.DataFrame(y_pred_prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Probability2(features_test2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this way we can not only find the binary result, but also find the actual predicted probability of heart disease for the given feature."},{"metadata":{},"cell_type":"markdown","source":"### 2.1.3 KNN model\n\nWe can also use k nearest neighbors model to classify the data points into two categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_train3,features_test3,target_train3,target_test3 = train_test_split(X_encode_scaled,y,test_size=0.2,random_state=42,stratify=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model Construction"},{"metadata":{},"cell_type":"markdown","source":"Hyperparameter Tuning:\n\n`n_neighbors` is a hyperparamter which means we should decide its value before we train the model. In order to find the best value of it, I use *hperparamter tunning* to find the optimal n_neighbors between 1 and 49. During the process cross validation will be used to find the best parameter and its score."},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'n_neighbors': np.arange(1,50)}\nknn=KNeighborsClassifier()\nknn_cv=GridSearchCV(knn,param_grid,cv=5)\nknn_cv.fit(features_train3,target_train3)\n#find best parameter\nn_neighbor=knn_cv.best_params_\nn_neighbor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#find score of best parameter\nknn_cv.best_score_\nprint('parameter score (n_neighbors=45)is ', knn_cv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`KNN` score is 0.85, which is a quite good number for hyperparameter score"},{"metadata":{},"cell_type":"markdown","source":"#### Evaluate the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_pred3=knn_cv.predict(features_test3)  #calculate the predicted target\nknn_cv.score(features_test3,target_test3)\nprint('KNN accuracy on test data is ',knn_cv.score(features_test3,target_test3) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion Matrix and classification report"},{"metadata":{"trusted":true},"cell_type":"code","source":"C_KNN=confusion_matrix(\n    target_test3,   # array, Gound true (correct) target values\n    target_pred3,  # array, Estimated targets as returned by a classifier\n    labels=[0,1],        # array, List of labels to index the matrix.\n    sample_weight=None  # array-like of shape = [n_samples], Optional sample weights\n)\npd.DataFrame(C_KNN,columns=col_name,index=row_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(C_KNN,['low risk','high risk'], \"KNN Confusion Matrix\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(target_test3,target_pred3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1.4 Decision Tree model\n\nWe can also use `decision tree` as classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"features_train4,features_test4,target_train4,target_test4 = train_test_split(X_encode_scaled,y,test_size=0.2,random_state=42,stratify=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model Construction"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = tree.DecisionTreeClassifier()\nclf = clf.fit(features_train4,target_train4)\nimport graphviz \ndot_data = tree.export_graphviz(clf, out_file=None,\n                     filled=True, rounded=True,  \n                     special_characters=True) \ngraph = graphviz.Source(dot_data)  \ngraph ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluate the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_accuracy=clf.score(features_test4,target_test4)\ntarget_pred4=clf.predict(features_test4)\nprint('Decision tree model accuracy on test data is ',clf_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"C_decisiontree=confusion_matrix(\n    target_test4,   # array, Gound true (correct) target values\n    target_pred4,  # array, Estimated targets as returned by a classifier\n    labels=[0,1],        # array, List of labels to index the matrix.\n    sample_weight=None  # array-like of shape = [n_samples], Optional sample weights\n)\npd.DataFrame(C_decisiontree,columns=col_name,index=row_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(C_decisiontree,['low risk','high risk'], \"Decision Tree Confusion Matrix\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(target_test4,target_pred4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1.5 Random forest model"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_train5,features_test5,target_train5,target_test5 = train_test_split(X_encode_scaled,y,test_size=0.2,random_state=42,stratify=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model Construction\nHyperparameter Tuning:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nparam_grid = {'n_estimators':np.arange(10,101,10)}        # set the range of n_estimators that we want to search for\nrfc = RandomForestClassifier(n_jobs=4)                  # build up a model\nrfc_cv = GridSearchCV(rfc,param_grid,scoring='accuracy', cv=3) \nrfc_cv.fit(features_train5,target_train5) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fubd the best parameter\nrfc_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find the best score of best parameter\nrfc_cv.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluate the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_pred5=rfc_cv.predict(features_test5)\n\n# find the accuracy on test data\nrfc_accuracy=rfc_cv.score(features_test5,target_test5)\nprint('Random forest model accuracy on test data is',rfc_accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"confusion matrix and classification report"},{"metadata":{"trusted":true},"cell_type":"code","source":"C_rfc=confusion_matrix(\n    target_test5,   # array, Gound true (correct) target values\n    target_pred5,  # array, Estimated targets as returned by a classifier\n    labels=[0,1],        # array, List of labels to index the matrix.\n    sample_weight=None  # array-like of shape = [n_samples], Optional sample weights\n)\npd.DataFrame(C_rfc,columns=col_name,index=row_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(C_rfc,['low risk','high risk'], \"Random Forest Confusion Matrix\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(target_test5,target_pred5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1.6 SVM model\n#### Model 1: SVM.SVC(kernel='linear')"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_train6,features_test6,target_train6,target_test6 = train_test_split(X_encode_scaled,y,test_size=0.2,random_state=42,stratify=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model Construction\nHyperparameter Tuning:"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid={'C':np.arange(0.1,10,0.1)}\nmodel_svm1=svm.SVC(kernel='linear')\nmodel_svm1_cv=GridSearchCV(model_svm1,param_grid,scoring='accuracy',cv=3)\nmodel_svm1_cv.fit(features_train6,target_train6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model_svm1_cv.best_params_) # find best hyperparameter.\nprint(model_svm1_cv.best_score_)  # find the score of the best hyperparameter.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluate the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_pred6=model_svm1_cv.predict(features_test6)\n# find the accuracy on test data\nsvm1_accuracy=model_svm1_cv.score(features_test6,target_test6)\nprint('svm with linear kernel model accuracy on test data is',svm1_accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"confusion matrix and classification report"},{"metadata":{"trusted":true},"cell_type":"code","source":"C_svm1=confusion_matrix(\n    target_test6,   # array, Gound true (correct) target values\n    target_pred6,  # array, Estimated targets as returned by a classifier\n    labels=[0,1],        # array, List of labels to index the matrix.\n    sample_weight=None  # array-like of shape = [n_samples], Optional sample weights\n)\npd.DataFrame(C_svm1,columns=col_name,index=row_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(C_svm1,['low risk','high risk'], \"SVM(kernel=linear) Confusion Matrix\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(target_test6,target_pred6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model 2: SVM.SVC(kernel='rbf')"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_train8,features_test8,target_train8,target_test8 = train_test_split(X_encode_scaled,y,test_size=0.2,random_state=42,stratify=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model Construction\nHyperparameter Tuning:"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid={'C':np.arange(0.1,10,0.1)}\nmodel_svm3=svm.SVC(kernel='rbf',gamma='scale')\nmodel_svm3_cv=GridSearchCV(model_svm3,param_grid,scoring='accuracy',cv=3)\nmodel_svm3_cv.fit(features_train8,target_train8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model_svm3_cv.best_params_) # find best hyperparameter.\nprint(model_svm3_cv.best_score_)  # find the score of the best hyperparameter.\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluate the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_pred8=model_svm3_cv.predict(features_test8)\n# find the accuracy on test data\nsvm3_accuracy=model_svm3_cv.score(features_test8,target_test8)\nprint('svm with rbf kernel model accuracy on test data is',svm3_accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"confusion matrix and classification report"},{"metadata":{"trusted":true},"cell_type":"code","source":"C_svm3=confusion_matrix(\n    target_test8,   # array, Gound true (correct) target values\n    target_pred8,  # array, Estimated targets as returned by a classifier\n    labels=[0,1],        # array, List of labels to index the matrix.\n    sample_weight=None  # array-like of shape = [n_samples], Optional sample weights\n)\npd.DataFrame(C_svm3,columns=col_name,index=row_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(C_svm3,['low risk','high risk'], \"SVM.SVC(kernel='rbf') Confusion Matrix\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(target_test8,target_pred8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above we can conclude that `logistic regression model` and `SVM` with linear kernel perform best among those models."},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Unsupervised Learning <a id='ul'></a>\n### 2.2.1 Principle Component Analysis (PCA)\n\nProblem statement:\nClose scrutiny of the dataset,there are 13 features in total, so after using supervised machine learning to build predictive models and measure their performance, next I want to use `principle component analysis` to do feature extraction among the 13 features, and reduce the number of features(dimension reduction), and I want to see which of the features have large influence."},{"metadata":{},"cell_type":"markdown","source":"#### t-SNE for 2-dimensional maps \n\n`t-SNE` is t-distributed stochastic neighbor embeding, which can do dimension reduction in data visualization. And its x-axis is meaningless. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samples= heart_disease\nsamples[['sex','cp','fbs','restecg','exang','slope','ca','thal']]=(samples[['sex','cp','fbs','restecg','exang','slope','ca','thal']].astype(object))\nsamples.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samples_encode=pd.get_dummies(samples,drop_first=True)\nsamples_encode_scaled=scale(samples_encode)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_TSNE = TSNE (learning_rate=100)\ntransformed = model_TSNE.fit_transform(samples_encode_scaled)\nxs=transformed[:,0]\nys=transformed[:,1]\nplt.scatter(xs,ys,c=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the `t-SNE` graph shows,we could find a clear boundary between the two categories(target=0, target=1). From this perspective we can visualize samples in 2D."},{"metadata":{},"cell_type":"markdown","source":"#### Construct PCA model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# set number of components = 3.\nX_scale=scale(X)\npca = PCA(n_components=3)\nX_PCAtransform=pd.DataFrame(pca.fit_transform(X_scale))\nX_PCAtransform['target'] = y\nfig = px.scatter_3d(\n    X_PCAtransform, \n    x=0, \n    y=1,\n    z=2, \n    color=y, \n    title='3d scatter for PCA',\n      width=700,\n    height=700 \n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set number of components = 7.\nX_scale=scale(X)\npca = PCA(n_components=7)\npca.fit(X_scale)\n# plot the eigenvalue of 7 components\nplt.plot(pca.explained_variance_)\nplt.xlabel(r'$j$')\nplt.ylabel(r'$\\lambda_j$');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this eigenvalue graph we can see we need to cover 7 principle components in order to represent enough explained variance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot scree plot to show the percentage of each eigenvalue which indicates the explained variance.\nper_var=np.round(pca.explained_variance_ratio_*100, decimals=1)\nlabels= ['PC'+str(x) for x in range (1, len(per_var)+1)]\nplt.figure(figsize=(14,4))\nplt.bar(x=range(1,len(per_var)+1),height=per_var, tick_label = labels)\nplt.ylabel('percentage of explained variance')\nplt.xlabel('principal component')\nplt.title('scree plot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graph we can see each of the principle component contribute an amount of explained variance, which indicates that original features contribute to the result or target, and it is relatively hard to do feature extraction and compression using `PCA`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# show the eigenvectors for each component (n_component = 7)\ncolumns_name=heart_disease.columns[:-1]\nindex_name=['pc1','pc2','pc3','pc4','pc5','pc6','pc7']\npd.DataFrame(pca.components_,columns=columns_name,index=index_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show pca score (n_component=7) for each observations\nscores = pca.transform(X_scale)\npd.DataFrame(scores,columns=index_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the table of PCA score for each principle component among 303 observations."},{"metadata":{},"cell_type":"markdown","source":"### 2.2.3 K-means Clustering Model\n\nFor **unsupervised learning**, we can also use `K-means` Clustering model to cluster those given features into certain groups. We also apply `sihouette score` to evaluate the number of cluster.\n#### Model Construction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we set hyperparameter n_cluster =2 \nfrom sklearn.cluster import KMeans\nmodel6= KMeans(n_clusters=2)\nmodel6.fit(X_encode_scaled)\nkmeans_labels=model6.predict(X_encode_scaled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Crosstable labels vs actual target"},{"metadata":{"trusted":true},"cell_type":"code","source":"grouping1=pd.DataFrame({'kmeans_labels':kmeans_labels,'target':y})\ngrouping1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct1=pd.crosstab(grouping1['kmeans_labels'],grouping1['target'])\nct1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From crosstable we can know `Kmeans` can predict most labels correctly, but the accuracy is not good enough."},{"metadata":{},"cell_type":"markdown","source":"#### Draw inertia plot\n\nInertia can measure clustering quality, and it is the distance from samples to its cluster. Elbow of graph indicates best number of clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"inertia=[]\nfor k in range(1,11):\n    model6=KMeans(n_clusters=k)\n    model6.fit(X_encode_scaled)\n    inertia.append(model6.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(1,11),inertia)\nplt.title('Kmeans inertia')\nplt.xlabel('number of clusters')\nplt.ylabel('inertia value')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### evaluate model with sihouette score "},{"metadata":{"trusted":true},"cell_type":"code","source":"sc_scores = []\nclusters = range(2,11)\nfor i in clusters:  \n    model=KMeans(n_clusters=i)\n    model.fit(X_encode_scaled)\n    labels = model.predict(X_encode_scaled).ravel()\n    sc_scores.append(silhouette_score(X_encode_scaled, labels))\nplt.plot(clusters, sc_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2.4 Hierarchical Clustering model\nApart from `Kmeans` clustering, we also apply `hierachical clustering` to cluster the dataset without target. We also apply `sihouette score` to evaluate the number of clusters."},{"metadata":{},"cell_type":"markdown","source":"#### Model Construction"},{"metadata":{"trusted":true},"cell_type":"code","source":"lm1 = linkage(X_encode_scaled, method='ward')\nplt.figure(figsize=(12,4))\ndendrogram(lm1, p=2,truncate_mode='level');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cut the tree by 2 clusters\nout=hierarchy.cut_tree(lm1,n_clusters=2).ravel()\ngrouping2=pd.DataFrame({'hierachical_labels':out,'target':y})\ngrouping2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct2=pd.crosstab(grouping2['hierachical_labels'],grouping2['target'])\nct2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### evaluate model with sihouette score "},{"metadata":{"trusted":true},"cell_type":"code","source":"sc_scores = []\nclusters = range(2,11)\nfor i in clusters:  \n    labels = hierarchy.cut_tree(lm1, n_clusters=i).ravel()\n    sc_scores.append(silhouette_score(X_encode_scaled, labels))\nplt.plot(clusters, sc_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}