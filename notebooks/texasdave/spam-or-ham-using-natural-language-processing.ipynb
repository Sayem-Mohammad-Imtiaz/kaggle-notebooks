{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport string\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Load the csv file, in this case we needed to add some encoding to it so that the unicode characters wouldn't break the \n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/sms-spam-collection-dataset/spam.csv\", encoding='latin-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##  see what we're dealing with by getting the top 5 rows and columns\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that there are more than the columns we really need so let's subset them to the first two columns and all rows.  The format is iloc[ROW RANGE,COLUMN RANGE]"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sms = df.iloc[:,0:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sms.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's relabel the columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sms.columns = ['label', 'message']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sms.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's find out how many items labeled \"spam\" there are and use describe to get some more info like which sms message is the top spam message"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sms.groupby('label').describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's find out the length of these messages and place the value of length into a new column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sms['length'] = df_sms.message.apply(len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's feature engineer the \"spam\" \"ham\" label and make it binary but converting them to 1 , 0 into a new column."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sms['label_num'] = df_sms.label.map({'ham':0, 'spam':1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sms.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's set up a text processor function that uses 'stopwords' module and will remove puncuation and any extra stopwords we define"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sms_text_process(mess):\n    STOPWORDS = stopwords.words('english') + ['u', 'Ã¼', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']\n    nopunc = [char for char in mess if char not in string.punctuation]\n    nopunc = ''.join(nopunc)\n    return ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can view the result by applying it just to the first few rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sms['message'].head(5).apply(sms_text_process)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's apply the processor to all the messages and create a new column with the new clean output"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sms['message_clean'] = df_sms['message'].apply(sms_text_process)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sms.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to do predictive modeling, we must remove the context of the messages and agree that the context simply doesn't matter.\n\nWe don't necessarily care what the message is about, rather we need to determine a way to analyze the words and phrases in it.\n\nWe need to break down the sms messages into some form of numerical representation that the computer can ingest and analyze.\n\nIn NLP this is called tokenization whereby we count the number of times a word appears, no matter where it appears in a message.  Each new word gets a new token.  If a message has the same token structure, it's very likely it is a repeat, and therefore we can begin pattern recognition on that structure and determine if it is spam.\n\nCountVectorizer helps us break down text content into a tokenized structure."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bag of Words is a name for the process (not the name of a module) that takes the content of each message and breaks down the words into numerical counts.  You lose context of the message and essentially now you have a bag of words and not a real sentence. \n\nWe will apply this process to the clean messages."},{"metadata":{"trusted":true},"cell_type":"code","source":"bag_of_words = CountVectorizer(analyzer=sms_text_process).fit(df_sms['message_clean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bag_of_words.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(bag_of_words.vocabulary_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bag_of_words.vocabulary_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"message_bagofwords = bag_of_words.transform(df_sms['message_clean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\ntfidf_trans = TfidfTransformer().fit(message_bagofwords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"message_tfidf = tfidf_trans.transform(message_bagofwords)\nprint(message_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nspam_detect_model = MultinomialNB().fit(message_tfidf,df_sms['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"message = df_sms['message_clean'][4]\nprint(message)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bag_of_words_for_message = bag_of_words.transform([message])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = tfidf_trans.transform(bag_of_words_for_message)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"row = 688\nprint('predicted', spam_detect_model.predict(tfidf)[0])\nprint('actual', df_sms.label[row])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sms[row:row+1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}