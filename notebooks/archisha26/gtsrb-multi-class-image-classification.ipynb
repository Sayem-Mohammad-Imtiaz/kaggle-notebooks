{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os, re, timeit, json, warnings, random\n\nimport numpy as np \nimport pandas as pd \nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nfrom matplotlib.image import imread\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Tensorflow version \" + tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GPU Detection"},{"metadata":{"trusted":true},"cell_type":"code","source":"device_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Configuring and Visualising"},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyperparameters\nIMAGE_SIZE = 28\nEPOCHS = 25\nBATCH_SIZE = 32\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n# storing data directory path\ndata = \"../input/gtsrb-german-traffic-sign\"\n# reading Train.csv\ntrain_csv = pd.read_csv('../input/gtsrb-german-traffic-sign/Train.csv')\n# looking at the first 5 rows of Train.csv\ntrain_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading Test.csv\ntest_csv = pd.read_csv('../input/gtsrb-german-traffic-sign/Test.csv')\n# looking at the first 5 rows of Test.csv\ntest_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# identifying number of unique target labels in the train dataset\nCLASSES = train_csv[\"ClassId\"].nunique()\nprint(\"Number of unique classes: \", train_csv[\"ClassId\"].nunique())\n\n# visualising how many images of each class exist in the train dataset\nplt.figure(figsize=(20,10))\ntrain_csv[\"ClassId\"].value_counts(sort=True).plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a list of train and test images path\ntrain_img_path = data+'/'+train_csv[\"Path\"].values\ntest_img_path = data+'/'+test_csv[\"Path\"].values\n\n# a function that will plot a grid of random 9 images and accepts image path\ndef plot_img(img_path):\n    plt.figure(figsize=(15,15))\n    for i in range(1,10):\n        plt.subplot(3,3,i)\n        random_img_path = random.choice(img_path)\n        rand_img = imread(random_img_path)\n        plt.imshow(rand_img)\n        plt.grid(b=None)\n        plt.xlabel(rand_img.shape[1], fontsize = 20)#width of image\n        plt.ylabel(rand_img.shape[0], fontsize = 20)#height of image\n\n# visualising 9 train images\nplot_img(train_img_path)\nplt.suptitle('Train Images')\nplt.show()\n# visualising 9 test images\nplot_img(test_img_path)\nplt.suptitle('Test Images')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def img_to_array(data_path, csv):\n    data = []\n    labels= csv.ClassId\n    for i in range(len(data_path)):\n        image = tf.io.read_file(data_path[i])\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.cast(image, tf.float32)/256.0\n        image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n        # adding image to arrays\n        data.append(image)\n        \n    return data, labels\n\ndata, labels = img_to_array(train_img_path, train_csv)\n\n# changing the list to a numpy array\nX = np.array(data)\ny = np.array(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# before splitting\nprint(\"----Before Splitting----\")\nprint(\"X.shape\", X.shape)\nprint(\"y.shape\", y.shape)\n\n# splitting the train data into train and validation\nX_train, X_val, y_train, y_val = train_test_split( X, y, test_size=0.20, random_state=42, shuffle=True)\n\n# checking the shape of train and validation data after splitting\nprint(\"----After Splitting----\")\nprint(\"X_train.shape\", X_train.shape)\nprint(\"X_valid.shape\", X_val.shape)\nprint(\"y_train.shape\", y_train.shape)\nprint(\"y_valid.shape\", y_val.shape)\n\n# one-hot encoding the target labels\ny_train = tf.keras.utils.to_categorical(y_train, CLASSES)\ny_val = tf.keras.utils.to_categorical(y_val, CLASSES)\n# checking the shape of train and validation data after one-hot encoding\nprint(\"----After 1-hot encoding----\")\nprint(\"y_train.shape\", y_train.shape)\nprint(\"y_valid.shape\", y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Callbacks"},{"metadata":{"trusted":true},"cell_type":"code","source":"# using Cyclic Learning Rate\n! git clone https://github.com/bckenstler/CLR.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import *\nfrom CLR.clr_callback import *\n\n# using the triangular learning rate policy\nclr_triangular = CyclicLR(mode='triangular')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor = 'val_loss', \n    patience = 3, \n    verbose = 1, \n    factor = 0.5, \n    min_lr = 1e-6\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Augmenting and Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# applying random transformations to the images\naugment = tf.keras.preprocessing.image.ImageDataGenerator(\n    rotation_range=10,\n    zoom_range=0.15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.15,\n    horizontal_flip=False,\n    vertical_flip=False,\n    fill_mode=\"nearest\")\n\n# defining model-resnet50\nresnet_model = tf.keras.applications.ResNet50(\n    weights='imagenet',\n    include_top=False\n)\n\nx = resnet_model.output\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = tf.keras.layers.Dense(512,activation='relu')(x)\npredictions = tf.keras.layers.Dense(\n    CLASSES,\n    activation='softmax'\n)(x)\n\nresnet50_model = tf.keras.models.Model(\n    inputs= resnet_model.input, \n    outputs=predictions\n)\n\nresnet50_model.compile(\n    loss='categorical_crossentropy', \n    optimizer=tf.keras.optimizers.SGD(lr=1e-4, momentum=0.9),\n    metrics=['accuracy']\n)\n\nresnet50_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using `learning_rate_reduction` as callback"},{"metadata":{"trusted":true},"cell_type":"code","source":"startTime = timeit.default_timer()\nhistory_lrr = resnet50_model.fit(\n    augment.flow(X_train, y_train, batch_size=BATCH_SIZE),\n    epochs=50, \n    validation_data=(X_val, y_val),\n    callbacks = [learning_rate_reduction]\n)\nelapsedTime = timeit.default_timer() - startTime\nprint(\"Time taken for the Network to train : \",elapsedTime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Display of the accuracy and the loss values\nplt.figure(0)\nplt.plot(history_lrr.history['accuracy'], label='training accuracy')\nplt.plot(history_lrr.history['val_accuracy'], label='val accuracy')\nplt.title('Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.figure(1)\nplt.plot(history_lrr.history['loss'], label='training loss')\nplt.plot(history_lrr.history['val_loss'], label='val loss')\nplt.title('Loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using `clr` (Cyclic Learnig Rate) as callback"},{"metadata":{"trusted":true},"cell_type":"code","source":"startTime = timeit.default_timer()\nhistory_clr = resnet50_model.fit(\n    augment.flow(X_train, y_train, batch_size=BATCH_SIZE),\n    epochs=50, \n    validation_data=(X_val, y_val),\n    callbacks = [clr_triangular]\n)\nelapsedTime = timeit.default_timer() - startTime\nprint(\"Time taken for the Network to train : \",elapsedTime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Display of the accuracy and the loss values\nplt.figure(0)\nplt.plot(history_clr.history['accuracy'], label='training accuracy')\nplt.plot(history_clr.history['val_accuracy'], label='val accuracy')\nplt.title('Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.figure(1)\nplt.plot(history_clr.history['loss'], label='training loss')\nplt.plot(history_clr.history['val_loss'], label='val loss')\nplt.title('Loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test, y_test = img_to_array(test_img_path, test_csv)\n\n# changing the list to a numpy array\nX_test = np.array(X_test)\ny_test = np.array(y_test)\n\n# predictions\nprediction = resnet50_model.predict(X_test).argmax(axis=1)\n\n# accuracy\nacc = accuracy_score(y_test, prediction)\nprint(\"Accuracy: \", acc)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}