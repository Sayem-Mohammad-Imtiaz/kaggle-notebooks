{"cells":[{"metadata":{},"cell_type":"markdown","source":"I'm going to try something on the dataset that has to do with downsizing some of the features that you're going to see in the notebook. The code is pretty straightforward but if you have any questions/suggestions/comments, please let me know in the comments."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport plotly.express as px","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/health-insurance-cross-sell-prediction/train.csv\")\ntest_df  = pd.read_csv(\"../input/health-insurance-cross-sell-prediction/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.set_index('id')\ntest_df  = test_df.set_index('id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" test_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hmm. No missing values at all. To EDA we go!"},{"metadata":{},"cell_type":"markdown","source":"#### But first, `Region_Code` and `Policy_Sales_Channel` are in float, converting them to string"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Region_Code'] = train_df['Region_Code'].astype(int).astype(str)\ntest_df['Region_Code'] = test_df['Region_Code'].astype(int).astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Policy_Sales_Channel'] = train_df['Policy_Sales_Channel'].astype(int).astype(str)\ntest_df['Policy_Sales_Channel'] = test_df['Policy_Sales_Channel'].astype(int).astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"### Feature distribution in Train vs Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = 25, 3\nsns.color_palette(\"deep\")\nfor feature in ['Gender', 'Driving_License', 'Region_Code', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage', 'Policy_Sales_Channel']:\n    fig, ax = plt.subplots(nrows=1, ncols=2, sharex=True)\n    ax0 = sns.countplot(train_df[feature].sort_values(), ax=ax[0])\n    ax1 = sns.countplot(test_df[feature].sort_values(),  ax=ax[1])\n    ax0.set_title(f'{feature} - Train');\n    ax1.set_title(f'{feature} - Test');\n    \n    if feature == 'Region_Code':\n        for tick in ax0.get_xticklabels():\n            tick.set_rotation(90)\n        for tick in ax1.get_xticklabels():\n            tick.set_rotation(90)\n        \n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the non-quantitative features have the same distribution in Train and Test. Now for continuous features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = 25, 3\nfor feature in ['Age', 'Annual_Premium', 'Vintage']:\n    fig, ax = plt.subplots(nrows=2, ncols=2, sharex=True)\n    ax0 = sns.distplot(train_df[feature].sort_values(), ax=ax[0][0])\n    ax1 = sns.distplot(test_df[feature].sort_values(),  ax=ax[0][1])\n    ax0.set_title(f'{feature} - Train');\n    ax1.set_title(f'{feature} - Test');\n    \n    sns.boxplot(train_df[feature].sort_values(), ax=ax[1][0])\n    sns.boxplot(test_df[feature].sort_values(),  ax=ax[1][1])\n        \n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Same distributions again, which is good!"},{"metadata":{},"cell_type":"markdown","source":"### Lets figure they are related to the `Response` variable"},{"metadata":{},"cell_type":"markdown","source":"For categorical features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = 15, 8\nsns.color_palette(\"deep\")\nfor feature in ['Gender', 'Driving_License', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage']:\n    fig, ax = plt.subplots(nrows=1, ncols=2)\n    temp = train_df[[feature, 'Response']].groupby(feature)['Response'].apply(lambda x: x.sum()/x.count()).mul(100).rename('% (Response = 1)')\n    \n    train_df[feature].value_counts().plot.pie(ax=ax[1])\n    temp.plot(kind='bar', ax=ax[0])\n    \n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations:\n\n- People with no driver's license are not interested much (as Insurance people may ask for it during the process)\n- People who've not been insured before are way more interested than people who have their insurance\n- As Vehicle gets older, people tend to take insurance more\n- If a vehicle has been damaged before, people tend to respond. Maybe because they've had a bad experience where they had their vehicle damaged and the smallest of the part costed a lot"},{"metadata":{},"cell_type":"markdown","source":"But what about continuous features?"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dfc = train_df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = 7, 7\nfor feature in ['Age', 'Annual_Premium', 'Vintage']:    \n    ax = sns.distplot(train_dfc[train_dfc['Response'] == 1][feature], label='Response = 1')\n    sns.distplot(train_dfc[train_dfc['Response'] == 0][feature], ax=ax, label='Response = 0')\n    \n    plt.title(f'{feature} - Response == 0 vs Response == 1')\n    plt.legend();\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interestingly, the `Age` distribution of `Response` = 0 and `Response` = 1 is very different. Also, there seems to be a lot of population interested between 40 and 50. What % exactly? Let's dive in!"},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"Q = 10\ntrain_dfc['AgeGroups'] = pd.qcut(train_dfc['Age'], q=Q)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Why 10? No particular reason, could have used 11, 15, etc. as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dfc[['AgeGroups', 'Response']].groupby('AgeGroups')['Response'].apply(lambda x: x.sum()/x.count() * 100).rename('% (Response == 1)').to_frame()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears that the young population upto age 29 don't respond as they have a HUGE number of insurance options(that they need to research on) and since they have the energy and time, they don't response. As people enter their thirties, maybe because of their responsibilies towards family, to avoid further researching on their own, they respond much, much more. As people get old, they have their own contacts who can get them their insurance, whom they can trust. And as expected, a lot of interested people in the 40-50 age bucket."},{"metadata":{},"cell_type":"markdown","source":"Let's merge ages upto 29, 35 to 50 and 50 onwards"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dfc['self_defined_agegroups'] = pd.cut(train_dfc['Age'], bins=[0, 29, 35, 50, 100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dfc[['self_defined_agegroups', 'Response']].groupby('self_defined_agegroups')['Response'].apply(lambda x: x.sum()/x.count() * 100).rename('% (Response == 1)').to_frame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dfc['VehicleAgeDe'] = train_dfc['Vehicle_Age'].map({'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q = 10\ntrain_dfc['Annual_Premium_Groups'] = pd.qcut(train_dfc['Annual_Premium'], q=Q, duplicates='drop')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dfc[['Annual_Premium_Groups', 'Response']].groupby('Annual_Premium_Groups')['Response'].apply(lambda x: x.sum()/x.count() * 100).rename('% (Response == 1)').to_frame()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Response rate is increasing but very slowly. Let's define out own bins as per this."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dfc['Annual_Premium_self'] = pd.cut(train_dfc['Annual_Premium'], bins=[0, 30000, 35000, 37500, 41700, 48400, np.inf])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dfc[['Annual_Premium_self', 'Response']].groupby('Annual_Premium_self')['Response'].apply(lambda x: x.sum()/x.count() * 100).rename('% (Response == 1)').to_frame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q = 10\ntrain_dfc['Vintage_Groups'] = pd.qcut(train_dfc['Vintage'], q=Q, duplicates='drop')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dfc[['Vintage_Groups', 'Response']].groupby('Vintage_Groups')['Response'].apply(lambda x: x.sum()/x.count() * 100).rename('% (Response == 1)').to_frame()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No pattern at all, no point in binning `Vintage`"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dfc['Annual_Premium_de'] = train_dfc['Annual_Premium_self'].cat.codes\ntrain_dfc['self_defined_agegroups_en'] = train_dfc['self_defined_agegroups'].cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dfc['Gender_ohe'] = pd.get_dummies(train_dfc['Gender'], prefix='Gender', drop_first=True)['Gender_Male']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dfc['Vehicle_Damage_ohe'] = train_dfc['Vehicle_Damage'].map({'Yes': 1, 'No': 0})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Deriving some features for categorical features with many values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dfc[['Policy_Sales_Channel', 'Response']].groupby('Policy_Sales_Channel').apply(lambda x: x.sum()/x.count() * 100)['Response'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = train_df[['Policy_Sales_Channel', 'Response']].groupby('Policy_Sales_Channel').apply(lambda x: x.sum()/x.count() * 100)['Response'].sort_values()\nax = temp_df.plot.bar()\n\nax.set_xticks(temp_df.index[::5])\n\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`Policy_Sales_Channel` = 123 and 43 have 100% response rate! Let's look at these two `Policy_Sales_Channel` only"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[train_df['Policy_Sales_Channel'].isin(['123', '43'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hmm.. so there are only two customers with those `Policy_Sales_Channel`. But is that the case with test set as well? Let's find out."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[test_df['Policy_Sales_Channel'].isin(['123', '43'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4 customers only, which is okay."},{"metadata":{},"cell_type":"markdown","source":"I'm going to club similar `Policy_Sales_Channel` together on the basis of their Response rate using KMeans"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"km = KMeans(n_clusters=6).fit(X = temp_df.values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clusters = km.predict(temp_df.values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = temp_df.rename('% (Response == 1)').reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df['cluster'] = clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.barplot(x=temp_df['Policy_Sales_Channel'], y=temp_df['% (Response == 1)'], hue=temp_df['cluster']);\n\nax.set_xticks(temp_df.index[::5])\n\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dfc = train_dfc.merge(temp_df[['Policy_Sales_Channel', 'cluster']], how='left').rename(columns={'cluster': 'Policy_Sales_Channel_cluster'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dfc[['Region_Code', 'Response']].groupby('Region_Code').apply(lambda x: x.sum()/x.count() * 100)['Response'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = train_df[['Region_Code', 'Response']].groupby('Region_Code').apply(lambda x: x.sum()/x.count() * 100)['Response'].sort_values()\nax = temp_df.plot.bar()\n\nax.set_xticks(temp_df.index[::3])\n\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Repeating the same clustering method for `Region_Code` as well"},{"metadata":{"trusted":true},"cell_type":"code","source":"km = KMeans(n_clusters=6).fit(X = temp_df.values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clusters = km.predict(temp_df.values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = temp_df.rename('% (Response == 1)').reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df['cluster'] = clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.barplot(x=temp_df['Region_Code'], y=temp_df['% (Response == 1)'], hue=temp_df['cluster']);\n\nax.set_xticks(temp_df.index[::3])\n\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dfc = train_dfc.merge(temp_df[['Region_Code', 'cluster']], how='left').rename(columns={'cluster': 'Region_Code_cluster'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dfc = pd.concat([\\\n    train_dfc,\n    pd.get_dummies(train_dfc['Policy_Sales_Channel_cluster'], prefix='PSCC', drop_first=True)\n], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dfc = pd.concat([\\\n    train_dfc,\n    pd.get_dummies(train_dfc['Region_Code_cluster'], prefix='RCC', drop_first=True)\n], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dfc.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_to_consider = ['Driving_License', 'Previously_Insured', 'VehicleAgeDe', 'self_defined_agegroups_en', 'Annual_Premium_de', 'Gender_ohe', 'Vehicle_Damage_ohe', 'PSCC_1', 'PSCC_2', 'PSCC_3', 'PSCC_4', 'PSCC_5', 'RCC_1', 'RCC_2', 'RCC_3', 'RCC_4', 'RCC_5']\ntarget = 'Response'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Deciding on CV"},{"metadata":{},"cell_type":"markdown","source":"I'm going to use 10-fold stratified cross validation for this one"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_dfc.loc[:, features_to_consider].values\ny = train_dfc.loc[:, target].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=10, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dfc['Response'].value_counts(normalize=True).mul(100).round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import f1_score, roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"performance_tree = {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold_no, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    print(f'Running fold {fold_no}')\n    X_train = X[train_idx, :]\n    X_val   = X[val_idx,   :]\n    y_train = y[train_idx]\n    y_val   = y[val_idx]\n    \n    tree = DecisionTreeClassifier().fit(X_train, y_train)\n    predictions = tree.predict(X_val)\n    \n    performance_tree[fold_no] = roc_auc_score(y_val, predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"performance_tree","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_performance = {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold_no, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    print(f'Running fold {fold_no}')\n    X_train = X[train_idx, :]\n    X_val   = X[val_idx,   :]\n    y_train = y[train_idx]\n    y_val   = y[val_idx]\n    \n    rf = RandomForestClassifier(n_estimators=50).fit(X_train, y_train)\n    predictions = rf.predict(X_val)\n    \n    rf_performance[fold_no] = roc_auc_score(y_val, predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_performance.values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The performance is the worst imaginable. Should we use the original features?"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dfc = pd.concat([train_dfc,\n    pd.get_dummies(train_dfc['Region_Code'], prefix='Region_Code', drop_first=True)],\n         axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dfc = pd.concat([train_dfc,\n    pd.get_dummies(train_dfc['Policy_Sales_Channel'], prefix='Policy_Sales_Channel', drop_first=True)],\n                          axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_to_consider = ['Age', 'Driving_License', 'Previously_Insured', 'VehicleAgeDe', 'Vintage', 'Gender_ohe', 'Vehicle_Damage_ohe'] + \\\n                        [i for i in train_dfc.columns if 'Region_Code' in i and i != 'Region_Code_cluster' and i != 'Region_Code'] + \\\n                        [i for i in train_dfc.columns if 'Policy_Sales_Channel' in i and i != 'Policy_Sales_Channel_cluster' and i != 'Policy_Sales_Channel']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_dfc.loc[:, features_to_consider].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_performance_orig = {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold_no, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    print(f'Running fold {fold_no}')\n    X_train = X[train_idx, :]\n    X_val   = X[val_idx,   :]\n    y_train = y[train_idx]\n    y_val   = y[val_idx]\n    \n    rf = RandomForestClassifier(n_estimators=15).fit(X_train, y_train)\n    predictions = rf.predict(X_val)\n    \n    rf_performance_orig[fold_no] = roc_auc_score(y_val, predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_performance_orig","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hmm... a ~15% increase on average! Nice!"},{"metadata":{},"cell_type":"markdown","source":"Although this downsizing process didn't work out well but at least I got to know what happens, this was just something I've been meaning to try out and is **based on** something I've learnt during my time in Financial data science. \n\nApart from that, I know that while building a model, I'm missing many things like normalizing/standardizing features, hyperparameter optimization, etc. but I'm going to skip it for now as I'm learning something new. Anyways, it was fun writing this one, peace."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}