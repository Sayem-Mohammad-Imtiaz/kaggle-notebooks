{"cells":[{"metadata":{},"cell_type":"markdown","source":"Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download): \nhttp://nlp.stanford.edu/data/glove.6B.zip\n\nCommon Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download):\nhttp://nlp.stanford.edu/data/glove.42B.300d.zip \n\nCommon Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download):\nhttp://nlp.stanford.edu/data/glove.840B.300d.zip \n\nTwitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download):\nhttp://nlp.stanford.edu/data/glove.twitter.27B.zip","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!wget -c \"http://nlp.stanford.edu/data/glove.6B.zip\"\n#!wget -c \"http://nlp.stanford.edu/data/glove.42B.zip\"\n#!wget -c \"http://nlp.stanford.edu/data/glove.840B.zip\"\n#!wget -c \"http://nlp.stanford.edu/data/glove.twitter.27B.zip\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!unzip glove.6B.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nfrom wordcloud import WordCloud\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM\nfrom tensorflow.keras.layers import Embedding, Dropout, Activation, Flatten\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.models import model_from_json\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n\n\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\n\nimport pickle\nimport h5py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nfrom nltk.tokenize import word_tokenize, word_tokenize\nfrom nltk.stem import WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input/fake-and-real-news-dataset'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fake = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/True.csv\")\ndf_true = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/Fake.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fake['label'] = 1\ndf_true['label'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df_fake, df_true])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'] + \" \" + df['title']\ndel df['title']\ndel df['subject']\ndel df['date']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features, labels = df['text'].tolist(), df['label'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)\n\n#remove html tags\ndef strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n#Removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n\n\n# Removing URL's\ndef remove_urls(text):\n    return re.sub(r'http\\S+', '', text)\n\n\n#Removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n\n\ndef clean_data(feature_list):\n    feature_list = list(map(lambda x: x.lower(), feature_list))\n    feature_list = list(map(strip_html, feature_list))\n    feature_list = list(map(decontracted, feature_list))\n    feature_list = list(map(remove_between_square_brackets, feature_list))\n    feature_list = list(map(remove_urls, feature_list))\n    feature_list = list(map(remove_stopwords, feature_list))\n    \n    return feature_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = clean_data(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,20)) # Text that is Fake\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop).generate(\" \".join(df[df.label == 1].text))\nplt.imshow(wc , interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,20)) # Text that is not Fake\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop).generate(\" \".join(df[df.label == 0].text))\nplt.imshow(wc , interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#max length of each text \nMAX_SEQUENCE_LENGTH = 500\n\n#only take 10000 words from all unique words(or only 10000 features max)\nMAX_NUM_WORDS = 10000\n\n#each word should be represented by 300 dimension\nEMBEDDING_DIM = 200\n\n#ratio of train/test will be 80/20\nVALIDATION_SPLIT = 0.2\n\n#path for glove embedding file\nglove_txt_path = \"glove.6B.200d.txt\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(features, labels, stratify=labels, random_state = 42, test_size=VALIDATION_SPLIT, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_tokens(tokenizer, data, max_seq_len):\n    tokenized_data = tokenizer.texts_to_sequences(data)\n    padded_tokenized_data = pad_sequences(tokenized_data, maxlen=max_seq_len)\n    return padded_tokenized_data\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create tokens for train, test ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = create_tokens(tokenizer, x_train, MAX_SEQUENCE_LENGTH)\nX_test = create_tokens(tokenizer, x_test, MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert labels to numpy array ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = np.array(y_test)\ny_train = np.array(y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Function for Creating Glove embedding matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_txt_file = open(glove_txt_path, \"r\", encoding=\"utf8\")\nembeddings_index = {}\nfor line in glove_txt_file:\n    values = line.split()\n    word = ''.join(values[:-EMBEDDING_DIM])\n    coefs = np.asarray(values[-EMBEDDING_DIM:], dtype='float32')\n    embeddings_index[word] = coefs\nglove_txt_file.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\n\nprint('create embedding matrix')\nword_index = tokenizer.word_index\nnb_words = min(MAX_NUM_WORDS, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if i >= MAX_NUM_WORDS: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del embeddings_index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Design Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(embed_matrix, max_num_words, embed_dim, max_seq_len):\n    model = Sequential()\n    model.add(Embedding(max_num_words, output_dim=embed_dim, weights=[embed_matrix], input_length=max_seq_len, trainable=False))  \n\n    model.add(Conv1D(filters=128, kernel_size=4, activation='relu'))\n    model.add(MaxPooling1D(4))\n    model.add(Dropout(0.3))\n\n    model.add(Flatten())\n    model.add(Dense(units = 128 , activation = 'relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(1, activation='sigmoid')) #here activation function is sigmoid because we want only one output 0/1\n    \n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating model instance\nmodel = create_model(embedding_matrix, MAX_NUM_WORDS, EMBEDDING_DIM, MAX_SEQUENCE_LENGTH)\n\n#compile mode with optimizer = adam, loss = binary_crossentropy, metrics = accuracy\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nbatch_size = 128\nepochs = 10\n\n#setting callback function for reducing learning rate\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)\n\n#setting callback functiob for sarly stopping if loss is not decreasing\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n\n\nhistory = model.fit(X_train, \n                    y_train, \n                    batch_size = batch_size , \n                    validation_data = (X_test,y_test) ,\n                    epochs = epochs, \n                    shuffle=True,\n                    callbacks = [learning_rate_reduction, es])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accr_train = model.evaluate(X_train,y_train)\nprint('Accuracy Train: {}'.format(accr_train[1]*100))\naccr_test = model.evaluate(X_test,y_test)\nprint('Accuracy Test: {}'.format(accr_test[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict_classes(X_test)\ncf_matrix = confusion_matrix(y_test,pred)\nsns.heatmap(cf_matrix, annot=True, fmt='g')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# serialize model to json\njson_model = model.to_json()\n\n#save the model architecture to JSON file\nwith open('fake_true_news_model.json', 'w') as json_file:\n    json_file.write(json_model)\n\n#saving the weights of the model\nmodel.save_weights('fake_true_news_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}