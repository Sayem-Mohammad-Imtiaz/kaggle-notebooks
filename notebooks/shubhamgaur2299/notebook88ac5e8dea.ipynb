{"cells":[{"metadata":{"_cell_guid":"21cee267-f257-4ba3-88e1-d22e2fc57c7e","_uuid":"2fec7805d8d0e999fc405d136beff46ac331b963","trusted":true},"cell_type":"code","source":"# Basic packages\nimport pandas as pd \nimport numpy as np\nimport re\nimport collections\nimport matplotlib.pyplot as plt\n\n# Packages for data preparation\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\n\n# Packages for modeling\nfrom keras import models\nfrom keras import layers\nfrom keras import regularizers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"992184fc16ab7ce9917146d08581772c2349337d"},"cell_type":"code","source":"NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary\nVAL_SIZE = 1000  # Size of the validation set\nNB_START_EPOCHS = 20  # Number of epochs we usually start to train with\nBATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca3edd68b979a54e032b5eaa62d0ea1dc5caff66"},"cell_type":"code","source":"df = pd.read_csv('../input/Tweets.csv')\ndf = df.reindex(np.random.permutation(df.index))  \ndf = df[['text', 'airline_sentiment']]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1210dcf7d75e9cfd96d0ca489c7b9136aad21db"},"cell_type":"code","source":"def remove_stopwords(input_text):\n        stopwords_list = stopwords.words('english')\n        # Some words which might indicate a certain sentiment are kept via a whitelist\n        whitelist = [\"n't\", \"not\", \"no\"]\n        words = input_text.split() \n        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n        return \" \".join(clean_words) \n    \ndef remove_mentions(input_text):\n        return re.sub(r'@\\w+', '', input_text)\n       \ndf.text = df.text.apply(remove_stopwords).apply(remove_mentions)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"df742ade-52d3-4b47-80b0-1f7cc26545eb","_uuid":"6ab9296159749e4525244fae6d6f4e6a37121c93","trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df.text, df.airline_sentiment, test_size=0.1, random_state=37)\nprint('# Train data samples:', X_train.shape[0])\nprint('# Test data samples:', X_test.shape[0])\nassert X_train.shape[0] == y_train.shape[0]\nassert X_test.shape[0] == y_test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6cbf76b09ac27e67f7e92eb0986a963db00ddcd6"},"cell_type":"code","source":"tk = Tokenizer(num_words=NB_WORDS,\n               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n               lower=True,\n               split=\" \")\ntk.fit_on_texts(X_train)\n\nprint('Fitted tokenizer on {} documents'.format(tk.document_count))\nprint('{} words in dictionary'.format(tk.num_words))\nprint('Top 5 most common words are:', collections.Counter(tk.word_counts).most_common(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28435e6639f4dac6d9bdc9df65b1dd4651b15617"},"cell_type":"code","source":"X_train_seq = tk.texts_to_sequences(X_train)\nX_test_seq = tk.texts_to_sequences(X_test)\n\nprint('\"{}\" is converted into {}'.format(X_train[0], X_test_seq[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd91541a0f3c027dca3429af497379fd86a7079b"},"cell_type":"code","source":"def one_hot_seq(seqs, nb_features = NB_WORDS):\n    ohs = np.zeros((len(seqs), nb_features))\n    for i, s in enumerate(seqs):\n        ohs[i, s] = 1.\n    return ohs\n\nX_train_oh = one_hot_seq(X_train_seq)\nX_test_oh = one_hot_seq(X_test_seq)\n\nprint('\"{}\" is converted into {}'.format(X_train_seq[0], X_train_oh[0]))\nprint('For this example we have {} features with a value of 1.'.format(X_train_oh[0].sum()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f64570f56244464eaf88805a8c41b46f76db4668"},"cell_type":"code","source":"le = LabelEncoder()\ny_train_le = le.fit_transform(y_train)\ny_test_le = le.transform(y_test)\ny_train_oh = to_categorical(y_train_le)\ny_test_oh = to_categorical(y_test_le)\n\nprint('\"{}\" is converted into {}'.format(y_train[0], y_train_le[0]))\nprint('\"{}\" is converted into {}'.format(y_train_le[0], y_train_oh[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4b7683309d5501c80d9e4e7335a76e58e8999f0"},"cell_type":"code","source":"X_train_rest, X_valid, y_train_rest, y_valid = train_test_split(X_train_oh, y_train_oh, test_size=0.1, random_state=37)\n\nassert X_valid.shape[0] == y_valid.shape[0]\nassert X_train_rest.shape[0] == y_train_rest.shape[0]\n\nprint('Shape of validation set:',X_valid.shape)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}