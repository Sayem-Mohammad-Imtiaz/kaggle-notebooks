{"cells":[{"metadata":{"_uuid":"a52a26cc-7eee-40ad-a175-17ddb2ada4c4","_cell_guid":"16bf6f9c-a26c-47d7-a6ba-8189da2af131","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade tensorflow-gpu==2.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install plotly\n!pip install --upgrade nbformat\n!pip install nltk\n!pip install spacy # spaCy is an open-source software library for advanced natural language processing\n!pip install WordCloud\n!pip install gensim # Gensim is an open-source library for unsupervised topic modeling and natural language processing\nimport nltk\nnltk.download('punkt')\n\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nimport nltk\nimport re\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\n# import keras\nfrom tensorflow.keras.preprocessing.text import one_hot, Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional\nfrom tensorflow.keras.models import Model\n# setting the style of the notebook to be monokai theme  \n# this line of code is important to ensure that we are able to see the x and y axes clearly\n# If you don't run this code line, you will notice that the xlabel and ylabel on any plot is black on black and it will be hard to see them. \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the data\ndf_true = pd.read_csv(\"../input/fake-and-real-news-dataset/True.csv\")\ndf_fake = pd.read_csv(\"../input/fake-and-real-news-dataset/Fake.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_true.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fake.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_true.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add a target class column to indicate whether the news is real or fake\ndf_true['isfake'] = 1\ndf_true.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fake['isfake'] = 0\ndf_fake.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concatenate Real and Fake News\ndf = pd.concat([df_true, df_fake]).reset_index(drop = True)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns = ['date'], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine title and text together\ndf['original'] = df['title'] + ' ' + df['text']\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# download stopwords\nnltk.download(\"stopwords\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Obtain additional stopwords from nltk\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove stopwords and remove words with 2 or less characters\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words:\n            result.append(token)\n            \n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply the function to the dataframe\ndf['clean'] = df['original'].apply(preprocess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Obtain the total words present in the dataset\nlist_of_words = []\nfor i in df.clean:\n    for j in i:\n        list_of_words.append(j)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(list_of_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Obtain the total number of unique words\ntotal_words = len(list(set(list_of_words)))\ntotal_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# join the words into a string\ndf['clean_joined'] = df['clean'].apply(lambda x: \" \".join(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the number of samples in 'subject'\nplt.figure(figsize = (8, 8))\nsns.countplot(y = \"subject\", data = df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the word cloud for text that is Real\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(df[df.isfake == 1].clean_joined))\nplt.imshow(wc, interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the word cloud for text that is Fake\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(df[df.isfake == 0].clean_joined))\nplt.imshow(wc, interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# length of maximum document will be needed to create word embeddings \nmaxlen = -1\nfor doc in df.clean_joined:\n    tokens = nltk.word_tokenize(doc)\n    if(maxlen<len(tokens)):\n        maxlen = len(tokens)\nprint(\"The maximum number of words in any document is =\", maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize the distribution of number of words in a text\nimport plotly.express as px\nfig = px.histogram(x = [len(nltk.word_tokenize(x)) for x in df.clean_joined], nbins = 100)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into test and train \nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(df.clean_joined, df.isfake, test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import word_tokenize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a tokenizer to tokenize the words and create sequences of tokenized words\ntokenizer = Tokenizer(num_words = total_words)\ntokenizer.fit_on_texts(x_train)\ntrain_sequences = tokenizer.texts_to_sequences(x_train)\ntest_sequences = tokenizer.texts_to_sequences(x_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add padding can either be maxlen = 4406 or smaller number maxlen = 40 seems to work well based on results\npadded_train = pad_sequences(train_sequences,maxlen = 40, padding = 'post', truncating = 'post')\npadded_test = pad_sequences(test_sequences,maxlen = 40, truncating = 'post') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,doc in enumerate(padded_train[:2]):\n     print(\"The padded encoding for document\",i+1,\" is : \",doc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sequential Model\nmodel = Sequential()\n\n# embeddidng layer\nmodel.add(Embedding(total_words, output_dim = 128))\n# model.add(Embedding(total_words, output_dim = 240))\n\n\n# Bi-Directional RNN and LSTM\nmodel.add(Bidirectional(LSTM(128)))\n\n# Dense layers\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dense(1,activation= 'sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = np.asarray(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the model\nmodel.fit(padded_train, y_train, batch_size = 64, validation_split = 0.1, epochs = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make prediction\npred = model.predict(padded_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if the predicted value is >0.5 it is real else it is fake\nprediction = []\nfor i in range(len(pred)):\n    if pred[i].item() > 0.5:\n        prediction.append(1)\n    else:\n        prediction.append(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting the accuracy\nfrom sklearn.metrics import accuracy_score\n\naccuracy = accuracy_score(list(y_test), prediction)\n\nprint(\"Model Accuracy : \", accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(list(y_test), prediction)\nplt.figure(figsize = (25, 25))\nsns.heatmap(cm, annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# category dict\ncategory = { 0: 'Fake News', 1 : \"Real News\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data containing real news\ndf_true\n# data containing fake news\ndf_fake\n# dataframe information\ndf_true.info()\n# dataframe information\ndf_fake.info()\n# check for null values\ndf_true.isnull().sum()\n# check for null values\ndf_fake.isnull().sum()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}