{"cells":[{"metadata":{"_uuid":"04ecd3cd795b6e4cf4ff048c1f00601ef69f8e95","_cell_guid":"55cf54c6-60c1-4def-a8a2-996d89cf8d96","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom plotly import tools\nimport plotly.plotly as py\nimport plotly.figure_factory as ff\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n\nMAIN_PATH = '../input/'\ndf = pd.read_csv(MAIN_PATH +'bank.csv')\nterm_deposits = df.copy()\n# Have a grasp of how our data looks.\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c15933c3f2ae7fe44dde70f69dd96fe8f3a612b"},"cell_type":"code","source":"plt.style.use('ggplot')\n\nf, ax = plt.subplots(1,2, figsize=(16,8))\n\ncolors = [\"#FA5858\", \"#64FE2E\"]\nlabels =\"Did not Open Term Suscriptions\", \"Opened Term Subscriptions\"\n\nplt.suptitle('Information on Term Suscriptions', fontsize=20)\n\ndf[\"deposit\"].value_counts().plot.pie(explode=[0,0.15], autopct='%1.2f%%', ax=ax[0], shadow=True, colors=colors, \n                                             labels=labels, fontsize=12, startangle=25)\n\n    \n# ax[0].set_title('State of Loan', fontsize=16)\nax[0].set_ylabel('% of Condition of Loans', fontsize=14)\n\n# sns.countplot('loan_condition', data=df, ax=ax[1], palette=colors)\n# ax[1].set_title('Condition of Loans', fontsize=20)\n# ax[1].set_xticklabels(['Good', 'Bad'], rotation='horizontal')\npalette = [\"#64FE2E\", \"#FA5858\"]\n\nba = sns.barplot(x=\"education\", y=\"balance\", hue=\"deposit\", data=df, palette=palette, estimator=lambda x: len(x) / len(df) * 100)\n\nfor a in ba.patches:\n        \n    ba.annotate( format(a.get_height(), '.1f')+' %' , \n             (a.get_x() + a.get_width() / 2., a.get_height()), \n             ha = 'center', \n             va = 'center', \n             xytext = (0, 10), \n             textcoords = 'offset points')\n    \n\nax[1].set(ylabel=\"(%)\")\nax[1].set_xticklabels(df[\"education\"].unique(), rotation=0, rotation_mode=\"anchor\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92706f68c12026775143ec8b6f200cc94f739edd","_cell_guid":"10648601-36e1-464f-83e0-040b59326d1e","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Let's see how the numeric data is distributed.\n#import matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')\n\ndf.hist(bins=20, figsize=(14,10), color='#E14906')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['balance1'] = df.balance/1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.style.use('dark_background')\nfig = plt.figure(figsize=(20,20))\nax1 = fig.add_subplot(221)\nax2 = fig.add_subplot(222)\nax3 = fig.add_subplot(212)\n\nylab = \"Balance (thousands)\"\n\ng = sns.boxplot(x=\"default\", y=\"balance1\", hue=\"deposit\",\n                    data=df, palette=\"muted\", ax=ax1)\ng.set_ylabel(ylab)\ng.set_title(\"Amount of Balance by Term Suscriptions\")\n\n# ax.set_xticklabels(df[\"default\"].unique(), rotation=45, rotation_mode=\"anchor\")\n\ng1 = sns.boxplot(x=\"job\", y=\"balance1\", hue=\"deposit\",\n                 data=df, palette=\"RdBu\", ax=ax2)\n\ng1.set_xticklabels(df[\"job\"].unique(), rotation=90, rotation_mode=\"anchor\")\ng1.set_title(\"Type of Work by Term Suscriptions\")\ng1.set_ylabel(ylab)\n\ng2 = sns.violinplot(data=df, x=\"education\", y=\"balance1\", hue=\"deposit\", palette=\"RdBu_r\")\n\ng2.set_title(\"Distribution of Balance by Education\")\ng2.set_ylabel(ylab)\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ba88aeb22bcb93ee3810c86c494de31cda39844"},"cell_type":"markdown","source":"<h3> Analysis by Occupation: </h3>\n<ul> \n    <li> <b> Number of Occupations: </b>  Management is the occupation that is more prevalent in this dataset.</li>\n    <li><b>Age by Occupation: </b>  As expected, the retired are the ones who have the highest median age while student are the lowest.</li>\n    <li><b> Balance by Occupation: </b> Management and Retirees are the ones who have the highest balance in their accounts. </li>\n    </ul>"},{"metadata":{"trusted":true,"_uuid":"e6f0164ced43c674d386217be2b8bd3c082269dc","_kg_hide-input":true},"cell_type":"code","source":"# Drop the Job Occupations that are \"Unknown\"\ndf.drop(df.loc[df[\"job\"] == \"unknown\"].index,inplace= True )\n\n# Admin and management are basically the same let's put it under the same categorical value\ndf.loc[df[\"job\"] == \"admin.\", \"job\"] = \"management\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e01924e81719861c42a6775e42eca5188ea3dcd","_kg_hide-input":true},"cell_type":"code","source":"import squarify\ndf = df.drop(df.loc[df[\"balance\"] == 0].index)\n\n\nx = 0\ny = 0\nwidth = 100\nheight = 100\n\njob_names = df['job'].value_counts().index\nvalues = df['job'].value_counts().tolist()\n\nnormed = squarify.normalize_sizes(values, width, height)\nrects = squarify.squarify(normed, x, y, width, height)\n\ncolors = ['rgb(200, 255, 144)','rgb(135, 206, 235)',\n          'rgb(235, 164, 135)','rgb(220, 208, 255)',\n          'rgb(253, 253, 150)','rgb(255, 127, 80)', \n         'rgb(218, 156, 133)', 'rgb(245, 92, 76)',\n         'rgb(252,64,68)', 'rgb(154,123,91)']\n\nshapes = []\nannotations = []\ncounter = 0\n\nfor r in rects:\n    shapes.append(\n        dict(\n            type = 'rect',\n            x0 = r['x'],\n            y0 = r['y'],\n            x1 = r['x'] + r['dx'],\n            y1 = r['y'] + r['dy'],\n            line = dict(width=2),\n            fillcolor = colors[counter]\n        )\n    )\n    annotations.append(\n        dict(\n            x = r['x']+(r['dx']/2),\n            y = r['y']+(r['dy']/2),\n            text = values[counter],\n            showarrow = False\n        )\n    )\n    counter = counter + 1\n    if counter >= len(colors):\n        counter = 0\n    \n# For hover text\ntrace0 = go.Scatter(\n    x = [ r['x']+(r['dx']/2) for r in rects],\n    y = [ r['y']+(r['dy']/2) for r in rects],\n    text = [ str(v) for v in job_names],\n    mode='text',\n)\n\nlayout = dict(\n    title='Number of Occupations <br> <i>(From our Sample Population)</i>',\n    height=700, \n    width=700,\n    xaxis=dict(showgrid=False,zeroline=False),\n    yaxis=dict(showgrid=False,zeroline=False),\n    shapes=shapes,\n    annotations=annotations,\n    hovermode='closest'\n)\n\n# With hovertext\nfigure = dict(data=[trace0], layout=layout)\n\niplot(figure, filename='squarify-treemap')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6aeb678daa10227dfd875d69721184b6edcd4d4","_kg_hide-input":true},"cell_type":"code","source":"# Now let's see which occupation tended to have more balance in their accounts\n\nsuscribed_df = df.loc[df[\"deposit\"] == \"yes\"]\n\nages = [suscribed_df[\"age\"].loc[suscribed_df[\"job\"] == job].values for job in df.job.unique() ]\n\ncolors = ['rgba(93, 164, 214, 0.5)', 'rgba(255, 144, 14, 0.5)',\n          'rgba(44, 160, 101, 0.5)', 'rgba(255, 65, 54, 0.5)', \n          'rgba(207, 114, 255, 0.5)', 'rgba(127, 96, 0, 0.5)',\n         'rgba(229, 126, 56, 0.5)', 'rgba(229, 56, 56, 0.5)',\n         'rgba(174, 229, 56, 0.5)', 'rgba(229, 56, 56, 0.5)']\n\noccupations = df[\"job\"].unique().tolist()\n\n\ntraces = []\n\nfor xd, yd, cls in zip(occupations, ages, colors):\n        traces.append(go.Box(\n            y=yd,\n            name=xd,\n            boxpoints='all',\n            jitter=0.5,\n            whiskerwidth=0.2,\n            fillcolor=cls,\n            marker=dict(\n                size=2,\n            ),\n            line=dict(width=1),\n        ))\n\nlayout = go.Layout(\n    title='Distribution of Ages by Occupation',\n    yaxis=dict(\n        autorange=True,\n        showgrid=True,\n        zeroline=True,\n        dtick=5,\n        gridcolor='rgb(255, 255, 255)',\n        gridwidth=1,\n        zerolinecolor='rgb(255, 255, 255)',\n        zerolinewidth=2,\n    ),\n    margin=dict(\n        l=40,\n        r=30,\n        b=80,\n        t=100,\n    ),\n    paper_bgcolor='rgb(224,255,246)',\n    plot_bgcolor='rgb(251,251,251)',\n    showlegend=False\n)\n\nfig = go.Figure(data=traces, layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1eca4c22d89d9ffc1a28fd978629c9ed468c8c5d","_kg_hide-input":true},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\n\nvals = df['marital'].value_counts().tolist()\nlabels = df['marital'].unique().tolist()\n\ndata = [go.Bar(\n            x=labels,\n            y=vals,\n    marker=dict(\n    color=\"#FE9A2E\")\n    )]\n\nlayout = go.Layout(\n    title=\"Count by Marital Status\",\n)\n\nfig = go.Figure(data=data, layout=layout)\n\n\n\niplot(fig, filename='basic-bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b9af53b779d764cd7d020599b4f93fd77ca6da7","_kg_hide-input":true},"cell_type":"code","source":"# Distribution of Balances by Marital status\n\nstatus = ([df.loc[df['marital'] == stat]['balance'] for stat in df.marital.unique()])\n\nclrs = ['#6E6E6E', '#2E9AFE', '#FA5858']\n\nfig = tools.make_subplots(rows=3, print_grid=False)\n\nfor i, (s, c) in enumerate(zip(status,clrs)):\n   \n    trace1= go.Histogram(\n    x=s,\n    histnorm='density', \n    name='single',\n    marker=dict(\n        color=c\n    )\n)\n    \n\n    fig.append_trace(trace1, i+1, 1)\n    \nfig['layout'].update(showlegend=False,\n                     title=\"Price Distributions by Marital Status\",\n                     height=1000, \n                     width=800,\n                      )\n\niplot(fig, filename='custom-sized-subplot-with-subplot-titles')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43247a42ea5af2ea429f4b3a888b7d044ef4387f","_kg_hide-input":true},"cell_type":"code","source":"# Hmmm We have missed some important clients with some high balances. \n# This shouldn't be happening.\nfig = ff.create_facet_grid(\n    df,\n    y='balance',\n    facet_row='marital',\n    facet_col='deposit',\n    trace_type='box',\n)\n\niplot(fig, filename='facet - box traces')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5130fa87ead84a5310386ba62f43f1d5c805f78"},"cell_type":"markdown","source":"<h3>Clustering Marital Status and Education: </h3>\n\n<ul> \n    <li><b>Marital Status: </b>  As discussed previously, the impact of a divorce has a significant impact on the balance of the individual. </li>\n    <li><b>Education: </b> The level of education also has a significant impact on the amount of balance a prospect has.</li>\n    <li><b> Loans: </b> Whether the prospect has a previous loan has a significant impact on the amount of balance he or she has. </li>\n</ul>"},{"metadata":{"trusted":true,"_uuid":"9893d3fe4e2b230ea41172200470b03b77123a18","_kg_hide-input":true},"cell_type":"code","source":"df = df.drop(df.loc[df[\"education\"] == \"unknown\"].index)\n\ndf['education'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['marital/education'] = (df['marital']) + '/' + (df['education'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad652b23e6677d174bacb5c8fb92b937a8bbec7d","_kg_hide-input":true},"cell_type":"code","source":"pal = sns.cubehelix_palette(10, rot=-.25, light=.7)\ng = sns.FacetGrid(df, row=\"marital/education\", hue=\"marital/education\", aspect= 5, palette=pal)\n\ng.map(sns.kdeplot, \"balance\", clip_on=False, shade=True, alpha=1, lw=1.5, bw=.2)\ng.map(sns.kdeplot, \"balance\", clip_on=False, color=\"w\", lw=1, bw=0)\ng.map(plt.axhline, y=0, lw=2, clip_on=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['education/marital'] = (df['education']) + '/' + (df['marital'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11fa815fc82afc79c0d2fae3c0cab3a9a8f6de23","_kg_hide-input":true},"cell_type":"code","source":"education_groups = df.groupby(['education/marital'], as_index=False)['balance'].median()\n\nfig = plt.figure(figsize=(20,28))\n\n\n\n\nsns.barplot(x=\"balance\", y=\"education/marital\", data=education_groups,\n            label=\"Total\", palette=\"RdBu\")\n\nplt.title('Median Balance by Educational/Marital Group', fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loan_balance = df.groupby(['marital/education', 'loan'], as_index=False)['balance'].median()\n\nloans = [loan_balance['balance'].loc[loan_balance['loan'] == l].values for l in loan_balance.loan.unique()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf0f39195c185d682a28aab0129c44ad7ed8eb68","_kg_hide-input":true},"cell_type":"code","source":"# Let's see the group who had loans from the marital/education group\n\nloan_balance = df.groupby(['marital/education', 'loan'], as_index=False)['balance'].median()\n\n\nno_loan = loan_balance['balance'].loc[loan_balance['loan'] == 'no'].values\nhas_loan = loan_balance['balance'].loc[loan_balance['loan'] == 'yes'].values\n\n\nlabels = loan_balance['marital/education'].unique().tolist()\n\n\ntrace0 = go.Scatter(\n    x=no_loan,\n    y=labels,\n    mode='markers',\n    name='No Loan',\n    marker=dict(\n        color='rgb(175,238,238)',\n        line=dict(\n            color='rgb(0,139,139)',\n            width=1,\n        ),\n        symbol='circle',\n        size=16,\n    )\n)\ntrace1 = go.Scatter(\n    x=has_loan,\n    y=labels,\n    mode='markers',\n    name='Has a Previous Loan',\n    marker=dict(\n        color='rgb(250,128,114)',\n        line=dict(\n            color='rgb(178,34,34)',\n            width=1,\n        ),\n        symbol='circle',\n        size=16,\n    )\n)\n\ndata = [trace0, trace1]\nlayout = go.Layout(\n    title=\"The Impact of Loans to Married/Educational Clusters\",\n    xaxis=dict(\n        showgrid=False,\n        showline=True,\n        linecolor='rgb(102, 102, 102)',\n        titlefont=dict(\n            color='rgb(204, 204, 204)'\n        ),\n        tickfont=dict(\n            color='rgb(102, 102, 102)',\n        ),\n        showticklabels=False,\n        dtick=10,\n        ticks='outside',\n        tickcolor='rgb(102, 102, 102)',\n    ),\n    margin=dict(\n        l=140,\n        r=40,\n        b=50,\n        t=80\n    ),\n    legend=dict(\n        font=dict(\n            size=10,\n        ),\n        yanchor='middle',\n        xanchor='right',\n    ),\n    width=1000,\n    height=800,\n    paper_bgcolor='rgb(255,250,250)',\n    plot_bgcolor='rgb(255,255,255)',\n    hovermode='closest',\n)\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename='lowest-oecd-votes-cast')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5648949445431448917a6c2571b63cd796769790"},"cell_type":"code","source":"import seaborn as sns\nsns.set(style=\"ticks\")\n\nsns.pairplot(df, hue=\"marital/education\", palette=\"Set1\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c00f4366746d63c9f19f1d6822adbf3f2464b706"},"cell_type":"code","source":"fig = plt.figure(figsize=(12,8))\n\nsns.violinplot(x=\"balance\", y=\"job\", hue=\"deposit\", palette=\"RdBu_r\",\n            data=df);\n\nplt.title(\"Job Distribution of Balances by Deposit Status\", fontsize=16)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0394b5d2e3fc42cbe7a0edc98cfdf8c8940a1fc"},"cell_type":"markdown","source":"<h3><b>Campaign Duration:</b> </h3>\n<ul>\n    <li><b>Campaign Duration:</b> Hmm, we see that duration has a high correlation with term deposits meaning the higher the duration, the more likely it is for a client to open a term deposit.  </li>\n    <li> <b> Average Campaign Duration: </b> The average campaign duration is 374.76, let's see if clients that were above this average were more likely to open a term deposit. </b></li>\n    <li><b>Duration Status: </b> People who were above the duration status, were more likely to open a term deposit. 78% of the group that is above average in duration opened term deposits while those that were below average 32% opened term deposit accounts. This tells us that it will be a good idea to target individuals who are in the above average category.</li>\n    </ul>"},{"metadata":{"trusted":true,"_uuid":"f25f1b0ca9c69c1a5a0e5f8ff5958597f7c80e9e"},"cell_type":"code","source":"df.drop('marital/education', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46bdebb06eb0c68128098fb9804b301a4eb065f6"},"cell_type":"code","source":"\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfig = plt.figure(figsize=(30,25))\ndf['deposit'] = LabelEncoder().fit_transform(df['deposit'])\n\n\n\n# Separate both dataframes into \nnumeric_df = df.select_dtypes(exclude=\"object\")\n# categorical_df = df.select_dtypes(include=\"object\")\n\ncorr_numeric = numeric_df.corr()\n\n\nsns.heatmap(corr_numeric, cbar=True, cmap=\"RdBu_r\")\nplt.title(\"Correlation Matrix\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e34b61948a8d2dee918b4aca0a186e5d50b6392"},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.set_style('whitegrid')\navg_duration = df['duration'].mean()\n\ndf[\"duration_status\"] = [(\"below_average\" if b < avg_duration else \"above_average\") for b in df.duration ]\n\npct_term = pd.crosstab(df['duration_status'], df['deposit']).apply(lambda r: round(r/r.sum(), 2) * 100, axis=1)\n\nax = pct_term.plot(kind='bar', stacked=False, cmap='RdBu')\nplt.title(\"The Impact of Duration \\n in Opening a Term Deposit\", fontsize=18)\nplt.xlabel(\"Duration Status\", fontsize=18);\nplt.ylabel(\"Percentage (%)\", fontsize=18)\n\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.02, p.get_height() * 1.02))\n            \n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6715e27546ccd5cf59c97c677819c170275ec89a"},"cell_type":"markdown","source":"<h2> <b>Classification Model:</b> </h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"term_deposits.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a1227158f08fd3d7d23b14a2b5cca68fc3e8c32"},"cell_type":"code","source":"dep = term_deposits['deposit']\nterm_deposits.drop(labels=['deposit'], axis=1,inplace=True)\nterm_deposits.insert(0, 'deposit', dep)\nterm_deposits.head()\n# housing has a -20% correlation with deposit let's see how it is distributed.\n# 52 %\nterm_deposits[\"housing\"].value_counts()/len(term_deposits)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e8365a2431959fa9ce1c52568e7d66aee7a756c"},"cell_type":"code","source":"term_deposits[\"loan\"].value_counts()/len(term_deposits)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e48052d017060f95d5adf6c9423a17d12256c269"},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n# Here we split the data into training and test sets and implement a stratified shuffle split.\nstratified = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train_set, test_set in stratified.split(term_deposits, term_deposits[\"loan\"]):\n    stratified_train = term_deposits.loc[train_set]\n    stratified_test = term_deposits.loc[test_set]\n    \n\nprint(stratified_train[\"loan\"].value_counts()/len(stratified_train))\nprint(stratified_test[\"loan\"].value_counts()/len(stratified_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"692970c0c6d3a27d0c266a6cbe0b25bee5536884"},"cell_type":"code","source":"# Separate the labels and the features.\ntrain_data = stratified_train # Make a copy of the stratified training set.\ntest_data = stratified_test\nprint(train_data.shape)\nprint(test_data.shape)\ntrain_data['deposit'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3a2d7aae42d41a6d24cb11d2f562c21e3888fb5"},"cell_type":"code","source":"# Definition of the CategoricalEncoder class, copied from PR #9151.\n# Just run this cell, or copy it to your code, no need to try to\n# understand every line.\n# Code reference Hands on Machine Learning with Scikit Learn and Tensorflow by Aurelien Geron.\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import sparse\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"Encode categorical features as a numeric array.\n    The input to this transformer should be a matrix of integers or strings,\n    denoting the values taken on by categorical (discrete) features.\n    The features can be encoded using a one-hot aka one-of-K scheme\n    (``encoding='onehot'``, the default) or converted to ordinal integers\n    (``encoding='ordinal'``).\n    This encoding is needed for feeding categorical data to many scikit-learn\n    estimators, notably linear models and SVMs with the standard kernels.\n    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n    Parameters\n    ----------\n    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'\n        The type of encoding to use (default is 'onehot'):\n        - 'onehot': encode the features using a one-hot aka one-of-K scheme\n          (or also called 'dummy' encoding). This creates a binary column for\n          each category and returns a sparse matrix.\n        - 'onehot-dense': the same as 'onehot' but returns a dense array\n          instead of a sparse matrix.\n        - 'ordinal': encode the features as ordinal integers. This results in\n          a single column of integers (0 to n_categories - 1) per feature.\n    categories : 'auto' or a list of lists/arrays of values.\n        Categories (unique values) per feature:\n        - 'auto' : Determine categories automatically from the training data.\n        - list : ``categories[i]`` holds the categories expected in the ith\n          column. The passed categories are sorted before encoding the data\n          (used categories can be found in the ``categories_`` attribute).\n    dtype : number type, default np.float64\n        Desired dtype of output.\n    handle_unknown : 'error' (default) or 'ignore'\n        Whether to raise an error or ignore if a unknown categorical feature is\n        present during transform (default is to raise). When this is parameter\n        is set to 'ignore' and an unknown category is encountered during\n        transform, the resulting one-hot encoded columns for this feature\n        will be all zeros.\n        Ignoring unknown categories is not supported for\n        ``encoding='ordinal'``.\n    Attributes\n    ----------\n    categories_ : list of arrays\n        The categories of each feature determined during fitting. When\n        categories were specified manually, this holds the sorted categories\n        (in order corresponding with output of `transform`).\n    Examples\n    --------\n    Given a dataset with three features and two samples, we let the encoder\n    find the maximum value per feature and transform the data to a binary\n    one-hot encoding.\n    >>> from sklearn.preprocessing import CategoricalEncoder\n    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n    ... # doctest: +ELLIPSIS\n    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n              encoding='onehot', handle_unknown='ignore')\n    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()\n    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])\n    See also\n    --------\n    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n      integer ordinal features. The ``OneHotEncoder assumes`` that input\n      features take on values in the range ``[0, max(feature)]`` instead of\n      using the unique values.\n    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n      dictionary items (also handles string-valued features).\n    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n      encoding of dictionary items or strings.\n    \"\"\"\n\n    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n                 handle_unknown='error'):\n        self.encoding = encoding\n        self.categories = categories\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the CategoricalEncoder to X.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_feature]\n            The data to determine the categories of each feature.\n        Returns\n        -------\n        self\n        \"\"\"\n\n        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n                        \"or 'ordinal', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n                             \" encoding='ordinal'\")\n\n        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n        n_samples, n_features = X.shape\n\n        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n\n        for i in range(n_features):\n            le = self._label_encoders_[i]\n            Xi = X[:, i]\n            if self.categories == 'auto':\n                le.fit(Xi)\n            else:\n                valid_mask = np.in1d(Xi, self.categories[i])\n                if not np.all(valid_mask):\n                    if self.handle_unknown == 'error':\n                        diff = np.unique(Xi[~valid_mask])\n                        msg = (\"Found unknown categories {0} in column {1}\"\n                               \" during fit\".format(diff, i))\n                        raise ValueError(msg)\n                le.classes_ = np.array(np.sort(self.categories[i]))\n\n        self.categories_ = [le.classes_ for le in self._label_encoders_]\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : sparse matrix or a 2-d array\n            Transformed input.\n        \"\"\"\n        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n        n_samples, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n\n        for i in range(n_features):\n            valid_mask = np.in1d(X[:, i], self.categories_[i])\n\n            if not np.all(valid_mask):\n                if self.handle_unknown == 'error':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (\"Found unknown categories {0} in column {1}\"\n                           \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    X[:, i][~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n\n        if self.encoding == 'ordinal':\n            return X_int.astype(self.dtype, copy=False)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        indices = np.cumsum(n_values)\n\n        column_indices = (X_int + indices[:-1]).ravel()[mask]\n        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                                n_features)[mask]\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype).tocsr()\n        if self.encoding == 'onehot-dense':\n            return out.toarray()\n        else:\n            return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da24a63d65c5cf19fcbf5ec57a06e682dae2a0f6"},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# A class to select numerical or categorical columns \n# since Scikit-Learn doesn't handle DataFrames yet\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d978ce3e64e3d316bcb3e241d4dd9868240adb5"},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6f9d08d4ae5b059d138fbae9c6e48a54d2231c7"},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# Making pipelines\nnumerical_pipeline = Pipeline([\n    (\"select_numeric\", DataFrameSelector([\"age\", \"balance\", \"day\", \"campaign\", \"pdays\", \"previous\",\"duration\"])),\n    (\"std_scaler\", StandardScaler()),\n])\n\ncategorical_pipeline = Pipeline([\n    (\"select_cat\", DataFrameSelector([\"job\", \"education\", \"marital\", \"default\", \"housing\", \"loan\", \"contact\", \"month\",\n                                     \"poutcome\"])),\n    (\"cat_encoder\", CategoricalEncoder(encoding='onehot-dense'))\n])\n\nfrom sklearn.pipeline import FeatureUnion\npreprocess_pipeline = FeatureUnion(transformer_list=[\n        (\"numerical_pipeline\", numerical_pipeline),\n        (\"categorical_pipeline\", categorical_pipeline),\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c40f8e6eafdfa5c4b5d2055fa2f368cd4d07308"},"cell_type":"code","source":"X_train = preprocess_pipeline.fit_transform(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c40f8e6eafdfa5c4b5d2055fa2f368cd4d07308"},"cell_type":"code","source":"(X_train[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80bf8dc8ed2468adb524f8eb2429b9ea27b3cbab"},"cell_type":"code","source":"y_train = train_data['deposit']\ny_test = test_data['deposit']\ny_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28210aa6a40a03c9ff177bcc4f932e8f9b541cd6"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nencode = LabelEncoder()\ny_train = encode.fit_transform(y_train)\ny_test = encode.fit_transform(y_test)\ny_train_yes = (y_train == 1)\nprint(y_train)\nprint(y_train_yes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7d6a5a7613c9ee1d94aca70a7dc07eca782a63b"},"cell_type":"code","source":"some_instance = X_train[1250]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d724605d1dd77c122e6a65f2e221ad7aa26ad43"},"cell_type":"code","source":"# Time for Classification Models\nimport time\n\n\nfrom sklearn.decomposition import PCA\n#from sklearn.preprocessing import StandardScaler, LabelEncoder\n \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n\ndict_classifiers = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Nearest Neighbors\": KNeighborsClassifier(),\n    \"Linear SVM\": SVC(gamma = 'scale'),\n    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n    \"Decision Tree\": tree.DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier(n_estimators=18),\n    \"Neural Net\": MLPClassifier(alpha=1),\n    \"Naive Bayes\": GaussianNB()\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1a0b61d476d6bc5d2da3dfb27072d94dddd5585"},"cell_type":"code","source":"#  Thanks to Ahspinar for the function. \nno_classifiers= len(dict_classifiers.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1a0b61d476d6bc5d2da3dfb27072d94dddd5585"},"cell_type":"code","source":"def batch_classify(X_train, Y_train, verbose = True):\n    df_results = pd.DataFrame(data=np.zeros(shape=(no_classifiers,3)), columns = ['classifier', 'train_score', 'training_time'])\n    count = 0\n    for key, classifier in dict_classifiers.items():\n        t_start = time.clock()\n        classifier.fit(X_train, Y_train)\n        t_end = time.clock()\n        t_diff = t_end - t_start\n        train_score = classifier.score(X_train, Y_train)\n        df_results.loc[count,'classifier'] = key\n        df_results.loc[count,'train_score'] = train_score\n        df_results.loc[count,'training_time'] = t_diff\n        if verbose:\n            print(\"trained {c} in {f:.2f} s\".format(c=key, f=t_diff))\n        count+=1\n    return df_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecf17b0bf54b4624af0732ec34922aa9f9c30608"},"cell_type":"code","source":"df_results = batch_classify(X_train, y_train)\nprint(df_results.sort_values(by='train_score', ascending=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_results.sort_values(by='training_time', ascending=True))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b12c215615716b721db29b9ead5e6315be180ec2"},"cell_type":"markdown","source":"### Avoiding Overfitting:\nBrief Description of Overfitting?<br>\nThis is an error in the modeling algorithm that takes into consideration random noise in the fitting process rather than the pattern itself. You can see that this occurs when the model gets an awsome score in the training set but when we use the test set (Unknown data for the model) we get an awful score. This is likely to happen because of overfitting of the data (taking into consideration random noise in our pattern). What we want our model to do is to take the overall pattern of the data in order to correctly classify whether a potential client will suscribe to a term deposit or not. In the examples above, it is most likely that the Decision Tree Classifier and Random Forest classifiers are overfitting since they both give us nearly perfect scores (100% and 99%) accuracy scores. <br><br>\n\n\nHow can we avoid Overfitting?<br>\nThe best alternative to avoid overfitting is to use cross validation. Taking the training test and splitting it. For instance, if we split it by 3, 2/3 of the data or 66% will be used for training and 1/3 33% will be used or testing and we will do the testing process three times. This algorithm will iterate through all the training and test sets and the main purpose of this is to grab the overall pattern of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\ntemp = {}\n\nfor key ,classifier in dict_classifiers.items():\n    \n    temp[key] = (cross_val_score(classifier, X_train, y_train, cv=3).mean())\n\nresult_df =  pd.DataFrame.from_dict(temp, orient = 'index', columns = ['Mean Score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"438493015531a56fe52feb593861a4cc92c15a6d"},"cell_type":"code","source":"# All our models perform well but I will go with GradientBoosting.\nresult_df = result_df.sort_values(by=['Mean Score'], ascending=False)\nresult_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f9645627070ca75abc65db6d99a67aa822e3203"},"cell_type":"code","source":"# Cross validate our Gradient Boosting Classifier\nfrom sklearn.model_selection import cross_val_predict\n\ny_train_pred = cross_val_predict(GradientBoostingClassifier(), X_train, y_train, cv=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0984f57713acf2e96796bc388d680cd29e14c134"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint (\"Gradient Boost Classifier accuracy is %2.2f\" % accuracy_score(y_train, y_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5179f02a1c96fdecf45bf51c4d3ce86ee665da7"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n# 4697: no's, 4232: yes\nconf_matrix = confusion_matrix(y_train, y_train_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perc_conf_matrix = ((conf_matrix / conf_matrix.sum() * 100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in conf_matrix:\n    for i in c:\n        print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5179f02a1c96fdecf45bf51c4d3ce86ee665da7"},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 8))\n\nsns.heatmap(perc_conf_matrix, \n            annot=True, \n            fmt=\"f\", \n            linewidths=7, \n            ax=ax)\n\nplt.title(\"Confusion Matrix (percentages)\", fontsize=20)\n\nplt.subplots_adjust(left=0.15, right=0.99, bottom=0.15, top=0.99)\n\nax.set_yticks(np.arange(conf_matrix.shape[0]) + 0.5, minor=False)\nax.set_xticklabels(\"\")\nax.set_yticklabels(['Refused T. Deposits', 'Accepted T. Deposits'], fontsize=16, rotation=0)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b2bd7589bd4d98a22b6cceabcd1c637fdd139fc"},"cell_type":"code","source":"# Let's find the scores  for precision and recall.\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n\n# The model is 77% sure that the potential client will suscribe to a term deposit. \n# The model is only retaining 60% of clients that agree to suscribe a term deposit.\nprint('Precision Score: ', precision_score(y_train, y_train_pred))\n\n\n# The classifier only detects 60% of potential clients that will suscribe to a term deposit.\nprint('Recall Score: ', recall_score(y_train, y_train_pred))\n\nprint(\"f1 Score : \",  f1_score(y_train, y_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ee842bdb4299a2ced6e5a43fdeee3a0ccc85da7"},"cell_type":"code","source":"grad_clf = GradientBoostingClassifier()\n\ngrad_clf.fit(X_train, y_train)\n\ny_scores = grad_clf.decision_function([some_instance])\ny_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1838ee7172ea11daa91855495372367c8fa6aa3"},"cell_type":"code","source":"# Increasing the threshold decreases the recall.\nthreshold = 0\ny_some_digit_pred = (y_scores > threshold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(naives_y_scores.ndim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03bdc6f68008a4c485b6e0d8bfcb9b83786919f5"},"cell_type":"code","source":"y_scores = cross_val_predict(grad_clf, X_train, y_train, cv=3, method=\"decision_function\")\nneural_y_scores = cross_val_predict(MLPClassifier(alpha=1), X_train, y_train, cv=3, method=\"predict_proba\")\nnaives_y_scores = cross_val_predict(GaussianNB(), X_train, y_train, cv=3, method=\"predict_proba\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6a8126ddba06457326cffc0da5947541f0c1055"},"cell_type":"code","source":"neural_y_scores = neural_y_scores[:, 1]\n    \nnaives_y_scores = naives_y_scores[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c13dce9fd373ad6716bc28928097ba71c6c52a5"},"cell_type":"code","source":"# How can we decide which threshold to use? We want to return the scores instead of predictions with this code.\nfrom sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, threshold = precision_recall_curve(y_train, y_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d18db09d3cae0cdf72ff7b8d1bd6c54631713c4d"},"cell_type":"code","source":"def precision_recall_curve(precisions, recalls, thresholds):\n    \n    fig, ax = plt.subplots(figsize=(12,8))\n    \n    plt.plot(thresholds, precisions[:-1], \"r--\", label=\"Precisions\")\n    \n    plt.plot(thresholds, recalls[:-1], \"#424242\", label=\"Recalls\")\n    \n    plt.title(\"Precision and Recall \\n Tradeoff\", fontsize=18)\n    plt.ylabel(\"Level of Precision and Recall\", fontsize=16)\n    plt.xlabel(\"Thresholds\", fontsize=16)\n    plt.legend(loc=\"best\", fontsize=14)\n    plt.xlim([-2, 4.7])\n    plt.ylim([0, 1])\n    plt.axvline(x=0.13, linewidth=3, color=\"#0B3861\")\n    plt.annotate('Best Precision and \\n Recall Balance \\n is at 0.13 \\n threshold ', xy=(0.13, 0.83), xytext=(55, -40),\n             textcoords=\"offset points\",\n            arrowprops=dict(facecolor='black', shrink=0.05),\n                fontsize=12, \n                color='k')\n    \nprecision_recall_curve(precisions, recalls, threshold)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a7101621164cbe5f75d124be5013835fedda952","_cell_guid":"f07681cb-985d-476c-b7a4-0f9665240f11"},"cell_type":"markdown","source":"# ROC Curve (Receiver Operating Characteristic):\nThe **ROC curve** tells us how well our classifier is classifying between term deposit suscriptions (True Positives) and non-term deposit suscriptions. The **X-axis** is represented by False positive rates (Specificity) and the **Y-axis** is represented by the True Positive Rate (Sensitivity.) As the line moves the threshold of the classification changes giving us different values. The closer is the line to our top left corner the better is our model separating both classes.\n"},{"metadata":{"trusted":true,"_uuid":"7ecd4d6606c77ebc131379b0c3243c9ac3ecb84c"},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n# Gradient Boosting Classifier\n# Neural Classifier\n# Naives Bayes Classifier\ngrd_fpr, grd_tpr, thresold = roc_curve(y_train, y_scores)\nneu_fpr, neu_tpr, neu_threshold = roc_curve(y_train, neural_y_scores)\nnav_fpr, nav_tpr, nav_threshold = roc_curve(y_train, naives_y_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"990f2ac8ff9e21e3fcd6c105cd13bba89cda64db"},"cell_type":"code","source":"def graph_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.figure(figsize=(10,6))\n    plt.title('ROC Curve \\n Gradient Boosting Classifier', fontsize=18)\n    plt.plot(false_positive_rate, true_positive_rate, label=label)\n    plt.plot([0, 1], [0, 1], '#0C8EE0')\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('ROC Score of 91.73% \\n (Not the best score)', xy=(0.25, 0.9), xytext=(0.4, 0.85),\n            arrowprops=dict(facecolor='#F75118', shrink=0.05),\n            )\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#F75118', shrink=0.05),\n                )\n    \n    \ngraph_roc_curve(grd_fpr, grd_tpr, threshold)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd3b8c8d192d32ae5afecf0b74ee1f0b39ff529b"},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nprint('Gradient Boost Classifier Score: ', roc_auc_score(y_train, y_scores))\nprint('Neural Classifier Score: ', roc_auc_score(y_train, neural_y_scores))\nprint('Naives Bayes Classifier: ', roc_auc_score(y_train, naives_y_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad2e0880fb975987a696a1d092b5eb67cea6f378"},"cell_type":"code","source":"def graph_roc_curve_multiple(grd_fpr, grd_tpr, neu_fpr, neu_tpr, nav_fpr, nav_tpr):\n    plt.figure(figsize=(8,6))\n    plt.title('ROC Curve \\n Top 3 Classifiers', fontsize=18)\n    plt.plot(grd_fpr, grd_tpr, label='Gradient Boosting Classifier (Score = 91.72%)')\n    plt.plot(neu_fpr, neu_tpr, label='Neural Classifier (Score = 91.54%)')\n    plt.plot(nav_fpr, nav_tpr, label='Naives Bayes Classifier (Score = 80.33%)')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(grd_fpr, grd_tpr, neu_fpr, neu_tpr, nav_fpr, nav_tpr)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcc1233c7e65d9f707dd8016e07960ba538610b2"},"cell_type":"code","source":"grad_clf.predict_proba([some_instance])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7a26b34b6c2a135f7146161a662d1a11217b200"},"cell_type":"code","source":"# Let's see what does our classifier predict.\ngrad_clf.predict([some_instance]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"faa5bcf945021f81cbe60cac4824008948d07c3c"},"cell_type":"code","source":"y_train[1250]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e79649eb16c3fbe165b2b3c9594e5b5686e95b2d","_cell_guid":"02f8de28-5ea9-4f73-969c-ab6cee3c741b"},"cell_type":"markdown","source":"# Which Features Influence the Result of a Term Deposit Suscription?\n## DecisionTreeClassifier:\n<a id=\"decision\"></a>\nThe top three most important features for our classifier are **Duration (how long it took the conversation between the sales representative and the potential client), contact (number of contacts to the potential client within the same marketing campaign), month (the month of the year).\n\n\n"},{"metadata":{"trusted":true,"_uuid":"b6c6ae762fbc230e70f6aa8edbb912915a86b6e7"},"cell_type":"code","source":"from sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Convert the columns into categorical variables\nterm_deposits['job'] = term_deposits['job'].astype('category').cat.codes\nterm_deposits['marital'] = term_deposits['marital'].astype('category').cat.codes\nterm_deposits['education'] = term_deposits['education'].astype('category').cat.codes\nterm_deposits['contact'] = term_deposits['contact'].astype('category').cat.codes\nterm_deposits['poutcome'] = term_deposits['poutcome'].astype('category').cat.codes\nterm_deposits['month'] = term_deposits['month'].astype('category').cat.codes\nterm_deposits['default'] = term_deposits['default'].astype('category').cat.codes\nterm_deposits['loan'] = term_deposits['loan'].astype('category').cat.codes\nterm_deposits['housing'] = term_deposits['housing'].astype('category').cat.codes\n\nterm_deposits.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6c6ae762fbc230e70f6aa8edbb912915a86b6e7"},"cell_type":"code","source":"plt.style.use('seaborn-white')\n\n\n# Let's create new splittings like before but now we modified the data so we need to do it one more time.\n# Create train and test splits\ntarget_name = 'deposit'\nX = term_deposits.drop('deposit', axis=1)\n\n\nlabel=term_deposits[target_name]\n\nX_train, X_test, y_train, y_test = train_test_split(X,label,test_size=0.2, random_state=42, stratify=label)\n\n# Build a classification task using 3 informative features\ntree = tree.DecisionTreeClassifier(\n    class_weight='balanced',\n    min_weight_fraction_leaf = 0.01\n    \n)\n\n\n\ntree = tree.fit(X_train, y_train)\nimportances = tree.feature_importances_\nfeature_names = term_deposits.drop('deposit', axis=1).columns\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\ndef feature_importance_graph(indices, importances, feature_names):\n    plt.figure(figsize=(12,6))\n    plt.title(\"Determining Feature importances \\n with DecisionTreeClassifier\", fontsize=18)\n    plt.barh(range(len(indices)), importances[indices], color='#31B173',  align=\"center\")\n    plt.yticks(range(len(indices)), feature_names[indices], rotation='horizontal',fontsize=14)\n    plt.ylim([-1, len(indices)])\n    plt.axhline(y=1.85, xmin=0.21, xmax=0.952, color='k', linewidth=3, linestyle='--')\n    plt.text(0.30, 2.8, '46% Difference between \\n duration and contacts', color='k', fontsize=15)\n    \nfeature_importance_graph(indices, importances, feature_names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9007d33e822d9c9eb433114fbd83f6bf4171bc8e","_cell_guid":"548165af-15f7-4f98-a032-034fa2a4de03"},"cell_type":"markdown","source":"## GradientBoosting Classifier Wins!\nGradient Boosting classifier is the best model to predict whether or not a **potential client** will suscribe to a term deposit or not.  84% accuracy!"},{"metadata":{"trusted":true,"_uuid":"879e426aea63e85a90e130492b06ebb41784834f"},"cell_type":"code","source":"# Our three classifiers are grad_clf, nav_clf and neural_clf\nfrom sklearn.ensemble import VotingClassifier\n\nvoting_clf = VotingClassifier(\n    estimators=[('gbc', grad_clf), ('nav', nav_clf), ('neural', neural_clf)],\n    voting='soft'\n)\n\nvoting_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dfa86f6976cce98556d1ccca366d8e562681f7c"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nfor clf in (grad_clf, nav_clf, neural_clf, voting_clf):\n    clf.fit(X_train, y_train)\n    predict = clf.predict(X_test)\n    print(clf.__class__.__name__, accuracy_score(y_test, predict))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}