{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Subscription Predictor\nThe provided data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution.\n\nThe classification goal is to predict if the client will subscribe a term deposit (variable y).\n\n## Attribute Information:\n\n### Input variables:\n\n#### bank client data:\n1 - age (numeric)\n\n2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n\n3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n\n5 - default: has credit in default? (categorical: 'no','yes','unknown')\n\n6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n\n7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n\n8 - contact: contact communication type (categorical: 'cellular','telephone')\n\n9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n\n10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n\n11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\n12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n\n13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n\n14 - previous: number of contacts performed before this campaign and for this client (numeric)\n\n15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n\n16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n\n17 - cons.price.idx: consumer price index - monthly indicator (numeric) \n\n18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric) \n\n19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n\n20 - nr.employed: number of employees - quarterly indicator (numeric)\n\n### Output variable (desired target):\n\n21 - y - has the client subscribed a term deposit? (binary: 'yes','no')\n\n*NOTE - Data from the site was in comma-seperated csv format it was changed to excell sheet manually*"},{"metadata":{},"cell_type":"markdown","source":"## Applications of this Project  \n\n1. This Predictive Analysis of Data will help the user industry to plan there campaigns according to the previous data these predictions will help understand what the campaign should focus on and what should not to. \n\n2. It will simplyfy the methods of Approaching the customers and will also give exact customers to whom company should approch what kind of people the should keep in target will nbe known by using this project.\n\n3. It also gives you details how much time company should focus on an XYZ pperson so that he is convinced to subscribe to there plans.\n\n4. Most Imp Use Of This Project is - It will Save TIME of Campaign as well as the company as, all work will be focused on correct targets.\n\n5. Who Can Use This Project Rather Than Banks? Answers is It has no limits, any company selling products can use this as it can use there data of calls of customers and predict will he be interested in there product.\n\nSO LETS BEGIN!!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing basic but imp libraries.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading data file as 'Data' \nData = pd.read_csv('../input/bank-marketing/bank-additional-full.csv', sep = ';')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1st Problem here with this data was that it was having Yes/No\n# as its values which was needed to be converted to 1/0 for Machine Learning purpose\nData.replace(('yes','no'),(1,0),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2nd Major issue here was the Data was having 'unknown' as value instead of 'NaN'\n# Replacing 'unknown' to 'NaN'\nData.replace('unknown',np.nan,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.groupby('job').count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping \"NaN's\"\nData.dropna(inplace=True)\nData.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Understanding and Manipulating Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking on 'default' values from the Data\nsns.set_style('whitegrid')\nsns.countplot(x='default',data=Data,palette='BuGn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# as we can see here all data is 0;\n#dropping 'default' column from Data\nData.drop('default',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking 'age' Values relation with 'y'\nsns.jointplot(data=Data,x=Data['age'],y=Data['y'],kind='scatter')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# as we can see Data is focused between age>20 and age<70\n# so I added a restriction to Data for Age\n\nindexNames = Data[Data['age']<20].index\nData.drop(indexNames, inplace=True)\nindexNames = Data[Data['age']>70].index\nData.drop(indexNames, inplace=True)\nsns.jointplot(data=Data,x=Data['age'],y=Data['y'],kind='scatter')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking If Maritial Status had any effect on OutPut\nBut to check soo,we first need to convert the data to its equallent format\nas \n\nMarital had 3 classes - 'divorced','married','single'\nEncoding it as 1,2,3"},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.replace(('divorced','married','single'),(1,2,3),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(data=Data,x=Data['marital'],y=Data['y'],kind='scatter')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='marital',data=Data,palette='BuGn')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Marital colums has min effect on output so we can drop this column"},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.drop('marital',inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar to 'marital', 'job' and 'education' were also needed to be encoded \nso encoding 'education' and then 'job'"},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.replace((\"basic.4y\",\"basic.6y\",\"basic.9y\",\"high.school\",\"illiterate\",\"professional.course\",\"university.degree\"),(1,2,3,4,5,6,7),inplace=True)\nsns.jointplot(data=Data,x=Data['education'],y=Data['y'],kind='reg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.replace((\"admin.\",\"blue-collar\",\"entrepreneur\",\"housemaid\",\"management\",\"retired\",\"self-employed\",\"services\",\"student\",\"technician\",\"unemployed\"),(1,2,3,4,5,6,7,8,9,10,11),inplace=True)\nsns.jointplot(data=Data,x=Data['job'],y=Data['y'],kind='reg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking relation og 'housing' with 'y'\nsns.jointplot(data=Data,x=Data['housing'],y=Data['y'],kind='scatter')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking 'loan' column\nsns.countplot(x='loan',data=Data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(data=Data,x=Data['loan'],y=Data['y'],kind='scatter')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Method of Contact is no use for prediction so directly dropping it\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.drop('contact',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking relation of 'duration of contact' with 'y'\nsns.jointplot(data=Data,x=Data['duration'],y=Data['y'],kind='scatter')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking relation of 'campaign' with 'y'\n# campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\nsns.jointplot(data=Data,x=Data['campaign'],y=Data['y'],kind='scatter')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\nnot usefull so dropping"},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.drop('pdays',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"previous: number of contacts performed before this campaign and for this client (numeric)\nchecking with 'previous' column"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(data=Data,x=Data['previous'],y=Data['y'],kind='scatter')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It has Some Relation so we will keep it "},{"metadata":{},"cell_type":"markdown","source":"\nSimilar to 'education','job','marital', 'previous outcome' also needs  to be encoded \nso encoding it and checking relation with 'y'\n\npoutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')"},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.replace((\"failure\",\"nonexistent\",\"success\"),(1,2,3),inplace=True)\nsns.jointplot(data=Data,x=Data['poutcome'],y=Data['y'],kind='scatter')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"keeping it as it is and as well as checking relation of 'y' with -\n1. emp.var.rate: employment variation rate - quarterly indicator (numeric)\n\n2. cons.price.idx: consumer price index - monthly indicator (numeric)\n\n3. cons.conf.idx: consumer confidence index - monthly indicator (numeric)\n\n4. euribor3m: euribor 3 month rate - daily indicator (numeric)\n\n5. nr.employed: number of employees - quarterly indicator (numeric)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(data=Data,x=Data['emp.var.rate'],y=Data['y'],kind='scatter')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(data=Data,x=Data['cons.price.idx'],y=Data['y'],kind='scatter')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" sns.jointplot(data=Data,x=Data['cons.conf.idx'],y=Data['y'],kind='scatter')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" sns.jointplot(data=Data,x=Data['euribor3m'],y=Data['y'],kind='scatter')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(data=Data,x=Data['nr.employed'],y=Data['y'],kind='scatter')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping Day of week And Month of Contact Irrelevant\nData.drop('day_of_week',axis=1,inplace=True)\nData.drop('month',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now Data has been Cleaned Lets move on to Data Splitting and Model Selection\n\n# Data Splitting And Model Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Train_Test_Split Model for data Splitting\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = Data.drop('y', axis=1)\nY = Data['y']\n\nX_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.30,random_state=101)\n# test_size indicate how much portion of data to include in test dataset\n# random_state is the seed used by the random number generator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing model selection functions\nfrom sklearn import model_selection\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV # cross Validation method\nfrom sklearn.naive_bayes import GaussianNB\n\n# All Classification Algorithms Used \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n# Scaleling functions used\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Methods used for Accuracy Check \nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Trying Out All Algorithms on the data set for checking the cross validation score then picking the best algorithm for our task "},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\nresults = []\nnames = []\nn_splits = 5\n\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=n_splits, shuffle=True, \\\n                                  random_state=5)\n    cv_results = model_selection.cross_val_score(model, X_train, \\\n                                                 y_train, cv=kfold, \\\n                                                 scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %5.2f (%5.2f)\" % (name, cv_results.mean()*100, \\\n                           cv_results.std()*100)\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LR: 89.90 ( 0.27)\n\nLDA: 89.90 ( 0.25)\n\nKNN: 88.81 ( 0.20)\n\nCART: 87.81 ( 0.32)\n\nSVM: 87.44 ( 0.39)\n\nNB: 82.12 ( 0.53)\n\nAs Here Max Accuracy is achieved by Linear Dicremenent Model(LDA) & Linear Regression(LR)\n\nLets Try This again on Scaled Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df = pd.DataFrame(results, index=names,columns='CV1 CV2 CV3 CV4 CV5'.split())\nresults_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df['CV Mean'] = results_df.iloc[:,0:n_splits].mean(axis=1)\nresults_df['CV Std Dev'] = results_df.iloc[:,0:n_splits].std(axis=1)\npd.set_option('precision',2)\nresults_df*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As We Can see the Precise Accuracy of Models\n\nTrying out same with Scaled Data\n\n\n### Using StandardScaler() method \nThis Method is used to centrallize and Scale the Data Points.\n\nwhich basically helps to normalise the data within a particular range. Sometimes, it also helps in speeding up the calculations in an algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"pipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LogisticRegression())])))\npipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()),('LDA', LinearDiscriminantAnalysis())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsClassifier())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeClassifier())])))\npipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB', GaussianNB())])))\npipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVC())])))\nresults = []\nnames = []\nfor name, model in pipelines:\n    kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=5)\n    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean()*100, cv_results.std()*100)\n\nresults_df = pd.DataFrame(results, index=names, \\\n                          columns='CV1 CV2 CV3 CV4 CV5'.split())\nresults_df['CV Mean'] = results_df.iloc[:,0:n_splits].mean(axis=1)\nresults_df['CV Std Dev'] = results_df.iloc[:,0:n_splits].std(axis=1)\nresults_df.sort_values(by='CV Mean', ascending=False)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As We Can See That Scaling of data has improved accuracy of few models but LDA remains same with 89.90 +- 0.28 accuracy"},{"metadata":{},"cell_type":"markdown","source":"#### Lets Try our Luck with the Ensembels\n##### What are Ensembels ?\nEnsembels are Alogrithms that combine diverse set of learners (individual models) together to improvise on the stability and predictive power of the model.\n\nwhich means it will take the score of all small models and then collectively learn to get a single best model\n\n\nSO LETS TRY!!\n"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"ensembles = []\nensembles.append(('ScaledAB', Pipeline([('Scaler', StandardScaler()),('AB', AdaBoostClassifier())])))\nensembles.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingClassifier())])))  \nensembles.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestClassifier())])))\nensembles.append(('ScaledET', Pipeline([('Scaler', StandardScaler()),('ET', ExtraTreesClassifier())])))\nresults = []\nnames = []\nfor name, model in ensembles:\n    kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=5)\n    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean()*100, cv_results.std()*100)\n#     print(msg)\n    \nresults_df = pd.DataFrame(results, index=names, \\\n                          columns='CV1 CV2 CV3 CV4 CV5'.split())\nresults_df['CV Mean'] = results_df.iloc[:,0:n_splits].mean(axis=1)\nresults_df['CV Std Dev'] = results_df.iloc[:,0:n_splits].std(axis=1)\nresults_df.sort_values(by='CV Mean', ascending=False)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As From This Method We Have Got A Best method for our problem with Accuracy of 90.84 +- .32\n\nSo Now Scaled Gradient Boosting Method Will be Used for or Machine Learning And Prediction "},{"metadata":{},"cell_type":"markdown","source":"# Preparing And Training the Model\n#### (Gradient Boosting Classifier)\n\nThe statistical framework which use boosting as a numerical optimization problem where the objective is to minimize the loss of the model by adding weak learners using a gradient descent like procedure.\n\nThis class of algorithms were described as a stage-wise additive model. This is because one new weak learner is added at a time and existing weak learners in the model are frozen and left unchanged.\nThe generalization allowed arbitrary differentiable loss functions to be used, expanding the technique beyond binary classification problems to support regression, multi-class classification and more.\n\n##### How Gradient Boosting Works\nGradient boosting involves three elements:\n\nA loss function to be optimized.\n\nA weak learner to make predictions.\n\nAn additive model to add weak learners to minimize the loss function\n\n-----------------------------------------------------------------------------\n\nA gradient descent procedure is used to minimize the loss when adding trees.\n\nDecision trees are used as the weak learner in gradient boosting.\n\nTrees are added one at a time, and existing trees in the model are not changed."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling of Data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training of Model "},{"metadata":{"trusted":true},"cell_type":"code","source":"model = GradientBoostingClassifier(n_estimators=20, learning_rate=0.5, max_features=2, max_depth=2, random_state=0)\n# n_estimators - The number of boosting stages to perform.\n# learning_rate - learning rate shrinks the contribution of each tree by value of learning_rate provided.\n# max_features - The number of features to consider when looking for the best split.\n# max_depth - The maximum depth limits the number of nodes in the tree.\n# random_state - random_state is the seed used by the random number generator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting The Model\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction And Accuracy Of Model "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting from The Model\npredictions = model.predict(X_test)\n\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, predictions))\n\nprint(\"Classification Report :\")\nprint(classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix - \n\n[TP FP]\n\n[FN TN]\n\nTrue Positive: You predicted positive and it’s true.\n\nTrue Negative: You predicted negative and it’s true.\n\nFalse Positive: (Type 1 Error) You predicted positive and it’s false.\n\nFalse Negative: (Type 2 Error) You predicted negative and it’s false.\n\n\n### Classification Report - \nclass 0 - Not Subscribed\n\nclass 1 - Subscribed\n\nprecision - \nPrecision is the ability of a classiifer not to label an instance positive that is actually negative. For each class it is defined as as the ratio of true positives to the sum of true and false positives. Said another way, “for all instances classified positive, what percent was correct?”\n\nrecall - \nRecall is the ability of a classifier to find all positive instances. For each class it is defined as the ratio of true positives to the sum of true positives and false negatives. Said another way, “for all instances that were actually positive, what percent was classified correctly?”\n\nf1 score - \nThe F1 score is a weighted harmonic mean of precision and recall such that the best score is 1.0 and the worst is 0.0. Generally speaking, F1 scores are lower than accuracy measures as they embed precision and recall into their computation. As a rule of thumb, the weighted average of F1 should be used to compare classifier models, not global accuracy.\n\nsupport - \nSupport is the number of actual occurrences of the class in the specified dataset. Imbalanced support in the training data may indicate structural weaknesses in the reported scores of the classifier and could indicate the need for stratified sampling or rebalancing. Support doesn’t change between models but instead diagnoses the evaluation process."},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.score(X_test,y_test)\nscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Model is 90.28% Accurate and thats Not Bad!!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x=predictions,y=y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}