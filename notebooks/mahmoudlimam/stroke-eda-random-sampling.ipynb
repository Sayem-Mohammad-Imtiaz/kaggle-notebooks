{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Stroke: EDA, UMAP & Resampling**","metadata":{}},{"cell_type":"markdown","source":"**Hello and welcome**.  \n\n**This is part 2 to a 3-kernel project on Stroke Prediction.**\n\n  \n**Part 1 is Preprocessing: Data Cleaning, Target Encoding and MICE for missing values**  \nLink: **https://www.kaggle.com/mahmoudlimam/stroke-pre-processing-mice-target-encoding**\n\n  \n**Part 2 (which is this one) is EDA (including UMAP and PCA) and Random Oversampling**\n\n  \n**Part 3 is Detailed Feature extraction and Selection, and model evaluation**  \nLink: **https://www.kaggle.com/mahmoudlimam/stroke-pca-ica-lda-kmeans-dbscan-prediction** \n\nI didn't include a hyperparameter tuning section as Feature Engineering in an F1_Score of 1 with a somewhat deep Random Forest.","metadata":{}},{"cell_type":"markdown","source":"بسم الله","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.style as stl\nstl.use(\"ggplot\")\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(max_depth=10)\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv(\"../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\")\ndata.drop(\"id\",axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"impdata = pd.read_csv(\"../input/imputed-stroke-dataset/impstroke.csv\")\nimpdata.drop(\"Unnamed: 0\",axis=1,inplace=True)\nx=impdata.drop(\"stroke\",axis=1)\ny=impdata[\"stroke\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This indicates imbalance in the target variable.","metadata":{}},{"cell_type":"markdown","source":"# Exploratory Analysis","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\ncp=sns.countplot(x=data[\"stroke\"],palette=\"seismic\")\nplt.title(\"Imbalance in the Target Variable\\n\",fontsize=30)\nplt.xlabel(\"Stroke\",fontsize=15)\nplt.ylabel(\"Count\",fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SMOTE and its variants are common techniques for oversampling.  \nMany of them are quite sensitive to outliers though.  \nLet's make a quick EDA to search for outliers","metadata":{}},{"cell_type":"code","source":"continuous=[\"bmi\",\"avg_glucose_level\",\"age\"] #work type is an encoded categorical feature, not a continuous one","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,axes=plt.subplots(nrows=3,ncols=1,figsize=(12,20))\nfig.suptitle(\"Distributions of Continuous Features\",fontsize=25)\n\n\nlabel1='Mean = {}\\nMedian = {}\\nStandard Deviation = {}'.format(\"%.2f\"%data[\"bmi\"].mean(),\n                                                               data[\"bmi\"].median(),\n                                                               \"%.2f\"%data[\"bmi\"].std())\nsns.histplot(x=data[\"bmi\"],ax=axes[0],color='crimson',label=label1).legend(loc='best',fontsize=15)\naxes[0].set_title(\"BMI: Worrisome Outliers\")\naxes[0].set_xlabel(None)\n\n\nlabel2='Mean = {}\\nMedian = {}\\nStandard Deviation = {}'.format(\"%.2f\"%data[\"avg_glucose_level\"].mean(),\n                                                                \"%.2f\"%data[\"avg_glucose_level\"].median(),\n                                                                \"%.2f\"%data[\"avg_glucose_level\"].std())\nsns.histplot(x=data[\"avg_glucose_level\"],ax=axes[1],color=\"crimson\", label=label2).legend(loc='best',fontsize=15)\naxes[1].set_title(\"Average Glucose Level: Somewhat Skewed, but Nothing Awful\")\naxes[1].set_xlabel(None)\n\n\nlabel3='Mean = {}\\nMedian = {}\\nStandard Deviation = {}'.format(\"%.2f\"%data[\"age\"].mean(),\n                                                                data[\"age\"].median(),\n                                                                \"%.2f\"%data[\"age\"].std())\nsns.histplot(x=data[\"age\"],ax=axes[2],color=\"crimson\",label=label3).legend(loc='best',fontsize=15)\naxes[2].set_title(\"Age: No Outliers\")\naxes[2].set_xlabel(None)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categoricals = []\nfor col in data.drop(\"stroke\",axis=1):\n    if not(col in continuous):\n        categoricals.append(col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,axes=plt.subplots(nrows=7,ncols=1,figsize=(13,50))\nfig.suptitle(\"Distributions of Categorical Features\",fontsize=40)\ni=0\nfor col in (data.drop(\"stroke\",axis=1).columns):\n    if not(col in continuous):\n        sns.countplot(x=data[col],ax=axes[i],palette=\"cool\")\n        axes[i].set_title(col,fontsize=20)\n        axes[i].set_xlabel(None)\n        axes[i].set_ylabel(\"Count\",fontsize=15)\n        i+=1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in categoricals:\n    impdata[col] = impdata[col].astype(\"category\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"plt.figure(figsize=(20,15))\ncat_mi = pd.DataFrame(np.zeros((7,7)),columns=categoricals,index=categoricals)\nfor i in range(7):\n    for j in range(7):\n        print(data.columns[i]+\" vs \"+data.columns[j])\n        cat_mi.iloc[i,j] = mutual_info_classif(impdata[data.columns[i]].values.reshape(-1,1),impdata[data.columns[j]].values.reshape(-1,),random_state=11)\n        print(\"finished\")","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\nfrom umap import UMAP","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rb=RobustScaler()\nscaled_data = rb.fit_transform(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ump=UMAP(random_state=11,n_neighbors=5,min_dist=0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"umap_data = ump.fit_transform(scaled_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,10))\nsns.scatterplot(x=umap_data[:,0],y=umap_data[:,1],hue=y,palette=\"seismic\")\nplt.title('UMAP',fontsize=45)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Patients in the \"1\" class don't seem to be well differentiated in one separate cluster.  ","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=2)\npca_data=pca.fit_transform(scaled_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_data=pd.DataFrame(pca_data,columns=[\"PC1\",\"PC2\"])\npca_data[\"Stroke\"]=y\npca_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,8))\nsns.scatterplot(x=\"PC1\",y=\"PC2\",hue=\"Stroke\",data=pca_data, palette=\"seismic\")\nplt.title(\"Distribution across Top 2 PCA Components\",fontsize=30)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is very unusual.  \nBut these two would make nice features.  \nWe'll see about that in the feature engineering section.","metadata":{}},{"cell_type":"markdown","source":"# Random Sampling","metadata":{}},{"cell_type":"markdown","source":"## 1 - Random Oversampling","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"proportions = [0.1,0.33,0.5,0.66,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oversampled_data = {}\nfor i in proportions:\n    oversampler = RandomOverSampler(sampling_strategy=i)\n    x_ros, y_ros = oversampler.fit_resample(x, y)\n    x_ros = rb.fit_transform(x_ros)\n    oversampled_data[i] = [x_ros,y_ros]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,axes=plt.subplots(nrows=5,ncols=2,figsize=(20,35))\nfig.suptitle(\"Random Oversampling Results\\nWith Different Minority Class Proportions\",fontsize=40)\nfor i in range(5):\n    proportion = proportions[i]\n    x_ros, y_ros = oversampled_data[proportion]\n    ros_umap = ump.fit_transform(x_ros)\n    sns.scatterplot(x=ros_umap[:,0],y=ros_umap[:,1],hue=y_ros,palette=\"seismic\",ax=axes[i,0])\n    axes[i,0].set_title(f\"UMAP\\nMinority Class Proportion = {proportion}\")\n    pca_ros=pca.fit_transform(x_ros)\n    pca_ros=pd.DataFrame(pca_ros,columns=[\"PC1\",\"PC2\"])\n    pca_ros[\"Stroke\"]=y_ros\n    sns.scatterplot(x=\"PC1\",y=\"PC2\",hue=\"Stroke\",data=pca_ros, palette=\"seismic\",ax=axes[i,1])\n    axes[i,1].set_title(f\"Top 2 PCA Components\\nMinority Class Proportion = {proportion}\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training an SVM Classifier on the output of UMAP","metadata":{}},{"cell_type":"markdown","source":"That last UMAP scatterplot seems to somewhat separate the classes.  \nAn SVM classifer with an RBF kernel might do a decent job.","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nsv=SVC()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xg, yg = oversampled_data[1]\nxgumap = ump.fit_transform(xg)\nsv.fit(xgumap,yg)\nxump = ump.transform(x)\ny_pred = sv.predict(xump)\ntest_mat_ros=classification_report(y,y_pred)\nprint(f\"Gaussian SVM on UMAP output (Testing Results):\\n{test_mat_ros}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well that's quite awful.  \nDidn't expect that.  \nPerhaps it would've done much better on the training data.  \nBut no one cares about that.","metadata":{}},{"cell_type":"markdown","source":"### Model Evaluation with Random Oversampling","metadata":{}},{"cell_type":"code","source":"for i in range(5):\n    proportion = proportions[i]\n    x_ros, y_ros = oversampled_data[proportion]\n    rf.fit(x_ros,y_ros)\n    y_pred_ts=rf.predict(x)\n    test_mat_ros=classification_report(y,y_pred_ts)\n    print(\"Random Forest Results with Random Oversampling:\")\n    print(\"Proportion = {}\\n{}\\n\\n\".format(proportion,test_mat_ros))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow, this is funny.  \nApparently, the algorithm is now classifying pretty much all the minority-class samples correctly, and all the majority-class samples incorrectly.  \nQuite amusing, but totally awful.","metadata":{}},{"cell_type":"markdown","source":"## 2 - Random Undersampling","metadata":{}},{"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"undersampled_data = {}\nfor i in proportions:\n    undersampler = RandomUnderSampler(sampling_strategy=i)\n    x_rus, y_rus = undersampler.fit_resample(x, y)\n    x_rus = rb.fit_transform(x_rus)\n    undersampled_data[i] = [x_rus,y_rus]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,axes=plt.subplots(nrows=5,ncols=2,figsize=(20,35))\nfig.suptitle(\"Random Undersampling Results\\nWith Different Minority Class Proportions\",fontsize=40)\nfor i in range(5):\n    proportion = proportions[i]\n    x_rus, y_rus = undersampled_data[proportion]\n    rus_umap = ump.fit_transform(x_rus)\n    sns.scatterplot(x=rus_umap[:,0],y=rus_umap[:,1],hue=y_rus,palette=\"seismic\",ax=axes[i,0])\n    axes[i,0].set_title(f\"UMAP\\nProportion = {proportion}\")\n    pca_rus=pca.fit_transform(x_rus)\n    pca_rus=pd.DataFrame(pca_rus,columns=[\"PC1\",\"PC2\"])\n    pca_rus[\"Stroke\"]=y_rus\n    sns.scatterplot(x=\"PC1\",y=\"PC2\",hue=\"Stroke\",data=pca_rus, palette=\"seismic\",ax=axes[i,1])\n    axes[i,1].set_title(f\"Top 2 PCA Components\\nProportion = {proportion}\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Evaluation with Random Undersampling","metadata":{}},{"cell_type":"code","source":"for i in range(5):\n    proportion = proportions[i]\n    x_rus, y_rus = undersampled_data[proportion]\n    rf.fit(x_rus,y_rus)\n    y_pred_ts=rf.predict(x)\n    test_mat_ros=classification_report(y,y_pred_ts)\n    print(\"Random Forest Results with Random Undersampling:\")\n    print(\"Proportion = {}\\n{}\\n\\n\".format(proportion,test_mat_ros))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Question: Would other resampling techniques give better results?","metadata":{}},{"cell_type":"markdown","source":"My guess is: **probably not**. At least not considerably, and at least not with this data.  \nI say this because the minority-class samples are sparsely distributed throughout the majority-class distribution ie **the two classes are not well distinguished/separated**.   \n\nI actually went on and further experimented with SMOTE and as expected; no improvement.","metadata":{}},{"cell_type":"markdown","source":"**\"What to do now?\" I hear you ask, curious viewer.**  \nWell, we just try to separate them.  \nWith some ***Feature Engineering***  .    \n**Adding** more informative **features** (ones that reduce entropy in the target feature) would probably separate the two classes, at least a bit more.  \nThis can be found in the next episode here: https://www.kaggle.com/mahmoudlimam/stroke-pca-ica-lda-kmeans-dbscan-prediction  \nMake sure you see it, it's the last episode xD","metadata":{}},{"cell_type":"markdown","source":"الحمد لله الذي بنعمته تتم الصالحات","metadata":{}}]}