{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Hello community, my name is Tushar & this is my first of the two notebooks on Kaggle. I am a newbie in the world of ML & AI.\n\n# I am performing basic EDA & training KNN , Naive Bayes & Logistic Regression. \n\n# These are the only algorithms I have learned so far.\n\n# It would be great if you would let me know where did I go wrong, what did I miss & what I could have done better.\n\n# Thank You! :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport seaborn as sns\n\n \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2. Import data 2C_weka.csv for 2 Class Classification.\n\nmissing_value_formats = [\"n.a.\",\"?\",\"NA\",\"n/a\",\"na\",\"--\",\" \", \"  \"]\nTwoC_weka_data = pd.read_csv('../input/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv', na_values = missing_value_formats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Exploring the dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"TwoC_weka_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*There are 6 columns. 5 are of type numerical & 1 categorical*"},{"metadata":{"trusted":true},"cell_type":"code","source":"TwoC_weka_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Except \"degree_spondylolisthesis\", rest all of the columns have distribution close to normal. degree_spondylolisthesis seems to be right-tailed or positively skewed.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"TwoC_weka_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*The 2C_weka has 310 rows & 7 columns*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking top 5 rows\nTwoC_weka_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking last 5 rows\nTwoC_weka_data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for skewness\nTwoC_weka_data.skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*As mentioned above \"degree_spondylolisthesis\" is positively skewed.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for null values and duplicate data\nTwoC_weka_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TwoC_weka_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TwoC_weka_data.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*There are no null values and there is no duplicate data.*"},{"metadata":{},"cell_type":"markdown","source":"**Univariate Analysis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for unique values in target variable \"class\"\nprint(TwoC_weka_data['class'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Count of each class*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Method 1\npd.crosstab(TwoC_weka_data['class'],columns='Count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Method 2\nprint(TwoC_weka_data['class'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Method 3\nsns.countplot(x='class',data=TwoC_weka_data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Records for Abnormal class are more than Normal class*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identifying Type Of Features \n# Numerical Features & Categorical Features\n\nnumerical_features = TwoC_weka_data.select_dtypes(include = [np.number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(numerical_features.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we want to segregate discrete variables from continuous variables\n# So, we count the number of unique values in each feature. If count of unique values is less than 25 then we consider it as\n# discrete variable otherwise it is a continuous variable\n\ncontinuous_numerical_features = []\ndiscrete_numerical_features = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in numerical_features:\n    if(len(TwoC_weka_data[feature].unique())>25):\n        continuous_numerical_features.append(feature)\n        print('continuous_numerical_features ',feature)   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*This shows that all the independent features are continuous*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing Distribution For Numerical Columns\n# I wanted to use \"displot\" instead of \"distplot\" as \"distplot\" is going to be deprecated. See below link \n# https://seaborn.pydata.org/generated/seaborn.distplot.html?highlight=distplot#seaborn.distplot\n# But I couldn't use \"displot\" as using it threw error \"module 'seaborn' has no attribute 'displot'\"\n\nfor feature in numerical_features.columns:\n    sns.distplot(numerical_features[feature],kde=True)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Features \"pelvic_tilt\" & \"pelvic_radius\"  are very close to normal*. \n*Features \"pelvic_incidence\", \"lumbar_lordosis_angle\" & \"sacral_slope\" have some kind of uniform distribution.*\n*Feature degree_spondylolisthesis is highly positively skewed.*"},{"metadata":{},"cell_type":"markdown","source":"*Now looking for IQR & Outliers*"},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in numerical_features.columns:\n    sns.boxplot(TwoC_weka_data[feature])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*There are outliers in all of the features. Features \"lumbar_lordosis_angle\" & \"sacral_slope\" have just one outlier.\nRest all have many outlies*"},{"metadata":{},"cell_type":"markdown","source":"**Bivariate Analysis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting Correlation HeatMap\n\nplt.figure(figsize=(5,5))\nsns.heatmap(TwoC_weka_data.corr(),annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like there is some multicolinearity here. \nFor example: Feature \"pelvic_incidence\" seems to be correlated with all the other features except \"pelvic_radius\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting Barplot. Showing the numbers\n\nfor feature in numerical_features.columns:\n    sns.barplot(x='class',y=feature,data=TwoC_weka_data)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Class \"Abnormal\" has more count as compare to Normal for alsmot all of the features*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting swarmplot also\n\nfor feature in numerical_features.columns:\n    sns.swarmplot(TwoC_weka_data['class'],TwoC_weka_data[feature])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Swarmplot shows distributon of each feature for both the classes.* \n\n*For pelvic_incidence, majority of abnormal class points clustered between 40 & 95*\n\n*For pelvic_tilt numeric, majority of abnormal class points clustered between 10 & 25*\n\n*For lumbar_lordosis_angle, majority of abnormal class points clustered between 35 & 80*\n\n*For sacral_slope, majority of abnormal class points clustered between 40 & 60*\n\n*For pelvic_radius, majority of abnormal class points clustered between 100 & 130*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting pairplot\nsns.pairplot(TwoC_weka_data,size=3,hue='class')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*There seems to be good separation between normal and abnormal for all the features except \n\"degree_spondylolisthesis\" as it is highly skewed.* \n\n*Good separation indicates that, that particular feature could be a good indicator.* \n\n*There seems like the independent features have some kind of relation.* \n\n*For example: Feature 'pelvic_incidence' seems linearly related to 'pelvic_tilt_numeric' & 'lumbar_lordosis_angle' & 'sacral_slope'*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Power Transformation is done to make data normal.\n# Creating a dataset of numerical features only for transformation\n\nnumerical_dataset = TwoC_weka_data.iloc[:,:6]\nnumerical_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"power = PowerTransformer(method='yeo-johnson', standardize=True)\nTwoC_weka_data_transformed = power.fit_transform(numerical_dataset)\nTwoC_weka_data_transformed = pd.DataFrame(TwoC_weka_data_transformed,columns = numerical_dataset.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TwoC_weka_data_transformed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution after transformation. \n# I have plotted both the original & transformed distribution for comparison.\n# We can observe that features have been transformed into Normal distribution\n# Transformation is done as models perform better for Gaussian or Gaussian like distribution\n\nfor feature in TwoC_weka_data_transformed.columns:\n    #print(\"Original \", feature)\n    sns.distplot(numerical_features[feature],kde=True)\n    plt.show()\n    #print(\"             Transformed \", feature)\n    sns.distplot(TwoC_weka_data_transformed[feature],kde=True)\n    plt.show()  \n    #print(\"-----------------------------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, this feature \"degree_spondylolisthesis\" shown above is not fully normal. It has two peaks. At this stage of this course, what I know is we separate these two peaks then we move forward. But currently that is beyond the scope of my knowledge. So, I will keep this as it is."},{"metadata":{},"cell_type":"markdown","source":"# Create the X(Feature-set) and Y(Target-set) sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keeping X in uppercase & y in lowercase as per standard convention\n\nX = TwoC_weka_data_transformed\ny = TwoC_weka_data.iloc[:,6:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Training KNN**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First Splitting the data set into train & test data set so that while scaling or normalizing, test data should not affect train data\n# Second, different random states can give different results. So we need to test for multiple random states\n# Third, for every random state, different value of k can give different results. So, we need to test for multiple values of k\n# for each of the random state\n\n# The \"fit\" method gives mean and standard deviation.\n# So we do \"fit\" the model using train data and then \"transform\" or apply that mean & std on test data.\n\n# Scaling  or Normalization should be done separately on train data & test data.\n# This is done to scale or normalize all the variable with different scales so that all these variable become comparable.\n# We check for multiple random state & for each random state, we check for multiple K values\n# This is how we can come to a conclusion which random state and value of K is to be chosen\n\nran_state = np.arange(1,50)\nneighbours = np.arange(5,41) \n# I know that it is better to keep K-Value odd to have clear majority but I am not keeping it because I tried and I am getting much \n# better result with even numbers.\n\n \ntest_accuracy_list = []\ntrain_accuracy_list = []\ndesired_k_value_list = []\ndesired_random_state_list = []\nconf_matrix_report_list = []\nclass_report_list = []\n \n\nfor r_state in ran_state:\n     \n    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=r_state)\n    \n    std_scaler = StandardScaler() \n     \n    std_scaler.fit(X_train)     \n    X_train_scaled = std_scaler.transform(X_train)\n    X_test_scaled  = std_scaler.transform(X_test)\n    \n    for k_value in neighbours:\n        # For metric='minkowski' p=2 means using Euclidean distance &  p=1 means Manhattan distance \n        KNN = KNeighborsClassifier(n_neighbors=k_value,metric='minkowski',algorithm='auto',p=2) \n        \n        KNN.fit(X_train_scaled,y_train)\n        y_pred = KNN.predict(X_test_scaled)    \n        \n        conf_matrix = metrics.confusion_matrix(y_test,y_pred)\n        class_report = metrics.classification_report(y_test,y_pred)\n        train_score = np.round(KNN.score(X_train_scaled,y_train),2)\n        test_score = np.round(KNN.score(X_test_scaled,y_test),2)\n        test_accuracy_list.append(test_score)\n        train_accuracy_list.append(train_score)\n        desired_k_value_list.append(k_value)\n        desired_random_state_list.append(r_state)\n        conf_matrix_report_list.append(conf_matrix)\n        class_report_list.append(class_report)\n\n        \ntest_accuracy_array = np.array(test_accuracy_list)\nresult = np.where(test_accuracy_array>0.86)\nresult = result[0]\n\n     # If a patient is predicted Normal when he is Abnormal, then this prediction is bad. Patient is having medical issue but model \n     # predicted patient does not have any issue. \n     # We definitely need to minimize this (False Negative) as much as possible. So, I have chosen conf_matrix[1,0]<5 \n     # for the this reason.\n    \nfor r in result:  \n    conf = conf_matrix_report_list[r]\n    if(conf[1,0]<5):\n        print('Test Accuracy',test_accuracy_list[r],'Train Accuracy',train_accuracy_list[r],'K Value ' ,desired_k_value_list[r],'Random State ',desired_random_state_list[r])\n        print()\n        print(\"Confusion Matrix \")\n        print(conf_matrix_report_list[r])\n        print()\n        print(\"Classification Report \")\n        print(class_report_list[r])\n        print(\"--------------------------------------------------------\")\n\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**What is the best K value for your classification?**\n\n*Here, as per my understanding, keeping False Negative as much low as possible should be on priority keeping test accuracy high so, Random State 3 & K - Value = 13 gives us overall test accuracy 87, precision for Abnormal class 95 & false negative = 3 which is the minimum we got for KNN.* \n"},{"metadata":{},"cell_type":"markdown","source":"Training Gaussian Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"ran_state = np.arange(1,50)\n\ntest_accuracy_list = []\ntrain_accuracy_list = []\ndesired_random_state_list = []\nconf_matrix_report_list = []\nclass_report_list = []\n\nfor r_state in ran_state:\n    GNB_X_train,GNB_X_test,GNB_y_train,GNB_y_test = train_test_split(X,y,test_size=0.3,random_state=r_state)\n    \n    gnb = GaussianNB()\n    gnb.fit(GNB_X_train,GNB_y_train)\n    GNB_y_pred = gnb.predict(GNB_X_test)\n    \n    conf_matrix = metrics.confusion_matrix(GNB_y_test,GNB_y_pred)\n    class_report = metrics.classification_report(GNB_y_test,GNB_y_pred)\n    test_score = np.round(gnb.score(GNB_X_test,GNB_y_test),2)\n    train_score = np.round(gnb.score(GNB_X_train,GNB_y_train),2)\n    test_accuracy_list.append(test_score)\n    train_accuracy_list.append(train_score)   \n    desired_random_state_list.append(r_state)\n    conf_matrix_report_list.append(conf_matrix)\n    class_report_list.append(class_report)\n     \n        \ntest_accuracy_array = np.array(test_accuracy_list)\nresult = np.where(test_accuracy_array>0.80)\nresult = result[0]\n\n\nfor r in result:\n    conf = conf_matrix_report_list[r]\n    if(conf[1,0]<5):\n        print('Test Accuracy',test_accuracy_list[r],'Train Accuracy',train_accuracy_list[r],'Random State ',desired_random_state_list[r])\n        print()\n        print(\"Confusion Matrix \")\n        print(conf_matrix_report_list[r])\n        print()\n        print(\"Classification Report \")\n        print(class_report_list[r])\n        print(\"--------------------------------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, trying to keep False Negative as much low as possible should be on priority keeping test accuracy high so, Random State 3 \ngives us overall test accuracy 81, precision for Abnormal class 96 & false negative = 2 which is the minimum for Naive Bayes. "},{"metadata":{},"cell_type":"markdown","source":"Training Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"ran_state = np.arange(1,50)\n\ntest_accuracy_list = []\ntrain_accuracy_list = []\ndesired_random_state_list = []\nconf_matrix_report_list = []\nclass_report_list = []\n\n\nfor r_state in ran_state:\n    \n    LR_X_train,LR_X_test,LR_y_train,LR_y_test = train_test_split(X,y,test_size=0.3,random_state=r_state)\n    \n    logistic_regression = LogisticRegression()\n    logistic_regression.fit(LR_X_train,LR_y_train)\n    LR_y_predict = logistic_regression.predict(LR_X_test)\n    \n    conf_matrix = metrics.confusion_matrix(LR_y_test,LR_y_predict)\n    class_report = metrics.classification_report(LR_y_test,LR_y_predict)\n    test_score = np.round(logistic_regression.score(LR_X_test,LR_y_test),2)\n    train_score = np.round(logistic_regression.score(LR_X_train,LR_y_train),2)\n    test_accuracy_list.append(test_score)\n    train_accuracy_list.append(train_score)   \n    desired_random_state_list.append(r_state)\n    conf_matrix_report_list.append(conf_matrix)\n    class_report_list.append(class_report)\n    \n        \ntest_accuracy_array = np.array(test_accuracy_list)\nresult = np.where(test_accuracy_array>0.80)\nresult = result[0]\n\n\n             \nfor r in result:\n    conf = conf_matrix_report_list[r]\n    if(conf[1,0]<5):\n        print('Test Accuracy',test_accuracy_list[r],'Train Accuracy',train_accuracy_list[r],'Random State ',desired_random_state_list[r])\n        print()\n        print(\"Confusion Matrix \")\n        print(conf_matrix_report_list[r])\n        print()\n        print(\"Classification Report \")\n        print(class_report_list[r])\n        print(\"--------------------------------------------------------\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, trying to keep False Negative as much low as possible should be on priority keeping test accuracy high so, Random State 47 gives us overall test accuracy 84, precision for Abnormal class 93 & false negative = 4 which is the minimum for Logistic Regression. "},{"metadata":{},"cell_type":"markdown","source":"# Thank You!!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}