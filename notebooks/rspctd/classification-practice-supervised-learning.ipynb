{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['class'].value_counts()\nabnormal=data[data['class']=='Abnormal']\nnormal=data[data['class']=='Normal']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(data.corr(),annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(abnormal['pelvic_incidence'],abnormal['pelvic_radius'],c='r',label='abnormal',alpha=0.5)\nplt.scatter(normal['pelvic_incidence'],normal['pelvic_radius'],c='g',label='normal',alpha=0.6)\nplt.xlabel('pelvic incidence')\nplt.ylabel('pelvic_radius')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LOGISTIC REGRESSION CLASSIFICATION\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['class']=[0 if each=='Abnormal'else 1 for each in data['class']]\ny=data['class'].values\nx_data=data.drop(['class'],axis=1)\nx=(x_data-np.min(x_data)) / (np.max(x_data)-np.min(x_data)) # normalization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr.fit(x_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy of Logistic Regression Classification: ',lr.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train-test-split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#knn model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=26) #k_value\nknn.fit(x_train,y_train)\npredict=knn.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('{}nn score:  {}'.format(2,knn.score(x_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Which k value(n_neighbors) is the best for our model?\n* k value is a hyperparameter which means we need to tune manually.\n* In order to see it better, we can visualize it."},{"metadata":{"trusted":true},"cell_type":"code","source":"score=[]\n\nfor each in range(1,50):\n    knn2=knn=KNeighborsClassifier(n_neighbors=each)\n    knn2.fit(x_train,y_train)\n    knn2.score(x_test,y_test)\n    \n    score.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,50),score,c='b')\nplt.xlabel('k_value')\nplt.ylabel('Accuracy')\nplt.show()\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion:\n\n1. We get the best results when n_neighbors is given 22,26.\n1. Our model predicts correct results %81 of a chance"},{"metadata":{},"cell_type":"markdown","source":"# SVM(Support Vector Machine)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train1,x_test1,y_train1,y_test1=train_test_split(x,y,test_size=0.2,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvm=SVC(random_state=1)\nsvm.fit(x_train1,y_train1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('SVM prediction score :',svm.score(x_test1,y_test1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# NAIVE BAYES CLASSIFICATION\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train2,x_test2,y_train2,y_test2=train_test_split(x,y,test_size=0.3,random_state=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb=GaussianNB()\nnb.fit(x_train2,y_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy of Naive Bayes Algorithm: ',nb.score(x_test2,y_test2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DECISION TREE CLASSIFICATION\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train3,x_test3,y_train3,y_test3=train_test_split(x,y,test_size=0.2,random_state=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtc=DecisionTreeClassifier(random_state=1)\ndtc.fit(x_train3,y_train3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy of Decision Tree Regression: ', dtc.score(x_test3,y_test3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RANDOM FOREST CLASSIFICATION\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train4,x_test4,y_train4,y_test4=train_test_split(x,y,test_size=0.2,random_state=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier(n_estimators=100,random_state=1) # n_estimators bizim modelimizde kac tane tree olmasini istiyorsak ona gore secmeliyiz. Sectigimiz bu tree lerinde her defasinda aynilarinin olmasi icin random_state=1 yapiyoruz.\nrfc.fit(x_train4,y_train4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy of Random Forest Classification : ',rfc.score(x_test4,y_test4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# CONFUSION MATRIX"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train5,x_test5,y_train5,y_test5=train_test_split(x,y,test_size=0.2,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Our accuracy for random forest classification: 0.838\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_head=rfc.predict(x_test5)  # fit edilen modelimize, x_test verilerini koyarak modelimizin tahminlerini buluyoruz(random forest icin bunun accuracy sini yukarida bulduk: 0.838)\ny_true=y_test5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_true,y_head)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### * Confusion Matrix ile buldugumuz degerleri SEABORN kutuphanesinin HEATMAP plotu ile gorsellestirebiliriz.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax= plt.subplots(figsize=(7,7))\nsns.heatmap(cm,annot=True,fmt='.0f')\nplt.xlabel('y_predicted')\nplt.ylabel('y_true')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* TN = True Negative = 39  ---> *  Toplamda 42 tane 'abnormal' olan label larimin 39 tanesini dogru tahmin edip,3 tanesini 'Normal' olarak tahmin etmisim\n* TP = True Positive = 13  ---> *  Toplamda 20 tane 'normal' olan label larimin 13 tanesini dogru tahmin edip,7 tanesini 'Abnormal' olarak tahmin etmisim\n* FP = False Positive (TYPE 1 ERROR) = 3  ---> \n* FN = False Negative (TYPE 2 ERROR) = 7  --->"},{"metadata":{},"cell_type":"markdown","source":"# Conslusion:\n\n1. Accuracy of Logistic Regression Classification:      0.7741935483870968\n1. knn score:      0.8172043010752689\n1. SVM prediction score :     0.8225806451612904\n1. Accuracy of Naive Bayes Algorithm:      0.8172043010752689\n1. Accuracy of Decision Tree Regression:      0.8064516129032258\n1. Accuracy of Random Forest Classification :      0.8387096774193549\n\n\n* We have practiced Confusion Matrix on only Random Forest Classification. We can do the same process with the other classification models that has been listed with their accuracy above.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}