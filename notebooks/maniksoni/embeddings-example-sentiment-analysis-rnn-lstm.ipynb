{"cells":[{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def plot_model_output(history,epochs):\n    plt.figure()\n    plt.plot(range(epochs,),history.history['loss'],label = 'training_loss')\n    plt.plot(range(epochs,),history.history['val_loss'],label = 'validation_loss')\n    plt.legend()\n    plt.figure()\n    plt.plot(range(epochs,),history.history['acc'],label = 'training_accuracy')\n    plt.plot(range(epochs,),history.history['val_acc'],label = 'validation_accuracy')\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Flatten, Embedding, LSTM\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.datasets import imdb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/imdb-review-dataset/imdb_master.csv',encoding='ISO-8859-1')\nprint(data.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"sns.countplot(x='label',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data = data[data.label!='unsup']\nsns.countplot(x='label',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data['out'] = data['label'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data['out'][data.out=='neg']=0\ndata['out'][data.out=='pos']=1\n# Another way data['out'] = data['out'].map({1:'pos',0:'neg'})\nnp.unique(data.out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"sns.countplot(y='out',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"req_data = data[['review','out']]\nreq_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Tokenize","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"texts = np.array(req_data.review)\nlabels = np.array(req_data.out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"print(texts.shape, labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# num_words: Top No. of words to be tokenized. Rest will be marked as unknown or ignored.\ntokenizer = Tokenizer(num_words=20000) \n# tokenizing based on \"texts\". This step generates the word_index and map each word to an integer other than 0.\ntokenizer.fit_on_texts(texts)\n\n# generating sequence based on tokenizer's word_index. Each sentence will now be represented by combination of numericals\n# Example: \"Good movie\" may be represented by [22, 37]\nseq = tokenizer.texts_to_sequences(texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# padding each numerical representation of sentence to have fixed length.\npadded_seq = np.array(pad_sequences(seq,maxlen=100))\n\n#word_index of each token\nword_index = tokenizer.word_index\n\n# for shuffling\nindices = np.arange(padded_seq.shape[0])\nnp.random.shuffle(indices)\n\n# texts = texts[indices]\n# labels = labels[indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# this is how word_index looks like. It's a dict where each word has its own unique key \n# with which the word is represented in sequence.\n\nprint(len(word_index))\nprint(padded_seq.shape)\nprint()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Glove 100d Parsing","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# GloVe embedding matrix. This is like a pre-trained model. In an embedding matrix, each word is represented by a dense vector.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"vec_representations = {}\nf = open('/kaggle/input/glove6b/glove.6B.100d.txt','r')\n\nsample = True\n\nfor line in f:\n    if sample:\n        print(\"Sample weight of word: \")\n        print(line)\n        sample=False\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:],dtype = 'float32')\n    vec_representations[word] = coefs\n    \nf.close()\n\nprint(f'Found {len(vec_representations)} words')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"max_words = 20000\nembedding_dim = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"embedding_matrix = np.zeros((max_words,embedding_dim))\nfor word,i in word_index.items():\n    vec_representation = vec_representations.get(word)\n    if vec_representation is not None:\n        embedding_matrix[i]=vec_representation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"model = Sequential()\nembedding = Embedding(max_words,embedding_dim,input_length = 100,name='embedding')\nmodel.add(embedding)\n#model.add(LSTM(32, return_sequences = True))\nmodel.add(Flatten())\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# model.layers[0].set_weights=[(embedding_matrix)]\n# model.layers[0].trainable=True\nmodel.layers[0].get_weights()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"np.asarray(labels).astype(np.uint8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"model.compile(optimizer='adagrad',loss='binary_crossentropy',metrics=['acc'])\n\n# Change the epochs to 10 here.\nhistory = model.fit(padded_seq,np.asarray(labels).astype(np.uint8),epochs=10,validation_split=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"plot_model_output(history, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# RNN in Numpy","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"timesteps = 10000 # No. of timesteps in input_sequence\ninput_features = 32 # Dimensionality of input_feature space\noutput_features = 64 # Dimensionality of output_features space\n\ninputs = np.random.random((timesteps,input_features)) #random data for example 10000 X 32\n\nstate_t = np.zeros((output_features,)) # (64,)\n\nW = np.random.random((output_features,input_features)) # Weight matrix for current_state (64 X 32)\nU = np.random.random((output_features,output_features)) #Weight matrix for previous state (64 X 64)\n\nb = np.random.random((output_features,)) # bias to be added to output of each state (64,)\n\nsuccessive_outputs = [] # output of each timestep\n\nfor input_t in inputs:\n    # input_t is a vector of shape (input_features,)\n    output_t = np.tanh(np.dot(W,input_t)+np.dot(U,state_t) + b)\n    successive_outputs.append(output_t)\n    state_t = output_t\n\nfinal_outputs = np.concatenate(successive_outputs,axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"final_outputs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simple RNN","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,SimpleRNN,Dense, Flatten\nfrom keras.datasets import imdb\nfrom keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(10000,32)) # 10K is no of unique words and 32 is shape of each word's representation\nmodel.add(SimpleRNN(units=32,return_sequences = True)) #units is dimensionality of output space. https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN\nmodel.add(SimpleRNN(units=32,return_sequences = True)) \nmodel.add(SimpleRNN(units=32,return_sequences = True)) \nseq = model.add(SimpleRNN(units=32,return_sequences = True)) \nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using IMDB data with above RNN","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"max_features = 10000 # max num of words. others will be marked as unknown\nmaxlen = 500 # maximum length of each review\nbatch_size = 32\n(x_train,y_train),(x_test,y_test) = imdb.load_data(num_words=max_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"x_train = pad_sequences(x_train, maxlen=maxlen)\nx_test = pad_sequences(x_test, maxlen=maxlen)\nindices = list(range(len(x_train)))\nnp.random.shuffle(indices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"x_train = x_train[indices]\ny_train = y_train[indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_features,32)) # here 32 is no of bits with which a sentence with maxlen(500 set above) will be represented.\nmodel.add(SimpleRNN(32)) # 32 is output space dimensions\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy',metrics=['acc'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"history = model.fit(x_train,\n          y_train,\n         epochs=4,\n         batch_size = batch_size,\n         validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.figure()\nplt.plot(range(4,), history.history['acc'],'g',label = 'training acc')\nplt.plot(range(4,), history.history['val_acc'],'*',label = 'val_acc')\nplt.title(\"Training and validation acc\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.figure()\nplt.plot(range(4,),history.history['loss'],label = 'training_loss')\nplt.plot(range(4,),history.history['val_loss'],label = 'validation_loss')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sentiment Analysis with LSTMs","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,SimpleRNN,Dense, Flatten, LSTM\nfrom keras.datasets import imdb\nfrom keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"max_features = 20000 # max num of words. others will be marked as unknown\nmaxlen = 1000 # maximum length of each review\nbatch_size = 64\n(x_train,y_train),(x_test,y_test) = imdb.load_data(num_words=max_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"x_train = pad_sequences(x_train, maxlen=maxlen)\nx_test = pad_sequences(x_test, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_features,64)) # here 64 is no of bits with which a sentence with maxlen(500 set above) will be represented.\nmodel.add(LSTM(64,dropout=0.3))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['acc'])\nhistory = model.fit(x_train,y_train,\n         validation_split = 0.2,\n         epochs = 1,\n         batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plot_model_output(history,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"model.layers[-2].get_weights()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}