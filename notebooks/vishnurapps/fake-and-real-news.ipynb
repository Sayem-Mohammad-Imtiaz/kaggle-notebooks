{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Importing libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport random\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport pickle\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom scipy.sparse import hstack\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"true_data = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')\ntrue_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake_data = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv')\nfake_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Peeping into data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"random_true_news = random.randint(0,true_data.shape[0])\nrandom_fake_news = random.randint(0,fake_data.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_data['title'][random_true_news]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_data['text'][random_true_news]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake_data['title'][random_fake_news]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake_data['text'][random_fake_news]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can do some preprocessing to do some text cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fake_data['target'] = 'fake'\ntrue_data['target'] = 'true'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news = pd.concat([fake_data, true_data]).reset_index(drop = True)\nnews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/a/47091490/4084039\n'''Function to expand commonly occuring test'''\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://gist.github.com/sebleier/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combining all the above stundents \ndef preprocessTextData(dataToProcess):\n    \"\"\"This function do the preprocessing of the column text data in essay and title\"\"\"\n    processedData = []\n    # tqdm is for printing the status bar\n    for sentance in tqdm(dataToProcess):\n        lowersent = sentance.lower()\n        sent = decontracted(lowersent)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        # https://gist.github.com/sebleier/554280\n        sent = ' '.join(e for e in sent.split() if e not in stopwords)\n        processedData.append(sent.strip())\n    return processedData","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news['processed title'] = preprocessTextData(news['title'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news['processed text'] = preprocessTextData(news['text'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news['title'][random_fake_news]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news['processed title'][random_fake_news]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news['text'][random_fake_news]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news['processed text'][random_fake_news]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Spliting to train and test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(news[['processed title', 'processed text', 'subject']], news.target, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handling the catagorical data `subject`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting categorical data\ndef fitCatogarizedData(dataToProcess, vocab = None):\n    if vocab is None :\n        vectorizer = CountVectorizer()\n    else:\n        vectorizer = CountVectorizer(vocabulary=vocab, lowercase=False, binary=True)\n    vectorizer.fit(dataToProcess)\n    return vectorizer\n\n#transforming categorical data\ndef transformCatogarizedData(dataToProcess, vectorizer):\n    categories_one_hot = vectorizer.fit_transform(dataToProcess)\n    print(vectorizer.get_feature_names())\n    print(\"Shape of matrix after one hot encodig \",categories_one_hot.shape)\n    return categories_one_hot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_vector = fitCatogarizedData(x_train['subject'].values)\nx_train_cat = transformCatogarizedData(x_train['subject'].values, train_vector)\nx_test_cat = transformCatogarizedData(x_test['subject'].values, train_vector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stronging variables into pickle files python: http://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\n# make sure you have the glove_vectors file\nglove_vectors = '/kaggle/input/donors-chose/glove_vectors'\nwith open(glove_vectors, 'rb') as f:\n    model = pickle.load(f)\n    glove_words =  set(model.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# average Word2Vec\n# compute average word2vec for each review.\ndef fitAvgW2V(dataToProcess):\n    avg_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n    for sentence in tqdm(dataToProcess): # for each review/sentence\n        vector = np.zeros(300) # as word vectors are of zero length\n        cnt_words =0; # num of words with a valid vector in the sentence/review\n        for word in sentence.split(): # for each word in a review/sentence\n            if word in glove_words:\n                vector += model[word]\n                cnt_words += 1\n        if cnt_words != 0:\n            vector /= cnt_words\n        avg_w2v_vectors.append(vector)\n\n    print(len(avg_w2v_vectors))\n    print(len(avg_w2v_vectors[0]))\n    return avg_w2v_vectors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating avgw2v essay and title vectors\navgw2v_title_train = fitAvgW2V(x_train['processed title'])\navgw2v_text_train = fitAvgW2V(x_train['processed text'])\navgw2v_title_test = fitAvgW2V(x_test['processed title'])\navgw2v_text_test = fitAvgW2V(x_test['processed text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.drop(['subject'], axis = 1, inplace=True)\nx_test.drop(['subject'], axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to do batch prediction\ndef batch_predict(clf, data):\n    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n    # not the predicted outputs\n\n    y_data_pred = []\n    tr_loop = data.shape[0] - data.shape[0]%1000\n    # consider you X_tr shape is 49041, then your tr_loop will be 49041 - 49041%1000 = 49000\n    # in this for loop we will iterate unti the last 1000 multiplier\n    for i in range(0, tr_loop, 1000):\n        y_data_pred.extend(clf.predict_proba(data[i:i+1000])[:,1])\n    # we will be predicting for the last data points\n    if data.shape[0]%1000 !=0:\n        y_data_pred.extend(clf.predict_proba(data[tr_loop:])[:,1])\n    \n    return y_data_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we are writing our own function for predict, with defined thresould\n# we will pick a threshold that will give the least fpr\ndef find_best_threshold(threshould, fpr, tpr):\n    t = threshould[np.argmax(tpr*(1-fpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    return t\n\ndef predict_with_best_t(proba, threshould):\n    predictions = []\n    for i in proba:\n        if i>=threshould:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to do grid search cross validation\ndef doGridSearch(X_tr, y_train, dense=False):\n    neigh = KNeighborsClassifier(n_jobs=-1)\n    parameters = {'n_neighbors':[11, 21, 31, 41, 51]}\n    clf = GridSearchCV(neigh, parameters, cv=3, scoring='roc_auc',return_train_score=True, n_jobs=-1, verbose=10)\n    clf.fit(X_tr, y_train)\n\n    results = pd.DataFrame.from_dict(clf.cv_results_)\n    results = results.sort_values(['param_n_neighbors'])\n\n    train_auc= results['mean_train_score']\n    train_auc_std= results['std_train_score']\n    cv_auc = results['mean_test_score'] \n    cv_auc_std= results['std_test_score']\n    K =  results['param_n_neighbors']\n\n    plt.plot(K, train_auc, label='Train AUC')\n\n    plt.plot(K, cv_auc, label='CV AUC')\n\n    plt.scatter(K, train_auc, label='Train AUC points')\n    plt.scatter(K, cv_auc, label='CV AUC points')\n\n\n    plt.legend()\n    plt.xlabel(\"K: hyperparameter\")\n    plt.ylabel(\"AUC\")\n    plt.title(\"Hyper parameter Vs AUC plot\")\n    plt.grid()\n    plt.show()\n    \n    return clf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to plot auc and heat maps of confusion matrix\ndef plotAucAndHeatmap(neighbors, X_tr, X_te, y_train, y_test):\n    neigh = KNeighborsClassifier(n_neighbors=neighbors, n_jobs=-1)\n    neigh.fit(X_tr, y_train)\n    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n    # not the predicted outputs\n\n    y_train_pred = batch_predict(neigh, X_tr)    \n    y_test_pred = batch_predict(neigh, X_te)\n\n    train_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\n    test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\n    plt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\n    plt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\n    plt.legend()\n    plt.xlabel(\"FPR\")\n    plt.ylabel(\"TPR\")\n    plt.title(\"ERROR PLOTS\")\n    plt.grid()\n    plt.show()\n    \n    print(\"=\"*100)\n    best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n    print(\"Train confusion matrix\")\n    print(confusion_matrix(y_train, predict_with_best_t(y_train_pred, best_t)))\n    print(\"Test confusion matrix\")\n    print(confusion_matrix(y_test, predict_with_best_t(y_test_pred, best_t)))\n    print(\"=\"*100)\n\n    \n    plotheatMap(confusion_matrix(y_train, predict_with_best_t(y_train_pred, best_t)), confusion_matrix(y_test, predict_with_best_t(y_test_pred, best_t)))\n    return str(auc(train_fpr, train_tpr)), str(auc(test_fpr, test_tpr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# subplot seaborn : https://stackoverflow.com/a/41384984/8363466\n#confusion matrix heat map : https://seaborn.pydata.org/generated/seaborn.heatmap.html\n#plot confusion matrix of test and train\ndef plotheatMap(confusion_matrix_train, confusion_matrix_test):\n    fig, (ax1, ax2) = plt.subplots(1,2)\n    fig.set_figheight(5)\n    fig.set_figwidth(15)\n    \n    confusion_train_bow = pd.DataFrame(confusion_matrix_train)\n    sns.heatmap(confusion_train_bow, annot=True, fmt='d', ax=ax1)\n    ax1.set_title(\"Train Confusion matrix\")\n    ax1.set(xlabel='Actual', ylabel='Predicted')\n    \n    confusion_test_bow = pd.DataFrame(confusion_matrix_test)\n    sns.heatmap(confusion_test_bow, annot=True, fmt='d', ax=ax2)\n    ax2.set_title(\"Test Confusion matrix\")\n    ax2.set(xlabel='Actual', ylabel='Predicted')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating train and test data for KNN brute force on AVG W2V\nX_tr = hstack((avgw2v_title_train, avgw2v_text_train, x_train_cat)).tocsr()\nX_te = hstack((avgw2v_title_test, avgw2v_text_test, x_test_cat)).tocsr()\n\nprint(X_tr.shape, y_train.shape)\nprint(X_te.shape, y_test.shape)\nprint(\"=\"*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#do grid search to find best K\navgw2v_clf = doGridSearch(X_tr, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#randomizedsearchcv sklearn: https://www.youtube.com/watch?v=Gol_qOgRqfA\nprint(avgw2v_clf.best_score_)\nprint(avgw2v_clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of neighbours as per GridSearchCV : \",avgw2v_clf.best_params_['n_neighbors'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot auc and confusion matrix\navgw2v_train_auc, avgw2v_test_auc = plotAucAndHeatmap(avgw2v_clf.best_params_['n_neighbors'], X_tr, X_te, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}