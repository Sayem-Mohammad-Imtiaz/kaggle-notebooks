{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SELECT FEATURES & DATA CLEANING"},{"metadata":{"slideshow":{"slide_type":"skip"},"trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"runs = pd.read_csv(\"../input/hkracing/runs.csv\")\nruns.head()","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"races = pd.read_csv('../input/hkracing/races.csv')\nraces.head()","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"## Select features for modeling"},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"runs_data = runs[['race_id', 'won', 'horse_age', 'horse_country', 'horse_type', 'horse_rating',\n       'horse_gear', 'declared_weight', 'actual_weight', 'draw', 'win_odds',\n       'place_odds', 'horse_id']]\nruns_data.head()","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"races_data = races[['race_id', 'venue', 'config', 'surface', 'distance', 'going', 'race_class', 'date']]\nraces_data.head()","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"},"trusted":true},"cell_type":"code","source":"# merge the two datasets based on race_id column\ndf = pd.merge(runs_data, races_data)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"## Check missing values"},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"df.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"},"trusted":true},"cell_type":"code","source":"df.horse_country.isnull().value_counts(ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"df.horse_type.isnull().value_counts(ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"df.place_odds.isnull().value_counts(ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"}},"cell_type":"markdown","source":"The amount of rows for missing values is relatively small, therefore we decided to drop these rows. "},{"metadata":{"slideshow":{"slide_type":"subslide"},"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"df = df.dropna()\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"## Basic information of the data"},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"df.date = pd.to_datetime(df.date)\ndf.date.dtype","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"min(df.date), max(df.date)\n# 8-year duration","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"start_time = min(df.date).strftime('%d %B %Y')\nend_time = max(df.date).strftime('%d %B %Y')\nno_of_horses = df.horse_id.nunique()\nno_of_races = df.race_id.nunique()\n\nprint(f'The dataset was collected from {start_time} to {end_time}, which contains information about {no_of_horses} horses and {no_of_races} races. ')","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"skip"},"trusted":true},"cell_type":"code","source":"# drop the unnecessary columns\ndf = df.drop(columns=['horse_id', 'date'])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"skip"},"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"## Impute feature"},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"df.horse_gear.value_counts(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"df.horse_gear.nunique()","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"}},"cell_type":"markdown","source":"For horse_gear column, we dicided to impute the data into 1 and 0 (with gear and no gear), rather than one-hot labeling (which will lead to numerous features). "},{"metadata":{"slideshow":{"slide_type":"subslide"},"trusted":true},"cell_type":"code","source":"def horse_gear_impute(cols):\n    if cols == '--':\n        return 0\n    else: \n        return 1","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"df.horse_gear = df.horse_gear.apply(horse_gear_impute)","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"df.horse_gear.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"## One-hot encoding for categorical features"},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df, drop_first=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"## Features explanation:\nwon - whether horse won (1) or otherwise (0)<br/>\nhorse_age - current age of this horse at the time of the race<br/>\nhorse_rating - rating number assigned by HKJC to this horse at the time of the race<br/>\nhorse_gear - string representing the gear carried by the horse in the race. An explanation of the codes used may be found on the HKJC website.<br/>\ndeclared_weight - declared weight of the horse and jockey, in lbs<br/>\nactual_weight - actual weight carried by the horse, in lbs<br/>\ndraw - post position number of the horse in this race<br/>\nwin_odds - win odds for this horse at start of race<br/>\nplace_odds - place (finishing in 1st, 2nd or 3rd position) odds for this horse at start of race<br/>\nsurface - a number representing the type of race track surface: 1 = dirt, 0 = turf<br/>\ndistance - distance of the race, in metres<br/>\nrace_class - a number representing the class of the race<br/>\nhorse_country - country of origin of this horse<br/>\nhorse_type - sex of the horse, e.g. 'Gelding', 'Mare', 'Horse', 'Rig', 'Colt', 'Filly'<br/>\nvenue - a 2-character string, representing which of the 2 race courses this race took place at: ST = Shatin, HV = Happy Valley<br/>\nconfig - race track configuration, mostly related to the position of the inside rail. For more details, see the HKJC website.<br/>\ngoing - track condition. For more details, see the HKJC website.<br/>"},{"metadata":{},"cell_type":"markdown","source":"# MODELING"},{"metadata":{"slideshow":{"slide_type":"skip"},"trusted":true},"cell_type":"code","source":"from time import time\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.neighbors import KNeighborsClassifier\nimport lightgbm as lgb\nfrom sklearn.metrics import precision_score, classification_report, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"## Extract the last race data for model testing"},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"last_raceid = max(df.race_id)\nlast_raceid","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"# split the last race data for deployment & save it in last_race variable\nlast_race = df[df.race_id == last_raceid]\nlast_race","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"},"trusted":true},"cell_type":"code","source":"new_data = df[:75696]   # drop the last race data for modeling\nnew_data = new_data.drop(columns='race_id')   # drop the unnecessary race_id column\nnew_data.tail()","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"new_data.shape","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"## Distribution of labels"},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,4))\nsns.countplot(data=new_data, x='won')\nplt.title('Number of Labels by Class')","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"X = new_data.drop(columns='won')\ny = new_data['won']","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"},"trusted":true},"cell_type":"code","source":"# extermely skewed data\ny.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"## kNN Classifier (original data)"},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"k_range = range(1,10)\nscores = {}\nscores_list = []\n\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    # precision ratio: tp / (tp + fp), aiming at minimize fp (predict: win, actual: lose)\n    scores[k] = precision_score(y_test, y_pred)\n    scores_list.append(precision_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"# find the highest precision score of the positive class (1)\nimport operator\nmax(scores.items(), key=operator.itemgetter(1))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"},"trusted":true},"cell_type":"code","source":"plt.plot(k_range, scores_list)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Precision Score of the positive class (1)')\nplt.title('Original Data')","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"start = time()\n\nknn = KNeighborsClassifier(n_neighbors=8)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nend = time()\nrunning_time = end - start\nprint('time cost: %.5f sec' %running_time)","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"skip"},"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"skip"},"trusted":true},"cell_type":"code","source":"labels = ['lose', 'win']\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"## kNN Classifier (under-sampling)"},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"rus = RandomUnderSampler(random_state=0)\nX_rus, y_rus = rus.fit_sample(X_train, y_train)\n\nk_range = range(1,10)\nscores = {}\nscores_list = []\n\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_rus, y_rus)\n    y_pred = knn.predict(X_test)\n    scores[k] = precision_score(y_test, y_pred)\n    scores_list.append(precision_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"max(scores.items(), key=operator.itemgetter(1))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"},"trusted":true},"cell_type":"code","source":"plt.plot(k_range, scores_list)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Precision Score of the positive class (1)')\nplt.title('RUS Data')","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"start = time()\n\nknn_rus = KNeighborsClassifier(n_neighbors=8)\nknn_rus.fit(X_rus, y_rus)\ny_pred = knn_rus.predict(X_test)\n\nend = time()\nrunning_time = end - start\nprint('time cost: %.5f sec' %running_time)","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"skip"},"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"skip"},"trusted":true},"cell_type":"code","source":"labels = ['lose', 'win']\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"## kNN Classifier (over-sampling)"},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"sm = SMOTE(random_state=0)\nX_sm, y_sm = sm.fit_sample(X_train, y_train)\n\nk_range = range(1,10)\nscores = {}\nscores_list = []\n\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_sm, y_sm)\n    y_pred = knn.predict(X_test)\n    scores[k] = precision_score(y_test, y_pred)\n    scores_list.append(precision_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"max(scores.items(), key=operator.itemgetter(1))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"},"trusted":true},"cell_type":"code","source":"# SMOTE data\nplt.plot(k_range, scores_list)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Precision Score of the positive class (1)')\nplt.title('SMOTE Data')","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"start = time()\n\nknn_sm = KNeighborsClassifier(n_neighbors=2)\nknn_sm.fit(X_sm, y_sm)\ny_pred = knn_sm.predict(X_test)\n\nend = time()\nrunning_time = end - start\nprint('time cost: %.5f sec' %running_time)","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"skip"},"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"skip"},"trusted":true},"cell_type":"code","source":"labels = ['lose', 'win']\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n* Faster training speed and higher efficiency.\n* Lower memory usage.\n* Better accuracy.\n* Support of parallel and GPU learning.\n* Capable of handling large-scale data."},{"metadata":{"slideshow":{"slide_type":"fragment"}},"cell_type":"markdown","source":"## LightGBM (original data)"},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"start = time()\n\nd_train = lgb.Dataset(X_train, label = y_train)\nparams = {}\nparams['learning_rate'] = 0.003\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'binary'\nparams['metric'] = 'binary_logloss'\nparams['sub_feature'] = 0.5\nparams['num_leaves'] = 100\nparams['min_data'] = 500\nparams['max_depth'] = 100\nclf = lgb.train(params, d_train, 100)\n\nend = time()\nrunning_time = end - start\nprint('time cost: %.5f sec' %running_time)","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"},"trusted":true},"cell_type":"code","source":"#Prediction\ny_pred = clf.predict(X_test)\n#convert into binary values\nfor i in range(15140):\n    if y_pred[i] >= 0.0995:       # setting threshold \n        y_pred[i] = 1\n    else:  \n        y_pred[i] = 0","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"skip"},"trusted":true},"cell_type":"code","source":"labels = ['lose', 'win']\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"skip"},"trusted":true},"cell_type":"code","source":"# plot model’s feature importances (original data)\nlgb.plot_importance(clf, max_num_features=10)","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"## LightGBM (under-sampling)"},{"metadata":{"slideshow":{"slide_type":"skip"},"trusted":true},"cell_type":"code","source":"# convert array data into dataframe with column names, and feed into lgb model\nX_rus = pd.DataFrame(X_rus, columns=list(X_train))\nX_rus.head()","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"start = time()\n\nd_train = lgb.Dataset(X_rus, label = y_rus)\nparams = {}\nparams['learning_rate'] = 0.003\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'binary'\nparams['metric'] = 'binary_logloss'\nparams['sub_feature'] = 0.5\nparams['num_leaves'] = 100\nparams['min_data'] = 500\nparams['max_depth'] = 100\nclf_rus = lgb.train(params, d_train, 100)\n\nend = time()\nrunning_time = end - start\nprint('time cost: %.5f sec' %running_time)","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"},"trusted":true},"cell_type":"code","source":"#Prediction\ny_pred = clf_rus.predict(X_test)\n#convert into binary values\nfor i in range(15140):\n    if y_pred[i] >= 0.55:       # setting threshold \n        y_pred[i] = 1\n    else:  \n        y_pred[i] = 0","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"skip"},"trusted":true},"cell_type":"code","source":"labels = ['lose', 'win']\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"skip"},"trusted":true},"cell_type":"code","source":"# plot model’s feature importances (Random Under-sampling)\nlgb.plot_importance(clf_rus, max_num_features=10)","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"## LightGBM (over-sampling)"},{"metadata":{"slideshow":{"slide_type":"skip"},"trusted":true},"cell_type":"code","source":"# convert array data into dataframe with column names, and feed into lgb model\nX_sm = pd.DataFrame(X_sm, columns=list(X_train))\nX_sm.head()","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"start = time()\n\nd_train = lgb.Dataset(X_sm, label = y_sm)\nparams = {}\nparams['learning_rate'] = 0.003\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'binary'\nparams['metric'] = 'binary_logloss'\nparams['sub_feature'] = 0.5\nparams['num_leaves'] = 100\nparams['min_data'] = 500\nparams['max_depth'] = 100\nclf_sm = lgb.train(params, d_train, 100)\n\nend = time()\nrunning_time = end - start\nprint('time cost: %.5f sec' %running_time)","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"},"trusted":true},"cell_type":"code","source":"#Prediction\ny_pred = clf_sm.predict(X_test)\n#convert into binary values\nfor i in range(15140):\n    if y_pred[i] >= 0.5:       # setting threshold \n        y_pred[i] = 1\n    else:  \n        y_pred[i] = 0","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"skip"},"trusted":true},"cell_type":"code","source":"labels = ['lose', 'win']\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"skip"},"trusted":true},"cell_type":"code","source":"# plot model’s feature importances (SMOTE)\nlgb.plot_importance(clf_sm, max_num_features=10)","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"}},"cell_type":"markdown","source":"* By processing a lot of data, kNN model trained with over-sampled data took the longest time, while LightGBM model trained with under-sampled data took the shortest time. \n* kNN models performed relatively worse with low precision score and f1-score of the positive class (1). \n* Training models aimed at minimize False Positive (predict: win, actual: lose), but it seems True Positive and False Positive are correlated. Same as gambling and investment, you have the chance to win and the risk to lose at the same time.\n* File sizes of LightGBM models are incredibly small and the time spent on training models is really quick."},{"metadata":{"slideshow":{"slide_type":"skip"}},"cell_type":"markdown","source":"LightGBM code reference from Medium [article](https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc) by Pushkar Mandot. Thank you for sharing your experience! =]"},{"metadata":{"slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"# DEPLOY MODELS"},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"# data that never been seen by the models\nlast_race","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"# drop unnecessary columns & define data and labels\nX_deploy = last_race.drop(columns=['race_id', 'won'])\ny_deploy = last_race.won","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"## Load kNN model trained with original data"},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"predictions = knn.predict(X_deploy)\nprint(classification_report(y_deploy, predictions))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"}},"cell_type":"markdown","source":"Only class 0 (lose) can be predicted. "},{"metadata":{"slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"## Load kNN model trained with under-sampled data"},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"predictions = knn_rus.predict(X_deploy)\nprint(classification_report(y_deploy, predictions))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"}},"cell_type":"markdown","source":"kNN model trained with under-sampled data can predict the winning horse. However, there is also one False Positive in the prediction. "},{"metadata":{"slideshow":{"slide_type":"subslide"},"trusted":true},"cell_type":"code","source":"import numpy as np\n\ndata = confusion_matrix(y_deploy, predictions)\n\nfig, ax = plt.subplots()\ncax = ax.matshow(data, cmap='RdBu')\n\nfor (i, j), z in np.ndenumerate(data):\n    ax.text(j, i, '{}'.format(z), ha='center', va='center',\n            bbox=dict(boxstyle='round', facecolor='white', edgecolor='0.3'))\n    \nplt.title('Confusion matrix of kNN_rus', y=1.1)\nfig.colorbar(cax)\nlabels = ['lose', 'win']\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Prediction')\nplt.ylabel('Actual')","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"## Load kNN model trained with over-sampled data"},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"predictions = knn_sm.predict(X_deploy)\nprint(classification_report(y_deploy, predictions))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"}},"cell_type":"markdown","source":"Only class 0 (lose) can be predicted."},{"metadata":{"slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"## Load LightGBM models & Set threshold values same as the training models"},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"predictions = clf.predict(X_deploy)\n#convert into binary values\nfor i in range(14):\n    if predictions[i] >= 0.0995:       # setting threshold \n        predictions[i] = 1\n    else:  \n        predictions[i] = 0","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"predictions_rus = clf_rus.predict(X_deploy)\n#convert into binary values\nfor i in range(14):\n    if predictions_rus[i] >= 0.55:       # setting threshold \n        predictions_rus[i] = 1\n    else:  \n        predictions_rus[i] = 0","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"predictions_sm = clf_sm.predict(X_deploy)\n#convert into binary values\nfor i in range(14):\n    if predictions_sm[i] >= 0.5:       # setting threshold \n        predictions_sm[i] = 1\n    else:  \n        predictions_sm[i] = 0","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"## Predictions of the LightGBM models"},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"print(classification_report(y_deploy, predictions))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"print(classification_report(y_deploy, predictions_rus))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"},"trusted":true},"cell_type":"code","source":"print(classification_report(y_deploy, predictions_sm))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"}},"cell_type":"markdown","source":"All LightGBM models can achieve 100% accuracy rate. "},{"metadata":{"slideshow":{"slide_type":"subslide"},"trusted":true},"cell_type":"code","source":"data = confusion_matrix(y_deploy, predictions)\n\nfig, ax = plt.subplots()\ncax = ax.matshow(data, cmap='RdBu')\n\nfor (i, j), z in np.ndenumerate(data):\n    ax.text(j, i, '{}'.format(z), ha='center', va='center',\n            bbox=dict(boxstyle='round', facecolor='white', edgecolor='0.3'))\n    \nplt.title('Confusion matrix of LightGBM models', y=1.1)\nfig.colorbar(cax)\nlabels = ['lose', 'win']\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Prediction')\nplt.ylabel('Actual')","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"## Conclusions:\nFor KNeighborsClassifier, only model trained with under-sampled data can predict both class 0 and class 1 (with one False Positive error). The original data model and over-sampling model can only predict class 0.  <br/>\nLightGBM models can predict all data correctly, even using the model trained with skewed dataset (by tuning the threshold value). "},{"metadata":{"slideshow":{"slide_type":"skip"}},"cell_type":"markdown","source":"Confusion matrix plot code reference from [Stack Overflow](https://stackoverflow.com/questions/20998083/show-the-values-in-the-grid-using-matplotlib) user Joe Kington. Thank you for sharing your experience! =]"}],"metadata":{"celltoolbar":"Slideshow","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}