{"cells":[{"metadata":{"_uuid":"1024a1accbe2a69bedeb63b6393d999de7fefb22"},"cell_type":"markdown","source":"# LSTMs for review-text-based rating prediction\n\nProcessing text data usually leads to variable length sequences handling. Training networks for text analysis with regular LSTM cells is time consuming. However, the CuDNNLSTM - fast implementation of LSTM - does not support masking of input sequences so far, hence one can be concerned if they could be useful in NLP related tasks.  \nIn this notebook I made a simple benchmark of time/performance of LSTM/CuDNNLSTM based models trained with zero padded, concatenated, masked and unmasked data.\n\nThanks to @kratisaxena for the nice [EDA](https://www.kaggle.com/kratisaxena/eda-classification-for-reviews-using-rnn) I learned a lot from.\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Load Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nimport time\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model\nfrom keras.layers import Embedding, Flatten, Dense, LSTM, GRU, Dropout, Input","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"332380c0a58c10f7b1090ac8a8a46de49e688d99"},"cell_type":"markdown","source":"Let's preview the data file."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/Womens Clothing E-Commerce Reviews.csv\", index_col=0)\nprint('Records:',len(df))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c1b41b493484ef5bd46f9c3d5440fdb27b209f0"},"cell_type":"markdown","source":"So in this approach the Rating will be predicted based on the Review Text content. Hence let's drop records with missing Review Text value."},{"metadata":{"trusted":true,"_uuid":"d3bf921d75c165956c767122033a024ab4609614"},"cell_type":"code","source":"df = df[pd.notnull(df['Review Text'])]\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad4d01b881a01a0b14c9fc5d44bdb85a43cb89ef"},"cell_type":"markdown","source":"## Download and extract word embeddings"},{"metadata":{"trusted":true,"_uuid":"3c6d0ae0ef1d50d0a54e4bd6f8be3a7f4b055e49"},"cell_type":"code","source":"import requests\nimport zipfile\n\nurl = \"http://nlp.stanford.edu/data/glove.6B.zip\"\nr = requests.get(url, allow_redirects=True)\nopen('../working/test.zip', 'wb').write(r.content)\n\nwith zipfile.ZipFile('../working/test.zip', 'r') as zip_ref:\n    zip_ref.extractall('../working/')\n    \nos.listdir('../working/')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f31fe6e4de95ed7608580fa9edc82c0dbade5a53"},"cell_type":"markdown","source":"There are few versions avaliable in the downloaded package, I'll pick 300d."},{"metadata":{"trusted":true,"_uuid":"25e77dfb5f0090ca8586e8185f65011b4891ebe7"},"cell_type":"code","source":"n_embeddings=300","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"174e900b90699b463d03da30ba6936fcd1ab739a"},"cell_type":"markdown","source":"Configure a tokenizer and fit it on the available Review Text data:"},{"metadata":{"trusted":true,"_uuid":"e4f8ee7a9ac84d8bcc0722a43d20dc64743e22fc"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nmax_words = 15000\n\nt = Tokenizer(num_words=max_words, char_level=False, split=' ')\nt.fit_on_texts( df['Review Text'])\n\nvocab_size = len(t.word_index)+1\nvocab_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ecb1180b2b2a155acd60db9a2b699bd51d201ce"},"cell_type":"markdown","source":"Make sequences of tokens:"},{"metadata":{"trusted":true,"_uuid":"4f9d526c06c21246ad19d4c69b842306d5bbcca6"},"cell_type":"code","source":"sequences = t.texts_to_sequences(df['Review Text'])\n\ndf['n_tokens'] = [len(seq) for seq in sequences] # get length of sequences\ndf.drop(df[df['n_tokens']<2].index, inplace=True) # remove short sequences\ndf.reset_index(inplace=True)\n\nsequences = t.texts_to_sequences(df['Review Text']) # tokenize again\nmax_length = max(df['n_tokens'])\nprint('Max length of sequence:',max_length)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a3a812be8f40c0c655d284aa76b4a12809207ab"},"cell_type":"markdown","source":"## Two ways of representing tokenized text (sequences):\n* **Zero padding**  \nEach sequence can be zero-padded to create a dense ndarray."},{"metadata":{"trusted":true,"_uuid":"ca0cdb646de616874794dae27faaf6d1cde10d7e"},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nsequences_pad = pad_sequences(sequences=sequences, maxlen=max_length, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64cfa839db87737df59c4d7b3307eaaa5448e88b"},"cell_type":"markdown","source":"* **Concatenating**  \nAs CuDNNLSTM does not support masking, maybe the network would take advantage of concatenating word sequences instead of making zero padding. Therefore other way of representing descriptive data is: if the max_length is e.g. 10, shorter sequences will be copied, concatenated and clipped until they have 10 words. E.g. from the sequence [1, 43, 9] we will produce [1, 43, 9, 1, 43, 9, 1, 43, 9, 1]. This could refer to having \"this is great this is great this is great this\" instead of \"this is great _ _ _ _ _ _ _\" for underscore being a special masking character."},{"metadata":{"trusted":true,"_uuid":"e0b4267951ddf579bb95dd19876411d7c3a36383"},"cell_type":"code","source":"sequences_concat = [np.asarray( (seq*(int(max_length/(len(seq))+len(seq))))[0:max_length]) for seq in sequences]\nsequences_concat = np.vstack(sequences_concat)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6ed5ac147f2a0084c2b524a6fe81a226bf75a5d"},"cell_type":"markdown","source":"## Reading the embedding matrix"},{"metadata":{"trusted":true,"_uuid":"d9b28b825aa625cf8aa41c9ebb25f0c3055523f4"},"cell_type":"code","source":"# load the whole embedding into memory\nembeddings_index = dict()\nf = open('glove.6B.'+str(n_embeddings)+'d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0f4833c7ff94400738ad1f9fdc9ba1c3ffbbc70"},"cell_type":"code","source":"# create a weight matrix for words in training docs\nembedding_matrix = np.zeros((vocab_size, n_embeddings))\nfor word, i in t.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3546c7cc2a8f9b014bd21024b6c318115d8a9b8d"},"cell_type":"markdown","source":"## Prepare training/validation/testing sets"},{"metadata":{"trusted":true,"_uuid":"bba99133ab92c23176957fc567eb7ddac9d9dd67"},"cell_type":"code","source":"labels = np.asarray(df[\"Rating\"].values)\nprint('Shape of seq concat tensor:', sequences_concat.shape)\nprint('Shape of seq padded tensor:', sequences_pad.shape)\nprint('Shape of label tensor:', labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da9e288f4765947a4f3c2fcf64d4aaf849dedf5b"},"cell_type":"code","source":"indices = np.arange(df.shape[0])\nnp.random.shuffle(indices)\nsequences_pad = sequences_pad[indices]\nsequences_concat = sequences_concat[indices]\nlabels = labels[indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee8980bead6d5603dac7915fe872462e907400eb"},"cell_type":"code","source":"trainingP = 0.6\nvalidationP = 0.2\ntestP = 0.2\n\ntraining_samples = int(len(sequences_concat)*trainingP)\nvalidation_samples = training_samples + int(len(sequences_concat)*validationP)\n\nx_trainP = sequences_pad[:training_samples]\nx_trainC = sequences_concat[:training_samples]\ny_train = labels[:training_samples]\n\nx_valP = sequences_pad[training_samples: validation_samples] \nx_valC = sequences_concat[training_samples: validation_samples] \ny_val = labels[training_samples: validation_samples]\n\nx_testP = sequences_pad[validation_samples:]\nx_testC = sequences_concat[validation_samples:]\ny_test = labels[validation_samples:]\n\nx_test_text = df['Review Text'].loc[indices][validation_samples:]\nx_test_text = x_test_text.tolist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6aa02d337c1c5d9a82e691616f9338c925dac3b3"},"cell_type":"markdown","source":"## RNN network and helpers\n\nThe **Score1** metric tells about percentage of examples, which rating was predicted with error less than 1.  \nThe root mean squared error (**rmse**) will be used as the loss function."},{"metadata":{"trusted":true,"_uuid":"e30f12ac722d2f3b4d51d96db336993c35c4027d"},"cell_type":"code","source":"import keras.backend as K\nfrom keras.layers import add, Lambda\n\ndef Score1(y_true, y_pred):\n    minus_yt = Lambda(lambda x: -x)(y_true)\n    subtracted =  K.abs( add([y_pred, minus_yt]) )    \n    return K.mean(K.less(subtracted, 1.0), axis=-1)\n\ndef rmse(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a573e57c78f130236d9bcb6177e156f8b12dbaf"},"cell_type":"markdown","source":"Class that will help tracking the duration of each training epoch:"},{"metadata":{"trusted":true,"_uuid":"49b18510fff9efe1336fe13963d030997ec45039"},"cell_type":"code","source":"class TimeHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.times = []\n    def on_epoch_begin(self, batch, logs={}):\n        self.epoch_time_start = time.time()\n    def on_epoch_end(self, batch, logs={}):\n        self.times.append(time.time() - self.epoch_time_start)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4e721a69ea3b170c67739b32d233c382963326f"},"cell_type":"markdown","source":"Simple custom bidirectional RNN network with optional custom memory cell."},{"metadata":{"trusted":true,"_uuid":"720c29f72c0348424b44b42c26f6d107189b1dc7"},"cell_type":"code","source":"from keras.layers import Bidirectional, CuDNNLSTM\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.optimizers import adam\n\ndef build_RNN(LSTM_CELL=LSTM, mask_zero=False):\n            \n    model = Sequential()     \n    model.add(Embedding(input_dim    = vocab_size, \n                        output_dim   = n_embeddings, \n                        weights      = [embedding_matrix], \n                        input_length = max_length,\n                        mask_zero    = mask_zero,\n                        trainable    = False))    \n    model.add(Bidirectional( LSTM_CELL(24, return_sequences=False)) )\n    model.add(BatchNormalization())\n    model.add(Dropout(rate=0.7))         \n    model.add(Dense(units=4,  activation='relu'))\n    model.add(Dense(units=1,  activation='relu'))\n    \n    optimizer = adam(clipnorm=1.0)\n    model.compile(optimizer=optimizer, loss=rmse, metrics=['acc', Score1]) \n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5e4e9b61ad46bc140431164e673a745e5efb7a3"},"cell_type":"markdown","source":"Having unbalanced dataset (in terms of rating distribution) let's weight the input data for the loss function during the training:"},{"metadata":{"trusted":true,"_uuid":"5e579eba159b9811f52b483fe74a546e2cea0bf8"},"cell_type":"code","source":"from sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(df['Rating']),\n                                                 df['Rating'])\nclass_weight_dict = {i+1:class_weights[i] for i in range(5)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf95ed699f7be18d6f8c2aeff9cb5537246e131d"},"cell_type":"code","source":"def make_training(LSTM_CELL, mask_zero, x_train, x_val, epochs, batch_size):\n    #K.clear_session()\n    from numpy.random import seed as nseed\n    nseed(2019)\n    from tensorflow import set_random_seed\n    set_random_seed(2019)\n    \n    time_callback = TimeHistory()    \n    model = build_RNN(LSTM_CELL = LSTM_CELL, mask_zero = False)\n    \n    history_RNN = model.fit(x_train, y_train,\n                        callbacks  = [time_callback],\n                        epochs     = epochs,\n                        batch_size = batch_size,\n                        shuffle         = True,\n                        class_weight    = class_weight_dict,\n                        validation_data = (x_val, y_val))\n\n    return history_RNN, time_callback, model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2eec8fd5c38a4fd93c96b2130c3be76c82cbf164"},"cell_type":"markdown","source":"## Training different models (CuDNNLSTM/LSTM)\nAs the data is ready and model function is finished let's train four models: two with CuDNNLSTM, two with LSTM, fed with differently prepared data:"},{"metadata":{"trusted":true,"_uuid":"14071cccfee7f0953a91a2fe66c37a078fa0fcfa"},"cell_type":"code","source":"epochs = 10\nbatch  = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d64cb2af4c2c46ec5a2a5ae7641f9aaf2db4a05f","scrolled":true,"_kg_hide-output":true},"cell_type":"code","source":"h1, t1, m1 = make_training(CuDNNLSTM, False, x_trainC, x_valC, epochs=epochs, batch_size=batch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a41b60bdd0922eff5267d3ef9d5b5a7ae169315","scrolled":true,"_kg_hide-output":true},"cell_type":"code","source":"h2, t2, m2 = make_training(CuDNNLSTM, False, x_trainP, x_valP, epochs=epochs, batch_size=batch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d2e55c34437b00fa711c9341ebe8be0495eb5f4","_kg_hide-output":true},"cell_type":"code","source":"h3, t3, m3 = make_training(LSTM, False, x_trainC, x_valC, epochs=epochs, batch_size=batch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dff1ebf8a2ba92f3f0cd2c9897049d1ded4014fa","scrolled":true,"_kg_hide-output":true},"cell_type":"code","source":"h4, t4, m4 = make_training(LSTM, True, x_trainP, x_valP, epochs=epochs, batch_size=batch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23e1d7610af31821ba1241f66969570b4ae2210c"},"cell_type":"code","source":"hlist = [h1, h2, h3, h4]\ntlist = [t1, t2, t3, t4]\nlabels = ['CuDNN | no mask | con',\n          'CuDNN | no mask | pad',\n          'no Cu | no mask | con',\n          'no Cu |   mask  | pad']\nmarkers=['x','o','d','s']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31258156628c44510ee4107de7f3c3d873dba4b6"},"cell_type":"markdown","source":"Note that although charts plotted against the epoch number are very similar (which is good, CuDNNLSTM based models are learning well), the wall time plots unravel the obvious time-saving virtue of using fast cells."},{"metadata":{"trusted":true,"_uuid":"d6452b200d258b71fc755aff7997cf764fc9d487","scrolled":false,"_kg_hide-input":false},"cell_type":"code","source":"fig, axs = plt.subplots(nrows=4, ncols=3, figsize=(12, 10))\nfor x_sel in [0,1]:\n    for i, opt in enumerate(['loss','acc', 'Score1']+['val_loss','val_acc', 'val_Score1']):\n        for _h, _t, _l, _m in zip(hlist, tlist, labels, markers):\n            ax = axs[i//3+x_sel*2, i%3]             \n            x_axis = range(1,epochs+1) if x_sel == 0 else np.cumsum(_t.times)\n            x_label = 'epochs' if x_sel == 0 else 'time [s]'\n            ax.plot(x_axis, _h.history[opt], label=_l, marker=_m)\n        ax.set_title(opt)\n        ax.set_xlabel(x_label)    \nax.legend()\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"681802cbd79eb9c026098df5e90f89497d90da31"},"cell_type":"markdown","source":"If you look closely, validation plots obtained from the models trained on the concatenated data may be less noisy in comparison to those trained on the padded data (even with mask - for LSTM cell). On the other hand, they give better results, at least within the investigated 10 epochs period. With that, let's train CuDNNLSTM-based models for some more epochs to see in practice how the Rating prediction works."},{"metadata":{"trusted":true,"_uuid":"12db17b6961cfd492564c140de44723b5e158050"},"cell_type":"code","source":"more_epochs = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"084d61742c2182dcf0421b6704e1405dd16de511","_kg_hide-output":true},"cell_type":"code","source":"hc, tc, mc = make_training(CuDNNLSTM, False, x_trainC, x_valC, epochs=more_epochs, batch_size=batch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00dc4a1c68e42357928ac799d31bcedf8f9511a2","scrolled":true,"_kg_hide-output":true},"cell_type":"code","source":"hp, tp, mp = make_training(CuDNNLSTM, False, x_trainP, x_valP, epochs=more_epochs, batch_size=batch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b46193f8904863f4c47f8075636c224f974f2c6"},"cell_type":"code","source":"hlist2  = [hc, hp]\ntlist2  = [tc, tp]\nlabels2 = ['CuDNN | no mask | con',\n           'CuDNN | no mask | pad']\nmarkers2=['x','o']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4e178a7ad3e834777c7db800df4a266008f1d1b","scrolled":false},"cell_type":"code","source":"fig, axs = plt.subplots(nrows=4, ncols=3, figsize=(12, 10))\nfor x_sel in [0,1]:\n    for i, opt in enumerate(['loss','acc', 'Score1']+['val_loss','val_acc', 'val_Score1']):\n        for _h, _t, _l, _m in zip(hlist2, tlist2, labels2, markers2):\n            ax = axs[i//3+x_sel*2, i%3]             \n            x_axis = range(1,more_epochs+1) if x_sel == 0 else np.cumsum(_t.times)\n            x_label = 'epochs' if x_sel == 0 else 'time [s]'\n            ax.plot(x_axis, _h.history[opt], label=_l, marker=_m)\n        ax.set_title(opt)\n        ax.set_xlabel(x_label)    \nax.legend()\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4b897f378a63126022071fa4bc7f5132aee9e98"},"cell_type":"markdown","source":"## Model performance investigation\nTo tell which model (trained with concatenated or padded data) is better, more epochs should be considered. The concatenated-data-based model seems to give more promising and reliable results so Let's pick it for further performance investigation:"},{"metadata":{"trusted":true,"_uuid":"07864562b49b97803ddf0575c91193feb7fb0dac"},"cell_type":"code","source":"scores = mc.evaluate(x_testC, y_test)\nprint('Loss: {0:.4}\\t Accuracy: {1:.4}\\t Score1: {2:.4}'.format(scores[0],scores[1],scores[2]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d954be08e3475ec64935153d768285d26f01edd"},"cell_type":"markdown","source":"Let the model predict outputs of the testing set."},{"metadata":{"trusted":true,"_uuid":"49c96e49789b9e2a0fb758afecb4431140a71c5c"},"cell_type":"code","source":"output = mc.predict(x=x_testC)\ndiffs  = np.squeeze(output)-y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c5752b124c7966f9c4b1d318ad47a0cde1d9c2a","_kg_hide-input":true},"cell_type":"code","source":"p1=plt.hist(diffs, bins=9)\np1=plt.title('Rating prediction error histogram')\np1=plt.xlabel('Rating prediction error (RPE)')\np1=plt.ylabel('Quantity [n]')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67fd95357655e89a8bfefc6564a95f5d7e1af0a5","_kg_hide-input":true},"cell_type":"code","source":"abs_diff = np.abs(diffs)\nqu, vals = np.histogram(abs_diff, range=[0, 5], bins=20)\np2=plt.bar(np.arange(0,5,0.25),qu)\np2=plt.title('Absolute differences histogram')\np2=plt.xlabel('Absolute Rating prediction error (ARPE)')\np2=plt.ylabel('Quantity [n]')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29b9f7be23bea36d8fa6d534e4929398b0d735ad","_kg_hide-input":true},"cell_type":"code","source":"p3=plt.plot(np.arange(0,5,0.25),100*np.cumsum(qu)/len(abs_diff), 'b-o')\np3=plt.grid()\np3=plt.xlabel('Absolute Rating prediction error (ARPE)')\np3=plt.ylabel('Percentage of Ratings recognized\\nwith error less than given ARPE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21fd7398ab13352cc08478e0f9ec2038c6703dbc"},"cell_type":"code","source":"p4=plt.scatter(df['n_tokens'].loc[indices][validation_samples:].tolist(), np.abs(np.squeeze(output)-y_test), alpha=0.15)\np4=plt.xlabel('Review Text length (words) [n]')\np4=plt.ylabel('Absolute Rating prediction error (ARPE)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee8ab18da51f71c9ed73c72fdbb2df0ae15d347a","_kg_hide-input":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,np.round(output))\nprint('Confusion matrix:')\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81f6b045d824c215c9411290005c33fc6bdedae9"},"cell_type":"markdown","source":"What we got above in the confusion matrix may not look superb but it is a good improvement in comparison to training w/o class_weight=class_weight_dict."},{"metadata":{"_uuid":"8569cc95aff46b94ab5f150eeb748a145868d965"},"cell_type":"markdown","source":"Before checking the predicted Rating for selected examples, let's see if the decoded testing set (x_test_C) really contains the expected line:"},{"metadata":{"trusted":true,"_uuid":"40c1c194d81fbfafb390bffbc2dde5bf6d5cf5cf"},"cell_type":"code","source":"first_words = 12\ntext_concat = []\nfor i, word in enumerate(x_testC[0,:]):\n    if word == 0 or i == first_words: # if special character or we don't need more words\n        break\n    text_concat.append(t.index_word[word])    \n\nprint('Original text:\\t',' '.join(x_test_text[0].split(' ')[0:first_words])) # from df['Review Text']\nprint('From tokens:\\t',  ' '.join(text_concat))\nprint('Tokens: \\t', x_testC[0,:first_words])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4243a7c45322bb3b8fab9b4b3fbd1a2992fd113e"},"cell_type":"markdown","source":"The last thing to do is to take several text reviews from the testing set and to display them along with the associated customer rating and the rating predicted by the RNN network:"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"6aaddb61c24a88dd66ececc9ee4c4606c8f3374b"},"cell_type":"code","source":"for i in range(20):\n    print('---------------------------')\n    print('\"'+x_test_text[i]+'\"')\n    print('Rating:\\t\\t{0}'.format(y_test[i]))\n    print('Prediction:\\t{0:.3}'.format(output[i][0]))\n    print('Is correct: \\t{0}'.format(True if np.abs(output[i][0]-y_test[i])<0.5 else False))\n    print('Is close: \\t{0}'.format(True if np.abs(output[i][0]-y_test[i])<1.0 else False))\n    ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}