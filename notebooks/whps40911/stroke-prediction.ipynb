{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Stroke Prediction - NYCU Midterm Project","metadata":{}},{"cell_type":"markdown","source":"## Import Packages\nImport all necessary packages","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\nimport statsmodels.formula.api as smf\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import ensemble, preprocessing\nfrom xgboost.sklearn import XGBClassifier","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocess\nRead datasets and take a brief look.","metadata":{}},{"cell_type":"code","source":"# load data\ndf = pd.read_csv('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\ndf.info()\ndf.describe(include='all')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convert categorical datas to objects.<br>\nFill missing values in `bmi`.<br>\nRename `Residence_type` to `residence_type`.","metadata":{}},{"cell_type":"code","source":"# Convert dtype\ndf['stroke'] = df['stroke'].astype(object)\ndf['hypertension'] = df['hypertension'].astype(object)\ndf['heart_disease'] = df['heart_disease'].astype(object)\n\n# BMI missing value\ndf[\"bmi\"] = df[\"bmi\"].fillna(df[\"bmi\"].mean())\n\n# rename columns\ndf.rename(columns = {'Residence_type':'residence_type'}, inplace = True)\n\n# Drop id columns\ndf = df.drop(columns=[\"id\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Descriptive Statistics\nUse seaborn package to plot all categorical and numerical columns.","metadata":{}},{"cell_type":"code","source":"cat_data = [x for x in df.columns if df[x].dtype == \"object\"]\nnum_data = [y for y in df.columns if df[y].dtype != \"object\"]\n\nfor col in cat_data:\n    plt.title(col)\n    sns.countplot(df[col])\n    plt.show()\n\nfor col in num_data:\n    plt.title(col)\n    sns.histplot(df[col],kde=True)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression\nPerform logistic regression analysis on all fields.<br>\nSelect fields with significant differences for future analysis.","metadata":{}},{"cell_type":"code","source":"df['stroke'] = df['stroke'].astype(int)\nall_col = ['gender', 'age', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'residence_type', 'avg_glucose_level', 'bmi', 'smoking_status']\n\nY = 'stroke ~ gender'\nfor i in all_col[1:]:\n    Y = Y + '+' + i \nresults = smf.ols(Y, data=df).fit()\nprint(results.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorical Feature","metadata":{}},{"cell_type":"code","source":"gender = pd.get_dummies(df[['gender']])\nwork_type = pd.get_dummies(df[['work_type']])\n\n# ever_married\nmapping = {'Yes':1, 'No':0}\never_married = df['ever_married'].map(mapping)\n\n# residence_type\nmapping = {'Urban':1, 'Rural':0}\nresidence_type = df['residence_type'].map(mapping)\n\n# smoking_status\nmapping = {'smokes':3, 'formerly smoked':2, 'never smoked':1, 'Unknown':0}\nsmoking_status = df['smoking_status'].map(mapping)\n\ndf_combine = pd.concat([gender,\n                        df['age'],\n                        df['hypertension'],\n                        df['heart_disease'],\n                        ever_married,\n                        work_type,\n                        residence_type,\n                        df['avg_glucose_level'],\n                        df['bmi'],\n                        smoking_status, \n                        df['stroke']], axis=1)\n\ndf_select = pd.concat([df['age'],\n                       df['hypertension'],\n                       df['heart_disease'],\n                       ever_married,\n                       work_type,\n                       df['avg_glucose_level'], \n                       df['stroke']], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sampling\nIn order to avoid the huge difference data amounts between `stroke = 0` and `stroke = 1`,<br>\nwe select all datas from `stroke = 1` and randomly sample twice the amount of `stroke = 1`'s data from `stroke = 0`.","metadata":{}},{"cell_type":"code","source":"stroke_cnt = df_combine['stroke'].loc[df_combine['stroke']==1].count()\n\ndf_combine_equal = df_combine.loc[df_combine['stroke']==0].sample(n=stroke_cnt*2, random_state=1)\ndf_combine_equal = pd.concat([df_combine_equal, df_combine.loc[df_combine['stroke']==1]], axis=0)\n\ndf_select_equal = df_select.loc[df_select['stroke']==0].sample(n=stroke_cnt*2, random_state=1)\ndf_select_equal = pd.concat([df_select_equal, df_select.loc[df_select['stroke']==1]], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\nsns.heatmap(df_combine.corr(),annot=True,cmap=\"rainbow\")\nplt.title(\"Correleation Heatmap\",fontsize=20,color=\"c\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation Function\nCustomized function for result evaluation and result visualization.","metadata":{}},{"cell_type":"code","source":"def plot_cm(model, Y_test, Y_pred):\n    cm = metrics.confusion_matrix(Y_test, Y_pred)\n    \n    f, axs = plt.subplots(2,1,figsize=(5,10))\n    ax= plt.subplot(211)\n    sns.heatmap(cm, annot=True, fmt='g', ax=ax, cmap=\"YlGnBu\");\n    ax.set_title(model);\n    ax.set_xlabel('Predicted labels');\n    ax.set_ylabel('True labels');\n    \n    ax1= plt.subplot(212)\n    ax1.set_position([0.1, 0.13, 0.7, 0.6])\n    data=[['Accuracy:', round(metrics.accuracy_score(Y_test, Y_pred), 4)],\n          ['Precision:',round(metrics.precision_score(Y_test, Y_pred), 4)],\n          ['Recall:',round(metrics.recall_score(Y_test, Y_pred), 4)],\n          ['F1 Score:',round(metrics.f1_score(Y_test, Y_pred, average='weighted', labels=np.unique(Y_pred)), 4)]]\n    ax1.axis('tight')\n    ax1.axis('off')\n    ax1.table(cellText=data,loc=\"center\").scale(1, 1.5)\n\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models\n1. XGBoost Classifier\n1. Decision Tree Classifier\n1. Support Vector Machine\n1. Simple Neural Network","metadata":{}},{"cell_type":"markdown","source":"All model's test-train split ratio were set to 0.2","metadata":{}},{"cell_type":"code","source":"from keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.callbacks import EarlyStopping\n\ndef compile_nn_model(df_for_shape):\n    model = Sequential()\n    model.add(Dense(16, input_dim=df_for_shape.shape[1], activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(16, activation='relu'))\n    model.add(Dense(2, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combined_model(df_input):\n    \n    # Test train split\n    X = df_input.iloc[:, :-1].values\n    Y = df_input.iloc[:, -1].values\n\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=100)\n\n    # XGBClassifier\n    model = 'XGBClassifier'\n    xgb = XGBClassifier(\n        learning_rate= 0.01,\n        n_estimators=1000, \n        use_label_encoder =False)\n    xgb.fit(X_train,Y_train,eval_metric='auc')\n    Y_pred = xgb.predict(X_test)\n    plot_cm(model, Y_test, Y_pred)\n\n    # DecisionTreeClassifier\n    model = 'DecisionTreeClassifier'\n    clf = DecisionTreeClassifier(criterion=\"entropy\",\n                                 max_depth=7)\n    clf.fit(X_train,Y_train)\n    Y_pred = clf.predict(X_test)    \n    plot_cm(model, Y_test, Y_pred)\n\n    # RandomForestClassifier\n    model = 'RandomForestClassifier'\n    forest = ensemble.RandomForestClassifier(n_estimators = 1000)\n    forest.fit(X_train,Y_train)\n    Y_pred = forest.predict(X_test)\n    plot_cm(model, Y_test, Y_pred)\n\n    # SVM\n    model = 'SupportVectorMachine'\n    svc = SVC()\n    svc.fit(X_train,Y_train)\n    Y_pred = svc.predict(X_test)\n    plot_cm(model, Y_test, Y_pred)\n    \n    # SimpleNeuralNetwork\n    df_input = df_input.astype('float32')\n\n    X = df_input.iloc[:, 1:-1].values\n    Y = df_input.iloc[:, -1].values\n\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=100)\n    Y_train = np_utils.to_categorical(Y_train)\n\n    model = compile_nn_model(X_test)\n    callback = EarlyStopping(monitor='f1', patience=3)\n    history = model.fit(X_train, Y_train, epochs=150, batch_size=10, verbose=0, callbacks=[callback])\n    plt.plot(history.history['loss'])\n    plt.xlabel('epochs')\n    plt.ylabel('loss')\n    plt.show()\n\n    Y_pred = model.predict(X_test)\n    Y_pred = np.argmax(Y_pred, axis=1)\n\n    model = 'SimpleNeuralNetwork'\n    plot_cm(model, Y_test, Y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results","metadata":{}},{"cell_type":"markdown","source":"### Results for`df_combine` dataset","metadata":{}},{"cell_type":"code","source":"combined_model(df_combine)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results for`df_select` dataset","metadata":{}},{"cell_type":"code","source":"combined_model(df_select)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results for`df_combine_equal` dataset","metadata":{}},{"cell_type":"code","source":"combined_model(df_combine_equal)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results for`df_combine_equal` dataset","metadata":{}},{"cell_type":"code","source":"combined_model(df_select_equal)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}