{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(2018)\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"authors = pd.read_csv(\"/kaggle/input/nips-papers-1987-2019-updated/authors.csv\")\npapers  = pd.read_csv(\"/kaggle/input/nips-papers-1987-2019-updated/papers.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Have a look at the both of the data files that we read."},{"metadata":{"trusted":true},"cell_type":"code","source":"authors.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"papers.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Details about the Data\nDetailed information about the data with the number of columns, type of the column, and number of null entries in each column."},{"metadata":{"trusted":true},"cell_type":"code","source":"papers.info()\n\npapers.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"authors.info()\n\nauthors.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Handling the missing Values.\nSince we are taking abstract column and bbuilding a topic model on it, null values in the column is of no use to train the model. Since, you can't replace any value in place of the missing value it's better to drop the rows with missing entries for abstract."},{"metadata":{"trusted":true},"cell_type":"code","source":"papers_with_abstract = papers.dropna(subset = [\"abstract\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"papers_with_abstract.info()\n\npapers_with_abstract.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"papers_with_abstract.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocess the Data\n  following simple steps to prepare the raw text for model to get better results.  \n+ Bringing all the text to lower case.\n+ Considering only the words in the text by removing all the punctuations and non alphabetic words.\n+ Using porterstemmer, getting the stem of the word.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\n# nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nstop_words = set(stopwords.words('english'))\nporter_stemmer = PorterStemmer()\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(\"[^a-z A-Z]\", ' ', text)\n    text = [porter_stemmer.stem(word) for word in text.split(' ') if not word in stop_words and word != '']\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_abstract = papers_with_abstract[\"abstract\"].map(preprocess_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_abstract","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the Model\nUsing gensim to build the Latent Dirichlet Allocation (LDA) as topic model to find the top words of the topic to which the document belongs to."},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install gensim\nfrom gensim import corpora, models, similarities","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prepare the token dictionary out of abstract text, which is used later in creating the Bag of Words corpus for the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"token_dict = corpora.Dictionary(processed_abstract)\nprint(token_dict)\ntoken_dict.filter_extremes(no_below=5, no_above=0.2, keep_n=110000)\nprint(token_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Bow_corpus = [token_dict.doc2bow(doc) for doc in processed_abstract]\n# print(Bow_corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sam = Bow_corpus[1]\n# for i in range(len(sam)):\n#     print(my_dict[sam[i][0]], sam[i][1])\n# tfidf = models.TfidfModel(Bow_corpus)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### LDA Model with 10 toics"},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_model = models.LdaMulticore( corpus= Bow_corpus,\n                                num_topics=10,\n                                id2word= token_dict,\n                                random_state = 1,\n                                passes= 2,\n                                workers = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc_topics = [lda_model[c] for c in Bow_corpus]\nlen(doc_topics) == len(processed_abstract)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_abstract = pd.DataFrame(processed_abstract)\nprocessed_abstract.loc[:,\"high_probable_topic\"] = np.nan\nprocessed_abstract.loc[:,\"topic_probability\"] = np.nan\nprocessed_abstract.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mapping the topic with highest probbability for the document along with the probability."},{"metadata":{"trusted":true},"cell_type":"code","source":"from operator import itemgetter\nfor idx, doc in enumerate(doc_topics):\n    tmp = max(doc, key = itemgetter(1))\n    processed_abstract.iloc[idx,1] = tmp[0]\n    processed_abstract.iloc[idx,2] = tmp[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_abstract.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Result comprising the topic and probability with the papers data."},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_abstract.columns = [\"p_abstract\", \"high_probable_topic\", \"topic_probability\"]\nresult = pd.concat([papers_with_abstract, processed_abstract], axis = 1, join= \"inner\" )\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Have a look at the top words in the topic"},{"metadata":{"trusted":true},"cell_type":"code","source":"topics = lda_model.show_topics(formatted= False)\ntopics_df = pd.DataFrame(topics)\ntopics_df.columns = [\"topic\", \"top_words\"]\ntopics_df.top_words = topics_df.top_words.apply(lambda x: [word[0] for word in x])\ntopics_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_df = result.loc[:,[\"source_id\", \"high_probable_topic\"]]\noutput_df = pd.merge(output_df, topics_df, left_on= \"high_probable_topic\", right_on= \"topic\", how = \"left\")\noutput_df = output_df.loc[:,[\"source_id\", \"top_words\"]]\noutput_df.to_csv(\"/kaggle/working/output.csv\", index= False)\noutput_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.colors as mcolors\n\ndef plot_wordcloud(topic):\n    cloud = WordCloud(background_color= \"white\",\n                     width= 640,\n                     height= 640,\n                     prefer_horizontal= 1.0)\n    topic_words = dict(topic)\n    cloud.generate_from_frequencies(topic_words, max_font_size= 300)\n    plt.figure(figsize=[12,10])\n    plt.imshow(cloud)\n    plt.axis('off')\n    plt.margins(x= 0, y= 0)\n    plt.show()\n    \ndef plot_wordcloud_abs(abstract):\n    cloud = WordCloud(background_color= \"white\",\n                      width= 1366,\n                      height= 1080,\n                      stopwords= stop_words,\n                      prefer_horizontal= 1.0)\n    cloud.generate_from_text(abstract)\n    plt.figure(figsize=[15,10])\n    plt.imshow(cloud)\n    plt.axis('off')\n    plt.margins(x= 0, y= 0)\n    plt.show()\n\n# No_of docs per topic distribution\ndef plot_topic_dist(data):\n    x_pos = np.arange(len(data.keys()))\n    plt.figure(figsize=[12,8])\n    plt.bar(x_pos, data.values(), color = \"blue\")\n    plt.title('Number of Documents per Topic')\n    plt.xlabel('Topics')\n    plt.ylabel('Number of Documents')\n    \n    plt.xticks(x_pos, data.keys())\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing the Results\n\n##### Distribution of the topics over the Documents."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_topic_dist(result.high_probable_topic.value_counts().to_dict())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Word Cloud for the top words in the topic. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word Cloud for the Topic 8\nplot_wordcloud(topics[8][1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Word Cloud to Highlight most used words in Abstract."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word Cloud for the abstrract \nsam = result.iloc[0,3]\nplot_wordcloud_abs(sam)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}