{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Logistic Regression\n\n_class sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)_\n\nValue of Logistic regression strictly ranges from 0 to 1. Therefore, it can be used in Classification purpose like if Specifed email is Spam (1) or Not-Spam/Ham (0) {OR a tumour is malignant/cancerous or NOT}\n\nIf value is greater than 0.5 (> 0.5) it is classified in class 1 (Spam in this case) and if it is less than 0.5 (< 0.5) it is classified in class 0 (Ham in this case).\n\nIf model is predicting values very close to 1 like 0.90, 0.97, 0.99, etc... means model is strongly definite that it is in class 1 and vice versa.\n\n###### Now, Let us study about Mathematical formula of Logistic Regression\n\nFirst, it is essential to know the formula of Sigmoid function or Logistic Function which is:\n\n<img src=\"https://latex.codecogs.com/gif.latex?%5Cdpi%7B150%7D%20sigmoid%28t%29%20%3D%20%5Cfrac%7B1%7D%7B1%20&plus;%20e%5E%7B-t%7D%7D\" title=\"sigmoid(t) = \\frac{1}{1 + e^{-t}}\" />\n\n\nBasically, Sigmoid Function takes any value (from -$ \\infty $ to +$ \\infty $) and convert it to the range {0, 1}\n\n###### Now, Let's understand sigmoid function by visualization..."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\ndef sigmoid(t):\n    return 1/(1 + np.e**-t)\n\nrange_vals = np.linspace(-10, 10, 50)\n\nsigmoid_values = sigmoid(range_vals)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(range_vals, sigmoid_values)\nplt.title(\"Sigmoid Function\")\nplt.xlabel(\"t\")\nplt.ylabel(\"sigmoid(t)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression is used for Binary Classification as we discussed above then why we call it as Logistic 'Regression'? This is because we use Regression approach and then set a threshold, above or below, we can classify it to a specific class.\n\nLet us consider a completely ficticious dataset of a Person being Obese or NOT by their given Weight.\n\nObese == 1\n\nNot Obese == 0\n\nLet us Plot this..."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = [20, 23, 24, 25, 30, 35, 40, 50, 53, 55, 60, 65, 70] # Weights\ny = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1] # Obese or not","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(X[:7], y[:7], 'bo', color=\"black\")\nplt.plot(X[7:], y[7:], 'bo', color='red')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let us fit Linear Regression in this data..."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(X).reshape(-1, 1)\ny = np.array(y).reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nreg = LinearRegression().fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = reg.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(X[:7], y[:7], 'bo', color=\"black\")\nplt.plot(X[7:], y[7:], 'bo', color='red')\nplt.plot(X, y_pred)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see this is not at all a Good Technique and Linear Regression Fails to do this...\n\nHere, Sigmoid or Logistic Function comes in to play. Let me show you the data and Sigmoid function in one plot and you will automatically understand why is it so"},{"metadata":{"trusted":true},"cell_type":"code","source":"sig_log_vals = sigmoid(np.linspace(-100, 100, 90))\n\nplt.plot(X[:7], y[:7], 'bo', color=\"black\")\nplt.plot(X[7:], y[7:], 'bo', color=\"red\")\nplt.plot(sig_log_vals)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making Logistic Regression Algorithm with Scikit learn"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlogit_reg = LogisticRegression().fit(X, y.ravel())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Coefficient and intercept of fitted model"},{"metadata":{"trusted":true},"cell_type":"code","source":"logit_reg.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit_reg.intercept_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(logit_reg.predict([[10]]))\nprint(logit_reg.predict([[56]]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that our model is Predicting Weight 10 as 'NOT obese' and Weight 55 as 'Obese', which is True as per our data!"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = logit_reg.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Working with Real world Dataset\n\nNow, we will work with Real world dataset and make predictions using Logistic regression\n\n### About Dataset :\n\nThe dataset is publically available on the Kaggle website, and it is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. The classification goal is to predict whether the patient has 10-year risk of future coronary heart disease (CHD).The dataset provides the patients’ information. It includes over 4,000 records and 15 attributes.\n\n### Importing Dataset with Pandas:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(r'../input/heart-disease-prediction-using-logistic-regression/framingham.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### About dataset:\n\n• Sex: male or female(Nominal)\n\n• Age: Age of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)\n\n• Current Smoker: whether or not the patient is a current smoker (Nominal)\n\n• Cigs Per Day: the number of cigarettes that the person smoked on average in one day.(can be considered continuous as one can have any number of cigarettes, even half a cigarette.)\n\n• BP Meds: whether or not the patient was on blood pressure medication (Nominal)\n\n• Prevalent Stroke: whether or not the patient had previously had a stroke (Nominal)\n\n• Prevalent Hyp: whether or not the patient was hypertensive (Nominal)\n\n• Diabetes: whether or not the patient had diabetes (Nominal)\n\n• Tot Chol: total cholesterol level (Continuous)\n\n• Sys BP: systolic blood pressure (Continuous)\n\n• Dia BP: diastolic blood pressure (Continuous)\n\n• BMI: Body Mass Index (Continuous)\n\n• Heart Rate: heart rate (Continuous - In medical research, variables such as heart rate though in fact discrete, yet are considered continuous because of large number of possible values.)\n\n• Glucose: glucose level (Continuous)\n\n• 10 year risk of coronary heart disease CHD (binary: “1”, means “Yes”, “0” means “No”)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see this data has many Null values.\n\nSo we will replace this Null value by the mean. Using this pandas function :"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.fillna(df.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have succesfully got rid of the nan/Null values! Now we can fit our DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:, :-1] # X has all columns except the last column because it is the column we have to make predictions for.\ny = df.iloc[:, -1]  # y has last column\n\nX = np.array(X)\ny = np.array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nX = StandardScaler().fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making model"},{"metadata":{"trusted":true},"cell_type":"code","source":"chd = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chd.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chd.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = chd.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicted Values (top 10):- \\n\")\ny_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                        normalize=False,\n                        title='Confusion matrix',\n                        cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\n\nplot_confusion_matrix(cm, [\"No risk of CHD\", \"Risk of CHD\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model is performing good, but it has predicted 'No risk of CHD' for 117 people which actually have 'Risk of CHD'."},{"metadata":{},"cell_type":"markdown","source":"Thanks for reading!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}