{"cells":[{"metadata":{"_cell_guid":"14ff033f-691f-49e3-b1b2-260b28adfe58","_uuid":"9d5cdd7511862e240273f2687c2d69f3c9bc8568"},"cell_type":"markdown","source":"We begin by importing our dataset and printing the head, for an initial exploratory analysis.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c1984163-fffe-41aa-99a0-243673f1bf51","_uuid":"e1fd66d81a1166c3f8054857e1824c5de671bb09","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport re\nimport numpy as np\nimport collections\nfrom collections import Counter\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom bokeh.io import output_file, show\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource, HoverTool\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\ndf = pd.read_csv('../input/mbti_1.csv')\nprint(df.head(10))\nprint(\"*\"*40)\nprint(df.info())\nprint(type(df[\"type\"][[0]][0][0]))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fb2b5c26-42e1-4b04-8ef4-16999b6ab6f1","_uuid":"84dec5efb299b0d3fb4988a295e8c03087539f26"},"cell_type":"markdown","source":"We can thus see that there are no null inputs\n","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"e8029602-1dc9-4a2a-b4e1-7072f81c565b","_uuid":"ffcbdf4d43d2017e187f58a69d58cde88f5206b2","collapsed":true,"trusted":true},"cell_type":"code","source":"df['words_per_comment'] = df['posts'].apply(lambda x: len(x.split())/50)\nprint(df.head())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f2a405e1-afa0-4dbb-9ee8-1164e1be07bb","_uuid":"d1a10157637263e2867e24564a19368ea371708a"},"cell_type":"markdown","source":"Since we are dealing with the last 50 posts, we can divide the length of the amount of words by 50 to see an average of words per comment, and we create a new column for it.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"29fc9dd1-39f2-49fa-8034-a887def80113","_uuid":"903c6696d37f517ec51d35309e902e46daf2d789","collapsed":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n#sns.swarmplot(\"type\", \"words_per_comment\", data=df)\nsns.violinplot(x='type', y='words_per_comment', data=df, inner=None, color='lightgray')\nsns.stripplot(x='type', y='words_per_comment', data=df, size=4, jitter=True)\nplt.ylabel('Words per comment')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"437179f6-a512-4b4a-8e6f-f2dfbb2acd9b","_uuid":"bd3325583d5e73339f96203bdeec8aa935aaa4ce","collapsed":true,"trusted":true},"cell_type":"code","source":"df.groupby('type').agg({'words_per_comment':'mean'})","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"39a06823-5015-4e35-a89d-2429c4cad6b3","_uuid":"fa3468b683d0e90ea571c80696385ee4cfe70c4e"},"cell_type":"markdown","source":"We have constructed a violin plot and checked the mean words per comment, and the second feature does not show anything promising right now. Let's see what else we can do.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"70f0df4b-0310-4724-b488-fa7520f68e79","_uuid":"6f02709893398e2b1fcd300a8fa578b2b236ada1","trusted":true},"cell_type":"code","source":"for r in range(len(df['posts'])):\n    \n    for i in df[\"type\"][[r]]:\n        if i[0]==\"I\":\n            df[\"I-E\"][[r]] = 0\n        if i[0] == \"E\":\n            df[\"I-E\"] = 1\n        if i[1]==\"N\":\n            df[\"N-S\"] = 0\n        if i[1] == \"S\":\n            df[\"N-S\"] = 1\n        if i[2]==\"F\":\n            df[\"F-T\"] = 0\n        if i[2] == \"T\":\n            df[\"F-T\"] = 1\n        if i[3]==\"J\":\n            df[\"J-P\"] = 0\n        if i[3] == \"P\":\n            df[\"J-P\"] = 1\n\ndf['http_per_comment'] = df['posts'].apply(lambda x: x.count('http')/50)\ndf['music_per_comment'] = df['posts'].apply(lambda x: x.count('music')/50)\ndf['question_per_comment'] = df['posts'].apply(lambda x: x.count('?')/50)\ndf['img_per_comment'] = df['posts'].apply(lambda x: x.count('jpg')/50)\ndf['excl_per_comment'] = df['posts'].apply(lambda x: x.count('!')/50)\ndf['ellipsis_per_comment'] = df['posts'].apply(lambda x: x.count('...')/50)\nprint(df.head(10))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c321554f-c8d1-4398-a59d-c12ef31e4740","_uuid":"dd7d395f798bd7b7669ec244e86b93a9a79baf1f","collapsed":true,"trusted":true},"cell_type":"code","source":"print(df.groupby('type').agg({'http_per_comment':'mean'}))\nprint(df.groupby('type').agg({'music_per_comment':'mean'}))\nprint(df.groupby('type').agg({'question_per_comment':'mean'}))\nprint(df.groupby('type').agg({'img_per_comment':'mean'}))\nprint(df.groupby('type').agg({'excl_per_comment':'mean'}))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"08bbe211-a91c-45b3-9330-016dbecfca96","_uuid":"a72478621467a2a67d0f561a220e26af50e37ea2"},"cell_type":"markdown","source":"There's quite a lot of information there. We can do joint plots, pair plots and heat maps to explore relationship between data. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"a5365240-b4f1-45b9-801f-639155108f45","_uuid":"2a92888c4fae4668421f151609c47fde41764d22","collapsed":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.jointplot(x='words_per_comment', y='ellipsis_per_comment', data=df, kind='kde')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a14470ad-642a-4640-a64b-c6184c426f32","_uuid":"edf176de9553085fdecbd60a477ed77aa6f7ceeb"},"cell_type":"markdown","source":"So it seems there's a large correlation between words per comment ant the ellipsis the user types per comment!","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"7820e7c7-a4c8-4249-ae87-dae21a39608f","_uuid":"7b86180aa6b7409252aecb45ad065fd3faa49718","collapsed":true,"trusted":true},"cell_type":"code","source":"i = df['type'].unique()\nk = 0\nfor m in range(0,2):\n    for n in range(0,6):\n        df_2 = df[df['type'] == i[k]]\n        sns.jointplot(x='words_per_comment', y='ellipsis_per_comment', data=df_2, kind=\"hex\")\n        plt.title(i[k])\n        k+=1\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dcdb7444-2da6-4660-acfe-5a9321a3af35","_uuid":"831d063f8b916840403709f51736ad616aad99e8","collapsed":true,"trusted":true},"cell_type":"code","source":"\n\ni = df['type'].unique()\nk = 0\nTypeArray = []\nPearArray=[]\nfor m in range(0,2):\n    for n in range(0,6):\n        df_2 = df[df['type'] == i[k]]\n        pearsoncoef1=np.corrcoef(x=df_2['words_per_comment'], y=df_2['ellipsis_per_comment'])\n        pear=pearsoncoef1[1][0]\n        print(pear)\n        TypeArray.append(i[k])\n        PearArray.append(pear)\n        k+=1\n\n\nTypeArray = [x for _,x in sorted(zip(PearArray,TypeArray))]\nPearArray = sorted(PearArray, reverse=True)\nprint(PearArray)\nprint(TypeArray)\nplt.scatter(TypeArray, PearArray)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6441ac53-9d10-4312-9191-0f0ecd440f2e","_uuid":"1488ee7975b272f31b284e6b7e6c8580042c150d"},"cell_type":"markdown","source":"Let's do some Machine Learning now!","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"0acd5e9e-22e6-4f4f-afe0-76e2a54507e2","_uuid":"8c0d4b61a57e057d26dc858c317bab32d9541580","collapsed":true,"trusted":true},"cell_type":"code","source":"X = df.drop(['type','posts'], axis=1).values\ny = df['type'].values\n\nprint(y.shape)\nprint(X.shape)\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size = 0.1, random_state=5)\nknn=KNeighborsClassifier(n_neighbors=14)\n#knn.fit(X_train,y_train)\n#pred=knn.predict(X_test)\n#print(\"accuracy {:.2f}\".format(np.mean(pred==y_test)))\n#print(knn.score(X_test, y_test))\n\nclf = OneVsRestClassifier(LogisticRegression())\nclf.fit(X_train, y_train)\npred=clf.predict(X_test)\nprint(\"accuracy {:.2f}\".format(np.mean(pred==y_test)))\nprint(clf.score(X_test, y_test))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9375705f-79f3-4cb1-8d5a-16db200da250","_uuid":"4fc8d8f3cef8422c708cd10da7b38762ab729c01"},"cell_type":"markdown","source":"So we see our model has an accuracy of 0.20... which is not very good. Let's see what else we can do!","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"f5feb407-e16a-4bfd-ab8c-90426336d3b8","_uuid":"780fd3061b547a9c370c978648fde337bf47b332","collapsed":true,"trusted":true},"cell_type":"code","source":"nova_columna=[]\nfor z in range(len(df['posts'])):\n    prov=df['posts'][z]\n    prov2= re.sub(r'[“€â.|,?!)(1234567890:/-]', '', prov)\n    prov3 = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', prov)\n    prov4 = re.sub(r'[|||)(?.,:1234567890!]',' ',prov3)\n    prov5 = re.sub(' +',' ', prov4)\n    prov6 = prov5.split(\" \")\n    counter = Counter(prov6)\n    counter2 = counter.most_common(1)[0][0]\n    nova_columna.append(counter2)\ndf['most_used_word'] = nova_columna\nprint(df.head())\nprint(df['most_used_word'].unique())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}