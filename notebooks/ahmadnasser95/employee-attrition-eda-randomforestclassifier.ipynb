{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Preface\n\nHello, I am an aspiring HR Analyst who has been self-learning python for a few months now. I am hoping that through creating these notebooks for datasets that I find on Kaggle I can improve my python, data analysis, and machine learning algorithm skills. \n\nAny feedback would be greatly appreciated.\n\nNow onto the actual project.\n\n## Introduction\n\nMost, if not all companies invest significant resources into acquiring and training their employees, thus it is always important for them to retain the talent they have invested so much in. Having employees leave your company would mean that even more resources need to be poured into their replacements, who also face the same risks. \n\nThus, it is important for companies to be able to predict employee attrition in order to develop strategies to reduce the phenomena. \n\nIn this kaggle notebook, we will do the following:\n\n* **Exploratory Data Analysis** - Exploring the data and how features correlate to one another.\n* **Feature Engineering** - in order to prepare our categorical data for our machine learning model.\n* **Machine Learning Model Implementation** - Implementing a Random Forest Classifier model for our data.\n\n\n","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#importing the usual libraries for EDA\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Exploratory Data Analysis\n\nThe first step when tackling any dataset. We must first take a look at our data, explore the relationship between its features, and make some observations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading the data into a DataFrame\ndf = pd.read_csv('../input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#taking a peek into the DataFrame\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting some more information about the dataset\ndf.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking for null/missing values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()\n\n#seems that there are some categorical columns in this df, let's explore them","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Exploring the target feature\ndf['Attrition'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering\n\nBefore carrying on with EDA, I would like to convert categorical features into numerical ones through one of the many methods for doing so. This would help give a clearer idea of what's going on with the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's assign 1s and 0s to the Attrition column\ndf['Attrition'].replace(to_replace = dict(Yes = 1, No = 0), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Assigning categorical features to 'categorical_cols'\ncategorical_cols = []\nfor col, value in df.iteritems():\n    if value.dtype == 'object':\n        categorical_cols.append(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#storing these columns in a new dataframe called df_cat\ndf_cat = df[categorical_cols]\ndf_cat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#taking a peek at the unique values in each of the categorical columns\nfor column in categorical_cols:\n    print(f\"{column} : {df[column].unique()}\")\n    print(\"-\"*40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#assigning numerical variables to our categorical data through sklearn's LabelEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel = LabelEncoder()\nfor column in categorical_cols:\n    df[column] = label.fit_transform(df[column])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking our new DataFrame with numerical values\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we can carry on with our EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(figsize=(20, 20));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from looking at the histogram of the attrition feature, we can quickly notice that attrition is heavily skewed towards 0, meaning there is a lot less people that leave the company. Nonetheless, It is still important to learn why those that leave do so in order to develop strategies to retain them. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting some countplots, splitting on Attrition\nplt.figure(figsize=(20,20))\n\nplt.subplot(421)\nsns.countplot(x='Age',data=df,hue='Attrition')\nplt.subplot(422)\nsns.countplot(x='OverTime', data=df, hue='Attrition')\nplt.subplot(423)\nsns.countplot(x='MaritalStatus', data=df, hue='Attrition')\nplt.subplot(424)\nsns.countplot(x='JobRole', data=df, hue='Attrition')\nplt.subplot(425)\nsns.countplot(x='JobLevel', data=df, hue='Attrition')\nplt.subplot(426)\nsns.countplot(x='JobSatisfaction', data=df, hue='Attrition')\nplt.subplot(427)\nsns.countplot(x='TotalWorkingYears', data=df, hue='Attrition')\nplt.subplot(428)\nsns.countplot(x='WorkLifeBalance', data=df, hue='Attrition')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some observations that can be made:\n\n 1. Almost 50% of employees who work overtime end up leaving the company.\n 2. JobLevel = 1 has the highest percentage of attrition (Approx. 26%).\n 3. Almost 50% of employees with TotalWorkingYears = 1 end up leaving the company.\n 4. Laboratory Technicians have the highest percentage of attrition.\n 5. Single employees are more likely to leave the company.\n 6. Employees with a lower JobSatisfaction level are more likely to leave the company.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's get rid of the StandardHours, EmployeeCount and Over18 column, as all rows have the same value.\n\ndf.drop(['StandardHours','Over18','EmployeeCount','EmployeeNumber'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#this will be quite a large heatmap, but will be worth taking a look at to spot correlated features\nplt.figure(figsize=(25,25))\nsns.heatmap(df.corr(),annot=True,cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the heatmap, we remark the following:\n\n- Age is correlated with several features, including: NumCompaniesWorked, MonthlyIncome, JobLevel, Education, and other more obvious features such relating to seniority.\n\n- Attrition has some negative correlation with the following features: YearsWithCurrManager, YearsInCurrentRole, YearsAtCompany, TotalWorkingYears, StockOptionLevel, MonthlyIncome, JobLevel, JobInvolvement, EnvironmentSatisfaction, and Age. Attrition is also correlated with OverTime\n\n- As expected, JobLevel is perfectly correlated with monthly income. It is also highly correlated TotalWorkingYears, i.e. work experience.\n\n- Job satisfaction seems to have no correlation with any of the other features.\n\n- Performance Rating is highly correlated with PercentSalaryHike, i.e. high performance earn better raises.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the dataset\ndf_final = df.drop('Attrition',axis=1)\ny = df['Attrition']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling the data\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\nX = scaler.fit_transform(df_final)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Machine Learning Model Implementation\n\nNow that we've explored our data and converted categorical data into numerical data, we can now move forward with the implementation of our ML model.\n\nfor the puropose of this classification task, I've opted for the implementation of a Random Forest Classifier, as it combines the predictive powers of decision trees in order to create a more accurate model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#import the train_test_split model\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=71)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#initializing the RFC object\nrfc = RandomForestClassifier(n_estimators=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting the data\nrfc.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#making the predictions\npredictions = rfc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing some reporting tools\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('classification report: ')\nprint('='*40)\nprint(classification_report(y_test,predictions))\nprint('\\n')\nprint('confusion matrix: ')\nprint('='*40)\nprint(confusion_matrix(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see that our model did an alright job with an accuracy of 85%.\n\nAs noted before, there exists a significant imbalance between the count of each of the two attrition values. Let's see if we can improve our model using SMOTE. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=71)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sm = SMOTE(random_state=71)\nX_train, y_train = sm.fit_sample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = rfc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('classification report: ')\nprint('='*40)\nprint(classification_report(y_test,predictions))\nprint('\\n')\nprint('confusion matrix: ')\nprint('='*40)\nprint(confusion_matrix(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we observe that using SMOTE actually made our accuracy worse and only slightly improved our True Negative classifications. I am not sure why this is the case, but perhaps a more experienced/knowledgeable individual can point me in the right direction! \n\n- 17/Jul/2020 - EDIT_1: Scaled the data. This seemed to help SMOTE improve the model, but not by much.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Conclusion\n\nIn this notebook, we implemented a  simple pipeline of predicting employee attrition. We went over some EDA, Feature Engineering, and implemented a straightforward Random Forest Classifier with an 85% accuracy score (though I'm sure it can be improved).\n\nOn that note, more features can be derived from the data that might also help improve the model. I will be coming back to this notebook to give it another go as I improve my python skills.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}