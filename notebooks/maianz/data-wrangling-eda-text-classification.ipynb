{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThis project explores the coronavirus related Tweets dataset created by Shane Smith. This dataset contains 20 files  crawled from Internet, 18 of them are Tweet information and 2 of them are location and hashtag information. The main idea of this project is to find out the general trendings of the coronavirus related Tweets and classify the Tweets based on the text. Below are the contents:\n\n* Data wrangling (data cleaning, data imputation)\n* Data visualization (barplots, boxplots, pieplots, correlation heatmap)\n* Information extraction (tf-idf method)\n* Text classification (K-means)"},{"metadata":{},"cell_type":"markdown","source":"## Importing Library\n\nImport all the packages we need for this project."},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing Data"},{"metadata":{},"cell_type":"markdown","source":"This dataset contains 20 files including Tweet information on 18 different days, country information and hashtag information. When load data for this project, I skip some of the file for the following reasons.\n* I discard the Tweet information before March 12 becasue the format is not consistent with Tweet information on other days. \n* I skip the file \"Countires\" because most of the location information in this dataset is missing (please see data wrangling part for details), so it doesn't make too much sense to load this file. \n* File \"Hashtag\" is also not loaded since this file only records one of multuple potential hashtags and the criterion to filter the hashtags is not constent among all the dates.\n\nFor details of the dataset, please see dataset creator's description in discussion section."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This diction is used to store date of data source as string\nTweet_Date = []\n# this dictionary is used to store all data frames of tweet infomation\nTweet_Dict = {}\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nprint(\"--------------------Start loading data--------------------\")\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        # extract file path\n        filepath = os.path.join(dirname, filename)\n        print(filepath, end=\"\")\n        # append file name head as tweet date\n        head = filename.split()[0]\n        head = head.split(\".\")[0]\n        # filter out useless files\n        if head not in [\"2020-03-00\", \"Hashtags\", \"Countries\"]:\n            print(\" -----> loading\")\n            Tweet_Date.append(head)\n            # read csv file and store in dict\n            Tweet_Dict[head] = pd.read_csv(filepath)\n        else:\n            print()\nprint(\"--------------------Finish loading data--------------------\")\n# sort the list\nTweet_Date = sorted(Tweet_Date)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Wrangling"},{"metadata":{},"cell_type":"markdown","source":"This section aims to clean the data and get rid of unnecessary data.\n\nFirst let's compute missing value percentage of the dataset. It can be seen in the matrix below that \"country_code\", \"place_full_name\", \"place_type\" and \"account_lang\" have too many missing values. However, it is acceptable that columns with related to \"reply\" have high missing percentage since there is not information about reply if no reply happens. Note that there is also a few data missing for the \"source\" column. \n\nAfter delete fields with high missing ratio, drop columns containing information about id, reply or time since those are not focus of this project."},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate null value percentage\nstatistics = pd.concat([Tweet_Dict[date].isnull().mean().to_frame(name=date) for date in Tweet_Date], axis=1)\nstatistics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop columns that have too many missing values and useless columns\nfor date in Tweet_Date:\n    Tweet_Dict[date].drop(labels=[\"country_code\", \"place_full_name\", \"place_type\", \"account_lang\", # too many missing values\n                                 \"status_id\", \"user_id\", \"screen_name\",  # useless\n                                 \"reply_to_status_id\", \"reply_to_user_id\", \"reply_to_screen_name\", # useless\n                                 \"account_created_at\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After dropping some columns the dataset seems to be much cleaner and luckily we are able to concatenate all information to one dataframe. Please try using GPU / TPU if you have trouble fitting all the data in your RAM."},{"metadata":{"trusted":true},"cell_type":"code","source":"# concatenate all the data to one dataframe\ndf = pd.concat([Tweet_Dict[date] for date in Tweet_Date], ignore_index=True, sort=False)\nprint(\"Dataframe shape: \", df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The other important thing to do is dropping any duplicated rows. Even though we have already dropped all the id columns, the \"text\" column and \"created_at\" column can ensure that different Tweets would not be recognized as the same."},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop duplicate columns\ndf.drop_duplicates(inplace=True)\nprint(\"Dataframe shape: \", df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recall that we have a few missing values for feature \"source\", now let's fill in all missing values with the mode value of \"source\" column."},{"metadata":{"trusted":true},"cell_type":"code","source":"# impute null values\ndf[\"source\"].fillna(df[\"source\"].mode()[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So far we have finished all data wrangling proedures and have relative clean data. It's time to take a look at overall information of our dataframe and statistics of numeric data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# present statistics\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# present description\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"This section is to perform exploratory data analysis. Hopefully we can get some intuition about the corona Tweet dataset in this section.\n\nYou may wonder since the number of confirmed cases and death case is continuously increasing, whether the number of related Tweet also increase during March. Below shows the number of realted Tweets from March 12 to March 28, however, it doesn't increase as we expect. Instead, the number oscilliates around 700 thousand.\n\nOne instesting thing is that the number of Tweets realted to corona virus is much high than normal on Mar 13. Possible reasons are suspension of NBA and announcement of Europe travel ban by Trump aound March 13."},{"metadata":{"trusted":true},"cell_type":"code","source":"# list to store number of Tweets\nnum_tweet = []\n\n# calculate number of tweets on each day\nfor date in Tweet_Date:\n    num_tweet.append(Tweet_Dict[date].shape[0])\n\n# plot\nplt.figure(figsize=(12, 5))\nplt.bar(Tweet_Date, num_tweet, color=\"lightcoral\")\nplt.xticks(rotation=90)\nplt.xlabel('Date')\nplt.ylabel(\"Count\")\nplt.title('Number of Tweets Trendency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we have only four numeric features, including \"favourites_count\", \"retweet_count\", \"followers_count\" and \"friends_count\", let's draw the boxplots to see how they are distributed.\n\nIt can be observed from the boxplots that the mean values of all these four features are much more closer to the first quartile and the bottom than to the third quartile and the top. It indicates, at least according to this dataset, that the Tweet World is like a pyramid, where top accounts achieve much more attention (favourites, retweets, followers and friends) than normal accounts.\n\nYou may confused that why the max rewteet count is just two. Note that I didn't show outliers in all boxplots because those top outliers are too high to make the plots readable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# configure plot size\nplt.figure(figsize=(14, 5))\n# subplot for favourites_count\nplt.subplot(1,4,1)\ndf.boxplot(column=\"favourites_count\", rot=0, showfliers=False, figsize=(8,6))\n# subplot for retweet_count\nplt.subplot(1,4,2)\ndf.boxplot(column=\"retweet_count\", rot=0, showfliers=False, figsize=(8,6))\n# subplot for followers_count\nplt.subplot(1,4,3)\ndf.boxplot(column=\"followers_count\", rot=0, showfliers=False, figsize=(8,6))\n# subplot for friends_count\nplt.subplot(1,4,4)\ndf.boxplot(column=\"friends_count\", rot=0, showfliers=False, figsize=(8,6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After seeing the distribution of numberic values, let's figure out whether there is a relationship between those features. Below is the correlation matrix which indicates the linear relationship between numeric features (+1 indicates perfect positive correlation, -1 indicates perfect negative correlation, 0 indicates no association). It turns out that there is almost no correlation between any of two numeric features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# configure plot size\nplt.figure(figsize=(10, 6))\n# extract numeric columns\ndf_corr = df[[\"favourites_count\", \"retweet_count\", \"followers_count\", \"friends_count\"]]\n# generate correlation matrix\ncorrMatrix = df_corr.corr()\n# plot heatmap\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Word Cloud is very intuitive when explore text data. Word Cloud below presents the top 50 popular words among all English Tweets. It can be observed that \"COVID19\", \"coronavirus\" and \"outbreak\" are among the most popular words.\n\nOne interesting fact is that \"https\" is also a high-frequency word, which is due to the links that people commonly attach in their Tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform text contents to string variable\ntext = \"\"\nfor date in Tweet_Date:\n    text += str(Tweet_Dict[date][Tweet_Dict[date][\"lang\"]==\"en\"][\"text\"].values)\n# generate word cloud    \nwordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"black\", collocations=False).generate(str(text))\n# plot wordcloud\nplt.figure(figsize=(18, 8))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next is the pie plots show the distribution of boolean feature \"is_quote\" and \"is_retweet\". It can be seen that only a small part of all Tweets is a quote of another Tweet. \n\nHowever, things look wierd for the feature \"is_retweet\" since all of the values are false. According to the dataset creator, the retweet argument has all be set to false so this dataset doesn't included retweeted Tweets. What we have seen before are originally created Tweets that we retweeted by others. Tweets that retweet those will be filtered out."},{"metadata":{"trusted":true},"cell_type":"code","source":"# configure plot size\nplt.figure(figsize=(14, 5))\n# plot for is_quote\nplt.subplot(1,2,1)\nplt.pie([df[\"is_quote\"].mean(), 1-df[\"is_quote\"].mean()], labels=['True', 'False'], shadow=False, startangle=140)\nplt.title(\"is_quote\")\n# plot for is_retweet\nplt.subplot(1,2,2)\nplt.pie([df[\"is_retweet\"].mean(), 1-df[\"is_retweet\"].mean()], labels=['True', 'False'], shadow=False, startangle=140)\nplt.title(\"is_retweet\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally let's take a look at what the top 10 Tweet source and languages.\n\nJust for your reference, top 10 languages are English, Spanish, French, Undetermined Language, Italian, Turkish，Portuguese，German, Hindi and Indian. In this dataset English is in dominating position."},{"metadata":{"trusted":true},"cell_type":"code","source":"# count top 10 sources\ndf[\"source\"].value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count top 10 languages\ndf[\"lang\"].value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering"},{"metadata":{},"cell_type":"markdown","source":"This section focuses on text classification using \"text\" feature in the dataframe. This project only considers English Tweets for convenience. To better understand the over trending of Tweets and also for the consideration of reducing dataset dimensionality, we only analyze Tweet that have favourites over 10 times average sizes.\n\nFirst step is to extract the Tweets we want to use. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# filter out all tweets that are not using english\ndf_en = df[(df[\"lang\"]==\"en\") & (df[\"favourites_count\"]>=df[\"favourites_count\"].mean()*10)]\nprint(\"Number of English Tweets that are above average favourites: \", df_en.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To apply unsurprised machine learning algorithm, we have to transform out text data to numeric arrays. The method we use here is tf-idf, which stands for frequency-inverse document frequency and is commonly used in information retrieval and text mining. Here is the general idea to calculate tf-idf weights (sometimes we will have normalization or add 1 to denominator of idf).\n\n*tf(t) = (Number of times term t appears in a document) / (Total number of terms in the document)*\n\n*idf(t) = log_e(Total number of documents / Number of documents with term t in it)*\n\ntf (term frequency) measures how often the term appears while idf measures how rare the word is. The product of tf and idf is the weight for the word. Here we transform the feature column to the tf-idf weights matrix. Note that in practice the weight matrix can be extremely large, here we only consider the top max_features ordered by term frequency across the corpus."},{"metadata":{"trusted":true},"cell_type":"code","source":"# vectorize the text content\nvectorizer = TfidfVectorizer(max_features=20000, stop_words='english')\nX = vectorizer.fit_transform(df_en[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"shape of tf-idf weight matrix: \", X.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To implement unsurpvised text classification, we will use K-means algorithm. The methodology to find the number of clusters k here is to plot the sum of squared distances (Euclidean distances) between each point and the cluster centroid and find the elbow (elbow method). \n\nNormal batch K-means method is very time consuming. We will instead use Mini batch K-means algorithm to find the optimal number of clusters. The empirical results suggest that Mini batch K-means algorithm can obtain a substantial saving of computational time at the expense of some loss of cluster quality."},{"metadata":{"trusted":true},"cell_type":"code","source":"# range of number of clusters\nnum_clusters = range(2, 22, 2)\n\n# list to record sum of squared distances\nsum_square_error = []\n\n# iterate through different number of clusters and append sse\nfor k in num_clusters:\n        sum_square_error.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=42).fit(X).inertia_)\n        print('now fitting {} clusters using  Mini batch K-means algorithm'.format(k))\n\n# plot ssm vs k\nplt.figure(figsize=(12, 5))\nplt.plot(num_clusters, sum_square_error, \"g^-\")\nplt.xticks(num_clusters)\nplt.xlabel('Number of Clusters')\nplt.ylabel(\"Sum of Square Distance\")\nplt.title('Elbow Method')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that the sum of squared distances does't improve too much when we increase number of clusters. In this situation, our best number of clusters seems to be 8 even though this elbow is far from perfect (and also this number can be different due to randomness in each run). \n\nNow build the model and predict for each sample."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the models and fit \ncluster_predictions = MiniBatchKMeans(n_clusters=8, init_size=1024, batch_size=2048, random_state=42).fit_predict(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To visiualize our clustering results, we use PCA and t-SNE methods to visiualize the clustering on 2D plane. PCA is a technique to reduce the dimensionality of dataset using Singular Value Decomposition of the data to project it to a lower dimensional space. To visiualize the data in 2 dimension, eigenvectors with top 2 highest explained variance are kept.\n\nt-distributed Stochastic Neighbor Embedding is another technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. Different from PCA, it's a non-linear method.\n\nTo speed up the process, here we randomly choose 2000 samples as our input data for PCA and use the top 60 eigenvectors as the input of t-SNE. Finally randomly choose 400 data points for better visiualizon."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_tsne_pca(data, labels):\n    '''\n    This function plots the PCA and t-SNE on 2D plane.\n    args:\n        data: tf-idf weight matrix\n        labels: predictions from K-means\n    '''\n    # initial set up and random pick up samples\n    max_label = max(labels)\n    max_items = np.random.choice(range(data.shape[0]), size=2000, replace=False)\n    \n    # extract eigenvectors that have the most explained variance and feed the eigenvectors to t-SNE\n    pca = PCA(n_components=2).fit_transform(data[max_items,:].todense())\n    tsne = TSNE().fit_transform(PCA(n_components=60).fit_transform(data[max_items,:].todense()))\n    \n    # random pick centain size of data points for visiualization\n    idx = np.random.choice(range(pca.shape[0]), size=400, replace=False)\n    label_subset = labels[max_items]\n    label_subset = [cm.hsv(i/max_label) for i in label_subset[idx]]\n    \n    f, ax = plt.subplots(1, 2, figsize=(14, 6))\n    \n    # plot PCA\n    ax[0].scatter(pca[idx, 0], pca[idx, 1], c=label_subset)\n    ax[0].set_title('PCA Cluster')\n    \n    # plot t-SNE\n    ax[1].scatter(tsne[idx, 0], tsne[idx, 1], c=label_subset)\n    ax[1].set_title('TSNE Cluster')\n\n# plot PCA and t-SNE reduced data\nplot_tsne_pca(X, cluster_predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It turns out that our clustering result is far from perfect based on the PCA and t-SNE visiualization. Some possible solutions are listed in the future plan section.\n\nLast step of this project is to observe the extracted key words from the clusters. Note that we only present top 10 keywords for each cluster based on tf-idf weights.\n\nIt can be observed that most of the keywords are definitely related to coronavirus. What we expect is to find a pattern for each cluster, like positive/negative attitudes, politics or everyday life and so on. It seems that we cannot find those partterns easily with just top 10 keywords. However if we take a close look we can still find something, like clusters with topics like Trump, China can be distinguished from other clusters and key words like support, friends suggest a positive attitude."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_keywords(data, clusters, labels, n_terms):\n    '''\n    This function displays the top keywords based on tf-idf score.\n    '''\n    # group tf-idf array based on predictions\n    df = pd.DataFrame(data.todense()).groupby(clusters).mean()\n    \n    # loop through each clusters and print top 10 score words\n    for i,r in df.iterrows():\n        print('\\nCluster {}'.format(i))\n        print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]))\n\n# run the code\nget_top_keywords(X, cluster_predictions, vectorizer.get_feature_names(), 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Challenges"},{"metadata":{},"cell_type":"markdown","source":"In this project we face the following challenges:\n\n* Normal batch K-means algorithm is too slow to run and instead we use mini batch K-means, which is at the expense of clustering quality. As a result, our clustering doesn't converge very well.\n* Limit of RAM makes it hard for us to analyze data in bigger scale. One possible solution is listed in future plan section.\n* Even though we find the relative optimal number of clusters using elbow method, other possible choices have very close sum of squared distances. \n* It's hard to find the actual pattern of each cluster."},{"metadata":{},"cell_type":"markdown","source":"# Future Plan"},{"metadata":{},"cell_type":"markdown","source":"Here list a couple of things we can explore in the future.\n\n* Once we have the all the hashtags for the given dataset, we can try train a surpvised machine learning model to predict the hashtag of certain Tweets. One thing to notice is that each sample may have multiple labels, which is different from traditional surprised learning.\n\n* For convenience we only analyze Tweets in English in our K-means model. We can try to use any traslation packages or take a deeper look at the Tweets in other languages.\n\n* Due to the storage limit of RAM we cannot analyze data in bigger scale. AWS cloud may be a good choice if we want to manipulate big data.\n\n* Other features and methods can be included in the clustering process, which can probably help better converage the model."},{"metadata":{},"cell_type":"markdown","source":"# Acknowledge"},{"metadata":{},"cell_type":"markdown","source":"Thank Shane Smith for creating this dataset and answering my question.\nThank John B for his notebook on unsurprised text classification, which gives me lots of inspirations for this project."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}