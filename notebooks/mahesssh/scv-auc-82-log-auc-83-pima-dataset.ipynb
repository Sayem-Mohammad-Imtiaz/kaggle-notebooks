{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv', header=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_test=dataset.copy(deep=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig,ax = plt.subplots(figsize=(5,13))\nax.pie(dataset_test['Outcome'].value_counts(),labels=['0','1'] ,colors=['red','green'],autopct='%1.2f%%')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_test.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(dataset_test==0).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There are features that have 0. We need convert 0 to 'Nan'.  **"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfor i in range(len(dataset_test.columns)-1):\n    dataset_test.iloc[:,i]=dataset_test.iloc[:,i].replace(0,np.NaN)\n#     print(test[i])\n(dataset_test==0).sum()\n# dataset.iloc[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_test.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.plot(dataset['Pregnancies'],np.zeros_like(dataset['Pregnancies']),'o')\n# plt.plot(dataset['Glucose'],np.zeros_like(dataset['Pregnancies']),'o')\n# plt.plot(dataset['BloodPressure'],np.zeros_like(dataset['BloodPressure']),'o')\n# plt.plot(dataset['SkinThickness'],np.zeros_like(dataset['SkinThickness']),'o')\n# plt.plot(dataset['Insulin'],np.zeros_like(dataset['Insulin']),'o')\n# plt.plot(dataset['BMI'],np.zeros_like(dataset['BMI']),'o')\n# plt.show()\nimport seaborn as sns\ndef univariate(feat):\n#     plt.plot(dataset[feat],np.zeros_like(dataset[feat]),'o') ##scatter plot\n#     plt.xlabel(feat)\n#     plt.show()\n# #     plt.scatter(dataset.index,dataset[feat])\n#     sns.scatterplot(dataset.index,dataset[feat],hue=dataset['Outcome']) ##scatter plot\n#     plt.xlabel(feat)\n#     plt.show()\n#     plt.hist(dataset[feat]) # hist\n#     plt.show()\n#     dataset[feat].plot(kind='density') # kernel density function plot is probability density function (PDF)\n#     plt.show()\n#     dataset[feat].plot.kde()  ## kernel density function plot is probability density function (PDF)\n#     plt.show()\n    sns.distplot(dataset[feat],kde=True,hist=True,rug=False)  # dist plot (combine density plott , rug plot and hist plot)\n    plt.show()\n\n\n#     plt.boxplot(dataset[feat]) # bloxplot\n#     plt.show()\n\n    \n#     sns.stripplot(x=dataset['Outcome'],y=dataset[feat])\n#     plt.show()\n#     ls=sns.load_dataset(X)\n#     sns.kdeplot(data=X,x=X[feat])\n#     plt.show()\n    \n   \n\n    \nfor i in dataset.columns:\n    univariate(i)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Bvariate EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\ndef bivariate(feat1, feat2):\n\n#     sns.scatterplot(X[feat1],X[feat2],hue=y)\n#     plt.show()\n    \n#     sns.jointplot(data=X,x=feat1,y=feat2)\n#     plt.show()\n    sns.pairplot(dataset_test[[feat1,feat2]])\n    plt.show()\n        \n    \n    \n   \n\n    \n# for i in dataset.columns:\n#     bivariate(i)\nbivariate('Pregnancies','BMI') \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**In bivariate analysis we can take two feature. But in Multivariate Analysis, we will have\nall combination**"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_test.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multivariate EDS"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(dataset_test,hue='Outcome',size=3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns \nco=dataset_test.corr()\nfig,ax=plt.subplots(figsize=(10,10))\nsns.heatmap(co,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As you can see 'DiabetesPedigreeFunction' and 'BlooPresure' have lowest relationship with 'outcome'.\n**"},{"metadata":{},"cell_type":"markdown","source":"# Remove NAn Value with their mean accoring to class 0 and 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"def medtarget(var):\n    temp=dataset_test[dataset_test[var].notnull()]\n#     print(temp)\n    print(temp[[var, 'Outcome']].groupby(['Outcome']))\n    #print(temp)\n    return temp[[var, 'Outcome']].groupby(['Outcome']).mean()\n\nmedtarget('Insulin')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_test.loc[(dataset_test['Outcome']==0) & (dataset_test['Insulin'].isnull()), 'Insulin' ]=130.287879\ndataset_test.loc[(dataset_test['Outcome']==1) & (dataset_test['Insulin'].isnull()), 'Insulin' ]=206.846154","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"medtarget('Pregnancies')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_test.loc[(dataset_test['Outcome']==0) & (dataset_test['Pregnancies'].isnull()), 'Pregnancies' ]=3.861827\ndataset_test.loc[(dataset_test['Outcome']==1) & (dataset_test['Pregnancies'].isnull()), 'Pregnancies' ]=5.669565","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"medtarget('BloodPressure')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_test.loc[(dataset_test['Outcome']==0) & (dataset_test['BloodPressure'].isnull()), 'BloodPressure' ]=70.877339\ndataset_test.loc[(dataset_test['Outcome']==1) & (dataset_test['BloodPressure'].isnull()), 'BloodPressure' ]=75.321429","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"medtarget('SkinThickness')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_test.loc[(dataset_test['Outcome']==0) & (dataset_test['SkinThickness'].isnull()), 'SkinThickness' ]=27.235457\ndataset_test.loc[(dataset_test['Outcome']==1) & (dataset_test['SkinThickness'].isnull()), 'SkinThickness' ]=33","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"medtarget('BMI')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndataset_test.loc[(dataset_test['Outcome']==0) & (dataset_test['BMI'].isnull()), 'BMI' ]=30.859674\ndataset_test.loc[(dataset_test['Outcome']==1) & (dataset_test['BMI'].isnull()), 'BMI' ]=35.406767","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"medtarget('Glucose')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndataset_test.loc[(dataset_test['Outcome']==0) & (dataset_test['Glucose'].isnull()), 'Glucose' ]=110.643863\ndataset_test.loc[(dataset_test['Outcome']==1) & (dataset_test['Glucose'].isnull()), 'Glucose' ]=142.319549","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(dataset_test==0).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# for Outlier detection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for Outlier detection\n\nimport seaborn as sns\nfig,ax = plt.subplots(figsize=(20,12))\nsns.boxplot(data =dataset_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we will use 'quantile' technique to remove outliers.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_final=dataset_test.copy(deep=True)\nupbond = dataset_final['Pregnancies'].quantile(0.99) \ndataset_final['Pregnancies'] = np.where(dataset_final['Pregnancies']>upbond,upbond,dataset_final['Pregnancies'])\n\nupbond = dataset_final['BloodPressure'].quantile(0.98) \nlobond = dataset_final['BloodPressure'].quantile(0.01) \ndataset_final['BloodPressure'] = np.where(dataset_final['BloodPressure']>upbond,upbond,dataset_final['BloodPressure'])\ndataset_final['BloodPressure'] = np.where(dataset_final['BloodPressure']<lobond,lobond,dataset_final['BloodPressure'])\n\n\nupbond = dataset_final['SkinThickness'].quantile(0.93) \nlobond = dataset_final['SkinThickness'].quantile(0.10) \ndataset_final['SkinThickness'] = np.where(dataset_final['SkinThickness']>upbond,upbond,dataset_final['SkinThickness'])\ndataset_final['SkinThickness'] = np.where(dataset_final['SkinThickness']<lobond,lobond,dataset_final['SkinThickness'])\n\n\nupbond = dataset_final['Insulin'].quantile(0.93) \ndataset_final['Insulin'] = np.where(dataset_final['Insulin']>upbond,upbond,dataset_final['Insulin'])\n\n\nupbond = dataset_final['BMI'].quantile(0.98) \ndataset_final['BMI'] = np.where(dataset_final['BMI']>upbond,upbond,dataset_final['BMI'])\n\n\nupbond = dataset_final['DiabetesPedigreeFunction'].quantile(0.96) \ndataset_final['DiabetesPedigreeFunction'] = np.where(dataset_final['DiabetesPedigreeFunction']>upbond,upbond,dataset_final['DiabetesPedigreeFunction'])\n\n\nupbond = dataset_final['Age'].quantile(0.96) \ndataset_final['Age'] = np.where(dataset_final['Age']>upbond,upbond,dataset_final['Age'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfig,ax = plt.subplots(figsize=(20,12))\nsns.boxplot(data =dataset_final)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Above u can see, almost all outlier are removed.**"},{"metadata":{},"cell_type":"markdown","source":"# Spliting Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X= dataset_final.drop('Outcome',axis=1)\ny=dataset_final['Outcome']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test , y_train , y_test = train_test_split(X, y, test_size=0.25,stratify=y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape,\" \",y_train.shape)\nprint(X_test.shape,\" \",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input  and Output both are Continouts\n# two methos for Feature Selection \n# A -> Pearson Corelation -> f_regression\n# B -> Mutual INFO -> mutual_info_regression\n# C -> spearman \n\nfrom sklearn.feature_selection import  SelectKBest , f_regression , mutual_info_regression\nfs=SelectKBest(score_func=f_regression,k='all')\nfs.fit(X_train,y_train)\nprint(\"fss,\",fs.scores_)\nplt.bar(range(len(fs.scores_)),fs.scores_)\nplt.show()\n# scores,p_value = f_regression(X_train,y_train)\n# print(\"f \",f_regression(X_train,y_train))\n# print(\"p \",p_value)\n\n# fig ,ax =plt.subplots(figsize=(40,25))\n# plt.bar(range(len(p_value)),p_value)\n# # plt.bar(range(len(values)), list(values), align='center')\n# # plt.xticks(range(len(p_value)),list(p_value))\n# plt.show()\n# for i in range(len(fs.scores_)):\n#     print(i,\" \",fs.scores_[i])\n    \n# X_train_fs=fs.transform(X_train)\n# X_test_fs=fs.transform(X_test)\n\n    \nfs1=SelectKBest(score_func=mutual_info_regression,k='all')\nfs1.fit(X_train,y_train)\nplt.bar(range(len(fs1.scores_)),fs1.scores_)\nplt.show()\n\n# for i in range(len(fs1.scores_)):\n#     print(i,\" \",fs1.scores_[i])\n# fig ,ax =plt.subplots(figsize=(40,25))\n# plt.bar(range(len(scores)),scores)\n# # plt.bar(range(len(values)), list(values), align='center')\n# # plt.xticks(range(len(ch)),list(ch.keys()))\n# plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**IN feature selection result, both result are almost same. You can use any one of them.\nIm using 'f_regresssion'.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fs=SelectKBest(score_func=f_regression,k=7)\nfs.fit(X_train,y_train)\nX_train_fs=fs.transform(X_train)\nX_test_fs=fs.transform(X_test)\nprint(X_train_fs.shape,\" \",X_test_fs.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nstand=StandardScaler().fit(X_train_fs)\nX_stand_fs_train=pd.DataFrame(stand.transform(X_train_fs))\nX_stand_fs_test=pd.DataFrame(stand.transform(X_test_fs))\nprint(X_stand_fs_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_temp=pd.DataFrame(X_temp,columns=['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction'\n#                                    ,'Age'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_temp=dataset_final.drop(['BMI','Glucose','BloodPressure','Age'],axis=1 ) #,'BloodPressure','BMI','Glucose'\nfrom statsmodels.stats.outliers_influence import   variance_inflation_factor\nvif=pd.DataFrame()\nvif['Var Name']=X_stand_fs_train.columns\nvif['vif values']=[variance_inflation_factor(X_stand_fs_train.values,i) for i in range(X_stand_fs_train.shape[1])]\nprint(vif)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Te predictors may be moderately correlated. The output above shows that \nthe VIF for the Publication and Years factors are about 1.7, which indicates some correlation,\nbut not enough to be overly concerned about.**\n\n"},{"metadata":{},"cell_type":"markdown","source":"# I will remove feature 3, 4, 5, 6, because  they are correlated with each other.\n# And there is multicolinearity with these feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_stand_fs_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_stand_fs_train=X_stand_fs_train.drop([3,4,5,6],axis=1)\nX_stand_fs_test=X_stand_fs_test.drop([3,4,5,6],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_stand_fs_train.head())\nX_stand_fs_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix , accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nlog=LogisticRegression()\nlog.fit(X_stand_fs_train,y_train)\ny_pred_train=log.predict(X_stand_fs_train)\n# con=confusion_matrix(y_test,y_pred)\nplot_confusion_matrix(log,X_stand_fs_train,y_train)\nprint(\"train_accuracy \",accuracy_score(y_train,y_pred_train))\ny_pred=log.predict(X_stand_fs_test)\n# con=confusion_matrix(y_test,y_pred)\nplot_confusion_matrix(log,X_stand_fs_test,y_test)\nprint(\"test_Accuracy\",accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc=SVC()\nsvc.fit(X_stand_fs_train,y_train)\ny_pred_train=svc.predict(X_stand_fs_train)\n# con=confusion_matrix(y_test,y_pred)\nplot_confusion_matrix(svc,X_stand_fs_train,y_train)\nprint(\"train_accuracy \",accuracy_score(y_train,y_pred_train))\ny_pred=svc.predict(X_stand_fs_test)\n# con=confusion_matrix(y_test,y_pred)\nplot_confusion_matrix(svc,X_stand_fs_test,y_test)\nprint(\"test_Accuracy\",accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier()\nknn.fit(X_stand_fs_train,y_train)\ny_pred_train=knn.predict(X_stand_fs_train)\n# con=confusion_matrix(y_test,y_pred)\nplot_confusion_matrix(knn,X_stand_fs_train,y_train)\nprint(\"train_accuracy \",accuracy_score(y_train,y_pred_train))\ny_pred=knn.predict(X_stand_fs_test)\n# con=confusion_matrix(y_test,y_pred)\nplot_confusion_matrix(knn,X_stand_fs_test,y_test)\nprint(\"test_Accuracy\",accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **I have used all feature in my model and then\nmodel are overfitted. So i removed 3 ,4 ,5 ,6 feature above model. \nNow logistic , svc are not overfitted but knn is overfitted.**"},{"metadata":{},"cell_type":"markdown","source":"**We will use logistic regression and SVC \nApply grid search on LOgistic Regression and SVC****"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.pipeline import make_pipeline\n# from sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.metrics import classification_report\n# from sklearn.model_selection import train_test_split\n# X_train, X_test , y_train, y_test=  train_test_split(X,y,test_size=0.33,random_state=1,stratify=y)\n\n# pipe=make_pipeline(StandardScaler(), LogisticRegression())\n# #print(pipe)\n# C_range = 10. **np.arange(-3, 8)\n# penalt = ['l2']\n# #[1, 10, 100, 1000]\n# solver=['newton-cg', 'lbfgs', 'sag']\nparam_grid1 ={'penalty': ['l2'],'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000],'solver':['newton-cg', 'lbfgs', 'sag']} \n# dict(logisticregression__C=C_range,logisticregression__penalty=penalt,\n#                    logisticregression__solver=solver)\ngrid1 = GridSearchCV(LogisticRegression(), param_grid=param_grid1, cv=5,scoring='roc_auc')\nmodel=grid1.fit(X_stand_fs_train,y_train)\nprint('Best roc_auc: {:.4}, with best : {}'.format(grid1.best_score_, grid1.best_params_))\nprint(model.score(X_stand_fs_train,y_train))\nprint(model.score(X_stand_fs_test,y_test))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note: We can use 'accuray' or 'f1_score' instead of 'roc_curve' **"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\ny_pred=grid1.predict(X_stand_fs_test)\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log=LogisticRegression(C= 0.1,penalty= 'l2', solver= 'newton-cg')\nlog.fit(X_stand_fs_train,y_train)\ny_pred=log.predict(X_stand_fs_test)\nfrom sklearn.metrics import confusion_matrix\ntn, fp, fn, tp = confusion_matrix(y_test,y_pred).ravel()\naccuracy = (tp+tn)/(tp+tn+fp+fn)\n\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(log, X_stand_fs_test,y_test)\nplt.show()\nprecision=tp / (tp + fp) \nrecall=tp / (tp + fn)\nprint(\"test accuracy \", accuracy)\nprint(\"Precision \",precision)\nprint(\"Recall \",recall)\nprint(\"F1 score \",2*((precision*recall)/(precision+recall)))\n# print(tn, fp, fn, tp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SVC**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection  import GridSearchCV\nfrom sklearn.svm import SVC\n# from sklearn.model_selection import train_test_split\n# X_train, X_test , y_train, y_test=  train_test_split(X,y,test_size=0.33,random_state=1,stratify=y)\nC_range = [0.1,1, 10, 100]\ngamma_range = [1,0.1,0.01,0.001]\nkernel= ['rbf', 'poly', 'sigmoid']\n# ?pipe=make_pipeline(StandardScaler(),SVC())\nparam_grid = {'C':[0.1,1, 10, 100],'gamma':[1,0.1,0.01,0.001],'kernel':['rbf', 'poly', 'sigmoid']}\ngrid = GridSearchCV(SVC(),param_grid=param_grid, cv=5,scoring='roc_auc')\nmodel=grid.fit(X_stand_fs_train,y_train)\nprint('Best roc: {:.4}, with best C: {}'.format(grid.best_score_, grid.best_params_))\nprint(model.score(X_stand_fs_train,y_train))\nprint(model.score(X_stand_fs_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\ny_pred=grid.predict(X_stand_fs_test)\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsvc=SVC(C=1, gamma= 0.1, kernel='sigmoid',probability=True)\nsvc.fit(X_stand_fs_train,y_train)\ny_pred=svc.predict(X_stand_fs_test)\nfrom sklearn.metrics import confusion_matrix\ntn, fp, fn, tp = confusion_matrix(y_test,y_pred).ravel()\naccuracy = (tp+tn)/(tp+tn+fp+fn)\n\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(log, X_stand_fs_test,y_test)\nplt.show()\nprecision=tp / (tp + fp) \nrecall=tp / (tp + fn)\nprint(\"test accuracy \", accuracy)\nprint(\"Precision \",precision)\nprint(\"Recall \",recall)\nprint(\"F1 score \",2*((precision*recall)/(precision+recall)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.metrics import roc_curve, roc_auc_score,plot_roc_curve, confusion_matrix\ny_pred_log=log.predict_proba(X_stand_fs_test)\ny_pred_svc=svc.predict_proba(X_stand_fs_test)\n\n\nauc_l = roc_auc_score(y_test,y_pred_log[:,1])\nauc_s = roc_auc_score(y_test,y_pred_svc[:,1])\n\nfpr_l, tpr_l, thr_l = roc_curve(y_test, y_pred_log[:,1])\nfpr_s, tpr_s, thr_s = roc_curve(y_test, y_pred_svc[:,1])\n\nplt.subplots(1, figsize=(10,10))\n# plt.title('Receiver Operating Characteristic')\nplt.plot(fpr_l, tpr_l,\"blue\",label=\"log, auc=\"+str(auc_l))\nplt.plot(fpr_s, tpr_s,\"red\",label=\"svc, auc=\"+str(auc_s))\n# plt.plot(fpr_knn, tpr_knn,\"green\",label=\"knn, auc=\"+str(auc_knn))\nplt.plot([0, 1], ls=\"--\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}