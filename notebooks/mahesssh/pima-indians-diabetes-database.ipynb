{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv', header=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=dataset.drop('Outcome',axis=1)\ny=dataset.iloc[:,8]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.iloc[:,5]=X.iloc[:,5].astype('int64')\nX.iloc[:,6]=X.iloc[:,6].astype('int64')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sea\nheatmap=sea.heatmap(dataset.corr(),annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#checking outliers\nimport seaborn as sns\nfig,ax = plt.subplots(figsize=(12,12))\nsns.boxplot(data =X , ax = ax )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X['Age'].quantile(.10)) \nprint(X['Age'].quantile(.90)) \nprint(X['Pregnancies'].quantile(.90))\nprint(X['Pregnancies'].quantile(.10))\nprint(X['Glucose'].quantile(.90))\nprint(X['Glucose'].quantile(.10))\nprint(X['BloodPressure'].quantile(.90))\nprint(X['BloodPressure'].quantile(.10))\nprint(X['SkinThickness'].quantile(.90))\nprint(X['SkinThickness'].quantile(.10))\nprint(X['Insulin'].quantile(.90))\nprint(X['Insulin'].quantile(.10))\nprint(X['BMI'].quantile(.90))\nprint(X['BMI'].quantile(.10))\n\n      \n      \n      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X['Age']=np.where(X['Age']<22,22,X['Age'])\nX['Age']=np.where(X['Age']>51,51,X['Age'])\nX['Pregnancies']=np.where(X['Pregnancies']>9,9,X['Pregnancies'])\nX['Pregnancies']=np.where(X['Pregnancies']<0,0,X['Pregnancies'])\nX['Glucose']=np.where(X['Glucose']>167,167,X['Glucose'])\nX['Glucose']=np.where(X['Glucose']<85,85,X['Glucose'])\nX['BloodPressure']=np.where(X['BloodPressure']>88,88,X['BloodPressure'])\nX['BloodPressure']=np.where(X['BloodPressure']<54,54,X['BloodPressure'])\nX['SkinThickness']=np.where(X['SkinThickness']>40,40,X['SkinThickness'])\nX['SkinThickness']=np.where(X['SkinThickness']<0,0,X['SkinThickness'])\nX['Insulin']=np.where(X['Insulin']>210,210,X['Insulin'])\nX['Insulin']=np.where(X['Insulin']<0,0,X['Insulin'])\nX['BMI']=np.where(X['BMI']>41,41,X['BMI'])\nX['BMI']=np.where(X['BMI']<23,23,X['BMI'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(12,12))\nsns.boxplot(data =X , ax = ax )\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scaling the data\nfrom sklearn.preprocessing import StandardScaler\nstand=StandardScaler()\nX_scl=stand.fit_transform(X)\n\n#checking multicollinearity \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nX_new=pd.DataFrame(X_scl)\nprint(type(X_new))\nVIF=pd.DataFrame()\nVIF['Var Name']=X.columns\nVIF['vif values']=[variance_inflation_factor(X_new.values,i) for i in range(X_new.shape[1])]\nprint(VIF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#selecting best features\nfrom sklearn.model_selection import train_test_split\nX_train, X_test , y_train, y_test=  train_test_split(X,y,test_size=0.33,random_state=1)\nfrom sklearn.feature_selection import SelectKBest , f_classif\nfs=SelectKBest(score_func=f_classif,k=6)\nfs.fit(X_train,y_train)\nX_new_train=fs.transform(X_train)\nX_new_test=fs.transform(X_test)\nfor i in range(len(fs.scores_)):\n    print(i,\" \", fs.scores_[i])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar([i for i in range(len(fs.scores_))],fs.scores_)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score,plot_roc_curve, confusion_matrix\nlog=LogisticRegression()\nlog.fit(X_new_train,y_train)\nprint(\"log  with train \",log.score(X_new_train,y_train))\nprint(\"log  with test \",log.score(X_new_test,y_test))\n\nfrom sklearn.svm import SVC\nsvc= SVC(kernel='rbf')\nsvc.fit(X_new_train,y_train)\nprint(\"svc with train \",svc.score(X_new_train,y_train))\nprint(\"svc with test \",svc.score(X_new_test,y_test))\n\n\ny_pred_l=log.predict(X_new_test)\nlr_probs_l=log.predict_proba(X_new_test)\n\nauc_l = roc_auc_score(y_test, lr_probs_l[:,1])\n\n\nfpr_l, tpr_l, thr_l = roc_curve(y_test, lr_probs_l[:,1])\nplt.subplots(1, figsize=(10,10))\nplt.title('Receiver Operating Characteristic - DecisionTree')\nplt.plot(fpr_l, tpr_l,\"-b\",label=\"log, auc=\"+str(auc_l))\nplt.plot([0, 1], ls=\"--\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend(loc='best')\nplt.show()\n                                \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score,plot_roc_curve, confusion_matrix\nlog=LogisticRegression()\nlog.fit(X_new_train,y_train)\nprint(\"log  with train \",log.score(X_new_train,y_train))\nprint(\"log  with test \",log.score(X_new_test,y_test))\n\nfrom sklearn.svm import SVC\nsvc= SVC(kernel='rbf')\nsvc.fit(X_new_train,y_train)\nprint(\"svc with train \",svc.score(X_new_train,y_train))\nprint(\"svc with test \",svc.score(X_new_test,y_test))\n\n\ny_pred_l=log.predict(X_new_test)\n\ny_pred_s=svc.predict(X_new_test)\n\n\nauc_l = roc_auc_score(y_test,y_pred_l)\nauc_s = roc_auc_score(y_test,y_pred_s)\nfpr_l, tpr_l, thr_l = roc_curve(y_test, y_pred_l)\nfpr_s, tpr_s, thr_s = roc_curve(y_test, y_pred_s)\nplt.subplots(1, figsize=(10,10))\nplt.title('Receiver Operating Characteristic - DecisionTree')\nplt.plot(fpr_l, tpr_l,\"-b\",label=\"log, auc=\"+str(auc_l))\nplt.plot(fpr_s, tpr_s,\"-r\",label=\"svc, auc=\"+str(auc_s))\nplt.plot([0, 1], ls=\"--\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend(loc='best')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(log, X_new_test,y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tn, fp, fn, tp = confusion_matrix(y_test,y_pred_l).ravel()\nprint(confusion_matrix(y_test,y_pred_l))\nprint(tn, fp, fn, tp)\naccuracy = (tp+tn)/(tp+tn+fp+fn)\nprint(\"test accuracy \", accuracy)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}