{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import networkx as nx\nimport sklearn\nfrom hmmlearn import hmm\nimport gensim\nimport nltk\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer, porter\n#from nltk.stem import PorterStemmer.ORIGINAL_ALGORITHM as PSOA\nfrom nltk.stem.porter import *\nfrom scipy.cluster.vq import vq, kmeans, whiten\nfrom nltk import StemmerI","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"porter_stemmer = PorterStemmer.NLTK_EXTENSIONS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dir(nltk)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DJIA = pd.read_csv('../input/stocknews/upload_DJIA_table.csv')\ncombined_DJIA = pd.read_csv('../input/stocknews/Combined_News_DJIA.csv')\nReddit_News = pd.read_csv('../input/stocknews/RedditNews.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Intuitively, a document titlet combined_DJIA would have a relationship to the DJIA - oddly, I am not seeing it here.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Reddit_News.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"R_text = Reddit_News[['News']]\nR_text['index'] = R_text.index\nR_docs = R_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_DJIA.columns\n#There is clearly some sort of advanced technique involved wherein I can weight each next level of news story\n#for now I can mash them all into one big word salad/date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"R_text.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_DJIA['All News'] = combined_DJIA['Top1'] + combined_DJIA['Top2'] + combined_DJIA['Top3'] + combined_DJIA['Top4']\ncombined_DJIA['All News'] + combined_DJIA['Top5'] + combined_DJIA['Top6'] + combined_DJIA['Top7'] + combined_DJIA['Top8']\ncombined_DJIA['All News'] + combined_DJIA['Top9'] + combined_DJIA['Top10'] + combined_DJIA['Top11'] + combined_DJIA['Top12']\ncombined_DJIA['All News'] + combined_DJIA['Top13'] + combined_DJIA['Top14'] + combined_DJIA['Top15'] + combined_DJIA['Top16']\ncombined_DJIA['All News'] + combined_DJIA['Top17'] + combined_DJIA['Top18'] + combined_DJIA['Top19'] + combined_DJIA['Top20']\ncombined_DJIA['All News'] + combined_DJIA['Top21'] + combined_DJIA['Top22'] + combined_DJIA['Top23'] + combined_DJIA['Top24'] \n+ combined_DJIA['Top25']\n#for simplicity sake, I am collapsing all news into one single column per day","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#How can I rate these different levels of information - this should be useful for nexttime\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%pip install nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(combined_DJIA['All News'].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = str(combined_DJIA['All News'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here I clean the data to make them all lower case - a little unclear because proper nouns can be extremely informative\n#\n#\n#\n#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_words = [ ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import sent_tokenize, word_tokenize\ndef stemSentence(text):\n    token_words = word_tokenize(text)\n    token_words\n    print(token_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stemSentence(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stem_sentence = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for word in token_words:\n        stem_sentence.append(porter_stemmer(word))\n        stem_sentence.append(\" \")\n        \"\".join(stem_sentence)\n\nx=stemSentence(text)\nprint(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatize_stemming(text):\n    return((WordNetLemmatizer().lemmatize(text, pos='v')))\n#lemmatizing_stemmers are useful, in general, however, in this case, common sense would suggest words like: Israel, United States,\n#President, Minister, impeach, genocide, war, ozone, European Union, petroleum, etc.. would be the words I am looking \n#for..","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocessing(text):\n        if token in gensim.preprocessing.STOPWORDS() and len(token)> 3:\n            result.append(lemmatize_steming(token))\n        return(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc_sample = combined_DJIA['All News'] \n#Assign as a variable so as *not* to change the original\n\nprint('original document: ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = []\nfor word in doc_sample:\n   #words.append(word)\n   print(words)\n   #print('\\n\\n tokenized and lemmatized document: ')\n   print(preprocess(doc_sample))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lemmatize_stemming(doc_sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessed_docs = DJIA['All News'].map(preprocess)\npreprocessed_docs[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dictionary = gensim.corpora.Dictionary(processed_docs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = 0\nfor k,v in dictionary.iteritems():\n    print(k, v)\n    count += 1\n    if count > 10:\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\nbow_corpus[4310]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Keep it very simple, since this is first base\nnum_topics = 3\nchunksize = 100 \npasses = 10 \niterations = 10\neval_every = 1  \n\n# Make a index to word dictionary.\ntemp = dictionary[0]  # This is only to \"load\" the dictionary.\nid2word = dictionary.id2token\n\nmodel = LdaModel(corpus=bow_corpus, id2word=id2word, chunksize=chunksize, \n                       alpha='auto', eta='auto', \n                       iterations=iterations, num_topics=num_topics, \n                       passes=passes, eval_every=eval_every)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Test Performance**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#I will test this by simply seeing, cutting off the last 500 lines, and treating it as an out-of-sample test ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DJIA.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"**Hidden Markov Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#The way that someone else did this online was simply to just apply an HMM to 40 days and then use it to prodict the future","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"open_price = np.array(DJIA['Open'])\nclose_price= np.array(DJIA['Close'])\nhigh_price= np.array(DJIA['High'])\nlow_price= np.array(DJIA['Low'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frac_change = np.array((close_price-open_price)/open_price)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frac_high = np.array((high_price-open_price)/open_price)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frac_low = np.array((open_price-low_price)/open_price)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#So far, my model is that there is one node for the entre stock market index and each node goes to one of: very high, small high, no change, small low, very low\n#The guy who did his senior thesis on this said the best fit was at 40 days, so I'll have a 5X40 markov chain\n# I start with just one stock, it should be easy to upgrade complexity from there","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DJIA['frac_change'] = frac_change\nDJIA['frac_high'] = frac_high\nDJIA['frac_low'] = frac_low","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DJIA['frac_change'].max(),\n#DJIA['frac_change'].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DJIA['normalized_change'] = DJIA['frac_change']/(DJIA['frac_high']-DJIA['frac_low'])\n#I got something wrong with this equation and it actually exacerbates changes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Vector Quantizer here: Separate it into 5 distinct states\ncode_book = np.array([-2, -1, 0, 1, 2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DJIA['vectorized_change'] == vq(DJIA['frac_change'], code_book)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(vectorized_changes)\n#This does not look very good yet... I must tinker","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DJIA['normalized_change'].max()\n#DJIA['normalized_change'].min()\n#They don't seem normalized at all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(np.column_stack((frac_change, frac_high, frac_low)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#It seems like he solves this problem by extracting the columns into individual arrays first...\n\n#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = hmm.GaussianHMM(n_components=5, n_iter=40, covariance_type='tied')\n#Are these inputs correct though?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.startprob_= np.array()\n#model.transmat_ = np.array()\n#model.means_ = np.array()\n#model.covars_ = np.tile(np.identity(), (,,,))\n#X, Y = model.sample(100)\n#This will be generated by the LDA in the previous section","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#just tinkering here \n#vec_change = np.array(DJIA['vectorized_change'])\n#vec_change = vec_change.reshape(1, -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DJIA.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(DJIA[['Date','vectorized_change']])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"**Test Goodness of Fit**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#I will use a Chi-Square test to test the Goodness of Fit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}