{"cells":[{"outputs":[],"metadata":{"_uuid":"afbf21d6b85bbc6c19568af7ba16f6d6a3576087","collapsed":true,"_cell_guid":"5e9e88b6-3bd7-4051-a5d9-f5781488161f"},"cell_type":"code","source":"#This is how you can easily get 100 % accuracy in the poisonous mushroom dataset\n#Firstly lets import some helpful libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","execution_count":1},{"outputs":[],"metadata":{"_uuid":"dbbc91ced4b0349a14db3b7fcfc470a884c3e85a","_cell_guid":"c0269815-9c61-4696-b952-d9f28f9277e9"},"cell_type":"code","source":"df=pd.read_csv(\"../input/mushrooms.csv\")\n\n#Firstly we will see some useful information about data\nprint(df.head(3))","execution_count":2},{"outputs":[],"metadata":{"_uuid":"679ad2073966a9d1777dbb26e43969b93a1721e2","_cell_guid":"9c4494d0-ea68-4452-85be-b9cbeba44c0b"},"cell_type":"code","source":"data=df.as_matrix()\nprint(\"Shape of data is\",data.shape)","execution_count":3},{"outputs":[],"metadata":{"_uuid":"ad295d95297b0d9cd41964f76219a4098fc3b24c","_cell_guid":"92712be2-cb9a-4130-9900-c3ed9daa1ffa"},"cell_type":"code","source":"#Lets check if there are any missing values in any column\nprint(\"No. of missing values in column are:\")\nnames = df.columns.values\nfor i in range(1,23):\n    col_is_null=df.iloc[:,i].isnull().sum()\n    print(names[i],\":\",col_is_null)","execution_count":4},{"metadata":{"_uuid":"ae78a88ff3726bd167ce8cf2839f6d749aafed94","_cell_guid":"ef1ed602-4d22-4cfe-8a61-53aecda7b49a"},"cell_type":"markdown","source":"As we can see that the first column indicates whether the mushroom is poisonous(p) or edible(e).\nSo we will seperate the first column as our target(y). \nLets make a function to do this.\nThe  function below has an important variable label_col. This is our target col. In this case it is column 0. If label_col is False then there will be no labels as in the case of test data."},{"outputs":[],"metadata":{"_uuid":"64bbfcc25003abee226b317b32539f5f32c83238","collapsed":true,"_cell_guid":"b8a0d02d-c6ea-412b-9f1d-8a6f1070f34f"},"cell_type":"code","source":"def get_data(file_path,label_col=\"False\"):\n    df=pd.read_csv(file_path)\n    data=df.as_matrix()\n    \n    if(label_col==\"False\"): #No labels.Used for test data\n        return data\n    \n    y=data[:,label_col]\n    X=np.delete(data,label_col,axis=1)\n    return X,y        ","execution_count":5},{"outputs":[],"metadata":{"_uuid":"e68930b749151b46492fc0a61eea16ff415d3382","_cell_guid":"2fdc4668-8f91-4582-8593-53f77424d296"},"cell_type":"code","source":"X,y=get_data(\"../input/mushrooms.csv\",label_col=0)\nprint(\"Shape of data and labels is\",X.shape,y.shape)","execution_count":6},{"outputs":[],"metadata":{"_uuid":"483ef3f7142f3be3b5ec222520ae0c076f60f8a5","_cell_guid":"9b16dbc3-79d3-4834-aa47-1f8ef362aab6"},"cell_type":"code","source":"#Now we will one hot encode the data.This is necessary because our data is categorical\n#Firstly lets define few constants for our data\nN=X.shape[0] #This is the no. of data points or training examples we have\nD=X.shape[1] #This is the dimensionality or no. of features in our dataset\n\nX_encoded=np.empty((N,1))\nfor i in range(D):\n    dum=pd.get_dummies(X[:,i])\n    X_encoded=np.hstack((X_encoded,dum))\n#Since first col is empty we need to remove it\nX_encoded=X_encoded[:,1:]\nprint(\"Shape of X_encoded is\",X_encoded.shape)\nX=X_encoded","execution_count":7},{"outputs":[],"metadata":{"_uuid":"6955b03a565be3514c9000b685b13e1566aa6249","_cell_guid":"bdc23fb8-c2c8-4a9d-ba1c-05f78f4d2e58"},"cell_type":"code","source":"#Lets find out the no. of poisonous and edible mushrooms\nn_edible= np.sum(y=='p')\nn_poisonous= np.sum(y=='e')\ntotal= n_edible + n_poisonous\nprint(\"No. of poisonous mushrooms is\",n_poisonous)\nprint(\"No. of edible mushrooms is\",n_edible)\nprint(\"% of posionous mushrooms is\",(n_poisonous/total)*100)\nprint(\"% of edible mushrooms is\",(n_edible/total)*100)","execution_count":8},{"metadata":{"_uuid":"c217cfcfed1fe92f22f143c8c935aa46b5dee804","_cell_guid":"67581b74-8030-4f36-8d89-9b9b39bc29d3"},"cell_type":"markdown","source":"From the above we can see that classes are almost perfectly balanced. This helps our dataset achieve very high accuracies.\n\n\n"},{"outputs":[],"metadata":{"_uuid":"71253c7e1dbba2b7a64e1ea62017f54a5589c66a","_cell_guid":"8a7c9d3d-959f-47e2-a034-e89b11909f66"},"cell_type":"code","source":"#Now after we have processed our data we are good to go.\n#Firstly we will split our data into train and test sets\n#For this we will be using a sklearn function train_test_split\nfrom sklearn.model_selection import train_test_split\n#We will keep 20% of the data for testing\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n\nprint(\"Shape of X_train, y_train is \",X_train.shape,y_train.shape)\nprint(\"Shape of X_test, y_test is \",X_test.shape,y_test.shape)","execution_count":9},{"outputs":[],"metadata":{"_uuid":"e0e6383813f8c4274f311867ea1c2800292c8efa","_cell_guid":"2f0a4018-3483-4bc8-9d58-e0902a51f6eb"},"cell_type":"code","source":"#To select a model we will use cross validation for our training set.\n#Then we will apply the selected model to our test set and compare accuracies\n#Let import cross_val_scores from sklearn to aid us\nfrom sklearn.model_selection import cross_val_score\n\n#Firstly we will define a baseline model\n#A baseline model is basically a very simple or trivial approach to our dataset.\n#All our models should have a greater performance than the baseline\n#Logistic regression will be used as baseline here\nfrom sklearn.linear_model import LogisticRegression\n#Logistic Regression has a few parameters which you can try tuning\nprint(LogisticRegression())","execution_count":10},{"outputs":[],"metadata":{"_uuid":"42b018b3c3c788eb7cec1e817d126675624396a4","_cell_guid":"74ab9a79-a2ec-47e7-89a7-f8f0ff1ff7d7"},"cell_type":"code","source":"model=LogisticRegression()\ncv_baseline=cross_val_score(model,X_train,y_train)\nprint(\"Cross val scores have a mean\",cv_baseline.mean(),\" and standard deviation \",cv_baseline.std())\n","execution_count":11},{"outputs":[],"metadata":{"_uuid":"e122f92e6ce77a4113db2521d9fa26febb2c4075","_cell_guid":"2bc5a8da-f023-4716-8463-f2f0db7b2a95"},"cell_type":"code","source":"# A low  cross val std indicates that our model is not overfitting.\n# A high mean cross val score also shows that our model is not underfitting\nmodel.fit(X,y)\ny_test_pred=model.predict(X_test)\nprint(\"Score on test set is\",np.mean(y_test==y_test_pred))\n\n#We are a getting 100 % accuracy in our baseline model itself.","execution_count":12},{"outputs":[],"metadata":{"_uuid":"dd1d3af761a528f109bfc26353bcc1117625a8da","collapsed":true,"_cell_guid":"f33bd3b6-75db-4850-b0b6-a27237d1e7c3"},"cell_type":"code","source":"#We will use some other algorithms and compare accuracies\n#Lets first import them\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom xgboost import XGBClassifier","execution_count":13},{"outputs":[],"metadata":{"_uuid":"e62b89cbccf5c928b9d4de4feb29a77a170e5adc","_cell_guid":"66c67e54-5ecf-4d46-9029-10cf4058bb30"},"cell_type":"code","source":"models=[LogisticRegression(),DecisionTreeClassifier(),RandomForestClassifier(),\n        AdaBoostClassifier(),XGBClassifier()]\nnames=[\"Logistic Regression\",\"DecisionTree\",\"RandomForest\",\"AdaBoost\",\"XGBoost\"]\nprint(\"For given model mean cross_val_scores standard deviation\\n\")\nfor model,name in zip(models,names):\n    model.fit(X,y)\n    cv_baseline=cross_val_score(model,X_train,y_train)\n    y_test_pred=model.predict(X_test)\n    print(name,\"Mean\",cv_baseline.mean(),\"Standard dev:\",cv_baseline.std(),\"Test Set Score:\",\n          np.mean(y_test==y_test_pred))\n    ","execution_count":14}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"nbconvert_exporter":"python","version":"3.6.4","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","name":"python","file_extension":".py","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}