{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Pima Indians Diabetes Database\n# Inspiration\n# Can you build a machine learning model to accurately predict whether or not the patients in the dataset have diabetes or not?"},{"metadata":{},"cell_type":"markdown","source":"# Wondering what is Pima ?\nThe Pima are a group of Native Americans living in an area consisting of what is now central and southern Arizona, as well as northwestern Mexico in the states of Sonora and Chihuahua. The majority population of the surviving two bands of the Akimel O'odham are based in two reservations: the Keli Akimel OÊ¼otham on the Gila River Indian Community and the On'k Akimel O'odham on the Salt River Pima-Maricopa Indian Community.\n\nWikipedia"},{"metadata":{},"cell_type":"markdown","source":"# 1)Let's import all the required Packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2)Split the dataset into features and target variable\n\nFeatures are the Independant variables\n\nOutcome is the Class label : 0 - No Diabetes , 1 - Diabetes Present"},{"metadata":{"trusted":true},"cell_type":"code","source":"Features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness','Insulin','BMI','DiabetesPedigreeFunction', 'Age']\nX = data[Features]   \ny = data.Outcome ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3)Data Munging\nCheck the descriptive information of Dataset using pandas describe and info methods\n# pandas describe - This is an important step to understand the distribution of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# pandas info - This is an important step as it gives you information about the data types"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check for nulls in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We have No Null values in this dataset so lets get going!!"},{"metadata":{},"cell_type":"markdown","source":"# 4)PLOTS for Visualization and Insights\nLet us Plot a suitable graph to find multicollinearity i.e relationship between independent features in the given dataset.\n\n# Pair Plot is a good idea to represent the relationship between the independant features \neg. Glucose vs Insulin or Age vs BMI\n\nWe have aleady imported matplotlib and seaborn libraries so we are good to go ahead to map the Pair Plot!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (40, 41)\nplt.style.use('dark_background')\n\nsns.pairplot(data, hue = 'Outcome', palette = 'husl')\nplt.title('Pair plot for the data', fontsize = 40)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pair Plot analysis from above graph :\nThe histograms that appear across the diagonal in the graph is because its Pregnanies vs Pregnancies ..Glucose vs Glucose...BloodPressure vs BloodPressure ..etc\n\nSo, now focus on the actual pairplots above to draw some insights :\n\n# 0 - pink color scatter indicates No Diabetes\n# 1 - blue color scatter indicates Has Diabetes\nSo, look at -\n\n# Age vs Glucose\nwe can see from the plot that when the glucose level is below 100 aprx and check the age in Y axis they are classified as Not having Diabetes but as the glucose level increases beyond 100 to 200 and check the age in Y axis they are classified as having Diabetes\n\nSimilarly look at the pair plot -\n\n# Glucose vs BMI\nfor BMI between 20 and 40 range when the glucose level is above 100 apprx then the people are classified as having Diabetes.\n\nSuch intuitions can be infered using PairPlots"},{"metadata":{},"cell_type":"markdown","source":"# 5)Correlation Matrix for Visualization and Insights between the correlation of Independant Features and Dependant Feature by using Heatmaps.\n\nThe color scale on the right hand side is indicative of the correlation trending towards 1 (max positive correlation value)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15, 15)\n\nsns.heatmap(data.corr(), annot = True)\nplt.title('Correlation Plot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below can be the insights from the Correlation Matrix :\n\n# There are No negatively co-related features\n\n# Positively Correlated features :\n\n# Glucose\nIt has a value of .47 which is the highest correlation value in this graph and it is the most important factor determining Diabetes so this is how the machine has found the important feature. Hence Glucose is positively correlated and we can infer as glucose level increases patient is having Diabetes\n\n# BMI, Age, and Pregnancy\nIt has .29 , .24 and .22 values repectively so they can also be important features to predict presence of Diabetes\n\n# Low contributors to the correlation\nPedigree and Insulin are contributing to .17 and .13 correlation only\n\nOn the other hand BP, Skin thickness have low values of .065 and .075 so they are insignificant predictors for Diabetes diagnosis"},{"metadata":{},"cell_type":"markdown","source":"# 6)Apply Feature scaling to standardize the data columns to common units to avoid biased model.\nAll the Features are in different units of measurements so this is a very important preprocessing step to get good results!\n\nHence we use StandardScaler library to fit the data and then transform to convert the original values to the standardized values hence avoiding bias"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_scaled = sc.fit_transform(X)\nX_scaled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Incase you would like to see the transformed standardized data frame do the below step - "},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(X_scaled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7)Splitting the data\nTo understand the model performance, we divide the dataset into a training set and a test set. Let's split the dataset by using function train_test_split() from sklearn. The 3 parameters to be passed are Features, outcome, and test_set size. Additionally, you can use random_state to select records randomly."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.33,random_state=42) \nx_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8)Trying to fit the model\n# I am using a simple Logistic Regression model for this Binary classification problem!!\nI have tried to hypertune the parameters however there is not much difference in the Accuracy results so using default parameters Only. I have also tried to drop the poorly correlated features however the Accuracy remains the same. I am a newbie to Machine Learning so still learning other algorithms to experiment in future for better accuracy results!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\n\nclf.fit(x_train,y_train)\ny_pred=clf.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9)Evaluating the model\nSince this is a medical diagnosis along with Accuracy score always evaluate model results with Confusion Matrix/Precision/Recall/F1score to really understand the sensitivity and specificity of the predictions as you surely want to zero out those False Negatives to avoid any misses in diagnosing a patient who Truly has Diabetes for effective identification and treatment"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint(accuracy_score(clf.predict(x_train), y_train))\nprint(accuracy_score(y_pred, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Training Accuracy is 78 %\n...this to me is Okay on the Training set and not going to improve further as it can lead to Overfitting and our focus is more on How our model is going to perform on the Test Data!!\n\n# The Testing Accuracy is 74 %\n..We need to surely improve the Testing Accuray score,though I tried to hypertune the model parameters and tried to drop some of the poorly correlated features however the score did not change much.\n\nI have been told Neural Networks will yeild better results in such cases of Binary Classification"},{"metadata":{},"cell_type":"markdown","source":"# 10)Confusion Matrix\nSince this is a medical diagnosis along with Accuracy score always evaluate model results with Confusion Matrix as you surely want to zero out those False Negatives to avoid any misses in diagnosing a patient who Truly has Diabetes for effective identification and treatment"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix \nconfusion_matrix(y_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 32 False Negatives which mean 32 people have Diabetes present but my Model classified them as Not having Diabetes so definitely need to improve model performance!! I have been told Neural Networks will yeild better results in such cases of Binary Classification so I am still coming up that learning curve! "},{"metadata":{},"cell_type":"markdown","source":"# 11)Precision/Recall/F1score to really understand the sensitivity and specificity of the predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nclassification_report(y_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# I hope what I have learnt and shared is helpful to someone in someway!\n\n# PASSION FOR TECHNOLOGY"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}