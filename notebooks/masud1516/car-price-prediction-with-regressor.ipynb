{"cells":[{"metadata":{},"cell_type":"markdown","source":"##### Import necessary Libaries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Libaries for data read\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\n# Libaries for visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Libaries for model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_val_score\n\n# other's\nimport warnings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's hide Warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Read Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/vehicle-dataset-from-cardekho/car data.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Handle Mising / NAN value"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the missing value \ndf.isnull().sum() # There is no missing value feature","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Data Cleaning "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's Final Dataset without car name feature\ndataset = df.drop([\"Car_Name\"], axis=1)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Handle year feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create a current year feature\ncurrent_year = datetime.now().year\ndataset[\"Current_year\"] = current_year\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create Number of years the car uses \ndataset[\"Num_Years\"] = dataset[\"Current_year\"] - dataset[\"Year\"]\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's drop unnecessary features like- \"Year\", \"Current_year\"\ndataset.drop([\"Year\", \"Current_year\"], axis=1, inplace=True)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Handle Categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's find categorical features \ncate_features = [feature for feature in dataset.columns if dataset[feature].dtypes == \"O\"]\ncate_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create a function to do \"One Hot Encoding\" of Categorical Features \nencode = pd.get_dummies(dataset[cate_features], drop_first=True)\n# concatanet with dataset\ndata = pd.concat([dataset, encode], axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make final dataset for model\nfinal_dataset = data.drop(cate_features, axis=1)\nfinal_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Features Correlations\n    If we have so many features then we can check correlation among those features "},{"metadata":{"trusted":true},"cell_type":"code","source":"final_dataset.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = final_dataset.corr()\ncorr_features = corr.index\nplt.figure(figsize=(15,10))\nsns.heatmap(final_dataset[corr_features].corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Features Importance finding\n    If we have so many features, then we can work on Features Importance  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets divide X & Y dataset\nX = final_dataset.iloc[:, 1:]\ny = final_dataset.iloc[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features Importance \nfrom sklearn.ensemble import ExtraTreesRegressor\nmodel = ExtraTreesRegressor()\nmodel.fit(X, y)\nprint(model.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's find best 5 features by using plot\nfeature_importance = pd.Series(model.feature_importances_, index=X.columns)\nfeature_importance.nlargest(5).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build a Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's split train and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create model and train the model with Random Forest Regressor\ncpp_model = RandomForestRegressor()\ncpp_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's test the model\ny_pred = cpp_model.predict(X_test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's compare test result with actual\npred_dataset = pd.DataFrame({\"Actual_Data\": y_test, \"Predict_Data\": y_pred})\npred_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's check the model score"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model score\ncpp_model.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# r2 Score of Model\nR2Score = r2_score(y_test, y_pred)\nR2Score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's Use XGBoost Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"cpp_xgbr_model = XGBRegressor()\ncpp_xgbr_model.fit(X_train, y_train)\ny_pred_xgb = cpp_xgbr_model.predict(X_test)\ny_pred_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's compare test result with actual\nxgb_pred_dataset = pd.DataFrame({\"Actual_Data\": y_test, \"Predict_Data\": y_pred_xgb})\nxgb_pred_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model score\ncpp_xgbr_model.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# r2 Score of Model\nr2_score(y_test, y_pred_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Lets cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"cscore = cross_val_score(cpp_xgbr_model, X_train, y_train.ravel(), cv=5)\ncscore.mean()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}