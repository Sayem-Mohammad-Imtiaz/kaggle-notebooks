{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMDB movie review sentiment analysis","metadata":{}},{"cell_type":"markdown","source":"This notebook will compare and use different NLP techniques and perform sentiment analysis on [imdb-dataset-of-50k-movie-reviews](http://https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) dataset. \n\n### Table of Contents:\n\n1.  Importing libraries and dataset\n2.  exploratory analysis\n    - overview of dataset\n    - sample of dataset\n    - graphical representation of different sentiments in the dataset\n    - listing NLTK's stopwords\n3. processing features\n    - custom stemmer-tokenizer functions\n        - normal features\n        - ngram features\n    - calculating Tfdf and features intentionally\n4. processing labels\n5. analyzing processed features\n    - overview of normal features\n    - overview of n-gram features\n6. getting training and testing data ready\n7. sklearn's MultinomialNB","metadata":{}},{"cell_type":"markdown","source":"### importing libraries and dataset","metadata":{}},{"cell_type":"code","source":"# importing common libraries\n\nimport re\nimport spacy as sp\nimport pandas as pd\nimport pickle as pk\n\nfrom scipy import sparse\nfrom nltk.stem import PorterStemmer\nfrom nltk import (corpus, word_tokenize, WordNetLemmatizer, pos_tag)\nfrom sklearn import (feature_extraction, datasets, linear_model, naive_bayes, ensemble, model_selection)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-29T12:12:33.398645Z","iopub.execute_input":"2021-06-29T12:12:33.399031Z","iopub.status.idle":"2021-06-29T12:12:33.404901Z","shell.execute_reply.started":"2021-06-29T12:12:33.398998Z","shell.execute_reply":"2021-06-29T12:12:33.404047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing IMDB dataset into a pandas dataframe\n\nraw_df = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-29T12:12:37.283764Z","iopub.execute_input":"2021-06-29T12:12:37.284297Z","iopub.status.idle":"2021-06-29T12:12:37.955797Z","shell.execute_reply.started":"2021-06-29T12:12:37.284248Z","shell.execute_reply":"2021-06-29T12:12:37.954766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### exploratary analysis","metadata":{}},{"cell_type":"code","source":"# getting an overview of the dataset\n\nraw_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T12:12:40.708885Z","iopub.execute_input":"2021-06-29T12:12:40.709288Z","iopub.status.idle":"2021-06-29T12:12:40.740032Z","shell.execute_reply.started":"2021-06-29T12:12:40.709256Z","shell.execute_reply":"2021-06-29T12:12:40.738819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking a sample of the data\n\nraw_df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T12:12:43.553152Z","iopub.execute_input":"2021-06-29T12:12:43.553523Z","iopub.status.idle":"2021-06-29T12:12:43.565839Z","shell.execute_reply.started":"2021-06-29T12:12:43.55349Z","shell.execute_reply":"2021-06-29T12:12:43.56477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking types of sentiments in dataset\n\nraw_df.sentiment.hist(bins=3)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T12:12:46.323252Z","iopub.execute_input":"2021-06-29T12:12:46.323615Z","iopub.status.idle":"2021-06-29T12:12:46.468038Z","shell.execute_reply.started":"2021-06-29T12:12:46.323584Z","shell.execute_reply":"2021-06-29T12:12:46.467367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the existing list of stopwords\n\nstopWords = corpus.stopwords.words(\"english\")\nprint(\"NLTK's STOP WORDS LIST:\\n\\t\", stopWords)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T12:12:50.198242Z","iopub.execute_input":"2021-06-29T12:12:50.19886Z","iopub.status.idle":"2021-06-29T12:12:50.206155Z","shell.execute_reply.started":"2021-06-29T12:12:50.198809Z","shell.execute_reply":"2021-06-29T12:12:50.205035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking difference between tokenization using regex vs NLTK's word_tokenize()\n\nsample_review = raw_df.review[7]\nregex_tk = re.compile(r\"\\b[A-Za-z0-9']+\\b\")\n\nprint(\"ORIGINAL SENTENCE:\\n\\t\", sample_review, \"\\n\")\nprint(\"REGEX TOKENIZED WORDS:\\n\\t\", re.findall(regex_tk, sample_review), \"\\n\")\nprint(\"WORD_TOKENIZED WORDS:\\n\\t\", word_tokenize(sample_review), \"\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2021-06-29T12:12:52.988408Z","iopub.execute_input":"2021-06-29T12:12:52.988765Z","iopub.status.idle":"2021-06-29T12:12:52.999405Z","shell.execute_reply.started":"2021-06-29T12:12:52.988734Z","shell.execute_reply":"2021-06-29T12:12:52.998043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apparently regex performs tokenization in a meaningful way and is fast too. \nSo, we will be using regex for tokenizing the data.","metadata":{}},{"cell_type":"markdown","source":"### processing features","metadata":{}},{"cell_type":"markdown","source":"We will try two types of feature processing models. \n1. Both will use Tfidf to vectorize the data.\n2. Both will use regex to filter out necessary words using regex regex pattern- `r\"\\b[A-Za-z0-9']{2,}\\b\"`\n3. Both will use NLTK's PorterStemmer for stemming words\n\nThe key differences are: \n1. Excluding and including stop words in the features\n2. Using different ngram ranges- (1, 1) and (1, 2)","metadata":{}},{"cell_type":"code","source":"# this stemmer-tokenizer will not include stop words\n\nclass stemTokenizer:\n    def __init__(self):\n        self.stemmer = PorterStemmer()\n        self.token_pattern = re.compile(r\"\\b[A-Za-z0-9']{2,}\\b\")\n        \n    def __call__(self, sent):\n        sent = re.findall(self.token_pattern, sent)\n        return [self.stemmer.stem(word) for word in sent if word not in stopWords]\n    \n    \n# creating TFIDF matrix from raw data\n\ntfidf_vec = feature_extraction.text.TfidfVectorizer(tokenizer = stemTokenizer())","metadata":{"execution":{"iopub.status.busy":"2021-06-29T12:12:58.808354Z","iopub.execute_input":"2021-06-29T12:12:58.808783Z","iopub.status.idle":"2021-06-29T12:12:58.822733Z","shell.execute_reply.started":"2021-06-29T12:12:58.808744Z","shell.execute_reply":"2021-06-29T12:12:58.821869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this stemmer-tokenizer will include stop words\n# and with ngram range (1, 2)\n\nclass stemTokenizer_ngram:\n    def __init__(self):\n        self.stemmer = PorterStemmer()\n        self.token_pattern = re.compile(r\"[A-Za-z0-9']{2,}\")\n        \n    def __call__(self, sent):\n        sent = re.findall(self.token_pattern, sent)\n        return [self.stemmer.stem(word) for word in sent]\n\n    \n# creating TFIDF matrix with ngrams and stopwords\n\ntfidf_vec_ngram = feature_extraction.text.TfidfVectorizer(tokenizer = stemTokenizer_ngram(), \n                                                          ngram_range = (1, 2), \n                                                          max_features = 500000)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T12:13:01.433464Z","iopub.execute_input":"2021-06-29T12:13:01.433992Z","iopub.status.idle":"2021-06-29T12:13:01.702722Z","shell.execute_reply.started":"2021-06-29T12:13:01.433957Z","shell.execute_reply":"2021-06-29T12:13:01.701708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### calculating Tfidf of features","metadata":{}},{"cell_type":"code","source":"# creating features \n\nX = tfidf_vec.fit_transform(list(raw_df.review))","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-29T12:09:06.271168Z","iopub.execute_input":"2021-06-29T12:09:06.271532Z","iopub.status.idle":"2021-06-29T12:09:06.31402Z","shell.execute_reply.started":"2021-06-29T12:09:06.271501Z","shell.execute_reply":"2021-06-29T12:09:06.312892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating n-grammed features \n\nX_ngram = tfidf_vec_ngram.fit_transform(list(raw_df.review))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T11:42:04.074007Z","iopub.execute_input":"2021-06-29T11:42:04.074313Z","iopub.status.idle":"2021-06-29T11:47:28.098348Z","shell.execute_reply.started":"2021-06-29T11:42:04.074284Z","shell.execute_reply":"2021-06-29T11:47:28.097164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### processing labels","metadata":{}},{"cell_type":"code","source":"# creating labels \n\ny = [1 if (i == \"positive\") else 0 for i in raw_df.sentiment]","metadata":{"execution":{"iopub.status.busy":"2021-06-29T11:47:47.933878Z","iopub.execute_input":"2021-06-29T11:47:47.934319Z","iopub.status.idle":"2021-06-29T11:47:47.94978Z","shell.execute_reply.started":"2021-06-29T11:47:47.934284Z","shell.execute_reply":"2021-06-29T11:47:47.948825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### analyzing processed features","metadata":{}},{"cell_type":"code","source":"# getting an overview of the normal features\n\nprint(f\"There are total {len(tfidf_vec.get_feature_names())} features in the matrix\")\nprint(\"some of the features are: \", tfidf_vec.get_feature_names()[0:-1:10000])","metadata":{"execution":{"iopub.status.busy":"2021-06-29T11:47:47.951028Z","iopub.execute_input":"2021-06-29T11:47:47.951501Z","iopub.status.idle":"2021-06-29T11:47:48.12594Z","shell.execute_reply.started":"2021-06-29T11:47:47.951466Z","shell.execute_reply":"2021-06-29T11:47:48.124917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting an overview of the n-grammed features\n\nprint(f\"There are total {len(tfidf_vec_ngram.get_feature_names())} features in the matrix\")\nprint(\"some of the features are: \", tfidf_vec_ngram.get_feature_names()[0:-1:50000])","metadata":{"execution":{"iopub.status.busy":"2021-06-29T11:47:48.127258Z","iopub.execute_input":"2021-06-29T11:47:48.127601Z","iopub.status.idle":"2021-06-29T11:47:50.10556Z","shell.execute_reply.started":"2021-06-29T11:47:48.127563Z","shell.execute_reply":"2021-06-29T11:47:50.104519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### getting training and testing data ready","metadata":{}},{"cell_type":"markdown","source":"We will be using Sklearn's `train_test_split()` function to split the processed data. \\\nThe train:test ratio is 70:30 to avoid overfitting.","metadata":{}},{"cell_type":"code","source":"# splitting features and labels into training and testing data\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T11:47:50.106732Z","iopub.execute_input":"2021-06-29T11:47:50.107002Z","iopub.status.idle":"2021-06-29T11:47:50.147289Z","shell.execute_reply.started":"2021-06-29T11:47:50.106974Z","shell.execute_reply":"2021-06-29T11:47:50.146374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting ngrammed features and labels into training and testing data\n\nX_train_ngram, X_test_ngram, y_train_ngram, y_test_ngram = model_selection.train_test_split(X_ngram, y, test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T11:47:50.148598Z","iopub.execute_input":"2021-06-29T11:47:50.148878Z","iopub.status.idle":"2021-06-29T11:47:50.26424Z","shell.execute_reply.started":"2021-06-29T11:47:50.14885Z","shell.execute_reply":"2021-06-29T11:47:50.263249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### sklearn's MultinomialNB","metadata":{}},{"cell_type":"markdown","source":"We will use Naive Bayes classifier because it is useful for binary classification. \\\nSklearn' MultinomialNB is mostly used for term frequency data. ","metadata":{}},{"cell_type":"code","source":"# testing normal features using MultinomialNB\n\nnb = naive_bayes.MultinomialNB()\nnb.fit(X_train, y_train)\nprint(\"Accuracy with normal features using Multinomial Naive Bayes:\\n\", nb.score(X_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T11:47:50.265618Z","iopub.execute_input":"2021-06-29T11:47:50.265903Z","iopub.status.idle":"2021-06-29T11:47:50.329896Z","shell.execute_reply.started":"2021-06-29T11:47:50.265876Z","shell.execute_reply":"2021-06-29T11:47:50.328668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing ngrammed features using MultinomialNB\n\nnb_ngram = naive_bayes.MultinomialNB()\nnb_ngram.fit(X_train_ngram, y_train_ngram)\nprint(\"Accuracy with n-gram features using Multinomial Naive Bayes:\\n\", nb_ngram.score(X_test_ngram, y_test_ngram))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T11:47:50.33154Z","iopub.execute_input":"2021-06-29T11:47:50.331968Z","iopub.status.idle":"2021-06-29T11:47:50.562886Z","shell.execute_reply.started":"2021-06-29T11:47:50.331923Z","shell.execute_reply":"2021-06-29T11:47:50.561731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### apparantly using stopwords and ngrams wins. ","metadata":{}}]}