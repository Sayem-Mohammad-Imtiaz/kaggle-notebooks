{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Abstract\n\nThis notebook attempts to predict the churning of bank clients after six (6) months based on the dataset (see [Bank Turnover Dataset](https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling)). The data is composed of 12 variables and 10,000 records.\n\nInitially, a baseline was established using unscaled data to determine the best model which yieled the highest accuracy. At this stage, `XGB` topped all models\nwith an accuracy of 83.05%. \n\nSelect variables (`CreditScore`, `Age`, `Tenure`, `Balance`, and `EstimatedSalary`) where then scaled and `SVC` achieved the highest accuracy rate\nwith 83.75% while `XGB` closely followed with 83.10%. \n\nBoth `SVC` and `XGB` where then tuned based on a grid search. `SVC` used `C=0.1` and `kernel_values=linear` while `XGB` was tuned using`colsample_bytree=0.8`, `gamma=5`, `max_depth=4`, `min_child_weight=1`, and `subsample=1`, resulting to an accuracy score of 83.10% for `SVC` and a significant improvement for\n`XGB` with 85.30%. \n\nEnsemble methods were used and it only yielded an accuracy score of 84.95% using `GradientBoosting`. \n\nPredicting the test dataset using the tuned `XGB` and `GradientBoosting` resulted to accuracy scores of 85.78% and 86.9%, respectively.\n\nInterpreting the models, `NumOfProducts`, `Age`, and `Balance` are the top predictors for `XGB`. For `GradientBoosting`, `Age`, `NumOfProducts`, and `IsActiveMember` are the\ntop predictors of churn."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\nfrom xgboost import plot_importance\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/bank-customer-churn-modeling/Churn_Modelling.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove RowNumber and Customer Id Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove RowNumber and CustomerId\ndf = df.iloc[:,2:14]\n\n# Add Surname2 col\nfoo = df['Surname'].value_counts()\nfoo = pd.DataFrame(foo)\nfoo['Name'] = foo.index\nfoo = foo[foo['Surname'] > 1]\nfoo = foo.Name.unique()\nfoo = list(foo)\nfoo.sort()\nlen(foo)\nlen(df.Surname.unique())\ndf['Surname2'] = 0\ndf.loc[df['Surname'].isin(foo), ['Surname2']] = 1\n\n# # Remove Surname col\n# df = df.iloc[:,1:14]\n\n# Rearrange cols\ncols = ['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance',\n       'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary',\n       'Surname2', 'Exited']\n\ndf = df[cols]\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot of Variables"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Analyze numeric cols\nnum_cols = ['CreditScore', 'Age','Tenure','Balance','NumOfProducts','HasCrCard','IsActiveMember','EstimatedSalary', 'Surname2', 'Exited']\ndf_num = df[num_cols]\n\nsns.set_style('darkgrid')\nsns.pairplot(df_num, height=1)\n\n#...scale CreditScore, Age, Tenure, Balance, EstimatedSalary\n# NumOfProducts is already categorized\n#...retain binomial: HasCrCard, IsActiveMember, Surname2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Re-encode Label Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"lab_enc = LabelEncoder()\ncols = ['Geography','Gender']\n\nfor _ in cols:\n    df[_] = lab_enc.fit_transform(df[_])\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Train and Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.iloc[:,0:11]\ny = df.iloc[:,-1]\nx_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.2, random_state=7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compare Algorithms on Unscaled Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(('LR', LogisticRegression(max_iter=1000)))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('SVC', SVC()))\nmodels.append(('XGB', XGBClassifier()))\nmodels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_cv = []\nmy_names = []\n\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    cv = cross_val_score(model, x_train, y_train, cv=kfold, scoring='accuracy')\n    my_names.append(name)\n    my_cv.append(cv)\n    msg = ('%s %f (%f)' % (name, cv.mean(), cv.std()))\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nfig.suptitle('Comparison of Algorithms on Unscaled Data (Baseline)')\nax = fig.add_subplot(111)\nplt.boxplot(my_cv)\nax.set_xticklabels(my_names)\nplt.show()\n#...XGB has the highest accuracy at 83.05% with a std of 0.01850","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Comparison of Algorithms on Scaled Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all_cols = df.columns\ncols = ['CreditScore','Age','Tenure','Balance','EstimatedSalary']\ncols2 = ['Geography', 'Gender', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'Surname2', 'Exited']\n\ndf_cats = df[cols]\nscaler = StandardScaler().fit(df_cats)\nfoo = pd.DataFrame(scaler.transform(df_cats))\nfoo.columns = ['CreditScore','Age','Tenure','Balance','EstimatedSalary']\n\ndf_scaled = pd.concat([foo, df[cols2]], axis=1)\n\nsns.set_style('darkgrid')\nsns.pairplot(df_scaled[cols], height=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Train and Test Split from Scaled Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df_scaled.iloc[:,0:11]\ny = df_scaled.iloc[:,-1]\nx_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.2, random_state=7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_cv = []\nmy_names = []\n\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    cv = cross_val_score(model, x_train, y_train, cv=kfold, scoring='accuracy')\n    my_names.append(name)\n    my_cv.append(cv)\n    msg = ('%s %f (%f)' % (name, cv.mean(), cv.std()))\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nfig.suptitle('Comparison of Algorithms on Scaled Data')\nax = fig.add_subplot(111)\nplt.boxplot(my_cv)\nax.set_xticklabels(my_names)\nplt.show()\\\n#...SVC's accuracy significantly jumped to 83.75% with an std of 0.026481","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tune SVC and Use on Scaled Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\nkernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\nparam_grid = dict(C=c_values, kernel=kernel_values)\n\n#Note: x_train data that was used is scaled\nmodel = XGBClassifier()\nkfold = KFold(n_splits = 10, random_state=7)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=kfold)\ngrid_result = grid.fit(x_train, y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('%f (%f) with %r' % (mean, stdev, param))\n#...accuracy for tuned SVC on scaled data slightly worsened to 83.10%","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tune XGB and Use on Scaled Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"min_child_weight = [1, 5, 10]\ngamma = [0.5, 1, 1.5, 2, 5]\nsubsample = [0.6, 0.8, 1.0]\ncolsample_bytree =  [0.6, 0.8, 1.0]\nmax_depth = [3, 4, 5]\n        \nparam_grid = dict(min_child_weight = min_child_weight, gamma = gamma, subsample = subsample, colsample_bytree = colsample_bytree, max_depth = max_depth)\nmodel = XGBClassifier()\nkfold = KFold(n_splits = 10, random_state=7)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=kfold)\ngrid_result = grid.fit(x_train, y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('%f (%f) with %r' % (mean, stdev, param))\n#...the accuracy of the tuned xgb on scaled data significantly jumped to 85.3% using colsample_bytree=0.8, gamma=5, max_depth=4, min_child_weight=1, and subsample=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensemble Methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"ensembles = []\nensembles.append(('ADA', AdaBoostClassifier()))\nensembles.append(('GB', GradientBoostingClassifier()))\nensembles.append(('BC', BaggingClassifier()))\nensembles.append(('ET', ExtraTreesClassifier()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_cv = []\nmy_names = []\n\n#\nfor name, model in ensembles:\n    kfold = KFold(n_splits=10, random_state=7)\n    cv = cross_val_score(model, x_train, y_train, cv=kfold, scoring='accuracy')\n    my_names.append(name)\n    my_cv.append(cv)\n    msg = ('%s %f (%f)' % (name, cv.mean(), cv.std()))\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nfig.suptitle('Accuracy Comparison of Algorithms Using Ensembles')\nax = fig.add_subplot(111)\nplt.boxplot(my_cv)\nax.set_xticklabels(my_names)\nplt.show()\n#...GB resulted to 84.95 using scaled data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict Using Tuned XGB on Scaled Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier(colsample_bytree=0.8, gamma=5, max_depth=4, min_child_weight=1, subsample=1)\nmodel.fit(x_train, y_train)\npredictions = model.predict(x_test)\nprint(accuracy_score(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))\nplot_importance(model)\n#...accuracy is at 85.78%, important vars are NumOfProducts, Age, and Balance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict Using Gradient Boosting on Scaled Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = GradientBoostingClassifier()\nmodel.fit(x_train, y_train)\npredictions = model.predict(x_test)\nprint(accuracy_score(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))\nprint(model.feature_importances_)\nprint(x_test.columns)\n#...acccuracy is highest at 85.9%","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}