{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction to time series using Tensorflow","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### What is Time Series?\n\nTime series is every where, like stock prices, wheather forecast, historical trends and much more. \n\nWhat exactly is a time series? It's typically defined as an ordered sequence of values that are usually equally spaced over time.\n\nWhen we work with single value in time series it is called as Univariate Time Series and similarly when we work with multiple values in time series it is called as Multivariate Time Series. Like stock price of a company, wheather forecast of a place or one historical trend are all Univariate Time Series whereas birth versus death in one place between a time period is Multivariate Time Series. Multivariate Time Series can be a useful in understanding impact of related data just like in birth versus death case. Although they can be treated as separate univariate time series but when shown together they give more insights. Movement of a body can also be plotted as a series of univariates or as a combined multivariate.\n\nAnything that has time factor in it can be analysed using time series.\n\n### What are Applications of Machine Learning in Time Series?\n\n- Prediction of Forecasts of the data\n- Imputation (projecting backwards in past)\n- Detect Anaomalies (For example, in website logs so that you could see potential denial of service attacks showing up as a spike on the time series)\n- Analyze the time series to spot patterns in them that determine what generated the series itself (A classic example of this is to analyze sound waves to spot words in them which can be used as a neural network for speech recognition)\n\n### Common patterns in time series\n\n- Trends (upward, downward, constant) Here time series has specific dimension in which they move\n- Seasonality (pattern repeats in predictable intervals)\n- Trends + Seasonaity\n- Noise (example: white noise)\n- Autocorrelation (no trend, no seasonality, it correlates with a delayed copy of itself often called a lag)\n\nReal word = Trends + Seasonality + Noise + Autocorrelation\n\nA machine-learning model is designed to spot patterns, and when we spot patterns we can make predictions. For the most part this can also work with time series except for the noise which is unpredictable. But we should recognize that this assumes that patterns that existed in the past will of course continue on into the future. Of course, real life time series are not always that simple. Their behavior can change drastically over time. \n\nFor example price which was in upward trend goes down drastically (maybe due to financial crisis, big event in company, etc...)\n\nWe'll typically call this a **non-stationary time series**. To predict on this we could just train for limited period of time. For example, here where I take just the last 100 steps. You'll probably get a better performance than if you had trained on the entire time series. But that's breaking the mold for typical machine, learning where we always assume that more data is better. But for time series forecasting it really depends on the time series. If it's stationary, meaning its behavior does not change over time, then great. The more data you have the better. But if it's not stationary then the optimal time window that you should use for training will vary. Ideally, we would like to be able to take the whole series into account and generate a prediction for what might happen next. As you can see, this isn't always as simple as you might think given a drastic change like the one we see here.","metadata":{}},{"cell_type":"markdown","source":"### Different scenarios for Time Series","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\nfrom pandas.plotting import autocorrelation_plot\n\nfrom statsmodels.tsa.arima_model import ARIMA","metadata":{"execution":{"iopub.status.busy":"2021-07-27T12:39:51.484206Z","iopub.execute_input":"2021-07-27T12:39:51.484591Z","iopub.status.idle":"2021-07-27T12:39:52.326495Z","shell.execute_reply.started":"2021-07-27T12:39:51.484511Z","shell.execute_reply":"2021-07-27T12:39:52.325684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For plot time series plots\ndef plot_series(time, series, format='-', start=0, end=None, label=None):\n    plt.plot(time[start:end], series[start:end], format, label=label)\n    plt.xlabel('Time')\n    plt.ylabel('Value')\n    \n    if label:\n        plt.legend(fontsize=14)\n    plt.grid(True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Trends","metadata":{}},{"cell_type":"code","source":"def trend(time, slope=0):\n    return slope * time\n\n\n# 4years (including leap year, therefore + 1 day)\ntime = np.arange(4 * 365 + 1)\n\nupward_series = trend(time, 0.1)\nconstant_series = trend(time, 0)\ndownward_series = trend(time, -0.1)\n\nf, axs = plt.subplots(1, 3, figsize=(20, 4))\naxs[0].plot(time, upward_series)\naxs[1].plot(time, constant_series)\naxs[2].plot(time, downward_series)\n\nfor ax in axs:\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Seasonal","metadata":{}},{"cell_type":"code","source":"def seasonal_pattern(season_time):\n    '''Just an arbitrary pattern, you can change it if you wish'''\n    \n    # For season_time < 0.4, np.cos(season_time * 2 * np.pi) transformation will be applied\n    # and for season_time >= 0.4, 1 / np.exp(3 * season_time) transformation will be applied\n    # This will bring a single pattern\n    \n    return np.where(\n        season_time < 0.4,\n        np.cos(season_time * 2 * np.pi),\n        1 / np.exp(3 * season_time)\n    )\n\n\ndef seasonality(time, period, amplitude=1, phase=0):\n    '''Repeat the same pattern at each period'''\n    season_time = ((time + phase) % period) / period\n    return amplitude * seasonal_pattern(season_time)\n\n\namplitude = 40\nseries = seasonality(time, period=365, amplitude=amplitude)\n\nplt.figure(figsize=(10, 4))\nplot_series(time, series)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Trends + Seasonality","metadata":{}},{"cell_type":"code","source":"baseline = 0\nslope = 0.05\nseries = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n\nplt.figure(figsize=(10, 6))\nplot_series(time, series)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Noise","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:41:21.64562Z","iopub.execute_input":"2021-07-26T07:41:21.646189Z","iopub.status.idle":"2021-07-26T07:41:21.650379Z","shell.execute_reply.started":"2021-07-26T07:41:21.646153Z","shell.execute_reply":"2021-07-26T07:41:21.649275Z"}}},{"cell_type":"code","source":"def white_noise(time, noise_level=1, seed=None):\n    rnd = np.random.RandomState(seed)\n    return rnd.randn(len(time)) * noise_level\n\n\nnoise_level = 5\nnoise = white_noise(time, noise_level, seed=10)\n\nplt.figure(figsize=(10, 4))\nplot_series(time, noise)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Time Series + Noise","metadata":{}},{"cell_type":"code","source":"series += noise\n\nplt.figure(figsize=(10, 4))\nplot_series(time, series)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Simple forecasting using the above data","metadata":{}},{"cell_type":"code","source":"split_time = 1000\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def autocorrelation(time, amplitude, seed=None):\n    rnd = np.random.RandomState(seed)\n    φ1 = 0.5\n    φ2 = -0.1\n    ar = rnd.randn(len(time) + 50)\n    ar[:50] = 100\n    for step in range(50, len(time) + 50):\n        ar[step] += φ1 * ar[step - 50]\n        ar[step] += φ2 * ar[step - 33]\n    return ar[50:] * amplitude\n\n\ndef autocorrelation(time, amplitude, seed=None):\n    rnd = np.random.RandomState(seed)\n    φ = 0.8\n    ar = rnd.randn(len(time) + 1)\n    for step in range(1, len(time) + 1):\n        ar[step] += φ * ar[step - 1]\n    return ar[1:] * amplitude","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series = autocorrelation(time, 10, seed=42)\nplot_series(time[:200], series[:200])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series = autocorrelation(time, 10, seed=42) + trend(time, 2)\nplot_series(time[:200], series[:200])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series = autocorrelation(time, 10, seed=42) + seasonality(time, period=50, amplitude=150) + trend(time, 2)\nplot_series(time[:200], series[:200])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series = autocorrelation(time, 10, seed=42) + seasonality(time, period=50, amplitude=150) + trend(time, 2)\nseries2 = autocorrelation(time, 5, seed=42) + seasonality(time, period=50, amplitude=2) + trend(time, -1) + 550\nseries[200:] = series2[200:]\n# series += noise(time, 30)\nplot_series(time[:300], series[:300])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def impulses(time, num_impulses, amplitude=1, seed=None):\n    rnd = np.random.RandomState(seed)\n    impulse_indices = rnd.randint(len(time), size=10)\n    series = np.zeros(len(time))\n    for index in impulse_indices:\n        series[index] += rnd.rand() * amplitude\n    return series    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series = impulses(time, 10, seed=42)\nplot_series(time, series)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def autocorrelation(source, φs):\n    ar = source.copy()\n    max_lag = len(φs)\n    for step, value in enumerate(source):\n        for lag, φ in φs.items():\n            if step - lag > 0:\n                ar[step] += φ * ar[step - lag]\n    return ar","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"signal = impulses(time, 10, seed=42)\nseries = autocorrelation(signal, {1: 0.99})\nplot_series(time, series)\nplt.plot(time, signal, 'k-')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"signal = impulses(time, 10, seed=42)\nseries = autocorrelation(signal, {1: 0.70, 50: 0.2})\nplot_series(time, series)\nplt.plot(time, signal, 'k-')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series_diff1 = series[1:] - series[:-1]\nplot_series(time[1:], series_diff1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autocorrelation_plot(series)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ARIMA, short for 'Auto Regressive Integrated Moving Average' is actually a class of models that 'explains' a given time series based on its own past values, that is, its own lags and the lagged forecast errors, so that equation can be used to forecast future values.","metadata":{}},{"cell_type":"code","source":"model = ARIMA(series, order=(5, 1, 0))\nmodel_fit = model.fit(disp=0)\nprint(model_fit.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sunspots_path = '../input/sunspots/Sunspots.csv'\ndf = pd.read_csv(sunspots_path, parse_dates=['Date'], index_col='Date')\nseries = df['Monthly Mean Total Sunspot Number'].asfreq('1M')\nseries.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series.plot(figsize=(12, 4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series['1995-01-01':].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series.diff(1).plot()\nplt.axis([0, 100, -50, 50])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autocorrelation_plot(series)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autocorrelation_plot(series.diff(1)[1:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autocorrelation_plot(series.diff(1)[1:].diff(11 * 12)[11*12+1:])\nplt.axis([0, 500, -0.1, 0.1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autocorrelation_plot(series.diff(1)[1:])\nplt.axis([0, 50, -0.1, 0.1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"116.7 - 104.3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[series.autocorr(lag) for lag in range(1, 50)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series_diff = series\nfor lag in range(50):\n    series_diff = series_diff[1:] - series_diff[:-1]\n\nautocorrelation_plot(series_diff)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series_diff1 = pd.Series(series[1:] - series[:-1])\nautocorrs = [series_diff1.autocorr(lag) for lag in range(1, 60)]\nplt.plot(autocorrs)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training and Validation sets\n\n**Naive Forecasting** (take the last value and assume that the next value will be the same one, We can do that to get a baseline at the very least, and believe it or not, that baseline can be pretty good).\n\nTo measure the performance of our forecasting model, we typically want to split the time series into a training period, a validation period and a test period. This is called **fixed partitioning**. If the time series has some seasonality, you generally want to ensure that each period contains a whole number of seasons.\n\nYou'll train the model on the training period and evaluate it on the validation period. Once you get good results in training and validation data, train the model using the training and validation data then test the model on test data if model is giving good results then you can go ahead and train the model with test data too. Test data is the closest data you have to the current point in time therefore you might want to train using the test data. If your model is not trained using that data, too, then it may not be optimal. Due to this it is very common to use just training and validation data and the test data is in future.\n\nFixed partitioning like this is very simple and very intuitive, but there's also another way. We start with a short training period, and we gradually increase it, say by one day at a time, or by one week at a time. At each iteration, we train the model on a training period. And we use it to forecast the following day, or the following week, in the validation period. And this is called **roll-forward partitioning**. You could see it as doing fixed partitioning a number of times, and then continually refining the model as such.","metadata":{}},{"cell_type":"markdown","source":"### Metrics for evaluating performance\n\n**error = forecast - actual**\n\n#### Metrics\n\n**mse = np.square(errors).mean()**\nWe square to errors to get rid of negative values, so that negative and positive values don't cancel each other\n\nIf we want the mean of our errors' calculation to be of the same scale as the original errors\n**rmse = np.sqrt(mse)**\n\n**mae = np.abs(errors).mean()**\nMean Absolute Deviation. In this case, instead of squaring to get rid of negatives, it just uses their absolute value. This does not penalize large errors as much as the mse does. Depending on your task, you may prefer the mae or the mse. For example, if large errors are potentially dangerous and they cost you much more than smaller errors, then you may prefer the mse. But if your gain or your loss is just proportional to the size of the error, then the mae may be better.\n\n**mape = np.abs(errors / x_valid).mean()**\nMean Absolute Percentage Error. This gives an idea of the size of the errors compared to the values.","metadata":{}},{"cell_type":"markdown","source":"### Moving average and differencing\n\nA common and very simple forecasting method is to calculate **moving average**. We take average of our time series over a fixed period called an averaging window. Now this nicely eliminates a lot of the noise and it gives us a curve roughly emulating the original series, but it does not anticipate trend or seasonality. Depending on the current time i.e. the period after which you want to forecast for the future, it can actually end up being worse than a naive forecast.\n\nForecasts = Window Mean\n\nOne method to avoid this is to remove the trend and seasonality from the time series with a technique called **differencing**. So instead of studying the time series itself, we study the difference between the value at time t and the value at an earlier period (t - period). Depending on the time of your data, that period might be a year, a day, a month or whatever. We'll get this difference time series which has no trend and no seasonality. We can then use a moving average to forecast this time series which gives us these forecasts. But these are just forecasts for the difference time series, not the original time series. To get the final forecasts for the original time series, we just need to add back the value at difference (t - period)\n\nForecasts = Moving average of different series + series(t + period)\n\nOur moving average removed a lot of noise but our final forecasts are still pretty noisy. Where does that noise come from? Well, that's coming from the past values that we added back into our forecasts. So we can improve these forecasts by also removing the past noise using a moving average on that. If we do that, we get much smoother forecasts.\n\nForecasts = Trailing moving average of differenced series + centered moving average of past series (t - period)\n\n#### Trailing versus centered window\n\nNote that when we use the trailing window when computing the moving average of present values, let's say from t minus 30 to t minus one. But when we use a centered window to compute the moving average of past values from one year ago, let's say t minus one year minus five days, to t minus one year plus five days. Then moving averages using centered windows can be more accurate than using trailing windows. But we can't use centered windows to smooth present values since we don't know future values. However, to smooth past values we can afford to use centered windows. \n\nSimple approaches sometimes can work just fine.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras","metadata":{"execution":{"iopub.status.busy":"2021-07-27T12:41:05.439545Z","iopub.execute_input":"2021-07-27T12:41:05.43987Z","iopub.status.idle":"2021-07-27T12:41:09.479086Z","shell.execute_reply.started":"2021-07-27T12:41:05.439838Z","shell.execute_reply":"2021-07-27T12:41:09.47826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For plot time series plots\ndef plot_series(time, series, format='-', start=0, end=None, label=None):\n    plt.plot(time[start:end], series[start:end], format, label=label)\n    plt.xlabel('Time')\n    plt.ylabel('Value')\n    \n    if label:\n        plt.legend(fontsize=14)\n    plt.grid(True)\n    \n    \ndef trend(time, slope=0):\n    return slope * time\n\n\ndef seasonal_pattern(season_time):\n    '''Just an arbitrary pattern, you can change it if you wish'''\n    \n    # For season_time < 0.4, np.cos(season_time * 2 * np.pi) transformation will be applied\n    # and for season_time >= 0.4, 1 / np.exp(3 * season_time) transformation will be applied\n    # This will bring a single pattern\n    \n    return np.where(\n        season_time < 0.4,\n        np.cos(season_time * 2 * np.pi),\n        1 / np.exp(3 * season_time)\n    )\n\n\ndef seasonality(time, period, amplitude=1, phase=0):\n    '''Repeat the same pattern at each period'''\n    season_time = ((time + phase) % period) / period\n    return amplitude * seasonal_pattern(season_time)\n\n\ndef noise(time, noise_level=1, seed=None):\n    rnd = np.random.RandomState(seed)\n    return rnd.randn(len(time)) * noise_level","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time = np.arange(4 * 365 + 1, dtype='float32')\nseries = trend(time, 0.1)  \nbaseline = 10\namplitude = 40\nslope = 0.05\nnoise_level = 5\n\n# Create the series\nseries = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n# Update with noise\nseries += noise(time, noise_level, seed=42)\n\nplt.figure(figsize=(10, 6))\nplot_series(time, series)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting the data","metadata":{}},{"cell_type":"code","source":"split_time = 1000\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]\n\nplt.figure(figsize=(10, 6))\nplot_series(time_train, x_train)\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Naive Forecast","metadata":{}},{"cell_type":"code","source":"naive_forecast = series[split_time - 1: -1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, naive_forecast)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's zoom in on the start of the validation period","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid, start=0, end=150)\nplot_series(time_valid, naive_forecast, start=1, end=151)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see that the naive forecast lags 1 step behind the time series.\n\nNow let's compute the mean squared error and the mean absolute error between the forecasts and the predictions in the validation period","metadata":{}},{"cell_type":"code","source":"print(keras.metrics.mean_squared_error(x_valid, naive_forecast).numpy())\nprint(keras.metrics.mean_absolute_error(x_valid, naive_forecast).numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's our baseline, now let's try a moving average","metadata":{}},{"cell_type":"code","source":"def moving_average_forecast(series, window_size):\n    '''Forecasts the mean of the last few values.\n        If window_size=1, then this is equivalent to naive forecast'''\n    \n    forecast = []\n    for time in range(len(series) - window_size):\n        forecast.append(series[time: time+window_size].mean())\n    return np.array(forecast)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"moving_avg = moving_average_forecast(series, 30)[split_time - 30:]\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, moving_avg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(keras.metrics.mean_squared_error(x_valid, moving_avg).numpy())\nprint(keras.metrics.mean_absolute_error(x_valid, moving_avg).numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's worse than naive forecast! The moving average does not anticipate trend or seasonality, so let's try to remove them by using differencing. Since the seasonality period is 365 days, we will subtract the value at time *t* – 365 from the value at time *t*.","metadata":{}},{"cell_type":"code","source":"diff_series = (series[365:] - series[:-365])\ndiff_time = time[365:]\n\nplt.figure(figsize=(10, 6))\nplot_series(diff_time, diff_series)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great, the trend and seasonality seem to be gone, so now we can use the moving average","metadata":{}},{"cell_type":"code","source":"diff_moving_avg = moving_average_forecast(diff_series, 50)[split_time - 365 - 50:]\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, diff_series[split_time - 365:])\nplot_series(time_valid, diff_moving_avg)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's bring back the trend and seasonality by adding the past values from t – 365","metadata":{}},{"cell_type":"code","source":"diff_moving_avg_plus_past = series[split_time - 365:-365] + diff_moving_avg\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, diff_moving_avg_plus_past)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(keras.metrics.mean_squared_error(x_valid, diff_moving_avg_plus_past).numpy())\nprint(keras.metrics.mean_absolute_error(x_valid, diff_moving_avg_plus_past).numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Better than naive forecast, good. However the forecasts look a bit too random, because we're just adding past values, which were noisy. Let's use a moving averaging on past values to remove some of the noise","metadata":{}},{"cell_type":"code","source":"diff_moving_avg_plus_smooth_past = moving_average_forecast(series[split_time - 370:-360], 10) + diff_moving_avg\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, diff_moving_avg_plus_smooth_past)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(keras.metrics.mean_squared_error(x_valid, diff_moving_avg_plus_smooth_past).numpy())\nprint(keras.metrics.mean_absolute_error(x_valid, diff_moving_avg_plus_smooth_past).numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deep Neural Network for Time Series\n\n### Preparing features and labels","metadata":{}},{"cell_type":"code","source":"dataset = tf.data.Dataset.range(10)\nfor val in dataset:\n    print(val.numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = tf.data.Dataset.range(10)\ndataset = dataset.window(5, shift=1)\nfor window_dataset in dataset:\n    for val in window_dataset:\n        print(val.numpy(), end=' ')\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = tf.data.Dataset.range(10)\ndataset = dataset.window(5, shift=1, drop_remainder=True) # having the dataset window of same size\nfor window_dataset in dataset:\n    for val in window_dataset:\n        print(val.numpy(), end=' ')\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = tf.data.Dataset.range(10)\ndataset = dataset.window(5, shift=1, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(5)) # creating batch of 5 values\nfor window in dataset:\n    print(window.numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the dataset into features and labels\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.window(5, shift=1, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(5))\ndataset = dataset.map(lambda window: (window[:-1], window[-1:]))\nfor x, y in dataset:\n    print(x.numpy(), y.numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shuffling the dataset\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.window(5, shift=1, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(5))\ndataset = dataset.map(lambda window: (window[:-1], window[-1:]))\ndataset = dataset.shuffle(buffer_size=10)  # 10 - number of items\nfor x, y in dataset:\n    print(x.numpy(), y.numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating batches\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.window(5, shift=1, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(5))\ndataset = dataset.map(lambda window: (window[:-1], window[-1:]))\ndataset = dataset.shuffle(buffer_size=10)\ndataset = dataset.batch(2).prefetch(1)\nfor x, y in dataset:\n    print(f'x = {x.numpy()}')\n    print(f'y = {y.numpy()}')\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Sequence bias\n\nSequence bias is when the order of things can impact the selection of things. For example, if I were to ask you your favorite TV show, and listed \"Game of Thrones\", \"Killing Eve\", \"Travellers\" and \"Doctor Who\" in that order, you're probably more likely to select 'Game of Thrones' as you are familiar with it, and it's the first thing you see. Even if it is equal to the other TV shows. So, when training data in a dataset, we don't want the sequence to impact the training in a similar way, so it's good to shuffle them up. ","metadata":{}},{"cell_type":"markdown","source":"#### Feeding windowed dataset into neural network","metadata":{}},{"cell_type":"code","source":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    dataset = tf.data.Dataset.from_tensor_slices(seires)\n    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n    dataset = dataset.shuffle(shuffle_buffer)\n    dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n    dataset = dataset.batch(batch_size).prefetch(1)\n    return dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Single Neural Network","metadata":{}},{"cell_type":"markdown","source":"Creating synthetic data","metadata":{}},{"cell_type":"code","source":"# For plot time series plots\ndef plot_series(time, series, format='-', start=0, end=None, label=None):\n    plt.plot(time[start:end], series[start:end], format, label=label)\n    plt.xlabel('Time')\n    plt.ylabel('Value')\n    \n    if label:\n        plt.legend(fontsize=14)\n    plt.grid(True)\n    \n    \ndef trend(time, slope=0):\n    return slope * time\n\n\ndef seasonal_pattern(season_time):\n    '''Just an arbitrary pattern, you can change it if you wish'''\n    \n    # For season_time < 0.4, np.cos(season_time * 2 * np.pi) transformation will be applied\n    # and for season_time >= 0.4, 1 / np.exp(3 * season_time) transformation will be applied\n    # This will bring a single pattern\n    \n    return np.where(\n        season_time < 0.4,\n        np.cos(season_time * 2 * np.pi),\n        1 / np.exp(3 * season_time)\n    )\n\n\ndef seasonality(time, period, amplitude=1, phase=0):\n    '''Repeat the same pattern at each period'''\n    season_time = ((time + phase) % period) / period\n    return amplitude * seasonal_pattern(season_time)\n\n\ndef noise(time, noise_level=1, seed=None):\n    rnd = np.random.RandomState(seed)\n    return rnd.randn(len(time)) * noise_level","metadata":{"execution":{"iopub.status.busy":"2021-07-27T12:40:53.524361Z","iopub.execute_input":"2021-07-27T12:40:53.524736Z","iopub.status.idle":"2021-07-27T12:40:53.534155Z","shell.execute_reply.started":"2021-07-27T12:40:53.524705Z","shell.execute_reply":"2021-07-27T12:40:53.533244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time = np.arange(4 * 365 + 1, dtype='float32')\nseries = trend(time, 0.1)  \nbaseline = 10\namplitude = 40\nslope = 0.05\nnoise_level = 5\n\n# Create the series\nseries = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n\n# Update with noise\nseries += noise(time, noise_level, seed=42)\n\nsplit_time = 1000\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]\n\nwindow_size = 20\nbatch_size = 32\nshuffle_buffer_size = 1000","metadata":{"execution":{"iopub.status.busy":"2021-07-27T12:40:54.168598Z","iopub.execute_input":"2021-07-27T12:40:54.168931Z","iopub.status.idle":"2021-07-27T12:40:54.178243Z","shell.execute_reply.started":"2021-07-27T12:40:54.1689Z","shell.execute_reply":"2021-07-27T12:40:54.176795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    dataset = tf.data.Dataset.from_tensor_slices(series)\n    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n    dataset = dataset.shuffle(shuffle_buffer)\n    dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n    dataset = dataset.batch(batch_size).prefetch(1)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2021-07-27T12:40:55.0354Z","iopub.execute_input":"2021-07-27T12:40:55.035723Z","iopub.status.idle":"2021-07-27T12:40:55.042184Z","shell.execute_reply.started":"2021-07-27T12:40:55.035693Z","shell.execute_reply":"2021-07-27T12:40:55.04112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\nprint(dataset)\n\nlayer_0 = tf.keras.layers.Dense(1, input_shape=[window_size])\nmodel = tf.keras.models.Sequential([layer_0])\n\nmodel.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))\nmodel.fit(dataset, epochs=100, verbose=0)\n\nprint(f'Layer weights: {layer_0.get_weights()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forecast = []\n\nfor time in range(len(series) - window_size):\n    forecast.append(model.predict(series[time: time+window_size][np.newaxis]))\n    \nforecast = forecast[split_time-window_size:]\nresults = np.array(forecast)[:, 0, 0]\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Deep Neural Network","metadata":{}},{"cell_type":"code","source":"dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(10, input_shape=[window_size], activation='relu'), \n    tf.keras.layers.Dense(10, activation='relu'), \n    tf.keras.layers.Dense(1)\n])\n\nmodel.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))\nmodel.fit(dataset, epochs=100, verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forecast = []\n\nfor time in range(len(series) - window_size):\n    forecast.append(model.predict(series[time: time+window_size][np.newaxis]))\n    \nforecast = forecast[split_time-window_size:]\nresults = np.array(forecast)[:, 0, 0]\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(10, input_shape=[window_size], activation='relu'), \n    tf.keras.layers.Dense(10, activation='relu'), \n    tf.keras.layers.Dense(1)\n])\n\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 20))\noptimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n\nmodel.compile(loss='mse', optimizer=optimizer)\nhistory = model.fit(dataset, epochs=100, callbacks=[lr_schedule], verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lrs = 1e-8 * (10 ** (np.arange(100) / 20))\nplt.semilogx(lrs, history.history['loss'])\nplt.axis([1e-8, 1e-3, 0, 300])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"window_size = 30\ndataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Dense(10, activation='relu', input_shape=[window_size]),\n  tf.keras.layers.Dense(10, activation='relu'),\n  tf.keras.layers.Dense(1)\n])\n\noptimizer = tf.keras.optimizers.SGD(lr=8e-6, momentum=0.9)\nmodel.compile(loss='mse', optimizer=optimizer)\nhistory = model.fit(dataset, epochs=500, verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = history.history['loss']\nepochs = range(len(loss))\nplt.plot(epochs, loss, 'b', label='Training Loss')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot all but the first 10\nloss = history.history['loss']\nepochs = range(10, len(loss))\nplot_loss = loss[10:]\nprint(plot_loss)\nplt.plot(epochs, plot_loss, 'b', label='Training Loss')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forecast = []\nfor time in range(len(series) - window_size):\n    forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n\nforecast = forecast[split_time-window_size:]\nresults = np.array(forecast)[:, 0, 0]\n\n\nplt.figure(figsize=(10, 6))\n\nplot_series(time_valid, x_valid)\nplot_series(time_valid, results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RNN","metadata":{}},{"cell_type":"code","source":"# For plot time series plots\ndef plot_series(time, series, format='-', start=0, end=None, label=None):\n    plt.plot(time[start:end], series[start:end], format, label=label)\n    plt.xlabel('Time')\n    plt.ylabel('Value')\n    \n    if label:\n        plt.legend(fontsize=14)\n    plt.grid(True)\n    \n    \ndef trend(time, slope=0):\n    return slope * time\n\n\ndef seasonal_pattern(season_time):\n    '''Just an arbitrary pattern, you can change it if you wish'''\n    \n    # For season_time < 0.4, np.cos(season_time * 2 * np.pi) transformation will be applied\n    # and for season_time >= 0.4, 1 / np.exp(3 * season_time) transformation will be applied\n    # This will bring a single pattern\n    \n    return np.where(\n        season_time < 0.4,\n        np.cos(season_time * 2 * np.pi),\n        1 / np.exp(3 * season_time)\n    )\n\n\ndef seasonality(time, period, amplitude=1, phase=0):\n    '''Repeat the same pattern at each period'''\n    season_time = ((time + phase) % period) / period\n    return amplitude * seasonal_pattern(season_time)\n\n\ndef noise(time, noise_level=1, seed=None):\n    rnd = np.random.RandomState(seed)\n    return rnd.randn(len(time)) * noise_level","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time = np.arange(4 * 365 + 1, dtype='float32')\nseries = trend(time, 0.1)  \nbaseline = 10\namplitude = 40\nslope = 0.05\nnoise_level = 5\n\n# Create the series\nseries = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n\n# Update with noise\nseries += noise(time, noise_level, seed=42)\n\nsplit_time = 1000\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]\n\nwindow_size = 20\nbatch_size = 32\nshuffle_buffer_size = 1000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    dataset = tf.data.Dataset.from_tensor_slices(series)\n    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n    dataset = dataset.shuffle(shuffle_buffer)\n    dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n    dataset = dataset.batch(batch_size).prefetch(1)\n    return dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = windowed_dataset(x_train, window_size, batch_size=128, shuffle_buffer=shuffle_buffer_size)\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]),\n  tf.keras.layers.SimpleRNN(40, return_sequences=True),\n  tf.keras.layers.SimpleRNN(40),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 100.0)\n])\n\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 20))\noptimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n\nmodel.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=['mae'])\nhistory = model.fit(train_set, epochs=100, callbacks=[lr_schedule], verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.semilogx(history.history['lr'], history.history['loss'])\nplt.axis([1e-8, 1e-4, 0, 30])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\ndataset = windowed_dataset(x_train, window_size, batch_size=128, shuffle_buffer=shuffle_buffer_size)\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]),\n  tf.keras.layers.SimpleRNN(40, return_sequences=True),\n  tf.keras.layers.SimpleRNN(40),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 100.0)\n])\n\n# Explain last Lambda layer\n# if we scale up the outputs by 100, we can help training. The default activation function in the \n# RNN layers is tan H which is the hyperbolic tangent activation. This outputs values between \n# negative one and one. Since the time series values are in that order usually in the 10s like 40s, \n# 50s, 60s, and 70s, then scaling up the outputs to the same ballpark can help us with learning. \n\noptimizer = tf.keras.optimizers.SGD(lr=5e-5, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=['mae'])\nhistory = model.fit(dataset, epochs=400, verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forecast=[]\nfor time in range(len(series) - window_size):\n    forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n\nforecast = forecast[split_time-window_size:]\nresults = np.array(forecast)[:, 0, 0]\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nmae = history.history['mae']\nloss = history.history['loss']\n\nepochs = range(len(loss)) # Get number of epochs\n\n#------------------------------------------------\n# Plot MAE and Loss\n#------------------------------------------------\nplt.plot(epochs, mae, 'r')\nplt.plot(epochs, loss, 'b')\nplt.title('MAE and Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['MAE', 'Loss'])\n\nplt.figure()\n\nepochs_zoom = epochs[200:]\nmae_zoom = mae[200:]\nloss_zoom = loss[200:]\n\n#------------------------------------------------\n# Plot Zoomed MAE and Loss\n#------------------------------------------------\nplt.plot(epochs_zoom, mae_zoom, 'r')\nplt.plot(epochs_zoom, loss_zoom, 'b')\nplt.title('MAE and Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['MAE', 'Loss'])\n\nplt.figure()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LSTMs\n\nBidirectional LSTMs may not always make sense but if it does like in stock price then using it may give good results.","metadata":{}},{"cell_type":"code","source":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\ndataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * 100.0)\n])\n\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 20))\noptimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n\nmodel.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=['mae'])\nhistory = model.fit(dataset, epochs=100, callbacks=[lr_schedule], verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.semilogx(history.history['lr'], history.history['loss'])\nplt.axis([1e-8, 1e-4, 0, 30])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\ndataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * 100.0)\n])\n\nmodel.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9), metrics=['mae'])\nhistory = model.fit(dataset, epochs=500, verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forecast = []\nresults = []\nfor time in range(len(series) - window_size):\n    forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n\nforecast = forecast[split_time-window_size:]\nresults = np.array(forecast)[:, 0, 0]\n\n\nplt.figure(figsize=(10, 6))\n\nplot_series(time_valid, x_valid)\nplot_series(time_valid, results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt\n\n#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nmae = history.history['mae']\nloss = history.history['loss']\n\nepochs = range(len(loss)) # Get number of epochs\n\n#------------------------------------------------\n# Plot MAE and Loss\n#------------------------------------------------\nplt.plot(epochs, mae, 'r')\nplt.plot(epochs, loss, 'b')\nplt.title('MAE and Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['MAE', 'Loss'])\n\nplt.figure()\n\nepochs_zoom = epochs[200:]\nmae_zoom = mae[200:]\nloss_zoom = loss[200:]\n\n#------------------------------------------------\n# Plot Zoomed MAE and Loss\n#------------------------------------------------\nplt.plot(epochs_zoom, mae_zoom, 'r')\nplt.plot(epochs_zoom, loss_zoom, 'b')\nplt.title('MAE and Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['MAE', 'Loss'])\n\nplt.figure()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\ndataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 100.0)\n])\n\n\nmodel.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))\nmodel.fit(dataset,epochs=100, verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\ndataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 100.0)\n])\n\n\nmodel.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))\nmodel.fit(dataset,epochs=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using Convolutional layers","metadata":{}},{"cell_type":"code","source":"time = np.arange(4 * 365 + 1, dtype=\"float32\")\nbaseline = 10\nseries = trend(time, 0.1)  \nbaseline = 10\namplitude = 40\nslope = 0.05\nnoise_level = 5\n\n# Create the series\nseries = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n# Update with noise\nseries += noise(time, noise_level, seed=42)\n\nsplit_time = 1000\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]\n\nwindow_size = 20\nbatch_size = 32\nshuffle_buffer_size = 1000","metadata":{"execution":{"iopub.status.busy":"2021-07-27T12:45:00.384348Z","iopub.execute_input":"2021-07-27T12:45:00.384684Z","iopub.status.idle":"2021-07-27T12:45:00.393132Z","shell.execute_reply.started":"2021-07-27T12:45:00.384653Z","shell.execute_reply":"2021-07-27T12:45:00.392194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    series = tf.expand_dims(series, axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer)\n    ds = ds.map(lambda w: (w[:-1], w[1:]))\n    return ds.batch(batch_size).prefetch(1)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T12:45:05.438626Z","iopub.execute_input":"2021-07-27T12:45:05.438957Z","iopub.status.idle":"2021-07-27T12:45:05.444463Z","shell.execute_reply.started":"2021-07-27T12:45:05.438926Z","shell.execute_reply":"2021-07-27T12:45:05.443589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_forecast(model, series, window_size):\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size))\n    ds = ds.batch(32).prefetch(1)\n    forecast = model.predict(ds)\n    return forecast","metadata":{"execution":{"iopub.status.busy":"2021-07-27T12:45:05.613608Z","iopub.execute_input":"2021-07-27T12:45:05.613961Z","iopub.status.idle":"2021-07-27T12:45:05.619072Z","shell.execute_reply.started":"2021-07-27T12:45:05.613927Z","shell.execute_reply":"2021-07-27T12:45:05.618252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n#batch_size = 16\ndataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv1D(filters=32, kernel_size=3, strides=1, padding='causal', activation='relu', input_shape=[None, 1]),\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * 200)\n])\n\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 20))\noptimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n\nmodel.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=['mae'])\nhistory = model.fit(dataset, epochs=100, callbacks=[lr_schedule], verbose=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T12:45:06.068808Z","iopub.execute_input":"2021-07-27T12:45:06.069134Z","iopub.status.idle":"2021-07-27T12:45:39.973704Z","shell.execute_reply.started":"2021-07-27T12:45:06.069102Z","shell.execute_reply":"2021-07-27T12:45:39.972494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.semilogx(history.history['lr'], history.history['loss'])\nplt.axis([1e-8, 1e-4, 0, 30])","metadata":{"execution":{"iopub.status.busy":"2021-07-27T12:45:39.977853Z","iopub.execute_input":"2021-07-27T12:45:39.978123Z","iopub.status.idle":"2021-07-27T12:45:40.615666Z","shell.execute_reply.started":"2021-07-27T12:45:39.978094Z","shell.execute_reply":"2021-07-27T12:45:40.614691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n#batch_size = 16\ndataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=32, kernel_size=3, strides=1, padding='causal', activation='relu', input_shape=[None, 1]),\n  tf.keras.layers.LSTM(32, return_sequences=True),\n  tf.keras.layers.LSTM(32, return_sequences=True),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 200)\n])\n\noptimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=['mae'])\nhistory = model.fit(dataset, epochs=500, verbose=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T12:45:54.244311Z","iopub.execute_input":"2021-07-27T12:45:54.244651Z","iopub.status.idle":"2021-07-27T12:48:00.77199Z","shell.execute_reply.started":"2021-07-27T12:45:54.244621Z","shell.execute_reply":"2021-07-27T12:48:00.770862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\nrnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]","metadata":{"execution":{"iopub.status.busy":"2021-07-27T12:48:00.773881Z","iopub.execute_input":"2021-07-27T12:48:00.77426Z","iopub.status.idle":"2021-07-27T12:48:01.67488Z","shell.execute_reply.started":"2021-07-27T12:48:00.774226Z","shell.execute_reply":"2021-07-27T12:48:01.673874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, rnn_forecast)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T12:48:01.678992Z","iopub.execute_input":"2021-07-27T12:48:01.679288Z","iopub.status.idle":"2021-07-27T12:48:01.850791Z","shell.execute_reply.started":"2021-07-27T12:48:01.67926Z","shell.execute_reply":"2021-07-27T12:48:01.849888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T12:48:01.852303Z","iopub.execute_input":"2021-07-27T12:48:01.852788Z","iopub.status.idle":"2021-07-27T12:48:01.862299Z","shell.execute_reply.started":"2021-07-27T12:48:01.85275Z","shell.execute_reply":"2021-07-27T12:48:01.860883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt\n\n#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nmae = history.history['mae']\nloss = history.history['loss']\n\nepochs = range(len(loss)) # Get number of epochs\n\n#------------------------------------------------\n# Plot MAE and Loss\n#------------------------------------------------\nplt.plot(epochs, mae, 'r')\nplt.plot(epochs, loss, 'b')\nplt.title('MAE and Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['MAE', 'Loss'])\n\nplt.figure()\n\nepochs_zoom = epochs[200:]\nmae_zoom = mae[200:]\nloss_zoom = loss[200:]\n\n#------------------------------------------------\n# Plot Zoomed MAE and Loss\n#------------------------------------------------\nplt.plot(epochs_zoom, mae_zoom, 'r')\nplt.plot(epochs_zoom, loss_zoom, 'b')\nplt.title('MAE and Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['MAE', 'Loss'])\n\nplt.figure()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T12:48:01.864123Z","iopub.execute_input":"2021-07-27T12:48:01.864631Z","iopub.status.idle":"2021-07-27T12:48:02.254406Z","shell.execute_reply.started":"2021-07-27T12:48:01.864586Z","shell.execute_reply":"2021-07-27T12:48:02.253469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Working with Real Data","metadata":{}},{"cell_type":"code","source":"import csv","metadata":{"execution":{"iopub.status.busy":"2021-07-27T12:55:25.611076Z","iopub.execute_input":"2021-07-27T12:55:25.611461Z","iopub.status.idle":"2021-07-27T12:55:25.616993Z","shell.execute_reply.started":"2021-07-27T12:55:25.611429Z","shell.execute_reply":"2021-07-27T12:55:25.615735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_step = []\nsunspots = []\n\nwith open('../input/sunspots/Sunspots.csv') as csv_file:\n    reader = csv.reader(csv_file, delimiter=',')\n    next(reader) # skipping the header\n    for row in reader:\n        sunspots.append(float(row[2]))\n        time_step.append(int(row[0]))\n\nseries = np.array(sunspots)\ntime = np.array(time_step)\n\nplt.figure(figsize=(10, 6))\nplot_series(time, series)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T12:57:22.706031Z","iopub.execute_input":"2021-07-27T12:57:22.706392Z","iopub.status.idle":"2021-07-27T12:57:22.916996Z","shell.execute_reply.started":"2021-07-27T12:57:22.706361Z","shell.execute_reply":"2021-07-27T12:57:22.916196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the dataset\n\nsplit_time = 3000\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]\n\nwindow_size = 30\nbatch_size = 32\nshuffle_buffer_size = 1000","metadata":{"execution":{"iopub.status.busy":"2021-07-27T12:58:27.684564Z","iopub.execute_input":"2021-07-27T12:58:27.6849Z","iopub.status.idle":"2021-07-27T12:58:27.689424Z","shell.execute_reply.started":"2021-07-27T12:58:27.68487Z","shell.execute_reply":"2021-07-27T12:58:27.688541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    series = tf.expand_dims(series, axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer)\n    ds = ds.map(lambda w: (w[:-1], w[1:]))\n    return ds.batch(batch_size).prefetch(1)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T12:58:32.468929Z","iopub.execute_input":"2021-07-27T12:58:32.46928Z","iopub.status.idle":"2021-07-27T12:58:32.476107Z","shell.execute_reply.started":"2021-07-27T12:58:32.469246Z","shell.execute_reply":"2021-07-27T12:58:32.475291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_forecast(model, series, window_size):\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size))\n    ds = ds.batch(32).prefetch(1)\n    forecast = model.predict(ds)\n    return forecast","metadata":{"execution":{"iopub.status.busy":"2021-07-27T12:58:48.978508Z","iopub.execute_input":"2021-07-27T12:58:48.978887Z","iopub.status.idle":"2021-07-27T12:58:48.986362Z","shell.execute_reply.started":"2021-07-27T12:58:48.978855Z","shell.execute_reply":"2021-07-27T12:58:48.985321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\nwindow_size = 64\nbatch_size = 256\ntrain_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\nprint(train_set)\nprint(x_train.shape)\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, padding='causal', activation='relu', input_shape=[None, 1]),\n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.Dense(30, activation='relu'),\n  tf.keras.layers.Dense(10, activation='relu'),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 20))\noptimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n\nmodel.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=['mae'])\nhistory = model.fit(train_set, epochs=100, callbacks=[lr_schedule], verbose=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T12:59:44.826932Z","iopub.execute_input":"2021-07-27T12:59:44.827278Z","iopub.status.idle":"2021-07-27T13:00:43.813932Z","shell.execute_reply.started":"2021-07-27T12:59:44.82724Z","shell.execute_reply":"2021-07-27T13:00:43.81301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.semilogx(history.history['lr'], history.history['loss'])\nplt.axis([1e-8, 1e-4, 0, 60])","metadata":{"execution":{"iopub.status.busy":"2021-07-27T13:00:51.725442Z","iopub.execute_input":"2021-07-27T13:00:51.725775Z","iopub.status.idle":"2021-07-27T13:00:52.555249Z","shell.execute_reply.started":"2021-07-27T13:00:51.725745Z","shell.execute_reply":"2021-07-27T13:00:52.554289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\ntrain_set = windowed_dataset(x_train, window_size=60, batch_size=100, shuffle_buffer=shuffle_buffer_size)\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=60, kernel_size=5, strides=1, padding='causal', activation='relu', input_shape=[None, 1]),\n  tf.keras.layers.LSTM(60, return_sequences=True),\n  tf.keras.layers.LSTM(60, return_sequences=True),\n  tf.keras.layers.Dense(30, activation='relu'),\n  tf.keras.layers.Dense(10, activation='relu'),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\noptimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=['mae'])\nhistory = model.fit(train_set, epochs=500, verbose=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T13:01:40.629018Z","iopub.execute_input":"2021-07-27T13:01:40.629408Z","iopub.status.idle":"2021-07-27T13:07:14.175692Z","shell.execute_reply.started":"2021-07-27T13:01:40.629374Z","shell.execute_reply":"2021-07-27T13:07:14.174716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\nrnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]","metadata":{"execution":{"iopub.status.busy":"2021-07-27T13:07:14.179382Z","iopub.execute_input":"2021-07-27T13:07:14.179658Z","iopub.status.idle":"2021-07-27T13:07:15.462549Z","shell.execute_reply.started":"2021-07-27T13:07:14.179632Z","shell.execute_reply":"2021-07-27T13:07:15.461641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, rnn_forecast)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T13:07:15.466008Z","iopub.execute_input":"2021-07-27T13:07:15.466294Z","iopub.status.idle":"2021-07-27T13:07:15.84684Z","shell.execute_reply.started":"2021-07-27T13:07:15.466268Z","shell.execute_reply":"2021-07-27T13:07:15.845853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T13:07:15.848383Z","iopub.execute_input":"2021-07-27T13:07:15.848736Z","iopub.status.idle":"2021-07-27T13:07:15.856601Z","shell.execute_reply.started":"2021-07-27T13:07:15.8487Z","shell.execute_reply":"2021-07-27T13:07:15.855645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt\n\n#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nloss = history.history['loss']\n\nepochs = range(len(loss)) # Get number of epochs\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot(epochs, loss, 'r')\nplt.title('Training loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(['Loss'])\n\nplt.figure()\n\nzoomed_loss = loss[200:]\nzoomed_epochs = range(200,500)\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot(zoomed_epochs, zoomed_loss, 'r')\nplt.title('Training loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(['Loss'])\n\nplt.figure()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T13:07:15.85806Z","iopub.execute_input":"2021-07-27T13:07:15.858492Z","iopub.status.idle":"2021-07-27T13:07:16.178977Z","shell.execute_reply.started":"2021-07-27T13:07:15.858457Z","shell.execute_reply":"2021-07-27T13:07:16.178069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(rnn_forecast)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T13:07:16.180416Z","iopub.execute_input":"2021-07-27T13:07:16.180781Z","iopub.status.idle":"2021-07-27T13:07:16.189224Z","shell.execute_reply.started":"2021-07-27T13:07:16.180739Z","shell.execute_reply":"2021-07-27T13:07:16.18809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}}]}