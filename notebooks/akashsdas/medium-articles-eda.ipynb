{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Medium Articles EDA\n\nIn this kernel `intense EDA` is performed on [Medium Articles](https://www.kaggle.com/hsankesara/medium-articles) by [Hsankesara](https://www.kaggle.com/hsankesara) where he dataset contains `articles`, their `title`, `number of claps` it has received, their `links` and their `reading time`.\n\n**While doing this we'll go through:**\n- Preprocessing of text data\n- Removing outliers using `IQR` and `z-score` methods\n- Data visualization using `seaborn` and `word cloud`\n- Building `classes` following the `DRY` convention\n\nDuring `EDA` we'll use the preprocessed data to answer different questions.\n\n![](https://media.giphy.com/media/5zf2M4HgjjWszLd4a5/giphy.gif)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport math\nimport string\nfrom random import randint\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import zscore\nfrom nltk.corpus import stopwords\nfrom wordcloud import STOPWORDS, WordCloud\n\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading dataset\ndf = pd.read_csv('/kaggle/input/medium-articles/articles.csv')\ndf.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing data"},{"metadata":{},"cell_type":"markdown","source":"`CustomFormatter` class will have helper functions to format strings, just for `extra touch` 🍷."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Formatter to format anything\nclass CustomFormatter:\n    def __init__(self):\n        pass\n    \n    # convert number to K \n    # eg. 1,000 to 1K\n    # can't think of any better name for this func\n    @staticmethod\n    def format_likes_number_to_str(number):\n        rounded_num = round(number / 1000, 2)\n        frac, whole = math.modf(rounded_num)\n        frac = round(frac, 2) if frac != 0 else 0\n        return f'{int(whole) + frac}K'\n\n    \nprint(CustomFormatter.format_likes_number_to_str(5000))\nprint(CustomFormatter.format_likes_number_to_str(5200))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation\n\nHere we are going to clean the data known as `data cleaning` process and transform it for use know `data wrangling` process.\n\n> Data cleaning focuses on removing inaccurate data from your data set whereas data wrangling focuses on transforming the data's format, typically by converting “raw” data into another format more suitable for use."},{"metadata":{},"cell_type":"markdown","source":"Convert `claps` dtype from str to int."},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_clap_dtype(clap_str):\n    if 'K' not in clap_str:\n        # 32\n        return int(clap_str)\n    if 'K' in clap_str:\n        # 32K & 3.2K\n        return int(float(clap_str.split('K')[0]) * 1000)\n    print(f'🌊 Anomaly: {clap_str}')\n    return clap_str\n\n\ndf.claps = df.claps.apply(convert_clap_dtype)\ndf.claps.values[:10].tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a `domain` column will have all the links for the `articles` which I you want you can scrape data for more data analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_domain(link):\n    return link.split('https://')[1].split('/')[0]\n\n\ndf['domain'] = df.link.apply(extract_domain)\ndf.domain.values[:10].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove puncuation from word\ndef rm_punc_from_word(word):\n    clean_alphabet_list = [\n        alphabet for alphabet in word if alphabet not in string.punctuation]\n    return ''.join(clean_alphabet_list)\n\n\nprint(rm_punc_from_word('#cool!'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove puncuation from text\ndef rm_punc_from_text(text):\n    clean_word_list = [rm_punc_from_word(word) for word in text]\n    return ''.join(clean_word_list)\n\n\nprint(rm_punc_from_text(\"Frankly, my dear, I don't give a damn\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove numbers from text\ndef rm_number_from_text(text):\n    text = re.sub('[0-9]+', '', text)\n    return ' '.join(text.split())  # to rm `extra` white space\n\n\nprint(rm_number_from_text('You are 100times more sexier than me'))\nprint(rm_number_from_text('If you taught yes then you are 10 times more delusional than me'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove stopwords from text\ndef rm_stopwords_from_text(text):\n    _stopwords = stopwords.words('english')\n    text = text.split()\n    word_list = [word for word in text if word not in _stopwords]\n    return ' '.join(word_list)\n\n\nrm_stopwords_from_text(\"Love means never having to say you're sorry\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`clean_text` is the function used to apply all the `filters` for cleaning the `string` data i.e. the text here."},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = rm_punc_from_text(text)\n    text = rm_number_from_text(text)\n    text = rm_stopwords_from_text(text)\n\n    # there are hyphen(–) in many titles, so replacing it with empty str\n    # this hyphen(–) is different from normal hyphen(-)\n    text = re.sub('–', '', text)\n    text = ' '.join(text.split())  # removing `extra` white spaces\n\n    return text\n\n\nclean_text(\"Mrs. Robinson, you're trying to seduce me, aren't you?\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cleaning the texts in `text` and `title` columns in our `df`."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.text = df.text.apply(clean_text)\ndf.title = df.title.apply(clean_text)\n\ndf.title.values[:10].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting articles length\ndef get_article_len(text):\n    return len(text)\n\n\ndf['article_length'] = df.text.apply(get_article_len)\ndf.article_length.values[:10].tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis\n\n> Exploratory data analysis (EDA) is used by data scientists to analyze and investigate data sets and summarize their main characteristics, often employing data visualization methods. It helps determine how best to manipulate data sources to get the answers you need, making it easier for data scientists to discover patterns, spot anomalies, test a hypothesis, or check assumptions.\n\nFor more info on `EDA` read the following posts: [Post_1](https://www.ibm.com/cloud/learn/exploratory-data-analysis) and [Post_2](https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15)\n\nSo let's explore the data.\n\n![](https://media.giphy.com/media/l4KibOaou932EC7Dy/giphy.gif)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of claps in our data\ndef display_histplot_for_claps(df, claps_threshold=2_000):\n    claps_threshold_str = CustomFormatter.format_likes_number_to_str(claps_threshold)\n    \n    f, axs = plt.subplots(1, 2, figsize=(16, 4))\n\n    sns.histplot(x=df.claps, kde=False, ax=axs[0])\n    sns.histplot(x=df[df.claps <= claps_threshold].claps, kde=False, ax=axs[1])\n\n    axs[0].set_xlabel('Distribution of all the claps')\n    axs[1].set_xlabel(f'Distribution of claps (<= {claps_threshold_str})')\n\n    # percentage of claps less than equal to claps_threshold\n    pct_of_clap = round(len(df[df.claps <= claps_threshold]) / len(df), 2) * 100\n\n    print(f' {pct_of_clap}% of articles have less than eqaul to {claps_threshold_str} 👏 claps')\n\ndisplay_histplot_for_claps(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above `distribution plots` shows that there are some outliers in claps column."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=df.claps) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `claps` greater than `15K` are the `outliers` as they are not included in the box of other observation i.e no where near the `quartiles`.\n\nTo know more about `detecting and removing outliers` read the following [post](https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# ### Removing outliers using Z score ###\n\n# getting zscores of all the claps\nclaps_zscores = np.abs(zscore(df.claps))\n\n# keeping the threshold of 3 (above which a clap will be an outlier)\n# instead of 3, -3 can also be kept as threshold & in this case claps below -3 will be an outlier\nclap_outliers_row_idx = np.where(claps_zscores > 3)[0].tolist()\n\n# removing outliers\ndf.drop(clap_outliers_row_idx, axis='rows', inplace=True)\n\nsns.boxplot(x=df.claps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ### Removing outliers using IQR ###\n\nclaps_q1 = df.claps.quantile(0.25)\nclaps_q3 = df.claps.quantile(0.75)\niqr = claps_q3 - claps_q1\nprint(f'IQR for claps: {iqr}')\n\nclap_outliers_row_idx = df.claps[(df.claps < (claps_q1 - 1.5 * iqr)) | (df.claps > (claps_q3 + 1.5 * iqr))].index.tolist()\n\n# removing outliers\ndf.drop(clap_outliers_row_idx, axis='rows', inplace=True)\n\nsns.boxplot(x=df.claps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper functions to remove outliers\n\n\n# Using IQR method\ndef rm_outliers_in_col_using_iqr(df, col, inplace=False):\n    Q1 = col.quantile(0.25)\n    Q3 = col.quantile(0.75)\n    IQR = Q3 - Q1\n    print(f'IQR: {IQR}')\n    outliers_row_idx = col[(col < (Q1 - 1.5 * IQR)) | (col > (Q3 + 1.5 * IQR))].index.tolist()\n    return df.drop(outliers_row_idx, axis='rows', inplace=inplace)\n\n\n# Using the Zscore method\ndef rm_outliers_in_col_using_zscore(df, col, inplace=False, threshold=3):\n    zscores = np.abs(zscore(col))\n    outliers_row_idx = np.where(zscores > threshold)[0].tolist()\n    return df.drop(outliers_row_idx, axis='rows', inplace=inplace)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing remaining outliers \nfor _ in range(10):\n    rm_outliers_in_col_using_iqr(df, df.claps, inplace=True)\n\nsns.boxplot(x=df.claps)\n    \n# removing the outliers for claps column multiple time reason `maybe` that \n# majority of the claps are less 3K and the outliers were spread very far","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# distribution of reading_time in our data\ndef display_histplot_for_reading_time(df):\n    sns.histplot(\n        x=df.reading_time, \n        kde=False, bins=range(df.reading_time.max()), \n        color='#e61e64', alpha=.5\n    )\n    \n    avg_reading_time = round(df.reading_time.mean(), 2)\n    print(f'The average reading ⏰ time of an article is {avg_reading_time}mins')\n\n\ndisplay_histplot_for_reading_time(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=df.reading_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing outliers in reading_time column\nrm_outliers_in_col_using_iqr(df, df.reading_time, inplace=True)\nsns.boxplot(x=df.reading_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_claps_and_reading_time(df):\n    f, axs = plt.subplots(1, 2, figsize=(16, 4))\n\n    sns.scatterplot(\n        x='claps', y='reading_time', hue='article_length', data=df, \n        palette='mako', s=80, ax=axs[0]\n    )\n    sns.histplot(\n        x='claps', y='reading_time', data=df, \n        palette='mako', ax=axs[1]\n    )\n\n\ndisplay_claps_and_reading_time(df)\n    \n# Articles whose reading_time is more than 12.5mins won't get much claps","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['claps', 'reading_time']].corr() # pearson corr == 0.28...","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`claps` & `reading_time` have a `negligible correlation` i.e. they are not correlated."},{"metadata":{},"cell_type":"markdown","source":"Color functions to use different colours for `wordcloud` text."},{"metadata":{"trusted":true},"cell_type":"code","source":"def wc_blue_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n    return \"hsl(214, 67%%, %d%%)\" % randint(60, 100)\n\ndef wc_grey_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n    return \"hsl(0, 0%%, %d%%)\" % randint(60, 100)\n\ndef wc_green_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n    return \"hsl(123, 34%%, %d%%)\" % randint(50, 100)\n\ndef wc_red_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n    return \"hsl(23, 54%%, %d%%)\" % randint(50, 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting wordclouds**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# stopwords for wordcloud\ndef get_wc_stopwords():\n    wc_stopwords = set(STOPWORDS)\n\n    # Adding words to stopwords \n    # these words showed up while plotting wordcloud for text\n    wc_stopwords.add('s')\n    wc_stopwords.add('one')\n    wc_stopwords.add('using')\n    wc_stopwords.add('example')\n    wc_stopwords.add('work')\n    wc_stopwords.add('use')\n    wc_stopwords.add('make')\n    \n    return wc_stopwords\n\n\n# get title mega str (combined str of all titles)\ndef get_title_combined_str(df):\n    title_words = []\n    for title in df.title.values:\n        title_words.extend(title.split())\n    return ' '.join(title_words)\n\n\n# get text mega str (combined str of all text)\ndef get_text_combined_str(df):\n    text_words = []\n    for text in df.text.values:\n        text_words.extend(text.split())\n    return ' '.join(text_words)\n\n\n# plot wordcloud\ndef plot_wordcloud_for_title_and_text(title_wc, text_wc, title_color_func, text_color_func):\n    f, axs = plt.subplots(1, 2, figsize=(20, 10))\n    \n    with sns.axes_style(\"ticks\"):\n        sns.despine(offset=10, trim=True)\n\n        if not title_color_func:\n            # default color\n            axs[0].imshow(title_wc, interpolation=\"bilinear\")\n            axs[0].set_xlabel('Title WordCloud')\n        else:\n            # customized color\n            axs[0].imshow(title_wc.recolor(color_func=title_color_func, random_state=0), interpolation=\"bilinear\")\n            axs[0].set_xlabel('Title WordCloud')\n            \n        if not title_color_func:\n            axs[1].imshow(text_wc, interpolation=\"bilinear\")\n            axs[1].set_xlabel('Text WordCloud')\n        else:\n            axs[1].imshow(text_wc.recolor(color_func=text_color_func, random_state=0), interpolation=\"bilinear\")\n            axs[1].set_xlabel('Text WordCloud')\n\n            \n# display wordcloud\ndef wordcloud_for_title_and_text(df, title_color_func=None, text_color_func=None):\n    # This str will be used to create wordclouds for title & text\n    title_str = get_title_combined_str(df)\n    text_str = get_text_combined_str(df)\n        \n    wc_stopwords = get_wc_stopwords()\n\n    title_wc = WordCloud(stopwords=wc_stopwords, width=800, height=400, random_state=0).generate(title_str)\n    text_wc = WordCloud(stopwords=wc_stopwords, width=800, height=400, random_state=0).generate(text_str)\n    \n    plot_wordcloud_for_title_and_text(title_wc, text_wc, title_color_func, text_color_func)\n        \n        \nwordcloud_for_title_and_text(df, wc_blue_color_func, wc_grey_color_func)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`WordInfo` class will help us to `encapsulate` info about a `words` and will contain helper functions to work with `text` & `title` columns. Basically `WordInfo` class will act as `tokenizer` but is slightly customized as per my needs."},{"metadata":{"trusted":true},"cell_type":"code","source":"class WordInfo:\n    def __init__(self, word, domain, reading_time):\n        self.word = word\n        self.count = 1\n        self.reading_time = reading_time\n        \n        self.domains = set()  # domains in which it appeared\n        self.domains.add(domain)\n\n        \n    def increment(self, domain, reading_time):\n        self.count += 1\n        self.domains.add(domain)\n        self.reading_time += reading_time\n        \n        \n    def info(self):\n        print(f'Word: {self.word}')\n        print(f'Count: {self.count}')\n        print(f'Domains: {list(self.domains)}')\n        print(f'Reading time: {self.reading_time}mins')\n        \n        \n    @staticmethod\n    def exists(word, dictionary):\n        return dictionary[word] if word in dictionary.keys() else False\n    \n    \n    @staticmethod\n    def increment_or_create(dictionary, word, domain, reading_time):\n        if word not in stopwords.words('english'):\n            obj = WordInfo.exists(word, dictionary)\n            if not obj:\n                dictionary[word] = WordInfo(word, domain, reading_time)\n            else:\n                obj.increment(domain, reading_time)\n                \n                \n    @staticmethod\n    def export_count_dict(word_dict):\n        _dict = {}\n        for wordinfo in list(word_dict.values()):\n            _dict[wordinfo.word] = wordinfo.count\n        return _dict\n    \n    \n    @staticmethod\n    def sort_dict_using_values(_dict):\n        # in-place sorting\n        words = np.array(list(_dict.keys()))\n        counts = np.array(list(_dict.values()))\n        \n        sorted_idxs = counts.argsort()\n        sorted_counts = counts[sorted_idxs]\n        new_words_order = words[sorted_idxs]\n\n        # reversing the list (making it from ascending to decending)\n        _counts = list(reversed(sorted_counts))\n        _words = list(reversed(new_words_order))\n\n        return (_counts, _words)\n    \n    \n    @classmethod\n    def word_count_df(cls, _dict):\n        word_count_dict = cls.export_count_dict(_dict)\n        word_count_sorted = cls.sort_dict_using_values(word_count_dict)\n\n        word_count_df = pd.DataFrame({\n            'words': word_count_sorted[1],\n            'counts': word_count_sorted[0]\n        })\n\n        return word_count_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below is an example of how `WordInfo` class will be used to make our `EDA` easy."},{"metadata":{"trusted":true},"cell_type":"code","source":"# key - words: str\n# value - object: WordInfo\nWORD_DICT = {}\n\n\n# To test/see how our WORD_DICT will look \nfor word in ['hello', 'world', 'python', 'python', 'tensorflow']:\n    WordInfo.increment_or_create(WORD_DICT, word, 'deeplearning.io', 24)\n        \nprint(WORD_DICT)\n\nfor obj in WORD_DICT.values():\n    print()\n    obj.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extracting information about `words` in `title` and `text` columns in `df`."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_title_and_text_word_dict(df):\n    title_word_dict = {}\n    text_word_dict = {}\n    \n    for domain, title, text, reading_time in df[['domain', 'title', 'text', 'reading_time']].values:\n        for word_in_title in title.split():\n            WordInfo.increment_or_create(title_word_dict, word_in_title, domain, reading_time)\n        for word_in_text in text.split():\n            WordInfo.increment_or_create(text_word_dict, word_in_text, domain, reading_time)\n            \n    return (title_word_dict, text_word_dict)\n\n\ntitle_word_dict, text_word_dict = get_title_and_text_word_dict(df)\n\ntitle_word_dict['medium'].info()\nprint()\ntext_word_dict['medium'].info()\nprint()\ntitle_word_dict['neural'].info()\nprint()\ntext_word_dict['neural'].info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title_word_count_df = WordInfo.word_count_df(title_word_dict)\ntext_word_count_df = WordInfo.word_count_df(text_word_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_word_count(df, top=5, bottom=5):\n    # df here is word_count_df\n    \n    f, axs = plt.subplots(1, 2, figsize=(16, 4))\n\n    # most used words\n    sns.barplot(\n        x=df.head(top).words, y=df.head(top).counts, \n        color='#473991', alpha=.9, ax=axs[0]\n    )\n\n    # least used words\n    sns.barplot(\n        x=df.tail(bottom).words, y=df.tail(bottom).counts,\n        color='#399188', alpha=.9, ax=axs[1]\n    )\n\n    axs[0].set_xlabel('Words')\n    axs[0].set_ylabel('Counts')  \n    axs[1].set_xlabel('Words')\n    axs[1].set_ylabel('Counts')  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_word_count(title_word_count_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_word_count(text_word_count_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top 100 articles with respect to claps\ntop_atricles_wrt_claps = df.sort_values(by='claps', ascending=False).iloc[:100]\ntop_atricles_wrt_claps.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud_for_title_and_text(top_atricles_wrt_claps, wc_green_color_func, wc_red_color_func)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most clapped titles & articles includes AI topics"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_words_count(text):\n    info = {} # {word: count}\n    for word in text.split():\n        if word in info.keys():\n            info[word] += 1\n        else:\n            info[word] = 1\n    return info","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`AuthorInfo` class will encapsulate different informations about all the `authors`."},{"metadata":{"trusted":true},"cell_type":"code","source":"class AuthorInfo:\n    # this will contains author info \n    authors_df = pd.DataFrame({\n        'name': [],\n        'total_claps': [],\n        'avg_claps': [],\n        'total_reading_time': [],\n        'avg_reading_time': []\n    })\n    \n    # this will contain author name & domains\n    domains_df = pd.DataFrame({\n        'authors': [],\n        'domains': []\n    })\n    \n    # this will contain words used by authors & their count i.e. how much\n    words_df = pd.DataFrame({\n        'authors': [],\n        'words': [],\n        'counts': [],\n        'where': []     # title or text (where is the word used)\n    })\n    \n    \n    def __init__(self, author_name, author_df):\n        # add author info\n        AuthorInfo.authors_df = AuthorInfo.authors_df.append({\n            'name': author_name,\n            'total_claps': author_df.claps.sum(),\n            'avg_claps': author_df.claps.mean(),\n            'total_reading_time': author_df.reading_time.sum(),\n            'avg_reading_time': author_df.reading_time.mean(),\n        }, ignore_index=True)\n        \n        # add author domains\n        for domain in author_df.domain.values:\n            AuthorInfo.domains_df = AuthorInfo.domains_df.append({\n                'authors': author_name,\n                'domains': domain\n            }, ignore_index=True)\n            \n        # add word count\n        for title, text in author_df[['title', 'text']].values:\n            title_info = get_words_count(title)\n            text_info = get_words_count(text)\n            AuthorInfo.add_wordcount_using_dict(title_info, author_name, 'title')\n            AuthorInfo.add_wordcount_using_dict(text_info, author_name, 'text')            \n        \n        \n    @classmethod\n    def add_wordcount_using_dict(cls, _dict, author_name, where):\n        for word, count in _dict.items(): \n            cls.words_df = cls.words_df.append({\n                'authors': author_name,\n                'words': word,\n                'counts': count,\n                'where': where\n            }, ignore_index=True)\n            \n            \n    @classmethod\n    def get_domains_using_author_name(cls, author_name):\n        return AuthorInfo.domains_df[AuthorInfo.domains_df.authors == author_name].domains.unique().tolist()\n    \n    \n    @classmethod\n    def get_wordcount_df(cls, author_name, where, ascending=False):\n        return cls.words_df[\n            # using ['where'] since where is a method of pd.Series\n            (cls.words_df.authors == author_name) & (cls.words_df['where'] == where)\n        ].sort_values(by='counts', ascending=ascending)\n    \n\n    @classmethod\n    def reset_df(cls):\n        cls.authors_df = pd.DataFrame({\n            'name': [],\n            'total_claps': [],\n            'avg_claps': [],\n            'total_reading_time': [],\n            'avg_reading_time': []\n        })\n\n        cls.domains_df = pd.DataFrame({\n            'authors': [],\n            'domains': []\n        })\n\n        cls.words_df = pd.DataFrame({\n            'authors': [],\n            'words': [],\n            'counts': [],\n            'where': []\n        })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for author, author_df in top_atricles_wrt_claps.groupby(by='author'):\n    AuthorInfo(author, author_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AuthorInfo.domains_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AuthorInfo.words_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AuthorInfo.get_wordcount_df('Adam Geitgey', 'title').head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `words` column in `words_df` of `AuthorInfo` has word appeared in a `title (or text)` & the counts column in word_df of AuthorInfo has number of times the word appeared in a title (or text). So because of that their might be duplicate words in the words columns\n\nBut since the counts of some `duplicates` are same so it might hint that there are some duplicate rows in df."},{"metadata":{"trusted":true},"cell_type":"code","source":"# no duplicates\nprint(f'Number of duplicate rows: {len(df[df.duplicated()])}')\n\n# checking duplication in author name, title text\nprint(f\"Number of duplicate rows: {len(df[df[['author', 'title', 'text']].duplicated()])}\")\n\n# checking where these duplicates differentiate from each other\nprint(f\"Number of duplicate rows: {len(df[df[['author', 'title', 'text', 'claps']].duplicated()])}\")\nprint(f\"Number of duplicate rows: {len(df[df[['author', 'title', 'text', 'reading_time']].duplicated()])}\")\nprint(f\"Number of duplicate rows: {len(df[df[['author', 'title', 'text', 'link']].duplicated()])}\")\n\n# so `link` is the column that differentiate duplicates","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# duplicate rows\n\nprint(f\"Number of duplicate titles: {len(df[df[['title']].duplicated()])}\")\nprint(f\"Number of duplicate texts: {len(df[df[['text']].duplicated()])}\")\n\ndef get_duplicate_dfs(df, group_by, how_many=1):\n    dfs = []\n    \n    # considering duplicates on the basis of title & text columns & then grouping them by author\n    author_grp = df[df.duplicated(['title', 'text'])].groupby(by=group_by)\n    \n    for idx, (author, author_df) in enumerate(author_grp):\n        if idx <= how_many:\n            dfs.append(author_df)\n        else:\n            return dfs\n    \n    \n# the `duplicated` method on df by default returns all the duplicates `except the first` \nduplicate_sample_df = get_duplicate_dfs(df, group_by='author', how_many=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_links(df):\n    for link in df.link.values.tolist():\n        print(link)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_links(duplicate_sample_df[0])\nduplicate_sample_df[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_links(duplicate_sample_df[1])\nduplicate_sample_df[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping all the `duplicates` except for the first occurence since `link` column has all unique values even for the duplicates therefore removing the duplicate rows on the basis of `author`, `claps`, `title` & `text`."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop_duplicates(['author', 'claps', 'title', 'text'], ignore_index=True, inplace=True)\nlen(df) # remaining rows","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After this `catastrophic` event we can `re-run all of the analysis` to correct all of the `misinterpretation` happened due to these duplicate rows.\n\nIt's going to be very easy to re-run all the `analysis` as we have followed the `DRY` principle of programming."},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud_for_title_and_text(df, wc_blue_color_func, wc_grey_color_func)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title_word_dict, text_word_dict = get_title_and_text_word_dict(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title_word_count_df = WordInfo.word_count_df(title_word_dict)\ntext_word_count_df = WordInfo.word_count_df(text_word_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_word_count(title_word_count_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_word_count(text_word_count_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['claps', 'reading_time']].corr() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the corr increased from `0.28 to 0.32`, but it is still a `low positive correlation` so claps and reading_time have a `very low positive correlation`."},{"metadata":{"trusted":true},"cell_type":"code","source":"display_histplot_for_reading_time(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Top 100 articles with respect to claps**"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_atricles_wrt_claps = df.sort_values(by='claps', ascending=False).iloc[:100]\ntop_atricles_wrt_claps.sample(5)\nwordcloud_for_title_and_text(top_atricles_wrt_claps, wc_green_color_func, wc_red_color_func)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`Resetting` the author infos with data (with `no duplicates`)."},{"metadata":{"trusted":true},"cell_type":"code","source":"AuthorInfo.reset_df()\n\nfor author, author_df in top_atricles_wrt_claps.groupby(by='author'):\n    AuthorInfo(author, author_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_avg_claps_and_avg_reading_time(df):\n    f, axs = plt.subplots(1, 2, figsize=(16, 4))\n\n    sns.scatterplot(\n        x='avg_claps', y='avg_reading_time', data=df, \n        palette='mako', s=80, ax=axs[0]\n    )\n    sns.histplot(\n        x='avg_claps', y='avg_reading_time', data=df, \n        palette='mako', ax=axs[1]\n    )\n\n\ndisplay_avg_claps_and_avg_reading_time(AuthorInfo.authors_df)\n    \n# Articles whose reading_time is more than 12mins won't get much claps","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_words(author_name, words_df, where, top_words):\n    df = words_df[\n        (AuthorInfo.words_df.authors == author_name) & (AuthorInfo.words_df['where'] == where)\n    ].sort_values(by='counts', ascending=False).iloc[:top_words].values.tolist()\n    \n    data = {}\n    for _, word, count, _ in df:\n        if word in list(data.keys()):\n            data[word] += count\n        else:\n            data[word] = count\n            \n    return data\n\n    \ndef get_top_authors_info(authors_df, sort_by, top=5, top_words=5):\n    top_author_df = authors_df.sort_values(by=sort_by, ascending=False).iloc[:top]\n    df = top_author_df[['name', 'total_claps', 'total_reading_time']]\n    \n    for author_name, total_claps, total_reading_time in df.values:\n        print(f'Author name: {author_name}')\n        print(f'Total claps: {total_claps}')\n        print(f'Total reading time: {total_reading_time}')\n    \n        top_words_in_title = get_top_words(author_name, AuthorInfo.words_df, 'title', top_words)\n        top_words_in_text = get_top_words(author_name, AuthorInfo.words_df, 'text', top_words)\n\n        print(f'Top words used in title:')\n        for word, count  in top_words_in_title.items():\n            print(f'\\t{word} => {int(count)}x')\n        print(f'Top words used in text:')\n        for word, count in top_words_in_text.items():\n            print(f'\\t{word} => {int(count)}x')\n    \n        print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Top 5 authors info with respect to total claps**"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_top_authors_info(AuthorInfo.authors_df, 'total_claps')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Top 5 authors info with respect to total reading_time**"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_top_authors_info(AuthorInfo.authors_df, 'total_reading_time')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"I'll wrap things up there. If you want to find some other answers then go ahead `edit` this kernel. If you have any `questions` then do let me know. \n\nIf this kernel helped you then don't forget to 🔼 `upvote` and share your 🎙 `feedback` on improvements of the kernel.\n\n![](https://media.giphy.com/media/iFU36VwXUd2O43gdcr/giphy.gif)\n\n---"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}