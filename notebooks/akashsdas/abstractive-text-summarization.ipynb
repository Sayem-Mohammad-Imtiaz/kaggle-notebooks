{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Abstractive Text Summarization\n\n[Post](https://towardsdatascience.com/data-scientists-guide-to-summarization-fc0db952e363) on getting started with `text summarization`, their pros and cons and much more.\n\n**There 3 different training models used here**\n- `build_seq2seq_model_with_just_lstm` - **Seq2Seq model with just LSTMs**. Both `encoder` and `decoder` have just `LSTM`s.\n- `build_seq2seq_model_with_bidirectional_lstm` - **Seq2Seq model with Bidirectional LSTMs**. Both `encoder` and `decoder` have `Bidirectional LSTM`s.\n- `build_hybrid_seq2seq_model` - **Seq2Seq model with hybrid architecture**. Here `encoder` has `Bidirectional LSTM`s while `decoder` has just `LSTM`s.\n\n**To see the full learning and results of all the 3 model go to the end of the notebook in the `Running all the 3 different models` section**\n\nThe `model (the trained model)`, `encoder_model (for inference)` and `decoder_model (for inference)` for **Seq2Seq with just LSTMs** are only saved.\n\n![](https://media.giphy.com/media/dsKnRuALlWsZG/giphy.gif)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport re\nimport pickle\nimport string\nimport unicodedata\nfrom random import randint\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom nltk.corpus import stopwords\nfrom wordcloud import STOPWORDS, WordCloud\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dense, Embedding, TimeDistributed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q contractions==0.0.48","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from contractions import contractions_dict\n\nfor key, value in list(contractions_dict.items())[:10]:\n    print(f'{key} == {value}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using TPU\n\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 🪂 Getting the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"filename1 = '../input/news-summary/news_summary.csv'\nfilename2 = '../input/news-summary/news_summary_more.csv'\n\ndf1 = pd.read_csv(filename1, encoding='iso-8859-1').reset_index(drop=True)\ndf2 = pd.read_csv(filename2, encoding='iso-8859-1').reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1_columns = df1.columns.tolist()\ndf1_columns.remove('headlines')\ndf1_columns.remove('text')\ndf1.drop(df1_columns, axis='columns', inplace=True)\n\ndf = pd.concat([df1, df2], axis='rows')\ndel df1, df2\n\n# Shuffling the df\ndf = df.sample(frac=1).reset_index(drop=True)\n\nprint(f'Dataset size: {len(df)}')\ndf.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `headlines` column will be treated as `summary` for the text."},{"metadata":{},"cell_type":"markdown","source":"## 🏂 Data preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def expand_contractions(text, contraction_map=contractions_dict):\n    # Using regex for getting all contracted words\n    contractions_keys = '|'.join(contraction_map.keys())\n    contractions_pattern = re.compile(f'({contractions_keys})', flags=re.DOTALL)\n\n    def expand_match(contraction):\n        # Getting entire matched sub-string\n        match = contraction.group(0)\n        expanded_contraction = contraction_map.get(match)\n        if not expand_contractions:\n            print(match)\n            return match\n        return expanded_contraction\n\n    expanded_text = contractions_pattern.sub(expand_match, text)\n    expanded_text = re.sub(\"'\", \"\", expanded_text)\n    return expanded_text\n\n\nexpand_contractions(\"y'all can't expand contractions i'd think\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting to lowercase\ndf.text = df.text.apply(str.lower)\ndf.headlines = df.headlines.apply(str.lower)\n\ndf.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.headlines = df.headlines.apply(expand_contractions)\ndf.text = df.text.apply(expand_contractions)\ndf.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove puncuation from word\ndef rm_punc_from_word(word):\n    clean_alphabet_list = [\n        alphabet for alphabet in word if alphabet not in string.punctuation\n    ]\n    return ''.join(clean_alphabet_list)\n\nprint(rm_punc_from_word('#cool!'))\n\n\n# Remove puncuation from text\ndef rm_punc_from_text(text):\n    clean_word_list = [rm_punc_from_word(word) for word in text]\n    return ''.join(clean_word_list)\n\nprint(rm_punc_from_text(\"Frankly, my dear, I don't give a damn\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove numbers from text\ndef rm_number_from_text(text):\n    text = re.sub('[0-9]+', '', text)\n    return ' '.join(text.split())  # to rm `extra` white space\n\nprint(rm_number_from_text('You are 100times more sexier than me'))\nprint(rm_number_from_text('If you taught yes then you are 10 times more delusional than me'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove stopwords from text\ndef rm_stopwords_from_text(text):\n    _stopwords = stopwords.words('english')\n    text = text.split()\n    word_list = [word for word in text if word not in _stopwords]\n    return ' '.join(word_list)\n\nrm_stopwords_from_text(\"Love means never having to say you're sorry\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cleaning text\ndef clean_text(text):\n    text = text.lower()\n    text = rm_punc_from_text(text)\n    text = rm_number_from_text(text)\n    text = rm_stopwords_from_text(text)\n\n    # there are hyphen(–) in many titles, so replacing it with empty str\n    # this hyphen(–) is different from normal hyphen(-)\n    text = re.sub('–', '', text)\n    text = ' '.join(text.split())  # removing `extra` white spaces\n\n    # Removing unnecessary characters from text\n    text = re.sub(\"(\\\\t)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\\\r)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\\\n)\", ' ', str(text)).lower()\n\n    # remove accented chars ('Sómě Áccěntěd těxt' => 'Some Accented text')\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode(\n        'utf-8', 'ignore'\n    )\n\n    text = re.sub(\"(__+)\", ' ', str(text)).lower()\n    text = re.sub(\"(--+)\", ' ', str(text)).lower()\n    text = re.sub(\"(~~+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\+\\++)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\.\\.+)\", ' ', str(text)).lower()\n\n    text = re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!]\", ' ', str(text)).lower()\n\n    text = re.sub(\"(mailto:)\", ' ', str(text)).lower()\n    text = re.sub(r\"(\\\\x9\\d)\", ' ', str(text)).lower()\n    text = re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', str(text)).lower()\n    text = re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM',\n                  str(text)).lower()\n\n    text = re.sub(\"(\\.\\s+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\-\\s+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\:\\s+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\s+.\\s+)\", ' ', str(text)).lower()\n\n    try:\n        url = re.search(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)', str(text))\n        repl_url = url.group(3)\n        text = re.sub(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)', repl_url, str(text))\n    except Exception as e:\n        pass\n\n    text = re.sub(\"(\\s+)\", ' ', str(text)).lower()\n    text = re.sub(\"(\\s+.\\s+)\", ' ', str(text)).lower()\n\n    return text\n\nclean_text(\"Mrs. Robinson, you're trying to seduce me, aren't you?\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.text = df.text.apply(clean_text)\ndf.headlines = df.headlines.apply(clean_text)\ndf.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving the cleaned data\ndf.to_csv('cleaned_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To customize colours of wordcloud texts\ndef wc_blue_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n    return \"hsl(214, 67%%, %d%%)\" % randint(60, 100)\n\n\n# stopwords for wordcloud\ndef get_wc_stopwords():\n    wc_stopwords = set(STOPWORDS)\n\n    # Adding words to stopwords\n    # these words showed up while plotting wordcloud for text\n    wc_stopwords.add('s')\n    wc_stopwords.add('one')\n    wc_stopwords.add('using')\n    wc_stopwords.add('example')\n    wc_stopwords.add('work')\n    wc_stopwords.add('use')\n    wc_stopwords.add('make')\n\n    return wc_stopwords\n\n\n# plot wordcloud\ndef plot_wordcloud(text, color_func):\n    wc_stopwords = get_wc_stopwords()\n    wc = WordCloud(stopwords=wc_stopwords, width=1200, height=600, random_state=0).generate(text)\n\n    f, axs = plt.subplots(figsize=(20, 10))\n    with sns.axes_style(\"ticks\"):\n        sns.despine(offset=10, trim=True)\n        plt.imshow(wc.recolor(color_func=color_func, random_state=0), interpolation=\"bilinear\")\n        plt.xlabel('WordCloud')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_wordcloud(' '.join(df.headlines.values.tolist()), wc_blue_color_func)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_wordcloud(' '.join(df.text.values.tolist()), wc_blue_color_func)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using a `start` and `end` tokens in `headlines(summary)` to let the learning algorithm know from where the headlines start's and end's."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.headlines = df.headlines.apply(lambda x: f'_START_ {x} _END_')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again adding `tokens` ... but different ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"start_token = 'sostok'\nend_token = 'eostok'\ndf.headlines = df.headlines.apply(lambda x: f'{start_token} {x} {end_token}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's important to use `sostok` and `eostok` as start and end tokens respectively as later while using `tensorflow's Tokenizer` will filter the tokens and covert them to lowercase.\n\n**sostok** & **eostok** tokens are for us to know where to start & stop the summary because using `_START_` & `_END_`, tf's tokenizer with convert them to **start** & **end** respectively.\n\nSo while decoding the summary sequences of sentences like **'everything is going to end in 2012'** if use `_START_` & `_END_` tokens (which will make the sentence like **'start everything is going to end in 2012 end'** this) whome tf's tokenizer will convert to start and end then we will stop decoding as we hit first **end**, so this is bad and therefore **sostok** & **eostok** these tokens are used.\n\nSo we can just use these **sostok** & **eostok** instead of `_START_` & `_END_`, well you can but I tried both ways and while not using these `_START_` & `_END_` I was getting `undesired results` 🤯 😅 i.e. model's `results weren't good`."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finding what should be the `maximum length` of text and headlines that will be feed or accepted by the learning algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"text_count = [len(sentence.split()) for sentence in df.text]\nheadlines_count = [len(sentence.split()) for sentence in df.headlines]\n\npd.DataFrame({'text': text_count, 'headlines': headlines_count}).hist(bins=100, figsize=(16, 4), range=[0, 50])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To check how many rows in a column has length (of the text) <= limit\ndef get_word_percent(column, limit):\n    count = 0\n    for sentence in column:\n        if len(sentence.split()) <= limit:\n            count += 1\n\n    return round(count / len(column), 2)\n\n\n# Check how many % of headlines have 0-13 words\nprint(get_word_percent(df.headlines, 13))\n\n# Check how many % of summary have 0-42 words\nprint(get_word_percent(df.text, 42))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If the length of headlines or the text is kept large the deep learning model will face issues with performance and also training will slower.\n\nOne solution for creating summary for long sentences can be break a paragraph into sentences and then create a summary for them, this way the summary will make sence instead of giving random piece of text and creating summary for it."},{"metadata":{"trusted":true},"cell_type":"code","source":"max_text_len = 42\nmax_summary_len = 13","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# select the summary and text between their defined max lens respectively\ndef trim_text_and_summary(df, max_text_len, max_summary_len):\n    cleaned_text = np.array(df['text'])\n    cleaned_summary = np.array(df['headlines'])\n\n    short_text = []\n    short_summary = []\n\n    for i in range(len(cleaned_text)):\n        if len(cleaned_text[i].split()) <= max_text_len and len(\n            cleaned_summary[i].split()\n        ) <= max_summary_len:\n            short_text.append(cleaned_text[i])\n            short_summary.append(cleaned_summary[i])\n\n    df = pd.DataFrame({'text': short_text, 'summary': short_summary})\n    return df\n\n\ndf = trim_text_and_summary(df, max_text_len, max_summary_len)\nprint(f'Dataset size: {len(df)}')\ndf.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rare word analysis\ndef get_rare_word_percent(tokenizer, threshold):\n    # threshold: if the word's occurrence is less than this then it's rare word\n\n    count = 0\n    total_count = 0\n    frequency = 0\n    total_frequency = 0\n\n    for key, value in tokenizer.word_counts.items():\n        total_count += 1\n        total_frequency += value\n        if value < threshold:\n            count += 1\n            frequency += value\n\n    return {\n        'percent': round((count / total_count) * 100, 2),\n        'total_coverage': round(frequency / total_frequency * 100, 2),\n        'count': count,\n        'total_count': total_count\n    }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://media.giphy.com/media/3o72EUwmrRFtyT1Vhm/giphy.gif)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the training and validation sets\nx_train, x_val, y_train, y_val = train_test_split(\n    np.array(df['text']),\n    np.array(df['summary']),\n    test_size=0.1,\n    random_state=1,\n    shuffle=True\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tokenizing text 👉 x**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_tokenizer = Tokenizer()\nx_tokenizer.fit_on_texts(list(x_train))\n\nx_tokens_data = get_rare_word_percent(x_tokenizer, 4)\nprint(x_tokens_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"🔥 to `increase computation speed` use this\n\n```python\nx_tokenizer = Tokenizer(num_words=x_tokens_data['total_count'] - x_tokens_data['count'])\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"# else use this\nx_tokenizer = Tokenizer()\nx_tokenizer.fit_on_texts(list(x_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save tokenizer\nwith open('x_tokenizer', 'wb') as f:\n    pickle.dump(x_tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one-hot-encoding\nx_train_sequence = x_tokenizer.texts_to_sequences(x_train)\nx_val_sequence = x_tokenizer.texts_to_sequences(x_val)\n\n# padding upto max_text_len\nx_train_padded = pad_sequences(x_train_sequence, maxlen=max_text_len, padding='post')\nx_val_padded = pad_sequences(x_val_sequence, maxlen=max_text_len, padding='post')\n\n# if you're not using num_words parameter in Tokenizer then use this\nx_vocab_size = len(x_tokenizer.word_index) + 1\n\n# else use this\n# x_vocab_size = x_tokenizer.num_words + 1\n\nprint(x_vocab_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tokenizing headlines(summary) 👉 y**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_tokenizer = Tokenizer()\ny_tokenizer.fit_on_texts(list(y_train))\n\ny_tokens_data = get_rare_word_percent(y_tokenizer, 6)\nprint(y_tokens_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"🔥 to `increase computation speed` use this\n\n```python\ny_tokenizer = Tokenizer(num_words=y_tokens_data['total_count'] - y_tokens_data['count'])\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"# else use this\ny_tokenizer = Tokenizer()\ny_tokenizer.fit_on_texts(list(y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save tokenizer\nwith open('y_tokenizer', 'wb') as f:\n    pickle.dump(y_tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one-hot-encoding\ny_train_sequence = y_tokenizer.texts_to_sequences(y_train)\ny_val_sequence = y_tokenizer.texts_to_sequences(y_val)\n\n# padding upto max_summary_len\ny_train_padded = pad_sequences(y_train_sequence, maxlen=max_summary_len, padding='post')\ny_val_padded = pad_sequences(y_val_sequence, maxlen=max_summary_len, padding='post')\n\n# if you're not using num_words parameter in Tokenizer then use this\ny_vocab_size = len(y_tokenizer.word_index) + 1\n\n# else use this\n# y_vocab_size = y_tokenizer.num_words + 1\n\nprint(y_vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing summary which only has sostok & eostok\ndef remove_indexes(summary_array):\n    remove_indexes = []\n    for i in range(len(summary_array)):\n        count = 0\n        for j in summary_array[i]:\n            if j != 0:\n                count += 1\n        if count == 2:\n            remove_indexes.append(i)\n    return remove_indexes\n\n\nremove_train_indexes = remove_indexes(y_train_padded)\nremove_val_indexes = remove_indexes(y_val_padded)\n\ny_train_padded = np.delete(y_train_padded, remove_train_indexes, axis=0)\nx_train_padded = np.delete(x_train_padded, remove_train_indexes, axis=0)\n\ny_val_padded = np.delete(y_val_padded, remove_val_indexes, axis=0)\nx_val_padded = np.delete(x_val_padded, remove_val_indexes, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 🤼‍♂️ Modelling\n\n![](https://media.giphy.com/media/YTJXDIivNMPuNSMgc0/giphy.gif)"},{"metadata":{"trusted":true},"cell_type":"code","source":"latent_dim = 240\nembedding_dim = 300\nnum_epochs = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_embedding_matrix(tokenizer, embedding_dim, vocab_size=None):\n    word_index = tokenizer.word_index\n    voc = list(word_index.keys())\n\n    path_to_glove_file = '../input/glove6b/glove.6B.300d.txt'\n\n    embeddings_index = {}\n    with open(path_to_glove_file) as f:\n        for line in f:\n            word, coefs = line.split(maxsplit=1)\n            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n            embeddings_index[word] = coefs\n\n    print(\"Found %s word vectors.\" % len(embeddings_index))\n\n    num_tokens = len(voc) + 2 if not vocab_size else vocab_size\n    hits = 0\n    misses = 0\n\n    # Prepare embedding matrix\n    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            # Words not found in embedding index will be all-zeros.\n            # This includes the representation for \"padding\" and \"OOV\"\n            embedding_matrix[i] = embedding_vector\n            hits += 1\n        else:\n            misses += 1\n    print(\"Converted %d words (%d misses)\" % (hits, misses))\n\n    return embedding_matrix\n\n\nx_embedding_matrix = get_embedding_matrix(x_tokenizer, embedding_dim, x_vocab_size)\ny_embedding_matrix = get_embedding_matrix(y_tokenizer, embedding_dim, y_vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_embedding_matrix.shape)\nprint(y_embedding_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using `pre-trained` embeddings and keeping the `Embedding` layer `non-trainable` we get increase in computation speed as don't need to compute the embedding matrix."},{"metadata":{},"cell_type":"markdown","source":"**Here there 3 different training models**\n- `build_seq2seq_model_with_just_lstm` - **Seq2Seq model with just LSTMs**. Both `encoder` and `decoder` have just `LSTM`s.\n- `build_seq2seq_model_with_bidirectional_lstm` - **Seq2Seq model with Bidirectional LSTMs**. Both `encoder` and `decoder` have `Bidirectional LSTM`s.\n- `build_hybrid_seq2seq_model` - **Seq2Seq model with hybrid architecture**. Here `encoder` has `Bidirectional LSTM`s while `decoder` has just `LSTM`s.\n\n**Inference methods for the 3 different learning models - just add `_inference` as `prefix`**\n- `build_seq2seq_model_with_just_lstm_inference`\n- `build_seq2seq_model_with_bidirectional_lstm_inference`\n- `build_hybrid_seq2seq_model_inference`\n\n**Decoding sequence for the 3 different learning models - just add `decode_sequence_` as `suffix`**\n- `decode_sequence_build_seq2seq_model_with_just_lstm`\n- `decode_sequence_build_seq2seq_model_with_bidirectional_lstm`\n- `decode_sequence_build_hybrid_seq2seq_model`"},{"metadata":{},"cell_type":"markdown","source":"**Seq2Seq model with just LSTMs**. Both `encoder` and `decoder` have just `LSTM`s."},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_seq2seq_model_with_just_lstm(\n    embedding_dim, latent_dim, max_text_len, \n    x_vocab_size, y_vocab_size,\n    x_embedding_matrix, y_embedding_matrix\n):\n    # instantiating the model in the strategy scope creates the model on the TPU\n    with tpu_strategy.scope():\n\n        # =====================\n        # 🔥 Encoder\n        # =====================\n        encoder_input = Input(shape=(max_text_len, ))\n\n        # encoder embedding layer\n        encoder_embedding = Embedding(\n            x_vocab_size,\n            embedding_dim,\n            embeddings_initializer=tf.keras.initializers.Constant(x_embedding_matrix),\n            trainable=False\n        )(encoder_input)\n\n        # encoder lstm 1\n        encoder_lstm1 = LSTM(\n            latent_dim,\n            return_sequences=True,\n            return_state=True,\n            dropout=0.4,\n            recurrent_dropout=0.4\n        )\n        encoder_output1, state_h1, state_c1 = encoder_lstm1(encoder_embedding)\n\n        # encoder lstm 2\n        encoder_lstm2 = LSTM(\n            latent_dim,\n            return_sequences=True,\n            return_state=True,\n            dropout=0.4,\n            recurrent_dropout=0.4\n        )\n        encoder_output, *encoder_final_states = encoder_lstm2(encoder_output1)\n\n        # =====================\n        # 🌈 Decoder\n        # =====================\n\n        # Set up the decoder, using `encoder_states` as initial state.\n\n        decoder_input = Input(shape=(None, ))\n\n        # decoder embedding layer\n        decoder_embedding_layer = Embedding(\n            y_vocab_size,\n            embedding_dim,\n            embeddings_initializer=tf.keras.initializers.Constant(y_embedding_matrix),\n            trainable=True\n        )\n        decoder_embedding = decoder_embedding_layer(decoder_input)\n\n        # decoder lstm 1\n        decoder_lstm = LSTM(\n            latent_dim,\n            return_sequences=True,\n            return_state=True,\n            dropout=0.4,\n            recurrent_dropout=0.4\n        )\n        decoder_output, *decoder_final_states = decoder_lstm(\n            decoder_embedding, initial_state=encoder_final_states\n        )\n\n        # dense layer\n        decoder_dense = TimeDistributed(\n            Dense(y_vocab_size, activation='softmax')\n        )\n        decoder_output = decoder_dense(decoder_output)\n\n        # =====================\n        # ⚡️ Model\n        # =====================\n        model = Model([encoder_input, decoder_input], decoder_output)\n        model.summary()\n\n        optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return {\n            'model': model,\n            'inputs': {\n                'encoder': encoder_input,\n                'decoder': decoder_input\n            },\n            'outputs': {\n                'encoder': encoder_output,\n                'decoder': decoder_output\n            },\n            'states': {\n                'encoder': encoder_final_states,\n                'decoder': decoder_final_states\n            },\n            'layers': {\n                'decoder': {\n                    'embedding': decoder_embedding_layer,\n                    'last_decoder_lstm': decoder_lstm,\n                    'dense': decoder_dense\n                }\n            }\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Seq2Seq model with Bidirectional LSTMs**. Both `encoder` and `decoder` have `Bidirectional LSTM`s."},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_seq2seq_model_with_bidirectional_lstm(\n    embedding_dim, latent_dim, max_text_len, \n    x_vocab_size, y_vocab_size,\n    x_embedding_matrix, y_embedding_matrix\n):\n    # instantiating the model in the strategy scope creates the model on the TPU\n    with tpu_strategy.scope():\n\n        # =====================\n        # 🔥 Encoder\n        # =====================\n        encoder_input = Input(shape=(max_text_len, ))\n\n        # encoder embedding layer\n        encoder_embedding = Embedding(\n            x_vocab_size,\n            embedding_dim,\n            embeddings_initializer=tf.keras.initializers.Constant(x_embedding_matrix),\n            trainable=False,\n            name='encoder_embedding'\n        )(encoder_input)\n\n        # encoder lstm1\n        encoder_bi_lstm1 = Bidirectional(\n            LSTM(\n                latent_dim,\n                return_sequences=True,\n                return_state=True,\n                dropout=0.4,\n                recurrent_dropout=0.4,\n                name='encoder_lstm_1'\n            ),\n            name='encoder_bidirectional_lstm_1'\n        )\n        encoder_output1, forward_h1, forward_c1, backward_h1, backward_c1 = encoder_bi_lstm1(\n            encoder_embedding\n        )\n        encoder_bi_lstm1_output = [\n            encoder_output1, forward_h1, forward_c1, backward_h1, backward_c1\n        ]\n\n        # encoder lstm 2\n        encoder_bi_lstm2 = Bidirectional(\n            LSTM(\n                latent_dim,\n                return_sequences=True,\n                return_state=True,\n                dropout=0.4,\n                recurrent_dropout=0.4,\n                name='encoder_lstm_2'\n            ),\n            name='encoder_bidirectional_lstm_2'\n        )\n        encoder_output2, forward_h2, forward_c2, backward_h2, backward_c2 = encoder_bi_lstm2(\n            encoder_output1\n        )\n        encoder_bi_lstm2_output = [\n            encoder_output2, forward_h2, forward_c2, backward_h2, backward_c2\n        ]\n\n        # encoder lstm 3\n        encoder_bi_lstm = Bidirectional(\n            LSTM(\n                latent_dim,\n                return_sequences=True,\n                return_state=True,\n                dropout=0.4,\n                recurrent_dropout=0.4,\n                name='encoder_lstm_3'\n            ),\n            name='encoder_bidirectional_lstm_3'\n        )\n        encoder_output, *encoder_final_states = encoder_bi_lstm(encoder_output2)\n\n        # =====================\n        # 🌈 Decoder\n        # =====================\n\n        # Set up the decoder, using `encoder_states` as initial state.\n\n        decoder_input = Input(shape=(None, ))\n\n        # decoder embedding layer\n        decoder_embedding_layer = Embedding(\n            y_vocab_size,\n            embedding_dim,\n            embeddings_initializer=tf.keras.initializers.Constant(y_embedding_matrix),\n            trainable=False,\n            name='decoder_embedding'\n        )\n        decoder_embedding = decoder_embedding_layer(decoder_input)\n        \n        decoder_bi_lstm = Bidirectional(\n            LSTM(\n                latent_dim,\n                return_sequences=True,\n                return_state=True,\n                dropout=0.4,\n                recurrent_dropout=0.2,\n                name='decoder_lstm_1'\n            ),\n            name='decoder_bidirectional_lstm_1'\n        )\n        decoder_output, *decoder_final_states = decoder_bi_lstm(\n            decoder_embedding, initial_state=encoder_final_states\n            # decoder_embedding, initial_state=encoder_final_states[:2]\n        )  # taking only the forward states\n\n        # dense layer\n        decoder_dense = TimeDistributed(\n            Dense(y_vocab_size, activation='softmax')\n        )\n        decoder_output = decoder_dense(decoder_output)\n\n        # =====================\n        # ⚡️ Model\n        # =====================\n        model = Model([encoder_input, decoder_input], decoder_output, name='seq2seq_model_with_bidirectional_lstm')\n        model.summary()\n\n        optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return {\n            'model': model,\n            'inputs': {\n                'encoder': encoder_input,\n                'decoder': decoder_input\n            },\n            'outputs': {\n                'encoder': encoder_output,\n                'decoder': decoder_output\n            },\n            'states': {\n                'encoder': encoder_final_states,\n                'decoder': decoder_final_states\n            },\n            'layers': {\n                'decoder': {\n                    'embedding': decoder_embedding_layer,\n                    'last_decoder_lstm': decoder_bi_lstm,\n                    'dense': decoder_dense\n                }\n            }\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Seq2Seq model with hybrid architecture**. Here `encoder` has `Bidirectional LSTM`s while `decoder` has just `LSTM`s."},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_hybrid_seq2seq_model(\n    embedding_dim, latent_dim, max_text_len, \n    x_vocab_size, y_vocab_size,\n    x_embedding_matrix, y_embedding_matrix\n):\n    # instantiating the model in the strategy scope creates the model on the TPU\n    with tpu_strategy.scope():\n\n        # =====================\n        # 🔥 Encoder\n        # =====================\n        encoder_input = Input(shape=(max_text_len, ))\n\n        # encoder embedding layer\n        encoder_embedding = Embedding(\n            x_vocab_size,\n            embedding_dim,\n            embeddings_initializer=tf.keras.initializers.Constant(x_embedding_matrix),\n            trainable=False,\n            name='encoder_embedding'\n        )(encoder_input)\n\n        # encoder lstm1\n        encoder_bi_lstm1 = Bidirectional(\n            LSTM(\n                latent_dim,\n                return_sequences=True,\n                return_state=True,\n                dropout=0.4,\n                recurrent_dropout=0.4,\n                name='encoder_lstm_1'\n            ),\n            name='encoder_bidirectional_lstm_1'\n        )\n        encoder_output1, forward_h1, forward_c1, backward_h1, backward_c1 = encoder_bi_lstm1(\n            encoder_embedding\n        )\n        encoder_bi_lstm1_output = [\n            encoder_output1, forward_h1, forward_c1, backward_h1, backward_c1\n        ]\n\n        # encoder lstm 2\n        encoder_bi_lstm2 = Bidirectional(\n            LSTM(\n                latent_dim,\n                return_sequences=True,\n                return_state=True,\n                dropout=0.4,\n                recurrent_dropout=0.4,\n                name='encoder_lstm_2'\n            ),\n            name='encoder_bidirectional_lstm_2'\n        )\n        encoder_output2, forward_h2, forward_c2, backward_h2, backward_c2 = encoder_bi_lstm2(\n            encoder_output1\n        )\n        encoder_bi_lstm2_output = [\n            encoder_output2, forward_h2, forward_c2, backward_h2, backward_c2\n        ]\n\n        # encoder lstm 3\n        encoder_bi_lstm = Bidirectional(\n            LSTM(\n                latent_dim,\n                return_sequences=True,\n                return_state=True,\n                dropout=0.4,\n                recurrent_dropout=0.4,\n                name='encoder_lstm_3'\n            ),\n            name='encoder_bidirectional_lstm_3'\n        )\n        encoder_output, *encoder_final_states = encoder_bi_lstm(encoder_output2)\n\n        # =====================\n        # 🌈 Decoder\n        # =====================\n\n        # Set up the decoder, using `encoder_states` as initial state.\n\n        decoder_input = Input(shape=(None, ))\n\n        # decoder embedding layer\n        decoder_embedding_layer = Embedding(\n            y_vocab_size,\n            embedding_dim,\n            embeddings_initializer=tf.keras.initializers.Constant(y_embedding_matrix),\n            trainable=False,\n            name='decoder_embedding'\n        )\n        decoder_embedding = decoder_embedding_layer(decoder_input)\n        \n        decoder_lstm = LSTM(\n            latent_dim,\n            return_sequences=True,\n            return_state=True,\n            dropout=0.4,\n            recurrent_dropout=0.2,\n            name='decoder_lstm_1'\n        )\n        decoder_output, *decoder_final_states = decoder_lstm(\n            decoder_embedding, initial_state=encoder_final_states[:2]\n        )  # taking only the forward states\n\n        # dense layer\n        decoder_dense = TimeDistributed(\n            Dense(y_vocab_size, activation='softmax')\n        )\n        decoder_output = decoder_dense(decoder_output)\n\n        # =====================\n        # ⚡️ Model\n        # =====================\n        model = Model([encoder_input, decoder_input], decoder_output, name='seq2seq_model_with_bidirectional_lstm')\n        model.summary()\n\n        optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return {\n            'model': model,\n            'inputs': {\n                'encoder': encoder_input,\n                'decoder': decoder_input\n            },\n            'outputs': {\n                'encoder': encoder_output,\n                'decoder': decoder_output\n            },\n            'states': {\n                'encoder': encoder_final_states,\n                'decoder': decoder_final_states\n            },\n            'layers': {\n                'decoder': {\n                    'embedding': decoder_embedding_layer,\n                    'last_decoder_lstm': decoder_lstm,\n                    'dense': decoder_dense\n                }\n            }\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seq2seq = build_seq2seq_model_with_just_lstm(\n    embedding_dim, latent_dim, max_text_len, \n    x_vocab_size, y_vocab_size,\n    x_embedding_matrix, y_embedding_matrix\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"If you want to change `model` then just change the `function name` above."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = seq2seq['model']\n\nencoder_input = seq2seq['inputs']['encoder']\ndecoder_input = seq2seq['inputs']['decoder']\n\nencoder_output = seq2seq['outputs']['encoder']\ndecoder_output = seq2seq['outputs']['decoder']\n\nencoder_final_states = seq2seq['states']['encoder']\ndecoder_final_states = seq2seq['states']['decoder']\n\ndecoder_embedding_layer = seq2seq['layers']['decoder']['embedding']\nlast_decoder_lstm = seq2seq['layers']['decoder']['last_decoder_lstm']\ndecoder_dense = seq2seq['layers']['decoder']['dense']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.layers[-2].input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [\n    EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=0.000001, verbose=1),\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use a `tuple` instead of `list` in `validation_parameter` in `model.fit()`, to know the reason reading this [post](https://stackoverflow.com/questions/61586981/valueerror-layer-sequential-20-expects-1-inputs-but-it-received-2-input-tensor)."},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(\n    [x_train_padded, y_train_padded[:, :-1]],\n    y_train_padded.reshape(y_train_padded.shape[0], y_train_padded.shape[1], 1)[:, 1:],\n    epochs=num_epochs,\n    batch_size=128 * tpu_strategy.num_replicas_in_sync,\n    callbacks=callbacks,\n    validation_data=(\n        [x_val_padded, y_val_padded[:, :-1]],\n        y_val_padded.reshape(y_val_padded.shape[0], y_val_padded.shape[1], 1)[:, 1:]\n    )\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting model's performance**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy\nplt.plot(history.history['accuracy'][1:], label='train acc')\nplt.plot(history.history['val_accuracy'], label='val')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loss\nplt.plot(history.history['loss'][1:], label='train loss')\nplt.plot(history.history['val_loss'], label='val')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 🤸‍♂️ Inference\n\n![](https://media.giphy.com/media/5FJcVeYFVysda/giphy.gif)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next, let’s build the dictionary to convert the index to word for target and source vocabulary:\nreverse_target_word_index = y_tokenizer.index_word\nreverse_source_word_index = x_tokenizer.index_word\ntarget_word_index = y_tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_seq2seq_model_with_just_lstm_inference(\n    max_text_len, latent_dim, encoder_input, encoder_output,\n    encoder_final_states, decoder_input, decoder_output,\n    decoder_embedding_layer, decoder_dense, last_decoder_lstm\n):\n    # Encode the input sequence to get the feature vector\n    encoder_model = Model(\n        inputs=encoder_input, outputs=[encoder_output] + encoder_final_states\n    )\n\n    # Decoder setup\n    # Below tensors will hold the states of the previous time step\n    decoder_state_input_h = Input(shape=(latent_dim, ))\n    decoder_state_input_c = Input(shape=(latent_dim, ))\n    decoder_hidden_state_input = Input(shape=(max_text_len, latent_dim))\n\n    # Get the embeddings of the decoder sequence\n    decoder_embedding = decoder_embedding_layer(decoder_input)\n\n    # To predict the next word in the sequence, set the initial\n    # states to the states from the previous time step\n    decoder_output, *decoder_states = last_decoder_lstm(\n        decoder_embedding,\n        initial_state=[decoder_state_input_h, decoder_state_input_c]\n    )\n\n    # A dense softmax layer to generate prob dist. over the target vocabulary\n    decoder_output = decoder_dense(decoder_output)\n\n    # Final decoder model\n    decoder_model = Model(\n        [decoder_input] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c], \n        [decoder_output] + decoder_states\n    )\n\n    return (encoder_model, decoder_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Useful `stackoverflow` [post](https://stackoverflow.com/questions/60697843/tensorflow-keras-bidirectional-lstm-for-text-summarization) to understand `inference` process when using `bidirectional lstms` in `encoder` and `decoder` in the training model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_seq2seq_model_with_bidirectional_lstm_inference(\n    max_text_len, latent_dim, encoder_input, encoder_output,\n    encoder_final_states, decoder_input, decoder_output,\n    decoder_embedding_layer, decoder_dense, last_decoder_bi_lstm\n):\n\n    # Encode the input sequence to get the feature vector\n    encoder_model = Model(\n        inputs=encoder_input, outputs=[encoder_output] + encoder_final_states\n    )\n\n    # Decoder setup\n    # Below tensors will hold the states of the previous time step\n    decoder_state_forward_input_h = Input(shape=(latent_dim, ))\n    decoder_state_forward_input_c = Input(shape=(latent_dim, ))\n    decoder_state_backward_input_h = Input(shape=(latent_dim, ))\n    decoder_state_backward_input_c = Input(shape=(latent_dim, ))\n\n    # Create the hidden input layer with twice the latent dimension,\n    # since we are using bi - directional LSTM's we will get \n    # two hidden states and two cell states\n    decoder_hidden_state_input = Input(shape=(max_text_len, latent_dim * 2))\n\n    decoder_initial_state = [\n        decoder_state_forward_input_h, decoder_state_forward_input_c,\n        decoder_state_backward_input_h, decoder_state_backward_input_c\n    ]\n\n    # Get the embeddings of the decoder sequence\n    decoder_embedding = decoder_embedding_layer(decoder_input)\n\n    # To predict the next word in the sequence, set the initial\n    # states to the states from the previous time step\n    decoder_output, *decoder_states = last_decoder_bi_lstm(\n        decoder_embedding, initial_state=decoder_initial_state\n    )\n\n    # A dense softmax layer to generate prob dist. over the target vocabulary\n    decoder_output = decoder_dense(decoder_output)\n\n    # Final decoder model\n    decoder_model = Model(\n        [decoder_input] + [decoder_hidden_state_input] + decoder_initial_state,\n        [decoder_output] + decoder_states\n    )\n\n    return (encoder_model, decoder_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_hybrid_seq2seq_model_inference(\n    max_text_len, latent_dim, encoder_input, encoder_output,\n    encoder_final_states, decoder_input, decoder_output,\n    decoder_embedding_layer, decoder_dense, last_decoder_bi_lstm\n):\n\n    # Encode the input sequence to get the feature vector\n    encoder_model = Model(\n        inputs=encoder_input, outputs=[encoder_output] + encoder_final_states\n    )\n\n    # Decoder setup\n    # Below tensors will hold the states of the previous time step\n    decoder_state_forward_input_h = Input(shape=(latent_dim, ))\n    decoder_state_forward_input_c = Input(shape=(latent_dim, ))\n    # decoder_state_backward_input_h = Input(shape=(latent_dim, ))\n    # decoder_state_backward_input_c = Input(shape=(latent_dim, ))\n\n    # Create the hidden input layer with twice the latent dimension,\n    # since we are using bi - directional LSTM's we will get \n    # two hidden states and two cell states\n    decoder_hidden_state_input = Input(shape=(max_text_len, latent_dim * 2))\n\n    decoder_initial_state = [\n        decoder_state_forward_input_h, decoder_state_forward_input_c,\n        #decoder_state_backward_input_h, decoder_state_backward_input_c\n    ]\n\n    # Get the embeddings of the decoder sequence\n    decoder_embedding = decoder_embedding_layer(decoder_input)\n\n    # To predict the next word in the sequence, set the initial\n    # states to the states from the previous time step\n    decoder_output, *decoder_states = last_decoder_bi_lstm(\n        decoder_embedding, initial_state=decoder_initial_state\n    )\n\n    # A dense softmax layer to generate prob dist. over the target vocabulary\n    decoder_output = decoder_dense(decoder_output)\n\n    # Final decoder model\n    decoder_model = Model(\n        [decoder_input] + [decoder_hidden_state_input] + decoder_initial_state,\n        [decoder_output] + decoder_states\n    )\n\n    return (encoder_model, decoder_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_model, decoder_model = build_seq2seq_model_with_just_lstm_inference(\n    max_text_len, latent_dim, encoder_input, encoder_output,\n    encoder_final_states, decoder_input, decoder_output,\n    decoder_embedding_layer, decoder_dense, last_decoder_lstm\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_model.layers[-3].input","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://media.giphy.com/media/XKSa6XxpmHh1NEBvvl/giphy.gif)"},{"metadata":{},"cell_type":"markdown","source":"Converting from `sequence to text` for model `with just LSTM's` and for model `with Bidirectional LSTM's`."},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_sequence_seq2seq_model_with_just_lstm(\n    input_sequence, encoder_model, decoder_model\n):\n    # Encode the input as state vectors.\n    e_out, e_h, e_c = encoder_model.predict(input_sequence)\n\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1))\n\n    # Populate the first word of target sequence with the start word.\n    target_seq[0, 0] = target_word_index[start_token]\n\n    stop_condition = False\n    decoded_sentence = ''\n\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict(\n            [target_seq] + [e_out, e_h, e_c]\n        )\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_token = reverse_target_word_index[sampled_token_index]\n\n        if sampled_token != end_token:\n            decoded_sentence += ' ' + sampled_token\n\n        # Exit condition: either hit max length or find stop word.\n        if (sampled_token == end_token) or (len(decoded_sentence.split()) >= (max_summary_len - 1)):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update internal states\n        e_h, e_c = h, c\n\n    return decoded_sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_sequence_seq2seq_model_with_bidirectional_lstm(\n    input_sequence, encoder_model, decoder_model\n):\n    # Encode the input as state vectors.\n    e_out, *state_values = encoder_model.predict(input_sequence)\n    \n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1))\n\n    # Populate the first word of target sequence with the start word.\n    target_seq[0, 0] = target_word_index[start_token]\n\n    stop_condition = False\n    decoded_sentence = ''\n    \n    while not stop_condition:\n        output_tokens, *decoder_states = decoder_model.predict(\n            [target_seq] + [e_out] + state_values\n        )\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :]) # Greedy Search\n        sampled_token = reverse_target_word_index[sampled_token_index + 1]\n        \n        if sampled_token != end_token:\n            decoded_sentence += ' ' + sampled_token\n\n        # Exit condition: either hit max length or find stop word.\n        if (sampled_token == end_token) or (len(decoded_sentence.split()) >= (max_summary_len - 1)):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update internal states\n        state_values = decoder_states\n\n    return decoded_sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_sequence_hybrid_seq2seq_model(\n    input_sequence, encoder_model, decoder_model\n):\n    # Encode the input as state vectors.\n    e_out, *state_values = encoder_model.predict(input_sequence)\n    \n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1))\n\n    # Populate the first word of target sequence with the start word.\n    target_seq[0, 0] = target_word_index[start_token]\n\n    stop_condition = False\n    decoded_sentence = ''\n    \n    while not stop_condition:\n        output_tokens, *decoder_states = decoder_model.predict(\n            [target_seq] + [e_out] + state_values[:2]\n        )\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :]) # Greedy Search\n        sampled_token = reverse_target_word_index[sampled_token_index + 1]\n        \n        if sampled_token != end_token:\n            decoded_sentence += ' ' + sampled_token\n\n        # Exit condition: either hit max length or find stop word.\n        if (sampled_token == end_token) or (len(decoded_sentence.split()) >= (max_summary_len - 1)):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update internal states\n        state_values = decoder_states\n\n    return decoded_sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seq2summary(input_sequence):\n    new_string = ''\n    for i in input_sequence:\n        if (\n            (i != 0 and i != target_word_index[start_token]) and\n            (i != target_word_index[end_token])\n        ):\n            new_string = new_string + reverse_target_word_index[i] + ' '\n    return new_string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seq2text(input_sequence):\n    new_string = ''\n    for i in input_sequence:\n        if i != 0:\n            new_string = new_string + reverse_source_word_index[i] + ' '\n    return new_string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\nif len(l) % 3 != 0:\n    while len(l) % 3 != 0:\n        l.append(0)\nprint(l)\n\nlst_i = 3\nfor i in range(0, len(l), 3):\n    print(l[i:i + lst_i])\n\nprint(' '.join(['', 'james', 'ethan', '', 'tony']))\nprint(' '.join(' '.join(['', 'james', 'ethan', '', 'tony']).split()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For predicting `unseen` data pass `decode_sequence` function for which you want to decode."},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_text(text, decode_sequence, encoder_model, decoder_model):\n    original_text = text\n    text = clean_text([text])  # generator\n    text_list = original_text.split()\n\n    if len(text_list) <= max_text_len:\n        text = expand_contractions(text)\n        text = clean_text(text)\n        text = f'_START_ {text} _END_'\n        text = f'{start_token} {text} {end_token}'\n\n        seq = x_tokenizer.texts_to_sequences([' '.join(text_list)])\n        padded = pad_sequences(seq, maxlen=max_text_len, padding='post')\n        pred_summary = decode_sequence(\n            padded.reshape(1, max_text_len), encoder_model, decoder_model\n        )\n        return pred_summary\n    else:\n        pred_summary = ''\n\n        # breaking long texts to individual max_text_len texts and predicting on them\n        while len(text_list) % max_text_len == 0:\n            text_list.append('')\n\n        lst_i = max_text_len\n        for i in range(0, len(text_list), max_text_len):\n            _text_list = original_text.split()[i:i + lst_i]\n            _text = ' '.join(_text_list)\n            _text = ' '.join(\n                _text.split()\n            )  # to remove spaces that were added to make len(text_list) % max_text_len == 0\n\n            _text = expand_contractions(_text)\n            _text = clean_text(_text)  # generator\n            _text = f'_START_ {_text} _END_'\n            _text = f'{start_token} {_text} {end_token}'\n            # print(_text, '\\n')\n\n            _seq = x_tokenizer.texts_to_sequences([_text])\n            _padded = pad_sequences(_seq, maxlen=max_text_len, padding='post')\n            _pred = decode_sequence(\n                _padded.reshape(1, max_text_len), encoder_model, decoder_model\n            )\n            pred_summary += ' ' + ' '.join(_pred.split()[1:-2])\n            pred_summary = ' '.join(pred_summary.split())\n\n        return pred_summary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 🔮 Predictions\n\n![](https://media.giphy.com/media/1wqqlaQ7IX3TXibXZE/giphy.gif)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing on training data\nfor i in range(0, 15):\n    print(f\"# {i+1} News: \", seq2text(x_train_padded[i]))\n    print(\"Original summary: \", seq2summary(y_train_padded[i]))\n    print(\n        \"Predicted summary: \",\n        decode_sequence_seq2seq_model_with_just_lstm(\n            x_train_padded[i].reshape(1, max_text_len), encoder_model,\n            decoder_model\n        )\n    )\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing on validation data\nfor i in range(0, 15):\n    print(f\"# {i+1} News: \", seq2text(x_val_padded[i]))\n    print(\"Original summary: \", seq2summary(y_val_padded[i]))\n    print(\n        \"Predicted summary: \",\n        decode_sequence_seq2seq_model_with_just_lstm(\n            x_val_padded[i].reshape(1, max_text_len), encoder_model,\n            decoder_model\n        )\n    )\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 🎁 Saving the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# HDF5 format\nmodel.save('model.h5')    \nencoder_model.save('encoder_model.h5')\ndecoder_model.save('decoder_model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 🏌️‍♂️ Running all the 3 different models\n\nAfter understanding how all the pieces work, running all the `3 models` to understand how it `performs` and its `results`.\n\n**Here there 3 different training models**\n- `build_seq2seq_model_with_just_lstm` - **Seq2Seq model with just LSTMs**. Both `encoder` and `decoder` have just `LSTM`s.\n- `build_seq2seq_model_with_bidirectional_lstm` - **Seq2Seq model with Bidirectional LSTMs**. Both `encoder` and `decoder` have `Bidirectional LSTM`s.\n- `build_hybrid_seq2seq_model` - **Seq2Seq model with hybrid architecture**. Here `encoder` has `Bidirectional LSTM`s while `decoder` has just `LSTM`s.\n\n**Inference methods for the 3 different learning models - just add `_inference` as `prefix`**\n- `build_seq2seq_model_with_just_lstm_inference`\n- `build_seq2seq_model_with_bidirectional_lstm_inference`\n- `build_hybrid_seq2seq_model_inference`\n\n**Decoding sequence for the 3 different learning models - just add `decode_sequence_` as `suffix`**\n- `decode_sequence_build_seq2seq_model_with_just_lstm`\n- `decode_sequence_build_seq2seq_model_with_bidirectional_lstm`\n- `decode_sequence_build_hybrid_seq2seq_model`\n\n![](https://media.giphy.com/media/3ogmaPGsQOruw/giphy.gif)"},{"metadata":{"trusted":true},"cell_type":"code","source":"models_info = {\n    'just_lstm': {\n        'model': build_seq2seq_model_with_just_lstm,\n        'inference': build_seq2seq_model_with_just_lstm_inference,\n        'decode_sequence': decode_sequence_seq2seq_model_with_just_lstm\n    },\n    'bidirectional_lstm': {\n        'model': build_seq2seq_model_with_bidirectional_lstm,\n        'inference': build_seq2seq_model_with_bidirectional_lstm_inference,\n        'decode_sequence': decode_sequence_seq2seq_model_with_bidirectional_lstm\n    },\n    'hybrid_model': {\n        'model': build_hybrid_seq2seq_model,\n        'inference': build_hybrid_seq2seq_model_inference,\n        'decode_sequence': decode_sequence_hybrid_seq2seq_model\n    }\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model with just LSTMs**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_func = models_info['just_lstm']['model']\ninference_func = models_info['just_lstm']['inference']\ndecode_sequence_func = models_info['just_lstm']['decode_sequence']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seq2seq = model_func(\n    embedding_dim, latent_dim, max_text_len, \n    x_vocab_size, y_vocab_size,\n    x_embedding_matrix, y_embedding_matrix\n)\n\nmodel = seq2seq['model']\n\nencoder_input = seq2seq['inputs']['encoder']\ndecoder_input = seq2seq['inputs']['decoder']\n\nencoder_output = seq2seq['outputs']['encoder']\ndecoder_output = seq2seq['outputs']['decoder']\n\nencoder_final_states = seq2seq['states']['encoder']\ndecoder_final_states = seq2seq['states']['decoder']\n\ndecoder_embedding_layer = seq2seq['layers']['decoder']['embedding']\nlast_decoder_lstm = seq2seq['layers']['decoder']['last_decoder_lstm']\ndecoder_dense = seq2seq['layers']['decoder']['dense']\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(\n    [x_train_padded, y_train_padded[:, :-1]],\n    y_train_padded.reshape(y_train_padded.shape[0], y_train_padded.shape[1], 1)[:, 1:],\n    epochs=num_epochs,\n    batch_size=128 * tpu_strategy.num_replicas_in_sync,\n    callbacks=callbacks,\n    validation_data=(\n        [x_val_padded, y_val_padded[:, :-1]],\n        y_val_padded.reshape(y_val_padded.shape[0], y_val_padded.shape[1], 1)[:, 1:]\n    )\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy\nplt.plot(history.history['accuracy'][1:], label='train acc')\nplt.plot(history.history['val_accuracy'], label='val')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loss\nplt.plot(history.history['loss'][1:], label='train loss')\nplt.plot(history.history['val_loss'], label='val')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inference\nencoder_model, decoder_model = inference_func(\n    max_text_len, latent_dim, encoder_input, encoder_output,\n    encoder_final_states, decoder_input, decoder_output,\n    decoder_embedding_layer, decoder_dense, last_decoder_lstm\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing on training data\nfor i in range(0, 10):\n    print(f\"# {i+1} News: \", seq2text(x_train_padded[i]))\n    print(\"Original summary: \", seq2summary(y_train_padded[i]))\n    print(\n        \"Predicted summary: \",\n        decode_sequence_func(\n            x_train_padded[i].reshape(1, max_text_len), encoder_model,\n            decoder_model\n        )\n    )\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model with Bidirectional LSTMs**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_func = models_info['bidirectional_lstm']['model']\ninference_func = models_info['bidirectional_lstm']['inference']\ndecode_sequence_func = models_info['bidirectional_lstm']['decode_sequence']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seq2seq = model_func(\n    embedding_dim, latent_dim, max_text_len, \n    x_vocab_size, y_vocab_size,\n    x_embedding_matrix, y_embedding_matrix\n)\n\nmodel = seq2seq['model']\n\nencoder_input = seq2seq['inputs']['encoder']\ndecoder_input = seq2seq['inputs']['decoder']\n\nencoder_output = seq2seq['outputs']['encoder']\ndecoder_output = seq2seq['outputs']['decoder']\n\nencoder_final_states = seq2seq['states']['encoder']\ndecoder_final_states = seq2seq['states']['decoder']\n\ndecoder_embedding_layer = seq2seq['layers']['decoder']['embedding']\nlast_decoder_lstm = seq2seq['layers']['decoder']['last_decoder_lstm']\ndecoder_dense = seq2seq['layers']['decoder']['dense']\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(\n    [x_train_padded, y_train_padded[:, :-1]],\n    y_train_padded.reshape(y_train_padded.shape[0], y_train_padded.shape[1], 1)[:, 1:],\n    epochs=num_epochs,\n    batch_size=128 * tpu_strategy.num_replicas_in_sync,\n    callbacks=callbacks,\n    validation_data=(\n        [x_val_padded, y_val_padded[:, :-1]],\n        y_val_padded.reshape(y_val_padded.shape[0], y_val_padded.shape[1], 1)[:, 1:]\n    )\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy\nplt.plot(history.history['accuracy'][1:], label='train acc')\nplt.plot(history.history['val_accuracy'], label='val')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loss\nplt.plot(history.history['loss'][1:], label='train loss')\nplt.plot(history.history['val_loss'], label='val')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inference\nencoder_model, decoder_model = inference_func(\n    max_text_len, latent_dim, encoder_input, encoder_output,\n    encoder_final_states, decoder_input, decoder_output,\n    decoder_embedding_layer, decoder_dense, last_decoder_lstm\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing on training data\nfor i in range(0, 10):\n    print(f\"# {i+1} News: \", seq2text(x_train_padded[i]))\n    print(\"Original summary: \", seq2summary(y_train_padded[i]))\n    print(\n        \"Predicted summary: \",\n        decode_sequence_func(\n            x_train_padded[i].reshape(1, max_text_len), encoder_model,\n            decoder_model\n        )\n    )\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model with hybrid architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_func = models_info['hybrid_model']['model']\ninference_func = models_info['hybrid_model']['inference']\ndecode_sequence_func = models_info['hybrid_model']['decode_sequence']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seq2seq = model_func(\n    embedding_dim, latent_dim, max_text_len, \n    x_vocab_size, y_vocab_size,\n    x_embedding_matrix, y_embedding_matrix\n)\n\nmodel = seq2seq['model']\n\nencoder_input = seq2seq['inputs']['encoder']\ndecoder_input = seq2seq['inputs']['decoder']\n\nencoder_output = seq2seq['outputs']['encoder']\ndecoder_output = seq2seq['outputs']['decoder']\n\nencoder_final_states = seq2seq['states']['encoder']\ndecoder_final_states = seq2seq['states']['decoder']\n\ndecoder_embedding_layer = seq2seq['layers']['decoder']['embedding']\nlast_decoder_lstm = seq2seq['layers']['decoder']['last_decoder_lstm']\ndecoder_dense = seq2seq['layers']['decoder']['dense']\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(\n    [x_train_padded, y_train_padded[:, :-1]],\n    y_train_padded.reshape(y_train_padded.shape[0], y_train_padded.shape[1], 1)[:, 1:],\n    epochs=num_epochs,\n    batch_size=128 * tpu_strategy.num_replicas_in_sync,\n    callbacks=callbacks,\n    validation_data=(\n        [x_val_padded, y_val_padded[:, :-1]],\n        y_val_padded.reshape(y_val_padded.shape[0], y_val_padded.shape[1], 1)[:, 1:]\n    )\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy\nplt.plot(history.history['accuracy'][1:], label='train acc')\nplt.plot(history.history['val_accuracy'], label='val')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loss\nplt.plot(history.history['loss'][1:], label='train loss')\nplt.plot(history.history['val_loss'], label='val')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inference\nencoder_model, decoder_model = inference_func(\n    max_text_len, latent_dim, encoder_input, encoder_output,\n    encoder_final_states, decoder_input, decoder_output,\n    decoder_embedding_layer, decoder_dense, last_decoder_lstm\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing on training data\nfor i in range(0, 10):\n    print(f\"# {i+1} News: \", seq2text(x_train_padded[i]))\n    print(\"Original summary: \", seq2summary(y_train_padded[i]))\n    print(\n        \"Predicted summary: \",\n        decode_sequence_func(\n            x_train_padded[i].reshape(1, max_text_len), encoder_model,\n            decoder_model\n        )\n    )\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\nI'll wrap things up there. If you want to find some other answers then go ahead `edit` this kernel. If you have any `questions` then do let me know.\n\nIf this kernel helped you then don't forget to 🔼 `upvote` and share your 🎙 `feedback` on improvements of the kernel.\n\n![](https://media.giphy.com/media/3o85xAYQLOhSrmINHO/giphy.gif)\n\n---"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}