{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Open Coding Hour Machine Learning Workshop\nBy Matthew Smith and Alexandra Lukasiewicz\n\nThanks for joining us on Kaggle! This website hosts a wide variety of datasets and examples of machine learning in different programming languages. We've found it very useful in creating this tutorial.\n\n### During this workshop we will generate a binary classifier using the Scikit-Learn python package\n\n### We will be working with two datasets from the [UCI machine learning repository](https://archive.ics.uci.edu/ml/index.php)\n* [Mushrooms](https://www.kaggle.com/uciml/mushroom-classification) (a dataset containing key mushroom identification features and their edibility) \n* [Wine](https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009) (A dataset containing features of different red wines such as acidity and sugar content and their quality score)\n\n![](https://media.winefolly.com/Wine-pairing-portobello-mushroom-pinot-winefolly-1.jpg)\nImage from \"Wine and Grill Food Pairings Made For The Porch\" by Phil Keelig, Wine Folly\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Introduction to machine learning \n### What are examples of questions you can ask using ML tools in scikit learn?\nML can be useful in answering biological questions where the exact steps or conditions that generate an outcome are unknown. \nAn example can include: \n* Thermodynamic models with series of complex steps\n* Metabolic engineering and rule based systems\n* Identifying patterns in systems (unsupervised clustering)\n* Reducing heterogenous datasets (dimensionality reduction)\n\n### What are challenges in machine learning?\n**Poor dataset**\n* Training dataset has too few entries \n* Dataset is not representative of new cases it will encounter \n* Irrelevant features\n\n**Poor algorithm**\n* Overfitting to training dataset\n* Underfitting the training dataset (not enough factors included to be accurate when presented with new information)\n\n### Applying machine learning in your own research\n**Generating hypotheses**\n* Unsupervised clustering to observe patterns in data \n* Defining a clear goal or question to answer (am I categorizing data? Am I predicting a value (such as binding strength or enzyme production?)\n\n**Evaluating dataset (do I need more information?)** \n* Selecting a set of algorithms for your question\n* Evaluating performance "},{"metadata":{},"cell_type":"markdown","source":"### To begin, we will import packages and datasets for the rest of the workshop "},{"metadata":{"trusted":true},"cell_type":"code","source":"#packages for dataframe manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n#scikit-learn specific packages:\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.tree import DecisionTreeClassifier as DT\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import mushroom and wine datasets \nmushrooms = pd.read_csv(\"../input/mushroom-classification/mushrooms.csv\")\n\nwine = pd.read_csv(\"../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Quick modification of the Wine dataset\nWine contains the column 'quality' which contains numeric scores. For the purposes of using this dataset for classification we will convert this column to 'poor' or 'excellent' given an arbitrary cutoff score of 7. \n\n'quality' will be our column of categorical variables, and 'quality_score' will hold our numeric values. If you would like to use this dataset to generate a numeric predictor of quality, you can use several algorithmic approaches including linear regression, L1 or L2 regularization, SVMs, or even Decision trees. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert columns in wine to categorical values \nwine = wine.rename(columns = {\"quality\":'quality_score'})\ncutoff_key = {range(0,7):'poor', range(7,10):'excellent'}\nwine['quality'] = wine['quality_score'].apply(lambda x: next((v for k, v in cutoff_key.items() if x in k), 0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets take a look at our datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"mushrooms.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Working with non-numeric data\nEach of the columns contain different features of a mushroom that will help us classify whether our sample is edible or poisonous \nHowever, these are all in a non-numeric format and not all ML algorithms support categorical variables. \nWe can use the scikit-learn tool Label Encoder to convert our dataset into a numeric format."},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\nfor col in mushrooms.columns:\n    mushrooms[col] = le.fit_transform(mushrooms[col])\n\nmushrooms.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now see that the \"class\" column of (e)dible or (p)oisonous has been converted into a binary of 0 or 1. In addition all of the other letter categories have been converted into a number "},{"metadata":{},"cell_type":"markdown","source":"### Why do we use dummy variables?\n\nAfter converting our variables into numbers, we still have a problem. Most of our columns have more than 2 possibilities. For example, look at “cap-surface” in the mushroom data-set - it has three possibilities. If we look at these as numerical values, then we are artificially grouping things together that may have no relation. Most machine learning algorithms will have an easy time separating cap-surface 1 and 2 from cap-surface 3, or separating cap-surface 2 and 3 from caps-surface 1, but these algorithms will have a more difficult time (or even find it impossible), to separate cap-surface 1 and 3 from cap-surface 2. As the number of these classifications goes up for a column, this problem gets worse.\n\nTo fix this, we can break our existing columns each into multiple dummy columns. Each column is broken into the number of columns equal to its number of classifications. Then for the cap-surface N column, the value is 1 if the cap-surface is type N, and it’s 0 otherwise. This lets your machine learning algorithm handle arbitrary relationships between the classifications in a particular column of the original data-set.\nuld you add info here about the dummy variable section?)"},{"metadata":{"trusted":true},"cell_type":"code","source":"mushrooms = pd.get_dummies(mushrooms,columns=mushrooms.columns,drop_first=True)\nmushrooms.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the wine dataset gives numerical data, rather than categorical data, you don't need to do anything here."},{"metadata":{},"cell_type":"markdown","source":"## Splitting dataset into training and testing sets \nSplitting your dataset into training and testing sets is key to evaluating the performance of your algorithm later on (and in cross validating multiple algorithms against one another) \n* Training dataset- used to train the algorithm \n* Testing dataset- used to evaluate accuracy of algorithm\n\n### Is my dataset large enough to split and train? \nThe key question here is whether your dataset has enough entries that represent all possibilities that the model may encounter when applied to some unknown set of features. \nIf our training dataset represents overwhelmingly edible mushrooms and we encounter a poisonous one can we trust that the algorithm will accurately categorize this outcome?\n\n### What is a general percentage to aim for? \nHow much of my dataset should be split into testing and training?\n\nWhat do I do when my dataset is too small?\nK-fold cross validation may be a way to avoid being too optimistic in your fit. We won't cover this in today's code-along, but if you need to know more about this in the future, resources on this are available here (https://machinelearningmastery.com/k-fold-cross-validation/)\n\n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#first we split our dataset into the x input (mushroom features) and y response (edibility) variables \n\nmushrooms_x = mushrooms.drop('class_1', axis = 1)\nmushrooms_y = mushrooms['class_1']\n\n#Here we take 20% of the dataset to test with, and train with the leftover 80%\nmushrooms_x_train, mushrooms_x_test, mushrooms_y_train, mushrooms_y_test = train_test_split(mushrooms_x, mushrooms_y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take some time here to view and split the wine dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Your code here\n#first we split our dataset into the x input (wine features) and y response (quality) variables\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Extraction using PCA \n\n*What elements of our dataset account for the most variation?*\n\nOne way we can answer these questions is by performing a **Principal Component Analysis (PCA)**\n\nPCA is an unsupervised clustering method that can show you what features account for the greatest variability in your dataset, allowing you to condense your dataset into a set of 2 or 3 features (genes, treatments, traits) to feed into your algorithm. (Granted that your dataset can condense to these plot-able dimensions). PCA is a feature extraction technique that can aid in identifying those that best predict your dependent/response variable.\n\nOne thing to be aware of, is that PCA will completely ignore the classifications of your data. It is unsupervised, it simply tries to capture the greatest variability in your dataset that it can with a particular number of dimensions. It will do this even if the feature with the greatest variability tells you nothing about the classifications you want to predict. Usually this is fine, as features with greater variability tend to be good for classification, but it’s not always the case.\n\nPCA, in comparison to methods that will pick out a subset of your features for your classification, brings you an unusual trade-off. To understand this trade-off, take a look at this PCA diagram.\n\n\n![](http://upload.wikimedia.org/wikipedia/commons/f/f5/GaussianScatterPCA.svg)\n\nThe X and Y axes are our original features. The bold black arrows are the dimensions PCA might select on this data. Notice that they are running on diagonals; they are offset from the original dimensions. This is normal for PCA - PCA will tend to mish-mash the features of your original dataset together.\n\nA benefit of this, is that typically all of your features are accounted for to some degree. However, this comes at a cost; when you use PCA, it can make your results difficult to interpret - you won’t generally know which features were most important for your classification, if you run a classifier on the PCA-ed data.\n\nAlso important when using PCA, is to remember that it will find the greatest variance. This means units are important, and can affect the result. If you pick a smaller unit for a dimension; maybe you use grams instead of kilograms, then all of your values in that dimension become much farther apart. PCA will then likely try harder to capture that dimension, even if that’s not what it should be doing - so you should be careful that all of your features are in roughly the same range of numerical values.\n\n\n\nHere we will break down our mushroom dataset into two principal components \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# to start we need to scale our numeric dataset \n# so as not to overinflate the influence of a single feature in a different unit \nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\nmushrooms_x_train_scl = sc.fit_transform(mushrooms_x_train)\nmushrooms_x_test_scl = sc.transform(mushrooms_x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now do this for the wines too\n# YOUR CODE HERE\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(mushrooms_x_train_scl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nmushroom_pca = PCA(n_components=2) #create PCA class object\n\nmushroom_pca.fit_transform(mushrooms_x_train_scl) \nmushrooms_x_train_pca = mushroom_pca.transform(mushrooms_x_train_scl) #perform PCA on training dataset \nmushrooms_x_test_pca = mushroom_pca.transform(mushrooms_x_test_scl) #apply PCA transformation to scaled test dataset ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"original shape:   \", mushrooms_x_train.shape)\nprint(\"transformed shape:\", mushrooms_x_train_pca.shape) #what are the effects of dimensionality redution for this dataset?\n\n# Don't worry too much about the following code - we mostly included it in order to give you a useful visualization for understanding the dataset.\nscatter = plt.scatter(mushrooms_x_train_pca[:, 0], mushrooms_x_train_pca[:, 1], alpha=0.8, c = mushrooms_y_train)\nplt.axis('equal');\nhandles, labels = scatter.legend_elements(prop=\"colors\", alpha=0.6)\nlegend = plt.legend(handles, labels, loc=\"upper right\", title=\"edibility\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#what if we dont know the number of components that make up variability in our dataset?\nmushrooms_pca_n = PCA()\nmushrooms_pca_n = mushrooms_pca_n.fit(mushrooms_x_train_scl)\nmushrooms_variance = mushrooms_pca_n.explained_variance_ratio_[0:7]\nmushrooms_df = pd.DataFrame({'var':mushrooms_variance,\n             'PC':['PC1','PC2','PC3','PC4','PC5', 'PC6','PC7']})\nscree = plt.bar(mushrooms_df[\"PC\"],mushrooms_df['var'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Perform PCA on your dataset\nTake some time now to perform PCA on your wine training dataset.\n\nHow many components account for the highest variability in your data? Prepare a scree plot that shows this. When you've decided how many dimensions to take, use PCA to extract those dimensions in your wine dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# YOUR CODE HERE\n# Here we set up a scree plot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once you've chosen how many dimensions you want to use, run PCA on your dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# YOUR CODE HERE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Precision Recall Curves\n\nFor a precision-recall curve (sometimes called just a “PR-curve”), we plot recall on the horizontal axis, and precision on the vertical axis. \nTo build this plot, we are going to move around a threshold. This threshold tells us when we are certain enough of our result to classify something; if we aren’t certain enough, we just give no classification for the datapoint. Recall indicates what percentage of the total data-set was classified correctly. Precision indicates, of the points we chose to classify, what percentage of them were correct. Typically, there will be some points that you are certain of your classification; then, for low recall, your precision is very high. This is the top left of the chart. As you classify more things, the precision goes down; so the line will move from the top left towards the bottom right as you choose to classify more things.\n\nHere's a diagram of what precision and recall are, shamelessly copied from Wikipieda\n\n![](https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg)\n\nWe will have plenty of examples of these Precision Recall curves below.\n"},{"metadata":{},"cell_type":"markdown","source":"# Running different classification algorithms \nThere is no single classifier that will always perform best on your dataset. Because of this we run multiple algorithms on our training dataset and evaluate their predictive scores against one another. \nIn this tutorial we will use:\n1. Logistic Regression\n1. Support Vector Machine \n1. K- Nearest Neighbor Model\n1. Decision Tree\n1. Random Forest "},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression\n\nThis algorithm fits a “logistic function” to a dataset of true and false values. Here’s an example of a logistic function, shamelessly copied from Wikipedia. The black dots are the training points - they're either 0, for fail, or 1, for pass. Despite this, we can fit a curve that outputs a predicted probability. Notice how it doesn’t go higher than 1 or lower than 0 - the fit done with logistic regression will always give a fit like this.\n\n![](https://upload.wikimedia.org/wikipedia/commons/6/6d/Exam_pass_logistic_curve.jpeg)\n\n* When we fit this function, it doesn’t actually give us a classification - it suggests a probability that the correct class is one class or the other. We have to choose a cut-off.\n\n* One thing to note about Logistic regression, is that, when we do it in higher dimensional spaces, these cut-offs we choose will always be a straight line. We'll generate an example of this below.\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# to begin here, scikit learn has several functions to help with evaluating the accuracy of our model\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will also be using the following function to plot our data. It's for demonstration purposes only - I wouldn't worry about how it works.\ndef plot_predictions(model_name):\n    from matplotlib.colors import ListedColormap\n    X_set, y_set = mushrooms_x_train_pca, mushrooms_y_train\n    \n    # plot decision boundary\n    X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n    plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.6, cmap = ListedColormap(('firebrick', 'royalblue')))\n    plt.xlim(X1.min(), X1.max())\n    plt.ylim(X2.min(), X2.max())\n    for i, j in enumerate(np.unique(y_set)):\n        plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], alpha = 0.4,\n                    c = ListedColormap(('firebrick', 'royalblue'))(i), label = j)\n    plt.title(\"%s Training Set\" %(model_name))\n    plt.xlabel('PC 1')\n    plt.ylabel('PC 2')\n    plt.legend()    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\n\nclassifier.fit(mushrooms_x_train_pca,mushrooms_y_train)\nprint('Training accuracy Score: {0:.4f}\\n'.format(accuracy_score(mushrooms_y_train,classifier.predict(mushrooms_x_train_pca))))\nprint('Testing accuracy Score: {0:.4f}\\n'.format(accuracy_score(mushrooms_y_test,classifier.predict(mushrooms_x_test_pca))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_precision_recall_curve(classifier, mushrooms_x_test_pca, mushrooms_y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#How well did logistic regression predict edibility for our musrhooms?\nplot_predictions('Logistic Regression')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now try and fit a logistic regression classifier to the wine dataset, and analyze the results"},{"metadata":{"trusted":true},"cell_type":"code","source":"# YOUR CODE HERE\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Machines (SVM)\n\n* A support vector machine skips the parts with the logistic regression fit, and simply tries to stick a decision boundary into your dataset and the best possible place. It will often give very similar results to logistic regression.\n* However, most SVM implementations provide you with something called the “kernel trick.” You let it add more dimensions to your dataset, and those dimensions can be used to find better decision boundaries. \n\nHere’s what that can look like:\n\n![](https://miro.medium.com/max/2400/1*gXvhD4IomaC9Jb37tzDUVg.png)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n# kernel 'rbf' is the kernel function the classifier is using - feel free to try others, but this one works just fine.\nclassifier = SVC(kernel='rbf',random_state=42)\n\nclassifier.fit(mushrooms_x_train_pca,mushrooms_y_train)\nprint('Training accuracy Score: {0:.4f}\\n'.format(accuracy_score(mushrooms_y_train,classifier.predict(mushrooms_x_train_pca))))\nprint('Testing accuracy Score: {0:.4f}\\n'.format(accuracy_score(mushrooms_y_test,classifier.predict(mushrooms_x_test_pca))))\nplot_precision_recall_curve(classifier, mushrooms_x_test_pca, mushrooms_y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_predictions(\"SVM\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now try and run SVM on the wine dataset, and analyze the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"# YOUR CODE HERE\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K Nearest Neighbors\n\nThe idea behind K Nearest Neighbors is to identify K groupings of datapoints given their \"distance\" from one another. For each test point, look at the k nearest training points - these are its “neighbors.” We will then have those neighbors “vote” on how to classify the test point.\n\nThis method is really good at picking up bizarre decision boundaries that are difficult to capture with other methods.\n\nThis method can do poorly in areas where there are points coming from both classifications. For example, if a region of your feature space has a 60% chance of being a positive case, you probably want to mark this as positive, but 40% of your training points in that region will be negative, and it is very possible to end up near a cluster of negative neighbors and misclassify your test point.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier as KNN\nclassifier = KNN()  # by default 5 neighbors are used - feel free to mess around with this and see what happens.\n\nclassifier.fit(mushrooms_x_train_pca,mushrooms_y_train)\nprint('Training accuracy Score: {0:.4f}\\n'.format(accuracy_score(mushrooms_y_train,classifier.predict(mushrooms_x_train_pca))))\nprint('Testing accuracy Score: {0:.4f}\\n'.format(accuracy_score(mushrooms_y_test,classifier.predict(mushrooms_x_test_pca))))\nplot_precision_recall_curve(classifier, mushrooms_x_test_pca, mushrooms_y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_predictions('KNN')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now try and run K Nearest Neighbors on the wine dataset, and analyze the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"# YOUR CODE HERE\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Trees\n\nA decision tree makes a tree of “decisions” that give greater and greater quality predictions.\n\nIt deals with nonlinear situations much better than the logistic regression fit, but it can also give some really odd results. Take a look at the 2 dimensional decision diagram below to see how this can go.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier as DT\n\nclassifier = DT(criterion='entropy',random_state=42) # default is gini, that's probably fine too - feel free to try it.\n\nclassifier.fit(mushrooms_x_train_pca,mushrooms_y_train)\nprint('Training accuracy Score: {0:.4f}\\n'.format(accuracy_score(mushrooms_y_train,classifier.predict(mushrooms_x_train_pca))))\nprint('Testing accuracy Score: {0:.4f}\\n'.format(accuracy_score(mushrooms_y_test,classifier.predict(mushrooms_x_test_pca))))\nplot_precision_recall_curve(classifier, mushrooms_x_test_pca, mushrooms_y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_predictions('Decision Tree')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it's your turn - try using a decision tree on the wines dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# YOUR CODE HERE\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest\n\nFor a random forest, we train a *bunch* of decision trees on different subsets of the data. Then we average their results. \n\nThis gives us a much stronger classifier than any single decision tree can produce, and mitigates many of the negative effects of decision trees.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 50, criterion = 'entropy', random_state = 42)\n\nclassifier.fit(mushrooms_x_train_pca,mushrooms_y_train)\nprint('Training accuracy Score: {0:.4f}\\n'.format(accuracy_score(mushrooms_y_train,classifier.predict(mushrooms_x_train_pca))))\nprint('Testing accuracy Score: {0:.4f}\\n'.format(accuracy_score(mushrooms_y_test,classifier.predict(mushrooms_x_test_pca))))\nplot_precision_recall_curve(classifier, mushrooms_x_test_pca, mushrooms_y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_predictions('Random Forest')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now you try!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# YOUR CODE HERE\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Experiment and ask questions!\n\nFeel free to try other things out that you may not have had a chance for earlier. You can also ask us any questions and we'll do our best to answer."},{"metadata":{},"cell_type":"markdown","source":"### References\n* https://www.kaggle.com/raghuchaudhary/mushroom-classification\n* Aurélien Géron (2019) \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition\". *O'Reilly Media, Inc.*  https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\n* https://towardsdatascience.com/tidying-up-with-pca-an-introduction-to-principal-components-analysis-f876599af383\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}