{"cells":[{"metadata":{"id":"kbGTNCEO7JOQ"},"cell_type":"markdown","source":"# Import statements"},{"metadata":{"id":"4Z24vU0q3w1Q","trusted":true},"cell_type":"code","source":"from datetime import datetime\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, confusion_matrix, classification_report, log_loss\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n#from sklearn.feature_selection import SelectKBest, chi2, f_classif # For the dimensionality reduction\n\n# For the tree visualization\nimport pydot\nfrom IPython.display import Image\nfrom sklearn.externals.six import StringIO\n\n# To enable plotting graphs in Jupyter notebook\n%matplotlib inline ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### pandas display options settings"},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(pd.options.display.max_rows) # default is 60 \n#print(pd.options.display.max_columns) # default is 20\n#print(pd.options.display.max_colwidth) # default is 50\n\npd.set_option(\"display.max_rows\", 100)\npd.set_option(\"display.max_columns\",100)\npd.set_option(\"display.max_colwidth\", 500)\n\npd.set_option('precision', 4)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"R76NLhW47Pq6"},"cell_type":"markdown","source":"# Get the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"xlsx = pd.read_excel('/kaggle/input/hr-analytics-case-study/data_dictionary.xlsx')\ngeneral = pd.read_csv('/kaggle/input/hr-analytics-case-study/general_data.csv')\nemp_survey = pd.read_csv('/kaggle/input/hr-analytics-case-study/employee_survey_data.csv')\nmgr_survey = pd.read_csv('/kaggle/input/hr-analytics-case-study/manager_survey_data.csv')\nintime = pd.read_csv('/kaggle/input/hr-analytics-case-study/in_time.csv')\nouttime = pd.read_csv('/kaggle/input/hr-analytics-case-study/out_time.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA and Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('general data shape = ', general.shape)\nprint('employee survey data shape = ', emp_survey.shape)\nprint('manager survey data shape = ', mgr_survey.shape)\nprint('in time data shape = ', intime.shape)\nprint('out time data shape = ', outtime.shape)\nprint('excel data shape = ', xlsx.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xlsx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"general.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emp_survey.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mgr_survey.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"intime.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outtime.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# change column name 'Unnamed: 0' to 'EmployeeID' in the in-time and out-time datasets.\nintime.rename(columns={'Unnamed: 0':'EmployeeID'}, inplace=True)\nouttime.rename(columns={'Unnamed: 0':'EmployeeID'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's set the column named EmployeeID as index in all files. We are getting ready for merge."},{"metadata":{"trusted":true},"cell_type":"code","source":"general.set_index('EmployeeID', inplace=True)\nemp_survey.set_index('EmployeeID', inplace=True)\nmgr_survey.set_index('EmployeeID', inplace=True)\nintime.set_index('EmployeeID', inplace=True)\nouttime.set_index('EmployeeID', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Combine the files of general_data, employee_survey_data, manager_survey_data.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"main_df = pd.concat([general, emp_survey, mgr_survey], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtypes_df = pd.DataFrame(main_df.dtypes, columns=['Type'])\ndtypes_df['Null'] = main_df.isnull().sum()\ndtypes_df['N-Unique'] = main_df.nunique()\ndtypes_df['Unique'] = [main_df[col].unique() if main_df[col].dtype == 'object' else [] for col in main_df]\ndtypes_df","execution_count":null,"outputs":[]},{"metadata":{"id":"kuR9TCG9iPfX"},"cell_type":"markdown","source":"# Compute Working Hours DataFrames and Columns"},{"metadata":{},"cell_type":"markdown","source":"### We need to calculate total working hours column as below from working times. After that, we can calculate overtime column as well. Finally, we can merge all files by adding 'time_mean' and 'overtime_mean' columns to our main_df."},{"metadata":{"trusted":true},"cell_type":"code","source":"#intime = intime.apply(pd.to_datetime)\n#outtime = outtime.apply(pd.to_datetime)\n#main_df['WorkingHours'] = (outtime - intime).mean(axis=1)\n#main_df['WorkingHours'] = main_df['WorkingHours'] / np.timedelta64(1, 's') # Convert Timedelta units to seconds (float64)\n#main_df['Overtime'] = main_df['WorkingHours'] - 28800 # (8 working hours per day * 3600)\n#main_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"intime = intime.apply(pd.to_datetime)\nouttime = outtime.apply(pd.to_datetime)\n\ntime = outtime - intime\n\n# Convert Timedelta units to seconds (float64)\ntime = time / np.timedelta64(1, 's')\n\n# convert Time-stamp to seconds (float64)\nintime = intime.applymap(lambda x: 3600*x.hour + 60*x.minute + x.second)\nouttime = outtime.applymap(lambda x: 3600*x.hour + 60*x.minute + x.second)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute Mean and Std of working hours per employee in seconds (float64)\nmain_df['in_mean'] = intime.mean(axis=1)\nmain_df['out_mean'] = outtime.mean(axis=1)\n\nmain_df['in_std'] = intime.std(axis=1)\nmain_df['out_std'] = outtime.std(axis=1)\n\nmain_df['time_mean'] = time.mean(axis=1)\nmain_df['time_std'] = time.std(axis=1)\n\n# Compute 'overtime_mean' column in seconds (float64)\nmain_df['overtime_mean'] = main_df['time_mean'] - 28800 # (8 working hours a day * 3600) \n\nmain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute Ratio of NaN for specific employee\n#intime.iloc[4,].isnull().sum() / intime.iloc[4,].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute Ratio of NaN to not-NaN values\nmain_df['ratio_NaN_time'] = (intime.isnull().sum(axis=1)).divide(intime.count(axis=1)) # count() does not include NA values.\nmain_df['ratio_NaN_time']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"Attrition\", y=\"time_mean\", data=main_df);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"Attrition\", y=\"overtime_mean\", data=main_df);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"Attrition\", y=\"ratio_NaN_time\", data=main_df);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The usual way to test for a NaN is to see if it's equal to itself:\ndef isNaN(my_val):\n    return my_val != my_val\n#------------------------------------\ndef intime_to_cat(my_int):\n    \"\"\"\n    10:30 - after --> Late (2)\n    09:30 - 10:29 --> Normal (1)\n    before 09:30 --> Early (0)\n    \"\"\"\n    if isNaN(my_int):\n        return np.nan\n    time_str = datetime.fromtimestamp(my_int).strftime('%H:%M')\n    hh, mm = map(int, time_str.split(':'))\n    if (hh >= 11) or (hh == 10 and mm >= 30):\n        return 2 # 'InLate'\n    elif (hh >= 10) or (hh == 9 and mm >= 30):\n        return 1 # 'InNormal'\n    else:\n        return 0 # 'InEarly'\n#------------------------------------    \ndef outtime_to_cat(my_int):\n    \"\"\"\n    18:30 - after --> Late (2)\n    17:30 - 18:29 --> Normal (1)\n    before 17:30 --> Early (0)\n    \"\"\"\n    if isNaN(my_int):\n        return np.nan\n    time_str = datetime.fromtimestamp(my_int).strftime('%H:%M')\n    hh, mm = map(int, time_str.split(':'))\n    if (hh >= 19) or (hh == 18 and mm >= 30):\n        return 2 # 'OutLate'\n    elif (hh >= 18) or (hh == 17 and mm >= 30):\n        return 1 # 'OutNormal'\n    else:\n        return 0 # 'OutEarly'\n#------------------------------------\ndef time_to_cat(my_int):\n    \"\"\"\n    8:00 or more --> Over-Time (2)\n    7:00 - 7:59 --> Normal Time (1)\n    until 6:59 --> Short Time (0)\n    \"\"\"\n    if isNaN(my_int):\n        return np.nan\n    hours = int(datetime.fromtimestamp(my_int).strftime('%H'))\n    if hours >= 8:\n        return 2 # 'OverTime'\n    elif hours >= 7:\n        return 1 # 'NormalTime'\n    else:\n        return 0 # 'ShortTime'\n#------------------------------------    \ndef int_to_date_str(my_int): # translates seconds to date string\n    if isNaN(my_int):\n        return np.nan\n    return datetime.fromtimestamp(my_int).strftime('%H:%M:%S')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#time = outtime - intime\n\n# Convert Timedelta units to seconds (float64)\n#time = time / np.timedelta64(1, 's')\n\n# Convert Time-stamp to seconds (float64)\n#intime = intime.applymap(lambda x: 3600*x.hour + 60*x.minute + x.second)\n#outtime = outtime.applymap(lambda x: 3600*x.hour + 60*x.minute + x.second)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"intime_cat = intime.applymap(intime_to_cat)\n\nouttime_cat = outtime.applymap(outtime_to_cat)\n\ntime_cat = time.applymap(time_to_cat)\ntime_cat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_df['time_cat_mean'] = time_cat.mean(axis=1)\nmain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"intime.median(axis=1).hist(bins=100, figsize=(30,10));\nplt.title('MEDIAN IN TIME');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outtime.median(axis=1).hist(bins=100, figsize=(30,10), color=\"g\");\nplt.title('MEDIAN OUT TIME');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"intime.std(axis=1).hist(bins=100, figsize=(30,10));\nplt.title('STD IN TIME');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outtime.std(axis=1).hist(bins=100, figsize=(30,10), color=\"g\");\nplt.title('STD OUT TIME');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's look at working hours (in seconds) during the year for a specific employee\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"empID = 5 # empID starts from 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time.iloc[2-1,].plot(figsize=(30,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,8));\n\nplt.subplot(3,1,1);\nintime_cat.iloc[empID-1,].plot(figsize=(18,20));\nplt.title(\"In time categories\")\n\nplt.subplot(3,1,2);\nouttime_cat.iloc[empID-1,].plot(figsize=(18,20), color='g');\nplt.title(\"Out time categories\")\n\nplt.subplot(3,1,3);\ntime_cat.iloc[empID-1,].plot(figsize=(18,20), color='r');\nplt.title(\"Total Working time categories\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"********* All-employees Mean In-Times *********\")\nprint(intime.mean().apply(int_to_date_str))\nprint(\"\\nMean in-time:\", int_to_date_str(intime.mean().mean()))\n\nprint(\"\\n********* All-employees Std In-Times *********\")\nprint(intime.std().apply(int_to_date_str))\nprint(\"\\nMean Std in-time:\", int_to_date_str(intime.std().mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"********* All-employees Mean Out-Times *********\")\nprint(outtime.mean().apply(int_to_date_str))\nprint(\"\\nMean out-time:\", int_to_date_str(outtime.mean().mean()))\n\nprint(\"\\n********* All-employees Std Out-Times *********\")\nprint(outtime.std().apply(int_to_date_str))\nprint(\"\\nMean Std out-time:\", int_to_date_str(outtime.std().mean()))","execution_count":null,"outputs":[]},{"metadata":{"id":"DUigvAa8XDlk"},"cell_type":"markdown","source":"### Handle Missing Values\n\n"},{"metadata":{"id":"4HkGHsuhE_JS","outputId":"3f0cf349-88e0-4ba0-8543-06fdbf9d6451","trusted":true},"cell_type":"code","source":"# check missing values count\nnull_columns = main_df.columns[main_df.isnull().any()]\nmain_df[null_columns].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to handle the null values in our dataset. Firstly, let's visulize them, so that we can decide to fill them either with median or mean etc.."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,8))\n\nplt.subplot(1,5,1)\nmain_df['NumCompaniesWorked'].plot(kind='density', color='teal');\nplt.title('Density Plot Of Number Of \\nCompanies Worked')\n\nplt.subplot(1,5,2)\nmain_df['TotalWorkingYears'].plot(kind='density', color='blue');\nplt.title('Density Plot Of \\nTotal Working Years')\n\nplt.subplot(1,5,3)\nmain_df['EnvironmentSatisfaction'].plot(kind='density', color='teal');\nplt.title('Density Plot Of \\nEnvironment Satisfaction')\n\nplt.subplot(1,5,4)\nmain_df['JobSatisfaction'].plot(kind='density', color='blue');\nplt.title('Density Plot Of \\nJob Satisfaction')\n\nplt.subplot(1,5,5)\nmain_df['WorkLifeBalance'].plot(kind='density', color='green');\nplt.title('Density Plot Of \\nWork Life Balance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Other values in these columns are not normally distributed. It's better to use median value to fill the null values."},{"metadata":{"trusted":true},"cell_type":"code","source":"#null_col = ['NumCompaniesWorked', 'TotalWorkingYears', 'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance']\n#for i in null_col:\n#    main_data[i] = main_data[i].fillna(main_data[i].median())","execution_count":null,"outputs":[]},{"metadata":{"id":"-lsY8xtWHzv1","trusted":true},"cell_type":"code","source":"# drop rows with missing values\nmain_df.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"ERGBmw64IGwG","outputId":"bf641a5e-88c1-488a-b861-c5da09cd086d","trusted":true},"cell_type":"code","source":"# check again to confirm there are no more missing values\nnull_columns = main_df.columns[main_df.isnull().any()]\nmain_df[null_columns].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"4ueLnO9rJV7L","outputId":"929a24f0-f807-4e3c-b01c-e1ddafcb7c66","trusted":true},"cell_type":"code","source":"# descriptive statistics. We use .T for Transposition \nmain_df.describe(include='all').T","execution_count":null,"outputs":[]},{"metadata":{"id":"FRTBcAS_XgM1"},"cell_type":"markdown","source":"### Drop unnecessary columns (which have only 1 unique value in them)"},{"metadata":{"id":"L_JzpFo3LB3_","trusted":true},"cell_type":"code","source":"# There are columns like EmployeeCount, Over18, StandardHours that have only 1 value, hence we drop them \n# 'EmployeeCount' is always 1\n# 'Over18' is always 'Y'\n# 'StandardHours' is always 8\nmain_df.drop(['EmployeeCount', 'Over18', 'StandardHours'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"breakpoint_df = main_df.copy()\n#main_df = breakpoint_df.copy()","execution_count":null,"outputs":[]},{"metadata":{"id":"oywfvtjiLvr-","outputId":"47bee48e-3991-4671-b805-7060592fcabf","trusted":true},"cell_type":"code","source":"# Attrition Ratio Pie Diagram\n\nattrition_value_counts = main_df['Attrition'].value_counts()\n\nprint(attrition_value_counts)\npct_attrition = (len(main_df[main_df['Attrition']=='Yes'])/len(main_df))*100\nprint('Rate of Attrition for Entire Company: {:.2f}%'.format(pct_attrition))\n\nplt.pie(attrition_value_counts, labels=['Not Attrited', 'Attrited']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(main_df[['Age','MonthlyIncome','DistanceFromHome','Attrition']],hue = 'Attrition');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_main_df = main_df.select_dtypes(include=['number'])\nplt.figure(figsize=(16,10))\nsns.heatmap(num_main_df.corr(), annot=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_df.hist(figsize=(20, 15));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_df['YearsAtCompany'].hist(figsize=(20, 15), bins=100);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_df['YearsSinceLastPromotion'].hist(figsize=(20, 15), bins=100, color='k');","execution_count":null,"outputs":[]},{"metadata":{"id":"-HLKB8V2X-by"},"cell_type":"markdown","source":"### Let's look at the Age of people who have left the Organization"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"Attrition\", y=\"Age\", data=main_df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see that for \"Attrition=='Yes'\" the ages are lower."},{"metadata":{"id":"3sFnop_sTwIt","outputId":"2af2cbec-4e7f-42ba-a9f6-56e4d2d39aa8","trusted":true},"cell_type":"code","source":"main_df[main_df['Attrition']=='Yes'].groupby('Age')['Attrition'].count()\\\n .plot(figsize=(20,10), title='Age wise Attrition');","execution_count":null,"outputs":[]},{"metadata":{"id":"lQlVUZrbYrAD"},"cell_type":"markdown","source":"### High number of People between age of 25 and 35 have left an Organization with max being at age of 29 and 31"},{"metadata":{"trusted":true},"cell_type":"code","source":"main_df.groupby('YearsAtCompany')['Attrition'].apply(lambda s : (s=='Yes').sum()/len(s)).plot(figsize=(30, 10), linestyle='-.', marker=\"d\");\n# maybe throw away outlier's data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_df.groupby('YearsSinceLastPromotion')['Attrition'].apply(lambda s : (s=='Yes').sum()/len(s)).plot(figsize=(30, 10),color='r', linestyle='-.', marker=\"d\");\n# maybe throw away outlier's data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,5))\n\nplt.subplot(1,3,1)\nsns.distplot(main_df['Age'], color='green');\nplt.xlim(10,70)\nplt.title('Age Distribution')\n\nplt.subplot(1,3,2)\nmain_df['MaritalStatus'].value_counts().plot(kind='bar', color='lightblue');\nplt.xticks(rotation=0)\nplt.title('Marital Status Distribution')\n\nplt.subplot(1,3,3)\nmain_df['Gender'].value_counts().plot(kind='bar', color='lightpink');\nplt.xticks(rotation=0)\nplt.title('Gender Distribution');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Age : Age group of employees is usually 30 - 40.\n* Marital Status : The number of married people is highest, while the divorced is lowest.\n* Gender : Male population is higher than female."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,10))\n\nplt.subplot(2,3,4)\nmain_df['Department'].value_counts().plot(kind='bar', color='lightblue');\nplt.xticks(rotation=0)\nplt.title('Department Distribution');\n\nplt.subplot(2,3,5)\nmain_df['JobRole'].value_counts().plot(kind='bar', color='lightblue');\nplt.title('Job Role Distribution');\n\nplt.subplot(2,3,6)\nmain_df['EducationField'].value_counts().plot(kind='bar', color='lightblue');\nplt.title('Education Field Distribution');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Department : There are many people working in R&D department. The number of people works in HR is the lowest.\n* Job Role: There are so many sales executive in the company. Sales department mostly include Sales Executives.\n* Education Field: There are so many people in the company who studied Life Sciences."},{"metadata":{},"cell_type":"markdown","source":"### create groups of ages"},{"metadata":{"trusted":true},"cell_type":"code","source":"#main_df['AgeGroupCategory'] = pd.cut(main_df['Age'], bins=[0,26,35,120], labels=['Junior', 'Busy', 'Senior'])\nmain_df['AgeGroupRange'] = pd.cut(main_df['Age'], range(10, 70, 10))\n\nmain_df.drop('Age', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#def Age(dataframe):\n#    dataframe.loc[dataframe['Age'] <= 30, 'AgeGroup'] = 1\n#    dataframe.loc[(dataframe['Age'] > 30) & (dataframe['Age'] <= 40), 'AgeGroup'] = 2\n#    dataframe.loc[(dataframe['Age'] > 40) & (dataframe['Age'] <= 50), 'AgeGroup'] = 3\n#    dataframe.loc[(dataframe['Age'] > 50) & (dataframe['Age'] <= 60), 'AgeGroup'] = 4\n#    dataframe.loc[(dataframe['Age'] > 60), 'AgeGroup'] = 5\n#    return dataframe\n\n#Age(main_df); \n\n#main_df.drop(['Age'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Look at categorical columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(main_df.AgeGroupRange.unique())\nprint(main_df.BusinessTravel.unique())\nprint(main_df.Department.unique())\nprint(main_df.Gender.unique())\nprint(main_df.MaritalStatus.unique())\nprint(main_df.EducationField.unique())\nprint(main_df.JobRole.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_df.JobRole.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graphs = ['AgeGroupRange', 'MaritalStatus', 'Gender', 'Department', 'BusinessTravel', 'JobRole', 'EducationField']\nplt.figure(figsize=(20,15))\nfor index, item in enumerate(graphs):\n    plt.subplot(3,3,index+1)\n    ax = sns.countplot(x=item, hue='Attrition', data=main_df, palette='husl')\n    if index+1>3: plt.xticks(rotation=90)\n    index = int(len(ax.patches)/2)\n    for left,right in zip(ax.patches[:index], ax.patches[index:]):\n        left_height = left.get_height()\n        right_height = right.get_height()\n        total = left_height + right_height\n        ax.text(left.get_x() + left.get_width()/2., left_height + 20, '{:.1%}'.format(left_height/total), ha=\"center\")\n        ax.text(right.get_x() + right.get_width()/2., right_height + 20, '{:.1%}'.format(right_height/total), ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These graphs show the Attrition level by Age Group, Marital Status, Gender, Department, Business Travel, Job Role, Education Field.\n\n* Age Group : 23% of 20-30 age group want to leave the company.\n\n* Marital Status : 23% of single people want to leave the company.\n\n* Gender : 17% of male employess want to leave the company.\n\n* Department : 16% of R&D department want to leave the company.\n\n* Business Travel : 25% of Travel-Frequenlty want to leave the company.\n\n* Job Role: 24% of Research Directors want to leave the company.\n\n* Education Field : 41% of Human Resources want to leave the company. This is very high."},{"metadata":{"id":"52ENy221ZgFj","outputId":"5c3a2345-5851-4987-f467-ebfffa827a0f","trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1,2,figsize=(20,7))\n\nsns.countplot(ax=ax1, x='Attrition', data=main_df, hue='JobLevel');\nsns.countplot(ax=ax2, x='Attrition', data=main_df, hue='Gender');\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1,2,figsize=(20,7))\n\nsns.countplot(ax=ax1, x='Attrition', data=main_df, hue='MaritalStatus');\nsns.countplot(ax=ax2, x='Attrition', data=main_df, hue='AgeGroupRange');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1,2,figsize=(20,7))\n\nsns.countplot(ax=ax1, x='Attrition', data=main_df, hue='Department');\nsns.countplot(ax=ax2, x='Attrition', data=main_df, hue='BusinessTravel');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping features: 'EducationField' (6 categories), 'JobRole' (9 categories). \n# The are 2 reasons why i decided to drop these columns:\n# 1. They have a lot of unique values.\n# 2. They don't seem as important or interesing as other features. \nmain_df.drop(['EducationField', 'JobRole'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute average of scores from survey data files. Each score is ‘low’=1, ‘medium’=2, ‘high’=3, ‘very high’=4 or equivalent scale.\nmain_df['SurveyAverageScore'] = (main_df['EnvironmentSatisfaction'] + main_df['JobSatisfaction'] + main_df['WorkLifeBalance'] +\n                                 main_df['JobInvolvement'] + main_df['PerformanceRating']) / 5\nmain_df.drop(['EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance', 'JobInvolvement', 'PerformanceRating'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create X (feature matrix - the data to fit) and y (labels vector- The target variable to try to predict)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = main_df.drop(['Attrition'], axis=1)\ny = main_df['Attrition']\n#y.replace({'Yes':1, 'No':0}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"Lf4QTJD8hyIc"},"cell_type":"markdown","source":"### Handle Categorical Features using *get_dummies*"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.get_dummies(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"DlY9OwjZjz4x","trusted":true},"cell_type":"code","source":"#encoder = LabelBinarizer() # sparse_output=True \n#X['BusinessTravel'] = encoder.fit_transform(X['BusinessTravel'])\n#X['Department'] = encoder.fit_transform(X['Department'])    \n#X['EducationField'] = encoder.fit_transform(X['EducationField'])    \n#X['Gender'] = encoder.fit_transform(X['Gender'])    \n#X['JobRole'] = encoder.fit_transform(X['JobRole'])    \n#X['MaritalStatus'] = encoder.fit_transform(X['MaritalStatus'])","execution_count":null,"outputs":[]},{"metadata":{"id":"0Z4Tl9WXFrJh"},"cell_type":"markdown","source":"# Train-Test-Split"},{"metadata":{"id":"XdefpjBKFUtJ","trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scaling"},{"metadata":{"id":"S_aSaEIyrdo2","trusted":true},"cell_type":"code","source":"#scaler = StandardScaler()\n#X_train = scaler.fit_transform(X_train)\n#X_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"r0JGTXT4PcE3"},"cell_type":"markdown","source":"# First Model: Grid Search with DecisionTreeClassifier, cv=5 and 'roc_auc' scoring."},{"metadata":{},"cell_type":"markdown","source":"### We start by describing two helper functions."},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_tree(model, max_depth=5, width=800):\n    dot_data = StringIO()  \n    export_graphviz(model, out_file=dot_data, feature_names=X_train.columns, max_depth=max_depth, \n                    leaves_parallel=True, filled=True, class_names=model.classes_)\n    graph = pydot.graph_from_dot_data(dot_data.getvalue())[0]  \n    return Image(graph.create_png(), width=width) \n\ndef print_dot_text(model, max_depth=5):\n    \"\"\"The output of this function can be copied to http://www.webgraphviz.com/\"\"\"\n    dot_data = StringIO()\n    export_graphviz(model, out_file=dot_data, feature_names=X_train.columns, max_depth=max_depth,\n                   leaves_parallel=True, filled=True, class_names=model.classes_)\n    dot_text = dot_data.getvalue()\n    print(dot_text)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'max_depth': [3, 7, 12], 'min_samples_leaf': [5, 15, 40], 'min_samples_split': [5, 10]} ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_cv = GridSearchCV(DecisionTreeClassifier(), param_grid=param_grid, cv=5, scoring='roc_auc')\n\ngs_cv.fit(X_train, y_train)\n\ndf_results_train = pd.DataFrame(gs_cv.cv_results_)[['param_max_depth', 'param_min_samples_leaf', 'param_min_samples_split', 'mean_test_score']]\n\ndf_results_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train: Tuned Decision Tree Parameters: {}\".format(gs_cv.best_params_))\nprint(\"Train: Best score is {:.3f}\".format(gs_cv.best_score_))\n\nscores_mean = df_results_train['mean_test_score'].mean()\nscores_std = df_results_train['mean_test_score'].std()\n\nprint(\"Train: Mean {:.3f}, STD {:.3f}\".format(scores_mean, scores_std))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = list(zip(gs_cv.best_estimator_.feature_importances_, X_train.columns))\nsorted(importances, key = lambda x: x[0], reverse=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see that the higher 'max_depth' is, the higher the mean_test_score."},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_tree(gs_cv.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"id":"VM9E8M57fPzM"},"cell_type":"markdown","source":"# Second Model: 10-fold Cross-Validation with RandomForestClassifier and 'roc_auc' scoring."},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = RandomForestClassifier(n_estimators=100, max_depth=12, min_samples_leaf=5, min_samples_split=10) # n_estimators default is 100. (Changed from 10 to 100 in sklearn version 0.22.)\n\nk = 10\n\ncv_results = cross_val_score(rf_clf, X_train, y_train, cv=k, scoring='roc_auc')\n\nprint(\"'roc_auc' Scores : \" + (k * \"{:.3f} \").format(*cv_results))\nprint(\"Mean {:.3f}, STD {:.3f}\".format(cv_results.mean(), cv_results.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = list(zip(rf_clf.feature_importances_, X_train.columns))\nsorted(importances, key = lambda x: x[0], reverse=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot ROC Curve (RandomForestClassifier)"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = RandomForestClassifier(n_estimators=100, max_depth=12, min_samples_leaf=5, min_samples_split=5) # n_estimators default is 100. (Changed from 10 to 100 in sklearn version 0.22.)\n\nrf_clf.fit(X_train, y_train)\n\n# Compute predicted probabilities: y_pred_prob\ny_pred_prob = rf_clf.predict_proba(X_test)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob, pos_label='Yes')\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compute AUC Score (RandomForestClassifier)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute and print AUC score\nprint(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot ROC Curve (LogisticRegression)"},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression()\n\nlogreg.fit(X_train, y_train)\n\n# Compute predicted probabilities: y_pred_prob\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob, pos_label='Yes')\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compute AUC Score (LogisticRegression)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute and print AUC score\nprint(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RandomizedSearchCV"},{"metadata":{},"cell_type":"markdown","source":"### GridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. A solution to this is to use RandomizedSearchCV, in which not all hyperparameter values are tried out. Instead, a fixed number of hyperparameter settings is sampled from specified probability distributions.\n\n### Note that RandomizedSearchCV will never outperform GridSearchCV. Instead, it is valuable because it saves on computation time."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import randint\n# randint(a,b) creates a new random variable, that has a discrete uniform distribution with possible outcomes a, ..., b-1.\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": randint(1, 9),\n              \"min_samples_leaf\": randint(1, 9),\n              \"criterion\": [\"gini\", \"entropy\"]}\n\ntree = DecisionTreeClassifier()\n\ntree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n\n# Fit it to the data\ntree_cv.fit(X_train, y_train) \n\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\nprint(\"Best score is {}\".format(tree_cv.best_score_))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# From the internet: Grid Search with several Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_params_dict = {\n    'svm': {\n        'model': SVC(gamma='auto'),\n        'params': {\n            'gamma': ['scale', 'auto'], \n            'C': [1, 10, 20],\n            'kernel': ['rbf']\n        } \n    },\n    'random_forest': {\n        'model': RandomForestClassifier(),\n        'params': { \n            'n_estimators': [10, 100]\n        } \n    },\n    'logistic_regression': {\n        'model': LogisticRegression(solver='liblinear', multi_class='auto'),\n        'params': { \n            'C': [1, 5, 10]\n        } \n    },\n    'knn': {\n        'model': KNeighborsClassifier(),\n        'params': {\n            'n_neighbors': [2, 5]\n        }\n        \n    },\n    'decision_tree': {\n        'model': DecisionTreeClassifier(),\n        'params': {\n            'min_samples_leaf': [1, 3, 5]\n        }   \n    }\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = []\nfor model_name, model_params in model_params_dict.items():\n  clf = GridSearchCV(model_params['model'], model_params['params'], cv=5)\n  #clf = RandomizedSearchCV(model_params['model'], model_params['params'], cv=5, n_iter=2)\n  clf.fit(X_train, y_train)\n  scores.append({\n      'model': model_name,\n      'best_score': clf.best_score_,\n      'best_params': clf.best_params_\n  })\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_results = pd.DataFrame(scores, columns = ['model', 'best_score', 'best_params'])\ndf_results","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}