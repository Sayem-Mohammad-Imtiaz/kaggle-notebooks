{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport os\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\nimport os\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom librosa import feature\nimport librosa \nimport matplotlib.pyplot as plt\nimport IPython.display as ipd  # To play sound in the notebook\nimport librosa.display\nimport json\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport seaborn as sns\nimport glob \nimport os\nfrom tqdm import tqdm\nimport pickle\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom torchvision import transforms, utils\nfrom torch.autograd import Variable\nfrom torchvision import models","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Use one audio file in previous parts again\nimport os\nMUSIC = '/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original'\nmusic_dataset = []\ngenre_target = []\nfor root, dirs, files in os.walk(MUSIC):\n    for name in files:\n        filename = os.path.join(root, name)\n        if filename != '/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original/jazz/jazz.00054.wav':\n            music_dataset.append(filename)\n            genre_target.append(filename.split(\"/\")[6])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"music_dataset[67]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mel_spec=[]\ngenre_new=[]\nN_FFT = 512\nN_MELS = 96\nHOP_LEN = 256\nnum_div=8\nfor idx, wav in enumerate(music_dataset):\n    y, sfr = librosa.load(wav)\n    div= np.split(y[:660000], num_div)\n    for chunck in div:\n        melSpec = librosa.feature.melspectrogram(y=chunck, sr=sfr, n_mels=N_MELS,hop_length=HOP_LEN, n_fft=N_FFT)\n        melSpec_dB = librosa.power_to_db(melSpec, ref=np.max)\n        mel_spec.append(melSpec_dB)\n        genre_new.append(genre_target[idx])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"genres={'pop':1,'classical':2,'reggae':3,'disco':4,'jazz':5,'metal':6,'country':7,'blues':8,'hiphop':9,'rock':0}\ngenre_id = [genres[item] for item in genre_new]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(mel_spec, genre_id, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 256\n\ntorch_X_train = torch.unsqueeze(torch.cuda.FloatTensor(X_train),1)\ntorch_y_train = torch.cuda.LongTensor(y_train)\n\n# create feature and targets tensor for test set.\ntorch_X_test = torch.unsqueeze(torch.cuda.FloatTensor(X_test),1)\ntorch_y_test = torch.cuda.LongTensor(y_test)\n\n# Pytorch train and test sets\ntrain = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\ntest = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)\n\n# data loader\ntrain_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = True)\ntest_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class upchannel(nn.Module):\n    def __init__(self):\n        super(upchannel, self).__init__()\n\n        self._convblocks = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=4),\n\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=4)\n        )\n        self._classifier = nn.Sequential(nn.Linear(in_features=512*5, out_features=1024),\n                                         nn.ReLU(),\n                                         nn.Dropout(),\n                                         nn.Linear(in_features=1024, out_features=256),\n                                         nn.ReLU(),\n                                         nn.Dropout(),\n                                         nn.Linear(in_features=256, out_features=10))\n        self.apply(self._init_weights)\n\n    def forward(self, x):\n        x = self._convblocks(x)\n        x = x.view(x.size(0), -1)\n        score = self._classifier(x)\n        return score\n\n    def _init_weights(self, layer) -> None:\n        if isinstance(layer, nn.Conv1d):\n            nn.init.kaiming_uniform_(layer.weight)\n        elif isinstance(layer, nn.Linear):\n            nn.init.xavier_uniform_(layer.weight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=upchannel()\nmodel.cuda()\nerror = nn.CrossEntropyLoss()\nlearning_rate=0.001\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 60\nmodel.train()\nfor epoch in range(EPOCHS):\n    correct = 0\n    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n        var_X_batch = Variable(X_batch).float()\n        var_y_batch = Variable(y_batch)\n        optimizer.zero_grad()\n        output = model(var_X_batch)\n        loss = error(output, var_y_batch)\n        loss.backward()\n        optimizer.step()\n\n                # Total correct predictions\n        predicted = torch.max(output.data, 1)[1] \n        correct += (predicted == var_y_batch).sum()\n                #print(correct)\n        if batch_idx % 50 == 0:\n            print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.data, float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(),'./CNN60.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correct = 0\ntotal = 0\nwith torch.no_grad():\n    for data in test_loader:\n        images, labels = data\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint('Accuracy of the network on the test images: %d %%' % (\n    100 * correct / total))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}