{"cells":[{"metadata":{},"cell_type":"markdown","source":"**In this notebook I'm doing a demonstration of Principal Component Analysis on the types of glass; although PCA is extensively used in Machine Learning as a way to simplify the data by reducing dimensionality, I believe it can be very useful from a visual point of view for simpler analysis, especially in the field of consumer behaviour. **"},{"metadata":{},"cell_type":"markdown","source":"Firstly, let's import the data and have a quick look at it:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndb = pd.read_csv('/kaggle/input/glass/glass.csv')\ndb.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"db.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking correlations between the types of glass components: "},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = db.corr(method = 'pearson')\ncorr_matrix ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation = db.corr(method = 'pearson')\nax = sns.heatmap(correlation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I'll be splitting the variables that will be grouped in components and the type of glass that can be considered the target:"},{"metadata":{"trusted":true},"cell_type":"code","source":"db1 = db.iloc[:,0:9]\ndb_target = db[[\"Type\"]]\nprint(db1.head())\nprint(db_target.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I'll run PCA with two components at first:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components = 2)\ndb_pca = pca.fit_transform(db1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at the eigen vectors:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.components_ ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's see what each of the two components contain:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(pca.components_, columns=list(db1.columns))\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And the most important part, how much variance are these two components explaining:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_ ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"47% explained by the first one, 26% by the second, so 71,3%; for a Machine Learning project I would go further and try to add more components, but right now I'm more interested to see how to represent this visually, in the next steps."},{"metadata":{"trusted":true},"cell_type":"code","source":"main_db = pd.DataFrame(data = db_pca\n             , columns = ['principal component 1', 'principal component 2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_db = pd.concat([main_db, db_target], axis = 1)\nfinal_db.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's have a look at the graphic representation:"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(8,6))\nplt.scatter(db_pca[:,0],db_pca[:,1], c = final_db['Type'])\n\n\nplt.xlabel('First principal component')\nplt.ylabel('Second Principal Component')\n\nplt.legend(numpoints = 6)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For someone interested more in the visual explanation of PCA, I believe it would be great to see what are the elements in each of the two components; I couldn't find yet something similar to R's biplot so I used a function found on github that helps make it (https://github.com/teddyroland/python-biplot). I will replicate this analysis in R and hopefully it will look even better!"},{"metadata":{"trusted":true},"cell_type":"code","source":"#======== TEST WITH FUNCTION REPLICATING BIPLOT FUNCTION FROM R =========#\n\nxvector = pca.components_[0] \nyvector = pca.components_[1]\nxs = pca.transform(db1)[:,0] # see 'prcomp(my_data)$x' in R\nys = pca.transform(db1)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(xvector)):\n\n    plt.arrow(0, 0, xvector[i]*max(xs), yvector[i]*max(ys),\n              color='r', width=0.0005, head_width=0.0025)\n    plt.text(xvector[i]*max(xs)*1.2, yvector[i]*max(ys)*1.2,\n             list(db1.columns.values)[i], color='r')\n\nfor i in range(len(xs)):\n\n    plt.plot(xs[i], ys[i], 'bo')\n    plt.text(xs[i]*1.2, ys[i]*1.2, list(db1.index)[i], color='b')\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.close(fig) \n#fig.show('YourPathHere')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}