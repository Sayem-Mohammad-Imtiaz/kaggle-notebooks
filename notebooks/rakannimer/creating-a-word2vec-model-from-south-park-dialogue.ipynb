{"cells":[{"metadata":{"_uuid":"1a08ecbaac5ef524aea2ed0cc7cec4bc93f17938"},"cell_type":"markdown","source":"**Creating word2vec models for SouthPark characters**\n\nWe start by defining the input and output locations."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"INPUT_CSV_FILE = \"../input/All-seasons.csv\"\nOUTPUT_TEXT_FILE = \"./vector.txt\"\nOUTPUT_JSON_FILE = \"./vector.json\"\nprint(\"Ready\")\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fc18ffc90a25853a70cf97891935c36ec0322de"},"cell_type":"markdown","source":"Then we parse the csv file and store it in a Pandas Dataframe.\nThe info methods gives us a small summary of the Dataframe"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"import pandas as pd\n\nsouthpark_df = pd.read_csv(INPUT_CSV_FILE)\nsouthpark_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0c76df7662197cb5f94d67180efed261e548b85"},"cell_type":"code","source":"unique_characters = southpark_df.Character.unique()\nprint(\"Found \",len(unique_characters),\" unique characters\")\nprint(unique_characters)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"651e30761888fd08684e49e83c6c94522158bdd1"},"cell_type":"markdown","source":"Too many characters, let's get how many lines each has and grab the top 4 most prolific characters"},{"metadata":{"trusted":true,"_uuid":"9a5be886b876da0af5555a187ae6f5f1936a01af"},"cell_type":"code","source":"grouped_by_character = southpark_df.groupby(['Character']).count().reset_index()\nsorted_characters = grouped_by_character.sort_values('Line', ascending=False)\nsorted_characters.head(4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f1a17a7937517566d5bfd5d66344196cf265e4a"},"cell_type":"markdown","source":"Let's start with Eric Cartman and get all the lines he has said"},{"metadata":{"trusted":true,"_uuid":"cdda5a2f102ef245be3d7470e18621663b5dc6ea"},"cell_type":"code","source":"CHARACTER_NAME = \"Cartman\"\ncharacter_lines = southpark_df.loc[southpark_df.Character == CHARACTER_NAME][\"Line\"]\nprint(character_lines.describe())\ncharacter_lines.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e691ebf233f5ccad0fdb3fe407929982849d51cc"},"cell_type":"markdown","source":"**Tokenzing **\n\nNow we want to split sentences to an array of lower cased words, to do that we apply a lambda to the dataframe"},{"metadata":{"trusted":true,"_uuid":"d007767d237996941bb5d39c3a1401ea5a068b37"},"cell_type":"code","source":"import string\nimport re\ndef tokenize(s) :\n    lower_case = s.lower();\n    without_punctuation = re.sub(r'[^\\w\\s]','',lower_case)\n    return without_punctuation.split()\n\ntokenized_lines = character_lines.apply(lambda row: tokenize(row))\ntokenized_lines.tail(4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9138a3e36a4c60faa392e64ffbe2909fc6d8eb30"},"cell_type":"markdown","source":"**Creating the Word2Vec model from our tokenized lines**"},{"metadata":{"trusted":true,"_uuid":"6307a04770cdce4031becc65e65a75f6b4c6fb03"},"cell_type":"code","source":"from gensim.models import Word2Vec\nmodel = Word2Vec(tokenized_lines, size=100, window=5, min_count=5, workers=4)\nmodel.wv.save_word2vec_format(OUTPUT_TEXT_FILE, binary=False)\nimport os\nprint(os.listdir('./'))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12021adf11c040208e6c5dbc66aef938206a6044"},"cell_type":"markdown","source":"Next we need to convert the text output to a json file"},{"metadata":{"trusted":true,"_uuid":"4d11c0d0761c2bca44af59f9b7126769e22cbdc5"},"cell_type":"code","source":"import json\ndef to_json(input, output): \n    f = open(input)\n    v = {\"vectors\": {}}\n    for line in f:\n        w, n = line.split(\" \", 1)\n        v[\"vectors\"][w] = list(map(float, n.split()))\n    with open(output, \"w\") as out:\n        json.dump(v, out)\n\nto_json(OUTPUT_TEXT_FILE, OUTPUT_JSON_FILE)\nimport os\nprint(os.listdir('./'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0702d913e6558994408a92634df161062fbbecae"},"cell_type":"markdown","source":"In our ouput we now have a vector.json that we can download and use from anywhere. \n\nExample usage in this sandbox : https://codesandbox.io/s/32n4w2kmz5"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}