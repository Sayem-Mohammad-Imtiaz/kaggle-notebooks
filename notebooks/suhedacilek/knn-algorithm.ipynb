{"cells":[{"metadata":{},"cell_type":"markdown","source":"### In this notebook,I use Biomechanical features of orthopedic patients dataset.\n* About in this dataset,\n    * There are 2 task and I used the second task, the categories Disk Hernia and Spondylolisthesis were merged into a single category labelled as 'abnormal'. Thus, the second task consists in classifying patients as belonging to one out of two categories: Normal (100 patients) or Abnormal (210 patients).\n\n    \n<font color = 'red'>    \n   \n# Content:\n    \n1.  [Load and Check Data](#1)\n2. [Exploratory Data Analaysis (EDA)](#2)\n3. [Normalization](#3)\n4. [K-Nearest Neighbors (KNN)](#4)\n  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = '1'></a>\n # Load and Check Data","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv')\nprint(plt.style.available) # look at available plot styles\nplt.style.use('seaborn-dark')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '2'> </a>\n# Exploratary Data Analaysis (EDA)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# to see features and target variable\n\ndata.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Well know question is is there any NaN value and length of this data so lets look at info\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As you can see:\n \n * length: 310 (range index)\n * Features are float\n * Target variables are object that is like string","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* pd.plotting.scatter_matrix:\n \n * green: normal and blue: abnormal\n * c: color\n * figsize: figure size\n * diagonal: histohram of each features\n * alpha: opacity\n * s: size of marker\n * marker: marker type","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"color_list = ['blue' if i=='Abnormal' else 'green' for i in data.loc[:,'class']]\npd.plotting.scatter_matrix(data.loc[:, data.columns != 'class'],\n                                       c=color_list,\n                                       figsize= [15,15],\n                                       diagonal='hist',\n                                       alpha=0.5,\n                                       s = 200,\n                                       marker = '*',\n                                       edgecolor= \"black\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Searborn library has countplot() that counts number of classes\n* Also you can print it with value_counts() method\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"class\", data=data)\ndata.loc[:,'class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"A = data[data['class'] =='Abnormal']\nN = data[data['class'] == \"Normal\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scatter plot\nplt.scatter(A.lumbar_lordosis_angle,A.pelvic_radius,color=\"blue\",label=\"abnormal\")\nplt.scatter(N.lumbar_lordosis_angle,N.pelvic_radius,color=\"green\",label=\"normal\")\nplt.xlabel(\"lumbar_lordosis_angle\")\nplt.ylabel(\"pelvic_radius\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can say Abnormal = 1 , Normal =1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['class'] = [1 if each == 'Abnormal' else 0 for each in data['class']]\ny = data['class'].values\nx_data = data.drop([\"class\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '3'></a>\n# Normalization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Normalization is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information.\n* We scale values between 0 and 1.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nx = (x_data- np.min(x_data))/ (np.max(x_data)- np.min(x_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As you can see our values are created between zeros and ones.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = '4'></a>\n# K-Nearest Neighbors (KNN)\n\n* KNN: Look at the K closest labeled data points\n* Classification method.\n* First we need to train our data. Train = fit\n* fit(): fits the data, train the data.\n* predict(): predicts the data\n* x: features\n* y: target variables(normal, abnormal)\n* n_neighbors: K. In this example it is 3. it means that Look at the 3 closest labeled data points","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Train test split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#knn model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3) # n_neighbors = k\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\" {} knn score: {}\".format(3,knn.score(x_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Find k value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#find k value\nscore_list = []\nfor each in range(1,15):\n    knn2= KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n\nplt.plot(range(1,15),score_list, color='brown')\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding Model Complexity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# model complexity\nneig = np.arange(1, 25)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    # k from 1 to 25(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn.fit(x_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(x_train, y_train))\n    # test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy', color = 'orange')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy', color= 'purple')\nplt.legend()\nplt.title('-value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('graph.png')\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nx,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\nknn.fit(x,y)\nprediction = knn.predict(x)\nprint('Prediction: {}'.format(prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\" {} knn score: {}\".format(20,knn.score(x_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model complexity\nneig = np.arange(1, 25)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    # k from 1 to 25(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn.fit(x_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(x_train, y_train))\n    # test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy', color = 'orange')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy', color= 'purple')\nplt.legend()\nplt.title('-value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('graph.png')\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}