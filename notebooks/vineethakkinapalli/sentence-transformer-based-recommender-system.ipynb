{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><h3><b>A Recommender System that recommends movies based on movie description.</b></h3></center>","metadata":{}},{"cell_type":"markdown","source":"<center><img src = \"https://wallpapercave.com/wp/wp5063342.png\" width=\"650\"></center>","metadata":{}},{"cell_type":"markdown","source":"<html>\n<style>\nh2 {\n  text-align: center;\n} </style>\n<h2 text-align:center > Recommendation Systems - Terminology</h2>\n</html>","metadata":{}},{"cell_type":"markdown","source":"* **Items (also known as documents)**:\nThe entities a system recommends.\n* **Query (also known as context)**:\nThe information a system uses to make recommendations.\n* **Embedding**:\nA mapping from a discrete set (in this case, the set of queries, or the set of items to recommend) to a vector space called the embedding space. Many recommendation systems rely on learning an appropriate embedding representation of the queries and items","metadata":{}},{"cell_type":"markdown","source":"<h2> Types of Recommendation Engines </h2>","metadata":{}},{"cell_type":"markdown","source":"* **Content based Filtering**: Uses similarity between items to recommend items similar to what the user likes.\n* **Collaborative Filtering**: Uses similarities between queries and items simultaneously to provide recommendations.\n\nHere we are going to use description column for recommending movie. since we are recommending based on what the user has watched this comes under content based filtering","metadata":{}},{"cell_type":"markdown","source":"<h3>Load csv into Pandas Dataframe<h3>","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport missingno\nnetflix_data = pd.read_csv('/kaggle/input/netflix-shows/netflix_titles.csv')\nnetflix_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Check for missing data in description column<h3> ","metadata":{}},{"cell_type":"code","source":"missingno.bar(netflix_data,figsize=(12,5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>From above barplot there is no misisng data in description column, so no need of any missing data handling.<h3>","metadata":{}},{"cell_type":"markdown","source":"# **TF-IDF Based**","metadata":{}},{"cell_type":"markdown","source":"Term Frequency-inverse document frequency (or TF-idf) is an established technique for scoring document similarity based on the importance of the words that they share.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel \n\ntf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')\ntfidf_matrix = tf.fit_transform(netflix_data['description'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<h4>Cosine Similarity<h4>**It computes the L2-normalized dot product of vectors. This is called cosine similarity, because Euclidean (L2) normalization projects the vectors onto the unit sphere, and their dot product is then the cosine of the angle between the points denoted by the vectors.This kernel is a popular choice for computing the similarity of documents represented as tf-idf vectors.","metadata":{}},{"cell_type":"code","source":"cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix) \nresults = {}\nfor idx, row in netflix_data.iterrows():\n    similar_indices = cosine_similarities[idx].argsort()[:-100:-1] \n    similar_items = [(cosine_similarities[idx][i], netflix_data['show_id'][i]) for i in similar_indices] \n    results[row['show_id']] = similar_items[1:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def item(id):  \n    return netflix_data.loc[netflix_data['show_id'] == id]['title'].tolist()[0].split(' - ')[0] \n\n# Just reads the results out of the dictionary.def \ndef recommend(item_id, num):\n    print(\"Recommending \" + str(num) + \" products similar to \" + item(item_id) + \"...\")   \n    print(\"-------\")    \n    recs = results[item_id][:num] \n    for rec in recs: \n        print(\"Recommended: \" + item(rec[1]) + \" (score:\" +      str(rec[0]) + \")\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recommend('s1305',6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **From above results we can see that the recommendations for chef's table were not close to the show's theme, so there is a need to improve the embeddings representation. We now use sentence transformers to respresent descriptions of show.**","metadata":{}},{"cell_type":"markdown","source":"# **Sentence Transformer Based**","metadata":{}},{"cell_type":"markdown","source":"<h3>This framework provides an easy method to compute dense vector representations for sentences, paragraphs, and images. The models are based on transformer networks like BERT / RoBERTa / XLM-RoBERTa etc.</h3>","metadata":{}},{"cell_type":"code","source":"!pip install -U sentence-transformers","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With SentenceTransformer('paraphrase-distilroberta-base-v1') we define which sentence transformer model we like to load. In this example, we load paraphrase-distilroberta-base-v1, which is a DistilBERT-base-uncased model fine tuned on a large dataset of paraphrase sentences.","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('paraphrase-distilroberta-base-v1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Find Embeddings for all show descriptions in dataset.","metadata":{}},{"cell_type":"markdown","source":"<h4> <b> *** Run the below cell only to find embeddings for each description. ***</b> <br> I have added the embeddings as npy file seperately skip to next cell.</h4>","metadata":{}},{"cell_type":"code","source":"import numpy as np\ndescriptions = netflix_data['description'].tolist()\n# print(descriptions)\ndes_embeddings = []\nfor i,des in enumerate(descriptions):\n    des_embeddings.append(model.encode(des))\n    ","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ndes_embeddings = np.load('../input/netflix-descriptions-bert-embeddings/descriptions_embeddings.npy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>For a query show id lets find the top five shows with highest cosine similarity.</h4>","metadata":{}},{"cell_type":"code","source":"import torch\nfrom sentence_transformers import SentenceTransformer, util\n\ndef recommend(query):\n    #Compute cosine-similarities with all embeddings \n    query_embedd = model.encode(query)\n    cosine_scores = util.pytorch_cos_sim(query_embedd, des_embeddings)\n    top5_matches = torch.argsort(cosine_scores, dim=-1, descending=True).tolist()[0][1:6]\n    return top5_matches\n\nid = 's1305'\nquery_show_des = netflix_data.loc[netflix_data['show_id'] == id]['description'].to_list()[0]\nrecommendded_results = recommend(query_show_des)\n\nfor index in recommendded_results:\n    print(netflix_data.iloc[index,:])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above results show that the top 5 recommendations of the show are:\n\n1. **Chef's Table: France**\n2. **Rotten**\n3. **The Mind of a Chef**\n4. **Chef's Table: BBQ**\n5. **The Chef Show**\n\nThe recommendations are now more close to the query.","metadata":{}},{"cell_type":"markdown","source":"<center> <h3><b> Please upvote if you liked this approach. In case of improvements or suggestion write it in comments. </b></h3> </center> ","metadata":{}}]}