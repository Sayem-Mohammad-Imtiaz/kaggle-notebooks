{"cells":[{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"# Import dataset\n\nimport pandas as pd\ndf = pd.read_csv(\"../input/iris-flower-dataset/IRIS.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import utility.py\n\nimport numpy as np\nfrom scipy.special import softmax # use built-in function to avoid numerical instability\n\nclass Utility:\n    @staticmethod\n    def identity(Z):\n        return Z,1\n    \n    @staticmethod\n    def tanh(Z):\n        \"\"\"\n        Z : non activated outputs\n        Returns (A : 2d ndarray of activated outputs, df: derivative component wise)\n        \"\"\"\n        A = np.empty(Z.shape)\n        A = 2.0/(1 + np.exp(-2.0*Z)) - 1 # A = np.tanh(Z)\n        df = 1-A**2\n        return A,df\n    \n    @staticmethod\n    def sigmoid(Z):\n        A = np.empty(Z.shape)\n        A = 1.0 / (1 + np.exp(-Z))\n        df = A * (1 - A)\n        return A,df\n    \n    @staticmethod\n    def relu(Z):\n        A = np.empty(Z.shape)\n        A = np.maximum(0,Z)\n        df = (Z > 0).astype(int)\n        return A,df\n    \n    @staticmethod\n    def softmax(Z):\n        return softmax(Z, axis=0) # from scipy.special\n    \n    @staticmethod\n    def cross_entropy_cost(y_hat, y):\n        n  = y_hat.shape[1]\n        ce = -np.sum(y*np.log(y_hat+1e-9))/n\n        return ce\n    \n    \"\"\"\n    Explication graphique du MSE:\n    https://towardsdatascience.com/coding-deep-learning-for-beginners-linear-regression-part-2-cost-function-49545303d29f\n    \"\"\"\n    @staticmethod\n    def MSE_cost(y_hat, y):\n        mse = np.square(np.subtract(y_hat, y)).mean()\n        return mse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separate data and labels\nX = df.iloc[:,0:4]\ny = pd.get_dummies(df.iloc[:,4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split: 1 set to train, 1 set to test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a dataframe instance, return a numpy array\ndef toList(df,i):\n    return np.transpose(df.iloc[[i]].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statistics\nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt\n\nclass NeuralNet:\n    def __init__(self, X_train, y_train, X_test, y_test, hidden_layers_sizes, activation, learning_rate, epoch):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_test = X_test\n        self.y_test = y_test\n        self.hidden_layers_sizes = hidden_layers_sizes\n        self.activation = activation\n        self.learning_rate = learning_rate\n        self.epoch = epoch\n        self.n_layers = len(hidden_layers_sizes)\n        \n        # Initializaing weights matrices\n        self.weights = [None] * (self.n_layers + 1)\n        for i in range(self.n_layers + 1):\n            if i == 0:\n                self.weights[i] = self.__weights_initialization(len(self.X_train.columns), self.hidden_layers_sizes[i])\n            elif i == (self.n_layers):\n                self.weights[i] = self.__weights_initialization(self.hidden_layers_sizes[i-1], len(self.y_train.columns))\n            else:\n                self.weights[i] = self.__weights_initialization(self.hidden_layers_sizes[i-1], self.hidden_layers_sizes[i])\n                \n        # Initializing bias matrices\n        self.bias = [None] * (self.n_layers + 1)\n        for i in range(self.n_layers + 1):\n            if i == (self.n_layers):\n                self.bias[i] = self.__bias_initialization(len(self.y_train.columns))\n            else:\n                self.bias[i] = self.__bias_initialization(self.hidden_layers_sizes[i])\n        \n        # Initializing the derivative list\n        self.df = [None]*(self.n_layers + 1)\n        \n        # Initializing the application list\n        self.A = [None]*(self.n_layers + 1)\n        \n    def __weights_initialization(self, X, y):\n        return np.random.uniform(low=0.0, high=1.0, size=(y*X)).reshape(y,X)\n    \n    def __bias_initialization(self, y):\n        return np.random.uniform(low=0.0, high=1.0, size=y).reshape(y,1)\n    \n    # Forward propagation\n    def forwardPropagation(self, X, y, i):\n        for k in range(self.n_layers + 1):\n            if k == 0:\n                A = np.transpose((X.iloc[[i]].values))\n                W = self.weights[k]\n                b = self.bias[k]\n                Z = np.add(np.matmul(W,A), b)\n                self.A[k], self.df[k] = self.activation(Z)\n            elif k == self.n_layers:\n                A = self.A[k-1]\n                W = self.weights[k]\n                b = self.bias[k]\n                Z = np.add(np.matmul(W,A), b)\n                self.A[k] = Utility.softmax(Z)\n            else:\n                A = self.A[k-1]\n                W = self.weights[k]\n                b = self.bias[k]\n                Z = np.add(np.matmul(W,A), b)\n                self.A[k], self.df[k] = self.activation(Z)\n\n        # Error\n        y_hat = self.A[-1]\n        y = toList(y, i)\n        return Utility.cross_entropy_cost(y, y_hat)\n    \n    # Backward propagation\n    def backwardPropagation(self, X, y):\n        delta = [None] * (self.n_layers + 1)\n        dW = [None] * (self.n_layers + 1)\n        db = [None] * (self.n_layers + 1)\n        \n        delta[-1] = (self.A[-1] - y)\n        dW[-1] = np.matmul(delta[-1], np.transpose(self.A[-2]))\n        db[-1] = delta[-1]\n        \n        for i in range(self.n_layers):\n            l = self.n_layers - 1 - i\n            delta[l] = np.multiply(np.matmul(np.transpose(self.weights[l+1]), delta[l+1]), self.df[l])\n            if l == 0:\n                dW[l] = np.matmul(delta[l], np.transpose(X))\n            else:\n                dW[l] = np.matmul(delta[l], np.transpose(self.A[l-1]))\n            db[l] = delta[l]\n    \n        for j in range(self.n_layers + 1):\n            self.weights[j] = np.subtract(self.weights[j], np.dot(self.learning_rate, dW[j]))\n            self.bias[j] = np.subtract(self.bias[j], np.dot(self.learning_rate, db[j]))\n    \n    def trainEpoch(self):\n        erreur_train_list = ([],[])\n        erreur_test_list = ([],[])\n        for e in range(self.epoch):\n            # Shuffle dataset\n            self.X_train, self.y_train = shuffle(self.X_train, self.y_train)\n            \n            # Training\n            erreur_train = []\n            for i in range(len((self.X_train).index)):\n                erreur_train.append(self.forwardPropagation(self.X_train, self.y_train, i))\n                self.backwardPropagation(toList(self.X_train, i), toList(self.y_train, i))\n            erreur_train_list[0].append(e)\n            erreur_train_list[1].append(statistics.mean(erreur_train))\n            \n            # Testing\n            erreur_test = []\n            for i in range(len((self.X_test).index)):\n                erreur_test.append(self.forwardPropagation(self.X_test, self.y_test, i))\n            erreur_test_list[0].append(e)\n            erreur_test_list[1].append(statistics.mean(erreur_test))\n        \n        plt.plot(erreur_train_list[0], erreur_train_list[1], label='Train')\n        plt.plot(erreur_test_list[0], erreur_test_list[1], label='Test')\n        plt.xlabel('Epoch of training')\n        plt.ylabel('Error')\n        plt.legend()\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NeuralNet(X_train, y_train, X_test, y_test, (3,2), Utility.identity, 0.01, 100).trainEpoch()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}