{"nbformat_minor":1,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load Packages","metadata":{"_cell_guid":"c60544c6-f2bc-40e2-958e-afe024359c91","_uuid":"1d378d12f78e0738c6a81d5f5e0942c9bda045b0"}},{"cell_type":"code","source":"# Packages for data loading and manipulation\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport scipy\nimport re\n\n# Packages for display\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib.dates as mdates\nfrom matplotlib.ticker import AutoMinorLocator\n## import mglearn as mg\nfrom tqdm import tqdm\nfrom IPython.display import display, HTML, Image\n## import gmplot\n## from pandas_ml import ConfusionMatrix\n\n# Plot graphviz\nfrom sklearn.externals.six import StringIO\nfrom sklearn.tree import export_graphviz\n## import pydotplus\n\n# Packeages for time series\nfrom time import time\nfrom datetime import datetime, timedelta\n## from bdateutil import isbday\n## import holidays\n\n# Data manipulation tools\n## import more_itertools as mit\n\n# Math\nimport random\nfrom scipy.stats.stats import pearsonr  \n\n# System\n## from wurlitzer import sys_pipes # This is used to read the Jupyter console output.\nimport warnings\nimport socket # Check if there is internet connection","metadata":{"_cell_guid":"8483ab94-7039-4ab5-a4bb-dcf0879dd6ea","collapsed":true,"_uuid":"d0d5939e225a3e4f3483f84cef9a734d621653ed"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Machine learning models\nimport sklearn\n\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold, ShuffleSplit, TimeSeriesSplit, cross_val_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, learning_curve\n\nfrom sklearn.preprocessing import MinMaxScaler, minmax_scale\nfrom sklearn.decomposition import PCA\n\n# Metrics\nfrom sklearn.metrics import fbeta_score, accuracy_score, mean_squared_log_error, median_absolute_error, make_scorer\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report\n\n# Unsupervised learning \nfrom sklearn.cluster import KMeans\n\n# Regression algorithms\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.ensemble import AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\n# from sklearn.isotonic import IsotonicRegression # This regressor doesn't have a default parameter setting.\n# from sklearn.linear_model import ARDRegression # This regressor takes too much time to model.\nfrom sklearn.linear_model import HuberRegressor, Lasso, LassoCV, LinearRegression, PassiveAggressiveRegressor\nfrom sklearn.linear_model import RANSACRegressor, Ridge, SGDRegressor, TheilSenRegressor \nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.neighbors import KNeighborsRegressor, RadiusNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.svm import LinearSVR, NuSVR, SVR\nfrom sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\nfrom xgboost import XGBRegressor\n\n# Classification algorithms\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n# from sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier, RidgeClassifier, SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB # BernoulliNB and MultinomialNB are suitable only for discrete features\nfrom sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n# from sklearn.neighbors import RadiusNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import LinearSVC, NuSVC, SVC\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom xgboost import XGBClassifier\n\nimport xgboost as xgb","metadata":{"_cell_guid":"015a60c6-9dff-4ba2-a413-65b28f99f609","collapsed":true,"_uuid":"a21d47b67d8f3fa1029085cf2d01a7ca0ffb8690"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One can ignore the warning as this is a known issue for tensorflow 1.4.1, but it doesn't affect the usage.\n# https://github.com/tensorflow/tensorflow/issues/14182\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    import tensorflow as tf\n    from tensorflow.python.client import device_lib","metadata":{"_cell_guid":"46cf0bf2-b007-467c-b0a2-c419e7d6a7d9","scrolled":false,"collapsed":true,"_uuid":"ef6ef7f9938a4a4c779f03978e3558a4efbd594a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Deep learning packages\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\n# from keras.callbacks import Callback\nfrom keras.callbacks import ModelCheckpoint\nimport keras.backend as K\nfrom keras import initializers as K_init\nfrom keras.optimizers import Adam","metadata":{"_cell_guid":"53fbf2f9-c79d-493e-9b14-3f57541c99d8","_uuid":"871dfef1ce8ca48bda808e1c4e7851a0157ac0f5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set a global seed for randomization\nrandom_state = 16\nnp.random.seed(random_state)","metadata":{"_cell_guid":"ca5769c0-0c4e-4252-8d18-8dcd822c02ef","collapsed":true,"_uuid":"816a5c59bbda28ed57177afe001bdf43dcdf60e7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nprint(sys.version)","metadata":{"_cell_guid":"efc0b38d-192e-4379-9aa5-2afd60faef1f","_uuid":"4a9486e33fe81d9976b00108130d92c96f81cad5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print version of important Python packages\nimportant_packages = [np, scipy, pd, matplotlib, sklearn, xgb, tf, keras]\n\nfor package in important_packages:\n    print('{}: {}'.format(package.__name__, package.__version__))","metadata":{"_cell_guid":"f21ecc6f-74d0-4c29-ba6a-f41a46bc2e51","_uuid":"efc7060f338ac024c0fa4d1b5d165c06e191751f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"notebook_start_time = time()","metadata":{"_cell_guid":"77b7c0b4-b409-4e99-b9f8-1107bbba8f9a","collapsed":true,"_uuid":"5ad8ebe98b3d4541596351367297f9991a7e6f86"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Import","metadata":{"_cell_guid":"4b9ce269-960b-4628-bfc3-7c20219761a7","_uuid":"dc283ad21dc611b3342c5880b481c8389a5c532d"}},{"cell_type":"code","source":"# First, import all the datasets.\n# The status data cannot be directly imported due to its size. Below I will read the data through the database.\n# The 'station.csv' needs to be read before one can process the sqlite in the below functions.\n# Zip codes should be loaded as strings.\nstation = pd.read_csv('../input/station.csv')\ntrip = pd.read_csv('../input/trip.csv', dtype={'zip_code':'str'})\nweather = pd.read_csv('../input/weather.csv')","metadata":{"_cell_guid":"18b395ae-3a6a-49b9-8b95-bdb1e2074f80","collapsed":true,"_uuid":"5e76bb7807917a4dfa803270f125bcb459abb5ce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If sql_status is true, the status data will be obtained from the sqlite file (which takes about 10 min) and saved to a new csv file named \"status_change.csv\".\n# If false, the status data will be obtained from the preprocessed \"status_change.csv\" file.\nsql_status = True","metadata":{"_cell_guid":"31b2d75f-6200-4a56-9acf-f56d6f1431a7","collapsed":true,"_uuid":"9324c1d7c95d2e340998973e77b6712652bce119"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a convenient funtion to use SQL query.\ndef sql(cursor, command):    \n    cursor.execute(command)\n    result = cursor.fetchall()\n    return result     ","metadata":{"_cell_guid":"c9287aac-eb14-4698-b35b-671e17714db9","scrolled":true,"collapsed":true,"_uuid":"e136c049f5202210a369adb2cb2ce3b17d74e4d3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bike_status(station_id, sql_conn):\n    '''The bike_status function iterates over stations and records only the changes of \"bikes_available\" and \"docks_available\" status.\n    A Python 3.6 feature is used in this function. The f-string.'''\n    \n    status = pd.read_sql_query(f\"SELECT * FROM status WHERE station_id = {station_id} ORDER BY time\", con=sql_conn)\n    \n    # Register the change of status once the change is detected.\n    # The exploration of status data later suggests that the datetime in status data is generally 1 minute later than that in the trip data. Both are recorded roughly every minute.\n    status_compact = status[(status.bikes_available.diff(-1) * status.docks_available.diff(-1)) != 0]\n    return status_compact","metadata":{"_cell_guid":"62510d64-f048-4ba6-a381-8cedafe79672","collapsed":true,"_uuid":"0188a472a99ec396e415697713811e481d329c79"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if sql_status == True:\n    # Connect to the database in sqlite format.\n    conn = sqlite3.connect('../input/database.sqlite')\n    cursor = conn.cursor()  \n    \n    # Explore the tables in the database.\n    find_tables = \"SELECT name FROM sqlite_master;\"\n    tables = sql(cursor, find_tables)\n    print(tables)","metadata":{"_cell_guid":"b460b430-a3ec-4502-9824-7c6d87980592","_uuid":"8f224f8b876f514ed50c88eac00bfc009d2af59d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if sql_status == True:\n    # Investigate the \"status\" columns in the database.\n    # Python 3.6 f-string used.\n    print(f\"Table {tables[1][0]} contains the following columns:\")\n    display(sql(cursor, f\"PRAGMA table_info({tables[1][0]});\")) # tables[1][0] is 'status'.","metadata":{"_cell_guid":"96e04fc3-c67b-4a65-a927-8f22223dc993","_uuid":"2a3426f4bade7e261aef9e3f2834a3aaf3264164"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if sql_status == True:\n    # It is nearly impossible to directly import the status.csv to a pandas dataframe. It takes too much time and space.\n    # In fact, it is not necessary to take all the data as plenty of them contain little information. \n    # The bike_status function will \"compress\" the original big data.\n    # It takes about 11 minutes to process the data.\n    # The 'station.csv' needs to be read before running the following code.\n    \n    status_list = [bike_status(station_id, conn) for station_id in tqdm(station.id)]\n    \n    # Concatenate all the station status to form a dataframe \"status\".\n    status = pd.concat(status_list, ignore_index=True)\n            \n    # Double check that the concatenation doesn't remove rows.\n    sum_status = 0\n    for bike_station in status_list:\n        sum_status += len(bike_station)\n    \n    print(sum_status)\n    print(status.shape)\n       \n    # Write a .csv data for future import to save time. \n    # The size of data shrinks from the original 2.0GB to less than 60MB without losing important information.\n    status.to_csv(\"status_change.csv\", index=False)\n    conn.close()\nelse:\n    status = pd.read_csv(\"status_change.csv\")","metadata":{"_cell_guid":"c02e12a3-9054-4248-9afe-a535a2f1885d","_uuid":"38470b6b1a62139c94eead68bc58ad337eaa8e8a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the pandas option to display all the weather columns.\npd.set_option('display.max_columns', 50)\n\nprint(f\"\\nA sample of status data {status.shape}:\"); display(status.head(3))\nprint(f\"\\nA sample of station data {station.shape}:\"); display(station.head(3))\nprint(f\"\\nA sample of trip data {trip.shape}:\"); display(trip.head(3))\nprint(f\"\\nA sample of weather data {weather.shape}:\"); display(weather.head(3))","metadata":{"_cell_guid":"a3e543b4-f5c1-433b-8a2e-c40a0efa7184","scrolled":true,"_uuid":"e1dce44b0d6707b3be0b9bebca8ef9e096693915"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{"_cell_guid":"a82fc09d-caaf-4824-b2a4-906d386cc922","collapsed":true,"_uuid":"5cf77634657977a57af389a062b8aac57ff12f29"}},{"cell_type":"markdown","source":"## 1.  Exploration on 'station' data.","metadata":{"_cell_guid":"48f952c1-a143-4734-9f58-208df70f0f55","_uuid":"2d6b9f6e74892118d4754737a758cb775383dbc9"}},{"cell_type":"code","source":"# First check whether there are nan values in station data.\nstation.isnull().sum()","metadata":{"_cell_guid":"06885060-5afe-4225-8d32-5780c1e6de80","_uuid":"2b0bd3c6be45a759cbf68299fa37903e2ebe982b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1  Mark stations on Google Map.","metadata":{"_cell_guid":"77ff5a04-2498-4915-8b93-af652a67a5ae","_uuid":"b116353fb7d7cb28070675a621e4e7021df04bea"}},{"cell_type":"code","source":"# Create a location format that is compatible with that for data in the below html file.\nlocations = np.column_stack((station.name, station.lat, station.long, station.id)).tolist()\nprint(locations[0])","metadata":{"_cell_guid":"f104e284-d8aa-4de8-b91a-1bc06f78560f","_uuid":"ee3981884af750e44e300b8cdce35ff12fbfd2d4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a function here:\n# Codes borrowed from:\n# https://gist.github.com/parth1020/4481893\n# https://github.com/vgm64/gmplot/blob/master/gmplot/gmplot.py\n# Fremont location (37.548270, -121.988572)\n\npara1 = '''\n<html>\n<head>  \n    <title>Google Maps Multiple Markers</title>\n    <script src=\"http://maps.google.com/maps/api/js?sensor=false&key=AIzaSyCsdvHMtyKFqb98ybFMw_q5QSipYEoZU7Y\" type=\"text/javascript\"></script>\n</head>\n<body>\n    <div id=\"map\" style=\"height: 600px; width: 1200px;\">\n    </div>\n    <script type=\"text/javascript\">\n'''\npara2 = f'var locations = {locations};'\n\npara3 = '''\n    var map = new google.maps.Map(document.getElementById('map'), {\n      zoom: 10,\n      center: new google.maps.LatLng(37.548270, -121.988572),\n      mapTypeId: google.maps.MapTypeId.ROADMAP\n    });\n\n    var infowindow = new google.maps.InfoWindow();\n\n    var marker, i;\n\n    for (i = 0; i < locations.length; i++) { \n      marker = new google.maps.Marker({\n        position: new google.maps.LatLng(locations[i][1], locations[i][2]),\n        map: map\n      });\n\n      google.maps.event.addListener(marker, 'click', (function(marker, i) {\n        return function() {\n          infowindow.setContent(locations[i][0]);\n          infowindow.open(map, marker);\n        }\n      })(marker, i));\n    }\n    </script>\n</body>\n</html>'''","metadata":{"_cell_guid":"d7069666-5cc2-43d8-a0db-8ad4b4855fec","collapsed":true,"_uuid":"85700b598a4b76d700201e9c27e32aebeeff37c7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# File name for writing the html code.\nbike_map = \"Bike_Map.html\"","metadata":{"_cell_guid":"b999a7d3-4602-4968-bd27-f77256be48dd","collapsed":true,"_uuid":"20f9284f316409ba8ba85078987310c35f4820ea"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Open the file to write the code and then exit it.\nf = open(bike_map, 'w')\nf.write(para1 + para2 + para3)\nf.close()","metadata":{"_cell_guid":"c139ae84-ab75-4769-b402-bd4a60a58d90","collapsed":true,"_uuid":"90208bd97c15d61731f911d79814257c3b0442c9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a function to check if there is internet connection.\n# Code borrowed from:\n# https://stackoverflow.com/questions/20913411/test-if-an-internet-connection-is-present-in-python\n\nREMOTE_SERVER = \"www.google.com\"\ndef is_connected():\n  try:\n    # see if we can resolve the host name -- tells us if there is a DNS listening.\n    host = socket.gethostbyname(REMOTE_SERVER)\n    # connect to the host -- tells us if the host is actually reachable.\n    s = socket.create_connection((host, 80), 2)\n    return True\n  except:\n     pass\n  return False","metadata":{"_cell_guid":"6b0c50b2-82a1-465b-b844-0c869218310f","collapsed":true,"_uuid":"eaab0e6bed1000ab8e32a556245e9699da67711e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if is_connected():\n    # Plot the markers on the interactive map with the station names, if there is internet\n    display(HTML(f'<iframe src={bike_map} height=\"630px\" width=\"100%\"></iframe>'))","metadata":{"_cell_guid":"ec467bf6-4bd4-4a53-ba49-98f1a382d7f0","collapsed":true,"_uuid":"21d8927e2132a32bf81222d3c77524ea9340a217"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2 Create a heat map to view the distribution of docks, thus the hot bike-sharing spots.","metadata":{"_cell_guid":"e4a0484d-d852-4a41-b07d-402be2e1f984","_uuid":"f4b8d094362611f0daf7dbfb3ee0351f4ec25537"}},{"cell_type":"code","source":"# https://manojsaha.com/2017/03/08/drawing-locations-google-maps-python/\n# https://pypi.python.org/pypi/gmplot/1.1.1\n# Initialize two empty lists to hold the latitude and longitude values\n# Obtain coordinates of the stations multiplied by the number of dock counts on each station to see the heatmap of the available bikes.\nlatitudes = []\nlongitudes = []\nfor i in station.index:\n    for dock in range(station.dock_count[i]):\n        latitudes.append(station.lat[i])\n        longitudes.append(station.long[i])","metadata":{"_cell_guid":"2ab036fa-c3c9-4286-8df6-9971cec59dd9","collapsed":true,"_uuid":"e4cbd5c0bffa9b9a18b265223706acfd500e48b8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if is_connected():\n    # Locate the center of the map to Fremont.\n    # Sometimes the geocoding doesn't work very well. Just re-run the cell to clear the error message.\n    # center = gmplot.GoogleMapPlotter.geocode(\"Fremont\")\n    \n    center = (37.5482697, -121.9885719) # This is the Fremont coordinator\n    gmap0 = gmplot.GoogleMapPlotter(center[0], center[1], zoom=10.5, apikey='AIzaSyCsdvHMtyKFqb98ybFMw_q5QSipYEoZU7Y')\n    \n    gmap0.heatmap(latitudes, longitudes)\n    gmap0.draw(\"docks_heatmap.html\")\n    \n    # Show the interactive hot map, if there is internet\n    # As expected, most of the bikes are located in San Francisco and San Jose city centers as designed.\n    display(HTML('<iframe src=\"docks_heatmap.html\" height=\"630px\" width=\"150%\"></iframe>'))","metadata":{"_cell_guid":"2549a9b7-8911-4aaa-b8d1-6232969c608a","collapsed":true,"_uuid":"b1063b71dafa947cef7af5666f8b6902b59f97d9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, San Francisco and San Jose are the two hottest spot, while Redwood city is also a hot spot. A reasonable clustering of stations will be explored below.","metadata":{"_cell_guid":"690ec7e6-14a9-4777-95b1-c0d6ded04996","_uuid":"3da1438c283049326ff7d7aebf98345be5351cfa"}},{"cell_type":"markdown","source":"### 1.3 Station installation dates.","metadata":{"_cell_guid":"7bd5b82c-71f6-4f32-a2bd-65f03b6bb3b3","_uuid":"3c10dce1aeed316b1ff25559c00cfc4871c47b8f"}},{"cell_type":"code","source":"station.groupby('installation_date').agg({'dock_count': ['count', 'sum']})","metadata":{"_cell_guid":"9b46cf59-38ef-4533-9aea-7f5f68fd5e66","_uuid":"926cf4412755b0e0e1d605df3997f16ff3917457"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the stations are built around the same time in August 2013. However, there are still a few built later. In populous areas, if many people are using the sharebike service, the increase of stations wil have quite some effect on the trip counts in that area. This information will be used later in the regression problem.","metadata":{"_cell_guid":"cae58dbb-c75c-4497-8c5d-733cb231ac8b","_uuid":"aef0d221a70aa1b1b629422fb756ccc8427e06ce"}},{"cell_type":"markdown","source":"## 2. Exploration on 'trip' data.","metadata":{"_cell_guid":"a5a7de62-e7aa-49cc-b366-c059daa78b09","_uuid":"d3c809894891a6a278e4a1814dc98abb25538dde"}},{"cell_type":"markdown","source":"### 2.1 Find independent station groups with least inter-group trips.","metadata":{"_cell_guid":"53522328-80c2-4891-9cb7-7b2e1d99129b","_uuid":"3bb1e49819f917354edd138d987cb0cd1348cce3"}},{"cell_type":"code","source":"# Use clustering techniques to plot locations.\nb_loc = station[['lat', 'long']] # b_loc: bike locations\n\n# Use KMeans to find the clusters with the number of 3 and 5.\n## fig, axes = plt.subplots(1, 2, figsize=(10,5))\n\n# using three cluster centers:\nkmeans_3 = KMeans(n_clusters=3, random_state=random_state).fit(b_loc)\n## mg.discrete_scatter(b_loc['lat'], b_loc['long'], kmeans_3.labels_, ax=axes[0])\n\n# using five cluster centers:\nkmeans_5 = KMeans(n_clusters=5, random_state=random_state).fit(b_loc)\n## mg.discrete_scatter(b_loc['lat'], b_loc['long'], kmeans_5.labels_, ax=axes[1])\n\n## plt.legend()\n## plt.show()","metadata":{"_cell_guid":"c585820c-6641-4204-b0cd-420eee1a3fe2","collapsed":true,"_uuid":"00e7ecd98494bafc0f37a9f4105f1fc8a8cfbbb6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the percentage of trips between stations. Most of the trips are between stations.\nsame_station = (trip.start_station_id != trip.end_station_id).nonzero()[0]\nprint(\"The percentage of trips between stations is {:.3%}.\".format(len(same_station)/len(trip)))","metadata":{"_cell_guid":"18a1b8ad-aeed-4521-8834-facf3f439e9e","_uuid":"c42b1786013cdc196cca607861bf98e3064e74fc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a function to quantify the reasonability of the KMeans clustering.\n# Trips between clusters should be minimal.\ndef cluster2cluster(kmeans_labels, n_cluster, color='g'):\n    # Group stations with the same cluster label.\n    id_groups = [station.loc[kmeans_labels == i, 'id'].values for i in range(0, n_cluster)]\n    \n    s2s = trip.loc[:, ['start_station_id', 'end_station_id']]    \n    # Create two columns to store the group ids for start and end stations.\n    s2s['start_group'] = None\n    s2s['end_group'] = None\n    \n    # Create new columns of group ids for the start station and the end station.\n    group_id = 0\n    for group in id_groups:\n        s2s.loc[s2s.start_station_id.isin(group), 'start_group'] = group_id\n        s2s.loc[s2s.end_station_id.isin(group), 'end_group'] = group_id\n        group_id += 1\n    \n    # g2g = s2s[s2s.start_group != s2s.end_group][['start_group', 'end_group']] # One cluster to another cluster.\n    g2g = s2s.loc[s2s.start_group != s2s.end_group, ['start_group', 'end_group']] # One cluster to another cluster.\n    g2g_size = g2g.groupby(['start_group', 'end_group']).size() # The number of trips between groups.\n    print(\"The percentage of trips between station clusters is {:.3%}.\".format(len(g2g)/len(trip)))\n    \n    # Draw a group of bar charts.\n    fig, axes = plt.subplots(1, n_cluster, figsize=(20, 5))    \n    for i in range(n_cluster):\n        x_ind = np.arange(g2g_size[i].size)\n        axes[i].bar(x_ind, g2g_size[i], color=color)\n        axes[i].set_xticks(x_ind)\n        axes[i].set_xticklabels(g2g_size[i].index)\n        axes[i].set_xlabel('Start station group %s' % i)\n    \n    plt.show()\n    # The index of the group-to-group is returned. Those data will be deleted.\n    return g2g.index","metadata":{"_cell_guid":"45cdc306-8564-4315-b8f2-1efe9d839c12","collapsed":true,"_uuid":"11a8a17a184cbe4e55d52700adf71dc6dedbdd93"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Investigate group-to-group trips with 3 groups.\ncluster3_index = cluster2cluster(kmeans_3.labels_, n_cluster=3, color='b')","metadata":{"_cell_guid":"bcb15406-d28a-472c-a97b-12839d9e9dfe","_uuid":"2d85be3c6e1b3bb3f2d321f048d2eed416fb736f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The number of trips between the three bigger clusters are negligible compared to the total number of trips. The trips between San Jose to San Francisco are even fewer. This makes a lot of sense as few people can ride bikes for such a long trip. ","metadata":{"_cell_guid":"3dacc255-acb0-4cca-8e24-84f873188248","_uuid":"136f71b201d77d71c3e1d56ece5a84fe8a2124a7"}},{"cell_type":"code","source":"# Investigate group-to-group trips with 5 groups.\ncluster5_index = cluster2cluster(kmeans_5.labels_, n_cluster=5, color='g')","metadata":{"_cell_guid":"84028003-db9c-406d-94da-d90180fea873","scrolled":true,"_uuid":"97512c93ce8e35375c64337ae117540f2123f83c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verify the group is assigned correctly. \nstation.loc[:, 'group'] = kmeans_5.labels_\nstation[['group','name']].head(3)","metadata":{"_cell_guid":"34f0e0db-343d-44d1-97cb-59763845423b","scrolled":true,"_uuid":"691ea681984286dcaec73846a839621723057d92"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are five unique zip codes from the weather data.\nweather.zip_code.unique()","metadata":{"_cell_guid":"591e1c88-c2e3-46bc-a816-daac5640842d","_uuid":"325a3e18b4eaff2d334bef79d5a2a69844da62e2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Arrange region zip codes from north to south\nregions = [94107, 94063, 94301, 94041, 95113]\nprint(regions)","metadata":{"_cell_guid":"dafead8f-3b42-4ad3-a2c6-e47ad258b7dd","_uuid":"876fac23db509f635c6c319f1309152c2e396a55"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The five zip codes correspond to:\n94107: San Francisco; 94063: Redwood City; 94301: Palo Alto; 94041: Mountain View; 95113: San Jose.\nThe weather condition has influence on people's decision whether to ride a bike or not, so it is very important. The five areas defined in the weather data match the station groups when the stations are clustered into five. It makes great sense to cluster the stations by five groups instead of three to better use the weather data and facilitate the machine learning technique.","metadata":{"_cell_guid":"98ae15fb-17f5-4eab-b0bd-f284ad442cd9","_uuid":"991a0433fc89659b59d3a44e9a164137f6ec544f"}},{"cell_type":"code","source":"# Group the station data by the group id and then sort the averaged longitude values in ascending order, which suggest from north to south.\nnorth_to_south_groups = station.groupby('group')['long'].mean().sort_values()\nnorth_to_south_idx = north_to_south_groups.index\nprint(north_to_south_groups)","metadata":{"_cell_guid":"051d3871-9a5b-4a52-9f1f-f31c3b271ee1","_uuid":"a9643bcb72b8a9a1cd7fef819a10821ba6181999"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a mapping from the group ids to zip codes.\nregion_dict = dict(zip(north_to_south_idx, regions))\nprint(region_dict)","metadata":{"_cell_guid":"54486985-7404-48a0-b76f-785e77847ff3","_uuid":"58c3b1021447b218c0ce76e82e9effa660140bd6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Associate the zip codes with the cities.\nregion_names = dict(zip(regions, ['San Francisco', 'Redwood City', 'Palo Alto', 'Mountain View', 'San Jose']))\nprint(region_names)","metadata":{"_cell_guid":"3f856876-c821-420b-afd1-1299bc95667a","_uuid":"5d896acff8e13a38d89359bbf3136e0f05617c1a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a new column of regions for trip data.\nid_groups = [station['id'][kmeans_5.labels_ == i].values for i in range(5)]\n\ntrip['station_region'] = None\nregion_stations_dict = {} # Initiate a dictionary from the region code to stations.\ngroup_id = 0\nfor group in id_groups:\n    region_code = region_dict[group_id]\n    region_idx = trip.start_station_id.isin(group)\n    \n    region_stations_dict[region_code] = group\n    trip.loc[region_idx, 'station_region'] = region_code\n    group_id += 1","metadata":{"_cell_guid":"212409d0-0b19-4e77-b6d6-85e762f9f172","collapsed":true,"_uuid":"1ce6a208d25ab29869994ad5625257b61d0f7365"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Investigate the percentage of inter-group trips among the groups. The group is assigned to the start station.\ninter_group_trips = trip.station_region.loc[cluster5_index].value_counts()\nregion_trips = trip.station_region.value_counts()\ninter_group_ratio = inter_group_trips / region_trips * 100","metadata":{"_cell_guid":"51fbc69e-db35-4fe6-9deb-52f4b1ff2304","collapsed":true,"_uuid":"4c94f44fa26753560e3702cd5727b81140d955b8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for region in region_trips.index:\n    print('The percentage of trips from {} to a different region is {:.2f}% out of total {} trips.'.format(region_names[region], inter_group_ratio[region], region_trips[region]))","metadata":{"_cell_guid":"71f097a8-a81a-4cb5-8f7e-4226631e467f","_uuid":"0effcaa92b5828ce8d3e3490680a8d9f0ff69b9a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Investigate how many inter-group trips are made by a subscriber or a non-subscriber\ntrip.subscription_type.loc[cluster5_index].value_counts()","metadata":{"_cell_guid":"33a89ce8-00d6-4082-8188-dcb005104eec","_uuid":"c2802f1ad073bed19ca471f0e9da967e12dc6bc9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The inter-group trips are not negligible for some regions, especially for the three stations that are geographically close to each other. So the inter-group trips will be deleted. A new column will not be created to indicate whether a trip is inter-group or not. Although there are about the same number of subscribers and non-subscribers for inter-group trips, labeling an inter-group trip can still be useful to find out different patterns for different subscription types when other features are taken into account.","metadata":{"_cell_guid":"777efd24-c136-43e7-b6c5-04f971f937ec","_uuid":"e2f658b3f72868cb9fe402add4efd4fa5931c5b7"}},{"cell_type":"code","source":"# Create a new column to label a trip as an inter-group trip.\ntrip['inter_group'] = 0\ntrip.loc[cluster5_index, 'inter_group'] = 1","metadata":{"_cell_guid":"ea3649af-168e-470a-a16f-b55718fee7d1","collapsed":true,"_uuid":"ae0e285ba8d76b6439d64b5c555dc36ae7f9685a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a new column in station to cluster the stations into groups\nstation['region'] = None\nfor region, group in region_stations_dict.items():\n    station.loc[station.id.isin(group), 'region'] = region","metadata":{"_cell_guid":"96c3136f-6ed1-455b-96c3-a37c9867c306","collapsed":true,"_uuid":"2211c9dcab8569e093115cddc776f8cf3e0337b1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Hourly trip counts for different subscription types.","metadata":{"_cell_guid":"1ba45afc-671c-4044-a89c-74f8d74c58f5","_uuid":"3bea8f1742fce0199827af5d93e04d2fec128e38"}},{"cell_type":"code","source":"# trip_date = trip['start_date'].str.split(' |/|:')\n# Separate the date and time. Only the start date is important. Anyway, the number of trips over 24 hours is rare.\n\ntrip_datetime = trip['start_date'].apply(lambda x: x.split())\ntrip_date = trip_datetime.apply(lambda x: datetime.strptime(x[0], \"%m/%d/%Y\").date())\ntrip_time = trip_datetime.apply(lambda x: x[1])\ntrip_hour = trip_time.apply(lambda x: int(x.split(':')[0]))\ntrip['hour'] = trip_hour\ntrip['date'] = trip_date","metadata":{"_cell_guid":"af9f1405-6f74-49a0-b712-9397af747099","scrolled":true,"collapsed":true,"_uuid":"1dc81c88ed8b1163c4d7f1a74aed1e05e60bbb90"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convert a string into datetime https://stackoverflow.com/questions/466345/converting-string-into-datetime","metadata":{"_cell_guid":"f2dbacfd-7cd1-47b7-80c4-e3abe0569cce","_uuid":"3bd80faf799b6342bc5f9b5ad416e32a6153c014"}},{"cell_type":"code","source":"# Alternative way to judge whether a day is a business day.\n# Codes borrowed from https://www.kaggle.com/currie32/a-model-to-predict-number-of-daily-trips\nfrom pandas.tseries.holiday import USFederalHolidayCalendar\nfrom pandas.tseries.offsets import CustomBusinessDay\n\ncalendar = USFederalHolidayCalendar()\nholidays = calendar.holidays(start=trip.date.min(), end=trip.date.max())\n\n#Find all of the business days in our time span\nus_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())\nbusiness_days = pd.DatetimeIndex(start=trip.date.min(), end=trip.date.max(), freq=us_bd)\nbusiness_days = pd.to_datetime(business_days, format='%Y/%m/%d').date","metadata":{"_cell_guid":"b6219691-3111-4cac-9ffd-d1034cf6a0ac","_uuid":"ac58aaec09e4340fcbd43be33a5efb302bd07d3b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def isbday(x, bday=business_days):\n    if x in bday:\n        return True\n    else:\n        return False","metadata":{"_cell_guid":"4a11ba7d-ee13-4781-a0a1-39600cd68852","collapsed":true,"_uuid":"0ed494094794c09c375dbc0efc26eda1dc6e8fcd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To expedite the process, the judgement of whether a day is a business day will be applied on unique days and then mapped to all rows in trip data.\ntrip_date_unique = pd.Series(trip_date.unique())\ntrip_BDay_unique = trip_date_unique.apply(lambda x: isbday(x))\n\ntrip_date_dict = dict(zip(trip_date_unique, trip_BDay_unique))\ntrip_BDay = trip_date.apply(lambda x: trip_date_dict[x])","metadata":{"_cell_guid":"019fa4d6-9f55-449d-8e04-93f45e6b4c5b","collapsed":true,"_uuid":"bf84d16053bae3fb089bdd539ff104c53d8f5946"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a new column in trip data for business day judgement.\ntrip['BDay'] = trip_BDay\nprint('There are {} business days in the record.'.format(len(trip_date[trip_BDay==True].unique())))\nprint('There are totally {} days in the record.'.format(len(trip_date.unique())))","metadata":{"_cell_guid":"606e1a32-b664-4999-bd28-4fe0cb22ce66","_uuid":"631e616cb1eebeec00627a109aafaee8fc1820a8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a function to visualize the number of trips per hour in order to find a reasonable way to divide the day.\ndef trip_hour_freq(trip_hour, trip_date, trip_day_idx, data_idx, ax, xlabel):\n    # Slice the trip_hour, count each value, sort the count series by index and then plot it.\n    trip_counts = trip_hour[trip_day_idx & data_idx].value_counts().sort_index() # Calculate the number of trips for each hour\n    freq_day = trip_counts/len(trip_date[trip_day_idx].unique()) # Calculate the frequency per day\n    ax.bar(freq_day.index, freq_day) # Plot the trip hours.\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel('Trip Frequency Per Day')\n    ax.set_xticks(freq_day.index) # Display all the xticks, which are 24 hours.","metadata":{"_cell_guid":"7ec218a4-b11e-4407-b50e-e23cb8dec046","collapsed":true,"_uuid":"cef39de4b28083d27c5231975f4557e4d6da652e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Counts for subscribers and non-subscribers\ntrip['subscription_type'].value_counts()","metadata":{"_cell_guid":"e112455f-786d-4fd1-a30e-9cd5150fc628","_uuid":"117f8505aac7aa20ba4c3953fe6586e7a1ccc57b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show bar charts for user behaviors on during business days and rest days.\nfig, axes = plt.subplots(3, 2, figsize=(20,10))\n\ntrip_hour_freq(trip_hour, trip_date, trip_BDay==True, trip.subscription_type=='Subscriber', axes[0][0], 'Subscriber on Business Day')\ntrip_hour_freq(trip_hour, trip_date, trip_BDay==False, trip.subscription_type=='Subscriber', axes[0][1], 'Subscriber on non-Business Day')\n\ntrip_hour_freq(trip_hour, trip_date, trip_BDay==True, trip.subscription_type=='Customer', axes[1][0], 'Customer on Business Day')\ntrip_hour_freq(trip_hour, trip_date, trip_BDay==False, trip.subscription_type=='Customer', axes[1][1], 'Customer on non-Business Day')\n\ntrip_hour_freq(trip_hour, trip_date, trip_BDay==True, True, axes[2][0], 'All on Business Day')\ntrip_hour_freq(trip_hour, trip_date, trip_BDay==False, True, axes[2][1], 'All on non-Business Day')\n\nplt.show()","metadata":{"_cell_guid":"bdf03b79-8e48-4c40-bd04-69ee6a51741a","_uuid":"75ea1722ea86ceb8719c93f0105dd97783c56fa0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In fact, there are not too many trips per day. The dominant usage is from subscribers for business days, i.e. for work. For work, there are two main peaks, one at 8 am and the other at 5 pm. The division point will be set to (8 + 17)/2 = 12.5. Subscribers tend to use the sharebike for commuting to work place, while non-subscribers tend to use that for lunches, etc. Interestingly, the behavior of non-subscribers during weekends and holidays is almost a perfect Gaussian shape peaking between 1-2 pm.","metadata":{"_cell_guid":"dd0b114b-2a1d-439e-ba34-9509ed0e08bd","_uuid":"261099dc855f2543d2caa39371fd138bd4f775eb"}},{"cell_type":"markdown","source":"### 2.3 Trip counts for different weekdays.","metadata":{"_cell_guid":"140898da-3d15-4d23-9e87-54cac9f96ba9","_uuid":"1b6243120415b5083a2bc0172204e9977d3a52ec"}},{"cell_type":"code","source":"# Create a new column to record weekdays\ntrip['weekday'] = pd.to_datetime(trip['date']).dt.weekday","metadata":{"_cell_guid":"78e452ab-3d49-4817-83ea-7dc0cd694580","collapsed":true,"_uuid":"232a149bbd55ce9ef9d86ec80caa3531a5904bd6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the trip counts for each weekday. 0 is Monday and 6 is Sunday.\ntrip.groupby('weekday')['id'].count().plot('bar')\nplt.show()","metadata":{"_cell_guid":"2e5b350b-a310-4312-8186-ec6a7f95c79e","_uuid":"6af662dd52f9fb5463a500f9a9328babb295e714"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4 Monthly trip counts for different description types to reveal the seasonal patterns.","metadata":{"_cell_guid":"7974e84f-df2f-44c9-90e7-821ce01892f6","_uuid":"ec00f605382deff54dd699ea3ebc25454859dfe7"}},{"cell_type":"code","source":"# Create a new column for month.\ntrip['month'] = trip_datetime.apply(lambda x: int(x[0].split('/')[0]))","metadata":{"_cell_guid":"01a25b60-1fdb-4f31-a2f1-50ff4cf52754","collapsed":true,"_uuid":"55a5db2654ef3a6d19d04e632bf49d124db6f44a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group the trip first by subscription type and then by month.\ntrip_monthly = trip.groupby(['subscription_type', 'month'])['id'].count()","metadata":{"_cell_guid":"b1181117-3ab8-4185-9c6a-99c7fcc3e7a7","scrolled":true,"collapsed":true,"_uuid":"7de73151c368e8c9976c8390c2bd95aa54265cdf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View the number of trips on each month.\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\naxes[0].bar(trip_monthly['Subscriber'].index, trip_monthly['Subscriber'])\naxes[0].set_xlabel('Month (Subscriber)')\naxes[1].bar(trip_monthly['Customer'].index, trip_monthly['Customer'])\naxes[1].set_xlabel('Month (Customer)')\nplt.show()","metadata":{"_cell_guid":"2dcaa3bb-e0b5-46ba-b342-01598fead5dc","scrolled":true,"_uuid":"b7fdaf07a5c713635072310d9444134d738739b5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly, there is no significant variation on the number of monthly trips for subscribers except the holiday season (December); for non-subscribers we observe a clear trend of increase from the beginning of the year peaking at September and then decrease: this suggests that summer is generally a good time to bike in Bay Area for fun and September is the best season for tourists to visit Bay Area?","metadata":{"_cell_guid":"2d187802-7482-431d-bcfb-c50441e2ddea","_uuid":"41538b79534bccddbacd06f09b8f2c99a15ae680"}},{"cell_type":"markdown","source":"### 2.5 Hourly trip counts for each station group.","metadata":{"_cell_guid":"3ee3f996-4ded-4fca-9489-87857c29eb55","_uuid":"04aef464af41cb36d34b1c933bf8e83c4ff3d008"}},{"cell_type":"code","source":"# Group the trips first by region and then by hour.\ntrip_groupby_station = trip.groupby(['station_region', 'hour'])['id'].count()","metadata":{"_cell_guid":"1e8c2408-c663-44ea-8640-61eb977417fc","collapsed":true,"_uuid":"f1b5f8d87fb9fe041a5e4077c61b1941ec44fcb1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the hour out as a feature column. Keep the station region as the index.\ntrip_station_region = trip_groupby_station.reset_index(level=1)","metadata":{"_cell_guid":"aaf92a36-03f5-4a4b-8da5-aa11830433d0","collapsed":true,"_uuid":"282f0e80dda9568f686e9f6f99982cac139cab89"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The station groups in the trip data match those in the weather data.\nprint(trip_groupby_station.index.levels[0])\nprint(regions)","metadata":{"_cell_guid":"6578d7dd-c835-4a13-b395-98e12b1c52c8","_uuid":"a633ecb5d3b0e68fb5510c190d8375d09263fbb3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trip.groupby(['station_region'])['id'].count()","metadata":{"_cell_guid":"aa0f716d-f6d8-4e12-8cc3-4145b45f8c51","scrolled":true,"_uuid":"9293cd2f5c2698003df07c7f8bc528fade820ffc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the trips per hour for each city.\nfig, axes = plt.subplots(1, 5, figsize=(20,5))\ni = 0\nfor region in regions:\n    region_counts = trip_station_region.loc[region]\n    axes[i].bar(region_counts.hour, region_counts.id, color='b')\n    axes[i].set_xlabel(region_names[region])\n    i += 1\nplt.show()","metadata":{"_cell_guid":"c9cecade-d8a8-49cd-a948-69967b279306","_uuid":"9ed3136cdb5bb63e32871a39e705f69138970f93"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from the above figure, the number of trips in San Francisco is way larger than the other regions. Even the city ranking the second only has a fraction number of trips of it. This is going to skew the data analysis for trips in cities other than San Francisco, so it is necessary to divide the trips into different groups and treat them separately.","metadata":{"_cell_guid":"af2c02a1-335b-4d8a-9f77-2c31ea46faae","_uuid":"02e717a2a6fb468aed6b0b2339966fbab82d073c"}},{"cell_type":"markdown","source":"### 2.6 Bike durations exploration.","metadata":{"_cell_guid":"c3b56d8e-d220-4c0f-bb12-ee7ff877c265","_uuid":"e0cbb57bec070fa6342a95d649c6963ba2c172bf"}},{"cell_type":"code","source":"# Get the mean trip duration for each region in minute.\nbike_durations = trip.groupby('station_region')['duration'].mean()/60\nbike_durations_median = trip.groupby('station_region')['duration'].median()/60\ntrip_region_counts = trip.groupby('station_region')['id'].count()","metadata":{"_cell_guid":"7cf81b7f-ac2f-4424-8bf2-4186f4c044dc","collapsed":true,"_uuid":"55e5d48a4698d381301215b80ce73125ccf197dc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for region, duration in bike_durations.iteritems():\n    print('There are {0} trips in {1}, lasting {2:.1f} minutes in average.'.format(trip_region_counts[region], region_names[region], duration))","metadata":{"_cell_guid":"c2667117-b7bb-4bb5-9ab9-9be42359d21e","_uuid":"add0d467fb8ee3342c906592b6d18a358976416d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for region, duration in bike_durations_median.iteritems():\n    print('There are {0} trips in {1}, lasting {2:.1f} minutes in average.'.format(trip_region_counts[region], region_names[region], duration))","metadata":{"_cell_guid":"c5e4436f-2376-4985-b6a7-bb889f7d8cfd","_uuid":"682d4c8585d2fb4b4007f3569e45265fbc78ea19"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bike_durations.index = bike_durations.index.to_series().map(region_names)\nbike_durations_median.index = bike_durations.index","metadata":{"_cell_guid":"b3a70079-8938-45ae-9fdc-ce6dd8363bf9","collapsed":true,"_uuid":"d782a80099e2db6e0b8b4c26e04ec1b406caad33"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\n\nplt.subplot(121)\nbike_durations.plot.bar()\nplt.ylabel('Average Trip Duration (min)')\n\nplt.subplot(122)\nbike_durations_median.plot.bar()\nplt.ylabel('Median Trip Duration (min)')\nplt.show()","metadata":{"_cell_guid":"5de524ba-cc56-4df0-a549-2b1c5bd459ff","_uuid":"43dd8854e0bbcc25c978889226782766b67d3849"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trip_duration = trip.duration.sort_values()","metadata":{"_cell_guid":"25d77c63-7476-4fca-a352-e0cd3b86609f","collapsed":true,"_uuid":"60262d448744879aaa31d29d215ae8021d4f7235"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"div = 10 # Set the number of pieces to divide the trip_duration\ntrip_duration_idx = np.linspace(0, len(trip_duration), num=div, endpoint=False, dtype='int')\ndivision_points = trip_duration.iloc[trip_duration_idx[1:]]/60\nprint(\"All trips can be divided into {} pieces with minute division points at:\".format(div))\nprint(['%.1f' % elem for elem in division_points])","metadata":{"_cell_guid":"f1922683-770f-4487-be76-c791f55b449f","scrolled":true,"_uuid":"cbfb2c25f84c3c7f02231a226fdeede2c59b832f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see above, most of the trips are pretty short. More than 90% of the trips are shorter than 20 minutes.","metadata":{"_cell_guid":"89fc92f9-a767-4ea0-8ea5-311d70d33418","_uuid":"35a4e7652e19bcb02372b489f477d32cebe38082"}},{"cell_type":"code","source":"trip_duration_subscriber = trip_duration[trip.subscription_type=='Subscriber'].mean()/60\ntrip_duration_customer = trip_duration[trip.subscription_type=='Customer'].mean()/60\n\nprint('The average trip duration for a subscriber is %.1f minutes.' % trip_duration_subscriber)\nprint('The average trip duration for a non-subscriber is %.1f minutes.' % trip_duration_customer)","metadata":{"_cell_guid":"a4484cbc-3d90-4a55-8691-8832f045f2ef","_uuid":"53f8b89de4672915bc86721e9e40b49aead67a8b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A non-subscriber tends to have a much longer trip than a subscriber. This is reasonable as most subscribers probably use sharebikes for work commuting while non-subscribers use it for leisure purposes.","metadata":{"_cell_guid":"e7a6fb21-c325-4230-888d-838bf4eac2bf","_uuid":"32f3df1cf78acc28e085fc2edc735fc405c96a0a"}},{"cell_type":"markdown","source":"### 2.7  Bike usage exploration.","metadata":{"_cell_guid":"dd8e46dd-a375-41e1-bb83-6ec298df71a2","_uuid":"f3f73adca59956171c35150b1661d383e8d1dd6e"}},{"cell_type":"code","source":"# Calculate the bike usage based on the bike id.\nbike_usage = trip.bike_id.value_counts()","metadata":{"_cell_guid":"16eb2f07-0a72-4033-834b-7a7857765cf6","scrolled":true,"collapsed":true,"_uuid":"aa650f73825ad94afd7cd8bd72ffa6aa1362c6e8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# It would be interesting to figure out where the most used bikes come from and the potential lifetime of the bike\nplt.figure(figsize=(15,6))\nplt.plot(bike_usage.index, bike_usage, '.')\nplt.ylabel(\"Number of Trips\")\nplt.xlabel(\"Bike ID\")\nplt.title(\"Bike Usage View\")\nplt.show()","metadata":{"_cell_guid":"c8c42887-f5a8-45da-b219-a27e4af6eb4e","scrolled":false,"_uuid":"dac7a2fa477d0c474eb36fee3ee2b5830379e1f5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the bike ids \nmost_used_bikes = bike_usage[bike_usage > 1500].index\nleast_used_bikes = bike_usage[bike_usage < 500].index","metadata":{"_cell_guid":"67a6652d-2b10-4335-948d-d636c242bc07","collapsed":true,"_uuid":"cb2a0a720f9fc5128c9c6f3ccc9457796397a46d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the bike trips\nmost_used_bikes_trips = trip[trip.bike_id.isin(most_used_bikes)]\nleast_used_bikes_trips = trip[trip.bike_id.isin(least_used_bikes)]","metadata":{"_cell_guid":"27abf100-13f8-4616-a2bb-c9b02e4196d3","collapsed":true,"_uuid":"03d6d55403e812f4ddae7fdfeb80e6238357b046"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group by the station regions\nmost_used_bikes_group = most_used_bikes_trips.groupby('station_region')['bike_id'].count()\nleast_used_bikes_group = least_used_bikes_trips.groupby('station_region')['bike_id'].count()","metadata":{"_cell_guid":"4748b414-2ff6-409e-a881-73d5ab245834","collapsed":true,"_uuid":"4b2e2dba96edd85e0750a7498fd6898fe3bfdd25"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change the index from zip code to city name\nmost_used_bikes_group.index = most_used_bikes_group.index.to_series().map(region_names)\nleast_used_bikes_group.index = least_used_bikes_group.index.to_series().map(region_names)","metadata":{"_cell_guid":"e7cf8b25-2627-4cd3-bdd0-696dfc670a25","collapsed":true,"_uuid":"99056471071edf032c05695ef9c8e922d6bb46fc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 5))\n\nplt.subplot(121)\nmost_used_bikes_group.plot.bar(logy=True)\nplt.ylabel('Logarithm of Trip Counts')\nplt.title('Trips with Most Used Bikes')\n\nplt.subplot(122)\nleast_used_bikes_group.plot.bar()\nplt.ylabel('Trip Counts')\nplt.title('Trip Counts with Least Used Bikes')\nplt.show()","metadata":{"_cell_guid":"0183f940-7f3c-4ff9-8324-0e28bc935a12","_uuid":"664dc0a28fb81a4f3d910b123005b59cdcf4b823"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"station.head()","metadata":{"_cell_guid":"17ab2ee4-8ed4-4b43-a4bc-bb66a0338e56","_uuid":"5c1153f4e7760a3f5a862e31032d2633e8addf8c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the number of docks in each region\nregion_dock_counts = station.groupby('region')['dock_count'].sum()\nprint(region_dock_counts)","metadata":{"_cell_guid":"0162eacc-ad4d-4ffc-b4f1-441ad86dcff5","_uuid":"990655668ad94a14c352ee6c6a9e7cc0ed535585"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"region_dock_counts.index = region_dock_counts.index.to_series().map(region_names)","metadata":{"_cell_guid":"8a00ff4a-9ef1-4c9d-85d8-93562231ee92","collapsed":true,"_uuid":"c0025a92390d25ec7bb3732a7b0c61be85da23dc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"least_used_bikes_group / region_dock_counts","metadata":{"_cell_guid":"aa596efd-b90e-4235-b870-ba68a3359e59","_uuid":"315a9bd183ce9775c47192bfc1bf4dcd78ed967c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.8 Local zip code investigation.","metadata":{"_cell_guid":"e40d803d-45bb-4ce2-8463-380b9ce4412e","_uuid":"5f73a03791e32e16e13deb77c28fe108d258f8e9"}},{"cell_type":"code","source":"# Fill NaN with an empty string.\ntrip.zip_code.fillna('', inplace=True)","metadata":{"_cell_guid":"bcf7aba6-4e5c-4846-9bab-599910af1855","collapsed":true,"_uuid":"9cde461a3c85a4439208102aa88240047390686e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display bay area with 3-digit zip codes.\n## display(Image(filename='Zipcode_Map.png'))","metadata":{"_cell_guid":"bd63133a-39b3-4a7c-b932-6011823675e4","collapsed":true,"_uuid":"158d74276711522c74863e433ad777244313cf1a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sectional Center Facility: \n# Obtain zip codes (first three digits) for bay area from online source: http://maps.huge.info/zip3.htm\nzip3 = ['948', '945', '947', '946', '941', '944', '940', '943', '950']","metadata":{"_cell_guid":"b08744db-d459-4fe3-b08a-09b3ffa2d986","collapsed":true,"_uuid":"f822bb2747af02f9a428b4d2868e982f1bc6ca2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a new column with True and False values indicating whether the rider is from Bay Area or not (roughly).\ntrip['local_zip'] = trip.zip_code.apply(lambda x: x[0:3] in zip3 and len(x)>=5)","metadata":{"_cell_guid":"2e256157-c19b-4c64-a9e7-7c85d4a21f54","collapsed":true,"_uuid":"68f30c1395ec1fb9394341e381c5aeefc2bd4473"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the ratio of the local zip number and the total zip number\nlocal_percentage = trip.groupby('subscription_type')['local_zip'].sum() / trip.subscription_type.value_counts() * 100\nprint('{:.2f}% of the subscribers are local people.'.format(local_percentage[1]))\nprint('{:.2f}% of the non-subscribers are local people.'.format(local_percentage[0]))","metadata":{"_cell_guid":"c6c6098e-41c0-42ae-b7a7-edea55f6c91d","_uuid":"fefb54522bed06bc5262d14f187f619cb4acda2d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is as expected. Most subscribers are local residents, while most of the non-subscribers are tourists.","metadata":{"_cell_guid":"954724af-3a5b-4515-a4d8-71bbea9bb12f","_uuid":"2401cbb3ef6fa8856cbb7ccb9818b3898cbf175f"}},{"cell_type":"markdown","source":"### 2.8 Trip trend over time","metadata":{"_cell_guid":"e80c2fb6-7506-4311-b93e-ce9c8aefcdc1","_uuid":"4c3c4cc3e23617123ccece0d9336740bd9460df6"}},{"cell_type":"code","source":"# Obtain the daily trips groupby the business day column\ndaily_trips = trip.groupby('date').agg({'BDay': ['median', 'count']})","metadata":{"_cell_guid":"eeaaa3b8-4e40-408e-b1c0-fcff1f8ceaa4","collapsed":true,"_uuid":"0003ffb23998349431a4c550bfcea80cc58d8814"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separate the business and non-business day trips\nBDay_trips = daily_trips[daily_trips['BDay']['median'] == True].reset_index()\nnonBDay_trips = daily_trips[daily_trips['BDay']['median'] == False].reset_index()","metadata":{"_cell_guid":"bb3922d8-df33-45f5-bb85-723dd76df247","collapsed":true,"_uuid":"3a49422a64b44fc18c6a2c5591b09f83e0df305c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the trip counts per day \nfig, ax = plt.subplots(figsize=(20, 8))\nplt.plot(BDay_trips.date, BDay_trips['BDay']['count'], 'bo', label='Business Day')\nplt.plot(nonBDay_trips.date, nonBDay_trips['BDay']['count'], 'rx', label='Non-business Day')\n\n# Set the x axis so that every month will be displayed\nmonths = mdates.MonthLocator()\nyear_month_Fmt = mdates.DateFormatter('%y/%m')\nax.xaxis.set_major_locator(months)\nax.xaxis.set_major_formatter(year_month_Fmt)\n\nplt.legend()\nplt.ylabel('Trip Counts')\nax.grid(True)\n\n\nplt.show()","metadata":{"_cell_guid":"2022cf40-80bc-4283-be8b-a59f620d8246","_uuid":"f9abb3d585a54fd96ab6fa25d1d03e787bc6f8b6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find out the outliers of the non-business day trips. Four are found on the above graph.\nnonBDay_outliers = nonBDay_trips.loc[nonBDay_trips['BDay'].sort_values('count').iloc[-4:].index]\nprint(nonBDay_outliers)","metadata":{"_cell_guid":"70adde3c-f744-445d-b5bd-1a57aa749ad7","_uuid":"41ef2b1b56c28bc449684c3e4829a8810ace6a76"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nonBDay_outliers.date","metadata":{"_cell_guid":"bd6b8a4f-0cc0-4460-82f0-93e7041d95f5","_uuid":"24e12ea05df07c2b08ded10cbaab75a84b630bd2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://pypi.python.org/pypi/holidays\n# Apparently some companies don't have Veterans Day and Columbus Day off, and this seems to be a common situation.\n## us_holidays = holidays.US()\n## [us_holidays.get_list(hol) for hol in nonBDay_outliers.date.tolist()]","metadata":{"_cell_guid":"fa570c3e-c239-420b-af7f-b75bd8d92ba3","collapsed":true,"_uuid":"e1514d8bb2fbbe5ea843c9d0c3b97e52ddf28ed0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exclude these two holidays from the non-business days.\ntrip.loc[trip.date.isin(nonBDay_outliers.date), 'BDay'] = True","metadata":{"_cell_guid":"f7bc6d25-fdb9-4f1b-b98d-44689189b2bd","collapsed":true,"_uuid":"9f102d4634404daed021dfa57145fc30e07e1c38"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data before 2013/10 should be treated differently\nbefore_Oct2013 = np.count_nonzero(trip.date < datetime(2013, 10, 1).date())\nprint(\"Percentage of trips happening before 2013/10 is {:.2%}\".format(before_Oct2013/len(trip)))","metadata":{"_cell_guid":"4260322f-c4f6-47fc-97a4-8286cca916b7","_uuid":"7cc1eb4a950cc62470298e3f328d54336326d545"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clearly the patterns before and after october are different. In fact, I can just delete the data, but it is also fine to just label it.\ntrip['before_Oct2013'] = trip.date < datetime(2013, 10, 1).date()","metadata":{"_cell_guid":"d2bab2f4-915e-4c57-af8b-7ee299a773ee","collapsed":true,"_uuid":"b585084b29f3b61ad752bc75a464091770c1d49b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is also noticeable that the business day trips are much less during December. December is considered the holiday season in the US. This means that the month can be an important feature.","metadata":{"_cell_guid":"36d63f43-3ee1-485f-914c-94848cb5bffe","_uuid":"b6defd1fddc3338e7c5a44d2b0d8fb1b8e63293f"}},{"cell_type":"markdown","source":"### 2.9 Check if there is nan value in trip data","metadata":{"_cell_guid":"7143c3d6-4f63-42fe-9a32-f02c713a5795","_uuid":"44d589c44565c9475ad062a300f3062b25e94f2b"}},{"cell_type":"code","source":"# There are no nan values in the trip data.\ntrip.isnull().sum()","metadata":{"_cell_guid":"7ba90a61-7dcb-4dfe-ad38-bc4694c2bbd5","_uuid":"062761342b051a6f73954e1bb00e627c950a9169"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Exploration on 'status' data.","metadata":{"_cell_guid":"61b62649-c5b0-4a44-b91f-30b3b488d399","_uuid":"bc3f467316b803d10bd05e674e05dd6de2635cc6"}},{"cell_type":"markdown","source":"### 3.1 Total docks.","metadata":{"_cell_guid":"63edebf5-9837-42f8-9974-aaa7e7e0d7df","_uuid":"f7749f93a866bcd1254bc8a79ee78f9e4e1833b9"}},{"cell_type":"code","source":"# Create a dictionary of station to its originally designed dock counts.\n# Create two more columns for the status data: the originally designed dock counts and the real number of docks.\nstation_docks = dict(zip(station.id, station.dock_count))\nstatus['dock_count'] = status.station_id.apply(lambda x: station_docks[x])\nstatus['total'] = status['bikes_available'] + status['docks_available']","metadata":{"_cell_guid":"0bbce109-ad3a-46c0-b069-44660687535d","collapsed":true,"_uuid":"9692dd4993a83ab8373bc8aa8813b445180bc345"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1.1 More docks.","metadata":{"_cell_guid":"14488067-4b3c-43ee-ae43-06967f5160a6","_uuid":"4edeb475662c5b16295f81518db47759ae6fd59c"}},{"cell_type":"code","source":"# Check situations when the sum of bikes and docks is larger than the originally designed number of docks.\nmore_docks = status.loc[status.total - status.dock_count > 0, :]\nmore_docks.groupby(['station_id', 'total']).count()","metadata":{"_cell_guid":"1dbdb20e-7c8b-4720-953d-5e4ca8f9f0e4","scrolled":true,"_uuid":"ef889ff1ba2187caa26080d29c5a266eb957e5e8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the above tables shows, only two stations have the total number of docks larger than the originally designed dock counts. Close to 10000 entries in the status data has dock count to be 27, which is pretty suspicious. Below we will explore further whether the total number of docks indeed changes or just system malfunction.","metadata":{"_cell_guid":"d2e37ff5-d2f8-48fe-8ab4-0bd0e5370c7d","_uuid":"196ea326a6e8496554f84d78586e8d5ce307bea4"}},{"cell_type":"code","source":"# Show both the head and the tail of station 22. \n# It shows that the number of docks for station 22 is actually 27 in the beginning and the end. The number of dock count for station 22 needs to be updated.\nstatus[status.station_id == 22].iloc[np.r_[0:5, -5:0]]","metadata":{"_cell_guid":"5bb32068-f074-4bed-bf79-3141246fa5e2","_uuid":"72f0bc56e87fcdcb405dff21c034595bdd96c3b9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show both the head and the tail of station 39.\n# It shows that the number of docks for station 39 is 19 in the beginning and the end. No change. The larger count could be just a system error.\nstatus[status.station_id == 39].iloc[np.r_[0:5, -5:0]]","metadata":{"_cell_guid":"66fe2efc-dc6b-4126-83f1-6d72bad56e29","_uuid":"cf74c02f830546eddd1ed139814b2d11d8646dc3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update the station 22 dock count to 27\nstation_22_index = (station.id == 22).nonzero()[0][0]\nstation.loc[station_22_index, 'dock_count'] = 27\nstation_docks = dict(zip(station.id, station.dock_count))\nstatus['dock_count'] = status.station_id.apply(lambda x: station_docks[x])","metadata":{"_cell_guid":"9bbbab36-8ca2-45bd-9b70-d7d5e7468fc0","collapsed":true,"_uuid":"c44dd749df5764855bf6229f9eaeb355e4dc60b2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check situations when the sum of bikes and docks is larger than the originally designed number of docks.\nmore_docks = status[status.total - status.dock_count > 0]\nmore_docks.groupby(['station_id', 'total']).count()","metadata":{"_cell_guid":"ce4c0a19-caae-4db4-b459-5f0d346a314f","_uuid":"3b1b40ab7eb86da6f8fc06f65eb5888c1a6b9b20"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1.2 Less docks.","metadata":{"_cell_guid":"b944fba4-846a-4b55-b6a8-c584eb61583b","_uuid":"3e021cb661620099f0381ef19163ab07034bf8dc"}},{"cell_type":"markdown","source":"Find those stations when the total dock counts keep changing. This may have something to do with the vandalism:\nhttps://www.theguardian.com/us-news/2017/aug/21/bike-sharing-scheme-san-francisco-gentrification-vandalism","metadata":{"_cell_guid":"6963e0d2-317f-48ca-8a33-91d0aa7b8cf5","_uuid":"cbe674afc0884747a5891ebb804355b6aa091e6b"}},{"cell_type":"code","source":"# Count the number of status changes when the sum of available bikes and docks is smaller than the actual dock count, which shouldn't change in the two years' operation.\nless_docks = status.loc[status.total - status.dock_count < 0, :]\nless_docks_counts = less_docks.groupby(['station_id'])['total'].count()\nless_docks_station_id = less_docks_counts.index","metadata":{"_cell_guid":"1e89d469-302d-4cbc-ab23-4ef7147b92e2","collapsed":true,"_uuid":"519aace82e9a286d64b1b05345acb502840d4bab"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 5))\nfor region, color in zip(region_stations_dict, ['b', 'r', 'k', 'g', 'y']):\n    station_docks_count = less_docks_counts.loc[region_stations_dict[region]]\n    ax.bar(station_docks_count.index, station_docks_count, color=color, label=region_names[region])\n\nplt.xlabel('Station ID')\nplt.ylabel('Status Changes')\nplt.title('Counts of status changes on suspected dock malfunction')\nplt.xticks(range(0, max(less_docks_station_id)+1, 2))\nplt.legend()\nplt.show()","metadata":{"_cell_guid":"cd19a952-14ac-40a6-9d92-52acc988e30a","_uuid":"f87621cd1d3831d31475d5c22ccb047273579695"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Demonstrate how to obtain the hour of the status data.\nprint(less_docks.time.iloc[1])\nprint(int(less_docks.time.iloc[1].split()[1].split(':')[0]))","metadata":{"_cell_guid":"c5ac3035-0277-4eea-b0e8-4cf905b39ebb","_uuid":"c1946e09df213f847d6fc89670a9cea0edeb54bc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"less_docks['hour'] = less_docks.time.apply(lambda x: int(x.split()[1].split(':')[0]))","metadata":{"_cell_guid":"9bc64d7a-0993-4766-803c-cf8586946683","_uuid":"3178dd154cbb699a80abc6850acd33d31dfb0136"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"less_docks_counts_hour = less_docks.groupby('hour')['time'].count()\n\nplt.figure(figsize=(10, 5))\nless_docks_counts_hour.plot.bar(color='blue')\nplt.ylabel('Counts of dock counts discrepancy')\nplt.show()","metadata":{"_cell_guid":"9196f41d-b8dc-46cb-acf7-2e11c4de5490","_uuid":"cef3cdfacef1ef0b447b672b0a9a5b5c59864e0d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Although the dock malfunction may indeed happen possible due to vandalism, it is also possible that the dock status may not be well reflected when frequent change of status happends in less than one minute.","metadata":{"_cell_guid":"6730dc8b-2110-4adf-af27-b4eddfae7a60","_uuid":"d9b11669d0c8d9c9cbadddf0bd99cf1c87109c24"}},{"cell_type":"markdown","source":"#### 3.1.3 Total dock changes","metadata":{"_cell_guid":"a4b54ae8-bc31-47e5-8c95-f58a6fb63307","_uuid":"674e422a3699b1cfdd7a6eb3f559ca471575d51b"}},{"cell_type":"code","source":"status['hour'] = status.time.apply(lambda x: int(x.split()[1].split(':')[0]))","metadata":{"_cell_guid":"1fcee46c-2a14-4b49-8fd8-9220bcf733d3","collapsed":true,"_uuid":"926920d6319aaf3b2efc6603b3bbaede12aa7702"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nstatus['datetime'] = pd.to_datetime(status.time)\nstatus['date'] = status.datetime.apply(lambda x: x.date())\nstatus['weekday'] = status.datetime.dt.weekday","metadata":{"_cell_guid":"f82c6f55-d1cb-41a3-a074-d33914f4bc0b","_uuid":"afd4fa051da9867f7e1bbdf9cf83fcf5d94297c3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the total dock count change events per station\nstation_status_list = []\nfor station_id in status.station_id.unique():\n    station_status = status[status.station_id == station_id]\n    station_status_list.append(station_status[station_status.total.diff() != 0])","metadata":{"_cell_guid":"4825af34-ee48-4dd7-b147-1d78deb1b0f5","collapsed":true,"_uuid":"a6173af8ddddd0d60f090246a53f3f89bc791e45"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate the list to form the full dataframe\ndock_changes = pd.concat(station_status_list)\nprint(dock_changes.shape)\nprint('The total dock count change events occurs in {:.2f}% of the total status changes.'.format(len(dock_changes) / len(status) * 100))","metadata":{"_cell_guid":"667f5f16-96da-4a97-8e09-1145205b7c36","_uuid":"9b32ea5dd1932b8ebcdcad39f11a324d6f81c059"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\ndock_changes.groupby('hour')['total'].count().plot.bar(color='green')\nplt.ylabel('Counts of total dock count change events')\nplt.show()","metadata":{"_cell_guid":"66f9b702-512a-4fc1-8dd2-c2ecc10a7bfe","_uuid":"d230d5875b9242cc7fd0244d6f07b8a4843fd1a0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2 Extra trips in status data","metadata":{"_cell_guid":"c1c1b3a1-f331-4754-85a5-379caa2d5a1b","_uuid":"995efd41daf887010357fbf3eba1c2acde68d924"}},{"cell_type":"code","source":"# Get the number of station status changes from the trip data. Start suggests the decrease of available bikes. End suggests the increase.\ntrip_status_start = trip.loc[:, ['start_date', 'weekday']]\ntrip_status_end = trip.loc[:, ['end_date', 'weekday']]\n\ntrip_status_start['start_hour'] = trip_status_start.loc[:, 'start_date'].apply(lambda x: int(x.split()[1].split(':')[0]))\ntrip_status_end['end_hour'] = trip_status_end.loc[:, 'end_date'].apply(lambda x: int(x.split()[1].split(':')[0]))","metadata":{"_cell_guid":"2a97a071-b7b1-42a5-a37d-1f3824246a91","collapsed":true,"_uuid":"23352f24f7e9732eee36f5fcf57427c8d850290c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the event counts per hour for trip data\ntrip_status_start_count = trip_status_start.groupby(['weekday', 'start_hour'])['start_hour'].count()\ntrip_status_end_count = trip_status_end.groupby(['weekday', 'end_hour'])['end_hour'].count()","metadata":{"_cell_guid":"2cbd9240-03ba-468f-9528-196f4cd5769e","collapsed":true,"_uuid":"17e77c2eee77f6e32bf19f817fe54a56c4a2a7ed"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the change of number of the available bikes\nstatus['bike_diff'] = status.bikes_available.diff()","metadata":{"_cell_guid":"ac31f702-ad31-46b5-af8a-bc4c55ea2f34","collapsed":true,"_uuid":"3dcea07f4c96fdf87b893ff773bddba2d871ff50"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the bike count increase and decrease in status data\nbike_incr = status.loc[status.bike_diff > 0, :]\nbike_decr = status.loc[status.bike_diff < 0, :]","metadata":{"_cell_guid":"8c4138bc-e958-43a6-992f-1eedcbabb763","collapsed":true,"_uuid":"fe3275f42d05b08d0968e40719a32d072334f43a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the event counts per hour for status data\nbike_incr_count = bike_incr.groupby(['weekday', 'hour'])['bike_diff'].sum()\nbike_decr_count = - bike_decr.groupby(['weekday', 'hour'])['bike_diff'].sum() # All bike_diff values are negative. Need to be negated.","metadata":{"_cell_guid":"f4e308dd-cf25-48ab-bdb6-f687d31d7923","collapsed":true,"_uuid":"93173712eea7617de4ae3cfab3c4fa0cd2c16a07"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the extra trips in status data than in trip data\nbike_incr_diff = bike_incr_count - trip_status_end_count\nbike_decr_diff = bike_decr_count - trip_status_start_count","metadata":{"_cell_guid":"78715907-66d9-4eac-bc4a-d2e6338ab811","collapsed":true,"_uuid":"5331666cff33a224bd868f602342e7a192f979c4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bike_incr_diff_reset = bike_incr_diff.reset_index()\nbike_decr_diff_reset = bike_decr_diff.reset_index()\n\nbike_incr_diff_reset.columns = ['weekday', 'hour', 'counts']\nbike_decr_diff_reset.columns = ['weekday', 'hour', 'counts']","metadata":{"_cell_guid":"4dc0d689-ce99-4039-94eb-27f5944b4fb0","collapsed":true,"_uuid":"59a8d38d08e550f34ce172c681e84f50eab75927"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"number_of_weeks = len(trip.date.unique()) / 7\nbike_incr_diff_reset.counts = bike_incr_diff_reset.counts / number_of_weeks # Calculate the averaged daily extra trips\nbike_decr_diff_reset.counts = bike_decr_diff_reset.counts / number_of_weeks # Calculate the averaged daily extra trips\n\n# Convert into a 2D table.\nbike_incr_diff_pivot = bike_incr_diff_reset.pivot(index='hour', columns='weekday', values='counts')\nbike_decr_diff_pivot = bike_decr_diff_reset.pivot(index='hour', columns='weekday', values='counts')","metadata":{"_cell_guid":"1c3f5b8d-01b5-49f0-9cf8-66dd98edd392","collapsed":true,"_uuid":"f9cf8b148c81beb9ebdf5fb315cb5775fdb1eb71"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate the contour plot.\nX = bike_incr_diff_pivot.columns.values\nY = bike_incr_diff_pivot.index.values\nZ_incr = bike_incr_diff_pivot.values\nZ_decr = bike_decr_diff_pivot.values\n\nXi,Yi = np.meshgrid(X, Y)\nplt.figure(figsize=(15,10))\n\nplt.subplot(211)\nplt.xlabel('Hour')\nplt.ylabel('Weekday')\nplt.title('Extra trips when the number of bikes increases')\nplt.xticks(range(0, 24, 1))\nplt.yticks(range(0, 7, 1))\nplt.contourf(Yi, Xi, Z_incr, 300, cmap=plt.cm.jet)\nplt.colorbar()\n\nplt.subplot(212)\nplt.xlabel('Hour')\nplt.ylabel('Weekday')\nplt.title('Extra trips when the number of bikes decreases')\nplt.xticks(range(0, 24, 1))\nplt.yticks(range(0, 7, 1))\nplt.contourf(Yi, Xi, Z_decr, 300, cmap=plt.cm.jet)\nplt.colorbar()\n\nplt.show()","metadata":{"_cell_guid":"69ead41e-c99a-4caf-ba84-02bdef17d0a8","_uuid":"64bfee37ec989292a25f510148806ba4d118ab62"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('There are {:.1f}% more trips with bike count increase in status data.'.format((bike_incr.bike_diff.sum() - len(trip_status_end)) / len(trip_status_end) * 100))\nprint('There are {:.1f}% more trips with bike count decrease in status data.'.format((- bike_decr.bike_diff.sum() - len(trip_status_start)) / len(trip_status_start) * 100))","metadata":{"_cell_guid":"fd5b5821-1ce3-4e06-9e48-c8811fdabaac","_uuid":"7e82f7cfe5b4207e3fb819aa759966aa1acd49c6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean up the status data so that only available bike changes are used. It contributes more than 99% to the data.\nbike_avail_changes = status[status.bikes_available.diff() != 0]\nlen(bike_avail_changes) / len(status)","metadata":{"_cell_guid":"bd65a2ca-a428-407e-9f5e-bb9a61d50dd7","_uuid":"c3c08fd730768ac613d3a01e547e584563732d74"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### View only on the hour axis.","metadata":{"_cell_guid":"cb6d64fe-3f77-4bf1-b79b-54f08d389bd9","_uuid":"302a10cd5be863d679d7f63158c21cfffa7e8d1e"}},{"cell_type":"code","source":"bike_incr_diff_hour = bike_incr_diff_reset.groupby('hour')['counts'].sum()\nbike_decr_diff_hour = bike_decr_diff_reset.groupby('hour')['counts'].sum()","metadata":{"_cell_guid":"c2a48bd3-d0e3-4669-ae8e-b4ad5c36fa2b","collapsed":true,"_uuid":"1f0930ea182983f43ce36c013889fa058d383923"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the feature importances from the daily, am, pm trip models\n# Matplotlib code borrowed from: https://chrisalbon.com/python/matplotlib_grouped_bar_plot.html\nX_tick_labels = bike_incr_diff_hour.index\npos = np.array(range(len(X_tick_labels)))\n\nplt.figure(figsize=(20, 7))\nax = plt.subplot(111)\nwidth = 0.25\nax.bar(pos-0.5*width, bike_incr_diff_hour, width=width, color='#7BC024', label='Bike Count Increase')\nax.bar(pos+0.5*width, bike_decr_diff_hour, width=width, color='#17C9BF', label='Bike Count Decrease')\nax.legend()\n\nax.set_xlabel('Hour')\nax.set_ylabel('Extra status changes')\nax.set_xticks(pos)\nax.set_xticklabels(X_tick_labels)\n\n# Adding the legend and showing the plot\nplt.legend(loc='upper right')\nplt.grid()\nplt.show()","metadata":{"_cell_guid":"193617e1-837c-4f2a-a62f-58e036297ca5","_uuid":"93be57d8c0b1b276241854d780d6a2e11fe62799"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3 When are stations empty?","metadata":{"_cell_guid":"e1d094a0-dfe2-49f6-93d5-53ab922c3506","_uuid":"298efee0c3d840efab6589a08665cc4e59381ca7"}},{"cell_type":"code","source":"%%time\n# Read all the status data that have zero bikes available.\nconn = sqlite3.connect('../input/database.sqlite')\ncursor = conn.cursor()\nno_bikes = pd.read_sql_query(f\"SELECT * FROM status WHERE bikes_available = 0 ORDER BY time\", con=conn)\nconn.close()","metadata":{"_cell_guid":"20707513-a9a0-43b7-b6c8-bb86267f58f2","_uuid":"655be84cb9593a62fe38344359a3b1441fe0a281"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_bikes.head()","metadata":{"_cell_guid":"2984bc7c-c805-46d2-8211-9f1ca671fb5a","_uuid":"74be2f9eb55b3eeb96647f82cd58cfb4d25e424e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the business day\nno_bikes['datetime'] = pd.to_datetime(no_bikes.time)\nno_bikes['date'] = no_bikes.datetime.apply(lambda x: x.date())\nno_bikes['hour'] = no_bikes.time.apply(lambda x: int(x.split()[1].split(':')[0])) # Get the hours in a day.\n\nno_bikes_date_unique = pd.Series(no_bikes.date.unique())\nno_bikes_BDay_unique = no_bikes_date_unique.apply(lambda x: isbday(x))\n\nno_bikes_date_dict = dict(zip(no_bikes_date_unique, no_bikes_BDay_unique))\nno_bikes['BDay'] = no_bikes.date.apply(lambda x: no_bikes_date_dict[x])","metadata":{"_cell_guid":"49351158-f5ac-4fa2-90cb-5e9dd0bd1c3e","collapsed":true,"_uuid":"ff2a1bd2adc96a32316b5dc5402350b20fbdb01b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assign the regions to stations.\nno_bikes['station_region'] = None\nno_bikes_region_dict = {} # Initiate a dictionary from the region code to stations.\nstation_num_dict = {}\ngroup_id = 0\n\nfor group in id_groups:\n    region_code = region_dict[group_id]\n    region_idx = no_bikes.station_id.isin(group)\n    \n    no_bikes_region_dict[region_code] = group    \n    station_num_dict[region_code] = len(group)\n    \n    no_bikes.loc[region_idx, 'station_region'] = region_code\n    group_id += 1","metadata":{"_cell_guid":"496f6d85-a119-4d1b-9f56-5e22fa7ddbb5","collapsed":true,"_uuid":"f3a365e1d61c102770045dc4d20c88030852d046"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Divide the group into BDay and non-BDay\nno_bikes_BDay = no_bikes.loc[no_bikes.BDay == True, :]\nno_bikes_nonBDay = no_bikes.loc[no_bikes.BDay == False, :]","metadata":{"_cell_guid":"a172c690-b8d3-4e15-b6be-97abbb65125f","collapsed":true,"_uuid":"ffbf6c4b6d951c813f62d292d66751a8bd5eed25"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the empty minutes per hour per region in a single day.\nnumber_of_BDays = no_bikes_BDay_unique.sum() # Calculate the number of business days.\nnumber_of_nonBDays = len(no_bikes_date_unique) - number_of_BDays\n\nregion_hour_BDay = no_bikes_BDay.groupby(['station_region', 'hour'])['bikes_available'].count() / number_of_BDays\nregion_hour_nonBDay = no_bikes_nonBDay.groupby(['station_region', 'hour'])['bikes_available'].count() / number_of_nonBDays","metadata":{"_cell_guid":"12c8c852-59f4-4343-b037-6a4930404db6","collapsed":true,"_uuid":"06f3bbd0593a6d19d8873e3c8428a1461ef75402"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot empty station events per station per hour.\nregion_count = 1\nplt.figure(figsize=(12, 12))\nfor region in region_hour_BDay.index.levels[0]:\n    plt.subplot(5, 1, region_count) \n    \n    # Show the minutes count per station   \n    (region_hour_BDay[region] / station_num_dict[region]).plot.bar(label=region_names[region]) \n    \n    plt.ylabel('Empty Station (min)')\n    plt.xlabel('')\n    plt.legend()\n    region_count += 1","metadata":{"_cell_guid":"45cf20f5-c456-4891-92f7-f16f283ec380","_uuid":"38e9e3e887e780a40c94bd31975b5531b5fc353e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot empty station events per station per hour.\nregion_count = 1\nplt.figure(figsize=(12, 12))\nfor region in region_hour_nonBDay.index.levels[0]:\n    plt.subplot(5, 1, region_count) \n    \n    # Show the per station minutes count    \n    (region_hour_nonBDay[region] / station_num_dict[region]).plot.bar(label=region_names[region]) \n    \n    plt.ylabel('Empty Station (min)')\n    plt.xlabel('')\n    plt.legend()\n    region_count += 1","metadata":{"_cell_guid":"5f6a6594-e6cc-46e6-b9d6-b12404f310b4","_uuid":"476619ebfa36a70911eab690851aec7543887eb4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.4 When are stations full?","metadata":{"_cell_guid":"4a15036e-1437-413d-bbbd-9831fd03763a","_uuid":"12039f5097b564af00af992f9a90876440aa2513"}},{"cell_type":"code","source":"%%time\n# Read all the status data that have zero docks available.\nconn = sqlite3.connect('../input/database.sqlite')\ncursor = conn.cursor()\nstation_full = pd.read_sql_query(f\"SELECT * FROM status WHERE docks_available = 0 ORDER BY time\", con=conn)\nconn.close()","metadata":{"_cell_guid":"a433fabd-7fa1-4df2-824c-ae4971f1ed20","_uuid":"a1aa6673c48c278a6d321335a440a463696c5057"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The ratio of station full over empty\nlen(station_full) / len(no_bikes)","metadata":{"_cell_guid":"4ea8de35-0b87-4082-8ab3-aa8c8543e005","_uuid":"21c4bfe0061ccdddce6f6c51a249d47ffa5c726f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"station_full['datetime'] = pd.to_datetime(station_full.time)\nstation_full['date'] = station_full.datetime.apply(lambda x: x.date())\nstation_full['hour'] = station_full.time.apply(lambda x: int(x.split()[1].split(':')[0])) # Get the hours in a day.","metadata":{"_cell_guid":"452c1024-39f3-4620-af46-a8a7b96358b3","collapsed":true,"_uuid":"89742161b42a313de79389963ac1ecfb5a2fe408"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assign the regions to stations.\nstation_full['station_region'] = None\nstation_full_region_dict = {} # Initiate a dictionary from the region code to stations.\nstation_num_dict = {}\ngroup_id = 0\n\nfor group in id_groups:\n    region_code = region_dict[group_id]\n    region_idx = station_full.station_id.isin(group)\n    \n    station_full_region_dict[region_code] = group    \n    station_num_dict[region_code] = len(group)\n    \n    station_full.loc[region_idx, 'station_region'] = region_code\n    group_id += 1","metadata":{"_cell_guid":"fde85264-e5b1-49f8-877b-4e8fb84b1a92","collapsed":true,"_uuid":"ac30709e22f6c928d45405b1302ecdb8164dab7d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the full minutes per hour per region in a single day.\nnumber_of_days = len(station_full.date.unique())\n\nregion_hour_full = station_full.groupby(['station_region', 'hour'])['bikes_available'].count() / number_of_days","metadata":{"_cell_guid":"d804ea3d-6a4c-49e8-987e-4905f4187b24","collapsed":true,"_uuid":"bd9952e1e746cfff843b2e62a711745b5e9b4e2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot full station events per station per hour.\nregion_count = 1\nplt.figure(figsize=(12, 12))\nfor region in region_hour_full.index.levels[0]:\n    plt.subplot(5, 1, region_count) \n    \n    # Show the minutes count per station   \n    (region_hour_full[region] / station_num_dict[region]).plot.bar(label=region_names[region]) \n    \n    plt.ylabel('Full Station (min)')\n    plt.xlabel('')\n    plt.legend()\n    region_count += 1","metadata":{"_cell_guid":"119f2027-6d7a-442f-9967-51c7a3caca94","_uuid":"5de4d7d14a805dca26b55dd37d130a5960162d17"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5 Check whether status has NaN values","metadata":{"_cell_guid":"6bda94c6-2c12-4351-ba30-7410e596c9a8","_uuid":"683d587521ded30f4f133b9d3b5387e31effc223"}},{"cell_type":"code","source":"# Status data don't have nan values.\nstatus.isnull().sum()","metadata":{"_cell_guid":"efd32576-3285-41a0-87f5-937a6b34ffd4","_uuid":"08ea299e8588354565f77a8429638fdbc02b536e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Exploration on 'weather' data.","metadata":{"_cell_guid":"4c0d6894-43df-4e44-9206-eecc4b954008","_uuid":"98a98c4d9fde53dff82da24bc252b55fef1e85fb"}},{"cell_type":"code","source":"weather.head(1)","metadata":{"_cell_guid":"5f334a61-61d5-4979-8bc1-c2d2df862456","scrolled":false,"_uuid":"a8938c4f48dfffb0a1f3c462fc83c369408c32e3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weather.shape","metadata":{"_cell_guid":"a7f950b3-c934-4588-986b-f874b92ff948","_uuid":"6854440d9a233aecf1fe533305df620800688459"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.1 Explore the precipitation column","metadata":{"_cell_guid":"732d0438-6ad5-48a2-8e1b-a3f3e9052b5d","_uuid":"b15d4089558dcefe0f365bf76904fb02fe37a298"}},{"cell_type":"code","source":"# Precipitation column is not numeric!!\nprecip = weather.precipitation_inches\nprecip.dtype","metadata":{"_cell_guid":"d728effe-9ada-4819-8879-ba5b4b86fa06","_uuid":"eaf18c67a543594d4396211ccc74d4444a7416ec"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show unique values in precipitation data\nprecip.unique()","metadata":{"_cell_guid":"90137d70-af86-4d5b-93ff-9e3368b11bf5","scrolled":true,"_uuid":"5fe362247c9e24daf52b3904165e165c578e81da"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('{:.2}% of the weather data are labeled \"T\" for precipitation.'.format(len(weather[precip=='T'])/len(weather) * 100))\nprint('{:.2}% of the weather data are labeled \"nan\" for precipitation.'.format(len(weather[precip.isnull()])/len(weather) * 100))","metadata":{"_cell_guid":"e7bee077-6e59-42c3-a86e-ecdf46c38d29","scrolled":true,"_uuid":"5d1e85ab8b87db23656fbcc77a1919048e332a69"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following is quoted from http://www.nws.noaa.gov/om/csd/info/NOWdata/FAQ.php:\n> **Looking at daily data, some dates have an \"M\" or a &\"T\" in the field. What does this mean?**\n\n> \"M\" stands for \"Missing\". Data for an element will be missing if the primary sensor for that weather element is inoperable (e.g., has an outage) or malfunctioning (e.g., producing errant data) AND any collocated backup sensor is also inoperable or malfunctioning. \"T\" stand for \"Trace\". This is a small amount of precipitation that will wet a raingage but is less than the 0.01 inch measuring limit.\n\nThus, it is reasonable to assign a small value to 'T', 0.005 inches for example.","metadata":{"_cell_guid":"1e573183-5cd0-4041-88d1-a32f68662b6a","_uuid":"9f9533ab4b364c7d867792aef51ef16403a617a1"}},{"cell_type":"code","source":"weather.loc[precip=='T', 'precipitation_inches'] = 0.005","metadata":{"_cell_guid":"7f7f0737-25a4-4687-8162-b9651bc4b1c5","collapsed":true,"_uuid":"e8eba6229b8e6d9cfe829fc8435210ab47323a99"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Only one entry contains NaN value. This will be kept to analyze later after the merge into the complete datasets.\nweather[precip.isnull()]","metadata":{"_cell_guid":"07d93c05-7142-481c-8cb0-445abc350de4","_uuid":"b2eb7e4c493d24587a76761ecabe740799efba51"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weather.precipitation_inches = weather.precipitation_inches.astype('float')\nprint(weather.precipitation_inches.dtype)","metadata":{"_cell_guid":"e4afa5a2-8377-4ef4-a67b-b2a80a825707","scrolled":false,"_uuid":"7af03284470e8ea505b678bf41082ff63bfaa6e6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2 Deal with NaN values in numeric columns","metadata":{"_cell_guid":"06302a0c-7f8a-4551-9413-0072cb86be79","_uuid":"9d4b917e121f433f8f58522e081971bcd98c4d75"}},{"cell_type":"code","source":"# Summarize the number of NaNs in each column.\nweather.isnull().sum()","metadata":{"_cell_guid":"9d661f20-8214-4c14-8fda-898b5085cd15","scrolled":false,"_uuid":"7ec1cdf012ea854827960b07d74a7f5b350fd6a5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For weather parameters, the best guess can be the average value of the adjacent days. One can argue that the weather may abruptly change during these days, but an assumption of some mild change is still better than just deleting the whole row. However, the max_gust_speed_mph needs to be investigated independently as there are too many nan values.","metadata":{"_cell_guid":"d0dc251b-fa0a-48cf-acda-8fe8ca6beb1d","_uuid":"2db83a71e71b7393b087b5f99866b780e358a3cf"}},{"cell_type":"code","source":"# Function from more_itertools.consecutive_groups\n# https://more-itertools.readthedocs.io/en/latest/api.html#more_itertools.consecutive_groups\nfrom itertools import groupby\nfrom operator import itemgetter\n\ndef consecutive_groups(iterable, ordering=lambda x: x):\n    \"\"\"Yield groups of consecutive items using :func:`itertools.groupby`.\n    The *ordering* function determines whether two items are adjacent by\n    returning their position.\n    By default, the ordering function is the identity function. This is\n    suitable for finding runs of numbers:\n        >>> iterable = [1, 10, 11, 12, 20, 30, 31, 32, 33, 40]\n        >>> for group in consecutive_groups(iterable):\n        ...     print(list(group))\n        [1]\n        [10, 11, 12]\n        [20]\n        [30, 31, 32, 33]\n        [40]\n    For finding runs of adjacent letters, try using the :meth:`index` method\n    of a string of letters:\n        >>> from string import ascii_lowercase\n        >>> iterable = 'abcdfgilmnop'\n        >>> ordering = ascii_lowercase.index\n        >>> for group in consecutive_groups(iterable, ordering):\n        ...     print(list(group))\n        ['a', 'b', 'c', 'd']\n        ['f', 'g']\n        ['i']\n        ['l', 'm', 'n', 'o', 'p']\n    \"\"\"\n    for k, g in groupby(\n        enumerate(iterable), key=lambda x: x[0] - ordering(x[1])\n    ):\n        yield map(itemgetter(1), g)","metadata":{"_cell_guid":"bedef968-3b36-4a7e-ad49-111afe98a5d2","collapsed":true,"_uuid":"16e8ed3a86e7d7c51bbb17f41c6b105c07bf2749"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def replace_nan(data, col):\n    col_nan = data.loc[data[col].isnull(), col] # Get the nan entries of the specified column\n    nan_idx = col_nan.index # Get the index of the col_nan\n    \n    # Group adjacent indices from col_nan_idx according to the method from the following link:\n    # https://stackoverflow.com/questions/2154249/identify-groups-of-continuous-numbers-in-a-list\n    for group in consecutive_groups(nan_idx):\n        col_idx = list(group)\n        \n        # Set the mean of the adjacent six values as the value for the NaN\n        weather.loc[col_idx, col] = weather.loc[(col_idx[0]-3):(col_idx[-1]+3), col].mean()","metadata":{"_cell_guid":"cd82a3f3-0c65-43f4-be3f-775b58d1682d","collapsed":true,"_uuid":"76b51d14f25a84f31d44dcbbce0fa7a9b12380ff"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The replace_nan function will be applied to all numeric columns except the gust column\nnan_cols = ['max_temperature_f', 'mean_temperature_f', 'min_temperature_f',\n       'max_dew_point_f', 'mean_dew_point_f', 'min_dew_point_f',\n       'max_humidity', 'mean_humidity', 'min_humidity',\n       'max_sea_level_pressure_inches', 'mean_sea_level_pressure_inches',\n       'min_sea_level_pressure_inches', 'max_visibility_miles',\n       'mean_visibility_miles', 'min_visibility_miles', 'max_wind_Speed_mph',\n       'mean_wind_speed_mph', 'precipitation_inches',\n       'cloud_cover', 'wind_dir_degrees']","metadata":{"_cell_guid":"e1005dec-1277-4ee4-9772-a8c3a0d5f998","collapsed":true,"_uuid":"680c1025c4699a537e6fdb382a5690e229591432"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in nan_cols:\n    replace_nan(weather, col)","metadata":{"_cell_guid":"3ed8a6bf-a6e6-4789-95f6-fc6281201406","collapsed":true,"_uuid":"9c4ce630bec9e9da001a1ad1b39c94d6cd25f990"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quoted from the following link:\nhttp://www.differencebetween.net/science/nature/difference-between-gust-and-wind/\n\n> A gust is a sudden increase of the wind’s speed that lasts no more than 20 seconds. \n\nThe max gust speed should be highly correlated with the max wind speed.","metadata":{"_cell_guid":"25928247-b2b4-47ec-acd3-f90747a1fc40","_uuid":"04a3653ebadb907ec73cfc98e5203078daf0478a"}},{"cell_type":"code","source":"# If the gust data is missing, it should mean that there is no gust.\n# This is somewhat confirmed by the fact that there is no gust value of 0 in the data.\nweather.loc[weather.max_gust_speed_mph == 0, 'max_gust_speed_mph']","metadata":{"_cell_guid":"08adb168-c7d4-4b38-a678-bed7c93c1fb3","_uuid":"80534ca8a9f7680a6cec855393fdbc00eb39af4d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weather.max_gust_speed_mph.fillna(0, inplace=True) # Replace all the nan values in gust data with 0.","metadata":{"_cell_guid":"5a6a687e-fe5b-410f-8bd1-f0c2f1e6661c","collapsed":true,"_uuid":"051c3b28b894b1462038e4f85df84dc87084c0dd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weather.isnull().sum()","metadata":{"_cell_guid":"d50900aa-80c7-448b-9289-148f9d658037","_uuid":"6184dc51ea7dc564006e7f84666528adba7a3147"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3 Find the correlations between features.","metadata":{"_cell_guid":"eeeecb67-ceb9-40b9-b2e9-67f3797901d9","_uuid":"b87c32d16a72f7f7a5698ac5b8309cfe7e1fb693"}},{"cell_type":"markdown","source":"The weather data have too many feature columns, some seeming to be correlated. To fight against the curse of dimensionality, the strongly correlated columns will be filtered out.","metadata":{"_cell_guid":"073078de-2a06-487c-8999-d3ec4ba77960","_uuid":"48b1d51e729e8e950f3e71dc80099b9d2cf901cc"}},{"cell_type":"code","source":"# Create a correlation table between all the features.\nweather_corr_table = weather.corr(method='pearson', min_periods=1)\n\ndisplay(weather_corr_table)","metadata":{"_cell_guid":"281aa118-f0e8-4fdc-856c-8e65ae18def7","scrolled":true,"_uuid":"4c040f1b8e679eb3308128477525d4877d3b18c1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quoted from this link http://janda.org/c10/Lectures/topic06/L24-significanceR.htm:\n> For large samples, it is easy to achieve significance, and one must pay attention to the strength of the correlation to determine if the relationship explains very much.\n\nSo the strength of the correlation is more important for the bikeshare data.","metadata":{"_cell_guid":"af11f242-6a52-4e5e-baff-4e0dc8703852","_uuid":"57b801878841a8b99a35357a25d4bbd8c0ec7868"}},{"cell_type":"code","source":"r_squared = 0.5 # Set the coefficient of determination to be 0.5 so that more than half of the variance can be explanable.\n\n# Obtain the correlated features.\nweather_corr = []\nfor col in weather_corr_table.columns:\n    feature = weather_corr_table[col]\n    corr_cols = feature[(feature.pow(2) > r_squared) & (feature != 1)]\n    corr_idx = corr_cols.index.values.tolist()\n    corr_dict = {col: corr_idx}\n    if corr_idx != []:\n        weather_corr.append(corr_dict)","metadata":{"_cell_guid":"41e3e1c5-96f8-4623-bf34-5ad71c07a936","scrolled":true,"collapsed":true,"_uuid":"283f3cd8af79442ae7bb6b4c15423e410bd28d42"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(weather_corr)","metadata":{"_cell_guid":"0b14aa34-84f9-4b27-a03b-e8aed478f37f","scrolled":true,"_uuid":"43ffcd4a440916c61b4bd4c52c8072a5f77b6afc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It turns out that:\n* 'mean_temperature_f' can well represent 'max_temperature_f', 'min_temperature_f', 'max_dew_point_f' and 'mean_dew_point_f', but not 'min_dew_point_f';\n* 'mean_humidity' can well represent 'max_humidity' and 'min_humidity';\n* 'mean_sea_level_pressure_inches' can well represent 'max_sea_level_pressure_inches' and 'min_sea_level_pressure_inches'.\n\nAll dependent features will be removed to reduce the dimension of the data.","metadata":{"_cell_guid":"29e4a0aa-3eae-48c8-b94e-7dac8abca99e","_uuid":"3b51812560e6c920ee2c2a5706b837994f04c98a"}},{"cell_type":"code","source":"# Explore the categorical column\nweather.events.unique()","metadata":{"_cell_guid":"08e8c839-312a-4871-8b76-a50974ec6f58","scrolled":true,"_uuid":"989aed421f8bba8ee7895f8f8c2650a0bf744ef2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weather.loc[weather.events == 'rain', 'events'] = 'Rain'\nprint(weather.events.unique())","metadata":{"_cell_guid":"1d172546-5e60-4708-a497-784b0abe0c8b","_uuid":"83e8b1c7183c26aa8d28b39a9d91550414cb87ce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a weather data with independent columns and ONEHOT the events column.\nweather_ind = pd.get_dummies(weather.drop(labels=['max_temperature_f', 'min_temperature_f', 'max_dew_point_f', 'mean_dew_point_f', 'max_humidity', 'min_humidity', 'max_sea_level_pressure_inches', 'min_sea_level_pressure_inches'], axis=1), columns=['events'])","metadata":{"_cell_guid":"665af221-a5c7-40ab-90e7-10930ee321ac","collapsed":true,"_uuid":"3b5b92d97cb72ed96276dededbbb477ce6d00018"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"** It is strange to find low correlation between the max wind speed and the max gust speed. In fact, this depends on the region:**","metadata":{"_cell_guid":"69c9901a-1787-469b-b5e6-de77baae2dd8","_uuid":"1280d21a786bb983aa9a9f442c1f9b559eebb7d8"}},{"cell_type":"code","source":"print('The correlation between the max wind speed and the max gust speed is: \\n')\nfor region in regions:\n    regionWeather = weather[weather.zip_code == region]\n    corrCoef = pearsonr(regionWeather.max_wind_Speed_mph, regionWeather.max_gust_speed_mph)[0]\n    print('{} : {:.2f}'.format(region_names[region], corrCoef))","metadata":{"_cell_guid":"64a5882b-c521-4354-bd66-ad31c56c8b5d","_uuid":"45491de7cc63ff6ede50a613a36c2fa2707fc597"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_count = 1\nplt.figure(figsize=(15, 20))\nfor region in regions:\n    weather_region = weather.loc[weather.zip_code == region, :]\n    plt.subplot(5, 1, plot_count)\n    plt.scatter(weather_region.max_wind_Speed_mph, \n                weather_region.max_gust_speed_mph, \n                label='{}'.format(region_names[region]))\n    plt.xticks(np.arange(0, 45, 5))\n    plt.xlim([0, 45])\n    plt.xlabel('Max Wind (mph)')\n    plt.ylabel('Max Gust (mph)')\n    plot_count += 1\n    plt.legend()\n    plt.grid()","metadata":{"_cell_guid":"28302b87-dff4-400c-96d1-6afc6732b20c","_uuid":"ec6e54f5b18f74744f4f545d7507422bbd632363"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly, the correlation is actually high for San Francisco and San Jose while very low for the other three cities with the latter sitting in the deep valley close by the mountain and the bay.","metadata":{"_cell_guid":"511dbeb8-3777-4895-8684-7ae37f2563d3","_uuid":"77cb4830e220701893ab4d4f8ac3d546f1149cb3"}},{"cell_type":"markdown","source":"### 4.4 Simplify the column names.","metadata":{"_cell_guid":"48892e8b-9953-49fd-aba4-fa25a70f6645","_uuid":"c324197d10b9e27347399a1413ca936e680597f9"}},{"cell_type":"code","source":"weather_ind.columns","metadata":{"_cell_guid":"7c026c92-abc5-4c53-92c7-39444a728667","_uuid":"e5c12aeb8ef2e1666a4e49741c7fd324646aadb1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weather_ind.columns= ['date', 'Temp', 'Dew', 'Humid', 'Pressure', 'Max_Vis', 'Mean_Vis', 'Min_Vis', 'Max_Wind', 'Mean_Wind', 'Gust', 'Precip', 'Cloud', 'Wind_Deg',  'zip_code', 'Fog', 'Fog_Rain', 'Rain', 'Thunder']","metadata":{"_cell_guid":"6fea23e4-f808-4093-a459-0214723d242e","collapsed":true,"_uuid":"480579b825e5afc5766b20348d6f57885cae1b3a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weather_ind.head(1)","metadata":{"_cell_guid":"6ea932b0-25bf-47c9-b7ef-9dd6ae84ae5f","_uuid":"cb5833e7c796dbccc0863e6d2c24a96bc58f3fb9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.5 Explore monthly variations for the features.","metadata":{"_cell_guid":"5fc32e85-9f70-4f54-b5c8-4ad2d5901592","_uuid":"c02557925919268bec17ef079fad1dacc0b9748c"}},{"cell_type":"code","source":"# Previous exploration suggests that the wind degree column has an outlier point. I will correct this point before making the plots.\nweather_ind[weather_ind.Wind_Deg > 360]","metadata":{"_cell_guid":"64565a3a-7379-41c6-a5fb-fa0b164c07cd","_uuid":"924c680002785aef8401e5b3a354a93169695063"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This seems to be a typo with an extra '7' in the middle.\nweather_ind.loc[weather_ind.Wind_Deg > 360, 'Wind_Deg'] = 272","metadata":{"_cell_guid":"6fe6ac77-48e8-4a92-b454-57693fb43e74","collapsed":true,"_uuid":"66ba2df7dcddedc5d3ef62f97b7a1d1c619f37db"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weather_month = weather_ind.date.apply(lambda x: int(x.split('/')[0]))","metadata":{"_cell_guid":"4afa67b5-449c-4085-920f-56c4de3b1557","collapsed":true,"_uuid":"da9c8a24eb8bba14c7643dd787b2d78ba04b878e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot all the numeric feature columns versus months\nplot_cols = weather_ind.columns[1:14]\n\nplt.figure(figsize=(25, 15))\nplot_idx = 1\nfor col in plot_cols:\n    plt.subplot(3, 5, plot_idx)\n    plt.scatter(weather_month, weather_ind[col])\n    plt.xlabel('Month')\n    plt.title(col)\n    plot_idx += 1","metadata":{"_cell_guid":"b695c8aa-ac40-46af-ad69-f59075e79041","scrolled":false,"_uuid":"0e84feb6ab9faad7b0872fe572859e32eea20ffb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above features don't show significant dependence on each other. This is obvious by looking at the point patterns. Some features have obvious seasonal changes, like the temperature, the dew point, the sea level pressure, the precipitation; some are less variable in seasons, like the visibility, the wind speed, gust speeds and the wind degrees; the others are not dependent on the season, like the cloud cover. No matter what, I feel all these features will affect people's choice in bike riding, so all will be kept for analysis.","metadata":{"_cell_guid":"c301a55f-ab94-47b3-88b6-e423b272d5e2","_uuid":"7752e6cd42772ca39f01e2738aea861ae4198eac"}},{"cell_type":"markdown","source":"### 4.6 Convert the date to datetime format.","metadata":{"_cell_guid":"ab3f3751-3dea-40de-a0bb-77064db12c14","_uuid":"f1369e2d69bc65b3b485a52b3498f0ddd16f419a"}},{"cell_type":"code","source":"# The conversion is necessary for the combination with other data\nweather_ind.date = weather_ind.date.apply(lambda x: datetime.strptime(x, \"%m/%d/%Y\").date())","metadata":{"_cell_guid":"eb9f4ff4-f6bb-4f72-969e-ba58df80978b","collapsed":true,"_uuid":"eefe51c78f78bf3df5e19ba494f532b0fe4a9d69"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Datasets Generation","metadata":{"_cell_guid":"bdb49b33-6b81-4b03-add9-9f578bcd0472","_uuid":"18816826ee04d814e93631616785cb79d1dd8615"}},{"cell_type":"markdown","source":"Drop columns that are not relevant for the datasets:\n1. 'id'. Trip id is unique for a specific trip. It doesn't contain any trip pattern.\n2. 'start_date' and 'end_date'. The previously generated 'date' and 'hour' contain the relevant information already.\n3. 'start_station_id', 'start_station_name', 'end_station_id' and 'end_station_name'. The station groups will be used instead to reduce the complexity of the problem, which reflects the essence of the problem.\n4. 'zip_code'. The useful information has been extracted into the column 'local_zip'.\n5. 'bike_id'. Bike id should not be an important factor as all the sharebikes look and function similarly.","metadata":{"_cell_guid":"c2fd72c3-fac8-4be3-94dc-87e13c77dbc0","_uuid":"fb41b86663eafe0d709c887eac7588b9a054427c"}},{"cell_type":"markdown","source":"### Pretreatment of the trip data.","metadata":{"_cell_guid":"4f57b253-c4c2-432c-904b-1f40bc5f0096","_uuid":"ac6cab86811c8ff0e573e79352ff1890c0d3af40"}},{"cell_type":"code","source":"trip.columns","metadata":{"_cell_guid":"eb235ef2-181b-4c2f-91fb-699567cf9d4b","_uuid":"d1b06ba96fb0c62ad63a6d70997844554940a5d5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop columns that not relevant for both datasets.\ndrop_cols = ['id', 'start_date', 'start_station_name', 'start_station_id', 'end_date', 'end_station_name', 'end_station_id', 'bike_id', 'zip_code']\ntrip_use = trip.drop(drop_cols, axis=1)","metadata":{"_cell_guid":"8533163f-ff13-4cbe-abeb-1dd18bf37167","collapsed":true,"_uuid":"c62564334bc2a2eeba76414c665788d37b68d408"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(trip_use.shape)\ntrip_use.head(1)","metadata":{"_cell_guid":"ed9d625f-8f03-45f3-9065-5a6b30743c3d","_uuid":"528ed8c6adf09c0133811c84d1277bfe74a6ad40"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Define valid trips.","metadata":{"_cell_guid":"3343a22e-dc6b-4c52-ae79-10953c62a4ea","_uuid":"ce40d9e4064a031f189ea382d884a3c1e0ec56ca"}},{"cell_type":"code","source":"# Only consider durations less than 12 hours as valid trips.\nduration_12hours = 60*60*12\ntrip_final = trip_use[trip_use.duration <= duration_12hours]","metadata":{"_cell_guid":"bfa24282-ed0e-49a3-bfae-d3bee272b688","collapsed":true,"_uuid":"75421b4a6cd77d1cb9d46c910d8e10bf0cedb6b1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('{:.1f}% of trips are within 12 hours.'.format(len(trip_final) / len(trip_use) * 100))","metadata":{"_cell_guid":"d6ad135b-8500-4468-ad97-af059503ed2a","_uuid":"aa73f6b048e8bfa6374a9c0e6dbc5e6df89e5211"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Inspect whether there are nan values in the two data","metadata":{"_cell_guid":"6fa4e558-365d-4402-b0f1-1fe8185a8add","_uuid":"047220b3ae61954df7be7f5a5d548e5900cf3798"}},{"cell_type":"code","source":"trip_final.isnull().sum()","metadata":{"_cell_guid":"0326ed51-0353-4cf2-ba75-b846ee63f0f3","_uuid":"8e42cec9446e73ae5778264a7b2a1feda7dc729b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Investigate the weather data.","metadata":{"_cell_guid":"70f38a8b-edee-4ec1-9e34-6dd03c1d2b37","_uuid":"7d71db49e61f5df428e481d093438c7a4a9ff8d8"}},{"cell_type":"code","source":"print(weather_ind.shape)\nweather_ind.head(1)","metadata":{"_cell_guid":"be80a9e8-344a-4856-b05e-a133b0829c9a","scrolled":false,"_uuid":"30c87c6c3ccd8953ab1f7bcb6d12e4c22c2d0abe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weather_ind.isnull().sum()","metadata":{"_cell_guid":"9f60d7dd-6985-4443-99bc-8debffe3281a","_uuid":"067a87740bce47a1bd534336b291082da564ea8c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Dataset for Regression","metadata":{"_cell_guid":"5697203d-9c7d-496f-b99c-a96009b07b2b","_uuid":"176fca884166c1a169bf3c509ba16df7dd0f106b"}},{"cell_type":"code","source":"trip_final.columns","metadata":{"_cell_guid":"3d2470dc-25e6-413d-9578-946b1296fa61","scrolled":false,"_uuid":"440e2de65ec0bbb59be4c3b9a2beca54bb6ad152"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The goal of tackling the problem for the daily trip counts is to predict the daily trips in the future. Thus, only those features that we know in advance or can be forecast in the future will be used. Anything that is related to the statistics of the trips during the day is not appropriate as the feature, e.g. the average duration of the trips, the ratio of subscribers over non-subscribers, the frequency of inter-group trips, the number of people that comes from a local zip code, etc. On the contrast, a specific date in the future has the following information known beforehand: the year, month, day, whether it is a business day and before October 2013, which weekday it is. However, one needs to be careful when using the year information. Since we are predicting the future, the year may either be irrelevant or can leak information from the test set to the training set. So I decide not to include the year as a feature for the regression.","metadata":{"_cell_guid":"08b81013-8a9c-47e9-89b5-7c5b6537c379","_uuid":"226cfccb0a342d96e7cb6ca6f2a6f6594e3e6e02"}},{"cell_type":"code","source":"trip_rgs = trip_final[['date', 'station_region', 'hour', 'month', 'BDay', 'weekday', 'before_Oct2013']]","metadata":{"_cell_guid":"c00fc480-ae28-4d5c-b475-91da3fc70922","collapsed":true,"_uuid":"672c4234c4bc85778fcc168d4e182f1ecee3cf08"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trip_rgs.head()","metadata":{"_cell_guid":"73b290ca-93ca-4109-bbaf-11caef221bb9","scrolled":true,"_uuid":"3d75cc660d4b146451b31d7d59ce096d3400c12d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Five datasets for regression will be created for different regions / station groups.","metadata":{"_cell_guid":"9092fb7e-6fc8-41da-92f0-7a51bc9fe294","_uuid":"74854afced05711f046271b555a0203dfe569acc"}},{"cell_type":"code","source":"# Define a function to be used in the below cell.\ndef region_weather_trips(data, weather):\n    region_data = data.groupby('date')['BDay', 'weekday', 'month', 'before_Oct2013'].median()\n    region_data['trip_counts'] = data.groupby('date')['hour'].count()   \n    region_data.columns=['BDay', 'weekday', 'month', 'before_Oct2013', 'trip_counts']\n    # region_data.rename(columns={'median': 'BDay', 'count': 'trip_counts'}, inplace=True)\n    \n    # Merge the region data with weather data. The inner join is used as some dates may not be in the region data and the weather data of the missing dates shouldn't be included in the merged data.\n    region_weather_merge = pd.merge(region_data, weather, left_index=True, right_on='date') \n    region_weather_merge.set_index('date', inplace=True) # Set the date as the index after merging\n    region_weather_merge.drop('zip_code', axis=1, inplace=True)\n    \n    return region_weather_merge","metadata":{"_cell_guid":"f1c45686-16e2-467a-8329-cdeec500e8fd","collapsed":true,"_uuid":"53e8942b30d3186edc2f8af464dd59c82d220c35"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate five datasets as a dictionary with the zip code as the key.\ntrip_count_rgs = {}\nfor group in regions:\n    region = trip_rgs[trip_rgs.station_region == group].drop('station_region', axis=1) # Obtain region by zip code\n    region_weather = weather_ind[weather_ind.zip_code == group] # Obtain region weather by zip code\n    \n    region_daily = region_weather_trips(region, region_weather)\n    region_am = region_weather_trips(region[region.hour < 13], region_weather)\n    region_pm = region_weather_trips(region[region.hour > 12], region_weather)\n    \n    trip_count_rgs[group] = [region_daily, region_am, region_pm]\n    \n    # Use OneHot for both the weekday and the month as both columns are actually categorical features rather than numeric features. This may not affect the decision tree algorithms, but will help with the linear regression algorithms.\n    # trip_count_rgs[group] = pd.get_dummies(region_daily, columns=['weekday', 'month'])\n    # trip_count_rgs[group] = region_daily","metadata":{"_cell_guid":"2fac2766-fd02-46ef-a572-ff5a8b2ca06f","collapsed":true,"_uuid":"d88ac2a3480986aa224e6abb20f780d65e0b49d4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(trip_count_rgs)","metadata":{"_cell_guid":"ebe2db2d-cf00-49b8-8b34-6418a004677f","scrolled":true,"_uuid":"ec8fa1bfee56b1d44910edf9bff36c42d9128290"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Some regions don't have data for all dates. There are totally 733 days. This also confirms the inner merge.\nprint(len(trip_count_rgs[94041][0]))\nprint(len(trip_count_rgs[95113][0]))\nprint(len(weather)//5) # Five regions.","metadata":{"_cell_guid":"e9704126-88f7-4807-9ba1-490177145520","_uuid":"2ce395a21a95f115209381b0226587205822f785"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"** Add the total dock counts for each day. The number of total counts is an important parameter for trip count prediction.** ","metadata":{"_cell_guid":"421b4f99-83af-4c8a-9213-e8009264bb4a","_uuid":"5f82c5512504c45502a08f966b4ed143e3f2ea1e"}},{"cell_type":"code","source":"# Convert the installation date to datetime object\nstation.installation_date = pd.to_datetime(station.installation_date, format = \"%m/%d/%Y\").dt.date","metadata":{"_cell_guid":"a6d1e682-c79e-4d4e-a5f9-20a55abc5141","collapsed":true,"_uuid":"ffd0696d6d74aae7498dda511b3edb7ea14ed7ff"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the total dock counts for each region on each day\n# The count_docks function is used to map the date (index) to the total docks.\ndef count_docks(date):\n    return sum(s_group[s_group.installation_date <= date].dock_count)\n\nfor region, group in trip_count_rgs.items():\n    s_group = station[station.region == region]\n    group[0]['total_docks'] = group[0].index.map(count_docks) # group[0] is the daily trip\n    group[1]['total_docks'] = group[1].index.map(count_docks) # group[1] is the morning trip\n    group[2]['total_docks'] = group[2].index.map(count_docks) # group[2] is the afternoon trip","metadata":{"_cell_guid":"bdde0004-ca35-4c69-9762-476b87a8ef25","collapsed":true,"_uuid":"50f8d9406629c69935adb5afcbc842c6ce33b06c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Dataset for Classification","metadata":{"_cell_guid":"1829e900-2364-43e0-8184-859a769c55d8","_uuid":"4e5346f73d13a8eab30e80fb791dc35cba432b73"}},{"cell_type":"code","source":"trip_final.head(1)","metadata":{"_cell_guid":"68159e7e-2db5-4d96-b831-53c59a35b596","_uuid":"9d37da7dee30e9c078bbd9e920c01f0ebe02083b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weather_ind.head(1)","metadata":{"_cell_guid":"346a60d3-c509-4f38-8ddb-7737bac45e56","_uuid":"e6346c4433bf1aacde9d3549a09760f64597ee5f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge the trip and weather data based on the date and the region.\ntrip_weather_merge = pd.merge(trip_final, weather_ind, left_on=['date', 'station_region'], right_on=['date', 'zip_code'])","metadata":{"_cell_guid":"b64e78c5-9793-441d-9b37-362593c24e97","collapsed":true,"_uuid":"b16b18fc244349a2179c2c9629a2199f3cdb7a83"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the sizes of the data before and after merge\nprint(trip_final.shape)\nprint(weather_ind.shape)\nprint(trip_weather_merge.shape)","metadata":{"_cell_guid":"bc5989c5-c19e-47d1-8888-7ba8f4493509","_uuid":"d7de3a69148abd35f312a6b070096a6b3574c948"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The merge makes sense: the total number of rows is the same as the trip data. Also, the number of columns equals to the sum of trip and weather data's minus one (same column name for the date, different columns names for the region).","metadata":{"_cell_guid":"5910da98-5a16-475b-af30-a3e5fdb7e852","_uuid":"df1bcd0b1e4539f2d84ad3c1bdb437f4bc903d39"}},{"cell_type":"code","source":"# Remove the date column. All features about a day have been extracted, so the date itself is not relevant.\n# Remove the zip_code column as it is a redundant column as the station_region column.\ntrip_weather_merge.drop(['date', 'zip_code'], axis=1, inplace=True)","metadata":{"_cell_guid":"090decb4-6e29-4e11-a619-b34ada0f0261","collapsed":true,"_uuid":"22eb8cb07e2ca8bb2f35f217d3d7d2d4e8779ec4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the dataset for classfication problem\nsubscriber_cls = pd.get_dummies(trip_weather_merge, columns=['station_region'])","metadata":{"_cell_guid":"9ccfc6b8-e170-484d-801c-2cea7a440161","collapsed":true,"_uuid":"b5c0f0ddd842ed1ae66ee5328acad33c37958fb4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('{} rows of the classification data contain the NaN values.'.format(len(subscriber_cls[subscriber_cls.isnull().any(axis=1)])))","metadata":{"_cell_guid":"3848dd72-2b5a-4aff-b76a-a3d480221b3b","_uuid":"9789957a63137ad80e6f145a36d5f55480b4dc42"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling and Fitting","metadata":{"_cell_guid":"2ec15302-a512-41db-abf9-54a0b6f6189e","_uuid":"95cd1ecc24c3199f6801578b1c978281f3cf952a"}},{"cell_type":"markdown","source":"## 1. Regression Problem for Daily Trip Counts Prediction","metadata":{"_cell_guid":"92a7f90e-111a-466c-992c-7796e8f610da","_uuid":"f6a3ebedc0328473b8478231e9154623262cda3c"}},{"cell_type":"code","source":"trip_rgs.head()","metadata":{"_cell_guid":"6e52ce4c-3882-495b-a404-02684fe7616c","_uuid":"e98f8ad6b15f9a2803767c9f07a8c231d031ebcf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show an example of a full dataset in a region with data of daily trip counts.\ntrip_count_rgs[94107][0].head(1)","metadata":{"_cell_guid":"0120db1f-bb84-445d-937f-7c5fca14ede8","_uuid":"4f9299f36ad887ac09b7bad8301fddc67c152e3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the regions in the datasets.\nprint(trip_count_rgs.keys())","metadata":{"_cell_guid":"7608649c-3cd8-4495-9284-f755468078a6","_uuid":"00542712c07b9c838853ea1e9a35c9d74dbb0bde"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"region_names[94107]","metadata":{"_cell_guid":"f60f3818-8f0d-46e4-aec1-dd624dc5d98c","_uuid":"2dc08a967bd0cb9d5aa633e806caf7d7dae3acea"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1 Predict the number of daily trips for each station","metadata":{"_cell_guid":"ad291f00-8617-4523-9f85-70024479baeb","_uuid":"f0fdb4786ff581fb5d41581d95efdcd2d295c318"}},{"cell_type":"code","source":"# A function to predict the label with a certain regressor\ndef rgs_pred(trip_data, rgs):\n    features = trip_data.drop('trip_counts', axis=1)\n    counts = trip_data.trip_counts\n    \n    trainX = features.loc[X_train.index]\n    trainY = counts.loc[y_train.index]\n    testX = features.loc[X_test.index]\n    \n    rgs.fit(trainX, trainY)\n    \n    return rgs.predict(testX)","metadata":{"_cell_guid":"4bf8b5dd-c217-4aa7-b011-536c4051756a","collapsed":true,"_uuid":"e52c454116d7aa5a506effd0eef0f747ccc2f2e4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This regressor is the one found to have the best performance.\ngbr = GradientBoostingRegressor(learning_rate = 0.3,\n                                n_estimators = 50,\n                                max_depth = 8,\n                                min_samples_leaf = 3,\n                                random_state = random_state)\n\nfor region in regions:\n    region_trips = trip_count_rgs[region][0]\n    \n    region_features = region_trips.drop('trip_counts', axis=1)\n    region_counts = region_trips.trip_counts\n    \n    X_train, X_test, y_train, y_test = train_test_split(region_features, region_counts, test_size=0.2, random_state=random_state)\n    \n    y_pred = rgs_pred(region_trips, gbr)\n    median_daily_trips = np.median(y_train)\n    y_benchmark = np.ones(len(y_test)) * median_daily_trips\n    average_total_docks = np.mean(X_train.total_docks)\n    \n    print('For {} (median {:.0f} trips daily with {:.0f} docks):'.format(region_names[region], median_daily_trips, average_total_docks))\n    print('The median absolute error for the regular prediction is {:.1f}.'.format(median_absolute_error(y_test, y_pred)))\n    print('The median absolute error for the benchmark prediction is {:.1f}. \\n'.format(median_absolute_error(y_test, y_benchmark)))\n    # print('The root mean square logarithmic error is {:.1f}%.'.format(np.sqrt(mean_squared_log_error(y_test, y_pred))*100))\n    # print('The root mean square logarithmic error is {:.1f}%. \\n\\n'.format(np.sqrt(mean_squared_log_error(y_test, y_benchmark))*100))","metadata":{"_cell_guid":"13032387-6677-4978-8b1e-9fc37c20dcb1","scrolled":true,"_uuid":"f492f11d0f7b8396f5e09ffca9c1323ce75724bf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I didn't pay attention to the number of daily trips for each region. This result is astonishing! Except San Francisco, there are so few trips in a day in the other regions. The number of trips in San Francisco is much higher than the docks, while in other cities, not all bikes are used daily! The prediction with the regressor performs even worse than the a random guess (median of the label column in the training set) for those stations with very small median daily trips, i.e. in Redwood City and Palo Alto. The regressor performs much better than the benchmark for the other three cities.","metadata":{"_cell_guid":"aa2cbd96-880c-41b4-a493-64a733422f2b","_uuid":"70e910764c35e5a75e7f05e628a9e9dd614bdb55"}},{"cell_type":"markdown","source":"### 1.2 Improve the model for San Francisco trip data","metadata":{"_cell_guid":"20182fc2-4de1-4328-9432-a69981c7abf8","_uuid":"d58602e852f821cc7aa56ae827588b513645a8dc"}},{"cell_type":"markdown","source":"#### 1.2.1 Normalize numeric feature columns","metadata":{"_cell_guid":"95fbc854-f1bc-42c4-98b8-238d7f73cd5f","_uuid":"7696385fc9ac4dbb9f52ea78dda54d83216e1c18"}},{"cell_type":"code","source":"# Collect all numeric features in the trip data\ntrip_num_features = ['weekday', 'month', 'Temp', 'Dew', 'Humid', 'Pressure', 'Max_Vis', 'Mean_Vis', 'Min_Vis', 'Max_Wind', 'Mean_Wind', 'Gust', 'Precip', 'Cloud', 'Wind_Deg', 'total_docks']","metadata":{"_cell_guid":"9e619e56-206e-4c85-95c2-2490d47953a9","collapsed":true,"_uuid":"741d7979e5f3602e8c1ccf1b732536c67fdd2063"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Investigate the trips in San Francisco. The numeric features are normalized to accommodate all the regressors.\ntrips_SF = trip_count_rgs[94107][0]\nfeatures_SF = trips_SF.drop('trip_counts', axis=1)\nfeatures_SF[trip_num_features] = MinMaxScaler().fit_transform(features_SF[trip_num_features])\ncounts_SF = trips_SF.trip_counts\n\nX_train, X_test, y_train, y_test = train_test_split(features_SF, counts_SF, test_size=0.2, random_state=random_state)","metadata":{"_cell_guid":"d13770be-9edc-448c-9f4f-62abaf0b44ee","collapsed":true,"_uuid":"b70b150cda7f713b41a804f382c5754cf98d5a2f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.2.2 Select proper models by investigating all regressors","metadata":{"_cell_guid":"05e35fac-c121-414d-9d2b-eb54eefbe19a","_uuid":"2399cdd0ee13f4f2fe4980643414ceef46e00806"}},{"cell_type":"code","source":"mae_scorer = make_scorer(median_absolute_error, greater_is_better=False) # mae: median absolute error\n# Define a clear cross validation set\n# Shuffle Split is used instead of KFold. A better method for a small dataset.\ncv_sets = ShuffleSplit(n_splits=15, test_size = 0.20, random_state=random_state) \n# cv_sets = KFold(n_splits=15, shuffle=True, random_state=random_state)","metadata":{"_cell_guid":"5a1e7942-6346-4337-b8ec-1374f231d1fa","collapsed":true,"_uuid":"b72e66eac9c0f21e051e9294e0a53b410b8c6b86"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The original functions comes from: https://www.kaggle.com/currie32/a-model-to-predict-number-of-daily-trips/notebook\ndef scoring(rgs):\n    scores = cross_val_score(rgs, X_train, y_train, cv=cv_sets, n_jobs=1, scoring = mae_scorer)\n    return np.mean(scores)","metadata":{"_cell_guid":"0b4f9532-0ee9-4fc4-8094-136f663accdc","collapsed":true,"_uuid":"92ae8dd8037692b58fe54fa6bb5f2c647b0e21f3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the performance of all regressors with default setting\nregressors = [DummyRegressor,\n              AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor,\n              GaussianProcessRegressor,\n              HuberRegressor, Lasso, LinearRegression, PassiveAggressiveRegressor, RANSACRegressor, Ridge, SGDRegressor, TheilSenRegressor,\n              KernelRidge,\n              KNeighborsRegressor,\n              MLPRegressor,\n              LinearSVR, NuSVR, SVR,\n              DecisionTreeRegressor, ExtraTreeRegressor,\n              XGBRegressor]\n\nrgs_dict = {} # Create a regressor dictionary to record the regressor with its score\nfor rgs in regressors:\n    begTime = time() # Get the beginning time\n    \n    # All parameters are set by default.\n    # Unfortunately, not all regressors have a random_state parameter, so each run will give a slightly different ranking.\n    try:\n        rgs_function = rgs(random_state=random_state) \n    except:\n        rgs_function = rgs()\n        \n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        rgs_error = scoring(rgs_function) * (-1) # Error is opposite of score\n        \n    rgs_dict[rgs.__name__] = rgs_error\n    \n    useTime = time() - begTime\n    print('The regressor {} takes {:.2f} seconds and has a median absolute error of {:.1f}.'.format(rgs.__name__, useTime, rgs_error))    ","metadata":{"_cell_guid":"c20ee432-f044-49f3-85fc-67cdb6278f55","scrolled":false,"_uuid":"e2f924c58b7055d8c16c4d37cc897da9a3c2ab65"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TheilSenRegressor and MLPRegressor take the longest time to process but not with the best scores.","metadata":{"_cell_guid":"6b52a6c8-c59b-49de-ba76-951a215affbf","_uuid":"b95474d0d539d29f2f96d611a94b692f2ff1834b"}},{"cell_type":"code","source":"# Plot the scores for all the regressor\npd.Series(rgs_dict).sort_values(ascending=False).plot.barh(figsize=(15, 10), grid=True, fontsize=12)\nplt.xticks(np.arange(0, 600, 50))\nplt.xlabel('Mean Median Absolute Error from Cross Validation')\nplt.show()","metadata":{"_cell_guid":"01983ad0-8278-4806-84bc-04757422b095","scrolled":false,"_uuid":"c498a34b447e3ac0a2a9b5658929c03b58237ffe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For regressors, we can use two benchmark models: the dummy regressor which always predicts the median value of the training set and the plain linear regression mode. Interestingly, some regressors perform worse than the dummy regressor (random guess): NuSVR and MLPRegressor. This doesn't necessarily mean that these two algorithms are not good. It only suggests that the default parameter setting doesn't give the model good performance on this dataset. Similarly, those regressors that have larger error than the linear regression method don't necessarily mean they are bad regressors. It is possible that some regressors have a great potential to improve by adjusting its parameters. However, one cannot explore all the regressors with all possible values of parameters. I will choose investigate further only for three regressors on the top list: Bagging Regressor, Gradient boosting Regressor and Random Forest Regressor. Note that the result may be slightly different for each run, but these three are always on the top list. ","metadata":{"_cell_guid":"a1fbb7a2-ed02-4ecc-bb76-fb7bf99dc838","_uuid":"2a62d0669af3f0bde36be55e1a6551323407163c"}},{"cell_type":"code","source":"rfr_params = {'n_estimators':[20, 30, 40], 'min_samples_leaf':[1, 2, 3]}\nbr_params = {'n_estimators':[20, 30, 40], 'max_samples':[0.2, 0.5, 1.0], 'max_features':[0.2, 0.5, 1.0]}\ngbr_params = {'learning_rate':[0.02, 0.05, 0.08], 'n_estimators':[150, 200, 250], 'min_samples_leaf':[3, 4, 5], 'max_depth':[8, 9, 10]}","metadata":{"_cell_guid":"ca775af0-20ef-4f60-9693-14f7e49b0fb6","collapsed":true,"_uuid":"06aba79d10c890b7ad6548482e3bbe961aa72cac"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regressors = [RandomForestRegressor, BaggingRegressor, GradientBoostingRegressor] # Note that the regressor needs to be a function rather than the name itself.\nrgs_params = [rfr_params, br_params, gbr_params]\n\nbest_rgs = {}\nfor rgs, rgs_params in zip(regressors, rgs_params):\n    \n    grid = GridSearchCV(estimator = rgs(random_state=random_state), param_grid = rgs_params, scoring = mae_scorer, cv = cv_sets, n_jobs=-1, verbose=1)\n    grid.fit(X_train, y_train)\n    \n    print('It takes {:.1f} seconds to grid search the regressor {}. The median absolute error is {:.1f}.'.format(useTime, rgs.__name__, (-1) * grid.best_score_))\n    best_rgs[rgs.__name__] = grid.best_estimator_","metadata":{"_cell_guid":"ecea7cb3-d147-41d3-b74d-c3bbbdc9667d","scrolled":true,"_uuid":"274f58877c548afbab3c5dd4ada5059da09d7ac9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_rgs","metadata":{"_cell_guid":"9d299d5d-b116-46f0-bf89-ca1d49d5f788","_uuid":"730e3d8a929bf2b574f20cdc312a3d10f08245c8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bestRegressor = best_rgs['GradientBoostingRegressor']","metadata":{"_cell_guid":"5e5d944b-64df-48a3-b2be-efa2e6d110f3","scrolled":true,"collapsed":true,"_uuid":"06d738b7a0526aa075a7115c5fbcbd367eb3ec94"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict the daily trips using daily data\nbestRegressor.fit(X_train, y_train)\nSF_daily_y_pred = bestRegressor.predict(X_test)\n\nprint('The median absolute error is {:.1f} with the best estimator using the daily trip data.'.format(median_absolute_error(y_test, SF_daily_y_pred)))","metadata":{"_cell_guid":"b9289e4b-8f2f-4b83-9a32-898ba5624093","_uuid":"d4d255e5f82294c58dbba506ed435e7b949dd791"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3 Check if dividing a day into several sections help with the prediction power","metadata":{"_cell_guid":"79103a75-1fa3-4ace-89eb-d2bbfc431cf6","_uuid":"75fc1ef1f20a9a8bf0bb7be437abdf60c58210b4"}},{"cell_type":"markdown","source":"Divide by different hours. To predict the number of trips on a particular day, the trip data need to be averaged to certain time sections. From the exploration, I decided to divide the data by the noon point, i.e. one division from 0 am to 12 pm and the other from 1 pm to 11 pm. Aggregating data into certain time slots will make certain feature columns not relevant unless the each element in the feature will be investigated individually (e.g. investigate only the subscriber's behavior or only the non-subscriber's behavior). ","metadata":{"_cell_guid":"a8ff9b02-38f6-4c88-a681-87d5078ae215","_uuid":"ae2d388f8ce50b7aaadc73136f704de3e78871f4"}},{"cell_type":"code","source":"def pred_trip_count(trip_data, bestRegressor, train_index, test_index):\n    features = trip_data.drop('trip_counts', axis=1)\n    features[trip_num_features] = MinMaxScaler().fit_transform(features[trip_num_features])\n    counts = trip_data.trip_counts\n    \n    trip_X_train = features.loc[train_index]\n    trip_y_train = counts.loc[train_index]\n    trip_X_test =features.loc[test_index]\n    \n    bestRegressor.fit(trip_X_train, trip_y_train)\n    \n    # feature_importance = pd.Series(dict(zip(trip_X_train.columns, bestRegressor.feature_importances_)))\n    \n    return bestRegressor.predict(trip_X_test), bestRegressor.feature_importances_","metadata":{"_cell_guid":"5c286963-9834-4cc3-ae91-b540f2cd7482","collapsed":true,"_uuid":"8dd39ff911a8000e4389ef0e2f3055d3b3c517b3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SF_am = trip_count_rgs[94107][1]\nSF_pm = trip_count_rgs[94107][2]\n\ntrain_index = X_train.index\ntest_index = X_test.index","metadata":{"_cell_guid":"1c85594d-846f-4a60-ac9c-42ade2205222","collapsed":true,"_uuid":"d19cc7c73d1360ab606359f775aa091afd8aaeb0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict the daily trips using morning and afternoon data\nSF_am_y_pred, SF_am_importance = pred_trip_count(SF_am, bestRegressor, train_index, test_index)\nSF_pm_y_pred, SF_pm_importance = pred_trip_count(SF_pm, bestRegressor, train_index, test_index)\nSF_ampm_y_pred = SF_am_y_pred + SF_pm_y_pred","metadata":{"_cell_guid":"2a42c0a3-f6bf-4785-91df-bcb713e392e7","collapsed":true,"_uuid":"a2e8b4012aafb13dd2a89fe3f7c1d187595d70c0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the median value of the y train as the benchmark prediction\ny_pred_benchmark = np.ones(len(y_test)) * y_train.median()","metadata":{"_cell_guid":"54c1417c-bb94-4607-afa5-6aaf1b120582","collapsed":true,"_uuid":"4a997bfecd4f53fd6566f232de29f94a31374498"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the median absolute error metric.\nprint('The median absolute error is {:.1f} with the best estimator using the daily trip data.'.format(median_absolute_error(y_test, SF_daily_y_pred)))\nprint('The median absolute error is {:.1f} with the best estimator using the am+pm trip data.'.format(median_absolute_error(y_test, SF_ampm_y_pred)))\nprint('The median absolute error is {:.1f} with the benchmark model using the daily trip data.'.format(median_absolute_error(y_test, y_pred_benchmark)))","metadata":{"_cell_guid":"b18b6690-fe0f-4c9b-aef2-ac3d886014be","_uuid":"0501e690b321705e97ccbab9851eba59f85438be"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the root mean square logarithmic error metric.\nprint('The root mean square logarithmic error is {:.4f} with the best estimator using the daily trip data.'.format(np.sqrt(mean_squared_log_error(y_test, SF_daily_y_pred))))\nprint('The root mean square logarithmic error is {:.4f} with the best estimator using the am+pm trip data.'.format(np.sqrt(mean_squared_log_error(y_test, SF_ampm_y_pred))))\nprint('The root mean square logarithmic error is {:.4f} with the benchmark using the daily trip data.'.format(np.sqrt(mean_squared_log_error(y_test, y_pred_benchmark))))","metadata":{"_cell_guid":"89c186a7-4500-40e6-af0d-a43a9cf4dd5c","_uuid":"77acf26b1f078fc5f4c775cd0e1e7b58be5f2f34"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.4 Explore the feature importance of the best model for daily, am and pm trips","metadata":{"_cell_guid":"a36425e2-0718-4ca9-8c32-0b28fa9b8416","_uuid":"66e76fc5a428afcbcb74661f82b25a69e2117028"}},{"cell_type":"code","source":"# daily_importance = pd.Series(dict(zip(X_train.columns, bestRegressor.feature_importances_)))\ndaily_importance = bestRegressor.feature_importances_","metadata":{"_cell_guid":"169390e3-4bb8-4ad1-a04d-419a3c46a574","collapsed":true,"_uuid":"f628bb033c21e7364a30b3bd5f11647e7f500620"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the feature importances from the daily, am, pm trip models\n# Matplotlib code borrowed from: https://chrisalbon.com/python/matplotlib_grouped_bar_plot.html\nX_tick_labels = X_train.columns\npos = np.array(range(len(X_tick_labels)))\n\nplt.figure(figsize=(20, 5))\nax = plt.subplot(111)\nwidth = 0.25\nax.bar(pos-width, daily_importance, width=width, color='#EE3224', label='daily')\nax.bar(pos, SF_am_importance, width=width, color='#F78F1E', label='morning')\nax.bar(pos+width, SF_pm_importance,width=width,color='#FFC222', label='afternoon')\nax.legend()\n\n# Set the y axis label\nax.set_ylabel('Feature Importance')\n\n# Set the chart's title\nax.set_title('Feature Importance from Gradient Boosting Regressor')\n\n# Set the position of the x ticks\nax.set_xticks(pos)\n\n# Set the labels for the x ticks\nax.set_xticklabels(X_tick_labels, {'rotation': 'vertical', 'fontsize': 14})\n\n# Adding the legend and showing the plot\nplt.legend(loc='upper right')\nplt.grid()\nplt.show()","metadata":{"_cell_guid":"bff2718f-9cb8-4dbd-a556-a5e7b4169545","_uuid":"8f26c884e4f2d1165bbbbcc396a263896c6bb804"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The importances have almost the same distribution among features for the three time periods (daily, am, pm).\n\nAmazingly the wind degree and pressure are the two most important features for the Gradient Boosting Regressor. This is strange. I will explore the two features below.","metadata":{"_cell_guid":"f86b6347-1cfe-4104-93ad-aabfe5bef7cd","_uuid":"e23af84590ed1cf755399fba236c1661028202ab"}},{"cell_type":"code","source":"daily_trip_count = daily_trips['BDay']['count']\ndaily_wind_degree = trips_SF['Wind_Deg']\ndaily_pressure = trips_SF['Pressure']","metadata":{"_cell_guid":"32ae5599-bcb8-4470-8243-3f1c1f696ab9","scrolled":true,"collapsed":true,"_uuid":"4de5608a9cb8c7799bcf8757686d96a07887b33e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the daily trip count vs. the wind degree\nplt.plot(daily_wind_degree, daily_trip_count, '.')\nplt.xlabel('Wind Degree')\nplt.ylabel('Daily Trip Count')\nplt.title('Correlation between Trips and Wind Degree')\nplt.show()","metadata":{"_cell_guid":"93ac248f-0b7b-4360-884b-a85529c8295e","_uuid":"7715f62b43080ed633a26f942f3f230dcc516bef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Picture from this link: http://snowfence.umn.edu/Components/winddirectionanddegreeswithouttable3.htm\n## display(Image(filename='WindDirection.png', width=600))","metadata":{"_cell_guid":"06ae380e-1588-472a-9aa7-87e19350e965","collapsed":true,"_uuid":"3bda070daa86c5ed0978c69ee861731c9589e456"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"240-320 degrees corespond to WSW, W, WNW, NW. So most of the time the wind comes from the north west.","metadata":{"_cell_guid":"47ac54e3-3701-46ad-a5e2-656e101f26c6","_uuid":"fb1471cb2ac042b0b00ad40834fa1b225ce4f597"}},{"cell_type":"code","source":"plt.plot(daily_pressure, daily_trip_count, '.')\nplt.xlabel('Sea Level Pressure')\nplt.ylabel('Daily Trip Count')\nplt.title('Correlation between Trips and Sea Level Pressure')\nplt.show()","metadata":{"_cell_guid":"27af391f-d8d8-4a13-8bb0-5b3bfc026163","_uuid":"5093d984b8bab40228c03a012d843bb04429bb92"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similarly with the wind degree, the sea level pressure also concentrates in a short range. ","metadata":{"_cell_guid":"9cc29690-c9c6-458e-a07c-e32443fbbf68","_uuid":"adba6635277a7f101a85db9a348254e8a30b3473"}},{"cell_type":"markdown","source":"** Feature importance explanation. **\n\nThe Gradient Boosting Regressor is essentially a decision tree method. So its importance can be explained by that for decision tree. http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n>The feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance [R255](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#varimp).","metadata":{"_cell_guid":"7b638747-97a5-42cd-8a3f-4fd4d3154160","_uuid":"4ed24bac3300132aca5c1b7694fad9720156b0d0"}},{"cell_type":"code","source":"importance_series = pd.Series(dict(zip(X_train.columns, daily_importance))).sort_values(ascending=False).reset_index()\nimportance_series.columns = ['feature_regular', 'importance_regular']\ndisplay(importance_series)","metadata":{"_cell_guid":"ad462607-91f6-4d94-a46b-f1a52e219e69","scrolled":false,"_uuid":"13df9faed56a7996b5af373c5fe6143a53328086"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The wind degree and the sea level pressure have been explained above. These two features have very concentrated distribution in a short range. \n\nThe business day ranks the third and is the first feature that is not weather-related. The second non-weather feature is the weekday, ranking 9. The humidity, temperature and wind speed are undoubtfully important. Surprisingly, the cloud, precipitation, rain, fog features are not so important, which is contradicting to common sense. I can only explain it that these weather information may not apply to and have very limited influence on the whole day. For instance, it may fog or rain in the early morning or during night when the bike trips are rare.\n\nIn fact, the above interpretation may not be right. According to [this link](https://datascience.stackexchange.com/questions/16693/interpreting-decision-tree-in-context-of-feature-importances):\n> Just because a node is lower on the tree does not necessarily mean that it is less important. The feature importance in sci-kitlearn is calculated by how purely a node separates the classes (Gini index).\n","metadata":{"_cell_guid":"d56068fe-28f1-42b5-8811-c8ac08821418","_uuid":"ea7aae3920fc82181645a824cb468b01cd7423c0"}},{"cell_type":"markdown","source":"### 1.5 Test the performance treating the data as time series","metadata":{"_cell_guid":"0f881fef-ca93-4877-9334-42dd9689822b","_uuid":"9ee5a9348d6e44f6e3632ad8e7faf1abb6263804"}},{"cell_type":"code","source":"# before_time_split = trips_SF.index < time_split\n# after_time_split = trips_SF.index >= time_split\n\nsplit_index = int(features_SF.shape[0] * 0.8) + 1\nprint('The splitting date is {}.'.format(features_SF.index[split_index]))\n\nX_train_time = features_SF.iloc[:split_index]\ny_train_time = counts_SF[:split_index]\n\nX_test_time = features_SF[split_index:]\ny_test_time = counts_SF[split_index:]","metadata":{"_cell_guid":"181fe819-954b-461b-886e-fcdedb94100e","_uuid":"3711d347768e67a6e3cc247c3cab9aefb9fdb4f5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The percentage of the training set is {:.1f}% of the entire dataset.'.format(len(X_train_time) / len(trips_SF) * 100))","metadata":{"_cell_guid":"7740976d-3ad4-4925-a056-49c1166651b1","_uuid":"2dd852d7877867e50f889f81e6d9f347279a8f17"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbr_params","metadata":{"_cell_guid":"fcc65d36-628c-4129-a827-7366f4b4ec1b","_uuid":"225521063cf7151e7adffd6fc4bf23fbe31128df"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbr_params_time = {'learning_rate': [0.05, 0.06, 0.07],\n                   'max_depth': [5, 6, 7],\n                   'min_samples_leaf': [1, 2, 3],\n                   'n_estimators': [50, 100, 150]}","metadata":{"_cell_guid":"4db3c75a-2ad8-47d3-946f-645036bf3d09","collapsed":true,"_uuid":"7bd5ba1338a1ac53c5d9246a7b0f2c6ba091b20a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Grid search\nrgs_time = GradientBoostingRegressor(random_state=random_state)\ncv_sets_time = TimeSeriesSplit(n_splits=15)\ngrid_time = GridSearchCV(estimator = rgs_time, param_grid = gbr_params_time, scoring = mae_scorer, cv = cv_sets_time, n_jobs=-1, verbose=1)\ngrid_time.fit(X_train_time, y_train_time)\n    \nprint('The grid search of the Gradient Boosting Regressor gives the median absolute error {:.1f} for the best estimator.'.format((-1) * grid_time.best_score_))","metadata":{"_cell_guid":"3cfbcfa9-a88e-4c2b-abf2-9fded34d6f54","_uuid":"7e7698486c557fc5932b7ab28f8cb377b493c36b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Best model for time series training vs. regular training sets.\nbestRegressor_time = grid_time.best_estimator_\n\nprint('The best regressor for time-series splitted training set is:')\nprint(bestRegressor_time)\nprint('\\n')\nprint('The best regressor for regularly splitted training set is:')\nprint(bestRegressor)","metadata":{"_cell_guid":"3657cd9e-5eb4-419b-a2b4-3bff6b0d110c","_uuid":"daeefb08f91f0aad1f062ccc4c69b072992ea618"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see above, the best regressor for a normal training is different than the time series training.","metadata":{"_cell_guid":"cd007654-0312-47bc-99f7-a1153c420a8b","_uuid":"20c77e8f22dff69e3a3dea982c813812e0ae06b1"}},{"cell_type":"code","source":"y_pred_time = bestRegressor_time.predict(X_test_time)\n# Calculate the median value of the y train as the benchmark prediction\ny_pred_benchmark_time =  np.ones(len(y_test_time)) * y_train_time.median()\n\nprint('The median absolute error is {:.1f} with the best estimator for the time-series splitted test set.'.format(median_absolute_error(y_test_time, y_pred_time)))\nprint('The median absolute error is {:.1f} with the benchmark for the time-series splitted test set.\\n'.format(median_absolute_error(y_test_time, y_pred_benchmark_time)))\nprint('The root mean square logarithmic error is {:.4f} with the best estimator for the time-series splitted test set.'.format(np.sqrt(mean_squared_log_error(y_test_time, y_pred_time))))\nprint('The root mean square logarithmic error is {:.4f} with the benchmark for the time-series splitted test set.'.format(np.sqrt(mean_squared_log_error(y_test_time, y_pred_benchmark_time))))","metadata":{"_cell_guid":"2bbb93f9-fe07-42a6-b111-52c0c8c288eb","scrolled":false,"_uuid":"6d9989368d8250d42d6ef2abf8f2e8bd4a4513a9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The median absolute error is {:.1f} with the best estimator using the daily trip data.'.format(median_absolute_error(y_test, SF_daily_y_pred)))\nprint('The median absolute error is {:.1f} with the benchmark model using the daily trip data.'.format(median_absolute_error(y_test, y_pred_benchmark)))\nprint('The root mean square logarithmic error is {:.4f} with the best estimator using the daily trip data.'.format(np.sqrt(mean_squared_log_error(y_test, SF_daily_y_pred))))\nprint('The root mean square logarithmic error is {:.4f} with the benchmark using the daily trip data.'.format(np.sqrt(mean_squared_log_error(y_test, y_pred_benchmark))))","metadata":{"_cell_guid":"040e4525-9c00-4c37-b885-83969e050396","scrolled":true,"_uuid":"588b11113437c8c05c4416c088798e59dbd5acb7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show the robustness of the model\nscores = cross_val_score(rgs_time, X_train_time, y_train_time, cv=cv_sets, n_jobs=-1, scoring = mae_scorer)","metadata":{"_cell_guid":"4214ce33-7b47-4b4f-b642-1338d45c623d","collapsed":true,"_uuid":"8fb9aeb9348173fded7363cdc6e45254e1d782e1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"-scores","metadata":{"_cell_guid":"5d25a360-27ae-408f-b311-4f972e05a343","_uuid":"822d6a1b8c5be5d9dafa475c4505d9bc797fe24e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.6 Decision tree display","metadata":{"_cell_guid":"5a33620a-b9d3-488d-944e-05b286b49dc8","_uuid":"29cb6fa9083de5ad34ccd5a704a339ace191869a"}},{"cell_type":"code","source":"importance_time = pd.Series(dict(zip(X_train_time.columns, bestRegressor_time.feature_importances_))).sort_values(ascending=False).reset_index()\nimportance_time.columns = ['feature_time', 'importance_time']\ndisplay(pd.concat([importance_time, importance_series], axis=1))","metadata":{"_cell_guid":"b5d502ee-b6af-4789-aad2-c58a8e245e48","_uuid":"56a76c72557ed76d03a6f962ecac6b9e00f8c8de"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The feature importances for the time-series splitted dataset are nearly the same as those of the regularly splitted dataset.","metadata":{"_cell_guid":"1cb7a811-424e-4ff0-82a5-5096a9416ec3","_uuid":"208538b00dec6f1d150475eb4ad37295a853a287"}},{"cell_type":"code","source":"# https://stackoverflow.com/questions/44974360/how-to-visualize-an-sklearn-gradientboostingclassifier\n# Pick a decision tree number from the 50 estimators of the bestRegressor_time.\nsub_tree_16 = bestRegressor_time.estimators_[16, 0]","metadata":{"_cell_guid":"ddfdde08-cc1a-40de-82d9-5e4ad4058160","collapsed":true,"_uuid":"b19d874630341a1aee2db8e7b64ddd8a3a946104"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the decision tree map. Follow the instruction from the following link:\n# https://medium.com/@rnbrown/creating-and-visualizing-decision-trees-with-python-f8e8fa394176\n# Change the node color following the instruction from this link:\n# https://stackoverflow.com/questions/43214350/color-of-the-node-of-tree-with-graphviz-using-class-names/43218264#43218264\n\ndot_data = StringIO()\nexport_graphviz(sub_tree_16, out_file=dot_data, max_depth=3,\n                feature_names=X_train_time.columns,\n                filled=True, rounded=True, rotate=True,\n                proportion=True, special_characters=True)\n## graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n## graph.write_png('sub_tree_16.png')\n## Image(graph.create_png(), width=800)","metadata":{"_cell_guid":"35a13d08-c4a3-408e-ab1c-d115258f229d","collapsed":true,"_uuid":"029c699ad6175ed2a6fee92fcef5be7e9f35ef03"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One decision tree of the 50 estimators in the bestRegressor_time (Gradient Boosting Regressor) is displayed above with three depth layers to show the most important features. The higher the value of information, the darker the color.\n\nNote that the feature importance of the regressor is ranked upon the votes from all 50 decision trees. Not every tree necessarily obeys the order of importance. ","metadata":{"_cell_guid":"b0f9fc62-b28a-4dbe-bf7c-d01997932ff9","_uuid":"8cc0612fc76fd24a220de19db9a0643d35f41b4a"}},{"cell_type":"markdown","source":"### 1.7 Feature importance ranking","metadata":{"_cell_guid":"8e28f20a-b0c3-4d93-9661-1a085e2fa21b","_uuid":"ca938578b6155b4cdcf7025f038ab03e4c832696"}},{"cell_type":"markdown","source":"http://scikit-learn.org/stable/modules/feature_selection.html\nhttp://blog.datadive.net/interpreting-random-forests/\nThe Gradient Boosting Regressor is a tree-based algorithm. It outputs the feature importance that can be used for feature selection by taking out the least important features. However, the ranking of the importance doesn't necessarily be the right order. \n\nhttps://medium.com/@rnbrown/creating-and-visualizing-decision-trees-with-python-f8e8fa394176","metadata":{"_cell_guid":"fb69b863-2594-44d0-81e7-0eb22b957701","_uuid":"7310e0595e5e10f39caa3d4055080465e6cb05cc"}},{"cell_type":"code","source":"# LassoCV implementation\n# http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html\nlassocv_time = LassoCV(cv = cv_sets_time, random_state=random_state).fit(X_train_time, y_train_time)\nprint('The optimal Lasso model has the alpha {:.2f}.'.format(lassocv_time.alpha_))","metadata":{"_cell_guid":"bea4b8d9-e4ee-44ad-a2b4-670bc50a4e0a","_uuid":"b9e2fb36ea0931f07b8dd12ce17a77b0a561567f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bestLasso = Lasso_grid_time.best_estimator_\ny_pred_Lasso = lassocv_time.predict(X_test_time)\n\nprint('The median absolute error is {:.1f} with the best Lasso for the time-series splitted test set.'.format(median_absolute_error(y_test_time, y_pred_Lasso)))\nprint('The median absolute error is {:.1f} with the benchmark for the time-series splitted test set.\\n'.format(median_absolute_error(y_test_time, y_pred_benchmark_time)))\nprint('The root mean square logarithmic error is {:.4f} with the best Lasso for the time-series splitted test set.'.format(np.sqrt(mean_squared_log_error(y_test_time, y_pred_Lasso))))\nprint('The root mean square logarithmic error is {:.4f} with the benchmark for the time-series splitted test set.'.format(np.sqrt(mean_squared_log_error(y_test_time, y_pred_benchmark_time))))","metadata":{"_cell_guid":"45a3e263-ae35-408c-aee8-07256db78c15","_uuid":"c649950417fee461f84a9a3d028c909e63b61a48"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results with Lasso regressor is worse than the Gradient Boosting Regressor, but still much better than the naive predictor. So it still makes sense to use the coefficients of the Lasso regressor for feature ranking and interpretation.","metadata":{"_cell_guid":"7b244437-0b7a-4aa7-831f-e3d941fa961f","_uuid":"42da99f208ed7c5491071c8b0fb051a5fdb2f9eb"}},{"cell_type":"code","source":"# Show the robustness of the model\nscores = cross_val_score(lassocv_time, X_train_time, y_train_time, cv=cv_sets, n_jobs=-1, scoring = mae_scorer)","metadata":{"_cell_guid":"52c6975c-47ee-4cd2-bbd1-142593dc870e","collapsed":true,"_uuid":"31de3c64dbdf1182aa471153c8993ec2133dd6c7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"-scores","metadata":{"_cell_guid":"6ea1a80d-f792-461c-9fe2-5695572145aa","_uuid":"e1bf1a0ae22cae84584145a0247c1b1f6ced4f22"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obtain the coefficients from the Lasso fitting.\nimportance_Lasso = pd.Series(dict(zip(X_train_time.columns, lassocv_time.coef_))).reset_index()\nimportance_Lasso['abs_coef'] = np.abs(importance_Lasso[0])\nimportance_Lasso.columns = ['features', 'coefficient', 'abs_coef']\nimportance_Lasso_sorted = importance_Lasso.sort_values('abs_coef', ascending=False).set_index('features')","metadata":{"_cell_guid":"e1aaf204-a2f9-4690-8c96-01afff8f4cb0","collapsed":true,"_uuid":"2865b64ba7b1a260f611aeea96c6de6949cebed6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importance_Lasso_sorted.coefficient","metadata":{"_cell_guid":"f15b9a61-6dd5-4966-aabb-7e7b32244e31","_uuid":"7a00a21c748495f06f087d7a88e3642da2a2d878"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lasso is a linear regression method. If all feature columns are normalized to (0, 1), then the fitted coefficients become comparable. A feature coefficient can be further adjusted by the center of the most frequently appearing values, i.e. the mean. Considering that only when a feature having nonzero value does it contribute to the final result, the mean of a feature column will be calculated after all zero values are removed from the column.","metadata":{"_cell_guid":"ad8e8438-f657-4815-a769-dc08addd2f18","_uuid":"71c6777915c04f86c3d9343e13b02c2eac2e640a"}},{"cell_type":"code","source":"# Calculate the median of nonzero values in the feature column.\nfeature_mean = []\nfor col in X_train_time[importance_Lasso_sorted.index]:\n    feature_col = X_train_time[col]\n    feature_mean.append(feature_col.iloc[feature_col.nonzero()[0]].mean())\n\nimportance_Lasso_sorted['mean_adjusted'] = importance_Lasso_sorted['coefficient'] * feature_mean\nimportance_Lasso_sorted['abs_mean_adjusted'] = importance_Lasso_sorted['abs_coef'] * feature_mean","metadata":{"_cell_guid":"13f14c81-811b-4bd2-ab83-e6f29e24548b","collapsed":true,"_uuid":"2fcc74d7f3a82ab334e68baf4898c440e495cef7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importance_Lasso_sorted.plot.bar(figsize=(20, 5), ylim=(-600,600), grid=True, fontsize=14)\nplt.xlabel('')\nplt.ylabel('Coefficient')\nplt.show()","metadata":{"_cell_guid":"2d8df5f3-8319-43b3-913d-75e438658c87","_uuid":"349c1e8b102e102dc23e0956693aa026c8d69493"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.8 How does non-relevant feature removal impact the prediction?","metadata":{"_cell_guid":"f7c1077a-5c4d-446b-9955-3cf1c6582cf9","_uuid":"5b6666de4838e12d3f6e30534861e4ba7d9d52a9"}},{"cell_type":"code","source":"# Collect all the features that have the coefficient equal to zero. These features are considered not very relevant and will be dropped.\nfeatures_to_drop = importance_Lasso_sorted.index[importance_Lasso_sorted.coefficient == 0].tolist()\nprint(features_to_drop)","metadata":{"_cell_guid":"93bde228-0335-4e1c-b507-68e24bf12567","scrolled":true,"_uuid":"f4465208b519deaf09cb1624040ae3b38166abe9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_time_drop = X_train_time.drop(features_to_drop, axis=1)\nX_test_time_drop = X_test_time.drop(features_to_drop, axis=1)","metadata":{"_cell_guid":"1e0eeafd-6733-41c2-8499-341205e6512f","collapsed":true,"_uuid":"f9e7b91a1ba1f51965256780889bce19e5ff4ad5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_time_drop.head(1)","metadata":{"_cell_guid":"9633af69-3d25-4115-8273-e93138e4cf12","_uuid":"7fbe0366ec87ae55fa6db4f8eb899ddf3ff75dc8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbr_params_time","metadata":{"_cell_guid":"7af20d5e-5b46-4ea0-8c24-efef68064285","_uuid":"0c7746e01bc24cf4e1bbeb69d51d601c7a3a80dc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbr_params_drop = {'learning_rate': [0.06, 0.07, 0.08],\n                   'max_depth': [5, 6, 7],\n                   'min_samples_leaf': [1, 2, 3],\n                   'n_estimators': [150, 200, 250]}","metadata":{"_cell_guid":"12c4fe1d-9112-40da-a9ca-eb8f55df3305","collapsed":true,"_uuid":"946a02365b4d3bef1b5b6eae6decdfc70509aa15"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Do grid search for Gradient Boosting Regressor again to find the optimal hyperparameters.\n# With less features, it allows to allocate more computation resources to do the grid search.\nrgs_time_drop = GradientBoostingRegressor(random_state=random_state)\ngrid_time_drop = GridSearchCV(estimator = rgs_time_drop, param_grid = gbr_params_drop, scoring = mae_scorer, cv = cv_sets_time, n_jobs=-1, verbose=1)\ngrid_time_drop.fit(X_train_time_drop, y_train_time)\n\nprint('The grid search of the Gradient Boosting Regressor with irrelevant features dropped gives the median absolute error {:.1f} for the best estimator.'.format((-1) * grid_time_drop.best_score_))","metadata":{"_cell_guid":"8841f3be-a5ed-47c3-b8b7-f2cee993d951","_uuid":"2824909e67448795d82c7d343f1ec22e3beee771"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compare with the full training set.\nbestRegressor_time_drop = grid_time_drop.best_estimator_\ny_pred_time_drop = bestRegressor_time_drop.predict(X_test_time_drop)\n\nprint('The median absolute error is {:.1f} with features dropped.'.format(median_absolute_error(y_test_time, y_pred_time_drop)))\nprint('The median absolute error is {:.1f} for the full time-series splitted test set.'.format(median_absolute_error(y_test_time, y_pred_time)))\nprint('The median absolute error is {:.1f} with the benchmark for the time-series splitted test set.\\n'.format(median_absolute_error(y_test_time, y_pred_benchmark_time)))\n\nprint('The root mean square logarithmic error is {:.4f} with features dropped.'.format(np.sqrt(mean_squared_log_error(y_test_time, y_pred_time_drop))))\nprint('The root mean square logarithmic error is {:.4f} for the full time-series splitted test set.'.format(np.sqrt(mean_squared_log_error(y_test_time, y_pred_time))))\nprint('The root mean square logarithmic error is {:.4f} with the benchmark for the time-series splitted test set.'.format(np.sqrt(mean_squared_log_error(y_test_time, y_pred_benchmark_time))))","metadata":{"_cell_guid":"ee10e42b-09b7-43d9-942e-cc3dc4f9f69c","_uuid":"7583600c47f9b5402355a8faa02adb07524405cb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_time_drop.best_params_","metadata":{"_cell_guid":"1ed1b977-1e41-42d1-a199-ba7b151826a5","_uuid":"6d29969cda4940af214616b946dd0d40e55ae161"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Classfication Problem for Subscriber Type Prediction","metadata":{"_cell_guid":"4de0ef23-9a89-4b69-af5e-1c456bbfabce","_uuid":"0a95d6e337b0117e15fd3b8baa3b418c37680b4b"}},{"cell_type":"code","source":"# Show the datasets.\nsubscriber_cls.head(3)","metadata":{"_cell_guid":"7722b3fc-3814-4fe9-968a-7b32afe5046b","_uuid":"908e4b7527e7d21b4f9ec82d65c072d53e91a13b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1 Feature data scaling","metadata":{"_cell_guid":"a4c6c6f4-43b3-4ca0-96b8-ec1ee2092917","_uuid":"0bc92fa586c07e4095f35e6fd3d6c0b94cd9179b"}},{"cell_type":"code","source":"# Select feature columns that have numeric values except those with only 0 and 1. \ntrip_num_features_cls = ['duration', 'hour', 'weekday', 'month', 'Temp', 'Dew', 'Humid', 'Pressure', 'Max_Vis', 'Mean_Vis', 'Min_Vis', 'Max_Wind', 'Mean_Wind', 'Gust', 'Precip', 'Cloud', 'Wind_Deg']\nlen(trip_num_features_cls)","metadata":{"_cell_guid":"c4e90984-1921-46c2-a6a3-5a0a3e900ca7","_uuid":"7ed18af98d7919c0b22283a642a3ec362ed6bc3e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show the distribution of data for each feature.\nsubscriber_cls[trip_num_features_cls].plot(kind='box', subplots=True, layout=(3,6), figsize=(20, 10))\nplt.show()","metadata":{"_cell_guid":"97a7d680-5040-4d6e-8261-7a828d1accd7","_uuid":"9be153b6a1e4b5cc05400e59449728a547bf1088"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The duration feature has a very skewed data distribution. The logarithm will be applied to it before normalization.","metadata":{"_cell_guid":"b2a91b20-5e5d-4d5c-9dac-460d3d3e1a55","_uuid":"d27caa07df33ea1e23ee6c99b8ec64161fc19390"}},{"cell_type":"code","source":"# Apply the logarithm to the duration feature. Then scale all features to the range of (0, 1).\ncls_scaler = MinMaxScaler() # Give this a name to use the inverse transform later\ncls_scaled = subscriber_cls.copy()\ncls_scaled['duration'] = np.log(cls_scaled['duration'])\ncls_scaled[trip_num_features_cls] = cls_scaler.fit_transform(cls_scaled[trip_num_features_cls])","metadata":{"_cell_guid":"db96b49b-c6d5-46e0-99a7-6413b77008a9","collapsed":true,"_uuid":"ba3a32c8e64f98f6f6ad2a59d6f88128003df653"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the box-and-whisker plot for the scaled data. Now the data look more reasonable.\ncls_scaled[trip_num_features_cls].plot(kind='box', figsize=(20, 5))\nplt.show()","metadata":{"_cell_guid":"2a9306c8-7500-4a25-8860-dc6227582316","_uuid":"5aa0086ca6c9491c77794d44b3e16b3387a18021"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Generate training and testing sets","metadata":{"_cell_guid":"a79e98ed-7702-47bd-8a17-667da25ee65a","_uuid":"67c0c64869506aa6fe1204ed119f4d06fb33ca16"}},{"cell_type":"code","source":"# Generate the feature (from the scaled data) and label columns.\nfeatures_cls = cls_scaled.drop('subscription_type', axis = 1)\nsubType_cls = (cls_scaled.subscription_type == 'Customer').astype('int')","metadata":{"_cell_guid":"236f0b97-e208-49eb-ab21-1842a7766953","collapsed":true,"_uuid":"e8adf3b4f66171899389b3aa6cc8193d07621af1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_cls.head(1) # Show the feature columns","metadata":{"_cell_guid":"c6b30896-e607-47bb-8929-a8e7a65dac35","_uuid":"02b48c3b7bbd64dedc9e204929c1cb023597b1b8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the 'features_cls' and 'subType' data into training and testing sets\nX_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(features_cls, subType_cls, test_size=0.2, random_state=random_state)\n\n# Show the results of the split\nprint(\"Training set has {} samples.\".format(X_train_cls.shape[0]))\nprint(\"Testing set has {} samples.\".format(X_test_cls.shape[0]))","metadata":{"_cell_guid":"b51c80a6-2f7d-4f76-b3d0-aa9ced71fe52","_uuid":"76ca6194b5c2e4948595229d24057df92a0939da"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_cls.shape","metadata":{"_cell_guid":"488b84aa-c6dd-40d2-bb2e-3bc288752ed3","_uuid":"3639e4a73b05ea1d943f43b4e5349a436c73807d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Set the scoring metric and the cross validation set.","metadata":{"_cell_guid":"1f64c51c-c5b9-4ca6-9c31-b345e5cb6474","_uuid":"41edf49f2e36b2fd2006d3ae7f20120ccb7d7239"}},{"cell_type":"code","source":"# Set the f0.5 as the scorer with more focus on the precision to predict the non-subscriber. The goal of this study is to figure out a special pattern for non-subscriber.\nbeta = 0.5\nf_score = make_scorer(fbeta_score, beta=beta)\n\n# Define a cross validation set with smaller number of split than for the classifier as even the downsampled data is still one order of magnitude than the regression data.\n# This will be used for full dataset training as well.\n# StratifiedKFold is used instead of the regular KFold to preserve the percentage of samples for each class. This is important for imbalanced datasets.\ncv_sets_cls = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)","metadata":{"_cell_guid":"259c489e-2fae-4210-b967-68153fc75301","collapsed":true,"_uuid":"af750cb882ce17923a33a843597dd3a7ff6b4a7e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 Downsample the training set for quick classifier exploration","metadata":{"_cell_guid":"8ca91826-2931-488e-a547-d0eaae19dc80","_uuid":"4216424259c2140c9e7f28d0d5ac121d8df90811"}},{"cell_type":"code","source":"# Define a function to downsample the data by removing adjacent similar samples.\n# The squared sum of element-wise difference between adjacent samples is calculated. Only those data with the squared sum above the threshold will be kept.\n# Iterate the process until the size of the data doesn't change more than min_diff from the previous run.\ndef downsample(data, threshold, min_diff):\n    before_size = len(data)\n    after_size = 0\n    while(before_size - after_size > min_diff):\n        before_size = len(data)\n        data = data[data.diff().pow(2).sum(axis=1) > threshold]\n        after_size = len(data)\n    return data","metadata":{"_cell_guid":"873b818c-29ad-4a6e-b4e5-f86c99764d45","collapsed":true,"_uuid":"b094844632ceb0804f0ce7c6a243b08de3d89c38"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Divide the X_train_cls data by the subscription type\ntrain_subs = X_train_cls[y_train_cls == 0]\ntrain_cust = X_train_cls[y_train_cls == 1]","metadata":{"_cell_guid":"f475ca92-49d3-4d5b-9e04-01b5f66a69fa","collapsed":true,"_uuid":"533db593ac4798824d51901daf087494024a7e3b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the number of unique values in each column\nunique_dict = {}\nfor col, data in X_train_cls.iteritems():\n    unique_dict[col] = len(data.unique())","metadata":{"_cell_guid":"b1b9c082-1db5-44f4-920a-6723ec3e0ca4","collapsed":true,"_uuid":"6ca58d67bb28f684c82c3a8e9a0d247b1afb4ac8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the uniqueness of all columns\nunique_dict","metadata":{"_cell_guid":"523d584d-f00f-49cd-8abc-ba63df6c5372","_uuid":"24aa201abdad4b70588a3a93d3879f296f9b0de0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort the column by the order of uniqueness. The less unique values a column has, the earlier the column will get sorted.\n# These features will contribute to square sum of difference substantially: even difference from one such feature will contribute 1 to the total.\n# Sorting the data with these features will help downsample the data to a small size while maintaining the sample variance necessary for finding good classifiers.\ncol_sort = sorted(unique_dict, key=unique_dict.get)\n\nsubs_sort = train_subs.sort_values(col_sort)\ncust_sort = train_cust.sort_values(col_sort)","metadata":{"_cell_guid":"3f8bba17-5525-49e9-9b13-72dc1adcb291","collapsed":true,"_uuid":"bb39fbb42fbbe4300f65723bedc02267b2ae5e51"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Set the allowed deviation from each feature to be 0.03. This suggests that within a column, two values will be considered to have similar impact on the prediction of the label if the value difference is less than 3% of the total range. This method is not ideal: if only one feature changes while the others remain the same, the deviation of that particular feature can be up to 0.51 before being considered different. Most likely the duration impact will be weakened as it changes in the smallest time scale (minute). The larger the time scale, the less frequent the feature will change value.\n\nNote that only columns with numeric values are counted. The deviation from features with only 0 and 1 values is just too big to be even considered. This means that all different combinations of these features will be kept.  ","metadata":{"_cell_guid":"53c07b7c-08d2-42d7-b1d6-cb9f7ca5ca59","_uuid":"3a479d3835797e4a37d8f93a96135da64ce19b0d"}},{"cell_type":"code","source":"len(trip_num_features_cls)","metadata":{"_cell_guid":"b16da5d4-1572-4e11-b366-52ed18df707a","_uuid":"006d82419fc8b8d5ac91fdaabf0402960df1f69d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dev = 0.03\nthreshold = (len(trip_num_features_cls) * dev) ** 2\nprint(threshold)","metadata":{"_cell_guid":"ec4a0945-8f4d-4fb4-b17c-e2f2c42345a6","_uuid":"aa209645782304f682f5d2d657742e7fcfb0db82"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Don't set the min_diff too small, otherwise the downsample function will take a longer time to converge.\n# Downsampling is done for data with different subscription type, respectively.\nsubs_sort_down = downsample(subs_sort, threshold=threshold, min_diff=10)\ncust_sort_down = downsample(cust_sort, threshold=threshold, min_diff=10)","metadata":{"_cell_guid":"510a698a-6e5f-49f8-a5af-1aca3ea9d016","_uuid":"345339e12696b58a131f6cc5bf8293b1b0743fad"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine the two downsized data to form the full data.\nX_train_down = pd.concat([subs_sort_down, cust_sort_down])\ny_train_down = y_train_cls.loc[X_train_down.index]\n\ndown_sample_size = len(X_train_down)\nprint('After downsampling, the data size shrinks by {:.1f}% to {}.'.format((1-down_sample_size/len(X_train_cls))*100, down_sample_size))","metadata":{"_cell_guid":"3c609b86-dc91-4f3a-ad96-1aa2604e660d","_uuid":"c2955af77afdc9d505af7d63f51cd93e809672c9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4 Explore proper classifiers for the down-sampled training set","metadata":{"_cell_guid":"743811e4-c400-4868-9bc7-e7d92a465e2e","_uuid":"f55a6053a952a72138eb521334c6e636e18c0d3c"}},{"cell_type":"code","source":"# The original functions comes from: https://www.kaggle.com/currie32/a-model-to-predict-number-of-daily-trips/notebook\n# Use all CPUs for the computation.\n# The mean is calculated instead of the median to be more comparable with the results from the GridSearchCV\ndef down_scoring(cls):\n    scores = cross_val_score(cls, X_train_down, y_train_down, cv=cv_sets_cls, n_jobs=-1, scoring=f_score)\n    return np.mean(scores)","metadata":{"_cell_guid":"bddd9329-2618-4327-90a1-b0a2941c4566","collapsed":true,"_uuid":"b2fef83f036e061cb57d5eb3f2ba771b584f2a27"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Check the performance of all regressors with default setting\n# Too slow or gives error: GaussianProcessClassifier, RadiusNeighborsClassifier. \nclassifiers = [DummyClassifier,\n               AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier,\n               LogisticRegression, PassiveAggressiveClassifier, RidgeClassifier, SGDClassifier,\n               GaussianNB,\n               KNeighborsClassifier, NearestCentroid,\n               MLPClassifier,\n               LinearSVC, NuSVC, SVC,\n               DecisionTreeClassifier, ExtraTreeClassifier,\n               XGBClassifier]\n\ncls_dict = {} # Create a classifier dictionary to record the classifier with its score\nfor cls in classifiers:\n    begTime = time() # Get the beginning time\n    \n    # All parameters are set by default\n    # Unfortunately, not all classifiers have a random_state classifier, so the ranking may change for different runs.\n    try:\n        cls_function = cls(random_state=random_state)\n    except:\n        cls_function = cls()\n    \n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        cls_score = down_scoring(cls_function)    \n        \n    useTime = time() - begTime\n    \n    cls_dict[cls.__name__] = (cls_score, useTime)\n    print('The classifier {} takes {:.2f} seconds and has a F0.5 score of {:.3f}.'.format(cls.__name__, useTime, cls_score))    ","metadata":{"_cell_guid":"ccf9a289-1efc-45ef-ab06-aec863b16755","scrolled":false,"_uuid":"83e4223b59a513d4369e6540b07529817dc01059"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cls = pd.DataFrame.from_dict(cls_dict, orient='index')\ndf_cls.columns = ['f_score', 'time_consumption']\ndf_cls.time_consumption = df_cls.time_consumption / 60 # Change the unit to minute\ndf_cls_sort = df_cls.sort_values('f_score')","metadata":{"_cell_guid":"3b4b4546-72f1-447e-813a-5b060ef156e0","collapsed":true,"_uuid":"bec1c6e3f0c83534b02fb3db02d47366c788d5fc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cls_sort.plot.barh(figsize=(15, 10), fontsize=14)\nplt.xlabel('F0.5_Score / Time Consumption (min)')\nplt.show()","metadata":{"_cell_guid":"d10ac1ad-4e31-44aa-ab11-f67a86ec3ab7","scrolled":false,"_uuid":"b07dfd22b1e46d32a26ea2068683a3de6abb8add"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is trickier to pick the right model for a large dataset. Both the performance and the complexity of a model will be considered: a high score is critical and a shorter training time is also important. A balance need to be found between them.\n\nThree classifiers will be picked out. I will follow these strategies:\n1. First, classifiers performing better than the average will be picked up.\n2. Normalize the F0.5 score on the good classifier group.\n3. Create a formula to balance the performance and the time consumption.\n4. Give more emphasis on the performance. Make sure two of the top three classifiers also rank in the top three following the new criterion.","metadata":{"_cell_guid":"fc105254-9b0a-44d4-9a47-f3f57070dae7","_uuid":"6432dd9e979f745747a3eae7ffd4d9962c9ff1dc"}},{"cell_type":"code","source":"# First select those classifiers that perform at least better than the mean. Then normalize values of the selected classifiers.\ndf_cls_2 = df_cls_sort.loc[df_cls_sort.f_score > df_cls_sort.f_score.mean(), :]\ndf_cls_2['f_score_norm'] = minmax_scale(df_cls_2.f_score)","metadata":{"_cell_guid":"75cec9af-26e3-4819-acec-4602ef8b0ac6","_uuid":"59addba5b9b9b4ac7f0e713b8ae2449b517e89c8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Give more emphasis on the performane by taking the cubic of each value.\n# Bagging Classifier ranks in the top three in both groups.\n(df_cls_2.f_score_norm ** 3 / df_cls_2.time_consumption).sort_values(ascending=False)","metadata":{"_cell_guid":"603106ae-d093-431f-a6f6-c50a84f252a5","_uuid":"bc9a5eec4b078df809e6b0d63d34d2b7ee156165"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Downsampling the training set undoubtfully helps with excluding those classifiers that are too time-consuming. It also gives an idea of the best-performing classifiers. We will check later if a good classifier will do well on both the full dataset and the downsized dataset.","metadata":{"_cell_guid":"a3b25151-3eac-4599-ab33-5b89b18426b2","_uuid":"b5a9fe805ad04636d94c619c8992145eb2c16176"}},{"cell_type":"markdown","source":"### 2.5 Find the optimal hyperparameters on the downsampled training data","metadata":{"_cell_guid":"c5cf9dcb-567e-4c89-b4c2-4ace0375b9e8","_uuid":"55a49b5b46a27ccc4933a7570504c7c0680a35ee"}},{"cell_type":"markdown","source":"#### 2.5.1  For sklearn classifiers","metadata":{"_cell_guid":"de17d772-0bc6-4bcc-b444-49093178dfbb","_uuid":"e7c8016e79ffd2f816b3b3ce73576bf212d90c9c"}},{"cell_type":"code","source":"# The XGBClassifier document can be found here: http://xgboost.readthedocs.io/en/latest/python/python_api.html\n# Tuning instruction: https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\nbc_params = {'n_estimators':[10, 30, 50], 'max_samples':[0.4, 0.5, 0.6, 0.7]} # For Bagging Classifier, the number of estimators shouldn't be much larger than the default (10), otherwise the training time can be untolerable.\nlr_params = {'penalty': ['l1', 'l2'], 'C': np.linspace(0.1, 1.0, 10)} # For Logistic Classifier\nrc_params = {'alpha': np.linspace(0.1, 1.0, 20)} # For Ridge Classifier","metadata":{"_cell_guid":"208fcabb-1557-4009-852d-cb1b3fc97fa9","collapsed":true,"_uuid":"f3479a394b00b2b53fc407fb5a7b7afe2e780822"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initiate a dictionary to hold the best classifiers\nbest_cls = {}","metadata":{"_cell_guid":"43a16cbe-3ac1-4177-a9ce-332230f4fc50","collapsed":true,"_uuid":"f4b726e24ecab1fd037cdd97b348a99e5bc20b46"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Randomized search will be utilized to save time as this dataset, though downsized, is still one order of magnitude than the regression set.\nclassifiers = [BaggingClassifier, LogisticRegression, RidgeClassifier] # Note that the classifier needs to be a function rather than the name itself.\ncls_params = [bc_params, lr_params, rc_params]\nn_iter_search = 10\n\nfor cls, cls_params in zip(classifiers, cls_params):\n    begTime = time()\n    \n    grid_down = RandomizedSearchCV(estimator=cls(random_state=random_state), param_distributions=cls_params, n_iter=n_iter_search, scoring = f_score, cv = cv_sets_cls, n_jobs=-1, verbose=1)\n    # The whole downsampled data is considered a \"training set\" for the full set, so all data will be used to capture the unique patterns.\n    grid_down.fit(X_train_down, y_train_down)\n    \n    useTime = time() - begTime\n    \n    print('It takes {:.1f} seconds to grid search the classifier {}. The median F0.5 score is {:.3f}.'.format(useTime, cls.__name__, grid_down.best_score_))\n    best_cls[cls.__name__] = grid_down.best_estimator_","metadata":{"_cell_guid":"05c4d638-f020-48c0-b852-913fd84616fe","_uuid":"15e50078b1400a2d616acd0b0464e672ed07ae3f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.5.2 For XGBClassifier","metadata":{"_cell_guid":"24096567-4c58-4030-992c-70b1f30391e5","_uuid":"d0ba26090e3cd5418cc8f2218a82ef67569b517c"}},{"cell_type":"markdown","source":"In on comparison run, the following data were recorded:\n* Using the CPU: 330 seconds\n* Using 'gpu_hist': 90 seconds\n* Using 'gpu_exact': 80 seconds\n\nFor small to medium dataset, exact greedy method is faster.\nFor very large-dataset, 'hist' method is supposed to be faster.\nUsing 'auto' actually makes the training slower.","metadata":{"_cell_guid":"1a34e869-0062-4529-a4d0-54486688fba1","_uuid":"216726a17da816197cdd22851b2b2e0cb9da7dbc"}},{"cell_type":"markdown","source":"I follow the instructions for the hyper-parameter tuning from these two webpages with a focus on controlling overfitting:\nhttps://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\nhttps://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\nhttp://xgboost.readthedocs.io/en/latest/model.html#","metadata":{"_cell_guid":"90804b24-ab0f-488a-8f17-1568db47873e","_uuid":"9ac0e5d5e36ce3f43a379b7e4ccd1210902651fa"}},{"cell_type":"markdown","source":"http://xgboost.readthedocs.io/en/latest/python/python_api.html\n\n> eval_metric (str, callable, optional) – If a str, should be a built-in evaluation metric to use. See doc/parameter.md. If callable, a custom evaluation metric. The call signature is func(y_predicted, y_true) where y_true will be a DMatrix object such that you may need to call the get_label method. It must return a str, value pair where the str is a name for the evaluation and value is the value of the evaluation function. This objective is always minimized.","metadata":{"_cell_guid":"7f169909-5da6-461f-88cf-28c84106f361","_uuid":"d9084e65f982384702b76bf046e56044ce54bc88"}},{"cell_type":"code","source":"# Define a customized evaluation function for the xgboost cross valiation -- the F0.5 score.\n# https://ajourneyintodatascience.quora.com/Custom-evaluation-function-and-early-stopping-for-xgboost-with-k-fold-validation-Python\ndef xgb_f_score(y_predicted, dtrain):\n    y_true = dtrain.get_label() # Use get_label() to obtain the y_true. Label means the label column.\n    y_pred = np.round(y_predicted) # The y_predicted is the sigmoid output. Rounding it will produce binary results.\n    score = fbeta_score(y_true, y_pred, beta=0.5)\n\n    return 'fbeta', score # Must return ('name', value)","metadata":{"_cell_guid":"dca88399-c026-47e8-8db6-e60f123e2e41","collapsed":true,"_uuid":"51db19ae398c3df2bc0b2b5fa2d818495b46324a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Don't set the n_jobs to -1 as the GPU will be used instead of the CPU.\n# The mean score is calculated to be comparable with the results from the GridSearchCV.\ndef xgb_scoring(cls, X_train, y_train):\n    scores = cross_val_score(cls, X_train, y_train, cv=cv_sets_cls, n_jobs=-1, scoring=f_score)\n    return np.mean(scores)","metadata":{"_cell_guid":"1fe67fa7-f5f5-4f2c-b933-4bfc651c0ef1","collapsed":true,"_uuid":"06b59448b6235813fe9a9bb1ef9777991feeffa0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First find the best number of estimators\n# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n# Use the customized evaluation function xgb_f_score. Remember to set the maximize to True.\n# Set the folds to be cv_sets_cls so that the result is reproducible. cv_sets_cls is a stratified K-Fold.\n# Note that the scoring is on the whole training dataset.\ndef modelfit(alg, X_train, y_train, useTrainCV=True, folds=cv_sets_cls, feval=xgb_f_score, stopping=50, verbose_eval=True):\n    \n    if useTrainCV:\n        print('Start the cross validation...')\n        xgb_param = alg.get_xgb_params()\n        xgtrain = xgb.DMatrix(X_train.values, label=y_train.values) # Convert the pandas dataframe to the xgboost data format for the cv\n        \n        # Unfortunately the cross-validation method in xgboost doesn't allow f_score as the metric. AUC should be a similar measure with the f_score.\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], folds=folds, \n                          feval=feval, maximize=True, early_stopping_rounds=stopping, verbose_eval=verbose_eval) \n        \n        alg.set_params(n_estimators=cvresult.shape[0])\n    \n    print('Start the scoring...')\n    xgb_f_score = cvresult['test-fbeta-mean'].max()\n    \n    #Print model report:\n    print(\"\\nModel Report\")\n    print(\"F0.5 Score of the whole training data: {:.4f}. \\n\".format(xgb_f_score))\n    print(alg)\n    \n    return xgb_f_score","metadata":{"_cell_guid":"0126270e-f222-40a1-bbcf-c8427735aec9","collapsed":true,"_uuid":"306a3cf3523dccffe99153658920bf375a75a2fe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Step 1: Fix learning rate and number of estimators for tuning tree-based parameters","metadata":{"_cell_guid":"8d39344c-59f8-4dc2-91df-38c785c11da7","_uuid":"6c80c9093b73abf648ed5b3b52415d003fae208d"}},{"cell_type":"code","source":"%%time\n# Initiate the XGBClassifier. Use 'gpu_exact' as this is a small dataset. Using 'gpu_hist' doesn't use as many GPU memory as the 'gpu_exact'\n# Note that the number of estimators is very large. This is the maximal round that the modelfit can try to find the optimal number of estimators..\n## init_params = {'tree_method': 'gpu_exact', 'predictor': 'gpu_predictor'}\ninit_params = {'predictor': 'cpu_predictor'}\nxgb_cls_0 = XGBClassifier(learning_rate= 0.1,\n                          n_estimators= 1000,\n                          max_depth= 5,\n                          min_child_weight= 1,\n                          gamma= 0,\n                          subsample= 0.8,\n                          colsample_bytree= 0.8,\n                          reg_alpha= 0,\n                          reg_lambda= 1,                          \n                          scale_pos_weight= 1,\n                          objective= 'binary:logistic',\n                          random_state=random_state, **init_params)\n\n# Find the best estimator number.\nxgb_cls_0_score = modelfit(xgb_cls_0, X_train_down, y_train_down, folds=cv_sets_cls, verbose_eval=False)","metadata":{"_cell_guid":"cdcd1ea4-8c8c-4487-a5c6-963eee6e31bc","_uuid":"12e50beb95d111bdeae617d83ef81d8b50a1009c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Step 2: Tune max_depth and min_child_weight","metadata":{"_cell_guid":"79f3301a-2eb9-42c0-b6b5-7b7d5862e07b","_uuid":"1b15707c6aeed744f206ab5dff9265c92d3edb79"}},{"cell_type":"code","source":"%%time\n# Note the parameters: https://github.com/dmlc/xgboost/blob/master/doc/parameter.md\n# Also check here: https://github.com/dmlc/xgboost/issues/2819\n# Grid search the 'max_depth' and 'min_child_weight' hyperparameters for the XGBClassifier using GPU to improve the training speed.\n\nxgb_params1={'max_depth': range(3,10,2), 'min_child_weight': range(1,6,2)}\nxgb_grid_1 = GridSearchCV(xgb_cls_0, param_grid=xgb_params1, scoring=f_score, \n                          cv=cv_sets_cls, n_jobs=-1, verbose=1)\nxgb_grid_1.fit(X_train_down, y_train_down)    \nxgb_cls_1 = xgb_grid_1.best_estimator_\n\nxgb_cls_1_score = xgb_grid_1.best_score_\nprint('F0.5 Score of the cross validation: {:.4f}. \\n'.format(xgb_cls_1_score))\nprint(xgb_cls_1)","metadata":{"_cell_guid":"6058e9ae-4911-42a7-9ff4-4d2e7e44a25a","_uuid":"56ab0f9ec424740e3bae564096c8a2c675cc57fd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_cls_1_score - xgb_cls_0_score","metadata":{"_cell_guid":"bdba90cf-5db3-42ec-bc0b-a96c1891fb69","_uuid":"e26dc70f0b04b15efafe2fe1f0b2e3e87261789f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Tune max_depth and min_child_weight in finer step","metadata":{"_cell_guid":"b6faee1f-3f9a-49af-b319-6e213d9878b8","_uuid":"b92605e25aa4f90b1904426d8fe8432338b9620a"}},{"cell_type":"code","source":"%%time\nxgb_params2 = {'max_depth':[6,7,8], 'min_child_weight':[4,5,6]}\nxgb_grid_2 = GridSearchCV(xgb_cls_1, param_grid=xgb_params2, scoring=f_score, \n                          cv=cv_sets_cls, n_jobs=-1, verbose=1)\nxgb_grid_2.fit(X_train_down, y_train_down)\nxgb_cls_2 = xgb_grid_2.best_estimator_\n\nxgb_cls_2_score = xgb_grid_2.best_score_\nprint('F0.5 Score of the cross validation: {:.4f}. \\n'.format(xgb_cls_2_score))\nprint(xgb_cls_2)","metadata":{"_cell_guid":"47e4c9e2-9e62-4168-9fde-4d09ecd35f9a","_uuid":"5afd44ba47fc69411b3c8b8955cdf351dfd5087c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_cls_2_score - xgb_cls_1_score","metadata":{"_cell_guid":"6975fed2-f655-4155-8ae9-9df5f529cb79","_uuid":"6af209d3950c1430884263afe12f8f38f3734eac"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Re-do the modelfit to find the best number of estimators","metadata":{"_cell_guid":"59893df2-16c6-43ac-913f-0a0e8146ca56","_uuid":"e9c13844090107351068992d0fb01eefd028580c"}},{"cell_type":"code","source":"# Copy all the parameters for xgb_cls_2 except setting the n_estimators to a large number.\nxgb_cls_2_b = XGBClassifier(learning_rate= 0.1,\n                            n_estimators= 1000,\n                            max_depth= 7,\n                            min_child_weight= 6,\n                            gamma= 0,\n                            subsample= 0.8,\n                            colsample_bytree= 0.8,\n                            reg_alpha= 0,\n                            reg_lambda= 1,\n                            scale_pos_weight=1,\n                            objective= 'binary:logistic',                            \n                            random_state=random_state, **init_params)\n\nxgb_cls_2_b_score = modelfit(xgb_cls_2_b, X_train_down, y_train_down, folds=cv_sets_cls, verbose_eval=False)","metadata":{"_cell_guid":"58c98e0e-de54-4af3-b970-e77b64168b4e","_uuid":"b07e9042341d90f236fd91ad704ccba3e3dfd456"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_cls_2_b_score - xgb_cls_2_score","metadata":{"_cell_guid":"c7044924-b79e-4ea5-9bd7-d5ba03d9c0ee","_uuid":"a8e820850caabdbfc3a3c9a02f32f739d03f8567"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Step 3: Tune gamma","metadata":{"_cell_guid":"be904b10-5305-4bee-967c-93505d9387eb","_uuid":"42edd36df0b81a5f90fac8b5820751e08c1ab344"}},{"cell_type":"code","source":"%%time\nxgb_params3 = {'gamma':[i/10.0 for i in range(0,5)]}\nxgb_grid_3 = GridSearchCV(xgb_cls_2_b, param_grid=xgb_params3, scoring=f_score, \n                          cv=cv_sets_cls, n_jobs=-1, verbose=1)\nxgb_grid_3.fit(X_train_down, y_train_down)\nxgb_cls_3 = xgb_grid_3.best_estimator_\n\nxgb_cls_3_score = xgb_grid_3.best_score_\nprint('F0.5 Score of the cross validation: {:.4f}. \\n'.format(xgb_cls_3_score))\nprint(xgb_cls_3)","metadata":{"_cell_guid":"1999ab19-5597-40ea-979f-7f4c01a42eb9","_uuid":"ec779992b4584942f75b9f3396c48e810725c143"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_cls_3_score - xgb_cls_2_b_score","metadata":{"_cell_guid":"c0464854-02a8-477c-afda-fb0fe7a89ea3","_uuid":"c6cea4fc126204c365b9942693982b5c1fc3d29c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Re-do the modelfit to find the best number of estimators","metadata":{"_cell_guid":"25116910-299a-45d5-ac4c-d8beb63bda34","_uuid":"39c634298623ed84f8f9afa7ee3547910dca8907"}},{"cell_type":"code","source":"# Copy all the parameters for xgb_cls_2 except setting the n_estimators to a large number.\nxgb_cls_3_b = XGBClassifier(learning_rate= 0.1,\n                            n_estimators= 1000,\n                            max_depth= 7,\n                            min_child_weight= 6,\n                            gamma= 0.4,\n                            subsample= 0.8,\n                            colsample_bytree= 0.8,\n                            reg_alpha= 0,\n                            reg_lambda= 1,\n                            scale_pos_weight=1,\n                            objective= 'binary:logistic',                            \n                            random_state=random_state, **init_params)\n\nxgb_cls_3_b_score = modelfit(xgb_cls_3_b, X_train_down, y_train_down, folds=cv_sets_cls, verbose_eval=False)","metadata":{"_cell_guid":"003be836-5ad1-40a4-968d-53c569aa1f2e","_uuid":"646de6745e1d0b87f97beb956e81ef049d70dd88"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_cls_3_b_score - xgb_cls_3_score","metadata":{"_cell_guid":"34ed7feb-b547-4bdb-b2d6-b4a17975185e","_uuid":"99130c280bc24f41238de03b0199fe6f3ed0d13a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Step 4: Tune subsample and colsample_bytree","metadata":{"_cell_guid":"65a53836-ba58-46d6-bdb0-4fee8e6f99e5","_uuid":"5d48c577c3b059dd726c44df38a5e8605da96f9f"}},{"cell_type":"code","source":"%%time\nxgb_params4 = {'subsample':[i/10.0 for i in range(6,10)], 'colsample_bytree':[i/10.0 for i in range(6,10)]}\nxgb_grid_4 = GridSearchCV(xgb_cls_3_b, param_grid=xgb_params4, scoring=f_score, \n                          cv=cv_sets_cls, n_jobs=-1, verbose=1)\nxgb_grid_4.fit(X_train_down, y_train_down)\nxgb_cls_4 = xgb_grid_4.best_estimator_\n\nxgb_cls_4_score = xgb_grid_4.best_score_\nprint('F0.5 Score of the cross validation: {:.4f}. \\n'.format(xgb_cls_4_score))\nprint(xgb_cls_4)","metadata":{"_cell_guid":"f73a55a3-e464-49c3-8ba8-9a17990c092a","_uuid":"2df6f20ef733a3995b6d1a45423bd45ffe0dac28"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_cls_4_score - xgb_cls_3_b_score","metadata":{"_cell_guid":"3e5ff870-0e5c-411c-ba0e-932437782983","_uuid":"3b6b7c775e554d916dba4d841d9a1396051e4a32"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Step 5: Tune regularization parameters","metadata":{"_cell_guid":"358afed3-c8db-44c8-996d-9cc8aa6ea709","_uuid":"546b79111f9e8a49ab01e6f6e850a049cc4d4947"}},{"cell_type":"code","source":"%%time\nxgb_params5 = {'reg_alpha':[0.01, 0.1, 1, 10], 'reg_lambda':[0.01, 0.1, 1, 10]}\nxgb_grid_5 = GridSearchCV(xgb_cls_4, param_grid=xgb_params5, scoring=f_score, \n                          cv=cv_sets_cls, n_jobs=-1, verbose=1)\nxgb_grid_5.fit(X_train_down, y_train_down)\nxgb_cls_5 = xgb_grid_5.best_estimator_\n\nxgb_cls_5_score = xgb_grid_5.best_score_\nprint('F0.5 Score of the cross validation: {:.4f}. \\n'.format(xgb_cls_5_score))\nprint(xgb_cls_5)","metadata":{"_cell_guid":"175e6ed1-be04-4470-a16d-4042d91383d8","_uuid":"05b5ba74d0a4e5262272ebe64737ab1307575ce9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_cls_5_score - xgb_cls_4_score","metadata":{"_cell_guid":"37d8855c-9abe-4203-b844-6c7e52a610ae","_uuid":"80f0103a5c826b53b66ec8205b161f7d7ad5173d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Tuning Regularization Parameters in a Smaller Step","metadata":{"_cell_guid":"42292fb5-09bd-4dcb-9911-bf1401d68b27","_uuid":"03424b3506b136121bff3fe9421983c2d1093b09"}},{"cell_type":"code","source":"%%time\nxgb_params6 = {'reg_alpha':[0.5, 1, 2, 3], 'reg_lambda':[0.5, 1, 2, 3]}\nxgb_grid_6 = GridSearchCV(xgb_cls_5, param_grid=xgb_params6, scoring=f_score, \n                          cv=cv_sets_cls, n_jobs=-1, verbose=1)\nxgb_grid_6.fit(X_train_down, y_train_down)\nxgb_cls_6 = xgb_grid_6.best_estimator_\n\nxgb_cls_6_score = xgb_grid_6.best_score_\nprint('F0.5 Score of the cross validation: {:.4f}. \\n'.format(xgb_cls_6_score))\nprint(xgb_cls_6)","metadata":{"_cell_guid":"96783a70-7000-4e3f-bb0b-b273c40e0b29","_uuid":"f566e506200745087d03e0a78b9d2e71ed0dd064"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_cls_6_score - xgb_cls_5_score","metadata":{"_cell_guid":"e05bf12c-113a-49f7-b2fd-ab792924e013","_uuid":"a7e1c90c3a08b8ebdcc95cdc893384b6252789a0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Re-do the modelfit to find the best number of estimators","metadata":{"_cell_guid":"a54d1bde-363b-4ffd-bf29-ec8615d37f8e","_uuid":"aab821d57270badf1f69d9e87f985e9298932b49"}},{"cell_type":"code","source":"xgb_cls_6_b = XGBClassifier(learning_rate= 0.1,\n                            n_estimators= 1000,\n                            max_depth= 7,\n                            min_child_weight= 6,\n                            gamma= 0.4,\n                            subsample= 0.6,\n                            colsample_bytree= 0.6,\n                            reg_alpha= 1,\n                            reg_lambda= 1,\n                            scale_pos_weight=1,\n                            objective= 'binary:logistic',                            \n                            random_state=random_state, **init_params)\n\nxgb_cls_6_b_score = modelfit(xgb_cls_6_b, X_train_down, y_train_down, folds=cv_sets_cls, verbose_eval=False)","metadata":{"_cell_guid":"cc6e8183-64fb-4104-8daf-e4a4fb95342f","_uuid":"68a2148470032c7c2280fe1632a2bc526241c29b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_cls_6_b_score - xgb_cls_6_score","metadata":{"_cell_guid":"65f41c8a-846b-4686-9810-aed84115400e","_uuid":"4a0bb48ed709b277b0ddf1f12f0a5f10d414d8b5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Step 6: Tune scale_pos_weight for balancing the positive and negative weight","metadata":{"_cell_guid":"f7afbaf6-f867-4f4f-be23-24e14fa9be50","_uuid":"2553afcf79aba95797d3dd0b2ca97a4027942552"}},{"cell_type":"code","source":"%%time\nxgb_params7 = {'scale_pos_weight': np.linspace(0.1, 1, 10)}\nxgb_grid_7 = GridSearchCV(xgb_cls_6_b, param_grid=xgb_params7, scoring=f_score, \n                          cv=cv_sets_cls, n_jobs=-1, verbose=1)\nxgb_grid_7.fit(X_train_down, y_train_down)\nxgb_cls_7 = xgb_grid_7.best_estimator_\n\nxgb_cls_7_score = xgb_grid_7.best_score_\nprint('F0.5 Score of the cross validation: {:.4f}. \\n'.format(xgb_cls_7_score))\nprint(xgb_cls_7)","metadata":{"_cell_guid":"eca3b6a6-49eb-4fb5-9c8d-8db2eb1fb1a8","scrolled":true,"_uuid":"dec81ed540aa1514b802edbd95af72016e24ce69"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_cls_7_score - xgb_cls_6_b_score","metadata":{"_cell_guid":"27714b04-0f3c-49ea-b0ea-a8b174fb97c5","_uuid":"caf482de4b351617031a2b65c374ee430615a50f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Tuning scale_pos_weight in a Smaller Step","metadata":{"_cell_guid":"86492211-2590-4970-a14f-e677f6432c99","_uuid":"74907544a9d4c942834b183203ad17110893f929"}},{"cell_type":"code","source":"%%time\nxgb_params8 = {'scale_pos_weight': np.arange(0.51, 0.69, 0.01)}\nxgb_grid_8 = GridSearchCV(xgb_cls_7, param_grid=xgb_params8, scoring=f_score, \n                          cv=cv_sets_cls, n_jobs=-1, verbose=1)\nxgb_grid_8.fit(X_train_down, y_train_down)\nxgb_cls_8 = xgb_grid_8.best_estimator_\n\nxgb_cls_8_score = xgb_grid_8.best_score_\nprint('F0.5 Score of the cross validation: {:.4f}. \\n'.format(xgb_cls_8_score))\nprint(xgb_cls_8)","metadata":{"_cell_guid":"3fe2437d-b3cb-451a-9d9d-6b13956809dd","scrolled":true,"_uuid":"b7925ba4c4a17618719a55ca5a99dfb1fd05d130"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_cls_8_score - xgb_cls_7_score","metadata":{"_cell_guid":"ce9458c5-fd6b-449a-8425-1e1993b9e2f7","_uuid":"67c0cbe37bc4d22c0067ba1646228296032dfc85"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Step 7: Optimizing Learning Rate","metadata":{"_cell_guid":"c6e9eba3-0a33-4b39-8835-5568f8d2ef78","_uuid":"8d98eae33a0132caa2a07a0edb0a0891772b71e1"}},{"cell_type":"code","source":"# Explore different learning rates using the modelfit, in place of the GridSearchCV.\nxgb_cls_9 = {}\nfor lr in [i/10.0 for i in range(1,4)]:\n    xgb_cls_temp = XGBClassifier(learning_rate= lr,\n                                 n_estimators= 1000,\n                                 max_depth= 7,\n                                 min_child_weight= 6,\n                                 gamma= 0.4,\n                                 subsample= 0.6,\n                                 colsample_bytree= 0.6,\n                                 objective= 'binary:logistic',\n                                 reg_alpha= 1,\n                                 reg_lambda= 1,\n                                 scale_pos_weight= 0.58,\n                                 random_state=random_state, **init_params)\n    \n    xgb_temp_score = modelfit(xgb_cls_temp, X_train_down, y_train_down, folds=cv_sets_cls, verbose_eval=False)    \n    xgb_cls_9[lr] = (xgb_cls_temp, xgb_temp_score)","metadata":{"_cell_guid":"71b54a42-61c4-4c03-86cf-a338f598dcf4","scrolled":true,"_uuid":"a20731cf8668e00f385f58ca2846d08a7ebcb764"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Tuning learning curve in a Smaller Step","metadata":{"_cell_guid":"6a62fc01-af0b-4053-b2c0-3f644ab57d67","_uuid":"04ae3b68373c0dafe612e70614cffe2998b82c48"}},{"cell_type":"code","source":"# Explore different learning rates using the modelfit, in place of the GridSearchCV.\nxgb_cls_10 = {}\nfor lr in [i/100.0 for i in range(12, 30, 2)]:\n    xgb_cls_temp = XGBClassifier(learning_rate= lr,\n                                 n_estimators= 1000,\n                                 max_depth= 7,\n                                 min_child_weight= 6,\n                                 gamma= 0.4,\n                                 subsample= 0.6,\n                                 colsample_bytree= 0.6,\n                                 objective= 'binary:logistic',\n                                 reg_alpha= 1,\n                                 reg_lambda= 1,\n                                 scale_pos_weight= 0.58,\n                                 random_state=random_state, **init_params)\n    \n    xgb_temp_score = modelfit(xgb_cls_temp, X_train_down, y_train_down, folds=cv_sets_cls, verbose_eval=False)    \n    xgb_cls_10[lr] = (xgb_cls_temp, xgb_temp_score)","metadata":{"_cell_guid":"b2d6bb7c-e770-47c8-abb9-a6bfc1a1bbd5","scrolled":true,"_uuid":"333af79a5ed5b5098cb18a27c5a9d282118e6b95"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_cls_10[0.2][1] - xgb_cls_8_score ","metadata":{"_cell_guid":"5360c589-9c3e-4d82-b6a7-10094c2c3ccc","_uuid":"3d74012dd39b5078ac590f8ffbc7255269ca942d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Step 8: Select the Best Predictor and Explore the Learning Curve","metadata":{"_cell_guid":"91c18c98-5ffd-4e00-ae28-d5887cb6857a","_uuid":"03b9e3e010658924f9611ea0083a765304a80ff1"}},{"cell_type":"code","source":"xgb_cls = xgb_cls_10[0.2][0]","metadata":{"_cell_guid":"34be559d-de93-4c65-87d3-8a4810be053e","collapsed":true,"_uuid":"fbfd7b28e8c199a5c6263926d64fa2b6e7ec02bf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a function to calculate the mean and standard deviation of nonzero values in a numpy array.\ndef nonzero_mean(np_array):\n    array_len = len(np_array)\n    mean_data_nonzero = np.zeros(array_len)\n    std_data_nonzero = np.zeros(array_len)\n    \n    for i in range(array_len):\n        nonzero_array = np_array[i][np_array[i].nonzero()]\n        if nonzero_array.any() == True:\n            mean_data_nonzero[i] = np.mean(nonzero_array)\n            std_data_nonzero[i] = np.std(nonzero_array)\n    \n    return mean_data_nonzero, std_data_nonzero","metadata":{"_cell_guid":"939e01e3-a973-47ff-bd68-2feb83d91bbf","collapsed":true,"_uuid":"7913997378355c3458cba8d08219cfd0f9239066"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_learning_curve(cls, X_train, y_train, train_sizes, scoring, cv=cv_sets_cls, verbose=1):\n\n    sizes, train_scores, test_scores = learning_curve(cls, X_train, y_train, cv=cv, \n                                                      n_jobs=-1, train_sizes=train_sizes, \n                                                      scoring=scoring, verbose=verbose)\n    \n    # Find the mean and standard deviation for smoothing.\n    train_mean, train_std = nonzero_mean(train_scores)\n    test_mean, test_std = nonzero_mean(test_scores)\n    \n    # Plot the learning curve \n    fig, ax = plt.subplots(figsize=(8, 10))\n    plt.plot(sizes, train_mean, 'o-', color = 'r', label = 'Training Score')\n    plt.plot(sizes, test_mean, 'o-', color = 'g', label = 'Testing Score')\n    plt.fill_between(sizes, train_mean - train_std, train_mean + train_std, alpha = 0.15, color = 'r')\n    plt.fill_between(sizes, test_mean - test_std, test_mean + test_std, alpha = 0.15, color = 'g')     \n    \n    plt.xlabel('Number of Training Samples')\n    plt.ylabel(str(scoring))\n    plt.title('Learning Performance of {}'.format(cls.__class__.__name__))\n    \n    minorLocator = AutoMinorLocator()\n    ax.yaxis.set_minor_locator(minorLocator)\n    \n    plt.grid(which='both')\n    plt.legend(loc='upper left')\n    plt.show()\n    \n    # return train_scores","metadata":{"_cell_guid":"35ac6bcb-54f6-4bf7-a89d-91627a08f70c","collapsed":true,"_uuid":"4030207155b2945aac503ac3ade5a15b8e4c3d74"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Generate the training set sizes increasing exponentially\n# train_sizes = np.geomspace(0.1, 1.0, num=9)\ntrain_sizes = np.linspace(0.1, 1.0, num=9)\n\n# Suppress the warnings due to an \"ill-defined\" f score\n# https://stackoverflow.com/questions/29086398/sklearn-turning-off-warnings\n# https://docs.python.org/2/library/warnings.html#temporarily-suppressing-warnings\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    plot_learning_curve(xgb_cls, X_train_down, y_train_down, train_sizes=train_sizes, \n                        cv=cv_sets_cls, scoring=f_score, verbose=1)","metadata":{"_cell_guid":"cbbfcc22-8ad2-4c2d-b82e-84e0d59a0705","scrolled":false,"_uuid":"fc9b1ed9128b287c568d935aad637d70014659ea"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Explore the accuracy learning curve.\nplot_learning_curve(xgb_cls, X_train_down, y_train_down, train_sizes=train_sizes, \n                    cv=cv_sets_cls, scoring='accuracy', verbose=1)","metadata":{"_cell_guid":"eed0ecf1-77af-4e0d-88b5-9e9602fe9f52","scrolled":false,"_uuid":"afb9cfc67a53f2c5edffb3236f8f2cfcb91ff58d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is very interesting to see how many training samples are required in order to obtain good performance on the the F0.5 score. We may have just enough samples for the training. On the other hand, additional training on the full dataset may still have some improvement. The learning curve will be investigated on the full dataset as well.","metadata":{"_cell_guid":"8c59e59b-c36f-4247-8d60-be47ee8edcff","_uuid":"26c7d116e34e2bbc9bff0aaa532019a80b22fe60"}},{"cell_type":"markdown","source":"### 2.6 Evaluate the classifiers on the entire datasets","metadata":{"_cell_guid":"6876b467-a754-4fa0-8c0d-26032bd52de5","_uuid":"481bf70a566238ae425a3f6818de9ce6c62be946"}},{"cell_type":"code","source":"xgb_cls","metadata":{"_cell_guid":"b9f62895-dbda-4abb-a813-f0f1d4ed7f5c","_uuid":"23fb404181c45202a6e3b85a101e64869c93060d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change the  use 'gpu_hist' instead of 'gpu_exact'.\n## init_params_full = {'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor'}\ninit_params_full = {'predictor': 'cpu_predictor'}\nxgb_cls = XGBClassifier(learning_rate= 0.2,\n                        n_estimators= 16,\n                        max_depth= 7,\n                        min_child_weight= 6,\n                        gamma= 0.4,\n                        subsample= 0.6,                         \n                        colsample_bytree= 0.6,\n                        reg_alpha= 1,\n                        reg_lambda= 1,\n                        objective= 'binary:logistic',\n                        scale_pos_weight= 0.58,\n                        random_state=random_state, **init_params_full)\n\nbest_cls[XGBClassifier.__name__] = xgb_cls","metadata":{"_cell_guid":"e913b20b-32b4-4a1b-989d-69e75be86c4f","collapsed":true,"_uuid":"cef73e94f1c5b4cf8ed16585cbd600299a27e386"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_cls","metadata":{"_cell_guid":"b1ed6fa0-13b0-4e0d-9445-ac60bf17993a","scrolled":true,"_uuid":"3b8053bdf6e275a34f66580d6149373c76afe963"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_predict(learner, X_train, y_train, X_test, y_test, best=False): \n    '''\n    inputs:\n       - learner: the learning algorithm to be trained and predicted on\n       - X_train: features training set\n       - y_train: income training set\n       - X_test: features testing set\n       - y_test: income testing set\n    '''\n    begTime = time()\n    \n    results = {}\n    \n    # Fit the learner to the training data\n    start = time() # Get start time\n    learner = learner.fit(X_train, y_train)\n    end = time() # Get end time\n    \n    # Calculate the total training time\n    results['train_time'] = end - start\n        \n    # Get the predictions on the test set(X_test), then get predictions on all training samples(X_train) using .predict()\n    pred_train_size = int(1 * len(X_train))\n    \n    start = time() # Get start time\n    predictions_test = learner.predict(X_test)\n    predictions_train = learner.predict(X_train[:pred_train_size])\n    end = time() # Get end time\n    \n    # Calculate the total prediction time\n    results['pred_time'] = end - start\n            \n    # Compute accuracy on all training samples\n    results['acc_train'] = accuracy_score(y_train[:pred_train_size], predictions_train)\n        \n    # Compute accuracy on test set using accuracy_score()\n    results['acc_test'] = accuracy_score(y_test, predictions_test)\n    \n    # Compute F0.5-score on all training samples using fbeta_score()\n    results['f_train'] = fbeta_score(y_train[:pred_train_size], predictions_train, beta=0.5)\n        \n    # Compute F0.5-score on the test set which is y_test\n    results['f_test'] = fbeta_score(y_test, predictions_test, beta=0.5)\n       \n    useTime = time() - begTime\n    # Success\n    if best == True:\n        print(\"Training with the classifier {} takes {:.1f} seconds.\".format((learner.__class__.__name__ + '_best'), useTime))\n    if best == False:\n        print(\"Training with the classifier {} takes {:.1f} seconds.\".format(learner.__class__.__name__, useTime))\n\n    # Return the results\n    return results","metadata":{"_cell_guid":"10f7c779-7cdd-477f-9dc5-900ab3377de6","collapsed":true,"_uuid":"94013d6ed0d8d03c65f26e61a12dc15407e7ce3b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_A = XGBClassifier(random_state=random_state, **init_params_full)\nclf_A_best = best_cls['XGBClassifier']\n\nclf_B = BaggingClassifier(random_state=random_state)\nclf_B_best = best_cls['BaggingClassifier']\n\nclf_C = LogisticRegression(random_state=random_state)\nclf_C_best = best_cls['LogisticRegression']\n\nclf_D = RidgeClassifier(random_state=random_state)\nclf_D_best = best_cls['RidgeClassifier']\n\n\n# Collect results on the learners\nresults = {}\nfor clf in [clf_A, clf_B, clf_C, clf_D]:    \n    clf_name = clf.__class__.__name__\n    results[clf_name] = {}\n    results[clf_name]['default'] = train_predict(clf, X_train_cls, y_train_cls, X_test_cls, y_test_cls)\n\n    \nfor clf in [clf_A_best, clf_B_best, clf_C_best, clf_D_best]:    \n    clf_name = clf.__class__.__name__\n    results[clf_name]['best'] = train_predict(clf, X_train_cls, y_train_cls, X_test_cls, y_test_cls, best=True)        ","metadata":{"_cell_guid":"beb1a6b7-7057-415d-904a-415512074c79","_uuid":"f9e9090a39e630d354656596d4036f396295217a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.7 Compare the default and optimized (using the downsampled training set) classifiers","metadata":{"_cell_guid":"be079584-0e2d-4e6a-9280-6b48f5a0dc20","_uuid":"2f4d848cb55f8a11ae3ed76dd2629d147af16c05"}},{"cell_type":"code","source":"# Visualization function borrowed from the 'finding_donors' project.\n# Modify the funtion to display the performance of each classifier with its default setting and the optimized hyper-parameters using the downsampled training set.\ndef evaluate(results, accuracy, f1):\n    \"\"\"\n    Visualization code to display results of various learners.\n    \n    inputs:\n      - learners: a list of supervised learners\n      - stats: a list of dictionaries of the statistic results from 'train_predict()'\n      - accuracy: The score for the naive predictor\n      - f1: The score for the naive predictor\n    \"\"\"\n  \n    # Create figure\n    fig, ax = plt.subplots(2, 3, figsize = (12, 8))\n\n    # Constants\n    bar_width = 0.47\n    # Check the color code from Google. https://www.google.com/search?q=color+%2300A0A0&oq=color+%2300A0A0&aqs=chrome..69i57.1192j0j7&sourceid=chrome&ie=UTF-8\n    # colors = ['#A00000','#08A000','#0092A0', '#8A00A0']\n    # colors = ['#E0F794','#F9EB90','#92B9FC', '#C092FC']\n    colors = ['#8fb21e','#ffdd00','#79a9fc','#8b6ab7']\n    \n    # Super loop to plot four panels of data\n    for k, learner in enumerate(results.keys()):\n        for j, metric in enumerate(['train_time', 'acc_train', 'f_train', 'pred_time', 'acc_test', 'f_test']):\n            for i, label in enumerate(['default', 'best']):\n                # Creative plot code\n                ax[j//3, j%3].bar(k+i*1.05*bar_width, results[learner][label][metric], width = bar_width, color = colors[k])\n                ax[j//3, j%3].set_xlabel(\"Classifier('default', 'best')\")\n                ax[j//3, j%3].set_xlim((-0.5, 4.0))\n    \n    # Add unique y-labels\n    ax[0, 0].set_ylabel(\"Time (in seconds)\")\n    ax[0, 1].set_ylabel(\"Accuracy Score\")\n    ax[0, 2].set_ylabel(\"F0.5-score\")\n    ax[1, 0].set_ylabel(\"Time (in seconds)\")\n    ax[1, 1].set_ylabel(\"Accuracy Score\")\n    ax[1, 2].set_ylabel(\"F0.5-score\")\n    \n    # Add titles\n    ax[0, 0].set_title(\"Model Training\")\n    ax[0, 1].set_title(\"Accuracy Score on Training Subset\")\n    ax[0, 2].set_title(\"F0.5-score on Training Subset\")\n    ax[1, 0].set_title(\"Model Predicting\")\n    ax[1, 1].set_title(\"Accuracy Score on Testing Set\")\n    ax[1, 2].set_title(\"F0.5-score on Testing Set\")\n    \n    # Add horizontal lines for naive predictors\n    ax[0, 1].axhline(y = accuracy, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n    ax[1, 1].axhline(y = accuracy, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n    ax[0, 2].axhline(y = f1, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n    ax[1, 2].axhline(y = f1, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n    \n    # Add horizontal lines for the best predictors\n    acc_test = []\n    acc_train = []\n    f_test = []\n    f_train = []\n    for _, result in results.items():\n        for _, values in result.items():\n            acc_test.append(values['acc_test'])\n            acc_train.append(values['acc_train'])\n            f_test.append(values['f_test'])\n            f_train.append(values['f_train'])    \n    \n    ax[0, 1].axhline(y = max(acc_train), xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dotted')\n    ax[0, 1].axhline(y = max(acc_test), xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'r', linestyle = 'dotted')\n    ax[1, 1].axhline(y = max(acc_test), xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dotted')\n    ax[0, 2].axhline(y = max(f_train), xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dotted')\n    ax[0, 2].axhline(y = max(f_test), xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'r', linestyle = 'dotted')   \n    ax[1, 2].axhline(y = max(f_test), xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dotted')\n    \n    \n    # Set y-limits for score panels\n    ax[0, 1].set_ylim((0, 1))\n    ax[0, 2].set_ylim((0, 1))\n    ax[1, 1].set_ylim((0, 1))\n    ax[1, 2].set_ylim((0, 1))\n\n    # Create patches for the legend\n    patches = []\n    for i, learner in enumerate(results.keys()):\n        patches.append(mpatches.Patch(color = colors[i], label = learner))\n    plt.legend(handles = patches, bbox_to_anchor = (-.80, 2.53), \\\n               loc = 'upper center', borderaxespad = 0., ncol = 4, fontsize = 'x-large')\n    \n    # Aesthetics\n    plt.suptitle(\"Performance Metrics for Four Supervised Learning Models\", fontsize = 16, y = 1.10)\n    plt.tight_layout()\n    plt.show()","metadata":{"_cell_guid":"9e6a5ca6-407a-4e2c-b2aa-71e0095b2edb","collapsed":true,"_uuid":"6bad118a7d9a02e25614705be1fd7acd4e06078e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the accuracy of a naive predictor that always predicts the subscriber as a benchmark for the accuracy.\n# Calculate the f0.5 score of a naive predictor that always predicts the non-subscriber as a benchmark for the f0.5 score.\naccuracy = np.sum(subType_cls)/float(subType_cls.count()) # If the classifier predicts all to be non-subscribers.\nsubscriber_accuracy = 1 - accuracy # If the classifier predicts all to be subscribers.\nrecall = 1\nprecision = accuracy\n\n# TODO: Calculate F-score using the formula above for beta = 0.5 and correct values for precision and recall.\n# HINT: The formula above can be written as (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\nbeta = 0.5\nfscore = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n\n# Run metrics visualization for the three supervised learning models chosen\nevaluate(results, subscriber_accuracy, fscore)","metadata":{"_cell_guid":"cf88190b-cc18-4d57-87cf-b5be335b1872","_uuid":"8ca812a107daa34dd85bfe10890438b3accd0aee"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above graph shows:\n1. The classifier optimization based on the downsampled training set helps improve both the accuracy and f0.5 score on the testing set for the two complex models (bagging and xgboost). The performance on the training set becomes worse, but this is a good sign as that suggests the optimized model tries not to overfit the training set. The performance of the logistic regression and the ridge regression doesn't change significantly before and after the optimzation.\n\n2. The red dotted line on the training set plot labels the best score on the testing set. We can see that the Bagging classifier tends to overfit, while the other three seem to perform well, with the logistic regression and the ridge classifier a bit underfitting.\n\n3. In terms of the overall performance, the xgboost is the best. The ridge regression ranks the third in F0.5 score, but the training and the predicting are pretty quick. So these two will be selected to be optimized further directly on the full dataset.\n\n4. The Bagging classifier takes too much time for the training and the predicting. Not a good classifier. The xgboost classifier is doing very well in time. With the power of the GPU, its training time is even shorter than the logistic regression; its predicting time is very close to both the logistic regression and the ridge classifier.","metadata":{"_cell_guid":"c5ac1ac9-b3a1-4bc0-9854-9fe42b27fcea","_uuid":"2943869d3a06bbcb40dcbb297fe70fd9609c9c6d"}},{"cell_type":"markdown","source":"### 2.8 Optimize the best classifiers on the entire dataset","metadata":{"_cell_guid":"48b08383-75ac-40a4-9cd5-f64c7cb5c7da","_uuid":"92a8870ceee5d7eb4bbc5dac3bd0ba3ed0001586"}},{"cell_type":"markdown","source":"##### Optimizing the XGBClassifier","metadata":{"_cell_guid":"fc7f61a4-f5f3-4f8f-8b68-29211fb8b727","_uuid":"daa20f9fb02b54315c743326b632fda5b4ec770e"}},{"cell_type":"markdown","source":"If the data is larger and more complex, to obtain the best model:\n1. The max_depth will be deeper / larger number.\n2. The learning rate is smaller.\n3. The number of estimator tends to be larger. Don't set that too high, otherwise the training speed is slower (almost a linear scale).\n4. The scale_pos_weight is an important parameter for imbalanced data, which is typical. Always use the stratified K-fold for the cross validation so that different splits of data will be similarly imbalanced.\n5. The gamma tends to be smaller.\n6. The subsample and colsample_bytree become more important.\n7. The reg_alpha and reg_lambda values can have more possibility, rather than just the plain 0 or 1.\n8. If the data is extremely imbalanced, max_delta_step should be tuned.","metadata":{"_cell_guid":"e5428439-60ee-41da-8902-214076996793","_uuid":"9afb727b43a403fd99d5828193341ec71ad2b6bb"}},{"cell_type":"markdown","source":"** Below is the XGBClassifier optimized for the entire dataset. Almost all parameters are different than that from the one trained on the downsampled dataset. The performance gets enhanced a lot. **","metadata":{"_cell_guid":"7a25a9bd-cd24-46aa-a37e-ccbce6b340d6","_uuid":"aaabb82bb661e6c1e6b84d910b379215c61d9353"}},{"cell_type":"code","source":"# For the full dataset, use 'gpu_hist' instead of 'gpu_exact'.\n## init_params_full = {'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor'}\ninit_params_full = {'predictor': 'cpu_predictor'}\nxgb_full = XGBClassifier(learning_rate= 0.05,\n                         n_estimators=250,\n                         max_depth= 9,\n                         min_child_weight= 5,\n                         gamma= 0,\n                         subsample= 0.9,                         \n                         colsample_bytree= 0.6,\n                         reg_alpha= 0.5,\n                         reg_lambda= 2,\n                         objective= 'binary:logistic',\n                         scale_pos_weight= 0.52,\n                         random_state=random_state, **init_params_full)","metadata":{"_cell_guid":"820244b9-224e-445a-aafd-7f76f7cc91fb","collapsed":true,"_uuid":"ec3dc4369282aee67d868b8eb2bdf28f3600b4e5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.geomspace(0.01, 1.0, num=9) * len(y_train_cls)","metadata":{"_cell_guid":"5c2d672e-dabc-45a7-92c3-4dbc6c91f317","_uuid":"e22157222bbc0007560365bf4e70733166d76e19"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate the training set sizes increasing exponentially. The whole process takes about 10 min.\ntrain_sizes = np.geomspace(0.01, 1.0, num=9)\n## plot_learning_curve(xgb_full, X_train_cls, y_train_cls, train_sizes=train_sizes, cv=cv_sets_cls, scoring=f_score, verbose=1)\n## Error from Kaggle: OSError: [Errno 28] No space left on device","metadata":{"_cell_guid":"7ba956f0-d49d-44ba-8a94-25b7329f9bed","scrolled":false,"collapsed":true,"_uuid":"140ed9589831b9fdcff9737b2e819a229c479e92"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_full","metadata":{"_cell_guid":"915a7502-6289-4f2b-996b-d536383463ee","_uuid":"cc1c18f6222ebc489d90ca1735f8fa76b2207a6e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is rather strange why XGBClasifier is doing this. The testing score seems to keep increasing with the number of training samples. The training score is doing normally when the number of training samples is relatively small (~ 50000), but the variance can become very large when there are more training samples. Is this the nature of the XGBClassifier?","metadata":{"_cell_guid":"e038d213-b436-42e2-8cf8-635765db342a","_uuid":"9e85d2e1763646a30c91d08c98f91de13eeb3015"}},{"cell_type":"code","source":"# Show the robustness of the model\n## scores = cross_val_score(xgb_full, X_train_cls, y_train_cls, cv=cv_sets_cls, n_jobs=-1, scoring=f_score)\n## OSError: [Errno 28] No space left on device","metadata":{"_cell_guid":"ab1de676-c6f4-40a8-aae8-c055ffa99a79","collapsed":true,"_uuid":"97c9de93f675475bbb046b99ecd7445116b85f90"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## scores","metadata":{"_cell_guid":"10e04c18-bc45-4109-a903-c1a339eab19a","collapsed":true,"_uuid":"19b49637312a949730caecf07cd0d6b26398929f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Optimizing the Ridge Classifier","metadata":{"_cell_guid":"fecbec66-394b-4b0b-9f9a-44dae42b0864","_uuid":"bdfd909cc39057ab7d51829dc1afabdcf914a86b"}},{"cell_type":"markdown","source":"%%time\n#### Find the best alpha for ridge classifier. The RidgeClassifierCV cannot be used as it will report an error for the f0.5 score.\nridge_full = RidgeClassifier(random_state=random_state)\n\nrc_params_full = {'alpha': np.linspace(0.1, 1.0, 5)}\n#### Remember to set the n_jobs to -1 to take the advantage of multiple CPUs.\nrc_grid_full = GridSearchCV(ridge_full, param_grid=rc_params_full, scoring=f_score, \n                            n_jobs=-1, cv=cv_sets_cls, verbose=1)\nrc_grid_full.fit(X_train_cls, y_train_cls)    \nrc_full = rc_grid_full.best_estimator_\n\nrc_full_score = rc_grid_full.best_score_\nprint('F0.5 Score of the cross validation: {:.4f}. \\n'.format(rc_full_score))\nprint(rc_full)","metadata":{"_cell_guid":"4d1e0769-0341-47b2-a4f7-bf42312ac431","collapsed":true,"_uuid":"8347eb79aeae141d6e72354d0a59a7c81a6d1b71"}},{"cell_type":"markdown","source":"No change in the alpha value, so the Ridge classifier doesn't get improved. Below I will tune the parameter in a finer step.","metadata":{"_cell_guid":"ddb88b38-1373-4ef3-9f1f-405ec32e174d","_uuid":"b89af08c6055a274a209d2e1ec06cb87912e6d7e"}},{"cell_type":"markdown","source":"%%time\nrc_params_full_b = {'alpha': np.linspace(0.8, 1.0, 5)}\n#### Remember to set the n_jobs to -1 to take the advantage of multiple CPUs.\nrc_grid_full_b = GridSearchCV(ridge_full, param_grid=rc_params_full_b, scoring=f_score, \n                              n_jobs=-1, cv=cv_sets_cls, verbose=1)\nrc_grid_full_b.fit(X_train_cls, y_train_cls)    \nrc_full_b = rc_grid_full.best_estimator_\n\nrc_full_b_score = rc_grid_full_b.best_score_\nprint('F0.5 Score of the cross validation: {:.4f}. \\n'.format(rc_full_b_score))\nprint(rc_full_b)","metadata":{"_cell_guid":"2683b110-c36b-4a78-89bd-b836cb9da79c","collapsed":true,"_uuid":"2ce871e529ec7e0a159ee46d5cdd7edd00735c71"}},{"cell_type":"markdown","source":"No change in the alpha value, so the Ridge classifier doesn't get improved. rc_full is the same as rc_full_b.","metadata":{"_cell_guid":"2b741bae-0382-4114-9300-59b2da3d2cb9","_uuid":"e948ba372651f9ecc851b22af8a4e10b87265108"}},{"cell_type":"markdown","source":"#### Generate the training set sizes increasing exponentially.\ntrain_sizes = np.geomspace(0.01, 1.0, num=9)\nplot_learning_curve(rc_full, X_train_cls, y_train_cls, train_sizes=train_sizes, \n                    cv=cv_sets_cls, scoring=f_score, verbose=1)","metadata":{"_cell_guid":"64591359-7c5e-4d21-9468-f50d48f1a136","scrolled":false,"collapsed":true,"_uuid":"421db92095437b9fd9cbf33617f8789f2f2be735"}},{"cell_type":"markdown","source":"The learning curve for the RidgeClassifier seems more normal than that for the XGBClassifier. When the training sample size is relatively small, there tends to be more variation for the testing score, then it gets improved. The training and testing scores converge at large number of training samples.","metadata":{"_cell_guid":"1e3062c1-ca5e-4f80-8c7a-f1cc22614c58","_uuid":"f8dfd8c3633c15e52d36b3c7caf7f99fae9b3ea2"}},{"cell_type":"markdown","source":"#### Show the robustness of the model\nscores = cross_val_score(rc_full, X_train_cls, y_train_cls, cv=cv_sets_cls, \n                         n_jobs=-1, scoring=f_score)","metadata":{"_cell_guid":"a1a9f474-d424-47af-a8ec-bbe8780166ae","collapsed":true,"_uuid":"9ae9069081376c56abce1b6330b942eec8582b36"}},{"cell_type":"code","source":"## scores","metadata":{"_cell_guid":"40d188cc-66c2-4cf3-9d11-07e8acb8c6e3","collapsed":true,"_uuid":"28bfce0b757612d1733e6280a1516d1e16b051fa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.9 Prediction on the testing set","metadata":{"_cell_guid":"785f5363-e3b2-4237-bbd0-c649dfc215f7","_uuid":"5a9f7626d12cffdc2212c74c06d4745c5225a560"}},{"cell_type":"code","source":"# rc_full is defined here as there is not enough resource on Kaggle's server.\nrc_full = RidgeClassifier(alpha=0.55000000000000004, class_weight=None, copy_X=True,\n                          fit_intercept=True, max_iter=None, normalize=False,\n                          random_state=16, solver='auto', tol=0.001)","metadata":{"_cell_guid":"d68097be-5305-470c-8975-34340742ae39","collapsed":true,"_uuid":"626af2dd9b0ac51c6968d9e48f05345ec278150e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Fit the training set with the best classifiers and predict on the X_test_cls data.\nxgb_cls.fit(X_train_cls, y_train_cls)\ny_pred_xgb_down = xgb_cls.predict(X_test_cls)\nfscore_xgb_down = fbeta_score(y_test_cls, y_pred_xgb_down, beta=0.5)\nprint('The F0.5 score on the testing set with trained model from the downsampled dataset is {:.4f}.'.format(fscore_xgb_down))\n\nxgb_full.fit(X_train_cls, y_train_cls)\ny_pred_xgb_full = xgb_full.predict(X_test_cls)\nfscore_xgb_full = fbeta_score(y_test_cls, y_pred_xgb_full, beta=0.5)\nprint('The F0.5 score on the testing set with the optimized XGBClassifier is {:.4f}.'.format(fscore_xgb_full))\n\nrc_full.fit(X_train_cls, y_train_cls)\ny_pred_rc_full = rc_full.predict(X_test_cls)\nfscore_rc_full = fbeta_score(y_test_cls, y_pred_rc_full, beta=0.5)\nprint('The F0.5 score on the testing set with the optimized RidgeClassifier is {:.4f}.'.format(fscore_rc_full))","metadata":{"_cell_guid":"adcb6c66-8c1e-488a-b8ea-bbd813f5dfac","_uuid":"085f2f77dd87ef70fa788196902ea6dc51c101bb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the confusion matrix with the log scale. The two classifiers have similar graphs, so I plot only one.\n# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\ncm = confusion_matrix(y_test_cls, y_pred_xgb_full)\n\nclasses = ['Subscriber', 'Non-Subscriber']\n\ncmap = plt.cm.Blues\nplt.imshow(np.log10(cm), interpolation='nearest', cmap=cmap)\nplt.colorbar()\ntick_marks = np.arange(2)\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\n\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(cm)","metadata":{"_cell_guid":"653a491c-e5f2-4341-8a2f-7ac1d8a25dd2","_uuid":"3031f1021253d40e6500aa16bed58eb2a5c9576d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show the classification report for both classifiers for comparison\nprint('A classification report for XGBClassifier trained with the downsampled: ')\nprint(classification_report(y_test_cls, y_pred_xgb_down))\nprint('\\n')\n\nprint('A classification report for XGBClassifier: ')\nprint(classification_report(y_test_cls, y_pred_xgb_full))\nprint('\\n')\n\nprint('A classification report for RidgeClassifier: ')\nprint(classification_report(y_test_cls, y_pred_rc_full))","metadata":{"_cell_guid":"c6a176d1-6ec6-47fb-9e2e-c2112827b8fb","_uuid":"5bb2e0ca5439d19fd9d03e9c8c8416c6a5f700db"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.10  Explore Feature Importances for Each Classifier","metadata":{"_cell_guid":"70c290b2-2b64-4993-9b48-1e5316b4cf7e","_uuid":"7754822f0558e39ae1bb34df51a90d968bdc1051"}},{"cell_type":"markdown","source":"##### Plot XGBClassifier Importances","metadata":{"_cell_guid":"c882e31a-82f8-4cd3-ac67-fff885b05268","_uuid":"842429bde4eda31f73f85b0c97554aa8e15e0da5"}},{"cell_type":"code","source":"# Plot the importance using xgboost's function. By Gain.\nplt.figure(figsize = (20, 15))\nax = plt.subplot()\nxgb.plot_importance(xgb_full, ax=ax, importance_type='gain', xlabel='The number of times a feature appears in a tree', height=0.5, grid=False)\nxgb_importance_gain = pd.Series(xgb_full.get_booster().get_score(importance_type='gain'))\nplt.show()","metadata":{"_cell_guid":"e95dfaaa-444b-4734-a3e4-82014b84063f","_uuid":"430680204916ed42591e91e893fc07746b9e3710"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the importance using xgboost's function. By Cover.\nplt.figure(figsize = (20, 15))\nax = plt.subplot()\nxgb.plot_importance(xgb_full, ax=ax, importance_type='cover', xlabel='The number of times a feature appears in a tree', height=0.5, grid=False)\nxgb_importance_cover = pd.Series(xgb_full.get_booster().get_score(importance_type='cover'))\nplt.show()","metadata":{"_cell_guid":"c271e5f7-fb6d-4940-94c9-ea34c5e6f92f","_uuid":"3fa6e1baa47bd044b5251f70228ff47779b41651"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the importance using xgboost's function. By Weight.\nplt.figure(figsize = (20, 15))\nax = plt.subplot()\nxgb.plot_importance(xgb_full, ax=ax, importance_type='weight', xlabel='The number of times a feature appears in a tree', height=0.5, grid=False)\nxgb_importance_weight = pd.Series(xgb_full.get_booster().get_score(importance_type='weight'))\nplt.show()","metadata":{"_cell_guid":"8f65b4a5-dfd4-4bf7-b4cf-d91d526c1367","_uuid":"2d046cfc227678f5991605474fab1a403e253301"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate all the importance series together to form a dataframe\nxgb_importance_df = pd.concat([xgb_importance_weight, xgb_importance_gain, xgb_importance_cover], axis=1)\nxgb_importance_df.columns = ['Weight', 'Gain', 'Cover']","metadata":{"_cell_guid":"41837eb1-34c0-4086-8481-48c052c72acd","collapsed":true,"_uuid":"c41034b0ad2d908dcc8a15537245a82cf5690f3a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change the gain and cover scales to logarithm to minimize the data skew.\nxgb_importance_df['LogGain'] = np.log10(xgb_importance_df.Gain)\nxgb_importance_df['LogCover'] = np.log(xgb_importance_df.Cover)","metadata":{"_cell_guid":"8b1973b1-d407-401c-a54c-a06639fc1f99","collapsed":true,"_uuid":"423d5e916ff26e867ffc7f40c5eabf354b818421"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize the three importances to \nxgb_importance_scaled = pd.DataFrame(MinMaxScaler().fit_transform(xgb_importance_df[['Weight', 'LogGain', 'LogCover']]))\nxgb_importance_scaled.index = xgb_importance_df.index\nxgb_importance_scaled.columns = ['Weight', 'LogGain', 'LogCover']","metadata":{"_cell_guid":"21fac10b-c7f0-4870-8836-30e25374aa6e","collapsed":true,"_uuid":"f470f4a9b76c1d25b06de126761a584ffd781a40"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_importance_scaled.sort_values('LogGain', ascending=False).plot.bar(figsize=(20, 5), grid=True, fontsize=14, )\nplt.ylabel('Normalized Importance')\nplt.show()","metadata":{"_cell_guid":"b9783a60-a291-4493-a2d2-e3d35bd106bd","_uuid":"eb41d94689b9bb4a2bdebacf28afdfccba14d52c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot a decision tree from the xgboost following the instruction below:\n# https://machinelearningmastery.com/visualize-gradient-boosting-decision-trees-xgboost-python/\n# A pdf file will be opened to see the details. \n# The xgboost plot function for python cannot specify the maximum depth to plot.\n\ntree_plot = xgb.to_graphviz(xgb_full, num_trees=16, rankdir='LR')\n# tree_plot.view()","metadata":{"_cell_guid":"a1b7fc37-1f0e-469e-b965-07a6bb85a718","collapsed":true,"_uuid":"fc14d17d4f36b5ef6416a51dd746f022f5932629"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dot_to_df(tree_body):\n    '''Extract the nodes information from a DOT file. The function returns a pandas series with two columns: \n       left nodes and right nodes'''\n    \n    edge_list = []\n    for line in tree_body:\n        try:\n            edge_left, edge_right = line.split('->')[0:2]\n        except:\n            continue\n            \n        node_left = int(edge_left)\n        node_right = int(edge_right.split()[0])\n        \n        edge_list.append((node_left, node_right))\n        \n    edge_df = pd.DataFrame(edge_list)\n    edge_df.columns = ['left_node', 'right_node']\n    \n    return edge_df","metadata":{"_cell_guid":"53071f44-39a1-4e17-aa57-fecdbd62dde9","collapsed":true,"_uuid":"633c6500e7be4fe1913bd5f4185b2699e119932b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def nodes_depth(tree_body, depth = 3):\n    '''This function collects the node indices up to the designated depths.'''\n    # Convert the DOT body code to a pandas DataFrame with the edge (node pair)\n    edge_df = dot_to_df(tree_body)\n       \n    d_nodes = [[0]]\n    for d in range(0, depth):\n        next_node = edge_df.loc[edge_df.left_node.isin(d_nodes[d]), 'right_node'].tolist()\n        d_nodes.append(next_node)\n        \n    return [node_point for d_node in d_nodes for node_point in d_node]","metadata":{"_cell_guid":"8df57306-1d49-4917-9323-c0107bb92ce2","collapsed":true,"_uuid":"5d6b2c616403bce0311e398d1ed443ddbd50bf0c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def change_depth(tree_body, depth = 3):\n    '''This function changes the depth of a decision tree to display via graphviz.\n       It returns a new DOT code with the designated depth for a decision tree.'''\n    \n    nodes_deep = nodes_depth(tree_body, depth = depth)\n    nodes_deeper = nodes_depth(tree_body, depth = depth + 1)\n    nodes_diff = list(set(nodes_deeper)^set(nodes_deep))\n\n    keep_line_idx = []\n    for idx, line in enumerate(tree_body):\n        item_list = line.split()\n        left_node = int(item_list[0])\n\n        if left_node in nodes_deep:\n            keep_line_idx.append(idx)\n        elif (left_node in nodes_diff) and (item_list[1] != '->'):\n            keep_line_idx.append(idx)\n    \n    new_lines = [tree_body[i] for i in keep_line_idx]\n    new_string = '\\n'.join(new_lines)\n    \n    new_dot = '\\n'.join(['digraph {', '\\tgraph [rankdir=LR]', new_string, '}'])\n    \n    return new_dot","metadata":{"_cell_guid":"ae2d04db-eaa1-4999-9d8e-fb388be1f956","collapsed":true,"_uuid":"c7e2ce4176e0656f2ff5b251c9adc689fb579766"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create new DOT source code\nnew_dot_code = change_depth(tree_plot.body, depth = 2)\n\n# Convert the DOT source code to a graph\n## graph = pydotplus.graph_from_dot_data(new_dot_code)\n## graph.write_png('xgb_tree_16.png')\n## Image(graph.create_png(), width=800)","metadata":{"_cell_guid":"f7e13b4c-d595-4c0d-b3df-7220f1e6109d","collapsed":true,"_uuid":"c28852f362b44d734edf734b93c735d1524f239a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Explore Important Splitting Points for Important Features","metadata":{"_cell_guid":"b1511608-d3d2-489c-bd57-328fedc143dc","_uuid":"dc20d14c1f2acac1f6ce9dfae1c2a1bc978e1ee1"}},{"cell_type":"code","source":"# Enumerate the numeric features to a dictionary\ncol_dict = {col: idx for idx, col in enumerate(trip_num_features_cls)}    ","metadata":{"_cell_guid":"2eff6b71-59e0-47f5-8016-20fc529184b3","collapsed":true,"_uuid":"d50cac95508d0e88751f26fc5e703cc56d8deea2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obtain the split value histogram for the duration feature\nxgb_split_duration = xgb_full.get_booster().get_split_value_histogram('duration')","metadata":{"_cell_guid":"a172dc46-d770-4d5b-9ff6-fe09e107dd2e","collapsed":true,"_uuid":"ec2698936a5d6c13ad585cdac27f925916003d40"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scale_col(x, col):\n    '''The function scales the data back for result interpretation.'''\n    scaler_min = cls_scaler.data_min_[col_dict[col]]\n    scaler_max = cls_scaler.data_max_[col_dict[col]]\n    return x * (scaler_max - scaler_min) + scaler_min","metadata":{"_cell_guid":"ff669b28-d192-42f6-ba15-a1acad2af960","collapsed":true,"_uuid":"2164ac975ebb1a534de1cd9f208d117a372d7160"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale back the duration data and change the unit to minute\nxgb_split_duration['duration_minute'] = xgb_split_duration.SplitValue.apply(lambda x: np.exp(scale_col(x, 'duration'))) / 60","metadata":{"_cell_guid":"1c389784-076c-43c5-9599-9ca842caf1a6","collapsed":true,"_uuid":"2223279e80c5e8b80deb6ca7d30d0c4bfce06a18"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use 5 min interval counts.\nxgb_split_duration['interval_5min'] = (xgb_split_duration.duration_minute // 5 + 1) * 5\nxgb_split_duration.groupby('interval_5min')['Count'].sum().plot.bar(figsize=(15, 5))\nplt.show()","metadata":{"_cell_guid":"c4cbd26f-bf2d-49a2-bad9-c78f4a16e90d","_uuid":"38fdc8033f1187674ff3b9487094a49a3b14763b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the mean value when duration is 10 min.\nxgb_split_duration.loc[xgb_split_duration.interval_5min == 10, 'SplitValue'].mean()","metadata":{"_cell_guid":"d75cea67-0395-448f-b424-52ad4300a2e3","scrolled":true,"_uuid":"5b233a6e1731195cdbcf561d9a5e433c2ead153f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This above graph shows that the most important splitting point for the duration is at 10 min. 5, 15, and 20 minutes are also important in different conditions.","metadata":{"_cell_guid":"9470f8f7-5622-4bc8-8cc6-761314f46acb","_uuid":"6309e3f9204c624244a2d53362c1368447b74014"}},{"cell_type":"code","source":"num_features = trip_num_features_cls[1:] # Don't include the duration as it has been explored above.","metadata":{"_cell_guid":"7b30b7aa-87d7-4f73-8211-0bdfd266b151","collapsed":true,"_uuid":"fd95728ce9b97de53aa9dc2c18657a1bf0b07959"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_div = 5 # number of division for the whole range\ninterval = 1 / num_div # the interval on the normalized values\n\nxgb_splits_max = {}\nxgb_splits_max['duration'] = 10\nplt_idx = 1 # The subplot index starts from 1\nr_color = lambda: random.randint(0,255) # function to generate random color: https://stackoverflow.com/questions/13998901/generating-a-random-hex-color-in-python\nplt.figure(figsize=(16, 16))\nfor col in num_features:\n    # Obtain the split values from the xgboost\n    split_hist = xgb_full.get_booster().get_split_value_histogram(col)\n    split_hist['interval'] = (split_hist['SplitValue'] // interval + 1) * interval # Interval starting not starting from zero.\n    split_hist['scaled'] = split_hist['interval'].apply(lambda x: scale_col(x, col))\n    \n    #xgb_splits[col] = split_hist.groupby('scaled')['Count'].sum()\n    split_group = split_hist.groupby('scaled')['Count'].sum()\n    xgb_splits_max[col] = split_group.idxmax()\n    \n    bar_width = 0.1 * (max(split_hist.scaled) - min(split_hist.scaled))\n    bar_color = '#%02X%02X%02X' % (r_color(),r_color(),r_color())\n    \n    plt.subplot(4, 4, plt_idx)\n    plt.bar(split_group.index, split_group, width=bar_width, color=bar_color)\n    plt.xlabel('%s' % col)\n    plt_idx += 1","metadata":{"_cell_guid":"09a2a38e-85d1-4a9a-a884-86eb5944e9bf","_uuid":"5864d0426332ba1e074e6e6206f429e3eaae5d79"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Only explore the first ten important features.\nxgb_importance = pd.Series(xgb_full.feature_importances_, index=X_train_cls.columns)\nxgb_splits_max = pd.Series(xgb_splits_max)\nxgb_unique = pd.Series(unique_dict)\n\nxgb_important_splits = pd.concat([xgb_importance, xgb_splits_max, xgb_unique], axis=1)\nxgb_important_splits.columns = ['Importance', 'MaxSplitPoint', 'Uniqueness']\nxgb_important_splits.sort_values('Importance', ascending=False)[0:10]","metadata":{"_cell_guid":"01f7edda-2e16-41d8-bfd3-2657e7c9fb2f","_uuid":"3b086ba9566fa7251c3955f0768eea9de8bb150a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Shown above is the most important splitting point for each feature for the 10 most important features for information gaining on decision trees. These ten features are all numerical, not categorical. This is because the importance is calculated by the times this features appears in the splitting. Numerical values appear more often as they have more different values to split than the categorical. In fact, the following plot with the feature value uniqueness vs. the feature importance shows a clear correlation. This suggests that the importance itself cannot be used to interpret the result. However, the most frequent splitting points, together with the graph of the decision tree, may give shed some light on the interpretation. ","metadata":{"_cell_guid":"371c26cb-1ea3-4127-aadc-6e90b912d2a0","_uuid":"abd141b556f26e9aa784681f0b8f1e96d55bb3e5"}},{"cell_type":"code","source":"# Plot the feature value uniqueness vs. the feature importance and find a strong correlation.\nplt.figure(figsize=(5, 5))\nplt.scatter(xgb_important_splits.Importance, np.log(xgb_important_splits.Uniqueness))\nplt.xlabel('Feature Importance')\nplt.ylabel('Logarithm of the Number of Unique Values')\nplt.show()","metadata":{"_cell_guid":"64538a34-8f22-44fa-9426-6d04e358794b","_uuid":"e187b00293b6140d7fc23b6ce41110ec7efaff55"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Result Interpretation from Ridge Classifier","metadata":{"_cell_guid":"d63f59d5-848b-4718-ac15-61406c0081d7","_uuid":"88a84093432c19a14c6940fb920fcc5ce54066c5"}},{"cell_type":"code","source":"# Obtain the coefficients from the Lasso fitting.\nimportance_Ridge = pd.Series(dict(zip(X_train_cls.columns, rc_full.coef_[0]))).reset_index()\nimportance_Ridge['abs_coef'] = np.abs(importance_Ridge[0])\nimportance_Ridge.columns = ['features', 'coefficient', 'abs_coef']\nimportance_Ridge_sorted = importance_Ridge.sort_values('abs_coef', ascending=False).set_index('features')","metadata":{"_cell_guid":"3b55a7e7-cd88-40b9-9ad7-115a1a91472f","collapsed":true,"_uuid":"2c2b4196a2f0290341aeb1aa9e36b7988c1dfd91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the mean of nonzero values in the feature column.\n# The feature importrance will be adjusted by multipling the mean value. \n# If a feature has a value distribute at the low value end (close to 0 in (0, 1) scale), the feature tends to be overemphasized on its coefficient. It will be adjusted by multipling the feature's mean value.\nfeature_mean_cls = []\nfor col in X_train_cls[importance_Ridge_sorted.index]:\n    feature_col_cls = X_train_cls[col]\n    feature_mean_cls.append(feature_col_cls.iloc[feature_col_cls.nonzero()[0]].mean())\n\nimportance_Ridge_sorted['mean_adjusted'] = importance_Ridge_sorted['coefficient'] * feature_mean_cls\nimportance_Ridge_sorted['abs_mean_adjusted'] = importance_Ridge_sorted['abs_coef'] * feature_mean_cls","metadata":{"_cell_guid":"a042903e-5a83-428a-81d6-751834f13895","collapsed":true,"_uuid":"e42b870bafbd2cea4fcf62b224e9fbee0e273066"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the coefficient column only to demonstrate the signs.\nimportance_Ridge_sorted[['coefficient', 'mean_adjusted']].plot.bar(figsize=(20, 5), ylim=(-0.8,0.8), grid=True, fontsize=14)\nplt.xlabel('')\nplt.show()","metadata":{"_cell_guid":"16a0bbb9-93c0-40e1-bda5-246341ab82fc","_uuid":"09ac1866bb69efea3114de5215cd68a1c246b491"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot columns with absolute values to demonstrate the relative importance. As the coefficient differ significantly from feature to feature, logarithm is used for better observation.\nimportance_Ridge_sorted[['abs_coef', 'abs_mean_adjusted']].plot.bar(figsize=(20, 5), ylim=(10e-5,3), grid=True, fontsize=14, logy=True)\nplt.xlabel('')\nplt.show()","metadata":{"_cell_guid":"b78a8f24-758f-4f0b-ad11-4f7fb7dd40c2","_uuid":"c5e51611db7e55e97434d2dbf117ea8ca0772a60"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.10 Remove the Unimportant Features","metadata":{"_cell_guid":"4cdbe76a-656a-4c2f-b6ce-69435d580eb5","_uuid":"51631eb391d1f718f015106d6fc778eb9fe1b56e"}},{"cell_type":"markdown","source":"##### Remove Unimportant Features Selected by Ridge Classifier","metadata":{"_cell_guid":"61863ab4-3cad-4dec-8035-2a0af19f7ff2","_uuid":"9b7d2edaec09345666d36af51ec5e4a945f42183"}},{"cell_type":"code","source":"# Absolute adjusted coefficients below 10% of the maximum coefficient will be considered not important.\nnot_important = max(importance_Ridge_sorted.abs_mean_adjusted) / 10\nnot_important_features = importance_Ridge_sorted[importance_Ridge_sorted.abs_mean_adjusted < not_important].index","metadata":{"_cell_guid":"ea29076d-0630-4446-a566-d66bc4caf1af","collapsed":true,"_uuid":"bf7031fa3774409516f8a983f32af90cff4f569b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_less = X_train_cls.drop(not_important_features, axis=1)\nX_test_less = X_test_cls.drop(not_important_features, axis=1)","metadata":{"_cell_guid":"f857c434-2b56-4dcd-8e78-a03e4a7e2639","collapsed":true,"_uuid":"56f7acb8cfa29979593c3f15777235c0179148a2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_less.columns","metadata":{"_cell_guid":"66d30552-a396-4191-9dcc-2775e812b322","_uuid":"a1df0219583f9f1584fc9bd2711aded7d916cb43"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Fit the training set with the best classifiers and predict on the X_test_cls data.\nxgb_full.fit(X_train_less, y_train_cls)\ny_pred_xgb_less = xgb_full.predict(X_test_less)\nfscore_xgb_less = fbeta_score(y_test_cls, y_pred_xgb_less, beta=0.5)\nprint('The F0.5 score on the testing set with the optimized XGBClassifier is {:.4f}.'.format(fscore_xgb_less))\n\nrc_full.fit(X_train_less, y_train_cls)\ny_pred_rc_less = rc_full.predict(X_test_less)\nfscore_rc_less = fbeta_score(y_test_cls, y_pred_rc_less, beta=0.5)\nprint('The F0.5 score on the testing set with the optimized RidgeClassifier is {:.4f}.'.format(fscore_rc_less))","metadata":{"_cell_guid":"ef9bcfba-2e0b-48bf-8d82-83edfb34be60","_uuid":"cb21d5c886c1257978ea706b31f94884bd5351ca"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Remove Unimportant Features (by weight) Selected from XGBClassifier","metadata":{"_cell_guid":"33c9e27e-bbda-4bf0-a687-e3415cb6b7d7","_uuid":"42802e63592c549dcf67a99cc8ba525cdcfc568f"}},{"cell_type":"code","source":"# Absolute adjusted coefficients below 10% of the maximum coefficient will be considered not important.\nnot_important_2 = max(xgb_importance) / 10\nnot_important_features_2 = xgb_importance[xgb_importance < not_important_2].index","metadata":{"_cell_guid":"d41f8dc1-7317-4da4-aa1d-a667745bfba8","collapsed":true,"_uuid":"69ae05844956e37b2707e12430db5ac3d596e2b3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_less_2 = X_train_cls.drop(not_important_features_2, axis=1)\nX_test_less_2 = X_test_cls.drop(not_important_features_2, axis=1)","metadata":{"_cell_guid":"c2df9f1d-0f20-470c-8091-51a31c556925","collapsed":true,"_uuid":"d21204a62583daadddc9b679be6ae3ed81170daf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_less_2.shape","metadata":{"_cell_guid":"b9a18b07-ccda-4c87-875f-0fd7310cecc8","_uuid":"60f71b027059a803e8c0e26a9248096c2befdc03"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Fit the training set with the best classifiers and predict on the X_test_cls data.\nxgb_full.fit(X_train_less_2, y_train_cls)\ny_pred_xgb_less_2 = xgb_full.predict(X_test_less_2)\nfscore_xgb_less_2 = fbeta_score(y_test_cls, y_pred_xgb_less_2, beta=0.5)\nprint('The F0.5 score on the testing set with the optimized XGBClassifier is {:.4f}.'.format(fscore_xgb_less_2))\n\nrc_full.fit(X_train_less_2, y_train_cls)\ny_pred_rc_less_2 = rc_full.predict(X_test_less_2)\nfscore_rc_less_2 = fbeta_score(y_test_cls, y_pred_rc_less_2, beta=0.5)\nprint('The F0.5 score on the testing set with the optimized RidgeClassifier is {:.4f}.'.format(fscore_rc_less_2))","metadata":{"_cell_guid":"5307a654-2e07-44ec-b6af-a6e21021f3b0","_uuid":"ad9aa81e333304fb25ca38c6dd98d9955a37a0f9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Compare all importance rankings.","metadata":{"_cell_guid":"e7f21dfd-491e-4732-98a6-34d35372dcd2","_uuid":"579474188d4b4acc2fc3d93eab735d91bcf2b66c"}},{"cell_type":"code","source":"Ridge_Importance = pd.Series(minmax_scale(importance_Ridge_sorted.abs_mean_adjusted))\nRidge_Importance.index = importance_Ridge_sorted.index","metadata":{"_cell_guid":"2ced6f24-6fb1-425b-82b2-b4082e2a4590","collapsed":true,"_uuid":"02c98d0b50aa82e21517c26ffc692c377e67742c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cls_importance = pd.concat([xgb_importance_scaled, Ridge_Importance], axis = 1)\ncls_importance.rename(columns = {0: 'Ridge'}, inplace=True)","metadata":{"_cell_guid":"7a0820d7-825c-44af-8433-23242742ca75","scrolled":true,"collapsed":true,"_uuid":"a988e950252604775c0d7fbded9b140a7328e770"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cls_importance.sort_values('Ridge', ascending=False).plot.bar(grid=True, figsize=(20, 5), fontsize=14)\nplt.show()","metadata":{"_cell_guid":"ea1dc8e8-9ce5-4e42-88bf-a8e0c7534071","scrolled":false,"_uuid":"0260e70d7f898c3bfcb8986ebbfab10ba4a5c6d2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.11 Importance Analysis on Dataset with Selected Features.","metadata":{"_cell_guid":"7bcc2e92-bcbc-4874-914a-7fbb20ae0227","_uuid":"3a0615a4c69bbe3a22029fc3c4c8ef1af4ff0c99"}},{"cell_type":"code","source":"# The unscaled dataset is used instead as tree-based method is not sensitive to value normalization.\nfeatures_sel = subscriber_cls.drop('subscription_type', axis = 1).drop(not_important_features, axis=1)\nsubType_sel = (subscriber_cls.subscription_type == 'Customer').astype('int')","metadata":{"_cell_guid":"efdb018f-7a70-45c1-96f8-0ce2e4eda1ac","collapsed":true,"_uuid":"311eb00cd1c2884f55a28a3a95bcc37e3277d785"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_sel.shape","metadata":{"_cell_guid":"1d941424-4b20-4a5c-aed8-f91e25683219","_uuid":"4664e40458d6054651237ed8743b650c79b60e2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the 'features_sel' and 'subType_sel' data into training and testing sets\nX_train_sel, X_test_sel, y_train_sel, y_test_sel = train_test_split(features_sel, subType_sel, test_size=0.2, random_state=random_state)\n\n# Show the results of the split\nprint(\"Training set has {} samples.\".format(X_train_sel.shape[0]))\nprint(\"Testing set has {} samples.\".format(X_test_sel.shape[0]))","metadata":{"_cell_guid":"9531dba0-6cb3-4f0d-8ef6-28f186391137","_uuid":"7e8c3df91d55f93a37afeb241a7f1fbb3697989a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_sel.shape","metadata":{"_cell_guid":"8e0b4bd1-555b-4e5a-8fb5-1e9caa45195a","_uuid":"56dd41cd53c797bf7957436baf732325145f0f7d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## init_params_sel = {'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor'}\ninit_params_sel = {'predictor': 'cpu_predictor'}\nxgb_sel = XGBClassifier(learning_rate= 0.1,\n                         n_estimators=88,\n                         max_depth= 7,\n                         min_child_weight= 1,\n                         gamma= 0,\n                         subsample= 0.9,                         \n                         colsample_bytree= 0.7,\n                         reg_alpha= 0,\n                         reg_lambda= 0,\n                         objective= 'binary:logistic',\n                         scale_pos_weight= 0.48,\n                         random_state=random_state, **init_params_sel)","metadata":{"_cell_guid":"9c708446-09ae-4cda-a166-df8228734dca","collapsed":true,"_uuid":"db95d7d85bd0dd1994ae88d1fbb8d5ac30a34df3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the performance of the optimized XGBClassifier on the testing set.\nxgb_sel.fit(X_train_sel, y_train_sel)\ny_pred_xgb_sel = xgb_sel.predict(X_test_sel)\nfscore_xgb_sel = fbeta_score(y_test_sel, y_pred_xgb_sel, beta=0.5)\nprint('The F0.5 score on the testing set with the optimized XGBClassifier is {:.4f}.'.format(fscore_xgb_sel))","metadata":{"_cell_guid":"82de4a0a-bd7a-43e8-8f70-e96d78988210","_uuid":"7df6a04dbeb6038ce3fd490af0c4440523f0c45b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_sel_imWeight = pd.Series(xgb_sel.get_booster().get_score(importance_type='weight'))\nxgb_sel_imGain = pd.Series(xgb_sel.get_booster().get_score(importance_type='gain'))\nxgb_sel_imCover = pd.Series(xgb_sel.get_booster().get_score(importance_type='cover'))","metadata":{"_cell_guid":"6532115f-4aef-4a11-9ffa-4090244b964d","collapsed":true,"_uuid":"a26e279c1c769e2c82e0592b42b6e67967584508"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate all the importance series together to form a dataframe\nxgb_imSel_df = pd.concat([xgb_sel_imWeight, xgb_sel_imGain, xgb_sel_imCover], axis=1)\nxgb_imSel_df.columns = ['Weight', 'Gain', 'Cover']","metadata":{"_cell_guid":"c46dcb96-85f0-4e3f-89e4-b32521de39ca","collapsed":true,"_uuid":"62a39c1749e89d0ad3ac7a4accfb57c22def0272"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change the gain and cover scales to logarithm to minimize the data skew.\nxgb_imSel_df['LogGain'] = np.log10(xgb_imSel_df.Gain)\nxgb_imSel_df['LogCover'] = np.log10(xgb_imSel_df.Cover)","metadata":{"_cell_guid":"82b57fc0-9143-449f-a9bb-4f2eae72eb77","collapsed":true,"_uuid":"9e69101dde32b8e5ad22cc52ed75069fb3af22b3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize the three importances to \nxgb_imSel_scaled = pd.DataFrame(MinMaxScaler().fit_transform(xgb_imSel_df[['Weight', 'LogGain', 'LogCover']]))\nxgb_imSel_scaled.index = xgb_imSel_df.index\nxgb_imSel_scaled.columns = ['Weight', 'LogGain', 'LogCover']","metadata":{"_cell_guid":"0122d7ab-3495-404d-ad69-eed057e40e82","collapsed":true,"_uuid":"e0d206091b7f254ec4a095202f3d0535eff176a3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Ridge_im = Ridge_Importance[Ridge_Importance >= not_important]\nxgb_imSel_comp = pd.concat([xgb_imSel_scaled, Ridge_im], axis=1)\nxgb_imSel_comp.rename(columns={0: 'Ridge'}, inplace=True)","metadata":{"_cell_guid":"3363b859-ef6e-40c3-a94a-3e079335347b","collapsed":true,"_uuid":"c283bcd8c7137e5d8d998d2e9b42fe972e63f009"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_imSel_comp.sort_values('Ridge', ascending=False).plot.bar(figsize=(20, 5), grid=True, fontsize=14, )\nplt.ylabel('Normalized Importance')\nplt.show()","metadata":{"_cell_guid":"740b0327-46ce-4801-81d2-5cbfc2c14330","_uuid":"4ba3b48d48a876fcf6e981429b9835d03e07578b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obtain the split value histograms for duration and hour\nxgb_sel_duration = xgb_sel.get_booster().get_split_value_histogram('duration') / 60\nxgb_sel_hour = xgb_sel.get_booster().get_split_value_histogram('hour')","metadata":{"_cell_guid":"f336b94a-3450-48a0-8151-e45af6a52d6f","collapsed":true,"_uuid":"df87114ac31d4f4f3d6f7587b0c39d32bd1b0022"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_interval = 5\nxgb_sel_duration['interval_5min'] = (xgb_sel_duration['SplitValue'] // time_interval + 1) * time_interval\nxgb_sel_duration.groupby('interval_5min')['Count'].sum().plot.bar(figsize=(15, 5))\nplt.show()","metadata":{"_cell_guid":"f5accfdb-8efc-4d93-b8b7-e5cea43e89ae","_uuid":"5229b80dc33308b018ef15fb5e75d896b7ca071b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hour_interval = 3\nxgb_sel_hour['interval_3h'] = (xgb_sel_hour['SplitValue'] // hour_interval + 1) * hour_interval\nxgb_sel_hour.groupby('interval_3h')['Count'].sum().plot.bar(figsize=(15, 5))\nplt.show()","metadata":{"_cell_guid":"775633b9-cc90-46a1-8880-e1ee43f97eb3","_uuid":"eecf85daad02aab4bb307bc6b80b21eb4782d139"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_sel_duration.head()","metadata":{"_cell_guid":"2155adf9-5aed-4b31-a5f9-ab5b8e1162f7","_uuid":"9b6ee18e2ca97a6b3de3de17bfac914688599b25"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.12 Explore PCA on the X_train_less","metadata":{"_cell_guid":"e23195a8-8a4e-460c-8e2d-0a5e26940054","_uuid":"5ad66c6990f41f0bcb77d4ee2b506d71bac57223"}},{"cell_type":"code","source":"# Explore the most important components when PCA is applied to X_train_less. Four components will explain more than 80% of the variance, so four should be good enough.\npca = PCA(n_components=4, random_state=random_state).fit(X_train_less)\nprint('Totally explained variance is {:.3f}.'.format(sum(pca.explained_variance_ratio_)))","metadata":{"_cell_guid":"a261438b-698a-49e9-8abd-d58a33f81e8a","_uuid":"4e7ed670911feb83b9696016898321f657e9e249"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the PCA components.\nplt_idx = 1\nplt.figure(figsize=(20, 5))\nfor idx, comp in enumerate(pca.components_):    \n    bar_color = '#%02X%02X%02X' % (r_color(),r_color(),r_color())\n    \n    plt.subplot(1, 4, plt_idx)\n    pd.Series(comp, index=X_train_less.columns).plot.bar(sharex=True, grid=True)\n    plt.title('Explain Variance Ration: {:.3f}'.format(pca.explained_variance_ratio_[idx]))\n    plt_idx += 1\n\nplt.show()","metadata":{"_cell_guid":"993adb89-5bc6-421f-a85c-2bdfc0b5e1ef","_uuid":"7c54078b2a1c759ddc87f53739649f8b0c2a2222"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.13 Can Deep Learning Improve the Prediction?","metadata":{"_cell_guid":"c6ff4883-aac9-4abd-99bf-bd618ea041d7","_uuid":"e4ec7cce6ee30f24fec449f85863534b39779450"}},{"cell_type":"markdown","source":"MLP for binary classification","metadata":{"_cell_guid":"94898f7d-18ac-4aae-9ff9-d2a4d1d9b097","_uuid":"d830a1a9dd8066fc86cc0f90d41a977dff44627a"}},{"cell_type":"code","source":"def get_available_devices():  \n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos]\n\n## with sys_pipes():\nprint(get_available_devices())","metadata":{"_cell_guid":"209c6499-3a6d-4de7-a52a-18b297741c73","_uuid":"8601f1a66594a18cf92ebae299b980a962d4d729"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Follow instructions in the following link:\nhttps://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/\n\n> To use Keras models with scikit-learn, we must use the KerasClassifier wrapper. This class takes a function that creates and returns our neural network model. It also takes arguments that it will pass along to the call to fit() such as the number of epochs and the batch size.","metadata":{"_cell_guid":"d48c918a-c3d9-4bfd-906c-02fe8c9808bd","_uuid":"0e98d8cd5e9ad350c872797eb95313a836e8ea75"}},{"cell_type":"markdown","source":"#### 2.12.1 Baseline Model","metadata":{"_cell_guid":"af69a5f9-c08d-4417-8d65-43bca4e660ea","_uuid":"320f9a3bfcd9bc8f5c763798be543c472e1cf813"}},{"cell_type":"markdown","source":"> Strangly: https://github.com/keras-team/keras/wiki/Keras-2.0-release-notes\nLosses & metrics: \nThe objectives module has been renamed losses.\nSeveral legacy metric functions have been removed, namely matthews_correlation, precision, recall, fbeta_score, fmeasure.\nCustom metric functions can no longer return a dict, they must return a single tensor.\nhttps://medium.com/@thongonary/how-to-compute-f1-score-for-each-epoch-in-keras-a1acd17715a2","metadata":{"_cell_guid":"97850236-4645-430a-b021-b61c04817e65","_uuid":"60136c54923f4585f10bab61a62cb73b92ce2406"}},{"cell_type":"code","source":"# Create a f0.5 metric function from keras following the code:\n# https://www.kaggle.com/arsenyinfo/f-beta-score-for-keras\ndef fbeta(y_true, y_pred):\n    beta = 0.5 # Set the beta to 0.5 to calculate the f0.5 score\n\n    # just in case of hipster activation at the final layer\n    y_pred = K.clip(y_pred, 0, 1)\n\n    # shifting the prediction threshold from .5 if needed\n    y_pred_bin = K.round(y_pred)\n\n    tp = K.sum(K.round(y_true * y_pred_bin)) + K.epsilon()\n    fp = K.sum(K.round(K.clip(y_pred_bin - y_true, 0, 1)))\n    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n\n    beta_squared = beta ** 2\n    return (beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall + K.epsilon())","metadata":{"_cell_guid":"7c5e46c5-643e-437d-887c-9d0a69910257","collapsed":true,"_uuid":"030ad9d5df202d3413d789031cecb16015c33f7f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_best_model(model, X_train, y_train, X_cross, y_cross, cp_filepath, class_weight=None, \n                   epochs=100, batch_size=1000, cp_verbose=0, fit_verbose=0):\n    '''The monitor is changed from the default 'val_loss' to 'fbeta' to monitor the F0.5 score change.\n    The mode must be set to 'max' as the fbeta score increases as the performance increases. \n    https://github.com/keras-team/keras/pull/188'''\n    \n    checkpointer = ModelCheckpoint(filepath=cp_filepath, monitor='val_fbeta', verbose=cp_verbose, \n                                   save_best_only=True, mode='max')\n    \n    history = model.fit(X_train, y_train, validation_data=(X_cross, y_cross),                        \n                        epochs=epochs, batch_size=batch_size, callbacks=[checkpointer], \n                        verbose=fit_verbose, class_weight=class_weight, shuffle=True)\n    \n    model.load_weights(cp_filepath)\n    \n    return history","metadata":{"_cell_guid":"2828bde2-47a3-4582-b6c1-60b3ecd47e71","collapsed":true,"_uuid":"f5abfc0f0d0bfcdef49f68c10fbff7aa954588e0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def deep_cv(model, X_train, y_train, X_test, y_test, cp_filepath, class_weight=None, epochs=150, \n            batch_size=1000, cp_verbose=0, fit_verbose=0):\n    '''The default batch size is set to 10000, which should be good for a small neural network.'''\n    cvscores = []\n    iter_i = 0\n    for train, test in cv_sets_cls.split(X_train, y_train):\n        \n        get_best_model(model, X_train.iloc[train], y_train.iloc[train], X_train.iloc[test], y_train.iloc[test], \n                       cp_filepath=cp_filepath, class_weight=class_weight, epochs=epochs, batch_size=batch_size, \n                       cp_verbose=cp_verbose, fit_verbose=fit_verbose)\n        \n        scores = model.evaluate(X_train.iloc[test], y_train.iloc[test], verbose=fit_verbose) # evaluate the model\n        print(\"{}: {:.4f}\".format(model.metrics_names[1], scores[1]))\n        cvscores.append(scores[1])\n        \n        # https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model\n        if cvscores[iter_i] - cvscores[iter_i - 1] > 0:\n            model.save_weights(cp_filepath)\n    \n    model.load_weights(cp_filepath)\n    test_beta_score = model.evaluate(X_test, y_test, batch_size=batch_size)[1]\n    \n    print('\\n')\n    print('The average F0.5 score of the cross validataion is {:.4f} (+/- {:.4f}).'.format(np.mean(cvscores), np.std(cvscores)))  \n    print('The F0.5 score for the testing set is {:.4f}.'.format(test_beta_score))\n    print('\\n')","metadata":{"_cell_guid":"e6bbd61c-99a2-4e4b-b057-016849a909a9","collapsed":true,"_uuid":"8a0b2bca66d71d0170e80931a1c0f5ccad0ef404"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_learning_history(model, X_train, y_train, cp_filepath, class_weight=None, epochs=100, \n                          batch_size=1000, cp_verbose=0, fit_verbose=0):\n    \n    cv_generator = cv_sets_cls.split(X_train, y_train)\n    train_idx, cross_idx = next(cv_generator)\n    \n    history = get_best_model(model, X_train.iloc[train_idx], y_train.iloc[train_idx], \n                                    X_train.iloc[cross_idx], y_train.iloc[cross_idx], \n                             cp_filepath, class_weight=class_weight, epochs=epochs, \n                             batch_size=batch_size, cp_verbose=cp_verbose, fit_verbose=fit_verbose)\n    \n    model.load_weights(cp_filepath)\n    \n    # https://machinelearningmastery.com/custom-metrics-deep-learning-keras-python/\n    plt.figure(figsize=(10, 5))\n    plt.plot(history.history['val_fbeta'])\n    plt.xlabel('Epoch')\n    plt.ylabel('F0.5 Score')\n    plt.title('Learning History')\n    plt.grid()\n    plt.show()\n    \n    print('The best F0.5 score is {:.4f}.'.format(max(history.history['val_fbeta'])))","metadata":{"_cell_guid":"a4a4b961-4f56-496b-a1c1-3f9199bc536e","collapsed":true,"_uuid":"8ab234da7957417a99341ec3a0d4e7debfe95595"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Train on the downsampled dataset with the baseline model","metadata":{"_cell_guid":"d3588db9-7006-41d6-bfee-2fa82300f8cd","_uuid":"62abb0160b43436d3794d6cc32b189406c4fd534"}},{"cell_type":"code","source":"# Set a repeatable initializer. The 'TruncatedNormal' is the recommended initializer by Keras.\nkernal_init = K_init.TruncatedNormal(mean=0.0, stddev=0.05, seed=random_state)","metadata":{"_cell_guid":"190af407-293d-4c69-ad09-f3275d183a55","collapsed":true,"_uuid":"7f023629887df58439e70bb841c8bbdfc0acde92"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# baseline model from: https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/\nmodel_dim = X_train_cls.shape[1]\ndef create_base(dim=model_dim):\n    model = Sequential()\n    model.add(Dense(dim, input_dim=dim, kernel_initializer=kernal_init, activation='relu'))\n    model.add(Dense(1, kernel_initializer=kernal_init, activation='sigmoid'))\n    \n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[fbeta])\n    # model.summary()\n    \n    return model","metadata":{"_cell_guid":"d6448f54-2c39-4571-ae85-a1574342460d","collapsed":true,"_uuid":"765e0a883ad74432abc7e170438cba287e6b7d0d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From Keras document:\n> class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class.","metadata":{"_cell_guid":"7a8e24d5-0980-4295-99fe-57a23fd4afb4","_uuid":"71a3442e2bda06be66b4d0527bfc9676db183dd2"}},{"cell_type":"code","source":"%%time\n# Test the simplest model trained by the downsampled dataset on the testing set.\nbase1 = create_base()\n\nbatch_size = max(len(y_train_down), len(y_test_cls))\ncp_filepath = 'weights.best.base_model_down_noWeight.hdf5'\n\ndeep_cv(base1, X_train_down, y_train_down, X_test_cls, y_test_cls, cp_filepath, \n        class_weight=None, batch_size=batch_size, epochs=50)","metadata":{"_cell_guid":"c6547aba-b123-4d04-81b6-4690af3832b7","_uuid":"093dd6922ba83e7da530e942f50fa94e1a725a71"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Figure out the best class_weight\npos_train_down = y_train_down.sum()\nneg_train_down = len(y_train_down) -  pos_train_down\nprint('The ratio between the negative and positive labels is {:.1f}'.format(neg_train_down / pos_train_down))\n\npos_weight_down = neg_train_down / pos_train_down / 2\nclass_weight_down = {0:1, 1:pos_weight_down}","metadata":{"_cell_guid":"6e589129-9355-4cd1-8c8e-d9decc9cd22c","_uuid":"8ac179cb1476d3427bf6ccb14910dfdfc8d0d36d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Still trained with the downsampled. The sample weight added.\nbase2 = create_base()\n\ncp_filepath = 'weights.best.base_model_down_withWeight.hdf5'\n\ndeep_cv(base2, X_train_down, y_train_down, X_test_cls, y_test_cls, cp_filepath, \n        class_weight=class_weight_down, batch_size=batch_size, epochs=50)","metadata":{"_cell_guid":"2bb1fe4b-e31a-479b-8659-73a022783fe7","_uuid":"ddd27296d9ac0775f58abfd7a5ebb9b4e6332dda"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"50 epochs is good enough. I have tried 1000 epochs, which doesn't give any improvement. As we can see, the performances on both the training set and the testing set are very bad. This is bad even for a baseline model. \n\nI check the performance of the model with and without the custom class weights. It turns out that the one with the custom class weights perform worse. This could be a random thing, so I will explore this parameter further with the full dataset. The weights with the better model will be used as the initial weights for the full dataset.","metadata":{"_cell_guid":"44bc968a-7311-4ede-9570-bc3910cff42d","_uuid":"74023b2090b0e4619eb1d742ff5d9271daafa8a8"}},{"cell_type":"markdown","source":"##### Train on the full dataset with the baseline model","metadata":{"_cell_guid":"a341fac5-47ad-47c5-a2c9-0cdc11a6bbee","_uuid":"91f362ef83112148d37179f04f66fdb05b3a9aaf"}},{"cell_type":"code","source":"%%time\nbase3 = create_base()\nbase3.load_weights('weights.best.base_model_down_noWeight.hdf5')\n\nbatch_size = max(len(y_train_cls), len(y_test_cls))\ncp_filepath = 'weights.best.base_model_full_noWeight.hdf5'\n\ndeep_cv(base3, X_train_cls, y_train_cls, X_test_cls, y_test_cls, cp_filepath, \n        class_weight=None, epochs=10, batch_size=batch_size)","metadata":{"_cell_guid":"466f6414-16df-4639-9701-ad586b98c365","scrolled":true,"_uuid":"45f9437fcca8a10f9d08ce202c79e764dae530f9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_train_full = y_train_cls.sum()\nneg_train_full = len(y_train_cls) -  pos_train_full\nprint('The ratio between the negative and positive labels is {:.1f}'.format(neg_train_full / pos_train_full))\n\npos_weight_full = neg_train_full / pos_train_full / 2\nclass_weight_full = {0:1, 1:pos_weight_full}","metadata":{"_cell_guid":"561e4d07-23b7-4143-a935-6aaf2956f9a1","_uuid":"03088a6b88f294924d195d55583a4e661c2e149c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nbase4 = create_base()\nbase4.load_weights('weights.best.base_model_down_noWeight.hdf5')\n\ncp_filepath = 'weights.best.base_model_full_withWeight.hdf5'\n\ndeep_cv(base4, X_train_cls, y_train_cls, X_test_cls, y_test_cls, cp_filepath, \n        class_weight=class_weight_full, epochs=10, batch_size=batch_size)","metadata":{"_cell_guid":"5b4d90da-9f2b-47a9-8b0c-2fe80adb3841","scrolled":true,"_uuid":"e220d6fea4dc52fbc6f67a6d8d05720322a8f66d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It turns out that only 10 epochs of the training on the full training set will give fairly good result! Deep learning doesn't like the downsampled training set. This is the nature of deep learning: more information will lead to better performance.\n\nNote that different validation splits give similar scores. The testing score is also similar. This is a very good sign. It suggests that stratified K-fold well separates different subsets so that each one is representative of the data, including the testing set. As deep neural network is very time-consuming, typically cross validation is not affordable. This discovery gives me more confidence in training a good model based on the training set and a single cross validation set.","metadata":{"_cell_guid":"58f389ea-34c1-410f-a9cd-004bcb1bf074","_uuid":"6d4c1ebae7147bdcd53a613fafee2f9191c94871"}},{"cell_type":"markdown","source":"** Check if more epochs will improve the results. ** ","metadata":{"_cell_guid":"9703e165-c534-48fb-b67f-43ee08f07ada","_uuid":"1be6f83a1e53af9e91035159a20a7900ac566f78"}},{"cell_type":"code","source":"%%time\nbase5 = create_base()\nbase5.load_weights('weights.best.base_model_full_withWeight.hdf5') # Picking up from the model with weights.\n\nepochs = 10 # Check 10 more epochs on top of the original 10 epochs.\nbatch_size = len(y_train_cls)\ncp_filepath = 'weights.best.base_model_full_withWeight_moreEpochs.hdf5'\n\nplot_learning_history(base5, X_train_cls, y_train_cls, cp_filepath=cp_filepath, epochs=epochs, \n                      class_weight=class_weight_full, batch_size=batch_size, cp_verbose=1, fit_verbose=0)","metadata":{"_cell_guid":"42052fcb-78a0-415f-bd67-a88ccd58c59f","scrolled":false,"_uuid":"7d17787819ffe9087cb829e968a0c28d8848d129"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the testing set.\n# base5.load_weights(cp_filepath)\nbase5.evaluate(X_test_cls, y_test_cls, batch_size=batch_size)[1]","metadata":{"_cell_guid":"cfacda4e-e7aa-41fc-a2cd-2608bf852e91","_uuid":"fa60af16bd31f1682e2be568209b6640f3621dcf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is an improvement with additional 5 epochs. The result is still not better than the best classifier so far.","metadata":{"_cell_guid":"677205e7-cf4e-4224-8d91-42a1d3f050f1","_uuid":"fe21af39e69b3a30aee9f7b001ee05f8f7378042"}},{"cell_type":"markdown","source":"##### Improved Model","metadata":{"_cell_guid":"ccce9d7c-a62f-4d41-8c12-869c05fe3044","_uuid":"fa76bcc5d1cfc63e8eaa40798d5eaf76a9f4111e"}},{"cell_type":"markdown","source":"##### Tried:\n1. Optimizers. Learning rate and the decay of the optimizer.\n2. Batch sizes.\n3. Dense and dropout Layers.\n4. Kernal initializer.\n5. Direct work on the full dataset vs first on the downsampled.\n6. Activation. Relu is the best, better than Sigmoid.\n7. Epochs. Now it can reach 0.84 in 200 epochs.\n8. Class weights.","metadata":{"_cell_guid":"cc1758e6-6583-4950-89e0-2b585068d428","_uuid":"d31e8210b9b26ce1fedc0d788cd39aaef36de895"}},{"cell_type":"markdown","source":"** DON'T USE CLASS WEIGHTS FOR TRAINING THE FULL DATASET!! **","metadata":{"_cell_guid":"de2ace31-7234-4d14-bde4-7be8a39d74c3","_uuid":"107a5ed3d8468dc97f03e67cd57db1749da025fc"}},{"cell_type":"code","source":"# Set a higher learning rate for Adam (default 0.001). The deeper the network, the smaller the maximum learning rate is.\nopt_Adam = Adam(lr=0.03, decay=0.0)\n\n# Improved model from: https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/\nmodel_dim = X_train_cls.shape[1]\n\nbest_model = Sequential()\nbest_model.add(Dense(model_dim * 1, input_dim=model_dim, kernel_initializer=kernal_init, activation='relu'))\nbest_model.add(Dense(model_dim // 2, kernel_initializer=kernal_init, activation='relu'))\nbest_model.add(Dense(model_dim // 2, kernel_initializer=kernal_init, activation='relu'))\nbest_model.add(Dense(model_dim // 3, kernel_initializer=kernal_init, activation='relu'))\nbest_model.add(Dense(1, kernel_initializer=kernal_init, activation='sigmoid'))\n\nbest_model.compile(loss='binary_crossentropy', optimizer=opt_Adam, metrics=[fbeta])\nbest_model.summary()","metadata":{"_cell_guid":"aff34e57-85b6-4304-9985-6420c98778d2","_uuid":"a4c625b0b3aab94d05d49c354d35c099db7b3df9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Improve the parameters on the full dataset with the pre-trained model.\n# This is just a demo. The best model has been saved.\nepochs = 500\n\n# The optimum batch size is about the length of the training set divided by 500, which equals to 1070.\n# The class weight is set to None.\nbatch_size = max(len(y_train_cls), len(y_test_cls)) // 1000\ncp_filepath = 'weights.best.best_model.hdf5'\nplot_learning_history(best_model, X_train_cls, y_train_cls, cp_filepath=cp_filepath, \n                      class_weight=None, epochs=epochs, batch_size=batch_size, cp_verbose=True)","metadata":{"_cell_guid":"971df162-38c9-4f59-aaa7-8ca41ae3bf08","scrolled":true,"_uuid":"6785e37ba44b5d9deb390d66564a4e9946890f63"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.12.3 Evaluate the best model on the testing set","metadata":{"_cell_guid":"4100013c-fb48-440d-8348-1d0a41e72871","_uuid":"9d62f8a5ff56ceefbb7998dfc868babe303ef926"}},{"cell_type":"markdown","source":"** Load the weights from the best model. ** ","metadata":{"_cell_guid":"51d6174f-ff69-448b-b77e-05d94a206716","_uuid":"a9c939621742c0f855591223ffc4e6a679300afd"}},{"cell_type":"code","source":"## best_model.load_weights('saved_models/weights.best.model_8.hdf5')","metadata":{"_cell_guid":"6067e1f4-55aa-4a12-8d29-c9a2e71c2a4d","collapsed":true,"_uuid":"3a1e8a032e45e676bdcb096c99c1a1efdf81de5c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_batch_size = len(y_train_cls)","metadata":{"_cell_guid":"bc1fa6d5-8afc-40d2-ac3c-ba62d8211da4","collapsed":true,"_uuid":"7d5823651e6362aabfd19c665be99353433565ad"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Note the difference of the result when the batch size of the evaluation is set differently.","metadata":{"_cell_guid":"99737981-5996-4c9f-a7b4-17feed47ef1a","_uuid":"008a3aa555146e0fad8168f689633405bfb85ffe"}},{"cell_type":"code","source":"# This is the evaluation with the default batch size.\nbest_model.evaluate(X_test_cls, y_test_cls)[1]","metadata":{"_cell_guid":"9d8cfc1b-d087-416f-8568-b4fc0bfca353","_uuid":"d5b660770e6224d4c1a187059e9a94080b6b5e47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is the evaluation with the maximum batch size.\nbest_model.evaluate(X_test_cls, y_test_cls, max_batch_size)[1]","metadata":{"_cell_guid":"cc6a03e6-7eb9-47ce-a038-8777288bde6f","_uuid":"907d436e840ecd5b85044391d334b1d92a1362d0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Predict the y values with the default batch size and the maximum batch size.\n# Note that the result needs to be rounded in order to compare with y_test_cls as the raw output is float from sigmoid function.\ny_pred_test_MLP_default = best_model.predict(X_test_cls).round()\ny_pred_test_MLP_max = best_model.predict(X_test_cls, batch_size=max_batch_size).round()","metadata":{"_cell_guid":"a785740f-bde3-46da-a177-68ed2c2cfe61","_uuid":"f36cde40b952de4d1aca13ee067fdc66b4d5a74c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fbeta_score(y_test_cls, y_pred_test_MLP_default, beta=0.5)","metadata":{"_cell_guid":"3ae823ed-4f78-4a18-85a9-dc2a80896fcb","_uuid":"f9be60f4b09ee95bdbb7ae4bbeea05f0aff718f4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fbeta_score(y_test_cls, y_pred_test_MLP_max, beta=0.5)","metadata":{"_cell_guid":"d4cbc188-9999-4db7-b286-dad99a90a055","_uuid":"88494781efb2a006137bc143bd9d8cd5ca7c3525"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As seen above, if the predict function is used instead of the evaluation function, both results match the evaluation result with the maximum batch size. This must be a Keras bug. One should be careful in using the evaluation function. ","metadata":{"_cell_guid":"35ea35b2-e3ff-45db-bebc-c899e4f8276d","_uuid":"027b17a559588e69beb2244536b66b5613148b10"}},{"cell_type":"code","source":"# Evaluation on the X_train_cls\nbest_model.evaluate(X_train_cls, y_train_cls, batch_size=max_batch_size)[1]","metadata":{"_cell_guid":"336eca0b-605c-4741-81cb-3b8cb6d0fa87","_uuid":"f61aacc224a7b7a983baf837e3fb5e9479731a59"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluation on the X_train_down\nbest_model.evaluate(X_train_down, y_train_down, batch_size=max_batch_size)[1]","metadata":{"_cell_guid":"13df32f7-f685-49d6-bde9-31808fcf51aa","_uuid":"59e141d9e862e5dbdec3fbbbe34d1be8afddfadb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model fits the whole dataset very well. However, the model doesn't work very well with the downsampled dataset. ","metadata":{"_cell_guid":"b80f3f96-7c29-48a6-9f30-d59e434fc01f","_uuid":"f97bbe37bffcef341dc8b8daffb14a3365fc218c"}},{"cell_type":"code","source":"%%time\nMLP_importance = {}\nfor col in X_test_cls.columns:\n    X_temp = X_test_cls.copy()\n    X_temp[col] = 0\n    X_temp_score = best_model.evaluate(X_temp, y_test_cls, batch_size=max_batch_size, verbose=0)[1]\n    MLP_importance[col] = X_temp_score","metadata":{"_cell_guid":"eb6a5ca5-117b-471f-a37f-9e076d327e2a","scrolled":true,"_uuid":"08b2d99bab91c070b8c9cdb18f5a0fa2a80438aa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MLP_importance_series = pd.Series(MLP_importance).sort_values()","metadata":{"_cell_guid":"0ab84f08-6a32-4aff-ae8f-55dad52c58a3","collapsed":true,"_uuid":"6602fa7e2c860fa34d646a5817f14dee59d18cf3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MLP_importance_series.plot.bar(figsize=(15, 5), fontsize=14)\nplt.ylabel('F0.5 score')\nplt.title('Feature Impact on the Score')\nplt.show()","metadata":{"_cell_guid":"d63aefd7-d321-4261-9aa3-d13029056967","_uuid":"20fbba8b40023b67036823c2d540d7f9928127de"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MLP_importance_series[-2:]","metadata":{"_cell_guid":"a66ea85d-ab57-407a-b726-a954ec818cf6","_uuid":"04c34a9be492b70b1deb9fc366d12d998cad8f8f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the importance gain from the least important feature importance.\nMLP_impact = -(MLP_importance_series - MLP_importance_series[-1])","metadata":{"_cell_guid":"0226cd6e-7f76-4eb9-b032-b30846c85275","collapsed":true,"_uuid":"733df6f3faa49552bc20b8d48f5b293d4b608682"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale the data to be comparable with other algorithms.\nMLP_impact_norm = pd.Series(minmax_scale(MLP_impact))\nMLP_impact_norm.index = MLP_impact.index","metadata":{"_cell_guid":"39f49aee-7dde-4c50-98d0-3136dba0a321","collapsed":true,"_uuid":"fff2f9e6afad6210160ba69927754f4751d284ef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cls_importance_all = pd.concat([cls_importance[['Ridge', 'LogGain']], MLP_impact_norm], axis = 1)\ncls_importance_all.rename(columns = {0: 'MLP', 'LogGain': 'XGB'}, inplace=True)","metadata":{"_cell_guid":"16528db1-e248-435d-8782-37fd219d8407","collapsed":true,"_uuid":"f86f3d7648d8d255a2329af99b60765ec05f69f4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cls_importance_all.sort_values('Ridge', ascending=False).plot.bar(grid=True, figsize=(20, 5), fontsize=14)\nplt.show()","metadata":{"_cell_guid":"9f2ae6e9-8044-43a2-aa66-36e1c73a1764","scrolled":false,"_uuid":"66049de504ec5a7270b69623a8654cfa96f2d1cd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"** The smaller the score, the more important the feature.**","metadata":{"_cell_guid":"cc7ea2da-70a8-4d2e-8a1d-afdf0b0d1da1","_uuid":"73acbc1c83d3ab8174fd091c62cf3866e81205de"}},{"cell_type":"code","source":"%%time\nMLP_importance_series_2 = MLP_importance_series.copy()\nMLP_index = MLP_importance_series.index\n\nfor col in MLP_index:\n    X_temp = X_test_cls.copy()\n    \n    col_idx = MLP_index.get_loc(col) # Get the index of a specific column from the MLP_importance_series\n    col_del = MLP_index[(col_idx+1):] # Set all columns after the specified column to value zero\n    X_temp.loc[:, col_del] = 0\n    \n    X_temp_score = best_model.evaluate(X_temp, y_test_cls, batch_size=max_batch_size, verbose=0)[1]\n    MLP_importance_series_2[col] = X_temp_score","metadata":{"_cell_guid":"45d3c0fc-d2a2-41b7-b3bb-3e6c4427d21f","_uuid":"97f3506389ec89364fcc51e4fbea54f528b481eb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MLP_importance_series_2.plot.bar(figsize=(15, 5), fontsize=14)\nplt.ylabel('F0.5 score')\nplt.title('Feature Impact on the Score')\nplt.show()","metadata":{"_cell_guid":"72201422-6d60-48d7-a6bf-e8de255e63e9","_uuid":"560a457f2e13b054449990f7f6ca30af642d00d8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above figure is plotted with only features more important than the specific feature. Columns with less important features are all set to zero. ","metadata":{"_cell_guid":"37cef7ed-54fc-4df3-8170-f128e03e84c7","_uuid":"f01b8ecd19d615df91bf194cfc94cc13f1316565"}},{"cell_type":"code","source":"# These features are not useful for the MLP model.\nMLP_importance_series_2[MLP_importance_series_2 > 0.84]","metadata":{"_cell_guid":"0132049d-e7f9-41f2-a704-b47027047eac","_uuid":"d68ccd2d4221d1a20592e321dd6577d9be39ba59"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# http://forums.fast.ai/t/how-could-i-release-gpu-memory-of-keras/2023/11\nK.clear_session()","metadata":{"_cell_guid":"30a4da89-9844-4135-bc4d-57a70bbab045","collapsed":true,"_uuid":"b36d2a910c7fb5246a892499d6865e9c40be1b48"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the total time used for running the whole notebook.\nnotebook_end_time = time()\nnotebook_running_time = (notebook_end_time - notebook_start_time) / 60\nprint('Running the entire notebook takes {:.2f} minutes.'.format(notebook_running_time))","metadata":{"_cell_guid":"e0fa9c07-6b20-4243-a6a4-c4d6952647fa","_uuid":"dbac75fd5e24481e17f307c973675ddf757c379a"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"}}}}