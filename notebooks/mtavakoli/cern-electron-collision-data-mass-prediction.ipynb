{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"step 1:  Reading Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/cern-electron-collision-data/dielectron.csv\")\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"step 2: Information about the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns\n#data.describe()\n#data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding attribute"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['E_total'] = data['E1'] + data['E2']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"step 2: knowing about type of data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking NaN value in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that data of \"M\" is less than the rest of the data"},{"metadata":{},"cell_type":"markdown","source":"step 3: Plotting the Histogram for knowing about data"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\ndata.hist(bins=50, figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"main idea: **prediction of \"M\"**.\n\nstep4:  Finding which of data has more correlation with \"M\""},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_Matrix=data.corr()\ncorr_Matrix[\"M\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation between \"M\" and feature \"E_total\" is more"},{"metadata":{},"cell_type":"markdown","source":"step5:  Splitting train set and test set by \"train_set split\" library. without cnsidering distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(data, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['pt2'].hist(bins=50, figsize=(10,5))\nplt.show()\ndata['E_total'].hist(bins=50, figsize=(10,5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"some feature is more important\nsplitting data with cosidering distribution form.\nlooking carefully at \"pt2\"(or E_total) histogram, it demands redefinition of the test and train set in order to avoid the \"sampling bias\" problem. To do this we use \"StratifiedShuffleSplit\" library. To do this first we have to use \"cut\" from numpy module.\n\n* Adding a column"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"pt2_cat\"]=pd.cut(data[\"pt2\"],\n                       bins=[0., 10, 20, 30, 40, np.inf],\n                       labels=[1, 2, 3, 4, 5])\n\n\ndata[\"E_total_cat\"]=pd.cut(data[\"E_total\"],\n                       bins=[0., 50, 100, 150, 200, np.inf],\n                       labels=[1, 2, 3, 4, 5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"E_total_cat\"].hist(bins=50, figsize=(10,5))\nplt.show()\n\ndata[\"pt2_cat\"].hist(bins=50, figsize=(10,5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(data, data[\"E_total_cat\"]):\n    strat_train_set = data.loc[train_index]\n    strat_test_set = data.loc[test_index]\n    \nfor train_index, test_index in split.split(data, data[\"pt2_cat\"]):\n    strat_train_set1= data.loc[train_index]\n    strat_test_set1= data.loc[test_index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Deleading the column"},{"metadata":{"trusted":true},"cell_type":"code","source":"for set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"E_total_cat\", axis =1, inplace = True)\n    \n    \n    \nfor set_ in (strat_train_set1, strat_test_set1):\n    set_.drop(\"pt2_cat\", axis =1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_train_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Corr_Matrix = strat_train_set.corr()\nprint(Corr_Matrix[\"M\"].sort_values(ascending= False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_train_set.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mass data is less than the rest of the data so we delete some of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_train_set.dropna(subset=['M'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_train_set.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix as sm\nattributes = ['pt1','pt2','E_total','M']\nsm(strat_train_set[attributes] , figsize=(15, 10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"step6:  Separateing the label data and the predictors"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_prepared = strat_train_set.drop(\"M\", axis=1, inplace=False)\ndata_lable = strat_train_set[\"M\"].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scaling the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"scale=StandardScaler()\ndata_prepared_scale=scale.fit_transform(data_prepared)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"step7:  Training Model"},{"metadata":{},"cell_type":"markdown","source":"* **The LinearRegresion model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lin_reg= LinearRegression()\ndata_lin_reg.fit(data_prepared_scale, data_lable)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error\ndata_prediction = data_lin_reg.predict(data_prepared_scale)\n\nlin_mse = mean_squared_error(data_lable, data_prediction)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_scores = cross_val_score(data_lin_reg, data_prepared_scale, data_lable, scoring='neg_mean_squared_error', cv=20)\nlin_rmse_score= np.sqrt(- lin_scores)\nlin_rmse_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **DecisionTreeRegressor model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"de_data=DecisionTreeRegressor()\nde_data.fit(data_prepared_scale, data_lable)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_p=de_data.predict(data_prepared_scale)\n\nlin_msed = mean_squared_error(data_lable, data_p)\nlin_rmsed = np.sqrt(lin_msed)\nlin_rmsed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Des_scoresd = cross_val_score(de_data ,data_prepared_scale, data_lable, scoring='neg_mean_squared_error', cv=20)\nDes_rmse_scored= np.sqrt(- Des_scoresd)\nDes_rmse_scored","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **RandomForest model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_reg = RandomForestRegressor()\nforest_reg.fit(data_prepared_scale, data_lable)\n\ndata_prediction = forest_reg.predict(data_prepared_scale)\nforest_mse = mean_squared_error(data_prediction,  data_lable)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_scores = cross_val_score(forest_reg, data_prepared_scale, data_lable,\n                                scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\nforest_rmse_scores\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\ndata_poly = PolynomialFeatures(2)\ndata_poly_2 =data_poly.fit_transform(data_prepared_scale, data_lable)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lin_reg.fit(data_poly_2, data_lable)\n\npoly_scores = cross_val_score(data_lin_reg, data_poly_2, data_lable,\n                              scoring=\"neg_mean_squared_error\", cv=10)\npoly_scores_rmse = np.sqrt(-poly_scores)\nprint(poly_scores_rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"de_data.fit(data_poly_2, data_lable)\ntree_poly_scores = cross_val_score(de_data, data_poly_2, data_lable,\n                        scoring='neg_mean_squared_error', cv=10)\ntree_poly_rmse = np.sqrt(-tree_poly_scores)\nprint(tree_poly_rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_reg = RandomForestRegressor()\nforest_reg.fit(data_poly_2, data_lable)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_poly_scores = cross_val_score(forest_reg, data_poly_2, data_lable,\n                        scoring='neg_mean_squared_error', cv=5)\nforest_poly_rmse = np.sqrt(-forest_poly_scores)\nprint(forest_poly_rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_test_set0 = strat_test_set.dropna(subset=['M'])\n\n\ndata_prepared_test = strat_test_set0.drop(\"M\", axis=1, inplace=False)\ndata_lable_test = strat_test_set0[\"M\"].copy()\n\n\ndata_prepared_test_scale = scale.transform(data_prepared_test)\ndata_prepared_test_prepared = data_poly.transform(data_prepared_test_scale)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = forest_reg\ntest_predictions = best_model.predict(data_prepared_test_prepared)\ntest_mse = mean_squared_error(test_predictions, data_lable_test)\ntest_rmse = np.sqrt(test_mse)\nprint(test_rmse)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}