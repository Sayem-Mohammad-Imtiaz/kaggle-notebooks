{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport xlrd \nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom numpy.random import seed\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import fbeta_score, confusion_matrix, precision_recall_curve, auc, make_scorer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nfrom scipy import stats\nfrom sklearn import preprocessing\nimport numpy as np\nfrom sklearn import metrics, svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn import utils\nfrom sklearn import datasets, linear_model\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nimport math\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nseed(1)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Questions1234(df):\n    # Dimension of the dataset\n        \n    print('the shape of the df is \\n' ,np.shape(df))\n    numberOfNulls = df.isna().sum().sum()\n    print('There are {0} Nulls in the data'.format(numberOfNulls))\n    df.MEDV.describe()\n    fig = plt.figure(figsize=(10, 5))\n    ax = sns.distplot(df.MEDV)\n    plt.show()\n    ax = sns.boxplot(y = df.MEDV)\n    plt.show()\n    print('we see the MEDV have ~ normal distribution and there are some outliers on the MEDV variable')\n    # Correlation mat\n    corrMatt = df[df.columns.values].corr().abs()\n    mask = np.array(corrMatt)\n    mask[np.tril_indices_from(mask)] = False\n    fig,ax= plt.subplots()\n    fig.set_size_inches(20,10)\n    sns.heatmap(corrMatt, mask=mask,vmax=.8, square=True,annot=True)\n    print('From correlation matrix, we see TAX and RAD are highly correlated'\n          'features. The columns LSTAT, RM has a correlation score above 0.69 '\n          'with MEDV which is a good indication of using as predictors, '\n          'However RM and LSTAT are also pretty correlated to each other '\n          'what might be not a good idea to pick both of them Lets plot these columns against MEDV')\n    #Plot top 2 correlated features against MEDV\n    min_max_scaler = preprocessing.MinMaxScaler()\n    column_sels = ['LSTAT', 'RM']\n    x = df.loc[:,column_sels]\n    y = df['MEDV']\n    x = pd.DataFrame(data=min_max_scaler.fit_transform(x), columns=column_sels)\n    fig, axs = plt.subplots(ncols=1, nrows=2, figsize=(10, 10))\n    index = 0\n    for i, k in enumerate(column_sels):\n        sns.regplot(y=y, x=x[k], ax=axs[i])\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n    # list the top correlated features to the target 'MEDV' (with abs value)\n    CorList = df.corr().unstack().sort_values().abs()['MEDV']\n    CorList = CorList.sort_values(ascending=False)\n    print(CorList)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class TestML:\n    def __init__(self,df):\n        self.df = df\n        \n    def Eda(self):\n        print(df.dtypes)\n        fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\n        index = 0\n        axs = axs.flatten()\n        for k,v in df.items():\n            sns.distplot(v, ax=axs[index])\n            index += 1\n        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n        print('        # The histogram also shows that columns CRIM, ZN has highly skewed distributions.'\n              '  #Also MEDV looks to have a normal distribution (the predictions) and other colums '\n                 '#seem to have norma or bimodel ditribution of data except CHAS and CAT.MEDV '\n              '  #(which are discrete variables)')\n        \n        fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\n        index = 0\n        axs = axs.flatten()\n        for k,v in df.items():\n            sns.boxplot(y=k, data=df, ax=axs[index])\n            index += 1\n        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n        for k, v in df.items():\n            q1 = v.quantile(0.25)\n            q3 = v.quantile(0.75)\n            irq = q3 - q1\n            v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n            perc = np.shape(v_col)[0] * 100.0 / np.shape(df)[0]\n            print(\"Column %s outliers = %.2f%%\" % (k, perc))\n            from sklearn import preprocessing\n        \n        #Plot top 2 correlated features against MEDV\n\n        from sklearn import preprocessing\n        # Let's scale the columns before plotting them against MEDV\n        min_max_scaler = preprocessing.MinMaxScaler()\n        column_sels = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n               'PTRATIO', 'LSTAT', 'CAT. MEDV']\n        x = df.loc[:,column_sels]\n        y = df['MEDV']\n        x = pd.DataFrame(data=min_max_scaler.fit_transform(x), columns=column_sels)\n        fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\n        index = 0\n        axs = axs.flatten()\n        for i, k in enumerate(column_sels):\n            sns.regplot(y=y, x=x[k], ax=axs[i])\n        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n        \n    def Model(df):\n        #first ill get rid of the outliers \n        df = df[~(df['MEDV'] >= 50.0)]\n        print(np.shape(df))\n        #remove the skewness of the data trough log transformation\n        y = df['MEDV']\n        X = df.drop(columns = ['MEDV'])\n       \n        y =  np.log1p(y)\n        for col in X.columns:\n            if np.abs(X[col].skew()) > 0.3:\n                X[col] = np.log1p(X[col])\n        fig = plt.figure(figsize=(10, 5))\n        ax = sns.distplot(df.MEDV)\n        plt.show()\n        print('now we see MEDV after removing the skewness of the data with log transformation') \n        \n        print(\"\"\" Fitting Linear Regression for more simple Regressor \n                        \"\"\")\n        from sklearn.model_selection import GridSearchCV, train_test_split\n        X = df.loc[:, df.columns.difference(['MEDV'])]\n        y = df.loc[:, 'MEDV']\n        X_train,X_test, y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        from sklearn.linear_model import LinearRegression\n        reg = LinearRegression().fit(X_train, y_train)\n        y_pred = reg.predict(X_test)\n        print('R2 lin_reg score',metrics.r2_score(y_pred,y_test))\n        \n        \n        print(\"\"\"\" Performing a Regression Using XGBoost to predict MEDV\n                            \"\"\")\n        # A parameter grid for XGBoost\n        from sklearn.metrics import r2_score\n        from sklearn.model_selection import GridSearchCV, train_test_split\n        from xgboost import XGBRegressor \n\n            \n        def algorithm_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, \n                   model, param_grid, cv=10, scoring_fit='neg_mean_squared_error',\n                   do_probabilities = False):\n            gs = GridSearchCV(\n                estimator=model,\n                param_grid=param_grid, \n                cv=cv, \n                n_jobs=-1, \n                scoring=scoring_fit,\n                verbose=2\n            )\n            fitted_model = gs.fit(X_train_data, y_train_data)\n\n            if do_probabilities:\n              pred = fitted_model.predict_proba(X_test_data)\n            else:\n              pred = fitted_model.predict(X_test_data)\n\n            return fitted_model, pred\n        \n        \n        X = df.loc[:, df.columns.difference(['MEDV'])]\n        y = df.loc[:, 'MEDV']\n        X_train,X_test, y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        from sklearn.model_selection import RandomizedSearchCV\n        model = XGBRegressor()\n        param_grid = {\n            'n_estimators': [400, 700, 1000],\n            'colsample_bytree': [0.7, 0.8],\n            'max_depth': [15,20,25],\n            'reg_alpha': [1.1, 1.2, 1.3],\n            'reg_lambda': [1.1, 1.2, 1.3],\n            'subsample': [0.7, 0.8, 0.9]\n        }\n\n        model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n                                         param_grid, cv=5)\n\n        # Root Mean Squared Error\n        print('R2 score :', metrics.r2_score(pred,y_test))\n        \n        print(\"\"\"\" We can clearly see that the XGboost model is much better than the lin_reg model by the R^2 Score \n        \"\"\")\n        \n        \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == '__main__':\n    df = pd.read_csv(\"../input/train-boston/DS.csv\")\n    Questions1234(df)\n    TestML.Eda(df)\n    TestML.Model(df)\n    ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}