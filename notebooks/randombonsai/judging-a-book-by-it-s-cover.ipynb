{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Judging a book by it's cover\n\nThe good read dataset provides data on books rated on goodread, with title, authors, etc. This is an EDA notebook to examine the language of book titles and whether or not it can be correlated with the rating obtained by a book.\n\n## preprocessing\n\nloading all the essential libraries, visualizing the architecture of the data, then selecting the most relevant aspects for analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport spacy\nimport numpy\nimport wordcloud\n\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en_core_web_lg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"../input/books.csv\"\ndfr = pd.read_csv('../input/books.csv', skiprows=[4011, 5687, 7055, 10600, 10667])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfr.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we observe informations about the book itself, like its title, number of pages, and authors, and informations about it's rating and reviews. We are missing the reviews so it's a bit minimalistic in terms of language analysis. I need to first clean the data, and to do that I will first see how many languages are available"},{"metadata":{"trusted":true},"cell_type":"code","source":"languages = dfr['language_code'].tolist()\nl_dict = {}\nfor l in set(languages):\n    l_dict[l] = languages.count(l)\nlists = sorted(l_dict.items())\nx, y = zip(*lists)\nf, ax = plt.subplots(figsize=(12, 5))\nplt.barh(x, y)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So most books are in english, I'm going to create a dataset based on the merge of 'eng', 'en-GB', 'eng-CA' and 'en-US', and I will keep only books that have at least 10 ratings in order to get rid of the books that are not rated"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = dfr[dfr['ratings_count'] >= 10]\nenglish_codes = [\"eng\", \"en-US\", \"en-CA\", \"en-GB\"]\ndf = df[df.language_code.isin(english_codes)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## Book metadata analysis\n\nso now I'm going to analyze the books rating based on their title and length"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"title_length\"] = df.apply(lambda row: len(row.title), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I use title length in chars, which is a bit minimalistic as an approach, it might be nice to add tokenization and maybe remove stopwords. I think there could be an interesting thing to do with word clouds after pre-processing stopwords on titles, see if some words are highly correlated with good (or bad) rankings"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(9, 6))\nsns.despine(f, left=True, bottom=True)\nsns.scatterplot(x=df.title_length, y=df.average_rating,\n                hue=df['# num_pages'], \n                palette=\"ch:r=-.2,d=.3_r\",\n                sizes=(1, 8), linewidth=0,\n                data=df, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Meh. No real indication that title length impacts significantly the rating of a book here"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(6.5, 6.5))\nsns.despine(f, left=True, bottom=True)\nsns.scatterplot(y=df.average_rating, x=df['# num_pages'],\n                hue=df.title_length, \n                palette=\"ch:r=-.2,d=.3_r\",\n                sizes=(1, 8), linewidth=0,\n                data=df, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observing some correlation between number of pages and ranking from here. I'm missing some important info like genre of the books, that would be great to see what kind of book gets the best ranking. Now, I'm going to add a categorical data about whether each book name contains a verb, an adjective and add it in one hot encoding to see if these values impact scoring."},{"metadata":{"trusted":true},"cell_type":"code","source":"def pos_in_doc(text, postag):\n    doc = nlp(text)\n    return 1 if postag in [token.pos_ for token in doc] else 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"verb_in_title\"] = df.apply(lambda row: pos_in_doc(row.title, \"VERB\"), axis=1)\ndf[\"noun_in_title\"] = dfr.apply(lambda row: pos_in_doc(row.title, \"NOUN\"), axis=1)\ndf[\"adv_in_title\"] = dfr.apply(lambda row: pos_in_doc(row.title, \"ADV\"), axis=1)\ndf[\"adj_in_title\"] = dfr.apply(lambda row: pos_in_doc(row.title, \"ADJ\"), axis=1)\ndf[\"propn_in_title\"] = dfr.apply(lambda row: pos_in_doc(row.title, \"PROPN\"), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation between title and rating\n\nNow about the ranking, I have infos about how many comments for each book, how many ratings, and the rating mean. I wonder if the number of rating tends to smooth the rating itself (I don't have any information about the variance between ratings, it would be great to see which books are more polarizing and which creates the most consensus)."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1, figsize=(10, 6))\nplt.subplot(2, 3, 1)\nsns.violinplot(x=df.verb_in_title, y=df.average_rating,\n               split=True, inner=\"quart\",\n               data=df)\nplt.subplot(2, 3, 2)\nsns.violinplot(x=df.noun_in_title, y=df.average_rating,\n               split=True, inner=\"quart\",\n               data=df)\nplt.subplot(2, 3, 3)\nsns.violinplot(x=df.adv_in_title, y=df.average_rating,\n               split=True, inner=\"quart\",\n               data=df)\nplt.subplot(2, 3, 4)\nsns.violinplot(x=df.adj_in_title, y=df.average_rating,\n               split=True, inner=\"quart\",\n               data=df)\nplt.subplot(2, 3, 5)\nsns.violinplot(x=df.propn_in_title, y=df.average_rating,\n               split=True, inner=\"quart\",\n               data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's visible that an adverb in your title gives you a slight advantage in terms of rating. Nice ! Now let's get a little more specific. I wonder if we can associate the words in the title with the rating ? Let's remove the stopwords, the punctuation, numbers, and make everything lowercase, then tokenize + lemmatize the title words and build another dataset from that, with each word associated with it's mean ratings, with variance associated."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['title'][0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen here, the way to write a book title here is  `title (series number)`. that's great because later on we can do analysis on sequels, see how bad they perform across long series, is the ratings stay consistents. for now we will exploit only the first part of the title."},{"metadata":{"trusted":true},"cell_type":"code","source":"relevant_pos = ['ADJ', 'VERB', 'NOUN', 'PROPN', 'ADV']\ncol = [\"word\", \"ratings_list\", \"pos_tag\"]\ntitle_words_df = pd.DataFrame(columns=col)\ntitle_words_df = title_words_df.set_index('word')\nfor index, row in df.iterrows():\n    rating = row['average_rating']\n    short_title = re.sub(r\" \\(.*\", \"\", row['title'])\n    doc = nlp(short_title)\n    tokens = [t for t in doc if t.pos_ in relevant_pos]\n    for token in tokens:\n        lemma = token.lemma_.lower()\n        pos_tag = token.pos_\n        if lemma not in title_words_df.index.values:\n            mini_df = pd.DataFrame({col[0]: lemma, col[1]: [numpy.array([rating])], col[2]: pos_tag})\n            mini_df = mini_df.set_index('word')\n            title_words_df = title_words_df.append(mini_df)\n        else:\n            ex_value = title_words_df.get_value(lemma, col[1])\n            new_value = numpy.append(ex_value, [rating])\n            title_words_df.at[lemma, col[1]] = new_value","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alright, now we end up with a dataset that contains a massive number of words, we are going to add some columns describing the word's frequency (number of different ratings it collaborated to, so length of rating vector), it's variance in rankings, it's mean ranking, and so on. Then I will extract another dataset composed of words that have a frequency count above ten, to get rid of hapaxes. And I will probably get rid of one-letter words too."},{"metadata":{"trusted":true},"cell_type":"code","source":"title_words_df[\"frequency\"] = title_words_df.apply(lambda row: len(row['ratings_list']), axis=1)\ntitle_words_df[\"variance\"] = title_words_df.apply(lambda row: numpy.var(row['ratings_list']), axis=1)\ntitle_words_df[\"mean_rating\"] = title_words_df.apply(lambda row: numpy.mean(row['ratings_list']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_words_df = title_words_df[title_words_df.frequency > 20]\nadj_freq_words_df = freq_words_df[freq_words_df.pos_tag == 'ADJ']\nnoun_freq_words_df = freq_words_df[freq_words_df.pos_tag == 'NOUN']\nadv_freq_words_df = freq_words_df[freq_words_df.pos_tag == 'ADV']\nverb_freq_words_df = freq_words_df[freq_words_df.pos_tag == 'VERB']\nnoun_freq_words_df.sort_values(\"mean_rating\", ascending=False).head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"noun_freq_words_df.sort_values(\"mean_rating\", ascending=True).head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So it put you in a better position to write about alchemist (I'm assuming this is from the manga series fullmetal alchemist, and not the indication of an ongoing booming interest for the old science of turning random materials laying around your house into gold), than about girls, mystery, or confession. "},{"metadata":{},"cell_type":"markdown","source":"## Wordclouds\n\nNow let's generate two wordclouds, one for the best ranked books, the other for the worst. To simplify this we will conside rthat a \"good\" book is a book above 4.5, and a bad book is below 3.5. These theshlds might require some adjustments later on, for example if we realize there isn't enough book that fall into one or the other category."},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = set(STOPWORDS)\ngood_text = \" \".join(re.sub(\"\\(.*\", \"\", t) for t in df[df.average_rating >= 4.5].title)\nbad_text = \" \".join(re.sub(\"\\(.*\", \"\", t) for t in df[df.average_rating <= 3.5].title)\ntext = \" \".join(re.sub(\"\\(.*\", \"\", t) for t in df.title)\ngood_wordcloud = WordCloud(stopwords=stop, background_color=\"white\").generate(good_text)\nbad_wordcloud = WordCloud(stopwords=stop, background_color=\"white\").generate(bad_text)\nplt.figure(1, figsize=(14, 10))\nplt.subplot(1, 2, 1)\nplt.imshow(good_wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.subplot(1, 2, 2)\nplt.imshow(bad_wordcloud, interpolation='bilinear')\nplt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In general, if you want to meet success with your book on that type of rating website, don't go for anything that involves men or girl or time or the color black cause people aren't that much into it. Go for something more classy, like \"Lord Alchemist of the Calvin Ring's Complete Guide\" and that's a guaranteed hit !"},{"metadata":{"trusted":true},"cell_type":"code","source":"text_wordcloud = WordCloud(stopwords=stop, background_color=\"white\").generate(text)\nplt.figure(1, figsize=(14, 10))\nplt.imshow(text_wordcloud, interpolation='bilinear')\nplt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And these are the most frequent words in titles, all ratings merged"},{"metadata":{},"cell_type":"markdown","source":"## Series examination\n\nNow, comes the time to study series, and whether or not their rating decrease across books ! We have seen earlier that the book title contains some informationabout the series it's part of, as well as the number in the series. Based on that we will cut ourselves a slice of that dataset with only the books that are part of a series, and add their number to a metadata."},{"metadata":{"trusted":true},"cell_type":"code","source":"series_df = df[df.title.str.endswith(')')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_number(text):\n    match = re.search(r\"#(\\d+)\", text) \n    if match:\n        return int(match.group(1))\n    else:\n        return 0\n    \nseries_df[\"n_in_series\"] = series_df.apply(lambda row: find_number(row.title), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 6.5))\nsns.despine(f, left=True, bottom=True)\nsns.scatterplot(y=series_df.average_rating, x=series_df.n_in_series,\n                hue=series_df['# num_pages'],\n                palette=\"ch:r=-.2,d=.3_r\",\n                sizes=(1, 8), linewidth=0,\n                data=series_df, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"okay, maybe we need to get rid of any book that has more than a 50 iterations, cause these sparse examples are making it harder to read"},{"metadata":{"trusted":true},"cell_type":"code","source":"small_series_df = series_df[series_df.n_in_series < 50]\nf, ax = plt.subplots(figsize=(10, 6.5))\nsns.despine(f, left=True, bottom=True)\nsns.scatterplot(y=small_series_df.average_rating, x=small_series_df.n_in_series,\n                hue=small_series_df['# num_pages'],\n                palette=\"ch:r=-.2,d=.3_r\",\n                sizes=(1, 8), linewidth=0,\n                data=series_df, ax=ax)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}