{"cells":[{"metadata":{"collapsed":true},"cell_type":"markdown","source":"### Linear Regression\n"},{"metadata":{},"cell_type":"markdown","source":"### Import all required packages in one go including stats, seaborn, pandas, numpy and sklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing all required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n%matplotlib inline\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\nimport statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standard reusable functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"epsilon = 0.000001\nveryLargeNum = 1/epsilon\n\ndef calculate_vif(input_data, showonlygtr3=False):\n    vif_df = pd.DataFrame( columns = ['Var', 'Vif'])\n    x_vars=input_data\n    xvar_names=input_data.columns\n    for i in range(0,xvar_names.shape[0]):\n        y=x_vars[xvar_names[i]] \n        x=x_vars[xvar_names.drop(xvar_names[i])]\n        rsq=sm.OLS(y,x).fit().rsquared\n        \n        #To avoid % by 0 at runtime\n        if (1-rsq) < epsilon:\n            vif=veryLargeNum\n        else:\n            vif=round(1/(1-rsq),2)\n            \n        vif_df.loc[i] = [xvar_names[i], vif]\n        df_2disp = vif_df.sort_values(by = 'Vif', axis=0, ascending=False, inplace=False)\n    if showonlygtr3:\n        return df_2disp.loc[df_2disp.Vif >= 3]\n    else :\n        return df_2disp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#geenrate a Dataframe extracting feature names and their p-values\ndef gen_pvaluesdf(statmodel):\n    df_pvalues = pd.DataFrame(round(statmodel.pvalues,2).reset_index(name='pvalues'))\n    varnames = np.array(df_pvalues.iloc[:, 0])\n    pvalues  = np.array(df_pvalues.iloc[:, 1])\n    df = pd.DataFrame(varnames, columns=['Var'])\n    df['pval'] = pvalues\n    df['Significant'] = np.where((df.pval < 0.05), 'Yes', 'No')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Common functions which can be reused by callers to plot %wise graphs\n\ndef get_percentage_cnt(df, col):   \n    t1 = pd.DataFrame(df.groupby(col)[col].count().rename('cnt%'))\n    t1.reset_index(inplace=True)\n    t1['cnt%'] = round((t1['cnt%'] * 100) / t1['cnt%'].sum(),2)\n    return t1\n\ndef myplot(x, y, df):\n    ax = sns.barplot(x=x, y=y, data=df)\n    for p in ax.patches:\n        ax.annotate('{0:.1f}%'.format(p.get_height()), (p.get_x()+0.1, p.get_height()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read data (sourcing)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfname = os.path.join(dirname, filename)\n# Any results you write to the current directory are saved as output.\n\n#Importing dataset\ninsurance = pd.read_csv(fname)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Examine the data\ninsurance.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data cleanup"},{"metadata":{"trusted":true},"cell_type":"code","source":"#insurance.drop(insurance.columns[0], axis=1, inplace=True)\ninsurance.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Statiscs for EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('='*20)\nprint('|', \"{0:^15}\".format('Statistics:'), '|')\nprint('='*85)\nprint('|', \"{0:^25}\".format('Column name'), '|', \"{0:^25}\".format('% Unique Values'),\n      '|', \"{0:^25}\".format('%Missing values'), '|')\nprint('='*85)\n\nfor col in insurance:\n    print(\"| {0:^25}\".format(col), '|', \"{0:^25.2%}\".format(round(insurance[col].nunique()/len(insurance[col]),2)), '|',\n          \"{0:^25.2%}\".format(round(insurance[col].isnull().sum()/len(insurance[col]),2)), '|')\n\nprint('='*85)\n\ninsurance.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Cleanup: Drop missing columns, duplicate rows if any applicable"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_with_zerodata_inpercent = round(100*(insurance.isnull().sum()/len(insurance.index)), 2)\n\nprint(\"Columns with 100% missing data:\")\ncolnames = insurance.columns[cols_with_zerodata_inpercent==100].values\nprint(\"Num col:\", len(colnames), colnames)\n\ncols_with_nonuniq_values = insurance.nunique()\ncolnamesnouniq = insurance.columns[(cols_with_nonuniq_values <= 1)].values\nprint(\"Columns with just 1 unique value:\", len(colnamesnouniq), colnamesnouniq)\n\ninsurance.drop_duplicates(keep=False, inplace=True)\n\ninsurance.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA : Understanding the Data"},{"metadata":{},"cell_type":"markdown","source":"## Classify Independent features into Continous and Categorical for ease of analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get list of categorical Features\ncategorical_feature_list_orig = list(map(str, insurance.columns[insurance.dtypes == object]))\n\n#Get list of continous Features\ncontinous_feature_list_orig = list(map(str, insurance.columns[insurance.dtypes != object]))\n\nprint('='*21)\nprint('Total Features:', len(categorical_feature_list_orig) + len(continous_feature_list_orig))\nprint('='*21)\nprint('Categorical Features:')\nprint('='*21)\nprint(categorical_feature_list_orig)\nprint('='*19)\nprint('Continous Features:')\nprint('='*19)\nprint(continous_feature_list_orig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Statistics of Continous Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('='*82)\nprint('|', \"{0:^23}\".format('Name'),\n    '|', \"{0:^7}\".format('Type'),\n    '|', \"{0:^6}\".format('Min'),\n    '|', \"{0:^9}\".format('Max'),\n    '|', \"{0:^5}\".format('Mean'),\n    '|', \"{0:^5}\".format('Medan'), \n    '|', \"{0:^5}\".format('Std_D'), '|'\n   )\nprint('='*82)\n\nfor col in continous_feature_list_orig:\n    print(\"| {0:^23}\".format(col), '|',\n          \"{0:^7}\".format(str(insurance.loc[:,col].dtype)), '|', \"{0:^6}\".format(round(insurance.loc[:,col].min())), '|',\n          \"{0:^9}\".format(round(insurance.loc[:,col].max())), '|', \"{0:^5}\".format(round(insurance.loc[:,col].mean())), '|',\n          \"{0:^5}\".format(round(insurance.loc[:,col].median())), '|', \"{0:^5}\".format(round(insurance.loc[:,col].std())), '|',\n          )              \n\nprint('='*82)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Statistics of Categorical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('='*82)\nprint('| Categorical Variable |', \"{0:^55}\".format('Possible Categorical Values'), '|')\nprint('='*82)\n\nfor catdata in categorical_feature_list_orig:\n    print('|', \"{0:^20}\".format(catdata), '|', \"{0:^55}\".format(str(insurance[catdata].unique())), '|')\nprint('='*82)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Univariate Analysis"},{"metadata":{},"cell_type":"markdown","source":"### Categorical Features which can be converted into Dichotomous and Ordinal values"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\n\ndual_categories = ['sex', 'smoker', 'region']\n\nplt_cnt = 1;\nfor feature in dual_categories:\n    plt.subplot(1,4,plt_cnt)\n    tempdf = get_percentage_cnt(insurance, feature)\n    myplot(feature, 'cnt%', tempdf)\n    plt_cnt += 1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Capture all the Continous Features and Categorical Features present in the system"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Continous Features\nContinous_feature_list   = list(map(str, insurance.columns[insurance.dtypes != object]))\n\n#Categorical Features\nCategorical_feature_list = list(map(str, insurance.columns[insurance.dtypes == object]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encode remaining Categorical Features using get_dummies()"},{"metadata":{"trusted":true},"cell_type":"code","source":"d_asp = pd.DataFrame(pd.get_dummies(insurance[Categorical_feature_list]))\n\n#Print shapes of all DFs\nprint('Original shape of insurance:', insurance.shape)\nprint('Original shape of Categorical Features DF:', d_asp.shape)\n\n#concatinate the dataframe with get_dummies to original DataFrame\ninsurance = pd.concat([insurance, d_asp], axis=1)\n\n#check the shape to confirm features are added\nprint('Modified shape of insurance:', insurance.shape)\n\ninsurance.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Drop all the original Categorical Features as we dont need them after get_dummies\n### Also, just n-1 subgroups are sufficient to represent all n subgroups within a categorical variable, so drop one column. Within each Categorical Feature, identify the subgroup which has lowest frequency and drop it"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n\"Categorical_feature_list\" now contains the original Categorical Features \nWe can safely drop original Categorical Features from Dataframe as these are no longer necessary\n'''\ninsurance_dummies = insurance.drop(Categorical_feature_list, axis=1)\n\ninsurance_dummies.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 8))\n\ncorr = insurance_dummies.corr()\n\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(240,10,as_cmap=True),\n            square=True, ax=ax)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance.drop(Categorical_feature_list, axis=1, inplace=True)\n\ndummycolumns = ['sex_female', 'smoker_no', 'region_southwest']\n\ninsurance.drop(dummycolumns, axis=1, inplace=True)\n\ninsurance.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check to confirm if we missed any categorical column during conversion "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Num of categorical columns remaining are:', len(insurance.columns[insurance.dtypes == object]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Standardize the entire DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_cp = (insurance - insurance.mean()) / insurance.std()\nnorm_cp.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Corelation Matrix - Price Vs Continous Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the correlation matrix. This gives us a clear idea of what might be the features and their corelation\nplt.figure(figsize = (12,8))     # Size of the figure\n\ndf_cor = pd.DataFrame(norm_cp[Continous_feature_list])\n\ncor_mat = round(df_cor.corr(),1)\n\n#obtain list of Continous Independent Features (CIF) based on their importance\ndf = pd.DataFrame(cor_mat.charges.sort_values(ascending=False))\ndf.reset_index(inplace=True)\n\n#Continous Independant Feature\nCIF = list(df.iloc[1:,0])\n\nax = plt.axes()\nsns.heatmap(cor_mat, annot=True, ax=ax)\nax.set_title('Heatmap Continous Features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Corelation Matrix - Price Vs Categorical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the correlation matrix. This gives us a clear idea of what might be the features and their corelation\nplt.figure(figsize = (12,8))     # Size of the figure\n\ndf_cor = pd.DataFrame(norm_cp)\n\n#Dropping all Continous Features for plotting purposes...\nfor feat in Continous_feature_list:\n    if feat == 'charges':\n        continue;\n    df_cor.drop(feat, axis=1, inplace=True)\n\nax = plt.axes()   \ncor_mat = round(df_cor.corr(),1)\nsns.heatmap(cor_mat, annot = True)\nax.set_title('Heatmap Continous Features')\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## To avoid overfitting of the models, splitting to test and train is a must! Global control parameters of train and test split along with random seed"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Declare a Training set of size 80%\nmtrnsz = 0.8\n\n#Declare a Testing set of size 20%\nmtstsz = (1 - mtrnsz)\n\n#Init a random seed value to be used throughout\nrsz = 100\n\n#Dependent Feature for our model\nDependant_feature = ['charges']\ny_feature = norm_cp[Dependant_feature]\n\n#Total independant features of the model\nIndependent_features = list(set(norm_cp.columns) - set(Dependant_feature))\n\n#Keeps track of the current feature to be tested\nFeatureToTest = []\n\n#Keeps track of chosen features\nChosenFeatureList = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Methodologies to approach multi-linear Regression\n### 1) Backward Elimination method\n####      a) Eliminate based on RFE\n####      b) Eliminate based on VIF & P-value\n### 2) Forward Injection method (injecting one Feature at a time)\n### The current solution chooses the Forward Injection approach as we have better control"},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_model_stats(curmodel, printop=False):\n    #genrate a Dataframe extracting feature names and their p-values\n    df = gen_pvaluesdf(curmodel)\n\n    #Variance Inflation Factor for detecting Multicollinearity\n    df_vif = calculate_vif(X)\n    #Merge the 2 dataframes to create one df and sort on significance of pvalue along with VIF info\n    #Also, display the R-square and Adjustd R-square\n    df_p_vif = pd.merge(df_vif, df)\n\n    if printop == True:\n\n        print('='*33)\n        print('| Current Features in the model: |')\n        print('='*33)\n        print(ChosenFeatureList)\n        \n        print('='*19)\n        print('| Important Stats: |')\n        print('='*48)\n        print('| R-squared:', round(curmodel.rsquared,3),\n              ' | Adjusted R-squared:', round(curmodel.rsquared_adj,3), '|')\n        print('='*48)\n        df_p_vif.sort_values(by='pval', ascending=False)\n        display(df_p_vif)\n\n        print('='*18)\n        print('| Tested Feature: |')\n        print('='*18)\n        print(FeatureToTest)       ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 1: Starting with Continous Features (Inject based on Corelation matrix)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Total numer of models tested to arrive at solution\nnumModelsTested = 0\n\n#Keeps track of features being tested\nFeatureListToTest = []\n\n#Keeps track of chosen features\nChosenFeatureList = []\n\nnumModelsTested += 1\ncont_featurenum = 0\n\nFeatureToTest = CIF[cont_featurenum]\nFeatureListToTest.append(FeatureToTest)\n\n#We could have chosen any arbitrary list of Continous features to begin with but as\n#the below features are very highly corelated with price of a car; hence choosing to start with these\nX_features = norm_cp[FeatureListToTest]\n\nX, X_test, y, y_test = train_test_split(X_features, y_feature, train_size=mtrnsz, test_size=mtstsz, random_state=rsz)\nX = sm.add_constant(X)\nlm1 = sm.OLS(y,X).fit()\n\ndisplay_model_stats(lm1, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AGE seems to be a good fit. Adding this to chosenfeaturelist"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Add the chosen feature to list of chosenfeatures\nChosenFeatureList.append(FeatureToTest)\n\nnumModelsTested += 1\ncont_featurenum += 1\n\nFeatureToTest = CIF[cont_featurenum]\nFeatureListToTest.append(FeatureToTest)\n\n#We could have chosen any arbitrary list of Continous features to begin with but as\n#the below features are very highly corelated with price of a car; hence choosing to start with these\nX_features = norm_cp[FeatureListToTest]\n\nX, X_test, y, y_test = train_test_split(X_features, y_feature, train_size=mtrnsz, test_size=mtstsz, random_state=rsz)\nX = sm.add_constant(X)\nlm2 = sm.OLS(y,X).fit()\n\ndisplay_model_stats(lm2, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"bmi also seems to be a good fit. Increased adjusted Rsq and all variables are significant and low VIF.\nAdding this to chosenfeaturelist"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Add the chosen feature to list of chosenfeatures\nChosenFeatureList.append(FeatureToTest)\n\nnumModelsTested += 1\ncont_featurenum += 1\n\nFeatureToTest = CIF[cont_featurenum]\nFeatureListToTest.append(FeatureToTest)\n\n#We could have chosen any arbitrary list of Continous features to begin with but as\n#the below features are very highly corelated with price of a car; hence choosing to start with these\nX_features = norm_cp[FeatureListToTest]\n\nX, X_test, y, y_test = train_test_split(X_features, y_feature, train_size=mtrnsz, test_size=mtstsz, random_state=rsz)\nX = sm.add_constant(X)\nlm3 = sm.OLS(y,X).fit()\n\ndisplay_model_stats(lm3, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation: At this point, children seems to be insignificant . Deciding to drop the feature now. We can revisit later to check if adding this feature makes sense"},{"metadata":{"trusted":true},"cell_type":"code","source":"FeatureListToTest.remove('children')\nChosenFeatureList","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CatIF = ['sex_male', 'smoker_yes', 'region_northeast', 'region_northwest', 'region_southeast']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Starting with Categorical Features; Adding single Features at a time\n#### Note that n-1 subgroups are enough to represent n Categorical subgroups. \n### The choice for not using 1 subgroup in each category can be arbitrary. **_A better way is to drop the least frequent subgroup_**"},{"metadata":{"trusted":true},"cell_type":"code","source":"numModelsTested += 1\ncat_var_num = 0\n\nFeatureToTest = CatIF[cat_var_num]\nFeatureListToTest.append(FeatureToTest)\n\n#We could have chosen any arbitrary list of Continous features to begin with but as\n#the below features are very highly corelated with price of a car; hence choosing to start with these\nX_features = norm_cp[FeatureListToTest]\n\nX, X_test, y, y_test = train_test_split(X_features, y_feature, train_size=mtrnsz, test_size=mtstsz, random_state=rsz)\nX = sm.add_constant(X)\nlm6 = sm.OLS(y,X).fit()\n\ndisplay_model_stats(lm6, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"fueltype has High VIF and also p-values become insignificant, drop the feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"FeatureListToTest.remove('sex_male')\nnumModelsTested += 1\n\ncat_var_num += 1\nFeatureToTest = CatIF[cat_var_num]\n\nFeatureListToTest.append(FeatureToTest)\n\n#We could have chosen any arbitrary list of Continous features to begin with but as\n#the below features are very highly corelated with price of a car; hence choosing to start with these\nX_features = norm_cp[FeatureListToTest]\n\nX, X_test, y, y_test = train_test_split(X_features, y_feature, train_size=mtrnsz, test_size=mtstsz, random_state=rsz)\nX = sm.add_constant(X)\nlm6_1 = sm.OLS(y,X).fit()\n\ndisplay_model_stats(lm6_1, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"smoker_yes has significant p-value "},{"metadata":{"trusted":true},"cell_type":"code","source":"ChosenFeatureList.append(FeatureToTest)\nnumModelsTested += 1\n\nFeatureToTest = ['region_northeast', 'region_northwest', 'region_southeast']\nFeatureListToTest.extend(FeatureToTest)\n\n#We could have chosen any arbitrary list of Continous features to begin with but as\n#the below features are very highly corelated with price of a car; hence choosing to start with these\nX_features = norm_cp[FeatureListToTest]\n\nX, X_test, y, y_test = train_test_split(X_features, y_feature, train_size=mtrnsz, test_size=mtstsz, random_state=rsz)\nX = sm.add_constant(X)\nlm6_2 = sm.OLS(y,X).fit()\n\ndisplay_model_stats(lm6_2, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Important Observation : region_southeast and region_northwest causes p-values to become insignificant for certain subgroups. One cannot selectively drop individual subgroups from a Categorical Feature as this will create an inherent bias in the system and destroy the interpretation. \n\n### Explanation: Due to get_dummies(), the reference subgroup is dropped and is interpreted based on remaining subgroups. basically if all subgroups = 0, then reference subgroup = 1. But lets say we drop > 1 subgroup, then its impossible to interpret the data and an artificial bias is created in the system. So it is imperative that a feature is either chosen as a whole or dropped fully"},{"metadata":{},"cell_type":"markdown","source":"## Finally lm7 survives with best possible predection with below features"},{"metadata":{"trusted":true},"cell_type":"code","source":"ChosenFeatureList","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FeatureToTest = FeatureListToTest\n\nX_features = norm_cp[ChosenFeatureList]\n\nX, X_test, y, y_test = train_test_split(X_features, y_feature, random_state=0)\n\nX = sm.add_constant(X)\nlm_final = sm.OLS(y,X).fit()\n\n#Print summary\nprint(lm_final.summary())\n\n#Print Variance Inflation Factor for detecting Multicollinearity\ndisplay_model_stats(lm_final, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Total number of models tested are:\n\nprint('Total num of Models tested are:', numModelsTested)\nprint('The final set of important features are:', ChosenFeatureList)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nX, X_test, y, y_test = train_test_split(X_features, y_feature, train_size=mtrnsz, test_size=mtstsz, random_state=rsz)\n\nforest = RandomForestRegressor(n_estimators = 20,\n                              criterion = 'mse',\n                              random_state = 0,\n                              n_jobs = -1)\n\nforest.fit(X, y)\nforest_train_pred = forest.predict(X)\nforest_test_pred = forest.predict(X_test)\n\nprint('MSE train data: %.3f, MSE test data: %.3f' % (\n\nmean_squared_error(y, forest_train_pred),\n\nmean_squared_error(y_test, forest_test_pred)))\n\nprint('R2 train data: %.3f, R2 test data: %.3f' % (\n\nr2_score(y, forest_train_pred),\nr2_score(y_test, forest_test_pred)))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)Clearly random forests are very good predictors! 80% but possibility of overfitting.\nLets see how a vanilla LR does on the same data"},{"metadata":{},"cell_type":"markdown","source":"## Model Evaluation: Making predictions with the final model and evaluating the model using the Test split data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Use Test data-set to evaluate the model\nX_test = sm.add_constant(X_test)\nPredicted_charges = lm_final.predict(X_test)\n\n#Obtain Mean Squared Error\nmse = mean_squared_error(y_test.charges, Predicted_charges)\n#Obtain Rsquare based score\nr_squared = r2_score(y_test.charges, Predicted_charges)\n\nprint('Mean_Squared_Error :' ,mse)\nprint('r_square_value :',r_squared)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Actual vs Predicted\nc = [i for i in range(1,len(y_test)+1,1)]\nfig = plt.figure(figsize=(20,5))\nplt.plot(c,y_test.charges, color=\"blue\", linewidth=1.5, linestyle=\"-\")\nplt.plot(c,Predicted_charges, color=\"red\",  linewidth=1.5, linestyle=\"-\")\nfig.suptitle('Actual and Predicted', fontsize=20)              # Plot heading \nplt.xlabel('Index', fontsize=18)                               # X-label\nplt.ylabel('Price', fontsize=16)                               # Y-label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Error terms\nc = [i for i in range(1,len(y_test)+1,1)]\nfig = plt.figure(figsize=(20,5))\nplt.plot(c,y_test.charges-Predicted_charges, color=\"blue\", linewidth=1.5, linestyle=\"-\")\nfig.suptitle('Error Terms', fontsize=20)              # Plot heading \nplt.xlabel('Index', fontsize=18)                      # X-label\nplt.ylabel('Y_test.price-Predicted_price', fontsize=16)                # Y-label\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### The prediction acccuracy of the model stands at an R-square of 74.6% and adjusted R-square of 74.5% based on this the final feature list can be decided"},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"### Final Feature List: Below features _(in order of priority)_ are what determine the charges\n    1) smoker\n    2) age\n    3) bmi"},{"metadata":{},"cell_type":"markdown","source":"charges = -0.0027 + (smoking $*$ 0.7937) + (age $*$ 0.3329) + (bmi $*$ 0.1916) \nBased on the above equation, we can easily interpret how a unit change in each variable would affect the charges"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":1}