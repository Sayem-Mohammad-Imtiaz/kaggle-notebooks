{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"file_extension":".py","nbconvert_exporter":"python","pygments_lexer":"ipython3","name":"python","version":"3.6.1","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python"}},"nbformat":4,"cells":[{"metadata":{"_uuid":"f327f21f9ca18747132644e41d9d81c8897f7c42","_cell_guid":"d7f2c0e7-1ac6-44ce-9331-935cae94e711"},"cell_type":"markdown","source":"# In this nodebook, simple RNN and LTSM are used for spam detection\n\nHave a look at this notebook for spam detection tutorial. (dataset is same ). Misclassifidation number is only 16 by logistic regression implemented here:  https://nbviewer.jupyter.org/gist/Juice178/6f5040fe6afc542c8db87ca865d2f21c"},{"metadata":{},"cell_type":"markdown","source":"# 1 RNN"},{"metadata":{},"execution_count":null,"cell_type":"code","source":"from keras.layers import SimpleRNN, Embedding, Dense, LSTM\nfrom keras.models import Sequential\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns; sns.set()","outputs":[]},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"data = pd.read_csv(\"../input/SPAM text message 20170820 - Data.csv\")","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"texts = []\nlabels = []\nfor i, label in enumerate(data['Category']):\n    texts.append(data['Message'][i])\n    if label == 'ham':\n        labels.append(0)\n    else:\n        labels.append(1)\n\ntexts = np.asarray(texts)\nlabels = np.asarray(labels)\n\n\nprint(\"number of texts :\" , len(texts))\nprint(\"number of labels: \", len(labels))","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"from keras.layers import SimpleRNN, Embedding, Dense, LSTM\nfrom keras.models import Sequential\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# number of words used as features\nmax_features = 10000\n# cut off the words after seeing 500 words in each document(email)\nmaxlen = 500\n\n\n# we will use 80% of data as training, 20% as validation data\ntraining_samples = int(5572 * .8)\nvalidation_samples = int(5572 - training_samples)\n# sanity check\nprint(len(texts) == (training_samples + validation_samples))\nprint(\"The number of training {0}, validation {1} \".format(training_samples, validation_samples))\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\n\nword_index = tokenizer.word_index\nprint(\"Found {0} unique words: \".format(len(word_index)))\n\ndata = pad_sequences(sequences, maxlen=maxlen)\n\nprint(\"data shape: \", data.shape)\n\nnp.random.seed(42)\n# shuffle data\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\n\n\ntexts_train = data[:training_samples]\ny_train = labels[:training_samples]\ntexts_test = data[training_samples:]\ny_test = labels[training_samples:]","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nhistory_rnn = model.fit(texts_train, y_train, epochs=10, batch_size=60, validation_split=0.2)","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"acc = history_rnn.history['acc']\nval_acc = history_rnn.history['val_acc']\nloss = history_rnn.history['loss']\nval_loss = history_rnn.history['val_loss']\nepochs = range(len(acc))\nplt.plot(epochs, acc, '-', color='orange', label='training acc')\nplt.plot(epochs, val_acc, '-', color='blue', label='validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, loss, '-', color='orange', label='training acc')\nplt.plot(epochs, val_loss,  '-', color='blue', label='validation acc')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"pred = model.predict_classes(texts_test)\nacc = model.evaluate(texts_test, y_test)\nproba_rnn = model.predict_proba(texts_test)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Test loss is {0:.2f} accuracy is {1:.2f}  \".format(acc[0],acc[1]))\nprint(confusion_matrix(pred, y_test))","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LTSM"},{"metadata":{},"execution_count":null,"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(LSTM(32))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nhistory_ltsm = model.fit(texts_train, y_train, epochs=10, batch_size=60, validation_split=0.2)","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"acc = history_ltsm.history['acc']\nval_acc = history_ltsm.history['val_acc']\nloss = history_ltsm.history['loss']\nval_loss = history_ltsm.history['val_loss']\nepochs = range(len(acc))\nplt.plot(epochs, acc, '-', color='orange', label='training acc')\nplt.plot(epochs, val_acc, '-', color='blue', label='validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, loss, '-', color='orange', label='training acc')\nplt.plot(epochs, val_loss,  '-', color='blue', label='validation acc')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"pred = model.predict_classes(texts_test)\nacc = model.evaluate(texts_test, y_test)\nproba_ltsm = model.predict_proba(texts_test)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Test loss is {0:.2f} accuracy is {1:.2f}  \".format(acc[0],acc[1]))\nprint(confusion_matrix(pred, y_test))","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble method (combining RNN and LTSM)"},{"metadata":{},"execution_count":null,"cell_type":"code","source":"ensemble_proba = 0.3 * proba_rnn + 0.7 * proba_ltsm","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"ensemble_proba[:5]","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"ensemble_class = np.array([1 if i >= 0.5 else 0 for i in ensemble_proba])","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"ensemble_class[:5]","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"print(confusion_matrix(pred, y_test))","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# How to improve further?\nUse ensemble method with logistic regression implemented above or other classifiers, which should imporve accuracy"},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"","outputs":[]}],"nbformat_minor":1}