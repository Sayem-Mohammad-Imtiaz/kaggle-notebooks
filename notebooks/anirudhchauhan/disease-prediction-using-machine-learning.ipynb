{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Overview \n[source](https://www.kaggle.com/uciml/pima-indians-diabetes-database)\n\n## Context\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n## Content\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n<hr />\n\n<h2>Table of Content:</h2>\n\n### 1. Import Libraries and Data \n    1.1 Top 5 rows of data\n    1.2 Last 5 rows of data\n    1.3 Some Random Values from our data \n    1.4 Feature overview\n    \n### 2. Imputations <br />\n\n    2.1 Check For Null/Missing values<br>\n    2.2 Check For Outliers<br>\n    \n### 3. Exploratory data analysis <br />\n    \n### 4. Modeling<br />\n\n<hr />    \n<hr />"},{"metadata":{},"cell_type":"markdown","source":"# Let's Start !\n# 1. Import Libraries and Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.1 Top 5 rows of data "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2 Last 5 rows of data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.3 Some Random Values from our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.4 Feature overview"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape,data.size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    So our data have 768 rows and 9 columns, with a total size of 6912 cells in it."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    From above we can see that none of our column in object or text. (only int and float)\n    \n    We can compute and visualize this data easily."},{"metadata":{},"cell_type":"markdown","source":"# 2. Imputations"},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Check For Null/Missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    So we do not have any null/missing value. Wow, thats great, now lets check for some outliers."},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Searching for Outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(1,1,figsize=(16,7))\nax.boxplot(data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"     So we have some outliers in column 5 i.e BMI column.\n     Come let's fix this."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['BMI'].max(),data['BMI'].min(),data['BMI'].mean(),data['BMI'].mode()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"     I will be using Interquartile range method for pointing out the outliers, \n     as in this we choose all the data between 15% and 85% rest data we can drop."},{"metadata":{"trusted":true},"cell_type":"code","source":"# First quartile (Q1) \nQ1 = np.percentile(data['BMI'], 15, interpolation = 'midpoint') \n  \n# Third quartile (Q3) \nQ3 = np.percentile(data['BMI'], 85, interpolation = 'midpoint') \n  \n# Interquaritle range (IQR) \nIQR = Q3 - Q1 \nprint(\"Q1 = \",Q1)\nprint(\"Q3 = \",Q3)\nprint(IQR) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(data['BMI']>Q3).sum(),(data['BMI']<Q1).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    Here we can see that we have alot data left from these quaritile values.(204 and we can not ignore such large data)\n    \n    So I manually checked for the outliers and got that we have most outliers after 99% of data,\n    means only 1% data contains outliers so lets drop them.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"np.percentile(data['BMI'], 98.5, interpolation = 'midpoint') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val = data['BMI'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"     These are the top values which I am counting as outliers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(val[:8])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[(data['BMI']>50)==False]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    So after removing outliers we are left with 760 rows. \n    \n    Lets check removal of Outliers by plotting a boxplot."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.boxplot(data['BMI'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    So we have our outliars removed and also we are clear from missing data. \n    \n    It's time for some visualizations. \n<hr />"},{"metadata":{},"cell_type":"markdown","source":"# 3. Exploratory data analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Firstly lets check that if our data is balanced or not.\n**Balanced data** : If there are two classes, then balanced data would mean 50% points for each of the class.)\n#### Then we will plot a Distribution plot and other visualizations."},{"metadata":{"trusted":true},"cell_type":"code","source":"xs = data['Outcome'].value_counts().index\nys = data['Outcome'].value_counts().values\n\nax = sns.barplot(xs, ys)\nax.set_xlabel(\"Outcome\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nOur Data is clearly not balanced. We will balance this data using **SMOTE technique** after some visualizations.\n\n**SMOTE** is an oversampling technique where the synthetic samples are generated for the minority class. This algorithm helps to overcome the **overfitting problem posed by random oversampling**. It focuses on the feature space to generate new instances with the help of interpolation between the positive instances that lie together.\n<hr />"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.plot(kind= 'kde' , subplots=True, layout=(3,3), sharex=False, sharey=False, figsize=(15,10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have our maximim columns normally distributed"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Age vs BloodPressure with hue = Outcome\nplt.figure(figsize=(12,8))\nax = sns.scatterplot(x=\"BloodPressure\", y=\"Age\", alpha=0.4, data=data[data['Outcome'] == 0])\nsns.scatterplot(x=\"BloodPressure\", y=\"Age\", alpha=1, data=data[data['Outcome'] == 1], ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# BloodPressure vs BMI with hue = Outcome\nplt.figure(figsize=(12,8))\nax = sns.scatterplot(y=\"BMI\", x=\"BloodPressure\", alpha=0.4, data=data[data['Outcome'] == 0])\nsns.scatterplot(y=\"BMI\", x=\"BloodPressure\", alpha=1, data=data[data['Outcome'] == 1], ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# BloodPressure vs BMI with hue = Outcome\nplt.figure(figsize=(12,8))\nax = sns.scatterplot(y=\"Glucose\", x=\"BloodPressure\", alpha=0.4, color=\"blue\", label=\"0\", data=data[data['Outcome'] == 0])\nsns.scatterplot(x=\"BloodPressure\", y=\"Glucose\", alpha=1, color=\"red\", label=\"1\", data=data[data['Outcome'] == 1], ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Insights\n<hr>\n<h4> - A younger person with high blood pressure level have more chances of getting diabetic positive than a elder person with high blood pressure\n<br><br>\n- An average person with BMI more than 35 have more chances of getting diabetic positive inspite of having a normal blood pressure also.\n<br><br>\n- A person with high glucose level and high blood pressure have more chances of getting diabetic positive.\n<br><br>\n- A person with high glucose level and high BMI can also come diabetic positive.\n</h4>"},{"metadata":{},"cell_type":"markdown","source":"<hr />\nNow let's apply SMOTE for balancing our data.\n\nBalancing data is important for better modeling results.\n<hr />"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting into features and value to be predicted\nX = data.drop(columns=['Outcome'])\ny = data['Outcome']\nfig, ax = plt.subplots(1,2 ,figsize = (10,5))\n\nsns.barplot(x=['0', '1'], y =[sum(y == 0), sum(y == 1)], ax = ax[0])\nax[0].set_title(\"Before Oversampling\")\nax[0].set_xlabel('Outcome')\n\n#Using SMOTE to balance the Data\nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state = 2) \nX, y = sm.fit_resample(X, y) \n\nsns.barplot(x=['0', '1'], y =[sum(y == 0), sum(y == 1)], ax = ax[1])\nax[1].set_title(\"After Oversampling\")\nax[1].set_xlabel('Outcome')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Modeling "},{"metadata":{},"cell_type":"markdown","source":"Let's first split the data into X,y using sklearn functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40)\n\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use the cross validation technique for testing different models on our data.\n\n\n<h4>What is Cross-validation?</h4>\nCross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. \nThe procedure has k number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation.(k is the only parameter given)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('DT', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10)\n    pipeline = make_pipeline(StandardScaler(), RandomForestClassifier())\n    pipeline.fit(X_train, y_train)\n    cv_results = model_selection.cross_val_score(pipeline, X, y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n    \n# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I am using RandomForest Classifier as base and for described modeling.\n\nI am using the **pipeline** feature to make a pipeline for standardising and then only applying proper algorithm. \n<hr />\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report, plot_confusion_matrix\n\npipeline = make_pipeline(StandardScaler(), RandomForestClassifier())\npipeline.fit(X_train, y_train)\nprediction = pipeline.predict(X_test)\n\nprint(f\"Accuracy Score : {round(accuracy_score(y_test, prediction) * 100, 2)}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, we got accuracy score above 80%. Thats great.\n\nNow let's check for the model report."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Thanks for your time :)\n\n## If you like this kernel an Upvote would be appreciated."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}