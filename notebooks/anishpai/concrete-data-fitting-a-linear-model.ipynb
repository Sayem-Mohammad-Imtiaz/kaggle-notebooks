{"cells":[{"metadata":{},"cell_type":"markdown","source":"Welcome to my analysis on Concrete Data. From exploring the data to fitting linear models and using feature engineering and/or selection methods to improve the fit of data on the linear model. I also explored the impact of outliers on the fitting of data onto the linear model."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import RobustScaler\nfrom scipy import stats\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading the dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dfs = pd.read_csv('../input/yeh-concret-data/Concrete_Data_Yeh.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs.columns = ['cement', 'blast_furnace_slag', 'fly_ash','water','superplast','course_aggregate','fine_aggregate','age','compressive_strength']\n#Checking for null values\ndfs.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A quick summary of the values in the dataset. Later we will check for outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting the correlation heatmap for all the features of the dataset\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,10))\nplt.title(\"Correlation Heatmap\")\nsns.heatmap(dfs.corr(), annot=True,cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking for outliers\nThe easiest way to check for outliers is by plotting boxplots"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots() #initialization","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The green diamonds indicate the outlier points in the specific variable "},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n# Green markers indicating outliers in the feature\n\ngreen_diamond = dict(markerfacecolor='g', marker='D')\n\n\nax1 = fig.add_subplot(111)\nax2 = fig.add_subplot(122)\nax3 = fig.add_subplot(133)\n\n\n\nfig, (ax1, ax2,ax3) = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n\nax1.set_title('Cement')\nax1.boxplot(dfs['cement'], flierprops=green_diamond)\n\nax2.set_title('Blast Furnace Slag')\nax2.boxplot(dfs['blast_furnace_slag'], flierprops=green_diamond)\n\nax3.set_title('Fly Ash')\nax3.boxplot(dfs['fly_ash'], flierprops=green_diamond)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax4 = fig.add_subplot(111)\nax5 = fig.add_subplot(122)\nax6 = fig.add_subplot(133)\nfig, (ax4, ax5, ax6) = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n\nax4.set_title('Water')\nax4.boxplot(dfs['water'], flierprops=green_diamond)\n\nax5.set_title('Super Plasticizer')\nax5.boxplot(dfs['superplast'], flierprops=green_diamond)\n\nax6.set_title('Coarse Aggregate')\nax6.boxplot(dfs['course_aggregate'], flierprops=green_diamond)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax7 = fig.add_subplot(111)\nax8 = fig.add_subplot(122)\nax9 = fig.add_subplot(133)\n\nfig, (ax7, ax8, ax9) = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n\nax7.set_title('Fine Aggregate')\nax7.boxplot(dfs['fine_aggregate'], flierprops=green_diamond)\n\nax8.set_title('Age')\nax8.boxplot(dfs['age'], flierprops=green_diamond)\n\nax9.set_title('Compressive Strength')\nax9.boxplot(dfs['compressive_strength'], flierprops=green_diamond)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see there are some outliers in the data mostly in water, compression strength and age. The impact of these can be accessed during predictive modelling."},{"metadata":{},"cell_type":"markdown","source":"## Fitting the data onto a Linear Model\n\n* Split the data set in train and test sets with a proprtion of 20% for test set\n* Apply linear regression\n* Calculate MAE, RMSE and R^2 performance metric for the above model"},{"metadata":{},"cell_type":"markdown","source":"The following linear model is the most simple model possible. The data is blindly fitted onto the model. The error metrics are defined in the next cell"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX= dfs.iloc[:, dfs.columns != 'compressive_strength']\ny = dfs.iloc[:, dfs.columns == 'compressive_strength']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=0)\n\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\n#Predicted Variable\ny_pred = regressor.predict(X_test)\n\n# MAE\nmae = mean_absolute_error(y_test, y_pred)\n\n#RMSE \nrmse = mean_squared_error(y_test,y_pred, squared = False)\n\n#R2_Score\nr2 = r2_score(y_test,y_pred)\nprint(\"For a linear model the errors are as follows-\")\nprint(\"Mean Absolute Error:\", mae)\nprint(\"Root Mean Squared Error:\", rmse)\nprint(\"R2 Score:\",r2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering and Selection Methods"},{"metadata":{},"cell_type":"markdown","source":"Using different techniques of feature selection and/or feature engineering to improve the performance of the model from last task. Calculate MAE, RMSE and R^2 performance metric\n\nFirst, we will visualize the data and make observations about the features."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualizing the correlation among the variables\n\ng = sns.pairplot(dfs, diag_kind=\"kde\")\ng.map_lower(sns.kdeplot, levels=4, color=\".2\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The KDE distributions are a bit off due to the outliers in the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting a Compressive Strength vs Cement, Age, Water plot\nfig_dims = (15, 8)\nfig, ax = plt.subplots(figsize=fig_dims)\nx = dfs['compressive_strength']\ny= dfs['cement']\nsns.scatterplot(y=y, x=x, hue=\"water\",size=\"age\", data=dfs, sizes=(50, 300))\nax.set_title('Compressive Strength vs Cement, Age, Water')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations from the plot are:\n* Compressive Strength increases when Cement composition increases.\n* Compressive Strength increases with Age as darker colors indicate greater age.\n* Compressive Strength increases when less Water is used."},{"metadata":{},"cell_type":"markdown","source":"Now the efficiency of model is accessed by first scaling the data to standardize and then removing the outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs = pd.read_csv('../input/yeh-concret-data/Concrete_Data_Yeh.csv')\nfrom sklearn.preprocessing import RobustScaler\nrs = RobustScaler()\ndfs_scaled = pd.DataFrame(rs.fit_transform(dfs),columns = dfs.columns)\n\nX= dfs_scaled.iloc[:, dfs.columns != 'compressive_strength']\ny = dfs_scaled.iloc[:, dfs.columns == 'compressive_strength']\n\n\n#Removing outliers using interquartile range\nQ1 = dfs.quantile(0.25)\nQ3 = dfs.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)\ndfs_out = dfs[~((dfs < (Q1 - 1.5 * IQR)) |(dfs > (Q3 + 1.5 * IQR))).any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pearson's correlation feature selection for numeric input and numeric output\nfrom sklearn.datasets import make_regression\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\n# generate dataset\nX= dfs_out.iloc[:, [0,1,2,3,4,5,6,7]]\ny = dfs_out.iloc[:, 8]\n\n# define feature selection SELECTING THE BEST FEATURES 7 OUT OF 8\nfs = SelectKBest(score_func=f_regression, k=7)\n# apply feature selection\nX_selected = fs.fit_transform(X, y)\nprint(X_selected.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size= 0.2, random_state=0)\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\ny_pre = regressor.predict(X_test)\nfrom sklearn.metrics import mean_absolute_error\nmaep = mean_absolute_error(y_test, y_pre)\n\nrmsep = rmse = mean_squared_error(y_test,y_pre, squared = False)\nfrom sklearn.metrics import r2_score\nr2p = r2_score(y_test,y_pre)\n\nprint(\"For a linear model the errors are as follows-\")\nprint(\"Mean Absolute Error:\", maep)\nprint(\"Root Mean Squared Error:\", rmsep)\nprint(\"R2 Score:\",r2p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### There is a 16% improvement over a simple linear model\nSelecting the features using Feature ranking with recursive feature elimination method is used to assess the model\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.svm import SVR\nX= dfs.iloc[:, dfs.columns != 'compressive_strength']\ny = dfs.iloc[:, 8]\nestimator = SVR(kernel=\"linear\")\nselector = RFE(estimator, n_features_to_select=5, step=1)\nselector = selector.fit(X, y)\nselector.ranking_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we obtain the best ranking features using SVR estimator.\nThe best ranking features are: cement, blast_furnace_slag,  water, superplast and age"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selecting the number 1 ranking variables only\n# The variables are: cement, blast_furnace_slag,  water, superplast and age\nX= dfs_out.iloc[:, [0,1,3,4,7]]\ny = dfs_out.iloc[:, 8]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=0)\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\ny_pre = regressor.predict(X_test)\nfrom sklearn.metrics import mean_absolute_error\nmaef = mean_absolute_error(y_test, y_pre)\n\nrmsef = mean_squared_error(y_test,y_pre, squared = False)\nfrom sklearn.metrics import r2_score\nr2f = r2_score(y_test,y_pre)\n\nprint(\"For a linear model the errors are as follows-\")\nprint(\"Mean Absolute Error:\", maef)\nprint(\"Root Mean Squared Error:\", rmsef)\nprint(\"R2 Score:\",r2f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Selecting the features using Principal Component Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"X= dfs_scaled.iloc[:, dfs.columns != 'compressive_strength']\ny = dfs_scaled.iloc[:, 8]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=0)\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 4)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\nexplained_variance = pca.explained_variance_ratio_\n\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\ny_pr = regressor.predict(X_test)\n\nfrom sklearn.metrics import mean_absolute_error\nmaepca = mean_absolute_error(y_test, y_pr)\n\nrmsepca = mean_squared_error(y_test,y_pr, squared = False)\nfrom sklearn.metrics import r2_score\nr2pca = r2_score(y_test,y_pr)\n\nprint(\"For a linear model the errors are as follows-\")\nprint(\"Mean Absolute Error:\", maepca)\nprint(\"Root Mean Squared Error:\", rmsepca)\nprint(\"R2 Score:\",r2pca)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Conclusion**"},{"metadata":{},"cell_type":"markdown","source":"Using Pearson's correlation coefficient to find the we obtain the highest Rsquared coefficient indicating a better fit fo the model. However using a different algorithm to predict the outcomes such as decision trees or random forest might be an alternative approach but they tend to overfit the data easily.\n\nUsing PCA methods with 4 features best fits the data into a linear model.\n\nThe best fitting linear model is with using PCA with R2_Score of ~0.8779\n\nAlso, it can be concluded that removal of outliers tends to improve the fit of the data onto a linear model. \n"},{"metadata":{},"cell_type":"markdown","source":"### Thank you for reading. Any suggestions or feedback is welcome!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}