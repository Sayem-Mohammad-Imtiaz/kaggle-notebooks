{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport scipy.stats as stats\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Image\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"EDA\n#Job Title có 2159 Unique value\nIdea: phân thành các nhóm ngành cơ bản : Finance, Police, Engineer, IT, ... 10 ngành\n\n#Tìm dictionary các ngành cơ bản để xử lý string cột Jobtitle (Thầy khuyên dùng Fuzzy text search)\n\n1. Sự khác biệt về lương giữa các ngành => Có thật là có ngành hot hơn\n2. Phân bố lương của từng ngành (Min, max, median, distribution) => ngành nào dễ lương cao. Level có quyết định\n3. Cơ cấu lương của ngành. => biết để né\n4. Model -> cluster\n\nFuzzy text searching"},{"metadata":{},"cell_type":"markdown","source":"# Read the data and first looking"},{"metadata":{},"cell_type":"markdown","source":"This data contains the names, job title, and compensation for San Francisco city employees on an annual basis from 2011 to 2014."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# df = pd.read_csv('/kaggle/input/sf-salaries/Salaries.csv')\ndf = pd.read_csv('/kaggle/input/sf-salaries/Salaries.csv/Salaries.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extract Jobtitle to Career and Level to get more features"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fuzzywuzzy import process\nstr2Match = \"police\"\nstrOptions = \"CAPTAIN III (POLICE DEPARTMENT)\".split()\nRatios = process.extract(str2Match,strOptions)\n# print(Ratios)\n# You can also select the string with the highest matching percentage\nhighest = process.extractOne(str2Match,strOptions)\nprint(highest[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fuzzywuzzy import process\ndef fuzzy_job_field(row):\n    strOptions = row.lower().split()\n    for field, field_key in all_career.items():\n        for key in field_key:\n            highest= process.extractOne(key,strOptions)\n            if highest[1] > 90:\n                return field\n    return \"Other\"\n\ndef fuzzy_job_level(row):\n    strOptions = row.lower().split()\n    for field, field_key in all_career.items():\n        for key in field_key:\n            highest= process.extractOne(key,strOptions)\n            if highest[1] > 90:\n                return field\n    return \"Staff\"\n\n\n#Map để Extract\n# df['Career'] = df['JobTitle'].map(fuzzy_job_field)\n# df['Level'] = df['JobTitle'].map(fuzzy_job_level)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_career = dict({\n    'Fire': ['fire'],\n    'Police': ['police', 'sherif', 'probation', 'sergeant', 'officer', 'lieutenant'],\n    'Transit': ['mta', 'transit', 'truck'],\n    'Medical': ['anesth', 'medical', 'nurs', 'health', 'physician',\n             'orthopedic', 'health', 'pharm', 'care'],\n    'Airport': ['airport'],\n    'Animal': ['animal'],\n    'Architectural': ['architect'],\n    'Court': ['court', 'legal'],\n    'Mayor': ['mayor'],\n    'Library': ['librar'],\n    'Parking': ['parking'],\n    'Public Works': ['public works'],\n    'Attorney': ['attorney'],\n    'Mechanic': ['mechanic', 'automotive'],\n    'Custodian': ['custodian'],\n    'Engineering': ['engineer', 'engr', 'eng', 'program'],\n    'Accounting': ['account'],\n    'Gardening': ['gardener'],\n    'General Laborer': ['general laborer', 'painter', 'inspector',\n                     'carpenter', 'electrician', 'plumber', 'maintenance',\n                        'custodian', 'garden', 'guard', 'clerk', 'porter'],\n    'Food Service': ['food serv'],\n    'Clerk': ['clerk'],\n    'Porter': ['porter'],\n    'Aide': ['aide', 'assistant', 'secretary', 'attendant'],\n    'Data': ['analyst', 'data'],\n    'Airport': ['airport'],\n    'Architect': ['architect'],\n    'Accountant': ['Accountant'],\n    'Mayoral': ['mayoral'],\n    'Recreation': ['recreation'], \n    'Admin': ['Admin', 'account'], \n    'Lawyer': ['attorney', 'lawyer'],\n    'Public Service': ['public service', 'Social Worker'],\n    'Food Service': ['food serv'],\n    'Not provided':['not provide']\n})\n\nall_level = dict({\n    'Manager': ['manager', 'chief'],\n    'Senior': ['senior'],\n    'Junior': ['Junior'],\n    'Trainee': ['trainee'],\n    'Not provided':['not provide']\n})\n\ndef find_job_field(row):\n    for field, field_key in all_career.items():\n        for key in field_key:\n            if key in row.lower():\n                return field\n    return \"Other\"\n\ndef find_job_level(row):\n    for field, field_key in all_level.items():\n        for key in field_key:\n            if key in row.lower():\n                return field\n    return \"Staff\"\n\ndef fuzzy_job_field(row):\n    strOptions = row.lower().split()\n    for field, field_key in all_career.items():\n        for key in field_key:\n            highest= process.extractOne(key,strOptions)\n            if highest[1] > 90:\n                return field\n    return \"Other\"\n\ndef fuzzy_job_level(row):\n    strOptions = row.lower().split()\n    for field, field_key in all_career.items():\n        for key in field_key:\n            highest= process.extractOne(key,strOptions)\n            if highest[1] > 90:\n                return field\n    return \"Staff\"\n\n#Map để Extract\ndf['Career'] = df['JobTitle'].map(find_job_field)\ndf['Level'] = df['JobTitle'].map(find_job_level)\n# df['Career'] = df['JobTitle'].map(fuzzy_job_field)\n# df['Level'] = df['JobTitle'].map(fuzzy_job_level)\n\n# df[df['JobTitle'].str.lower().str.contains('food serv')].JobTitle\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_pay_columns = ['BasePay', 'OvertimePay', 'OtherPay', 'Benefits',\n                   'TotalPay', 'TotalPayBenefits']\n\npay_columns = ['BasePay', 'OvertimePay', 'OtherPay', 'Benefits']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# There 4 sample which are \"Not Provided\" => remove them"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loại bỏ những cột mà được khai là \"Not provide\" (không có giá trị gì)\nprint('Number of Not Provided ', df[(df=='Not provided').any(axis=1)].shape[0])\ndf.drop(df[(df=='Not provided').any(axis=1)].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# There are many non-number value in Pay columns => Convert them to numberic"},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert the pay columns to numeric\nfor col in all_pay_columns:\n    df[col] = pd.to_numeric(df[col], errors='coerce')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We see that some Pay value is negative -> let's check them"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total sample with Negative value is ', df[(df[pay_columns] < 0).any(axis=1)].shape[0])\ndf[(df[all_pay_columns] < 0).any(axis=1)].head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Because there are only 21 sample with negative value -> remove them all."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Data shape before remove Negative sample ', df.shape)\ndf.drop(df[(df[all_pay_columns] < 0).any(axis=1)].index, inplace=True)\nprint('Data shape after remove Negative sample ', df.shape)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check NA value"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"is_null = df.isnull().sum()\nis_null = is_null[is_null>0]\nis_null.sort_values(inplace=True, ascending=False)\n\n#missing data\ntotal = is_null\npercent = is_null/len(df) * 100\n\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\n# plot missing data percent again\n\nprint(missing_data.index)\nplt.figure(figsize=(13, 5))\nsns.set(style='whitegrid')\ng = sns.barplot(x=missing_data.index, y='Percent', data=missing_data)\n\nplt.xticks(rotation = 90)\nplt.title(\"Actual Percentage of missing values.\")\nplt.xticks(rotation=45)\n\n#plot value on top of bar\nfor p in range(len(missing_data)):\n  value = missing_data.iloc[p, 1]\n  g.text(p, value, f'{value:1.2f}%', color='black', ha=\"center\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fillna by Group by median of each Career Group"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"BasePay\"].fillna(df.groupby(\"Career\")[\"BasePay\"].transform(\"median\"), inplace=True)\ndf[\"Benefits\"].fillna(df.groupby(\"Career\")[\"Benefits\"].transform(\"median\"), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Career'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"is_null = df.isnull().sum()\nis_null = is_null[is_null>0]\nis_null.sort_values(inplace=True, ascending=False)\n\n#missing data\ntotal = is_null\npercent = is_null/len(df) * 100\n\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\n# plot missing data percent again\n\nprint(missing_data.index)\nplt.figure(figsize=(13, 5))\nsns.set(style='whitegrid')\ng = sns.barplot(x=missing_data.index, y='Percent', data=missing_data)\n\nplt.xticks(rotation = 90)\nplt.title(\"Actual Percentage of missing values.\")\nplt.xticks(rotation=45)\n\n#plot value on top of bar\nfor p in range(len(missing_data)):\n  value = missing_data.iloc[p, 1]\n  g.text(p, value, f'{value:1.2f}%', color='black', ha=\"center\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We will not use Notes and Status for EDA. => Finished remove NA"},{"metadata":{},"cell_type":"markdown","source":"# Pre Process data"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_pay_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Histogram of all pay"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fig, axes = plt.subplots(6, 1, figsize=(8,24))\n\n# for i in range(6):\n#     df[all_pay_columns[i]].hist(bins=100, ax = axes[i])\n#     axes[i].set_title(all_pay_columns[i])\n\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df[pay_columns])\n# sns.pairplot(df[pay_columns], kind='reg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BasePay va Benefit có high correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sal = df[all_pay_columns]\n# df_sal = df[pay_columns]\n\n\n# Scale the data using the natural logarithm\ndf_log_sal = np.log1p(df_sal)\n\n# Produce a scatter matrix for each pair of newly-transformed features\n# sns.pairplot(df[pay_columns], kind='reg')\n# sns.pairplot(df_log_sal, diag_kind = 'kde', kind='reg')\n\nfig, axes = plt.subplots(6, 2, figsize=(12,24))\nfor i in range(6):\n    df[all_pay_columns[i]].hist(bins=100, ax = axes[i, 0])\n    df_log_sal[all_pay_columns[i]].hist(bins=100, ax = axes[i, 1])\n#     axes[i, 0].set_title(all_pay_columns[i])\n#     axes[i, 1].set_title(all_pay_columns[i])\n    axes[i, 0].set_xlabel(f'Original {all_pay_columns[i]}')\n    axes[i, 1].set_xlabel(f'Log {all_pay_columns[i]}')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We see that Log data is closer to Normal Distribution => use Log Data for DA"},{"metadata":{},"cell_type":"markdown","source":"# Remove Outlier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For each feature find the data points with extreme high or low values\nimport collections \n\noutliers_index_all = []\ndf_find_outliers = df_log_sal\nfor feature in df_find_outliers.keys():\n    \n    # TODO: CalAculate Q1 (25th percentile of the data) for the given feature\n    Q1 = df_find_outliers[feature].quantile(0.25)\n    \n    # TODO: Calculate Q3 (75th percentile of the data) for the given feature\n    Q3 = df_find_outliers[feature].quantile(0.75)\n    \n    # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n    step = (Q3 - Q1) * 1.5\n    \n    # Display the outliers\n    feature_outliers = df_find_outliers[~((df_find_outliers[feature] >= Q1 - step) & (df_find_outliers[feature] <= Q3 + step))]\n    print(f\"{feature_outliers.shape[0]} Data points considered outliers for the feature '{feature}':\")\n#     display(feature_outliers)\n   \n# OPTIONAL: Select the indices for data points you wish to remove\n    outliers_index_all  += feature_outliers.index.tolist()\n\nprint(\"\\nTotal outliers are\" , len(set(outliers_index_all)))\n# print(\"\\nFollowing index were found as outliers in more than one features\")\n# print([(item,count) for item, count in collections.Counter(outliers_index_all).items() if count > 1])\n\noutliers_index = list(set(outliers_index_all))\n# Remove the outliers, if any were specified\ndf_log_sal_no_outlier = df_find_outliers.drop(outliers_index) #.reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For each feature find the data points with extreme high or low values\nimport collections \n\noutliers_index_all = []\ndf_find_outliers = df_sal\nfor feature in df_find_outliers.keys():\n    \n    # TODO: CalAculate Q1 (25th percentile of the data) for the given feature\n    Q1 = df_find_outliers[feature].quantile(0.25)\n    \n    # TODO: Calculate Q3 (75th percentile of the data) for the given feature\n    Q3 = df_find_outliers[feature].quantile(0.75)\n    \n    # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n    step = (Q3 - Q1) * 1.5\n    \n    # Display the outliers\n    feature_outliers = df_find_outliers[~((df_find_outliers[feature] >= Q1 - step) & (df_find_outliers[feature] <= Q3 + step))]\n    print(f\"{feature_outliers.shape[0]} Data points considered outliers for the feature '{feature}':\")\n#     display(feature_outliers)\n   \n# OPTIONAL: Select the indices for data points you wish to remove\n    outliers_index_all  += feature_outliers.index.tolist()\n\nprint(\"\\nTotal outliers are\" , len(set(outliers_index_all)))\n# print(\"\\nFollowing index were found as outliers in more than one features\")\n# print([(item,count) for item, count in collections.Counter(outliers_index_all).items() if count > 1])\n\noutliers_index = list(set(outliers_index_all))\n# Remove the outliers, if any were specified\ndf_sal_no_outlier = df_find_outliers.drop(outliers_index) #.reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Distribution before & after removing outlier Of Original Data & Log Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(6, 2, figsize=(12,24))\nfor i in range(6):\n    df_sal[all_pay_columns[i]].hist(bins=100, ax = axes[i, 0])\n    df_sal_no_outlier[all_pay_columns[i]].hist(bins=100, ax = axes[i, 1])\n#     axes[i, 0].set_title(all_pay_columns[i])\n#     axes[i, 1].set_title(all_pay_columns[i])\n    axes[i, 0].set_xlabel(f'Original {all_pay_columns[i]}')\n    axes[i, 1].set_xlabel(f'Removed outlier {all_pay_columns[i]}')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(6, 2, figsize=(12,24))\nfor i in range(6):\n    df_log_sal[all_pay_columns[i]].hist(bins=100, ax = axes[i, 0])\n    df_log_sal_no_outlier[all_pay_columns[i]].hist(bins=100, ax = axes[i, 1])\n#     axes[i, 0].set_title(all_pay_columns[i])\n#     axes[i, 1].set_title(all_pay_columns[i])\n    axes[i, 0].set_xlabel(f'Log of {all_pay_columns[i]}')\n    axes[i, 1].set_xlabel(f'Removed outlier Log of {all_pay_columns[i]}')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Use Log Data without Outlier for DA"},{"metadata":{},"cell_type":"markdown","source":"# PCA by Salary\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = df.loc[df_log_sal_no_outlier.index]\ndf_model = df_clean[pay_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply PCA by fitting the good data with only two dimensions\ndf_pca = df_model\npca = PCA(n_components=2).fit(df_pca)\n\n# Transform the good data using the PCA fit above\nreduced_data = pca.transform(df_pca)\n\n# Create a DataFrame for the reduced data\nreduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'], index=df_pca.index)\n\n# Generate PCA results plot\npca_table = pd.DataFrame(np.round(pca.components_, 4), columns = list(df_pca.keys()))\nprint('% Variance ', pca.explained_variance_ratio_.cumsum())\npca_table.plot.bar(figsize=(15,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def biplot(good_data, reduced_data, pca, feature_scale_ratio = 1):\n    '''\n    Produce a biplot that shows a scatterplot of the reduced\n    data and the projections of the original features.\n    \n    good_data: original data, before transformation.\n               Needs to be a pandas dataframe with valid column names\n    reduced_data: the reduced data (the first two dimensions are plotted)\n    pca: pca object that contains the components_ attribute\n\n    return: a matplotlib AxesSubplot object (for any additional customization)\n    \n    This procedure is inspired by the script:\n    https://github.com/teddyroland/python-biplot\n    '''\n\n    fig, ax = plt.subplots(figsize = (14,8))\n    # scatterplot of the reduced data    \n    ax.scatter(x=reduced_data.loc[:, 'Dimension 1'], y=reduced_data.loc[:, 'Dimension 2'], \n        facecolors='b', edgecolors='b', s=70, alpha=0.5)\n    \n    feature_vectors = pca.components_.T * feature_scale_ratio\n\n    # we use scaling factors to make the arrows easier to see\n    arrow_size, text_pos = 7.0, 8.0,\n\n    # projections of the original features\n    for i, v in enumerate(feature_vectors):\n        ax.arrow(0, 0, arrow_size*v[0], arrow_size*v[1], \n                  head_width=0.2, head_length=0.2, linewidth=2, color='red')\n        ax.text(v[0]*text_pos, v[1]*text_pos, good_data.columns[i], color='black', \n                 ha='center', va='center', fontsize=18)\n\n    ax.set_xlabel(\"Dimension 1\", fontsize=14)\n    ax.set_ylabel(\"Dimension 2\", fontsize=14)\n    ax.set_title(\"PC plane with original feature projections.\", fontsize=16);\n    return ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"biplot(df_pca, reduced_data, pca, 10000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Insight"},{"metadata":{},"cell_type":"markdown","source":"df\n\ndf_sal & df_sal_no_outlier\n\ndf_log_sal & df_log_sal_no_outlier"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,5))\nsns.countplot('Career', data = df, order = df['Career'].value_counts().index)\nplt.xticks(rotation = 45)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,5))\nsns.countplot('Career', data = df, order = df['Career'].value_counts().index, hue = 'Year')\nplt.xticks(rotation = 45)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.barplot(data= df.groupby('Career')['TotalPayBenefits'].agg('median').reset_index(), x ='Career', y = 'TotalPayBenefits')\nplt.xticks(rotation=85)\nplt.title('Mean Pay')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# top_ten_occupations & df_log_sal_no_outlier"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_input = df.loc[df_log_sal_no_outlier.index]\ntop_ten_occupations = df_input['Career'].value_counts().sort_values(ascending=False)[:10].index\ntop_ten_occupations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"salaries_averages_by_occupation = (df_input[df_input.Career.isin(top_ten_occupations)]\n                                   .groupby('Career')[pay_columns]\n                                   .aggregate('mean')).sort_values('BasePay', 0)\n\nax = salaries_averages_by_occupation.plot(kind='barh', figsize=(8,8))\n\nax.set_xlabel('Mean Pay of Top 10 Polular Career ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"salaries_averages_by_occupation = (df_input[df_input.Career.isin(top_ten_occupations)]\n                                   .groupby('Career')[pay_columns]\n                                   .aggregate('median')).sort_values('BasePay', 0)\n\nax = salaries_averages_by_occupation.plot(kind='barh', figsize=(8,8))\n\nax.set_xlabel('Median Pay of Top 10 Polular Career ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"salaries_averages_by_occupation = (df_input[df_input.Career.isin(top_ten_occupations)]\n                                   .groupby('Career')[pay_columns]\n                                   .aggregate('min')).sort_values('BasePay', 0)\n\nax = salaries_averages_by_occupation.plot(kind='barh', figsize=(8,8))\n\nax.set_xlabel('Min Pay of Top 10 Polular Career ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"salaries_averages_by_occupation = (df_input[df_input.Career.isin(top_ten_occupations)]\n                                   .groupby('Career')[pay_columns]\n                                   .aggregate('max')).sort_values('BasePay', 0)\n\nax = salaries_averages_by_occupation.plot(kind='barh', figsize=(8,8))\n\nax.set_xlabel('Max Pay of Top 10 Polular Career ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"salaries_averages_by_occupation = (df_input[df_input.Career.isin(top_ten_occupations)]\n                                   .groupby('Career')[pay_columns]\n                                   .aggregate('var')).sort_values('BasePay', 0)\n\nax = salaries_averages_by_occupation.plot(kind='barh', figsize=(8,8))\n\nax.set_xlabel('Var Pay of Top 10 Polular Career ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"salaries_averages_by_occupation = (df_input[df_input.Career.isin(top_ten_occupations)]\n                                   .groupby('Career')[pay_columns]\n                                   .aggregate('mean')).sort_values('BasePay', 0)\n\nax.set_xlabel('Mean Pay of Top 10 Polular Career ')\n# the above graph can be transformed into a proportions stacked bar graph\n\n# use the dataframe method div to proportionalize the values by axis=0(row)\nsalary_percents = salaries_averages_by_occupation.div(salaries_averages_by_occupation.sum(1), \n                                                      axis=0)\n\n# and plot the bar graph with a stacked argument.  \nax = salary_percents.plot(kind='bar', stacked=True, rot=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"salaries_averages_by_occupation = (df_input[df_input.Career.isin(top_ten_occupations)]\n                                   .groupby('Career')[pay_columns]\n                                   .aggregate('median')).sort_values('BasePay', 0)\n\nax.set_xlabel('Mean Pay of Top 10 Polular Career ')\n# the above graph can be transformed into a proportions stacked bar graph\n\n# use the dataframe method div to proportionalize the values by axis=0(row)\nsalary_percents = salaries_averages_by_occupation.div(salaries_averages_by_occupation.sum(1), \n                                                      axis=0)\n\n# and plot the bar graph with a stacked argument.  \nax = salary_percents.plot(kind='bar', stacked=True, rot=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# T Test"},{"metadata":{},"cell_type":"markdown","source":"This is a two-sided test for the null hypothesis that 2 independent samples have identical average (expected) values. This test assumes that the populations have identical variances by default."},{"metadata":{"trusted":true},"cell_type":"code","source":"salaries_averages_by_occupation = (df_input[df_input.Career.isin(top_ten_occupations)]\n                                   .groupby('Career')[pay_columns]\n                                   .aggregate('mean')).sort_values('BasePay', 0)\n\nax = salaries_averages_by_occupation.plot(kind='barh', figsize=(8,8))\n\nax.set_xlabel('Mean Pay of Top 10 Polular Career ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sal_feauture = 'BasePay'\ngroup_data = df[df['Career'] == 'Data'][sal_feauture]\ngroup_2 = df[df['Career'] == 'Engineering'][sal_feauture]\n\nprint(\"Data group vs Engineering group\")\nstats.ttest_ind(group_data, group_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sal_feauture = 'TotalPay'\ngroup_data = df[df['Career'] == 'Data'][sal_feauture]\ngroup_medical = df[df['Career'] == 'Medical'][sal_feauture]\n\nprint(\"Data group vs Medical group\")\n\nstats.ttest_ind(group_data, group_medical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sal_feauture = 'TotalPay'\ngroup_1 = df[df['Career'] == 'Police'][sal_feauture]\ngroup_2 = df[df['Career'] == 'Fire'][sal_feauture]\n\n#Null\nprint(\"Police group vs Fire group\")\nstats.ttest_ind(group_1, group_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sal_feauture = 'BasePay'\ngroup_1 = df[df['Career'] == 'Data'][sal_feauture]\ngroup_2 = df[df['Career'] == 'Aide'][sal_feauture]\n\nprint(\"Data group vs Aide group\")\nstats.ttest_ind(group_1, group_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Anova"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"career_group = []\ntemp = top_ten_occupations\n# temp = df['Career'].unique()\nsal_feauture = 'TotalPay'\nfor i in temp:\n    df_temp = df[df['Career']== i]\n#     df_temp['Overtime/Total'] = df_temp['OvertimePay'] / df_temp['TotalPay']\n    result = df_temp['TotalPay'].dropna()\n    career_group.append(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats.levene(*career_group)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats.f_oneway(*career_group)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cluster by Salary"},{"metadata":{},"cell_type":"markdown","source":"# K mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install kneed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\nfrom kneed import KneeLocator\n\n# k means determine k\n# distortions = []\n# inertia = []\n# K = range(1,10)\n# X = df_model\n# for k in K:\n#     kmeanModel = KMeans(n_clusters=k).fit(X)\n#     kmeanModel.fit(X)\n#     #append error\n#     distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n#     inertia.append(kmeanModel.inertia_)\n\n# kn = KneeLocator(list(K), distortions, S=1.0, curve='convex', direction='decreasing')\n# kn2 = KneeLocator(list(K), inertia, S=1.0, curve='convex', direction='decreasing')\n\n# # Plot the elbow\n# fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,4))\n# fig.suptitle('The Elbow Method showing the optimal k')\n\n# ax1.plot(K, distortions, 'bx-')\n# ax1.set_xlabel('k')\n# ax1.set_ylabel('Distortion')\n# ax1.vlines(kn.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')\n\n# ax2.plot(K, distortions, 'bx-')\n# ax2.set_xlabel('k')\n# ax2.set_ylabel('Distortion')\n# ax2.vlines(kn2.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')\n\n# print(\"Number of clusters by distortions\", kn.knee)\n# print(\"Number of clusters by inertia\", kn2.knee)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"url = '/kaggle/input/sf-kmean/kmean1_9.png'\nImage(url)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"url = '/kaggle/input/sf-kmean/kmean10_15.png'\nImage(url)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans_centers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_input = df_model\n\nkmeans = KMeans(n_clusters=4)\nkmeans.fit(df_input)\n\nkmeans_centers = pd.DataFrame(kmeans.cluster_centers_, columns=df_input.columns)\nkmeans_centers_reduced = pd.DataFrame(pca.transform(kmeans_centers), columns = ['Dimension 1', 'Dimension 2'])\nkmeans_centers = pd.concat([kmeans_centers, kmeans_centers_reduced], axis=1)\n\nlabels = pd.DataFrame(kmeans.labels_, columns=['Label'], index = df_input.index)\ncareer_labels = df.loc[df_input.index]['Career']\n\nsal_centers = kmeans_centers\nclustered_data = pd.concat([df_input, reduced_data, career_labels, labels], axis=1)\n\ndisplay(sal_centers.head())\nclustered_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(10,6))\n\nax1.set_title('K Means')\nsns.scatterplot(x='Dimension 1', y='Dimension 2',hue = 'Label',data=clustered_data, ax =ax1)\nax1.scatter(x='Dimension 1', y='Dimension 2', marker=\"X\", c='r', s = 100, data=kmeans_centers)\n\nax2.set_title(\"Original\")\nsns.scatterplot(x='Dimension 1', y='Dimension 2',hue = 'Career',data=clustered_data, ax =ax2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sal_centers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(sal_centers.head())\n#BasePay very High, OT Average\n#BasePay High, OT High\n#BasePay Average, OT Average\n#BasePay Low, OT Low","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(12,8))\nfor i in range(2):\n    df[pay_columns[i]].hist(bins=100, ax = axes[i, 0])\n    df_input[pay_columns[i]].hist(bins=100, ax = axes[i, 1])\n#     axes[i, 0].set_title(all_pay_columns[i])\n#     axes[i, 1].set_title(all_pay_columns[i])\n    axes[i, 0].set_xlabel(f'Original {all_pay_columns[i]}')\n    axes[i, 1].set_xlabel(f'Removed outlier Log of {all_pay_columns[i]}')\n\naxes[1,0].set_xlim([0, 40000])\naxes[1,1].set_xlim([0, 40000])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GaussianMixture"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score\n\ndf_input = df_model\nGM_clusterer = GaussianMixture(4, random_state=0).fit(df_input)\n# GM_clusterer.fit(df_input)\n\nGM_centers = pd.DataFrame(GM_clusterer.means_, columns=df_input.columns)\nGM_centers_reduced = pd.DataFrame(pca.transform(GM_centers), columns = ['Dimension 1', 'Dimension 2'])\nGM_centers = pd.concat([GM_centers, GM_centers_reduced], axis=1)\n\nGM_labels = pd.DataFrame(GM_clusterer.predict(df_input), columns=['GM_labels'], index = df_input.index)\n\nclustered_data = pd.concat([clustered_data, GM_labels], axis=1)\n\ndisplay(GM_centers.head())\nclustered_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(10,6))\n\nax1.set_title('GM')\nsns.scatterplot(x='Dimension 1', y='Dimension 2',hue = 'GM_labels',data=clustered_data, ax =ax1)\nax1.scatter(x='Dimension 1', y='Dimension 2', marker=\"X\", c='r', s = 100, data=GM_centers)\n\nax2.set_title(\"Original\")\nsns.scatterplot(x='Dimension 1', y='Dimension 2',hue = 'Career',data=clustered_data, ax =ax2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.mixture import GaussianMixture\n# from sklearn.metrics import silhouette_score\n\n# def clusterGMM(input_df, k):\n#     global clusterer, preds, centers, sample_preds\n#     # Apply your GMM algorithm to the reduced data \n#     clusterer = GaussianMixture(k, random_state=0).fit(input_df)\n\n#     # Predict the cluster for each data point\n#     preds = clusterer.predict(input_df)\n\n#     # Find the cluster centers\n#     centers = clusterer.means_\n\n#     # Predict the cluster for each transformed sample data point\n# #     sample_preds = clusterer.predict(pca_samples)\n\n#     # Calculate the mean silhouette coefficient for the number of clusters chosen\n#     score = silhouette_score(input_df, preds)\n#     return score\n\n# results = pd.DataFrame(columns=['Silhouette Score'])\n# results.columns.name = 'Number of Clusters'\n\n# for k in range(2,16):\n#     score = clusterGMM(reduced_data, k)\n#     results = results.append(pd.DataFrame([score], columns=['Silhouette Score'], index=[k]))\n# results\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}