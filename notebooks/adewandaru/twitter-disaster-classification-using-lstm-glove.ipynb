{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Disaster NLP\nThis is the notebook for Twitter disaster classification. \\\nThe main technique used here is LSTM that is trained with appropriate word embedding. \\\nThe main framework used here is Pytorch.\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prefix = '/kaggle/input/nlp-getting-started/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest = pd.read_csv(prefix + 'test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(prefix + 'train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([train,test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Emoticons may provide signals for real disaster, so we do not remove it, instead we translate to text."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install emoji\nimport emoji","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_emojis(text):\n    try:\n        text = emoji.demojize(text)\n    except:\n        pass\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"Hello ðŸ˜‚\"\nconvert_emojis(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].apply(convert_emojis)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleanup_url(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub('', text)\n\ndf['text'] = df['text'].apply(cleanup_url)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"cleanup punctuations"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleanup_punctuations(text):\n    import string\n    table = str.maketrans('', '', string.punctuation)    \n    return text.translate(table)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].apply(cleanup_punctuations)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lemmatize sentence"},{"metadata":{"trusted":true},"cell_type":"code","source":"w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n\nwnl = WordNetLemmatizer()\n\ndef lemmatize(sentence):\n    sentence_words = w_tokenizer.tokenize(sentence)\n    new_sentence_words = list()\n    \n    for sentence_word in sentence_words:\n        \n        new_sentence_word = wnl.lemmatize(sentence_word.lower(), wordnet.VERB)\n        new_sentence_words.append(new_sentence_word)\n        \n    new_sentence = ' '.join(new_sentence_words)\n    new_sentence = new_sentence.strip()\n    \n    return new_sentence\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"small_df = df[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"small_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].apply(lemmatize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# just for checkpoint \ndf.to_csv('cleaned.2.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pytorch \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.nn import functional as F\nfrom torch.autograd import Variable\n\nfrom torchtext import data\nfrom torchtext import datasets\nfrom torchtext.vocab import Vectors, GloVe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_csv(df_train, df_test, seed=27, val_ratio=0.3):\n    idx = np.arange(df_train.shape[0])\n    \n    np.random.seed(seed)\n    np.random.shuffle(idx)\n    \n    val_size = int(len(idx) * val_ratio)\n    \n    if not os.path.exists('cache'):\n        os.makedirs('cache')\n    \n    df_train.iloc[idx[val_size:], :][['id', 'target', 'text']].to_csv(\n        'cache/dataset_train.csv', index=False\n    )\n    \n    df_train.iloc[idx[:val_size], :][['id', 'target', 'text']].to_csv(\n        'cache/dataset_val.csv', index=False\n    )\n    \n    df_test[['id', 'text']].to_csv('cache/dataset_test.csv',\n                   index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_iterator(dataset, batch_size, train=True,\n                 shuffle=True, repeat=False):\n    \n    device = torch.device('cuda:0' if torch.cuda.is_available()\n                          else 'cpu')\n    \n    dataset_iter = data.Iterator(\n        dataset, batch_size=batch_size, device=device,\n        train=train, shuffle=shuffle, repeat=repeat,\n        sort=False\n    )\n    \n    return dataset_iter","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Glove Embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"import logging\nimport os\nfrom copy import deepcopy\n\nLOGGER = logging.getLogger('tweets_dataset')\n\ndef get_dataset(fix_length=100, lower=False, vectors=None):\n    \n    if vectors is not None:\n        lower=True\n        \n    LOGGER.debug('Preparing CSV files...')\n    prepare_csv(train, test)\n    \n    TEXT = data.Field(sequential=True, \n                      lower=True, \n                      include_lengths=True, \n                      batch_first=True, \n                      fix_length=25)\n    LABEL = data.Field(use_vocab=True,\n                       sequential=False,\n                       dtype=torch.float16)\n    ID = data.Field(use_vocab=False,\n                    sequential=False,\n                    dtype=torch.float16)\n    \n    \n    LOGGER.debug('Reading train csv files...')\n    \n    train_temp, val_temp = data.TabularDataset.splits(\n        path='cache/', format='csv', skip_header=True,\n        train='dataset_train.csv', validation='dataset_val.csv',\n        fields=[\n            ('id', ID),\n            ('target', LABEL),\n            ('text', TEXT)\n        ]\n    )\n    \n    LOGGER.debug('Reading test csv file...')\n    \n    test_temp = data.TabularDataset(\n        path='cache/dataset_test.csv', format='csv',\n        skip_header=True,\n        fields=[\n            ('id', ID),\n            ('text', TEXT)\n        ]\n    )\n    \n    LOGGER.debug('Building vocabulary...')\n    \n    TEXT.build_vocab(\n        train_temp, val_temp, test_temp,\n        max_size=20000,\n        min_freq=10,\n        vectors=GloVe(name='twitter.27B', dim=200)  # We use it for getting vocabulary of words\n    )\n    LABEL.build_vocab(\n        train_temp\n    )\n    ID.build_vocab(\n        train_temp, val_temp, test_temp\n    )\n    \n    word_embeddings = TEXT.vocab.vectors\n    vocab_size = len(TEXT.vocab)\n    \n    train_iter = get_iterator(train_temp, batch_size=32, \n                              train=True, shuffle=True,\n                              repeat=False)\n    val_iter = get_iterator(val_temp, batch_size=32, \n                            train=True, shuffle=True,\n                            repeat=False)\n    test_iter = get_iterator(test_temp, batch_size=32, \n                             train=False, shuffle=False,\n                             repeat=False)\n    \n    \n    LOGGER.debug('Done preparing the datasets')\n    \n    return TEXT, vocab_size, word_embeddings, train_iter, val_iter, test_iter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEXT, vocab_size, word_embeddings, train_iter, val_iter, test_iter = get_dataset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTMClassifier(torch.nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, weights):\n        super(LSTMClassifier, self).__init__()\n        \n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.word_embeddings = torch.nn.Embedding(vocab_size,\n                                                  embedding_dim)\n        self.word_embeddings.weight = torch.nn.Parameter(weights,\n                                                         requires_grad=False)\n        \n        self.dropout_1 = torch.nn.Dropout(0.3)\n        self.lstm = torch.nn.LSTM(embedding_dim,\n                                  hidden_dim,\n                                  n_layers,\n                                  dropout=0.3,\n                                  batch_first=True)\n        \n        self.dropout_2 = torch.nn.Dropout(0.3)\n        self.label_layer = torch.nn.Linear(hidden_dim, output_size)\n        \n        self.act = torch.nn.Sigmoid()\n        \n    def forward(self, x, hidden):\n        batch_size = x.size(0)\n        \n        x = self.word_embeddings(x)\n        \n        x = self.dropout_1(x)\n        \n        lstm_out, hidden = self.lstm(x, hidden)\n                \n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        out = self.dropout_2(lstm_out)\n        out = self.label_layer(out)    \n        \n        out = out.view(batch_size, -1, self.output_size)\n        out = out[:, -1, :]\n\n        out = self.act(out)\n        \n        return out, hidden\n    \n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        \n        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n        \n        return hidden","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, train_iter, val_iter, optim, loss, num_epochs, batch_size=32):\n    h = model.init_hidden(batch_size)\n    \n    clip = 5\n    val_loss_min = np.Inf\n    \n    total_train_epoch_loss = list()\n    total_train_epoch_acc = list()\n        \n    total_val_epoch_loss = list()\n    total_val_epoch_acc = list()\n        \n    \n    device = torch.device('cuda:0' if torch.cuda.is_available()\n                           else 'cpu')\n    \n    for epoch in range(num_epochs):\n\n        model.train()\n        \n        train_epoch_loss = list()\n        train_epoch_acc = list()\n        \n        val_epoch_loss = list()\n        val_epoch_acc = list()\n        \n        for idx, batch in enumerate(tqdm(train_iter)):\n            h = tuple([e.data for e in h])\n\n            text = batch.text[0]\n            target = batch.target\n            target = target - 1\n            target = target.type(torch.LongTensor)\n\n            text = text.to(device)\n            target = target.to(device)\n\n            optim.zero_grad()\n            \n            if text.size()[0] is not batch_size:\n                continue\n            \n            prediction, h = model(text, h)\n                \n            loss_train = loss(prediction.squeeze(), target)\n            loss_train.backward()\n\n            num_corrects = (torch.max(prediction, 1)[1].\n                                view(target.size()).data == target.data).float().sum()\n\n            acc = 100.0 * num_corrects / len(batch)\n\n            train_epoch_loss.append(loss_train.item())\n            train_epoch_acc.append(acc.item())\n            \n            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n            \n            optim.step()\n    \n        print(f'Train Epoch: {epoch}, Training Loss: {np.mean(train_epoch_loss):.4f}, Training Accuracy: {np.mean(train_epoch_acc): .2f}%')\n\n        model.eval()\n\n        with torch.no_grad():\n            for idx, batch in enumerate(tqdm(val_iter)):\n                val_h = tuple([e.data for e in h])\n\n                text = batch.text[0]\n                target = batch.target\n                target = target - 1\n                target = target.type(torch.LongTensor)\n                \n                text = text.to(device)\n                target = target.to(device)\n                \n                if text.size()[0] is not batch_size:\n                    continue\n\n                prediction, h = model(text, h)\n                loss_val = loss(prediction.squeeze(), target)\n\n                num_corrects = (torch.max(prediction, 1)[1].\n                                view(target.size()).data == target.data).float().sum()\n\n                acc = 100.0 * num_corrects / len(batch)\n\n                val_epoch_loss.append(loss_val.item())\n                val_epoch_acc.append(acc.item())\n                \n            print(f'Validation Epoch: {epoch}, Training Loss: {np.mean(val_epoch_loss):.4f}, Training Accuracy: {np.mean(val_epoch_acc): .2f}%')\n                \n            if np.mean(val_epoch_loss) <= val_loss_min:\n                print('Validation loss decreased ({:.6f} --> {:.6f})'.\n                      format(val_loss_min, np.mean(val_epoch_loss)))\n                \n                val_loss_min = np.mean(val_epoch_loss)\n                \n        total_train_epoch_loss.append(np.mean(train_epoch_loss))\n        total_train_epoch_acc.append(np.mean(train_epoch_acc))\n    \n        total_val_epoch_loss.append(np.mean(val_epoch_loss))\n        total_val_epoch_acc.append(np.mean(val_epoch_acc))\n    \n    return (total_train_epoch_loss, total_train_epoch_acc,\n            total_val_epoch_loss, total_val_epoch_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = 1e-4\nbatch_size = 32\noutput_size = 2\nhidden_size = 128\nembedding_length = 200\n\nmodel = LSTMClassifier(vocab_size=vocab_size, \n                       output_size=output_size, \n                       embedding_dim=embedding_length,\n                       hidden_dim=hidden_size,\n                       n_layers=2,\n                       weights=word_embeddings\n)\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available()\n                      else 'cpu')\n    \nmodel.to(device)\noptim = torch.optim.Adam(model.parameters(), lr=lr)\nloss = torch.nn.CrossEntropyLoss()\n    \ntrain_loss, train_acc, val_loss, val_acc = train_model(model=model,\n                                                       train_iter=train_iter,\n                                                       val_iter=val_iter,\n                                                       optim=optim,\n                                                       loss=loss,\n                                                       num_epochs=20,\n                                                       batch_size=batch_size)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting Loss\nThis section plots loss that is calculated using Cross Entropy vs Epochs (X)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.title('Loss vs Epoch')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\ng = sns.lineplot(range(len(train_loss)), train_loss, label='train')\ng = sns.lineplot(range(len(val_loss)), val_loss, label='test')\ng.set_xticks(range(len(val_loss)))\ng.set_xticklabels(range(len(val_loss)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting Accuracy\n\nThis section plots accuracy vs epochs. \\\nAccuracy is calculated by number of correct prediction divided by total prediction in the validation phase. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.title('Accuracy')\nplt.xlabel('Accuracy')\nplt.ylabel('Loss')\ng = sns.lineplot(range(len(train_acc)), train_acc, label='train')\ng = sns.lineplot(range(len(val_acc)), val_acc, label='test')\ng.set_xticks(range(len(val_acc)))\ng.set_xticklabels(range(len(val_acc)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_target = list()\n\nwith torch.no_grad():\n    for batch in tqdm(test_iter):\n        for text, idx in zip(batch.text[0], batch.id):\n            text = text.unsqueeze(0)\n            res, _ = model(text, hidden=None)\n\n            target = np.round(res.cpu().numpy())\n            \n            results_target.append(target[0][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(prefix+'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['target'] = list(map(int, results_target))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#download the result CSV here\n\nfrom IPython.display import FileLink\nFileLink('/kaggle/working/submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}