{"cells":[{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport plotly.graph_objects as go\nimport plotly.express as px\n\nresultados = pd.DataFrame({\n    'Modelo':[],\n    'Detalles':[],\n    'R2 train':[],\n    'R2 test':[],\n    'MAE train':[],\n    'MAE test':[]\n})\n\ndef error_absoluto_medio(y,y_pred):\n    v = np.abs(y - y_pred)\n    return v.sum()/len(y)\n    \ndef error_gen(y_train,y_pred_train,y_test,y_pred_test):\n    MAE_TRAIN = error_absoluto_medio(y_train,y_pred_train)\n    MAE_TEST = error_absoluto_medio(y_test,y_pred_test)\n    return MAE_TRAIN,MAE_TEST","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lectura e inspección del dataset\n\nEl dataset utilizado no es uno de los más grandes que se pueden encontrar con lo que respecta al precio de venta de casas. No obstante, aprovecharemos la baja cantidad de caracteristicas recopiladas para graficar su relación con la variable a predecir, en este caso el precio; y la relacion entre las mismas. Antes de optar por escoger un algoritmo de regresión, la mejor opcion es dar un vistaso al dataset y analizar el comportamiento de las variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/housesalesprediction/kc_house_data.csv')\ndf.drop(['id'],axis=1,inplace=True)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Valores faltantes\n\nDado que el dataset ha sido acondicionado previamente para su uso, será raro encontrar valores faltantes o valores atápicos(outliers) pero nunca está de más asegurarnos de que los datos están completos.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizacion de datos","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Partiremos con la visualizacion de los histogramas de cada caracteristica del dataset. En la gráfica se observa que <b>price</b>, <b>sqft_living</b>, <b>sqft_above</b> y <b>sqft_living15</b> presentan skew positivo. Esta información será util para determinar que variables serán transformadas para que su distribucion se ajuste a la de la normal y, talvez, ayude a mejorar los resultados de la predicción.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from plotly.subplots import make_subplots\nimport numpy as np\n\nxpoint, ypoint = np.meshgrid(range(4),range(5),indexing='ij')\ncols = df.columns.values\n\nfig2 = make_subplots(rows=4,cols=5)\nfor i,j,col in zip(xpoint.ravel(),ypoint.ravel(),cols):\n    fig2.add_histogram(x=df[col],name=col,row=i+1,col=j+1)\n    \nfig2.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"El dataset utilizado nos proporcina los datos de longitud y latitud de cada casa. Sería un despercidio no utilizarlos para visualizar su posición en un mapa real. Dado esto gracias al token obtenido de <b>MapBox</b> obtenemos un mapa de las casas descritas en el dataset y como están agrupadas en relación a su código zip.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mapbox_access_token = \"pk.eyJ1IjoibGFlY3MiLCJhIjoiY2tkZ3RveWozMjUzNTJ3anFxaGNydnpvYyJ9.Qv6zEMCrHgxm01Xqn4L8gw\"\npx.set_mapbox_access_token(mapbox_access_token)\n\nfig = px.scatter_mapbox(df, lat=\"lat\", lon=\"long\",color=\"zipcode\",\n                        color_continuous_scale=px.colors.cyclical.IceFire,\n                        title='Mapa organizado por código zip',\n                        size_max=15, zoom=8.8,center=dict(lat=47.46,lon=-121.9))\n\nfig.update_layout(\n    autosize=False,\n    height=800,\n    width=1000,\n    hovermode='closest',\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Otra forma de aprovechar los datos de longitud y latitud es observar si estos afectan al precio de la casa. Una forma muy visual puede ser encontrando el precio promedio de cada zona urbana y ordenandolos. De esta forma obtenemos un mapa en el cual el precio promedio de las casas de una zona es mayor mientras más rojo sea y menor mientras más azul sea. Así podemos afirmar:\n\n* La ubicación de la casa si influye en su precio en la mayoria de los caos, siendo los que presentan mayor latitud los que tienen mayor valor.\n\n* Las zonas con mayor costo promedio son aquellas que se encuentran mas cercanas a fuentes naturales de agua tales como  lagos, rios o salidas al mar. Si se contara con la caracteristica de cercania a corrientes de agua seria un indicador que ayudaria a predecir el precio de la vivienda.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_zip = df.groupby(['zipcode']).mean()['price'].sort_values(ascending=True)\ncode_zip = mean_zip.index.values.tolist()\nval_zip = range(mean_zip.shape[0])\n\ncolor = []\nfor code in df['zipcode']:\n    ind = code_zip.index(code)\n    color.append(val_zip[ind])\n\ndf['mean_price_sort'] = color\n\nfig = px.scatter_mapbox(df, lat=\"lat\", lon=\"long\",color=\"mean_price_sort\",\n                        color_continuous_scale=px.colors.cyclical.IceFire, \n                        title='Mapa organizado por el precio medio de la zona',\n                        size_max=15, zoom=8.8,center=dict(lat=47.46,lon=-121.9))\n\nfig.update_layout(autosize=False,\n                  height=800,\n                  width=1000,\n                  hovermode='closest')\n\nfig.show()\n\ndf.drop('mean_price_sort',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cuando se explora un dataset es importante conocer como se relacionan los datos con la variable a predecir ya que esto ayudará a determinar cuales son las que ayudarán a que el modelo realice las mejores predicciones.  En este caso <b>bedrooms</b> y <b>bathrooms</b> parecen tener una relacion aparentemente lineal con <b>price</b>.  Tambien que parece existir una relación lineal entre <b>bathrooms</b> y <b>bedrooms</b>.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig = px.scatter_matrix(df,\n                       dimensions=['price','bedrooms','bathrooms','floors','waterfront']\n                       )\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Otra de las caracteristicas que parecen tener una relacion lineal son <b>sqft_living</b> y <b>sqft_above</b>.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig = px.scatter_matrix(df,\n                       dimensions=['price','sqft_living','sqft_living15','sqft_lot','sqft_lot15','sqft_above','sqft_basement']\n                       )\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En este caso en particular encontramos que <b>grade</b> y <b>price</b> poseen una relacion polinomial.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig = px.scatter_matrix(df,\n                       dimensions=['price', 'view', 'condition', 'yr_built', 'yr_renovated', 'grade']\n                       )\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dado que en el dataset estamos trabajando con 19 características, graficar la relación entre las mismas puede resultar una tarea excesiva por lo que es mejor mostrar solo aquellas que podrían tener una relación entre ellas. Para conocer como se relacionan todas las caracteristicas una opción más visual es realizar una matriz de correlación.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix_corr = df.corr()\n\nheat = go.Heatmap(z=matrix_corr.values,\n                  x=matrix_corr.index.values,\n                  y=matrix_corr.columns.values)\n\nlayout = go.Layout(title='Matriz de correlación',\n                   width=800, height=800,\n                   xaxis_showgrid=False,\n                   yaxis_showgrid=False,\n                   yaxis_autorange='reversed')\n\nfig=go.Figure(data=[heat],layout=layout)        \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En la matriz de correlacion podemos observar que aquellas caracteristicas que identificamos que poseen una aparente relacion lineal con el precio tambien poseen una gran correlacion. Las carecteristicas identificadas fueron : <b>bathrooms</b>, <b>bedrooms</b>, <b>grade</b>, <b>sqft_living</b> y <b>sqft_above</b>.\n\nLa finalidad de la matriz de correlacion no solo es identificar a aquellas caracteristicas que estan más relacionadas con la variable a predecir, sino tambien es identificar aquellas caracteristicas que presentan una alta correlación con otras. Proporcionar al modelo de regresión una gran cantidad de características no siempre es lo adecuado. la existencia de una alta correlacion entre dos características  puede ocasionar problemas de <b>overfitting</b> y fallos en las predicciones.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Otras visualizaciones\n\nAnteriormente hicimos un gráfico de dispersion par poder observa la relación entre las caracteristicas pero esta forma de representación no resulta ser la más simple de comprender si lo que se busca es que el lector entienda la intención del autor.\nDe esta forma utilizando un diagrama de caja  y bigote para cada caracteristica podemos apreciar como aumenta el precio de la casa mientras más baños y dormitorios tiene, y también mientras mejor sea su calidad(<b>grade</b>). \n","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig3 = make_subplots(rows=1,cols=2)\nfig3.add_box(x=df['bathrooms'],y=df['price'],row=1,col=1,name='bathroom')\nfig3.add_box(x=df['bedrooms'],y=df['price'],row=1,col=2,name='bedrooms')\nfig3.show()\n\nfig4 = px.box(df,x='grade',y='price',color='grade',title='Grade Box Plot')\nfig4.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tomando como ejemplo el mundo real, si observáramos una sombra circular no habría forma de saber si el cuerpo que proyecta dicha sombra es una esfera o un cilindro. De esta misma forma podriamos estar obviando alguna relación especial entre las caracteristicas ya que estas se encuentran en N-dimensiones. Dado que nosotros solo podemos hacer representaciones en 3D, aprovecharemos esto para explorar más a fondo los datos y así poder encontrar resultados que se obviaron en las gráficas 2D.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter_3d(df, x='price', y='sqft_living', z='sqft_above',color='grade')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocesamiento de los datos\n\nAntes de ingresar los datos a un modelo, estos deben ser acondicionados para que las predicciones sean las mas acertadas posibles. Para ello se utilizan técnicas como transformaciones, agrupamiento o normalización. Pero antes de realizar el preprocesamiento tengamos una idea de cual sería el resultado de ingresar los datos sin acondicionar a una regresión múltiple.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\ndf2 = df.drop(['date'],axis=1)\n\ndef regresion_lineal(df,test_size=0.2,Prec_var='price'):\n    x = df.drop(Prec_var,axis=1)\n    y = df[Prec_var]\n    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=test_size)\n    lr = LinearRegression()\n    lr.fit(x_train,y_train)\n\n    r2_train = lr.score(x_train,y_train)\n    r2_test = lr.score(x_test,y_test)\n    \n    y_train_pred = lr.predict(x_train)\n    y_test_pred = lr.predict(x_test)\n    \n    MAE_train,MAE_test = error_gen(y_train,y_train_pred,y_test,y_test_pred)\n    \n    return r2_train,r2_test,MAE_train,MAE_test\n\nr2_train,r2_test,MAE_train,MAE_test = regresion_lineal(df2)\n\nfinal = resultados.shape[0]\nresultados.loc[final] = ['Regresión Lineal','Sin procesar',r2_train,r2_test,MAE_train,MAE_test]\nresultados","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Escalación de los datos","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\naux1 =  df2.drop('price',axis=1)\ncol_esc = aux1.columns.values\n\nscale = MinMaxScaler()\n\ndf3 = df2\ndf3[col_esc] = scale.fit_transform(df2[col_esc])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transformación no linear","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Mapeo a distribución gausiana","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\n\nvar_label1 = ['sqft_living', 'sqft_above', 'sqft_living15']\ntransf = PowerTransformer(method='box-cox',standardize=False)\n\nvar_transf = transf.fit_transform(df[var_label1])\n\ndf_tf = df3\ndf_tf[var_label1] = var_transf\n\nxpoint, ypoint = np.meshgrid(range(1),range(3),indexing='ij')\nfig2 =  make_subplots(rows=1,cols=3)\nfor i,j,col in zip(xpoint.ravel(),ypoint.ravel(),var_label1):\n    fig2.add_histogram(x=df_tf[col],name=col,row=i+1,col=j+1)\n\nfig2.update_layout(title_text=\"Histograma de las variables transformadas\")\nfig2.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_train,r2_test,MAE_train,MAE_test = regresion_lineal(df_tf)\n\nfinal = resultados.shape[0]\nresultados.loc[final] = ['Regresión Lineal','Transf. a gausiana',r2_train,r2_test,MAE_train,MAE_test]\nresultados","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Agrupamiento","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import KBinsDiscretizer\n\nvar_label2 = ['yr_built','yr_renovated']\nagrup = KBinsDiscretizer(n_bins=7, encode='ordinal', strategy='uniform')\n\nvar_bin = agrup.fit_transform(df[var_label2])\n\ndf_bin = df3\ndf_bin[var_label2] = var_bin\n\nxpoint, ypoint = np.meshgrid(range(1),range(2),indexing='ij')\nfig2 =  make_subplots(rows=1,cols=2)\nfor i,j,col in zip(xpoint.ravel(),ypoint.ravel(),var_label2):\n    fig2.add_histogram(x=df_bin[col],name=col,row=i+1,col=j+1)\nfig2.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_train,r2_test,MAE_train,MAE_test = regresion_lineal(df_bin)\n\nfinal = resultados.shape[0]\nresultados.loc[final] = ['Regresión Lineal','Agrupamiento',r2_train,r2_test,MAE_train,MAE_test]\nresultados","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Juntando ambas transformaciones","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tf_bin = df3\ndf_tf_bin[var_label1] = var_transf\ndf_tf_bin[var_label2] = var_bin\n\nr2_train,r2_test,MAE_train,MAE_test = regresion_lineal(df_tf_bin,test_size=0.2)\n\nfinal = resultados.shape[0]\nresultados.loc[final] = ['Regresión Lineal','Agrup. y transf.',r2_train,r2_test,MAE_train,MAE_test]\nresultados","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Selección del modelo de regresión\n\nAl usar la regresion lineal obtenemos un porcentaje de precisión del 76%. Este resultado no está mal pero dado que el dataset posee varias características que no presentan una relación lineal con el precio, probaremos con modelos más flexibles.\n\n### Regresión polinómica","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\ndef regresion_poli(df,degree=2,test_size=0.2,Prec_var='price'):\n    x = df.drop(Prec_var,axis=1)\n    y = df[Prec_var]\n    \n    poly = PolynomialFeatures(degree=2,)\n    x_poly = poly.fit_transform(x)\n    \n    x_train,x_test,y_train,y_test = train_test_split(x_poly,y,test_size=test_size)\n        \n    lr = LinearRegression()\n    lr.fit(x_train,y_train)\n    \n    r2_train = lr.score(x_train,y_train)\n    r2_test = lr.score(x_test,y_test)\n    \n    y_train_pred = lr.predict(x_train)\n    y_test_pred = lr.predict(x_test)\n    \n    MAE_train,MAE_test = error_gen(y_train,y_train_pred,y_test,y_test_pred)\n    \n\n    return r2_train,r2_test,MAE_train,MAE_test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_train,r2_test,MAE_train,MAE_test = regresion_poli(df3,degree=3)\nfinal = resultados.shape[0]\nresultados.loc[final] = ['Regresión Polinómica','Sin procesar / Grado 3',r2_train,r2_test,MAE_train,MAE_test]\n\nr2_train,r2_test,MAE_train,MAE_test = regresion_poli(df_bin,degree=3)\nfinal = resultados.shape[0]\nresultados.loc[final] = ['Regresión Polinómica','Agrupamiento / Grado 3',r2_train,r2_test,MAE_train,MAE_test]\n\nr2_train,r2_test,MAE_train,MAE_test = regresion_poli(df_tf,degree=3)\nfinal = resultados.shape[0]\nresultados.loc[final] = ['Regresión Polinómica','Transf. a gausiana / Grado 3',r2_train,r2_test,MAE_train,MAE_test]\n\nr2_train,r2_test,MAE_train,MAE_test = regresion_poli(df_tf_bin,degree=3)\nfinal = resultados.shape[0]\nresultados.loc[final] = ['Regresión Polinómica','Agrup. y transf. / Grado 3',r2_train,r2_test,MAE_train,MAE_test]\nresultados","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Árbol de regresión ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ndef Arbol_Regresion(df,test_size=0.2,Prec_var='price'):\n    x = df.drop(Prec_var,axis=1)\n    y = df[Prec_var]\n    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=test_size)\n    \n    tree = DecisionTreeRegressor(min_samples_split=30,min_samples_leaf=10,random_state=0)\n    tree.fit(x_train,y_train)\n    \n    r2_train = tree.score(x_train,y_train)\n    r2_test = tree.score(x_test,y_test)\n    \n    y_train_pred = tree.predict(x_train)\n    y_test_pred = tree.predict(x_test)\n    \n    MAE_train,MAE_test = error_gen(y_train,y_train_pred,y_test,y_test_pred)\n    \n    return r2_train,r2_test, MAE_train,MAE_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_train,r2_tes, MAE_train,MAE_testt = Arbol_Regresion(df3)\nfinal = resultados.shape[0]\nresultados.loc[final] = ['Árbol de regresión','Sin procesar',r2_train,r2_test, MAE_train,MAE_test]\n\nr2_train,r2_test, MAE_train,MAE_test = Arbol_Regresion(df_bin)\nfinal = resultados.shape[0]\nresultados.loc[final] = ['Árbol de regresión','Agrupamiento',r2_train,r2_test, MAE_train,MAE_test]\n\nr2_train,r2_test, MAE_train,MAE_test = Arbol_Regresion(df_tf)\nfinal = resultados.shape[0]\nresultados.loc[final] = ['Árbol de regresión','Transf. a gausiana',r2_train,r2_test, MAE_train,MAE_test]\n\nr2_train,r2_test, MAE_train,MAE_test = Arbol_Regresion(df_tf_bin)\nfinal = resultados.shape[0]\nresultados.loc[final] = ['Árbol de regresión','Agrup. y transf.',r2_train,r2_test, MAE_train,MAE_test]\nresultados","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Redes neuronales - Regresión","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Compilación del modelo","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import regularizers\n\ndef get_model_compile():\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(32,activation='tanh',input_shape=(18,)),\n        tf.keras.layers.Dense(64,activation='relu'),\n        tf.keras.layers.Dropout(0.1),\n        tf.keras.layers.Dense(128,activation='relu'),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(64,activation='relu'),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(1)   \n    ])\n\n    optimizer=tf.keras.optimizers.Adam(0.001)\n    model.compile(loss='mse',\n                 optimizer=optimizer,\n                 metrics=['mae','mse'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### División del dataset y entrenamiento del modelo","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df_tf.drop('price',axis=1)\ny = df_tf['price']\n\nbatch_size = 128\nepochs=500\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)\n\nmodel = get_model_compile()\n\nhistory = model.fit(x_train,y_train,\n                    epochs=epochs,batch_size=batch_size,\n                    validation_data=(x_test,y_test),\n                    shuffle=True,\n                    verbose=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gráfica del progreso del entrenamiento","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"evol_train = pd.DataFrame(history.history)\nevol_train['epoch'] = history.epoch\n\ntrace1 = go.Scatter(x=evol_train['epoch'], y=evol_train['mae'], mode='lines',name='mae')\ntrace2 = go.Scatter(x=evol_train['epoch'], y=evol_train['val_mae'], mode='lines',name='val_mae')\n\ntrace3 = go.Scatter(x=evol_train['epoch'], y=evol_train['mse'], mode='lines',name='mse')\ntrace4 = go.Scatter(x=evol_train['epoch'], y=evol_train['val_mse'], mode='lines',name='val_mse')\n\nfig = make_subplots(rows=1,cols=2)\nfig.add_trace(trace1,row=1,col=1)\nfig.add_trace(trace2,row=1,col=1)\n\nfig.add_trace(trace3,row=1,col=2)\nfig.add_trace(trace4,row=1,col=2)\n\nfig.update_layout(title_text=\"Gráficas de MAE y MSE vs Época\")\nfig.show()\n\ndf_tf['price_pred'] = model.predict(x)\npx.scatter(df_tf,x='price',y='price_pred',title='Gráfica de los precios reales vs los precios predichos ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\ny_pred_train = model.predict(x_train)\ny_pred_test = model.predict(x_test)\n\nr2_train = r2_score(y_train,y_pred_train)\nr2_test = r2_score(y_test,y_pred_test)\n\nMAE_TRAIN,MAE_TEST = error_gen(y_train,y_pred_train[:,0],y_test,y_pred_test[:,0])\n\nfinal = resultados.shape[0]\nresultados.loc[final] = ['ANN','Transf. a gausiana',r2_train,r2_test,MAE_TRAIN,MAE_TEST]\nresultados","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"TensorFlow-GPU","language":"python","name":"trainingenvgpu"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}