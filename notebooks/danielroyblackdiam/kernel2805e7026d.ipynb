{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"papers = pd.read_csv('/kaggle/input/nips-papers/papers.csv')\nprint(type(papers))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group the papers by year\ngroups = papers.groupby('year')\n\n# Determine the size of each group\ncounts = groups.size()\n\n# Visualise the counts as a bar plot\nimport matplotlib.pyplot\n%matplotlib inline\ncounts.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"papers_2010 = papers.loc[papers['year']==2010].copy(deep=False)\npapers_2017 = papers.loc[papers['year']==2017].copy(deep=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the regular expression library\nimport re\n\n# Remove punctuation\npapers_2010['title_processed'] = papers_2010['title'].map(lambda x: re.sub('[,\\.!?]', '', x))\npapers_2017['title_processed'] = papers_2017['title'].map(lambda x: re.sub('[,\\.!?]', '', x))\n\n# Convert the titles to lowercase\npapers_2010['title_processed'] = papers_2010['title_processed'].map(lambda x: x.lower())\npapers_2017['title_processed'] = papers_2017['title_processed'].map(lambda x: x.lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the library with the CountVectorizer method\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\n\n# determine the most important subjects in 2010\n\ndef plot_10_most_common_words(count_data, count_vectorizer):\n    import matplotlib.pyplot as plt\n    words = count_vectorizer.get_feature_names()\n    total_counts = np.zeros(len(words))\n    for t in count_data:\n        total_counts+=t.toarray()[0]\n    \n    count_dict = (zip(words, total_counts))\n    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[1:21]\n    words = [w[0] for w in count_dict]\n    counts = [w[1] for w in count_dict]\n    x_pos = np.arange(len(words)) \n\n    plt.bar(x_pos, counts,align='center')\n    plt.xticks(x_pos, words, rotation=90) \n    plt.xlabel('words')\n    plt.ylabel('counts')\n    plt.title('20 most common words')\n    plt.show()\n    \n    #print(words, counts)\n    most_mentioned_subjects = list(zip(words, counts))\n    print(most_mentioned_subjects)\n\n# Initialise the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words='english')\n\n# Fit and transform the processed titles\ncount_data = count_vectorizer.fit_transform(papers_2010['title_processed'])\n\n# Visualise the 20 most common words\nplot_10_most_common_words(count_data, count_vectorizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(\"ignore\", DeprecationWarning)\n\n# Load the LDA model from sk-learn\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\n \n# Helper function\ndef print_topics(model, count_vectorizer, n_top_words):\n    words = count_vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n        \n# Tweak the two parameters below (use int values below 15)\nnumber_topics = 5\nnumber_words = 5\n\n# Create and fit the LDA model\nlda = LDA(n_components=number_topics)\nlda.fit(count_data)\n\n# Print the topics found by the LDA model\nprint(\"Topics found via LDA:\")\nprint_topics(lda, count_vectorizer, number_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# and in 2017\n\n# Fit and transform the processed titles\ncount_data = count_vectorizer.fit_transform(papers_2017['title_processed'])\n\n# Visualise the 10 most common words\nplot_10_most_common_words(count_data, count_vectorizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(\"ignore\", DeprecationWarning)\n\n# Load the LDA model from sk-learn\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\n \n# Helper function\ndef print_topics(model, count_vectorizer, n_top_words):\n    words = count_vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n        \n# Tweak the two parameters below (use int values below 15)\nnumber_topics = 5\nnumber_words = 5\n\n# Create and fit the LDA model\nlda = LDA(n_components=number_topics)\nlda.fit(count_data)\n\n# Print the topics found by the LDA model\nprint(\"Topics found via LDA:\")\nprint_topics(lda, count_vectorizer, number_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}