{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The goal of this project is to create a website where people can find their estimated house prices by providing relavent information. \nThis notebook is used for data visualization, helping us to understand the data and choose a good model.\n\nThis notebook is inspired by and mostly based on Ruchi Bhatia's work: [Housing Prices: EDA, Permutation Imp, Partial plot](http://https://www.kaggle.com/ruchi798/housing-prices-eda-permutation-imp-partial-plot)","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom geopy.geocoders import Nominatim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install geopy\n#!pip install Nominatim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Mumbai_df = pd.read_csv('../input/housing-prices-in-metropolitan-areas-of-india/Mumbai.csv')\nDelhi_df = pd.read_csv('../input/housing-prices-in-metropolitan-areas-of-india/Delhi.csv')\nChennai_df = pd.read_csv('../input/housing-prices-in-metropolitan-areas-of-india/Chennai.csv')\nHyderabad_df = pd.read_csv('../input/housing-prices-in-metropolitan-areas-of-india/Hyderabad.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Mumbai_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(Mumbai_df.shape)\nMumbai_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"for some reason, number 9 is used to represent if the existence of certain amenity is unknow, so we will just drop them later. Now we will change 9 to np.nan","metadata":{}},{"cell_type":"code","source":"Mumbai_df.replace(9, np.nan, inplace=True)\nDelhi_df.replace(9, np.nan, inplace=True)\nChennai_df.replace(9, np.nan, inplace=True)\nHyderabad_df.replace(9, np.nan, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Mumbai_df = Mumbai_df.dropna()\nDelhi_df = Delhi_df.dropna()\nChennai_df = Chennai_df.dropna()\nHyderabad_df = Hyderabad_df.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Mumbai:{Mumbai_df.shape}\\n\")\nprint(f\"Mumbai:{Delhi_df.shape}\\n\")\nprint(f\"Mumbai:{Chennai_df.shape}\\n\")\nprint(f\"Mumbai:{Hyderabad_df.shape}\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Data types of all columns: \\n{Mumbai_df.dtypes}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"write down the column names for amentities for future analysis.","metadata":{}},{"cell_type":"code","source":"amentities = [\"Wardrobe\",\"Refrigerator\",\"Sofa\",\"DiningTable\",\"TV\",\"GolfCourse\",\"Microwave\",\"BED\",\"LiftAvailable\",\"Children'splayarea\",\"Wifi\",\"AC\",\"Gasconnection\",\"WashingMachine\",\"Hospital\",\"MultipurposeRoom\",\"Cafeteria\",\"StaffQuarter\",\"CarParking\",\"PowerBackup\",\"24X7Security\",\"School\",\"ClubHouse\",\"ATM\",\"SportsFacility\",\"Intercom\",\"ShoppingMall\",\"IndoorGames\",\"RainWaterHarvesting\",\"JoggingTrack\",\"LandscapedGardens\",\"SwimmingPool\",\"Gymnasium\",\"MaintenanceStaff\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Modifying price to price in lakhs(INR) 1 lakhs(INR) == 100,000 rupees","metadata":{}},{"cell_type":"code","source":"Mumbai_df['Price'] = Mumbai_df['Price']/100000\nDelhi_df['Price'] = Delhi_df['Price']/100000\nChennai_df['Price'] = Chennai_df['Price']/100000\nHyderabad_df['Price'] = Hyderabad_df['Price']/100000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing Outliers\n#Mumbai_df = Mumbai_df[Mumbai_df['Price'] < 40000000]\n#Delhi_df = Delhi_df[Delhi_df['Price'] < 4000000]\n#Chennai_df = Chennai_df[Chennai_df['Price'] < 400]\n#Hyderabad_df = Hyderabad_df[Hyderabad_df['Price'] < 400]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"geolocator = Nominatim(user_agent=\"visual\")\n\ndef geogeneration(df):\n    lat = []\n    long = []\n    t = 0\n    for i in df['Location']:\n        location = geolocator.geocode(i, timeout=None)\n        if t%100 == 0:\n            print(t)\n        t += 1\n        try:\n            lat.append(location.latitude)\n            long.append(location.longitude)\n        except:\n            lat.append(\"NA\")\n            long.append(\"NA\")\n    df['Latitude'] = lat\n    df['Longitude'] = long","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"geogeneration(Mumbai_df)\ngeogeneration(Delhi_df)\ngeogeneration(Chennai_df)\ngeogeneration(Hyderabad_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Mumbai_df.to_csv('/kaggle/working/Mumbai_updated.csv')\nDelhi_df.to_csv('/kaggle/working/Delhi_updated.csv')\nChennai_df.to_csv('/kaggle/working/Chennai_updated.csv')\nHyderabad_df.to_csv('/kaggle/working/Hyderabad_updated.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = pd.read_csv('/kaggle/working/Mumbai_updated.csv')\ndf2 = pd.read_csv('/kaggle/working/Delhi_updated.csv')\ndf3 = pd.read_csv('/kaggle/working/Chennai_updated.csv')\ndf4 = pd.read_csv('/kaggle/working/Hyderabad_updated.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = df1.drop(['Unnamed: 0'],axis=1)\ndf2 = df2.drop(['Unnamed: 0'],axis=1)\ndf3 = df3.drop(['Unnamed: 0'],axis=1)\ndf4 = df4.drop(['Unnamed: 0'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def triple_plot(x,title):\n    fig, ax = plt.subplots(3,1,figsize=(20,10),sharex=True)\n    sns.distplot(x,ax=ax[0],bins=150)\n    #ax[0].set(xlabel=None)\n    ax[0].set_title('Histogram + KDE')\n    sns.boxplot(x,ax=ax[1])\n    #ax[1].set(xlabel=None)\n    ax[1].set_title('Boxplot')\n    sns.violinplot(x,ax=ax[2])\n    ax[2].set(xlabel=None)\n    ax[2].set_title('Violin plot')\n    fig.suptitle(title,fontsize=16)\n    #plt.tight_layout(pad=3.0)\n    plt.xlim(0,1000)\n    plt.show()\n\ntriple_plot(df1['Price'],'Distribution of Price(in lakhs) in Mumbai')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"triple_plot(df2['Price'],'Distribution of Price(in lakhs) in Delhi')\ntriple_plot(df3['Price'],'Distribution of Price(in lakhs) in Chennai')\ntriple_plot(df4['Price'],'Distribution of Price(in lakhs) in Hyderabad')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"based on the plots for prices, we see that this dataset has majority of its data points at price under 200 lakhs(INR). For the purpose of our project, we should limit the prices beyond 200 lakhs.","metadata":{}},{"cell_type":"code","source":"corr_matrix = df1.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(28, 16))\nsns.heatmap(data=corr_matrix, annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid = sns.PairGrid(df1)\ngrid.map_diag(sns.distplot)\ngrid.map_offdiag(sns.scatterplot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the image is hard to see, but basically it doesn't provide much information because most of the columns are 0,1. Now I want to investigate if properties having more amenties means they are more expensive. I will sum up all the columns and call it \"NumOfAmentities\"","metadata":{}},{"cell_type":"code","source":"df1['NumOfAmentities'] = df1[amentities].sum(axis=1)\ndf2['NumOfAmentities'] = df2[amentities].sum(axis=1)\ndf3['NumOfAmentities'] = df3[amentities].sum(axis=1)\ndf4['NumOfAmentities'] = df4[amentities].sum(axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x=\"NumOfAmentities\", y=\"Price\", data=df1,marker=\"P\")\nplt.title('Number of Amentities vs Price in Mumbai',size=16)\nplt.gcf().set_size_inches(6,8)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems like my hypothesis is wrong.","metadata":{}},{"cell_type":"code","source":"#Import modules from sklearn\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import f_regression\n\n#Create an instance of StandardScaler and LinearRegression\nscaler = StandardScaler()\nreg = LinearRegression()\n\n#Summary of Data\ndf1.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create independent and dependent variables of Mumbai\nx1 = df1[['Area', 'No. of Bedrooms', 'NumOfAmentities']]\ny1 = df1[['Price']]\n\n#Scaling Inputs\nscaler.fit(x1)\nx_scaled1 = scaler.transform(x1)\n\n#Regression\nreg.fit(x_scaled1, y1)\n\n#Summary table\nreg_summary = pd.DataFrame([['Area'],['No. of Bedrooms'],['NumOfAmentities']], columns = ['Features'])\nreg_summary['Coefficients'] = reg.coef_[0,0], reg.coef_[0,1], reg.coef_[0,2]\nf = f_regression(x_scaled1,y1)\nreg_summary['P-Values'] = f[1][0], f[1][1],f[1][2]\nreg_summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create independent and dependent variables of Delhi\nx2 = df2[['Area', 'No. of Bedrooms', 'NumOfAmentities']]\ny2 = df2[['Price']]\n\n#Scaling Inputs\nscaler.fit(x2)\nx_scaled2 = scaler.transform(x2)\n\n#Regression\nreg.fit(x_scaled2, y2)\n\n#Summary table\nreg_summary = pd.DataFrame([['Area'],['No. of Bedrooms'],['NumOfAmentities']], columns = ['Features'])\nreg_summary['Coefficients'] = reg.coef_[0,0], reg.coef_[0,1], reg.coef_[0,2]\nf2 = f_regression(x_scaled2,y2)\nreg_summary['P-Values'] = f2[1][0], f2[1][1],f2[1][2]\nreg_summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create independent and dependent variables of Chennai\nx3 = df3[['Area', 'No. of Bedrooms', 'NumOfAmentities']]\ny3 = df3[['Price']]\n\n#Scaling Inputs\nscaler.fit(x3)\nx_scaled3 = scaler.transform(x3)\n\n#Regression\nreg.fit(x_scaled3, y3)\n\n#Summary table\nreg_summary = pd.DataFrame([['Area'],['No. of Bedrooms'],['NumOfAmentities']], columns = ['Features'])\nreg_summary['Coefficients'] = reg.coef_[0,0], reg.coef_[0,1], reg.coef_[0,2]\nf3 = f_regression(x_scaled3,y3)\nreg_summary['P-Values'] = f3[1][0], f3[1][1],f3[1][2]\nreg_summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create independent and dependent variables of Hyderabad\nx4 = df4[['Area', 'No. of Bedrooms', 'NumOfAmentities']]\ny4 = df4[['Price']]\n\n#Scaling Inputs\nscaler.fit(x4)\nx_scaled4 = scaler.transform(x4)\n\n#Regression\nreg.fit(x_scaled4, y4)\n\n#Summary table\nreg_summary = pd.DataFrame([['Area'],['No. of Bedrooms'],['NumOfAmentities']], columns = ['Features'])\nreg_summary['Coefficients'] = reg.coef_[0,0], reg.coef_[0,1], reg.coef_[0,2]\nf4 = f_regression(x_scaled4,y4)\nreg_summary['P-Values'] = f4[1][0], f4[1][1],f4[1][2]\nreg_summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the module for the split\nfrom sklearn.model_selection import train_test_split\n\n# Split the variables with an 80-20 split and some random state\n# To have the same split as mine, use random_state = 365\nx_train, x_test, y_train, y_test = train_test_split(x_scaled1, y1, test_size=0.2, random_state=365)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a linear regression object\nreg = LinearRegression()\n# Fit the regression with the scaled TRAIN inputs and targets\nreg.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the outputs of the regression\n# I'll store them in y_hat as this is the 'theoretical' name of the predictions\ny_hat = reg.predict(x_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The simplest way to compare the targets (y_train) and the predictions (y_hat) is to plot them on a scatter plot\n# The closer the points to the 45-degree line, the better the prediction\nplt.scatter(y_train, y_hat)\n# Let's also name the axes\nplt.xlabel('Targets (y_train)',size=18)\nplt.ylabel('Predictions (y_hat)',size=18)\n# Sometimes the plot will have different scales of the x-axis and the y-axis\n# This is an issue as we won't be able to interpret the '45-degree line'\n# We want the x-axis and the y-axis to be the same\nplt.xlim(0,2000)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Another useful check of our model is a residual plot\n# We can plot the PDF of the residuals and check for anomalies\nsns.distplot(y_train - y_hat)\n\n# Include a title\nplt.title(\"Residuals PDF\", size=18)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}