{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-15T12:29:51.474412Z","iopub.execute_input":"2021-08-15T12:29:51.474994Z","iopub.status.idle":"2021-08-15T12:29:51.486925Z","shell.execute_reply.started":"2021-08-15T12:29:51.474955Z","shell.execute_reply":"2021-08-15T12:29:51.486025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\nimport numpy as np \nimport random\nimport pandas as pd \n\n\n\nfrom wordcloud import WordCloud, STOPWORDS\n\n\nimport nltk\n#libraries for stopwords removal\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport random\n\n\nfrom collections import defaultdict\nfrom collections import Counter\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.initializers import Constant\nfrom keras.layers import (LSTM, \n                          Embedding, \n                          BatchNormalization,\n                          Dense, \n                          TimeDistributed, \n                          Dropout, \n                          Bidirectional,\n                          Flatten, \n                          GlobalMaxPool1D)\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers.embeddings import Embedding\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam\n\nfrom sklearn import metrics\nfrom sklearn.metrics import (\n    precision_score, \n    recall_score, \n    f1_score, \n    classification_report,\n    accuracy_score\n)\nfrom sklearn.pipeline import Pipeline","metadata":{"execution":{"iopub.status.busy":"2021-08-15T12:29:51.488134Z","iopub.execute_input":"2021-08-15T12:29:51.488561Z","iopub.status.idle":"2021-08-15T12:29:51.498196Z","shell.execute_reply.started":"2021-08-15T12:29:51.488528Z","shell.execute_reply":"2021-08-15T12:29:51.496961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/disaster-tweets/tweets.csv\", encoding=\"latin-1\")\n\n\ndf = df.dropna(how=\"any\", axis=1)\n\n#finding the text length and creating a new column to save it\ndf['len_of_text'] = df['text'].apply(lambda x: len(x.split(' ')))\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T12:29:51.49997Z","iopub.execute_input":"2021-08-15T12:29:51.500431Z","iopub.status.idle":"2021-08-15T12:29:51.587569Z","shell.execute_reply.started":"2021-08-15T12:29:51.500396Z","shell.execute_reply":"2021-08-15T12:29:51.586673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the source for the included function is https://www.kaggle.com/andreshg/nlp-glove-bert-tf-idf-lstm-explained\n#This is used to clean the text\n\n\n#removing URL\ndef remove_url(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\n#removing emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n#removing html text\ndef remove_html(text):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', text)\n\n#removing improper data using regex \n'''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\ndef clean_text(text):    \n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub(\n        'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \n        '', \n        text\n    )\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    \n    text = remove_url(text)\n    text = remove_emoji(text)\n    text = remove_html(text)\n    \n    return text\n","metadata":{"execution":{"iopub.status.busy":"2021-08-15T12:29:51.588794Z","iopub.execute_input":"2021-08-15T12:29:51.589226Z","iopub.status.idle":"2021-08-15T12:29:51.598685Z","shell.execute_reply.started":"2021-08-15T12:29:51.589195Z","shell.execute_reply":"2021-08-15T12:29:51.597822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''This function makes the data cleaning process more concise as it removes all \nthe words which add very little value to the choosing of write sentences \nfor example 'can', 'be', 'in' etc. '''\n\nno_value_words = stopwords.words('english')   # a seperate dictionary sort of thing which stores all these words\nmore_words = ['u', 'im', 'c'] #these words were further observed in the data\nstop_words = no_value_words + more_words\n\n#stemmers here are the algorithms which help find the root word involved\nstemmer = nltk.SnowballStemmer(\"english\")\n\ndef pr_data(text):    \n    text = clean_text(text)\n    text = ' '.join(stemmer.stem(word) for word in text.split(' ') if word not in stop_words)\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-08-15T12:29:51.599746Z","iopub.execute_input":"2021-08-15T12:29:51.600158Z","iopub.status.idle":"2021-08-15T12:29:51.620407Z","shell.execute_reply.started":"2021-08-15T12:29:51.600127Z","shell.execute_reply":"2021-08-15T12:29:51.619589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''A new column is created to display results which are derived\nafter using stemming and lemmization'''\ndf['clean_text'] = df['text'].apply(pr_data)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-15T12:29:51.621512Z","iopub.execute_input":"2021-08-15T12:29:51.621927Z","iopub.status.idle":"2021-08-15T12:29:54.949275Z","shell.execute_reply.started":"2021-08-15T12:29:51.621896Z","shell.execute_reply":"2021-08-15T12:29:54.948445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(df['target'])\n\ndf['target_record_2'] = le.transform(df['target'])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T12:29:54.950886Z","iopub.execute_input":"2021-08-15T12:29:54.951303Z","iopub.status.idle":"2021-08-15T12:29:54.966134Z","shell.execute_reply.started":"2021-08-15T12:29:54.951272Z","shell.execute_reply":"2021-08-15T12:29:54.96536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nx = df['clean_text']\ny = df['target']\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42,test_size = 0.2)\nprint(len(x_train), len(y_train))\nprint(len(x_test), len(y_test))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T12:29:54.967406Z","iopub.execute_input":"2021-08-15T12:29:54.968063Z","iopub.status.idle":"2021-08-15T12:29:54.978345Z","shell.execute_reply.started":"2021-08-15T12:29:54.968029Z","shell.execute_reply":"2021-08-15T12:29:54.977318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tweets = df['clean_text'].values\ntest_tweets = df['clean_text'].values\ntrain_target = df['target'].values\n","metadata":{"execution":{"iopub.status.busy":"2021-08-15T12:29:54.97976Z","iopub.execute_input":"2021-08-15T12:29:54.980045Z","iopub.status.idle":"2021-08-15T12:29:54.9879Z","shell.execute_reply.started":"2021-08-15T12:29:54.980018Z","shell.execute_reply":"2021-08-15T12:29:54.986971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the length of our vocabulary\nword_tokenizer = Tokenizer()\n\nword_tokenizer.fit_on_texts(train_tweets)\n\nvocab_length = len(word_tokenizer.word_index) + 1\nvocab_length","metadata":{"execution":{"iopub.status.busy":"2021-08-15T12:29:54.989267Z","iopub.execute_input":"2021-08-15T12:29:54.989591Z","iopub.status.idle":"2021-08-15T12:29:55.285483Z","shell.execute_reply.started":"2021-08-15T12:29:54.989561Z","shell.execute_reply":"2021-08-15T12:29:55.284169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def embed(corpus): \n    return word_tokenizer.texts_to_sequences(corpus)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-15T12:29:55.287156Z","iopub.execute_input":"2021-08-15T12:29:55.287581Z","iopub.status.idle":"2021-08-15T12:29:55.29212Z","shell.execute_reply.started":"2021-08-15T12:29:55.287534Z","shell.execute_reply":"2021-08-15T12:29:55.291109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"longest_train = max(train_tweets, key=lambda sentence: len(word_tokenize(sentence)))\nlength_long_sentence = len(word_tokenize(longest_train))\n\ntrain_padded_sentences = pad_sequences(\n    embed(train_tweets), \n    length_long_sentence, \n    padding='post'\n)\ntest_padded_sentences = pad_sequences(\n    embed(test_tweets), \n    length_long_sentence,\n    padding='post'\n)\n\ntrain_padded_sentences","metadata":{"execution":{"iopub.status.busy":"2021-08-15T12:29:55.293616Z","iopub.execute_input":"2021-08-15T12:29:55.294174Z","iopub.status.idle":"2021-08-15T12:29:57.852865Z","shell.execute_reply.started":"2021-08-15T12:29:55.29413Z","shell.execute_reply":"2021-08-15T12:29:57.85169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"em_dictionary = dict()\nem_dim = 100\n\nembed_matrix = np.zeros((vocab_length, em_dim))\n\nfor word, index in word_tokenizer.word_index.items():\n    embed_vector = em_dictionary.get(word)\n    if embed_vector is not None:\n        embed_matrix[index] = embed_vector\n        \nembed_matrix","metadata":{"execution":{"iopub.status.busy":"2021-08-15T12:29:57.854267Z","iopub.execute_input":"2021-08-15T12:29:57.854683Z","iopub.status.idle":"2021-08-15T12:29:57.870811Z","shell.execute_reply.started":"2021-08-15T12:29:57.854641Z","shell.execute_reply":"2021-08-15T12:29:57.869708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def glove_lstm():\n    model = Sequential()\n    \n    model.add(Embedding(\n        input_dim=embed_matrix.shape[0], \n        output_dim=embed_matrix.shape[1], \n        weights = [embed_matrix], \n        input_length=length_long_sentence\n    ))\n    \n    model.add(Bidirectional(LSTM(\n        length_long_sentence, \n        return_sequences = True, \n        recurrent_dropout=0.2\n    )))\n    \n    model.add(GlobalMaxPool1D())\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(length_long_sentence, activation = \"relu\")) #rectified linear unit\n    model.add(Dropout(0.5))\n    model.add(Dense(length_long_sentence, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation = 'sigmoid'))\n    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model\n\nmodel = glove_lstm()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T12:29:57.872282Z","iopub.execute_input":"2021-08-15T12:29:57.872719Z","iopub.status.idle":"2021-08-15T12:29:58.200184Z","shell.execute_reply.started":"2021-08-15T12:29:57.872673Z","shell.execute_reply":"2021-08-15T12:29:58.199098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(train_padded_sentences,train_target,test_size=0.20)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T12:29:58.203451Z","iopub.execute_input":"2021-08-15T12:29:58.203898Z","iopub.status.idle":"2021-08-15T12:29:58.210588Z","shell.execute_reply.started":"2021-08-15T12:29:58.203848Z","shell.execute_reply":"2021-08-15T12:29:58.209239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = glove_lstm()\n\ncheckpoint = ModelCheckpoint(\n    'model.h5', \n    monitor = 'val_loss', \n    verbose = 1, \n    save_best_only = True\n)\nreduce_lr = ReduceLROnPlateau(\n    monitor = 'val_loss', \n    factor = 0.2, \n    verbose = 1, \n    patience = 5,                        \n    min_lr = 0.001\n)\nhistory = model.fit(\n    X_train, \n    y_train, \n    epochs = 7,\n    batch_size = 32,\n    validation_data = (X_test, y_test),\n    verbose = 1,\n    callbacks = [reduce_lr, checkpoint]\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T12:29:58.211849Z","iopub.execute_input":"2021-08-15T12:29:58.212129Z","iopub.status.idle":"2021-08-15T12:31:18.457553Z","shell.execute_reply.started":"2021-08-15T12:29:58.212102Z","shell.execute_reply":"2021-08-15T12:31:18.456711Z"},"trusted":true},"execution_count":null,"outputs":[]}]}