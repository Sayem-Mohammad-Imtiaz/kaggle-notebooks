{"cells":[{"metadata":{},"cell_type":"markdown","source":"#  Final Project - CSE 5334 - Data Mining\n\nName: Manthankumar Patel\n\nStudent ID: 1001778249\n"},{"metadata":{},"cell_type":"markdown","source":"# The Goal of Project:\n\nThe purpose of this project is to build a model that will be trained on given data and will predict the ranting from 0 to 10 from the comment that we give.First, we will import the necessary libraries and the will load the data from csv file 'bgg-13m-reviews.csv'. Now, we will pre-process the data by formating the comments and rating and will divide into DataFrames and vectorize every DataFrames. I have used SVM classifier because it is a supervised machine learning algorithm that can be used for classification. In general terms SVM is very good when you have a huge number of features. For example for text classification in a bag of words model. \n"},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries and Loading Data:\n\nFirst, we will import useful libraries and build csv path to load the data, which upon download is located in a directory called boardgamegeek-reviews, in csv file named ‘bgg-13m-reviews.csv’."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score\n\nfrom sys import path\nfrom os.path import join\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will load the data, and using the pandas library we will read csv file. Then we will use only 'comment' and 'rating' and will replace NaN with ' '."},{"metadata":{"trusted":true},"cell_type":"code","source":"def loadData(path):\n    d = pd.read_csv(path)\n    d = d[['comment', 'rating']].copy()\n    d = d.fillna('')\n    \n    return d","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Formatting Data:\n\nWe have loaded data so we will do some preprocessing by formatting comments and ratings. From the comment, we have formatted white space and lower all the characters. From the rating, we have removed data that has 0 ratings and ensure that all ratings have values that are type float. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def formatRawData(rawData):\n    rawData['comment'] = [c.strip().lower() for c in rawData['comment']]\n    rawData['comment'] = [c if c.islower() else '' for c in rawData['comment']]\n    \n    #only take data with comments\n    rawData = rawData[rawData['comment'].apply(lambda alpha: len(alpha) > 0)]\n    #only take data with ratings 1 or greater\n    rawData = rawData[rawData['rating'].apply(lambda alpha: float(alpha) >= 1)]\n    rawData = rawData.sample(frac = 1).reset_index(drop = True)  \n    \n    rawData['rating'] = [float(rating) for rating in rawData['rating']]\n    \n    \n    #for i in range(len(rawData)):\n     #   words = re.split(r'\\W+', rawData['comment'][i])\n      #  stop_words = set(stopwords.words('english'))\n       # words = [w for w in words if not w in stop_words]\n        #rawData.loc[i, 'comment'] = ' '.join(map(str, words)).lower()\n        \n    return rawData","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rawData = loadData('/kaggle/input/boardgamegeek-reviews/bgg-13m-reviews.csv')\n\nprint(rawData['comment'][0])\nprint(len(rawData))\n\ndata = formatRawData(rawData)\n\nprint(data['comment'][0])\nprint(len(data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # Splitting Data:\n \nNow we have to split dataframe into trainData, devData, testData and reset the index for every dataframe. From print below it can be seen that the train dataframe is close to 2100000 while development and test dataframe is nearly 9.5%  of all data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def divideData(data, train):\n    data = data.sample(frac=1).reset_index(drop = True)\n    #dataLen = len(data['comment'])\n    dataLen = data.shape[0]\n    \n    trainData = data[:int(train * dataLen)]\n    devData = data[int(train * dataLen):int((train + 1)/2 * dataLen)]\n    testData = data[int((train + 1)/2 * dataLen):]\n    \n    trainData = trainData.sample(frac = 1).reset_index(drop = True)\n    devData = devData.sample(frac = 1).reset_index(drop = True)\n    testData = testData.sample(frac = 1).reset_index(drop = True)\n    \n    return [trainData, devData, testData]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainData, devData, testData = divideData(data, 0.95).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(trainData.shape[0])\nprint(devData.shape[0])\nprint(testData.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vectorization:\n\nAs you can see above, I tried to remove stop words from comments but data size is too large so it took an enormously long time so I am using vectorization. Values in the comment are string so we need to convert every string into numerical series using vectorizer. Now we will copy and store every vectorized comment in train_c, test_c, dev_c respectively, and rating in train_r, test_r, dev_r."},{"metadata":{"trusted":true},"cell_type":"code","source":"def vectorizeData(data, vectorizer):\n    data_c = vectorizer.transform(data['comment'])\n    data_r = np.asarray([int(r) if r%int(r) <= 0.5 else int(r + 1) for r in data['rating']])\n    \n    return [data_c, data_r]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(ngram_range=(1, 2))\nvectorizer.fit(trainData['comment'])\ntrain_c, train_r = vectorizeData(trainData, vectorizer).copy()\ntest_c, test_r = vectorizeData(testData, vectorizer).copy()\ndev_c, dev_r = vectorizeData(devData, vectorizer).copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVM Classifier:\n\nFinally, we have data within the appropriate format and that we are able to begin classifier.\nHere I have used the SVM classifier because I researched on the internet that which classifier we can use for large data and I got on the conclusion that SVM is very good for text classification and  I have added the reference link bellow in References for this research. For example SVM with direct tf-idf vectors does the best both for quality & performance for text classification in a bag of words model."},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter Tuning:\n\nNow, it is time to make some improvement in accuracy by hyperparameter tuning. we have to find the Regularization parameter-C to increase performance. we test for different C and show a relation between C and accuracy and we select the most effective value of C."},{"metadata":{"trusted":true},"cell_type":"code","source":"#hyper parameter tuning \nC = [0.1, 0.3, 0.5, 0.7, 0.9, 1]\n\nfor c in C:\n    svm = LinearSVC(C = c)\n    svm.fit(train_c, train_r)\n    \n    pred_r = svm.predict(dev_c)\n\n    accuracy = accuracy_score(dev_r, pred_r)*100\n\n    print('SVM accuracy for C={}: {:.5f}%\\n'.format(c, accuracy))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = LinearSVC(C=0.1)\nsvm.fit(train_c, train_r)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_r = svm.predict(dev_c)\naccuracy = accuracy_score(dev_r, pred_r)*100\nprint('SVM accuracy for C=0.1: {:.5f}%\\n'.format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Final Accuracy\n\npredT_r = svm.predict(test_c)\naccuracy = accuracy_score(test_r, predT_r)*100\nprint('SVM final accuracy: {:.5f}%\\n'.format(accuracy))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, That is a good improvement in accuracy over what SVM could do when it was only by itself. We build the  model that uses SVM and after that, we have done hyperparameter tuning to find optimal value and tested on test data and got good accuracy as you can see above."},{"metadata":{},"cell_type":"markdown","source":"# Conclusion and Challenges: \n\nIn conclusion, we can say that by hyperparameter tuning we can increase accuracy in the SVM classifier. In addition, I learned, how vectorization is used to speed up the Python code without using a loop. \n\nIn this project, there were many challenges because the data set was very huge and tough to predict that's why it took enormous time to execute vectorization and SVM. Plenty of the comments was very long and not all encoded within the same format. As for the ratings, class representation wasn't exactly even. To develop a model with such accuracy that would cope with such issues could likely only be finished by Neural Network. So many questions came in my mind such as-  Which classifier should I use? In which way I can increase performance? How to increase accuracy using hyperparameter tuning? Am I able to execute an optimization idea to extend accuracy? For all of those questions, I got answers.\n\nThank you for reading my blog post.\n\n# References: \n\nShuffle the data:\nhttps://stackoverflow.com/questions/29576430/shuffle-dataframe-rows\n\nVectorization:\nhttps://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n\nSVM:\nhttps://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}