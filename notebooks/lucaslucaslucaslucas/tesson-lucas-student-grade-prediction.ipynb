{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Libraries utiles","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, explained_variance_score, max_error, mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\n\nmatplotlib.rcParams['figure.figsize'] = (10,10)\nsns.set_style('whitegrid')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On importe le dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/student-grade-prediction/student-mat.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Informations sur les données\n1. school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)\n2. sex - student's sex (binary: 'F' - female or 'M' - male)\n3. age - student's age (numeric: from 15 to 22)\n4. address - student's home address type (binary: 'U' - urban or 'R' - rural)\n5. famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)\n6. Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)\n7. Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)\n8. Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)\n9. Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n10. Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n11. reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')\n12. guardian - student's guardian (nominal: 'mother', 'father' or 'other')\n13. traveltime - home to school travel time (numeric: 1 - 1 hour)\n14. studytime - weekly study time (numeric: 1 - 10 hours)\n15. failures - number of past class failures (numeric: n if 1<=n<3, else 4)\n16. schoolsup - extra educational support (binary: yes or no)\n17. famsup - family educational support (binary: yes or no)\n18. paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n19. activities - extra-curricular activities (binary: yes or no)\n20. nursery - attended nursery school (binary: yes or no)\n21. higher - wants to take higher education (binary: yes or no)\n22. internet - Internet access at home (binary: yes or no)\n23. romantic - with a romantic relationship (binary: yes or no)\n24. famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n25. freetime - free time after school (numeric: from 1 - very low to 5 - very high)\n26. goout - going out with friends (numeric: from 1 - very low to 5 - very high)\n27. Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n28. Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n29. health - current health status (numeric: from 1 - very bad to 5 - very good)\n30. absences - number of school absences (numeric: from 0 to 93)","metadata":{}},{"cell_type":"markdown","source":"En plus de ces données, il y a les colonnes G1, G2 et G3 : \n1. G1 - first period grade (numeric: from 0 to 20)\n2. G2 - second period grade (numeric: from 0 to 20)\n3. G3 - final grade (numeric: from 0 to 20, output target)","metadata":{}},{"cell_type":"markdown","source":"Notre résultat sera donc la colonne G3, qui correspond à la note sur 20 de l'examen du troisième trimestre.","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Point faible de ce dataset : peu de données","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Je vais transformer les objets en integer, pour permettre d'utiliser toutes les données.","metadata":{}},{"cell_type":"code","source":"num_features = [name for name in df.columns if df[name].dtype in ['int64', 'float64']]\ncat_features = [name for name in df.columns if df[name].dtype == 'object']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x in cat_features:\n  print(x,\" = \",df[x].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On voit ici qu'on peut transformer chaque \"string\" en integer, par exemple dans \"internet\", 'no' devient 0, et 'yes' devient 1","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nfor i in list(cat_features):\n    df[i]=le.fit_transform(df[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x in cat_features:\n  print(x,\" = \",df[x].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Un peu de statistiques :** ","metadata":{}},{"cell_type":"markdown","source":"Légende :\n* Orange : Femme\n* Bleu : Homme","metadata":{}},{"cell_type":"code","source":"sns.kdeplot(df.groupby('sex').get_group(1)['age'], shade = True,label = 1)\nsns.kdeplot(df.groupby('sex').get_group(0)['age'], shade = True, label = 0)\nplt.xlabel('data range')\nplt.ylabel('% data distribution')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(df.groupby('sex').get_group(1)['studytime'], shade = True,label = 1)\nsns.kdeplot(df.groupby('sex').get_group(0)['studytime'], shade = True, label = 0)\nplt.xlabel('data range')\nplt.ylabel('% data distribution')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(df.groupby('sex').get_group(1)['G1'], shade = True,label = 1)\nsns.kdeplot(df.groupby('sex').get_group(0)['G1'], shade = True, label = 0)\nplt.xlabel('data range')\nplt.ylabel('% data distribution')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (20,50))\nfor i,item in enumerate(['school', 'sex', 'famsize', 'Pstatus', 'Mjob', 'Fjob',\n       'reason', 'guardian', 'schoolsup', 'famsup', 'paid', 'activities',\n       'nursery', 'higher', 'internet', 'romantic']):\n    plt.subplot(9,2,i+1)\n    sns.countplot(df[item])\n    plt.title(item)\n\nplt.show()   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize= (15,10))\nplt.subplot(1,2,1)\norder_by = df.groupby('Fjob')['G1'].median().sort_values(ascending = False).index\nsns.boxplot(x = df['Fjob'], y = df['G1'],order = order_by)\nplt.xticks(rotation = 90)\nplt.title('Fjob v/s G1')\n\nplt.subplot(1,2,2)\norder_by = df.groupby('Mjob')['G1'].median().sort_values(ascending = False).index\nsns.boxplot(x = df['Mjob'], y = df['G1'],order = order_by)\nplt.xticks(rotation = 90)\nplt.title('Mjob v/s G1')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize= (15,5))\nplt.subplot(1,2,1)\norder_by = df.groupby('Fedu')['G1'].median().sort_values(ascending = False).index\nsns.boxplot(x = df['Fedu'], y = df['G1'],order = order_by)\nplt.xticks(rotation = 90)\nplt.title('Fedu v/s G1')\n\nplt.subplot(1,2,2)\norder_by = df.groupby('Medu')['G1'].median().sort_values(ascending = False).index\nsns.boxplot(x = df['Medu'], y = df['G1'],order = order_by)\nplt.xticks(rotation = 90)\nplt.title('Medu v/s G1')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (15,15))\nfor i, item in enumerate(['schoolsup', 'famsup', 'paid', 'activities',\n       'nursery', 'higher', 'internet', 'romantic']):\n    plt.subplot(4,2,i+1)\n    order_by = df.groupby(item)['G1'].median().sort_values(ascending = False).index\n    sns.boxplot(x = df[item], y = df['G1'],order = order_by)\n    plt.xticks(rotation = 90)\n    plt.title(item+' v/s G1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nous pouvons à présent, observer les corrélations","metadata":{}},{"cell_type":"code","source":"corr = df.corr()\nplt.figure(figsize=(25,25))\nsns.heatmap(corr, annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix_corr = df.corr()\nmatrix_corr.G3.sort_values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.clustermap(abs(corr), cmap=\"coolwarm\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Machine learning pour la note G3","metadata":{}},{"cell_type":"code","source":"X = df.drop('G3',axis=1)\ny = df['G3']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Régression linéaire multiple","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr.fit(X_train,y_train)\ny_lr = lr.predict(X_test)\nprint(lr.intercept_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = lr.predict(X_test)  \nplt.scatter(y_test,predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot((y_test-predictions)); ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import mean_squared_error, r2_score\nprint('Erreur absolue médian:', metrics.mean_absolute_error(y_test, predictions))\nprint('Erreur des moindres carrés:', metrics.mean_squared_error(y_test, predictions))\nscoreR2 = r2_score(y_test, predictions)\nprint('Score R2 : ',scoreR2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,12))\nplt.scatter(y_test, predictions)\nplt.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()], color='red', linewidth=3)\nplt.xlabel(\"Note\")\nplt.ylabel(\"Prediction de la note\")\nplt.title(\"Note réelle VS prédiction\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On voit qu'on a de plutôt bons résultats avec cette méthode, avec une erreur médiane de 1,65 points par note et un score R2 de 0,81.\nOn voit néanmoins qu'il y a plusieurs problèmes avec la note de 0.","metadata":{}},{"cell_type":"markdown","source":"# Régression par forêts aléatoires","metadata":{}},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import ensemble\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\n\nrf = ensemble.RandomForestRegressor()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)\nprint(rf.score(X_test,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On voit qu'il y a une amélioration pour la régression par forêts aléatoires. ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,12))\nplt.scatter(y_test, y_rf)\nplt.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()], color='red', linewidth=3)\nplt.xlabel(\"Note\")\nplt.ylabel(\"Prediction de la note\")\nplt.title(\"Note réelle VS prédiction\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(y_test-predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"L'erreur sur les moindres carrés est divisée par deux ici, comparée à la régression linéaire","metadata":{}},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"import xgboost as XGB\nxgb  = XGB.XGBRegressor()\nxgb.fit(X_train, y_train)\ny_xgb = xgb.predict(X_test)\nprint(xgb.score(X_test,y_test))\n\nplt.figure(figsize=(12,12))\nplt.scatter(y_test, y_xgb)\nplt.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()], color='red', linewidth=3)\nplt.xlabel(\"Note\")\nplt.ylabel(\"Prediction de la note\")\nplt.title(\"Note réelle VS prédiction\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"XGBoost est bon mais moins bien que les forêts aléatoires. ","metadata":{}},{"cell_type":"markdown","source":"Nouveaux objectifs, essayer de faire une prédiction sur la note G1 et sur la note G2. Car pour prédire G3, l'aide de G1 et G2 est très importante.\nPourquoi ne pas tenter de prédire G3 sans G1 ni G2 ?\n","metadata":{}},{"cell_type":"markdown","source":"# Prédire G1","metadata":{}},{"cell_type":"markdown","source":"Pour prédire G3, j'ai utilisé G1 et G2, mon objectif maintenant est de prédire la note G1. ","metadata":{}},{"cell_type":"code","source":"X1 = df.drop('G1',axis=1)\nX2 = X1.drop('G2',axis=1)\nX = X2.drop('G3',axis=1)\ny = df['G1']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Régression linéaire multiple","metadata":{}},{"cell_type":"code","source":"lr = LinearRegression()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr.fit(X_train,y_train)\ny_lr = lr.predict(X_test)\nprint(lr.intercept_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = lr.predict(X_test)  \nplt.scatter(y_test,predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot((y_test-predictions)); ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Erreur absolue médian:', metrics.mean_absolute_error(y_test, predictions))\nprint('Erreur des moindres carrés:', metrics.mean_squared_error(y_test, predictions))\nscoreR2 = r2_score(y_test, predictions)\nprint('Score R2 : ',scoreR2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,12))\nplt.scatter(y_test, predictions)\nplt.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()], color='red', linewidth=3)\nplt.xlabel(\"Note\")\nplt.ylabel(\"Prediction de la note\")\nplt.title(\"Note réelle VS prédiction\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Régression par forêts aléatoires","metadata":{}},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = ensemble.RandomForestRegressor()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)\nprint(rf.score(X_test,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" plt.figure(figsize=(12,12))\nplt.scatter(y_test, y_rf)\nplt.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()], color='red', linewidth=3)\nplt.xlabel(\"Note\")\nplt.ylabel(\"Prediction de la note\")\nplt.title(\"Note réelle VS prédiction\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On voit très bien que mes résultats sont très décevants.\nJ'imagine que le résultat sera le même pour G2, je vais donc changer la sortie en disant :\n* Si un élève a plus de 10 / 20, il valide, la sortie sera égale à 1\n* Sinon, la sortie = 0 ","metadata":{}},{"cell_type":"markdown","source":"# Jeux d'apprentissage avec une sortie en booléen ","metadata":{}},{"cell_type":"markdown","source":"On fait la transformation.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[df.G3 <= 9, 'G3'] = 0\ndf.loc[df.G3 > 9, 'G3'] = 1\n\ndf.loc[df.G2 <= 9, 'G2'] = 0\ndf.loc[df.G2 > 9, 'G2'] = 1\n\ndf.loc[df.G1 <= 9, 'G1'] = 0\ndf.loc[df.G1 > 9, 'G1'] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.G3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On peut commencer les prédictions sur G1, pour voir si on fait mieux que 0,20. ","metadata":{}},{"cell_type":"code","source":"df.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X1 = df.drop('G1',axis=1)\nX2 = X1.drop('G2',axis=1)\nX = X2.drop('G3',axis=1)\ny = df['G1']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Régression logistique","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\ny_lr = lr.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(confusion_matrix(y_test,y_lr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(accuracy_score(y_test,y_lr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_lr))\nprobas = lr.predict_proba(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On remarque l'accurancy n'est pas très bon, mais c'est mieux que la prédiction de la note. \nLe problème vient des notes inférieures à 10 notamment la note de 0/20. Car on peut donner un 0/20 malgré qu'on soit dans une bonne situation au niveau de l'éducation etc. Et ça l'algorithme n'arrive pas à le déterminer.\nUne solution serait d'enlever tous les 0/20. ","metadata":{}},{"cell_type":"markdown","source":"# Random forests","metadata":{}},{"cell_type":"code","source":"from sklearn import ensemble\nrf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_rf))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_rf)\nprint(cm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf1 = ensemble.RandomForestClassifier(n_estimators=10, min_samples_leaf=10, max_features=3)\nrf1.fit(X_train, y_train)\ny_rf1 = rf.predict(X_test)\nprint(classification_report(y_test, y_rf1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La méthode des random forests est mieux que la méthode de la régression logistique.","metadata":{}},{"cell_type":"code","source":"xgb  = XGB.XGBClassifier()\nxgb.fit(X_train, y_train)\ny_xgb = xgb.predict(X_test)\ncm = confusion_matrix(y_test, y_xgb)\nprint(cm)\nprint(classification_report(y_test, y_xgb))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"XGBoost ne semble pas bien fonctionner avec ce dataset.","metadata":{}},{"cell_type":"markdown","source":"# Prédire G3 en booléen, sans G1 ni G2","metadata":{}},{"cell_type":"code","source":"X1 = df.drop('G1',axis=1)\nX2 = X1.drop('G2',axis=1)\nX = X2.drop('G3',axis=1)\ny = df['G3']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import ensemble\nrf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_rf))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On remarque qu'on arrive à avoir une accuracy de 0,76. Ce qui est un score moyen mais qui est nettement meilleur que les autres scores de prédiction. ","metadata":{}}]}