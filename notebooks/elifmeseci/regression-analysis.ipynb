{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Regression**\n* [Reading Data](#1)\n* [Linear Regression](#3)\n* [Multiple Linear Regression](#4)\n* [Polynomial Linear Regression](#5)\n* [Decision Tree Regression](#6)\n* [Random Forest Regression](#7)\n* [R Square with Random Forest Regression](#8)\n* [R Square with Linear Regression](#9)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n# Reading Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df1 = pd.read_csv(\"/kaggle/input/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv\")\ndf2 = pd.read_csv(\"/kaggle/input/biomechanical-features-of-orthopedic-patients/column_3C_weka.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"In 'column_2C_weka.csv'\n* there are 7 features and for each of them 310 samples.\n* Six feature are in float type and one of them is object.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In 'column_3C_weka.csv'\n* there are 7 features and for each of them 310 samples.\n* Six feature are in float type and one of them is object.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Correlation Between Features**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"class\", data=df1)\ndf1.loc[:,'class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df1[df1['class'] =='Abnormal']\npelvic_incidence = np.array(data.loc[:,'pelvic_incidence']).reshape(-1,1)\nsacral_slope = np.array(data.loc[:,'sacral_slope']).reshape(-1,1)\n# Scatter\nplt.figure(figsize=[10,10])\nplt.scatter(pelvic_incidence,sacral_slope)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n# **Linear Regression**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* We can say also \"line fit\"\n* **y = b0 + b1*x** where \n * b0 = constant (*the point where line intersects the y axis*)\n * b1 = coefficient (*slope of the line*)\n* The aim is to draw the line closest to the points. But the line may not pass exactly from the center of the dots. So there is the term \"residual\".\n  * **residual = y - y_head**\n      * y is where the point is\n      * y_head is where it hits when drawn upright to the line from the point\n* We can square the residual to reduce the error and get positive residual. By adding the squares of residual we can see how fit the line.\n* Mean Squared Error  **MSE = (sum(residual^2))/n**\n* The smaller value of the MSE, the better the line is fit.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nreg = LinearRegression()\n# Predict space\npredict_space = np.linspace(min(pelvic_incidence), max(pelvic_incidence)).reshape(-1,1)\n# Fit\nreg.fit(pelvic_incidence,sacral_slope)\n# Predict\npredicted = reg.predict(predict_space)\n# R^2 \nprint('R^2 score: ',reg.score(pelvic_incidence, sacral_slope))\n# Plot regression line and scatter\nplt.figure(figsize=(10,10))\nplt.plot(predict_space, predicted, color='green', linewidth=2)\nplt.scatter(pelvic_incidence,sacral_slope)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n# Multiple Linear Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **y = b0 + b1*x1 + b2*x2**\n* The aim is minimum MSE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = (df1.iloc[:,[0,2]]).values # [pelvic_incidence,lumbar_lordosis_angle]\ny = df1.sacral_slope.values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multiple_linear_regression = LinearRegression()\nmultiple_linear_regression.fit(x,y)\n\n\nprint(\"b0: \",multiple_linear_regression.intercept_)\nprint(\"b1,b2:\",multiple_linear_regression.coef_)\n\nmultiple_linear_regression.predict(np.array([[63.0278175 , 39.60911701],[40.47523153, 39.60911701]]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n# Polinomial Linear Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **y = b0 + b1*x1 + b2*x2 + ... + bn*xn**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.array(df1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(df1.loc[:,'sacral_slope']).reshape(-1,1)\n\nlr = LinearRegression()\n\nlr.fit(x,y)\n# predict\ny_head = lr.predict(x)\n\nplt.plot(x,y_head,color=\"purple\",label = \"linear\")\n\n# polynomial regression = y = b0 + b1*x + b2*x^2 + b3*x^3 + ... + bn*x^n\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\npolynomial_regression = PolynomialFeatures(degree = 4) # takes polynomial until fourth degree\n\nx_polynomial = polynomial_regression.fit_transform(x) #transform func makes x values polynomial\n# fit\nlinear_regression2 = LinearRegression()\nlinear_regression2.fit(x_polynomial,y) \n\n# visualize\n\ny_head2 = linear_regression2.predict(x_polynomial)\n\n\nplt.plot(x,y_head2,color = \"green\", label = \"poly\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n# Decision Tree Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **CART** : Classification and Regression Tree\n* One of the most important things is \"**split**\" in decision tree regression.\n* the areas that are splitted are called **terminal leaves**.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.array(df1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(df1.loc[:,'sacral_slope']).reshape(-1,1)\n\n\nfrom sklearn.tree import DecisionTreeRegressor # random state = 0\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(x,y)\n\n\ntree_reg.predict([[5.5]])\n\nx_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\n\ny_head = tree_reg.predict(x_)\n#%% visualize\nplt.figure(figsize = (10,10))\nplt.scatter(x,y,color = \"red\")\nplt.plot(x_,y_head,color=\"green\")\nplt.xlabel(\"Pelvic Incidence\")\nplt.ylabel(\"Sacral Slope\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n# Random Forest Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **random_state** allows selection of the same random values.\nIf we dont assign random state there would be different results every time the code executed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.array(df1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(df1.loc[:,'sacral_slope'])\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(n_estimators = 40,random_state = 42)\n\nrf.fit(x,y)\n\nx_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\ny_head = rf.predict(x_)\n\n#visualize\nplt.figure(figsize = (10,10))\nplt.scatter(x,y,color=\"blue\")\nplt.plot(x_,y_head,color = \"red\")\nplt.xlabel(\"Pelvic Incidence\")\nplt.ylabel(\"Sacral Slope\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a> <br>\n# R Square with Random Forest Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.array(df1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(df1.loc[:,'sacral_slope'])\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(n_estimators = 100,random_state = 42)\n\nrf.fit(x,y)\n\ny_head = rf.predict(x)\n\n\nfrom sklearn.metrics import r2_score\n\nprint(\"r_score\", r2_score(y,y_head))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9\"></a> <br>\n# R Square with Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.array(df1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(df1.loc[:,'sacral_slope'])\n\nplt.figure(figsize=(10,10))\nplt.scatter(pelvic_incidence,sacral_slope)\nplt.xlabel(\"Pelvic Incidence\")\nplt.ylabel(\"Sacral Slope\")\n\nfrom sklearn.linear_model import LinearRegression\n\n#linear regression model\nlinear_reg = LinearRegression()\n\n\nlinear_reg.fit(x,y)\n\ny_head = linear_reg.predict(x)\nplt.plot(x, y_head , color = \"red\")\n\nfrom sklearn.metrics import r2_score\n\nprint(\"r_square score: \", r2_score(y, y_head))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}