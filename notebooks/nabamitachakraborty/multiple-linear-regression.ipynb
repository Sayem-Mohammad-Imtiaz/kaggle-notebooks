{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv(r'../input/vehicle-dataset-from-cardekho/car data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATA PREPROCESSING","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removing duplicate entries\ndata.drop_duplicates(keep='first',inplace=True)\ndata.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check missing or null values\ndata.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hence there are no null values.","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe(include=\"all\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hence we have 5 categorical columns. If we do not drop \"Car_Name\" before converting the categorical data to indicators, the resultant data will give 98+ features making the data redundant. Hence:","metadata":{}},{"cell_type":"code","source":"del data[\"Car_Name\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=data[['Year', 'Present_Price', 'Kms_Driven', 'Fuel_Type',\n       'Seller_Type', 'Transmission', 'Owner', 'Selling_Price']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking unique values for the categorical data\nprint(data[\"Fuel_Type\"].unique())\nprint(data[\"Seller_Type\"].unique())\nprint(data[\"Transmission\"].unique())\nprint(data[\"Owner\"].unique())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datac=data.copy(deep=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Converting the categorical to indicator variables.\n\ndata=pd.get_dummies(data,drop_first =True)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The column \"Year\" is meaningless unless it is in terms of the number of years after which the selling price is being estimated. Hence:","metadata":{}},{"cell_type":"code","source":"from datetime import date\nyear=date.today().year\nyear\ndata.Year = year-data.Year","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.shape)\ndata.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.shape)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vizualisation","metadata":{}},{"cell_type":"markdown","source":"Use seaborn to create a jointplot to compare various columns to vizualise the correlation between them [to vaguely estimate the strength of the correlation or presence of multicollinearity(Multicollinearity generally occurs when there are high correlations between two or more predictor variables.)].","metadata":{}},{"cell_type":"code","source":"sns.jointplot(x='Present_Price', y='Selling_Price',data = datac)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.jointplot(x='Present_Price', y='Selling_Price',data = datac, kind= 'hex')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Enhancing a scatterplot by including a linear regression model (and its uncertainty) using lmplot().","metadata":{}},{"cell_type":"code","source":"sns.set(color_codes=True)\nsns.lmplot(x='Present_Price', y='Selling_Price',data = datac)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lmplot returns the FacetGrid object with the plot on it for further tweaking.FacetGrid class helps in visualizing distribution of one variable as well as the relationship between multiple variables separately within subsets of the dataset using multiple panels.\n\nA FacetGrid can be drawn with up to three dimensions âˆ’ row, col, and hue. The first two have obvious correspondence with the resulting array of axes; thinking of the hue variable as a third dimension along a depth axis, where different levels are plotted with different colors.\n\nFacetGrid object takes a dataframe as input and the names of the variables that will form the row, column, or hue dimensions of the grid.","metadata":{}},{"cell_type":"markdown","source":"\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"We now use pairplot to detect any multicollinearity between the predictors. While the column of charges in the plot will give us the dependence of the response variable on the predictor variable.","metadata":{}},{"cell_type":"code","source":"sns.pairplot(datac[['Year','Present_Price', 'Kms_Driven','Owner', 'Selling_Price']],hue=\"Owner\", markers=[\"h\", \"p\", \"D\"])  \n#\"Owner\" can be replaced with 'Fuel_Type_Diesel','Fuel_Type_Petrol','Seller_Type_Individual','Transmission_Manual' for more plots.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.catplot(data=datac, kind=\"swarm\", x=\"Owner\", y=\"Selling_Price\", col=\"Seller_Type\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlations=datac.corr()\ncorrelations","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This shows that the most important feature in predicting the selling price is the Present_Price!","metadata":{}},{"cell_type":"markdown","source":"# Training and Testing Data.\nWe now go ahead and split the data into training and testing sets. We define a variable X that will contain all the columns except the target column and store the target column, i.e, \"Selling_Price\" in another variable, say y.\n ","metadata":{}},{"cell_type":"code","source":"y = data.iloc[:,1]\nX = data.iloc[:,:-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train-test split of the data\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33,random_state=42)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train,\"\\n\\n\\n\\n\",X_test,\"\\n\\n\\n\\n\",y_train,\"\\n\\n\\n\\n\",y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the regression models.\n","metadata":{}},{"cell_type":"code","source":"#Importing libraries\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#linear regression\nmodel=linear_model.LinearRegression()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nprint('Coefficients: \\n', model.coef_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ridgeregr = linear_model.Ridge(alpha=30, normalize =True)\n\n# Train the model using the training sets\nridgeregr.fit(X_train, y_train)\n\n# Make predictions using the testing set\nridge_y_pred = ridgeregr.predict(X_test)\nprint('Coefficients: \\n', ridgeregr.coef_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lasso =linear_model.Lasso(alpha=50 , normalize = False) \nlasso.fit(X_train,y_train)\n\n# Make predictions using the testing set\nlasso_y_pred = lasso.predict(X_test)\nprint('Coefficients: \\n', lasso.coef_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Performance Evaluation. ","metadata":{}},{"cell_type":"markdown","source":"Before we evaluate our performance, I would like to mention the four principle assumptions which justify the use of linear regression models for purposes of inference or prediction, i.e,\n\"(i) linearity and additivity of the relationship between dependent and independent variables:\n\n    (a) The expected value of dependent variable is a straight-line function of each independent variable, holding the others fixed.\n\n    (b) The slope of that line does not depend on the values of the other variables.\n\n    (c)  The effects of different independent variables on the expected value of the dependent variable are additive.\n\n(ii) statistical independence of the errors (in particular, no correlation between consecutive errors in the case of time series data)\n\n(iii) homoscedasticity (constant variance) of the errors\n\n    (a) versus time (in the case of time series data)\n\n    (b) versus the predictions\n\n    (c) versus any independent variable\n\n(iv) normality of the error distribution.\"","metadata":{}},{"cell_type":"markdown","source":"Let's emphasize the last assumption, the normal distribution of the error wich is nothing but the difference in values between the actual target value and the predicted target value. Let's create a visualization of this difference and verify our implementation of the regression model on our data.","metadata":{}},{"cell_type":"code","source":"plt.pyplot.scatter(y_test, y_pred)\nplt.pyplot.ylabel('Predicted')\nplt.pyplot.xlabel('Actual')\nprint('Mean squared error: %.4f' % mean_squared_error(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(y_test-y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution of error follows normal distribution indeed. Though for plain regression model, the distribution is slightly left(negatively) skewed.","metadata":{}},{"cell_type":"code","source":"plt.pyplot.scatter(y_test, ridge_y_pred)\nplt.pyplot.ylabel('Predicted')\nplt.pyplot.xlabel('Actual')\nprint('Mean squared error: %.4f' % mean_squared_error(y_test, ridge_y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(y_test-ridge_y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    The distribution for error in case of normalized Ridge regression model is right(positively) skewed for the data used here.","metadata":{}},{"cell_type":"code","source":"plt.pyplot.scatter(y_test, lasso_y_pred)\nplt.pyplot.ylabel('Predicted')\nplt.pyplot.xlabel('Actual')\nprint('Mean squared error: %.4f' % mean_squared_error(y_test, lasso_y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(y_test-lasso_y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    Unlike Ridge, the distribution for error in case of Lasso regression model is left(negatively) skewed for the data used here.","metadata":{}},{"cell_type":"markdown","source":"CONCLUSION:\nLasso tends to perform better when compared to ridge but our regression model outperforms all.","metadata":{}}]}