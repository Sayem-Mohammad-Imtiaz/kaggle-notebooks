{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n# Perform Sentiment Analysis with scikit-learn(IMDb Reviews)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://d1sjtleuqoc1be.cloudfront.net/wp-content/uploads/2019/04/25112909/shutterstock_1073953772.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**So I took a project based course on coursera,the topic being Sentiment Analysis with sckit-learn.**\n\n**This kernel is the application of what I have learnt in the course.\n**\n**This is going to be a very short,easy but effective kernel!**\n\n**I hope you too will learn something new!**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![source](https://cdn-images-1.medium.com/max/361/0*ga5rNPmVYBsCm-lz.)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **Importing important libraries**","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom sklearn.feature_extraction.text import CountVectorizer\ncount=CountVectorizer()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **from sklearn.feature_extraction.text import CountVectorizer**:\nConvert a collection of text documents to a matrix of token counts\nThis implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv(\"../input/imdb-dataset-sentiment-analysis-in-csv-format/Train.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **After reading the csv file which contains 40k movie reviews from IMDB,we see that there are two prominent columns.One being TEXT which contains the review and the other being LABEL which contains O's and 1's ,where 0-NEGATIVE and 1-POSITIVE REVIEW!!**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=plt.figure(figsize=(5,5))\ncolors=[\"skyblue\",'pink']\npos=data[data['label']==1]\nneg=data[data['label']==0]\nck=[pos['label'].count(),neg['label'].count()]\nlegpie=plt.pie(ck,labels=[\"Positive\",\"Negative\"],\n                 autopct ='%1.1f%%', \n                 shadow = True,\n                 colors = colors,\n                 startangle = 45,\n                 explode=(0, 0.1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Lets understand how Count Vectorizer works which will be applied later to the dataset!**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **get_feature_names():Array mapping from feature integer indices to feature name.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df=[\"Hey Jude, refrain Dont carry the world upon your shoulders For well you know that its a fool Who plays it cool By making his world a little colder Na-na-na,a, na Na-na-na, na\"]\nbag=count.fit_transform(df)\nprint(count.get_feature_names())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Now from the array we can infer that conversion a collection of text documents to a matrix of token counts has been done below**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bag.toarray())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ** Next we are going to Import RE i.e Regular Expression Operation,we are using this library to remove html tags like '< a >'  or  <head/>.So whenever we come across these tage we replace them with an empty string ''.Next we will also be altering emojis/emoticons which can be smiley :) ,sad face :( or even some upset face :/.We will be shifting the emojis towards the end so that we can get a set of clean text.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef preprocessor(text):\n             text=re.sub('<[^>]*>','',text)\n             emojis=re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n             text=re.sub('[\\W]+',' ',text.lower()) +\\\n                ' '.join(emojis).replace('-','')\n             return text   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **So lets show you some examples to get some clarity incase you did not understand what has happened in the above code**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **So lets call the preprocessing function and pass some text in it and observe the output to understand what exactly is happening when the function is called**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessor(data.loc[0,'text'][-50:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **In the below text that has been passed in the function we can see that it contains HTML tag and emojis.The text returned will not contain the html tag and the emojis will be pushed towards the end of the text!**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessor(\"<a> this is :(  aweomee wohhhh :)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Now since we have seen the above examples we will be passing the text data from our train dataset to this preprocessor function to clean the data.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['text']=data['text'].apply(preprocessor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **We are importing a new library PorterStemmer from nltk.stem.porter.It follows an algorithm for suffix stripping i.e it will bring the word to its base meaning like running will be changed to run ,eating will be changed to eat.This is for simplifying the data and removing unnecessary complexities in our text data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer\n\nporter=PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **So before we perform stemming we need to split the sentences into words.Again to make you understand this concept of splitting of text we will see some Examples.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenizer(text):\n        return text.split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenizer_porter(text):\n    return [porter.stem(word) for word in text.split()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Lets see what will happen to this text that we are passing in the tokenizer function.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer(\"Haters love Hating as they Hate\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **In this next Example we will see the words being reduced to their base words,Haters will be changed to hater,hating is changed to hate**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer_porter(\"Haters love Hating as they Hate\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Stopwords are the most common words in any natural language. For the purpose of analyzing text data and building NLP models, these stopwords might not add much value to the meaning of the document.Generally, the most common words used in a text are “the”, “is”, “in”, “for”, “where”, “when”, “to”, “at” etc.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop=stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\npositivedata = data[ data['label'] == 1]\npositivedata =positivedata['text']\nnegdata = data[data['label'] == 0]\nnegdata= negdata['text']\n\ndef wordcloud_draw(data, color = 'white'):\n    words = ' '.join(data)\n    cleaned_word = \" \".join([word for word in words.split()\n                              if(word!='movie' and word!='film')\n                            ])\n    wordcloud = WordCloud(stopwords=stop,\n                      background_color=color,\n                      width=2500,\n                      height=2000\n                     ).generate(cleaned_word)\n    plt.figure(1,figsize=(10, 7))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\n    \nprint(\"Positive words are as follows\")\nwordcloud_draw(positivedata,'white')\nprint(\"Negative words are as follows\")\nwordcloud_draw(negdata)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Positive words that are highlighted are:love,great,perfect,good,beautiful,nice,excellent**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **Negative words that are highlighted are:awful,waste,problem,stupid,horrible,bad,poor**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **Surprisingly both have 'ONE' in common,i guess for the positve category reviews might be like \"One of the best characters/movies/films I've watched and for the negative comments it might be like \"I rate this movie one star or one of the worst directions/scripts etc**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **from sklearn.feature_extraction.text import TfidfVectorizer:Convert a collection of raw documents to a matrix of TF-IDF features.TfidfVectorizer:-Transforms text to feature vectors that can be used as input to estimator.****","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf=TfidfVectorizer(strip_accents=None,lowercase=False,preprocessor=None,tokenizer=tokenizer_porter,use_idf=True,norm='l2',smooth_idf=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Learn vocabulary and idf, return term-document matrix.This is equivalent to fit followed by transform, but more efficiently implemented.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y=data.label.values\nx=tfidf.fit_transform(data.text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **from sklearn.model_selection import train_test_split:Split arrays/matrices into train and test subsets**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test=train_test_split(x,y,random_state=1,test_size=0.5,shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **So now we will be using Logistic Regression for our Sentiment Analysis **","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.).CV here stands for Cross Validation.The user can pass the number of folds as an argument cv of the function to perform k-fold cross-validation**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.linear_model import LogisticRegressionCV\n\nclf=LogisticRegressionCV(cv=6,scoring='accuracy',random_state=0,n_jobs=-1,verbose=3,max_iter=500).fit(X_train,y_train)\n\ny_pred = clf.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **So now lets print the Accuracy Score!!**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Linear classifiers (SVM, logistic regression, a.o.) with SGD training.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nclf= SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}