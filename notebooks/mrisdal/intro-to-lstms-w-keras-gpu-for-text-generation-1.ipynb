{"cells":[{"metadata":{"_cell_guid":"c824420b-fcf3-4278-b3d3-982f76542918","_uuid":"1d797830444c423f554e045982b44255f6b60ee1"},"cell_type":"markdown","source":"# Applied Introduction to LSTMs with GPU for text generation\n\nIn this Python notebook kernel, **I will use the text from [freeCodeCamp's Gitter chat logs](https://www.kaggle.com/free-code-camp/all-posts-public-main-chatroom) to train an LSTM network to generate novel messages** (click on the \"Data\" tab of this kernel to view the data preview). \n\nNow that you can use **GPUs** in Kernels with **6 hours of run time**, you can train much more computationally intensive models than ever on Kaggle. I'll use a GPU to train the model in this notebook (you can request a GPU for your session by clicking on the \"Settings\" tab from a kernel editor)."},{"metadata":{"_cell_guid":"4ed69942-eef4-4f07-9bb6-a6a08603523b","_uuid":"e0b49d511f59f5bc07294c5a57c99884d1fd2bd8","trusted":true,"collapsed":true},"cell_type":"code","source":"import tensorflow as tf\nprint(tf.test.gpu_device_name())\n# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"0eaa76d4-70da-4564-834a-1be930f7866d","_uuid":"0b6e99af0ef68852b8e36da098ce8f6a0365c17b"},"cell_type":"markdown","source":"I'll use text from one of the channel's most prolific user ids as the training data. There are two parts to this notebook:\n\n1. Reading in, exploring, and preparing the data\n2. Training the LSTM on a single user id's chat logs and generating novel text as output\n\nYou can follow along by simply reading the notebook or you can fork it (click \"Fork notebook\") and run the cells yourself to learn what each part is doing interactively. By the end, you'll learn **how to format text data as input to a character-level LSTM model implemented in Keras** and in turn use the model's character-level predictions to **generate novel sequences of text**.\n\nBut first, what is an LSTM (\"Long Short-Term Memory\") network anyway? In this notebook, we'll take a hands-on approach to implementing this flavor of recurrent neural network especially equipped to handle longer distance dependencies (including ones you get with language) in Keras, a deep learning framework. If you want to review more of the theoretical underpinnings, I recommend that you check out this excellent blog post, [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n\n## Part one: Data Preparation\n\nIn part one, I'll first read in the data and try to explore it enough to give you a sense of what we're working with. One of my frustrations with following non-interactive tutorials (e.g., static code shared on GitHub) is that it's often hard to know how the data you want to work with differs from the code sample. You have to download it and compare it locally which is a pain. \n\nThe two nice things about this tutorial using Kernels is that a) I'll try to give you glimpses into the data at every significant step; and 2) you can always fork this notebook and ðŸ’¥boom ðŸ’¥ you've got a copy of my environment, data, Docker image, and all with no downloads or installs necessary whatsoever. Especially if you have experience installing CUDA to use GPUs for deep learning, you'll appreciate how wonderful it is to have an environment already setup for you. \n\n### Read in the data"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Read in only the two columns we need \nchat = pd.read_csv('../input/freecodecamp_casual_chatroom.csv', usecols = ['fromUser.id', 'text'])\n# We don't want bots :) \nchat = chat[chat['fromUser.id'] != '55b977f00fc9f982beab7883'] # user id for CamperBot\n\nchat.head()","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"62cf9332-b12d-47ee-aad7-155565b42efd","_uuid":"c96938d9f8a89f89caddf9e52c7624357b03ec90"},"cell_type":"markdown","source":"Looks good!\n\n### Explore the data\n\nIn my plot below, you can see the number of posts from the top ten most active chat participants by their user id in freeCodeCamp's Gitter:"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\nf, g = plt.subplots(figsize=(12, 9))\nchat['fromUser.id'].value_counts().head(10).plot.bar(color=\"green\")\ng.set_xticklabels(g.get_xticklabels(), rotation=25)\nplt.title(\"Most active users in freeCodeCamp's Gitter channel\")\nplt.show(g)","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"0b054e15-25f9-4cd8-aa52-085503f3dc7c","_uuid":"182b8d9f9ac79dc6d88574f87a8236b24cb5ac66"},"cell_type":"markdown","source":"So, userid `55a7c9e08a7b72f55c3f991e` is the most active user in the channel with over 140,000 messages. We'll use their messages to train the LSTM to generate novel `55a7c9e08a7b72f55c3f991e`-like sentences. But first, let's take a look at the first few messages from `55a7c9e08a7b72f55c3f991e` to get a sense for what they're chatting about:"},{"metadata":{"_cell_guid":"56e2daa9-da1e-46a3-a45f-a0e6c850ffb6","_uuid":"322d7e17a34851027e63b3f0c8936d5f6bdcc6ea","trusted":true,"collapsed":true},"cell_type":"code","source":"chat[chat['fromUser.id'] == \"55a7c9e08a7b72f55c3f991e\"].text.head(20)","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"1ecc73fc-3448-4695-955f-2815c67e0f37","_uuid":"0aefcac23a5b8718da854326ed70ae6855a28b4d"},"cell_type":"markdown","source":"I see words and phrases like \"documentation\", \"pair coding\", \"BASH\", \"Bootstrap\", \"CSS\", etc. And I can only assume the sentence starting \"With all of the various frameworks...\" is referring to JavaScript. Yep, sounds like they're on-topic as far as freeCodeCamp goes. So we'll expect our novel sentences to look roughly like this if we're successful.\n\n### Prepare sequence data for input to LSTM\n\nRight now we have a dataframe with columns corresponding to user ids and message text where each row corresponds to a single message sent. This is pretty far from the 3D shape the input layer of our LSTM network requires: `model.add(LSTM(batch_size, input_shape=(time_steps, features)))` where `batch_size` is the number of sequences in each sample (can be one or more), `time_steps` is the size of observations in each sample, and `features` is the number of possible observable features (i.e., characters in our case). \n\nSo how do we get from a dataframe to sequence data in the correct shape? I'll break it into three steps:\n\n1. Subset the data to form a corpus\n2. Format the corpus from #1 into arrays of semi-overlapping sequences of uniform length and next characters\n3. Represent the sequence data from #2 as sparse boolean tensors\n\n#### Subset the data to form a corpus\nIn the next two cells, we'll grab only messages from `55a7c9e08a7b72f55c3f991e` (`'fromUser.id' == '55a7c9e08a7b72f55c3f991e'`) to subset the data and collapse the vector of strings into a single string. Since we don't care if our model generates text with correct capitalization, we use `tolower()`. This give the model one less dimension to learn. \n\nI'm also just going to use the first 20% of the data as a sample since we don't need more than that to generate halfway decent text. You can try forking this kernel and experimenting with more (or less) data if you want."},{"metadata":{"_cell_guid":"08da8f76-f9f5-4fd1-a161-7546500aa23c","_uuid":"7bbee9dce830eeeb8ea83fd793ade259b21c31fd","collapsed":true,"trusted":true},"cell_type":"code","source":"user = chat[chat['fromUser.id'] == '55a7c9e08a7b72f55c3f991e'].text\n\nn_messages = len(user)\nn_chars = len(' '.join(map(str, user)))\n\nprint(\"55a7c9e08a7b72f55c3f991e accounts for %d messages\" % n_messages)\nprint(\"Their messages add up to %d characters\" % n_chars)","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"94ee4c53-ca78-4a30-ab07-6ad957c12c04","_uuid":"be3cd8c084c617523bf100650ec9fd67beb72fdb","collapsed":true,"trusted":true},"cell_type":"code","source":"sample_size = int(len(user) * 0.2)\n\nuser = user[:sample_size]\nuser = ' '.join(map(str, user)).lower()\n\nuser[:100] # Show first 100 characters","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7bc15ca7-0c17-447c-b26f-de28b72deae9","_uuid":"fee2f2fe8cf6588cf7af081fdb1f12016ce5f704"},"cell_type":"markdown","source":"#### Format the corpus into arrays of semi-overlapping sequences of uniform length and next characters\nThe rest of the code used here is adapated from [this example script](https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py), originally written by FranÃ§ois Chollet (author of Keras and Kaggler), to prepare the data in the correct format for training an LSTM. Since we're training a character-level model, we relate unique characters (e.g., \"a\", \"b\", \"c\", ...) to numeric indices in the cell below. If you rerun this code yourself by clicking \"Fork Notebook\" you can print out all of the characters used."},{"metadata":{"_cell_guid":"6fbf7e2e-c7b6-4f12-9648-471dea09b90f","_uuid":"8c3ac7ede01c9884c4e245d649dd383e3e1486ba","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"chars = sorted(list(set(user)))\nprint('Count of unique characters (i.e., features):', len(chars))\nchar_indices = dict((c, i) for i, c in enumerate(chars))\nindices_char = dict((i, c) for i, c in enumerate(chars))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b0dc2603-abea-47e7-8eb8-ddafc6e42f79","_uuid":"d3a03d2408b3eb19327e716f84d82cfb386ba3a1"},"cell_type":"markdown","source":"This next cell step gives us an array, `sentences`, made up of `maxlen` (40) character sequences chunked in steps of 3 characters from our corpus `user`, and `next_chars`, an array of single characters from `user` at `i + maxlen` for each `i`. I've printed out the first 10 strings in the array so you can see we're chunking the corpus into partially overlapping, equal length \"sentences\"."},{"metadata":{"_cell_guid":"44b8a3df-25fe-40d4-81c0-5ca0c7438337","_uuid":"74921d3196fef3d16cc264b4a9508bcf091d5c17","collapsed":true,"trusted":true},"cell_type":"code","source":"maxlen = 40\nstep = 3\nsentences = []\nnext_chars = []\nfor i in range(0, len(user) - maxlen, step):\n    sentences.append(user[i: i + maxlen])\n    next_chars.append(user[i + maxlen])\nprint('Number of sequences:', len(sentences), \"\\n\")\n\nprint(sentences[:10], \"\\n\")\nprint(next_chars[:10])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3af169dc-1bd0-4ac8-af8a-c802b55c7203","_uuid":"a45c79304c246dfb0e8767a31c7c5404344bd25e"},"cell_type":"markdown","source":"You can see how the next character following the first sequence `'hi folks. just doing the new signee stuf'` is the character `f` to finish the word \"stuff\". And the next character following the sequence `'folks. just doing the new signee stuff. '` is the character `h` to start the word \"hello\". In this way, it should be clear now how `next_chars` is the \"data labels\" or ground truth for our sequences in `sentences` and our model trained on this labeled data will be able to generate *new next characters* as predictions given sequence input.\n\n#### Represent the sequence data as sparse boolean tensors\nThe next cell will take a few seconds to run. We're creating a sparse boolean tensors `x` and `y` encoding character-level features from `sentences` and `next_chars` to use as inputs to the model we train. The shape we end up with will be: `input_shape=(maxlen, len(chars))` where `maxlen` is 40 and `len(chars)` is the number of features (i.e., unique count of characters from our corpus)."},{"metadata":{"_cell_guid":"e0061dbc-193a-4355-91e7-ff8d8f04f98e","_uuid":"820d224ac0842acf51240008f118ca75582dd817","collapsed":true,"trusted":true},"cell_type":"code","source":"x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\ny = np.zeros((len(sentences), len(chars)), dtype=np.bool)\nfor i, sentence in enumerate(sentences):\n    for t, char in enumerate(sentence):\n        x[i, t, char_indices[char]] = 1\n    y[i, char_indices[next_chars[i]]] = 1","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"77a203b8-f3dd-43fd-879f-0df2502720c1","_uuid":"edcbf43c649cef706caa039a573009df9abb7ef8"},"cell_type":"markdown","source":"## Part two: Modeling\n\nIn part two, we do the actual model training and text generation. We've explored the data and reshaped it correctly so we that we can use it as an input to our LSTM model. There are two sections to this part: \n\n1. Defining an LSTM network model\n2. Training the model and generating predictions\n\n### Defining an LSTM network model\n\nLet's start by reading in our libraries. I'm using Keras which is a popular and easy-to-use interface to a TensorFlow backend. Read more about [why to use Keras as a deep learning framework here](https://keras.io/why-use-keras/). Below you can see the models, layers, optimizers, and callbacks we'll be using."},{"metadata":{"_cell_guid":"92b02ca9-744a-4d16-8966-1f8e2460f4a5","_uuid":"d99cb5f3dd27ab9f9451e3f7008cb8fbf67fa0bd","collapsed":true,"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers import LSTM\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import LambdaCallback, ModelCheckpoint\nimport random\nimport sys\nimport io","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c1d82e3b-831e-4995-b6a2-c0cf8cad40e2","_uuid":"12d99cdfeefd6eaa06602c8555040b3a1e606658"},"cell_type":"markdown","source":"In the cell below, we define the model. We start with a sequential model and add an LSTM as an input layer. The shape we define for our input is identical to our data by this point which is exactly what we need. I've selected a `batch_size` of 128 which is the number of samples, or sequences, our model looks at during training before updating. You can experiment with different numbers here if you want. I'm also adding a dense output layer. Finally, we'll use add an activation layer with `softmax` as our activation function as we're in essence doing multiclass classification to predict the next character in a sequence."},{"metadata":{"_cell_guid":"cc5f72f0-89f9-4caa-8208-5968f2701a36","_uuid":"fd7a33b3ce13e07f27cf67c7d0608a63f2128fcd","collapsed":true,"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(LSTM(128, input_shape=(maxlen, len(chars))))\nmodel.add(Dense(len(chars)))\nmodel.add(Activation('softmax'))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0cbaaec6-b8bf-4189-9ecf-7c0277f7bfb6","_uuid":"9919faa8a156a6073586e41797fa4735967d89f0"},"cell_type":"markdown","source":"Now we can compile our model. We'll use `RMSprop` with a learning rate of `0.1` to optimize the weights in our model (you can experiment with different learning rates here) and `categorical_crossentropy` as our loss function. Cross entropy is the same as log loss commonly used as the evaluation metric in binary classification competitions on Kaggle (except in our case there are more than two possible outcomes)."},{"metadata":{"_cell_guid":"d0de86ee-80d1-4d17-8b2a-2359325f4a17","_uuid":"3776f49637669fb59bdec2b56c3a0bc766f2c25c","collapsed":true,"trusted":true},"cell_type":"code","source":"optimizer = RMSprop(lr=0.01)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2fb1244b-4168-4817-baff-f2a55bc274ba","_uuid":"3769758a992a93f3e8d07571371a0f3d7de2f331"},"cell_type":"markdown","source":"Now our model is ready. Before we feed it any data, the cell below defines a couple of helper functions [with code modified from this script](https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py). The first one, `sample()`, samples an index from an array of probabilities with some `temperature`. Quick pause to ask, what is temperature exactly?\n\n> **Temperature** is a scaling factor applied to the outputs of our dense layer before applying the `softmax`activation function. In a nutshell, it defines how conservative or \"creative\" the model's guesses are for the next character in a sequence. Lower values of `temperature` (e.g., `0.2`) will generate \"safe\" guesses whereas values of `temperature` above `1.0` will start to generate \"riskier\" guesses. Think of it as the amount of surpise you'd have at seeing an English word start with \"st\" versus \"sg\". When temperature is low, we may get lots of \"the\"s and \"and\"s; when temperature is high, things get more unpredictable.\n\nAnyway, so the second is defining a callback function to print out predicted text generated by our trained LSTM at the first and then every subsequent fifth epoch with five different settings of `temperature` each time (see the line `for diversity in [0.2, 0.5, 1.0, 1.2]:` for the values of `temperature`; feel free to tweak these, too!). This way we can fiddle with the `temperature` knob to see what gets us the best generated text ranging from conservative to creative. Note that we're using our model to predict based on a random sequence, or \"seed\", from our original subsetted data, `user`: `start_index = random.randint(0, len(user) - maxlen - 1)`.\n\nFinally, we name our callback function `generate_text` which we'll add to the list of callbacks when we fit our model in the cell after this one."},{"metadata":{"_cell_guid":"aa111cc9-4a2b-4781-9d32-2b2a07c782d0","_uuid":"85aaec3c1cf76eb94af499685bec3ae961f6f8bb","collapsed":true,"trusted":true},"cell_type":"code","source":"def sample(preds, temperature=1.0):\n    # helper function to sample an index from a probability array\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)\n\ndef on_epoch_end(epoch, logs):\n    # Function invoked for specified epochs. Prints generated text.\n    # Using epoch+1 to be consistent with the training epochs printed by Keras\n    if epoch+1 == 1 or epoch+1 == 15:\n        print()\n        print('----- Generating text after Epoch: %d' % epoch)\n\n        start_index = random.randint(0, len(user) - maxlen - 1)\n        for diversity in [0.2, 0.5, 1.0, 1.2]:\n            print('----- diversity:', diversity)\n\n            generated = ''\n            sentence = user[start_index: start_index + maxlen]\n            generated += sentence\n            print('----- Generating with seed: \"' + sentence + '\"')\n            sys.stdout.write(generated)\n\n            for i in range(400):\n                x_pred = np.zeros((1, maxlen, len(chars)))\n                for t, char in enumerate(sentence):\n                    x_pred[0, t, char_indices[char]] = 1.\n\n                preds = model.predict(x_pred, verbose=0)[0]\n                next_index = sample(preds, diversity)\n                next_char = indices_char[next_index]\n\n                generated += next_char\n                sentence = sentence[1:] + next_char\n\n                sys.stdout.write(next_char)\n                sys.stdout.flush()\n            print()\n    else:\n        print()\n        print('----- Not generating text after Epoch: %d' % epoch)\n\ngenerate_text = LambdaCallback(on_epoch_end=on_epoch_end)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"978ed431-3eb8-4105-bb42-184b4c412e29","_uuid":"5b8f1398fd5d50607e7ba92961bd43423583b384"},"cell_type":"markdown","source":"### Training the model and generating predictions\nFinally we've made it! Our data is ready (`x` for sequences, `y` for next characters), we've chosen a `batch_size` of `128`, and we've defined a callback function which will print generated text using `model.predict()` at the end of the first epoch followed by every fifth epoch with five different `temperature` setting each time. We have another callback, `ModelCheckpoint`, which will save the best model at each epoch if it's improved based on our loss value (find the saved weights file `weights.hdf5` in the \"Output\" tab of the kernel).\n\nLet's fit our model with these specifications and `epochs = 15` for the number of epochs to train. And of course, let's not forget to put our GPU to use! This will make training/prediction much faster than if we used a CPU. In any case, you will still want to grab some lunch or go for a walk while you wait for the model to train and generate predictions if you're running this code interactively.\n\nP.S. If you're running this interactively in your own notebook, you can click the blue square \"Stop\" button next to the console at the bottom of your screen to interrupt the model training."},{"metadata":{"_cell_guid":"3ef29cdd-e300-4860-b32f-df782e414c40","_uuid":"8fc88c921433d17a5d80a9be1916bd491ac74b75","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"# define the checkpoint\nfilepath = \"weights.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, \n                             monitor='loss', \n                             verbose=1, \n                             save_best_only=True, \n                             mode='min')\n\n# fit model using our gpu\nwith tf.device('/gpu:0'):\n    model.fit(x, y,\n              batch_size=128,\n              epochs=15,\n              verbose=2,\n              callbacks=[generate_text, checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e9f63b7d-7346-4c52-a8e0-1a4873fd5cbf","_uuid":"ddc8d991180276b38a2cc1109013472d102d8e61","collapsed":true},"cell_type":"markdown","source":"## Conclusion\n\nAnd there you have it! If you ran this notebook interactively, you hopefully caught the model printing out generated text character-by-character to dramatic effect. \n\nI hope you've enjoyed learning how to start from a dataframe containing rows of text to using an LSTM model implemented using Keras in Kernels to generate novel sentences thanks to the power of GPUs. You can see how our model improved from the first epoch to the last. The text generated by the model's predictions in the first epoch didn't really resemble English at all. And overall, lower levels of diversity generate text with a lot of repetitions, whereas higher levels of diversity correspond to more gobbledegook. \n\nCan you tweak the model or its hyperparameters to generate even better text? Try it out for yourself by forking this notebook kernel (click \"Fork Notebook\" at the top). \n\n### Inspiration for next steps\n\nHere are just a few ideas for how to take what you learned here and expand it:\n\n1. Experiment with different (hyper)-parameters like the amount of training data, number of epochs or batch sizes, `temperature`, etc.\n2. Try out the same code with different data; fork this notebook, go to the \"Data\" tab and remove the freeCodeCamp data source, then add a different dataset ([good examples here](https://www.kaggle.com/datasets?sortBy=hottest&group=public&page=1&pageSize=20&size=all&filetype=all&license=all&tagids=11208)).\n3. Try out more complicated network architectures like adding dropout layers.\n4. Learn more about deep learning on [Kaggle Learn](https://www.kaggle.com/learn/deep-learning), a series of videos and hands-on notebook tutorials in Kernels.\n5. Use `weights.hdf5` in the \"Output\" to predict based on different data in a new kernel what it would be like if the user in this tutorial completed someone else's sentences.\n6. Compare the speed-up effect of using a CPU versus a GPU on a minimal example."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}