{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Hotel Booking Analysis"},{"metadata":{},"cell_type":"markdown","source":"The aim is to create meaningful estimators from the data set we have and to select the model that predicts the cancellation best by comparing them with the accuracy scores of different ML models and ROC Curves.\n\n### **1- EDA** \n\nContent of exploratory data analysis.\n\n* Repeated guest effect on cancellations.\n* Night spent at hotels.\n* Hotel type with more time spent.\n* Effects of deposit on cancellations by segments.\n* Relationship of lead time with cancellation.\n* Monthly customers and cancellations.\n\n### **2- Preprocessing**\n\nThis part is not much organized because I decided what to do some features with missing values after Correlation and The fact about 'reservation_status' part.\n\n* Handling missing values\n* Handling features\n* Correlation\n* The fact about 'reservation status' (decision tree model)\n* Last arrangements before model comparisons.\n\n### **3- Models and ROC Curve Comparison**\n\nNot all models have tuning part, the best two models tuned.\n\n* Logistic Regression\n* Gaussian Naive Bayes\n* Support Vector Classification\n* Decision Tree Model\n* Random Forest\n* Model Tuning for Random Forest\n* XGBoost\n* Neural Network\n* Model Tuning for Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf\n\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, confusion_matrix, auc\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler \n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/hotel-booking-demand/hotel_bookings.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Quick look \ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"# of NaN in each columns:\", df.isnull().sum(), sep='\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It is better to copy original dataset, it can be needed in some cases.\ndata = df.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. EDA"},{"metadata":{},"cell_type":"markdown","source":"### Cancellations by repeated guests"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"sns.set(style = \"darkgrid\")\nplt.title(\"Canceled or not\", fontdict = {'fontsize': 20})\nax = sns.countplot(x = \"is_canceled\", hue = 'is_repeated_guest', data = data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no surprise that repeated guests do not cancel their reservations. Of course there are some exceptions.\nAlso most of the customers are not repeated guests."},{"metadata":{},"cell_type":"markdown","source":"### Boxplot Distribution of Nights Spent at Hotels by Market Segment and Hotel Type "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,10))\nsns.boxplot(x = \"market_segment\", y = \"stays_in_week_nights\", data = data, hue = \"hotel\", palette = 'Set1');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.boxplot(x = \"market_segment\", y = \"stays_in_weekend_nights\", data = data, hue = \"hotel\", palette = 'Set1');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that most of the groups are normal distributed, some of them have high skewness.\nLooking at the distribution, most people do not seem to prefer to stay at the hotel for more than 1 week. But it seems normal to stay in resort hotels for up to 12-13 days. Although this changes according to the segments, staying longer than 15 days certainly creates outliers for each segment. If the total time feature was created by summing up the weekend and week nights, this would be clearer, but it can be clearly seen when looking at the two visualizations together.\n\nAs it turns out, customers from Aviation Segment do not seem to be staying at the resort hotels and have a relatively lower day average. Apart from that, the weekends and weekdays averages are roughly equal. Customers in the Aviation Segment are likely to arrive shortly due to business. Also probably most airports are a bit away from sea and its most likely to be closer to city hotels.\n\nIt is obvious that when people go to resort hotels, they prefer to stay more."},{"metadata":{},"cell_type":"markdown","source":"### Countplot Distribution of Market Segments"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (13,10))\nsns.set(style = \"darkgrid\")\nplt.title(\"Countplot Distrubiton of Segment by Deposit Type\", fontdict = {'fontsize':20})\nax = sns.countplot(x = \"market_segment\", hue = 'deposit_type', data = data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (13,10))\nsns.set(style = \"darkgrid\")\nplt.title(\"Countplot Distributon of Segments by Cancellation\", fontdict = {'fontsize':20})\nax = sns.countplot(x = \"market_segment\", hue = 'is_canceled', data = data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at Offline TA/TO and Groups, the situations where the deposit was received were only in the scenarios where the groups came. It is quite logical to apply a deposit for a large number of customers who will fill important amount of the hotel capacity.\n\nAs a first thought, I expected the cancellation rate in the market segments where a deposit is applied to be lower than the other segments where no deposit applied. But when we look at the cancellations according to the segments in the other visualization, it seems that this is not the case. \n\n- Groups segment has cancellation rate more than 50%.\n- Offline TA/TO (Travel Agents/Tour Operators) and Online TA has cancellation rate more than 33%.\n- Direct segment has cancellation rate less than 20%.\n\nIt is surprising that the cancellation rate in these segments is high despite the application of a deposit. The fact that cancellations are made collectively like reservations may explain this situation a bit.\n\nCancellation rates for online reservations are as expected in a dynamic environment where the circulation is high.\n\nAnother situation that took my attention is that the cancellation rate in the direct segment is so low. At this point, I think that a mutual trust relationship has been established in case people are communicating one to one. I will not dwell on this much, but I think there is a psychological factor here."},{"metadata":{},"cell_type":"markdown","source":"### Density Curve of Lead Time by Cancelation"},{"metadata":{"trusted":true},"cell_type":"code","source":"(sns.FacetGrid(data, hue = 'is_canceled',\n             height = 6,\n             xlim = (0,500))\n    .map(sns.kdeplot, 'lead_time', shade = True)\n    .add_legend());","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While lead time is more than roughly 60, people tend to cancel their reservations (cancellation rate is higher after this point). \nAlso people want their holiday or work plans resulted in 100 days which equals to half of the data."},{"metadata":{},"cell_type":"markdown","source":"### Monthly Cancellations and Customers by Hotel Types"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize =(13,10))\nsns.set(style=\"darkgrid\")\nplt.title(\"Total Customers - Monthly \", fontdict={'fontsize': 20})\nax = sns.countplot(x = \"arrival_date_month\", hue = 'hotel', data = data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (13,10))\nsns.barplot(x = 'arrival_date_month', y = 'is_canceled', data = data);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,10))\nsns.barplot(x = 'arrival_date_month', y = 'is_canceled', hue = 'hotel', data = data);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the first graph, it can be seen that the city hotels have more customers in all months. Considering proportionally, resort hotels seem to be a little closer to city hotels in summer.\n\nAn important interpretation can be made by examining three graphics together. Fewer customers come in the winter months, so when we look at the cancellation rates, it is quite normal that it appears less in the winter months. The point to be noted on these months is that the cancellation rates of city hotels are almost equal to other months even in winter. The fact that the total cancellation rates of the winter months are low is that the cancellation rates of the resort hotels are low in these months.\nIn short, the possibility of cancellation of resort hotels in winter is very low. This information can be a very important factor when predicting 'is_canceled'."},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing \n### (Missing Values, Feature Engineering and Standardization)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"# of NaN in each columns:\", df.isnull().sum(), sep='\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def perc_mv(x, y):\n    perc = y.isnull().sum() / len(x) * 100\n    return perc\n\nprint('Missing value ratios:\\nCompany: {}\\nAgent: {}\\nCountry: {}'.format(perc_mv(df, df['company']),\n                                                                                   perc_mv(df, df['agent']),\n                                                                                   perc_mv(df, df['country'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"agent\"].value_counts().count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see 94.3% of company column are missing values. Therefore we do not have enough values to fill the rows of company column by predicting, filling by mean etc. It seems that the best option is dropping company column.\n\n13.68% of agent column are missing values, there is no need to drop agent column. But also we should not drop the rows because 13.68% of data is really huge amount and those rows have the chance to have crucial information. There are 333 unique agent, since there are too many agents they may not be predictable. \nAlso NA values can be the agents that are not listed in present 333 agents. We can't predict agents and since missing values are 13% of all data we can't drop them too. I will decide what to do about agent after correlation section.\n\nIt will not be a problem if we drop the rows that have missing values in country column. Still, I will wait for correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# company is dropped\ndata = data.drop(['company'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have also 4 missing values in children column. If there is no information about children, In my opinion those customers do not have any children.\ndata['children'] = data['children'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handling Features"},{"metadata":{},"cell_type":"markdown","source":"We should check the features to create some more meaningful variables and reduce the number of features if it is possible."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I wanted to label them manually. I will do the rest with get.dummies or label_encoder.\ndata['hotel'] = data['hotel'].map({'Resort Hotel':0, 'City Hotel':1})\n\ndata['arrival_date_month'] = data['arrival_date_month'].map({'January':1, 'February': 2, 'March':3, 'April':4, 'May':5, 'June':6, 'July':7,\n                                                            'August':8, 'September':9, 'October':10, 'November':11, 'December':12})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def family(data):\n    if ((data['adults'] > 0) & (data['children'] > 0)):\n        val = 1\n    elif ((data['adults'] > 0) & (data['babies'] > 0)):\n        val = 1\n    else:\n        val = 0\n    return val\n\ndef deposit(data):\n    if ((data['deposit_type'] == 'No Deposit') | (data['deposit_type'] == 'Refundable')):\n        return 0\n    else:\n        return 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature(data):\n    data[\"is_family\"] = data.apply(family, axis = 1)\n    data[\"total_customer\"] = data[\"adults\"] + data[\"children\"] + data[\"babies\"]\n    data[\"deposit_given\"] = data.apply(deposit, axis=1)\n    data[\"total_nights\"] = data[\"stays_in_weekend_nights\"]+ data[\"stays_in_week_nights\"]\n    return data\n\ndata = feature(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Information of these columns is also inside of new features, so it is better to drop them.\n# I did not drop stays_nights features, I can't decide which feature is more important there.\ndata = data.drop(columns = ['adults', 'babies', 'children', 'deposit_type', 'reservation_status_date'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After correlation we will decide what to do about country, agent and total_nights."},{"metadata":{},"cell_type":"markdown","source":"### Correlation"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets copy data to check the correlation between variables. \ncor_data = data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This data will not be used while predicting cancellation. This is just for checking correlation.\ncor_data['meal'] = le.fit_transform(cor_data['meal'])\ncor_data['distribution_channel'] = le.fit_transform(cor_data['distribution_channel'])\ncor_data['reserved_room_type'] = le.fit_transform(cor_data['reserved_room_type'])\ncor_data['assigned_room_type'] = le.fit_transform(cor_data['assigned_room_type'])\ncor_data['agent'] = le.fit_transform(cor_data['agent'])\ncor_data['customer_type'] = le.fit_transform(cor_data['customer_type'])\ncor_data['reservation_status'] = le.fit_transform(cor_data['reservation_status'])\ncor_data['market_segment'] = le.fit_transform(cor_data['market_segment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_data.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_data.corr()[\"is_canceled\"].sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see in the sorted list, reservation_status seems to be most impactful feature. With that information accuracy rate should be really high. It can be better to drop reservation_status column to see how other features can predict. I am going to try both.\n\nImpacts of three feature that are created:\n- deposit_given = 0,48131\n- is_family = -0,01327\n- total_customer = 0,04504\n\nApart from that, I will not use arrival_date_week_number, stays_in_weekend_nights and arrival_date_day_of_month since their importances are really low while predicting cancellations. \n\nAlso, still we have some missing values in agent column. It has nice importance on predicting cancellation but since the missing values are equal to 13% of the total data it is better to drop that column. It has a lot of class inside of it otherwise we could try predicting missing values but they may misguide the predictions."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# It is highly correlated to total_nights and also there is no much difference impact, so I will not use total_nights.\n# Week nights have higher impact.\n\"\"\"\nActually, I tried some models by using different features as (only total_nights | weekend_nights and week_nights | only week_nights ...) \nand the models using only week nights seems to have a bit higher accuracy score. \n\"\"\"\n\ncor_data.corr()['stays_in_week_nights']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_data = cor_data.drop(columns = ['total_nights', 'arrival_date_week_number', 'stays_in_weekend_nights', 'arrival_date_month', 'agent'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets delete the NA rows of country column\nindices = cor_data.loc[pd.isna(cor_data[\"country\"]), :].index \ncor_data = cor_data.drop(cor_data.index[indices])   \ncor_data.isnull().sum()\n\n#There is no missing value in the data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Since we have decided what to do with features and missing values, we can work on first data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"indices = data.loc[pd.isna(data[\"country\"]), :].index \ndata = data.drop(data.index[indices])   \ndata = data.drop(columns = ['arrival_date_week_number', 'stays_in_weekend_nights', 'arrival_date_month', 'agent'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I keep data in case of any changes on features, missing values etc.\ndf1 = data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#one-hot-encoding\ndf1 = pd.get_dummies(data = df1, columns = ['meal', 'market_segment', 'distribution_channel',\n                                            'reserved_room_type', 'assigned_room_type', 'customer_type', 'reservation_status'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['country'] = le.fit_transform(df1['country']) \n# There are more than 300 classes, so I wanted to use label encoder on this feature.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree Model (reservation_status included)"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df1[\"is_canceled\"]\nX = df1.drop([\"is_canceled\"], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cart = DecisionTreeClassifier(max_depth = 12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cart_model = cart.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = cart_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Decision Tree Model')\n\nprint('Accuracy Score: {}\\n\\nConfusion Matrix:\\n {}\\n\\nAUC Score: {}'\n      .format(accuracy_score(y_test,y_pred), confusion_matrix(y_test,y_pred), roc_auc_score(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(data = cart_model.feature_importances_*100,\n                   columns = [\"Importances\"],\n                   index = X_train.columns).sort_values(\"Importances\", ascending = False)[:20].plot(kind = \"barh\", color = \"r\")\n\nplt.xlabel(\"Feature Importances (%)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the correlation part, we have seen the impact of reservation status. Reservation status dominates other features totally. By keeping reservation_status in data, it is possible to achieve 100% accuracy rate because that feature is direct way to predict cancellations, its like cheating. For the sake of analysis I will drop reservation_status and continue analysis without it."},{"metadata":{},"cell_type":"markdown","source":"### Final Arrangements Before Comparing the Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df1.drop(columns = ['reservation_status_Canceled', 'reservation_status_Check-Out', 'reservation_status_No-Show'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df2[\"is_canceled\"]\nX = df2.drop([\"is_canceled\"], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can use the functions to apply the models and roc curves to save space.\ndef model(algorithm, X_train, X_test, y_train, y_test):\n    alg = algorithm\n    alg_model = alg.fit(X_train, y_train)\n    global y_prob, y_pred\n    y_prob = alg.predict_proba(X_test)[:,1]\n    y_pred = alg_model.predict(X_test)\n\n    print('Accuracy Score: {}\\n\\nConfusion Matrix:\\n {}'\n      .format(accuracy_score(y_test,y_pred), confusion_matrix(y_test,y_pred)))\n    \n\ndef ROC(y_test, y_prob):\n    \n    false_positive_rate, true_positive_rate, threshold = roc_curve(y_test, y_prob)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    \n    plt.figure(figsize = (10,10))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(false_positive_rate, true_positive_rate, color = 'red', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1], linestyle = '--')\n    plt.axis('tight')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model and ROC Curve Comparison"},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Model: Logistic Regression\\n')\nmodel(LogisticRegression(solver = \"liblinear\"), X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LogR = LogisticRegression(solver = \"liblinear\")\ncv_scores = cross_val_score(LogR, X, y, cv = 8, scoring = 'accuracy')\nprint('Mean Score of CV: ', cv_scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ROC(y_test, y_prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gaussian Naive Bayes Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Model: Gaussian Naive Bayes\\n')\nmodel(GaussianNB(), X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NB = GaussianNB()\ncv_scores = cross_val_score(NB, X, y, cv = 8, scoring = 'accuracy')\nprint('Mean Score of CV: ', cv_scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ROC(y_test, y_prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Support Vector Classification Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#I excluded probability in the function for SVC, also I could not use other kernel methods because it takes really long and I don't think SVC as a good model for this dateset. \nprint('Model: SVC\\n')\n\ndef model1(algorithm, X_train, X_test, y_train, y_test):\n    alg = algorithm\n    alg_model = alg.fit(X_train, y_train)\n    global y_pred\n    y_pred = alg_model.predict(X_test)\n    \n    print('Accuracy Score: {}\\n\\nConfusion Matrix:\\n {}'\n      .format(accuracy_score(y_test,y_pred), confusion_matrix(y_test,y_pred)))\n    \nmodel1(SVC(kernel = 'linear'), X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree Model (reservation_status excluded)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Model: Decision Tree\\n')\nmodel(DecisionTreeClassifier(max_depth = 12), X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DTC = DecisionTreeClassifier(max_depth = 12)\ncv_scores = cross_val_score(DTC, X, y, cv = 8, scoring = 'accuracy')\nprint('Mean Score of CV: ', cv_scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ROC(y_test, y_prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Model: Random Forest\\n')\nmodel(RandomForestClassifier(), X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RFC = RandomForestClassifier()\ncv_scores = cross_val_score(RFC, X, y, cv = 8, scoring = 'accuracy')\nprint('Mean Score of CV: ', cv_scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ROC(y_test, y_prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Model Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_parameters = {\"max_depth\": [10,13],\n                 \"n_estimators\": [10,100,500],\n                 \"min_samples_split\": [2,5]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = RandomForestClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_cv_model = GridSearchCV(rf_model,\n                           rf_parameters,\n                           cv = 10,\n                           n_jobs = -1,\n                           verbose = 2)\n\nrf_cv_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best parameters: ' + str(rf_cv_model.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_tuned = RandomForestClassifier(max_depth = 13,\n                                  min_samples_split = 2,\n                                  n_estimators = 500)\n\nprint('Model: Random Forest Tuned\\n')\nmodel(rf_tuned, X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tuned model has worse accuracy score than default one. In the default model there is no limit for max depth. Increasing max depth gives us better accuracy scores but may decrease generalization."},{"metadata":{},"cell_type":"markdown","source":"### XGBoost Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Model: XGBoost\\n')\nmodel(XGBClassifier(), X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB = XGBClassifier()\ncv_scores = cross_val_score(XGB, X, y, cv = 8, scoring = 'accuracy')\nprint('Mean Score of CV: ', cv_scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ROC(y_test, y_prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Neural Network Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Model: Neural Network\\n')\nmodel(MLPClassifier(), X_train_scaled, X_test_scaled, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ROC(y_test, y_prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Neural Network Model Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"mlpc_parameters = {\"alpha\": [1, 0.1, 0.01, 0.001],\n                   \"hidden_layer_sizes\": [(50,50,50),\n                                          (100,100)],\n                   \"solver\": [\"adam\", \"sgd\"],\n                   \"activation\": [\"logistic\", \"relu\"]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mlpc = MLPClassifier()\nmlpc_cv_model = GridSearchCV(mlpc, mlpc_parameters,\n                             cv = 10,\n                             n_jobs = -1,\n                             verbose = 2)\n\nmlpc_cv_model.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best parameters: ' + str(mlpc_cv_model.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mlpc_tuned = MLPClassifier(activation = 'relu',\n                           alpha = 0.1,\n                           hidden_layer_sizes = (100,100),\n                           solver = 'adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Model: Neural Network Tuned\\n')\nmodel(mlpc_tuned, X_train_scaled, X_test_scaled, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ROC(y_test, y_prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"### Feature Importances"},{"metadata":{"trusted":true},"cell_type":"code","source":"randomf = RandomForestClassifier()\nrf_model1 = randomf.fit(X_train, y_train)\n\npd.DataFrame(data = rf_model1.feature_importances_*100,\n                   columns = [\"Importances\"],\n                   index = X_train.columns).sort_values(\"Importances\", ascending = False)[:15].plot(kind = \"barh\", color = \"r\")\n\nplt.xlabel(\"Feature Importances (%)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary Table of the Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"table = pd.DataFrame({\"Model\": [\"Decision Tree (reservation status included)\", \"Logistic Regression\",\n                                \"Naive Bayes\", \"Support Vector\", \"Decision Tree\", \"Random Forest\",\n                                \"Random Forest Tuned\", \"XGBoost\", \"Neural Network\", \"Neural Network Tuned\"],\n                     \"Accuracy Scores\": [\"1\", \"0.804\", \"0.582\", \"0.794\", \"0.846\",\n                                         \"0.883\", \"0.851\", \"0.869\", \"0.848\", \"0.859\"],\n                     \"ROC | Auc\": [\"1\", \"0.88\", \"0.78\", \"0\",\n                                   \"0.92\", \"0.95\", \"0\", \"0.94\",\n                                   \"0.93\", \"0.94\"]})\n\n\ntable[\"Model\"] = table[\"Model\"].astype(\"category\")\ntable[\"Accuracy Scores\"] = table[\"Accuracy Scores\"].astype(\"float32\")\ntable[\"ROC | Auc\"] = table[\"ROC | Auc\"].astype(\"float32\")\n\npd.pivot_table(table, index = [\"Model\"]).sort_values(by = 'Accuracy Scores', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- As we can see from the summary table, the best algorithm is random forest for this data set. \n- 0 values are uncalculated ones.\n- We do not count decision tree with reservatiton status which is broken. All algorithms would give 100% accuracy scores while reservation status is included.\n- Tuning for XGBoost would be a good challenge too."},{"metadata":{},"cell_type":"markdown","source":"### Thanks for reading all of my analysis. If you have questions, do not hesitate to ask!\n### I am open to all the feedbacks. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}